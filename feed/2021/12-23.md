
## 2021-12-23

### [[2112.11513] Interference and Coverage Analysis of mmWave Inter-Vehicle Broadcast with Directional Antennas](http://arxiv.org/abs/2112.11513)


  Thanks to the availability of large bandwidth and high-gain directional
antennas at the millimeter-wave (mmWave) bands, mmWave communications have been
considered as one of the primary solutions to meet the high data rates needs in
vehicular networks. Unicast in mmWave vehicle-to-vehicle (V2V) communications
has been well-studied, but much less attention has been paid to V2V broadcast
which is required by many V2V applications such as active safety. To fill the
gap, this paper systematically investigates mmWave V2V broadcast by considering
the unique properties of mmWave signal propagation in V2V environments as well
as the impacts of directional antennas and interference. Based on
widely-accepted, high-fidelity system models, we mathematically analyze the
receiver-side signal-to-interference-plus-noise-ratio (SINR) and broadcast
coverage, and we study the impacts of blockage, inter-vehicle distance, vehicle
density and beam pattern. Through comprehensive numerical analysis, we find out
that, instead of a single unique optimal beamwidth, there exists an optimal
range of beamwidth, in which the beamwidths have similar performance and can
maximize the coverage. We also find out that the selection of carrier sensing
range plays an important role as it highly influences the performance of the
whole vehicular networks. Our analysis provides unique insight into mmWave V2V
broadcast, and it sheds light on designing effective V2V broadcast protocols.

    

### [[2112.11934] A Min-plus Model of Age-of-Information with Worst-case and Statistical Bounds](http://arxiv.org/abs/2112.11934)


  We consider networked sources that generate update messages with a defined
rate and we investigate the age of that information at the receiver. Typical
applications are in cyber-physical systems that depend on timely sensor
updates. We phrase the age of information in the min-plus algebra of the
network calculus. This facilitates a variety of models including wireless
channels and schedulers with random cross-traffic, as well as sources with
periodic and random updates, respectively. We show how the age of information
depends on the network service where, e.g., outages of a wireless channel cause
delays. Further, our analytical expressions show two regimes depending on the
update rate, where the age of information is either dominated by congestive
delays or by idle waiting. We find that the optimal update rate strikes a
balance between these two effects.

    

### [[2106.02533] Graph-based Deep Learning for Communication Networks: A Survey](http://arxiv.org/abs/2106.02533)


  Communication networks are important infrastructures in contemporary society.
There are still many challenges that are not fully solved and new solutions are
proposed continuously in this active research area. In recent years, to model
the network topology, graph-based deep learning has achieved the
state-of-the-art performance in a series of problems in communication networks.
In this survey, we review the rapidly growing body of research using different
graph-based deep learning models, e.g. graph convolutional and graph attention
networks, in various problems from different types of communication networks,
e.g. wireless networks, wired networks, and software defined networks. We also
present a well-organized list of the problem and solution for each study and
identify future research directions. To the best of our knowledge, this paper
is the first survey that focuses on the application of graph-based deep
learning methods in communication networks involving both wired and wireless
scenarios. To track the follow-up research, a public GitHub repository is
created, where the relevant papers will be updated continuously.

    

### [[2112.11461] Deep Reinforcement Learning for Optimal Power Flow with Renewables Using Spatial-Temporal Graph Information](http://arxiv.org/abs/2112.11461)


  Renewable energy resources (RERs) have been increasingly integrated into
modern power systems, especially in large-scale distribution networks (DNs). In
this paper, we propose a deep reinforcement learning (DRL)-based approach to
dynamically search for the optimal operation point, i.e., optimal power flow
(OPF), in DNs with a high uptake of RERs. Considering uncertainties and voltage
fluctuation issues caused by RERs, we formulate OPF into a multi-objective
optimization (MOO) problem. To solve the MOO problem, we develop a novel DRL
algorithm leveraging the graphical information of the distribution network.
Specifically, we employ the state-of-the-art DRL algorithm, i.e., deep
deterministic policy gradient (DDPG), to learn an optimal strategy for OPF.
Since power flow reallocation in the DN is a consecutive process, where nodes
are self-correlated and interrelated in temporal and spatial views, to make
full use of DNs' graphical information, we develop a multi-grained
attention-based spatial-temporal graph convolution network (MG-ASTGCN) for
spatial-temporal graph information extraction, preparing for its sequential
DDPG. We validate our proposed DRL-based approach in modified IEEE 33, 69, and
118-bus radial distribution systems (RDSs) and show that our DRL-based approach
outperforms other benchmark algorithms. Our experimental results also reveal
that MG-ASTGCN can significantly accelerate the DDPG training process and
improve DDPG's capability in reallocating power flow for OPF. The proposed
DRL-based approach also promotes DNs' stability in the presence of node faults,
especially for large-scale DNs.

    

### [[2112.11471] Towards a Science of Human-AI Decision Making: A Survey of Empirical Studies](http://arxiv.org/abs/2112.11471)


  As AI systems demonstrate increasingly strong predictive performance, their
adoption has grown in numerous domains. However, in high-stakes domains such as
criminal justice and healthcare, full automation is often not desirable due to
safety, ethical, and legal concerns, yet fully manual approaches can be
inaccurate and time consuming. As a result, there is growing interest in the
research community to augment human decision making with AI assistance. Besides
developing AI technologies for this purpose, the emerging field of human-AI
decision making must embrace empirical approaches to form a foundational
understanding of how humans interact and work with AI to make decisions. To
invite and help structure research efforts towards a science of understanding
and improving human-AI decision making, we survey recent literature of
empirical human-subject studies on this topic. We summarize the study design
choices made in over 100 papers in three important aspects: (1) decision tasks,
(2) AI models and AI assistance elements, and (3) evaluation metrics. For each
aspect, we summarize current trends, discuss gaps in current practices of the
field, and make a list of recommendations for future research. Our survey
highlights the need to develop common frameworks to account for the design and
research spaces of human-AI decision making, so that researchers can make
rigorous choices in study design, and the research community can build on each
other's work and produce generalizable scientific knowledge. We also hope this
survey will serve as a bridge for HCI and AI communities to work together to
mutually shape the empirical science and computational technologies for
human-AI decision making.

    

### [[2112.11478] LSH methods for data deduplication in a Wikipedia artificial dataset](http://arxiv.org/abs/2112.11478)


  This paper illustrates locality sensitive hasing (LSH) models for the
identification and removal of nearly redundant data in a text dataset. To
evaluate the different models, we create an artificial dataset for data
deduplication using English Wikipedia articles. Area-Under-Curve (AUC) over 0.9
were observed for most models, with the best model reaching 0.96. Deduplication
enables more effective model training by preventing the model from learning a
distribution that differs from the real one as a result of the repeated data.

    

### [[2112.11479] AtteSTNet -- An attention and subword tokenization based approach for code-switched Hindi-English hate speech detection](http://arxiv.org/abs/2112.11479)


  Recent advancements in technology have led to a boost in social media usage
which has ultimately led to large amounts of user-generated data which also
includes hateful and offensive speech. The language used in social media is
often a combination of English and the native language in the region. In India,
Hindi is used predominantly and is often code-switched with English, giving
rise to the Hinglish (Hindi+English) language. Various approaches have been
made in the past to classify the code-mixed Hinglish hate speech using
different machine learning and deep learning-based techniques. However, these
techniques make use of recurrence on convolution mechanisms which are
computationally expensive and have high memory requirements. Past techniques
also make use of complex data processing making the existing techniques very
complex and non-sustainable to change in data. We propose a much simpler
approach which is not only at par with these complex networks but also exceeds
performance with the use of subword tokenization algorithms like BPE and
Unigram along with multi-head attention-based technique giving an accuracy of
87.41% and F1 score of 0.851 on standard datasets. Efficient use of BPE and
Unigram algorithms help handle the non-conventional Hinglish vocabulary making
our technique simple, efficient and sustainable to use in the real world.

    

### [[2112.11480] On the Compression of Natural Language Models](http://arxiv.org/abs/2112.11480)


  Deep neural networks are effective feature extractors but they are
prohibitively large for deployment scenarios. Due to the huge number of
parameters, interpretability of parameters in different layers is not
straight-forward. This is why neural networks are sometimes considered black
boxes. Although simpler models are easier to explain, finding them is not easy.
If found, a sparse network that can fit to a data from scratch would help to
interpret parameters of a neural network. To this end, lottery ticket
hypothesis states that typical dense neural networks contain a small sparse
sub-network that can be trained to a reach similar test accuracy in an equal
number of steps. The goal of this work is to assess whether such a trainable
subnetwork exists for natural language models (NLM)s. To achieve this goal we
will review state-of-the-art compression techniques such as quantization,
knowledge distillation, and pruning.

    

### [[2112.11483] BACON: Deep-Learning Powered AI for Poetry Generation with Author Linguistic Style Transfer](http://arxiv.org/abs/2112.11483)


  This paper describes BACON, a basic prototype of an automatic poetry
generator with author linguistic style transfer. It combines concepts and
techniques from finite state machinery, probabilistic models, artificial neural
networks and deep learning, to write original poetry with rich
aesthetic-qualities in the style of any given author. Extrinsic evaluation of
the output generated by BACON shows that participants were unable to tell the
difference between human and AI-generated poems in any statistically
significant way.

    

### [[2112.11485] On-the-fly Resource-Aware Model Aggregation for Federated Learning in Heterogeneous Edge](http://arxiv.org/abs/2112.11485)


  Edge computing has revolutionized the world of mobile and wireless networks
world thanks to its flexible, secure, and performing characteristics. Lately,
we have witnessed the increasing use of it to make more performing the
deployment of machine learning (ML) techniques such as federated learning (FL).
FL was debuted to improve communication efficiency compared to conventional
distributed machine learning (ML). The original FL assumes a central
aggregation server to aggregate locally optimized parameters and might bring
reliability and latency issues. In this paper, we conduct an in-depth study of
strategies to replace this central server by a flying master that is
dynamically selected based on the current participants and/or available
resources at every FL round of optimization. Specifically, we compare different
metrics to select this flying master and assess consensus algorithms to perform
the selection. Our results demonstrate a significant reduction of runtime using
our flying master FL framework compared to the original FL from measurements
results conducted in our EdgeAI testbed and over real 5G networks using an
operational edge testbed.

    

### [[2112.11490] Do Androids Dream of Electric Fences? Safety-Aware Reinforcement Learning with Latent Shielding](http://arxiv.org/abs/2112.11490)


  The growing trend of fledgling reinforcement learning systems making their
way into real-world applications has been accompanied by growing concerns for
their safety and robustness. In recent years, a variety of approaches have been
put forward to address the challenges of safety-aware reinforcement learning;
however, these methods often either require a handcrafted model of the
environment to be provided beforehand, or that the environment is relatively
simple and low-dimensional. We present a novel approach to safety-aware deep
reinforcement learning in high-dimensional environments called latent
shielding. Latent shielding leverages internal representations of the
environment learnt by model-based agents to "imagine" future trajectories and
avoid those deemed unsafe. We experimentally demonstrate that this approach
leads to improved adherence to formally-defined safety specifications.

    

### [[2112.11491] Adversarial Neural Networks for Error Correcting Codes](http://arxiv.org/abs/2112.11491)


  Error correcting codes are a fundamental component in modern day
communication systems, demanding extremely high throughput, ultra-reliability
and low latency. Recent approaches using machine learning (ML) models as the
decoders offer both improved performance and great adaptability to unknown
environments, where traditional decoders struggle. We introduce a general
framework to further boost the performance and applicability of ML models. We
propose to combine ML decoders with a competing discriminator network that
tries to distinguish between codewords and noisy words, and, hence, guides the
decoding models to recover transmitted codewords. Our framework is
game-theoretic, motivated by generative adversarial networks (GANs), with the
decoder and discriminator competing in a zero-sum game. The decoder learns to
simultaneously decode and generate codewords while the discriminator learns to
tell the differences between decoded outputs and codewords. Thus, the decoder
is able to decode noisy received signals into codewords, increasing the
probability of successful decoding. We show a strong connection of our
framework with the optimal maximum likelihood decoder by proving that this
decoder defines a Nash equilibrium point of our game. Hence, training to
equilibrium has a good possibility of achieving the optimal maximum likelihood
performance. Moreover, our framework does not require training labels, which
are typically unavailable during communications, and, thus, seemingly can be
trained online and adapt to channel dynamics. To demonstrate the performance of
our framework, we combine it with the very recent neural decoders and show
improved performance compared to the original models and traditional decoding
algorithms on various codes.

    

### [[2112.11494] Sentence Embeddings and High-speed Similarity Search for Fast Computer Assisted Annotation of Legal Documents](http://arxiv.org/abs/2112.11494)


  Human-performed annotation of sentences in legal documents is an important
prerequisite to many machine learning based systems supporting legal tasks.
Typically, the annotation is done sequentially, sentence by sentence, which is
often time consuming and, hence, expensive. In this paper, we introduce a
proof-of-concept system for annotating sentences "laterally." The approach is
based on the observation that sentences that are similar in meaning often have
the same label in terms of a particular type system. We use this observation in
allowing annotators to quickly view and annotate sentences that are
semantically similar to a given sentence, across an entire corpus of documents.
Here, we present the interface of the system and empirically evaluate the
approach. The experiments show that lateral annotation has the potential to
make the annotation process quicker and more consistent.

    

### [[2112.11507] Multiple Imputation via Generative Adversarial Network for High-dimensional Blockwise Missing Value Problems](http://arxiv.org/abs/2112.11507)


  Missing data are present in most real world problems and need careful
handling to preserve the prediction accuracy and statistical consistency in the
downstream analysis. As the gold standard of handling missing data, multiple
imputation (MI) methods are proposed to account for the imputation uncertainty
and provide proper statistical inference.
In this work, we propose Multiple Imputation via Generative Adversarial
Network (MI-GAN), a deep learning-based (in specific, a GAN-based) multiple
imputation method, that can work under missing at random (MAR) mechanism with
theoretical support. MI-GAN leverages recent progress in conditional generative
adversarial neural works and shows strong performance matching existing
state-of-the-art imputation methods on high-dimensional datasets, in terms of
imputation error. In particular, MI-GAN significantly outperforms other
imputation methods in the sense of statistical inference and computational
speed.

    

### [[2112.11514] The Phonetic Footprint of Parkinson's Disease](http://arxiv.org/abs/2112.11514)


  As one of the most prevalent neurodegenerative disorders, Parkinson's disease
(PD) has a significant impact on the fine motor skills of patients. The complex
interplay of different articulators during speech production and realization of
required muscle tension become increasingly difficult, thus leading to a
dysarthric speech. Characteristic patterns such as vowel instability, slurred
pronunciation and slow speech can often be observed in the affected individuals
and were analyzed in previous studies to determine the presence and progression
of PD. In this work, we used a phonetic recognizer trained exclusively on
healthy speech data to investigate how PD affected the phonetic footprint of
patients. We rediscovered numerous patterns that had been described in previous
contributions although our system had never seen any pathological speech
previously. Furthermore, we could show that intermediate activations from the
neural network could serve as feature vectors encoding information related to
the disease state of individuals. We were also able to directly correlate the
expert-rated intelligibility of a speaker with the mean confidence of phonetic
predictions. Our results support the assumption that pathological data is not
necessarily required to train systems that are capable of analyzing PD speech.

    

### [[2112.11532] Off Environment Evaluation Using Convex Risk Minimization](http://arxiv.org/abs/2112.11532)


  Applying reinforcement learning (RL) methods on robots typically involves
training a policy in simulation and deploying it on a robot in the real world.
Because of the model mismatch between the real world and the simulator, RL
agents deployed in this manner tend to perform suboptimally. To tackle this
problem, researchers have developed robust policy learning algorithms that rely
on synthetic noise disturbances. However, such methods do not guarantee
performance in the target environment. We propose a convex risk minimization
algorithm to estimate the model mismatch between the simulator and the target
domain using trajectory data from both environments. We show that this
estimator can be used along with the simulator to evaluate performance of an RL
agents in the target domain, effectively bridging the gap between these two
environments. We also show that the convergence rate of our estimator to be of
the order of ${n^{-1/4}}$, where $n$ is the number of training samples. In
simulation, we demonstrate how our method effectively approximates and
evaluates performance on Gridworld, Cartpole, and Reacher environments on a
range of policies. We also show that the our method is able to estimate
performance of a 7 DOF robotic arm using the simulator and remotely collected
data from the robot in the real world.

    

### [[2112.11534] Noise-injected analog Ising machines enable ultrafast statistical sampling and machine learning](http://arxiv.org/abs/2112.11534)


  Ising machines are a promising non-von-Neumann computational concept for
neural network training and combinatorial optimization. However, while various
neural networks can be implemented with Ising machines, their inability to
perform fast statistical sampling makes them inefficient for training these
neural networks compared to digital computers. Here, we introduce a universal
concept to achieve ultrafast statistical sampling with Ising machines by
injecting analog noise. With an opto-electronic Ising machine, we demonstrate
that this can be used for accurate sampling of Boltzmann distributions and
unsupervised training of neural networks, with equal accuracy as software-based
training. Through simulations, we find that Ising machines can perform
statistical sampling orders-of-magnitudes faster than software-based methods.
This makes Ising machines into efficient tools for machine learning and other
applications beyond combinatorial optimization.

    

### [[2112.11541] Teacher-Student Architecture for Mixed Supervised Lung Tumor Segmentation](http://arxiv.org/abs/2112.11541)


  Purpose: Automating tasks such as lung tumor localization and segmentation in
radiological images can free valuable time for radiologists and other clinical
personnel. Convolutional neural networks may be suited for such tasks, but
require substantial amounts of labeled data to train. Obtaining labeled data is
a challenge, especially in the medical domain. Methods: This paper investigates
the use of a teacher-student design to utilize datasets with different types of
supervision to train an automatic model performing pulmonary tumor segmentation
on computed tomography images. The framework consists of two models: the
student that performs end-to-end automatic tumor segmentation and the teacher
that supplies the student additional pseudo-annotated data during training.
Results: Using only a small proportion of semantically labeled data and a large
number of bounding box annotated data, we achieved competitive performance
using a teacher-student design. Models trained on larger amounts of semantic
annotations did not perform better than those trained on teacher-annotated
data. Conclusions: Our results demonstrate the potential of utilizing
teacher-student designs to reduce the annotation load, as less supervised
annotation schemes may be performed, without any real degradation in
segmentation accuracy.

    

### [[2112.11542] MIA-Former: Efficient and Robust Vision Transformers via Multi-grained Input-Adaptation](http://arxiv.org/abs/2112.11542)


  ViTs are often too computationally expensive to be fitted onto real-world
resource-constrained devices, due to (1) their quadratically increased
complexity with the number of input tokens and (2) their overparameterized
self-attention heads and model depth. In parallel, different images are of
varied complexity and their different regions can contain various levels of
visual information, indicating that treating all regions/tokens equally in
terms of model complexity is unnecessary while such opportunities for trimming
down ViTs' complexity have not been fully explored. To this end, we propose a
Multi-grained Input-adaptive Vision Transformer framework dubbed MIA-Former
that can input-adaptively adjust the structure of ViTs at three
coarse-to-fine-grained granularities (i.e., model depth and the number of model
heads/tokens). In particular, our MIA-Former adopts a low-cost network trained
with a hybrid supervised and reinforcement training method to skip unnecessary
layers, heads, and tokens in an input adaptive manner, reducing the overall
computational cost. Furthermore, an interesting side effect of our MIA-Former
is that its resulting ViTs are naturally equipped with improved robustness
against adversarial attacks over their static counterparts, because
MIA-Former's multi-grained dynamic control improves the model diversity similar
to the effect of ensemble and thus increases the difficulty of adversarial
attacks against all its sub-models. Extensive experiments and ablation studies
validate that the proposed MIA-Former framework can effectively allocate
computation budgets adaptive to the difficulty of input images meanwhile
increase robustness, achieving state-of-the-art (SOTA) accuracy-efficiency
trade-offs, e.g., 20% computation savings with the same or even a higher
accuracy compared with SOTA dynamic transformer models.

    

### [[2112.11547] Decompose the Sounds and Pixels, Recompose the Events](http://arxiv.org/abs/2112.11547)


  In this paper, we propose a framework centering around a novel architecture
called the Event Decomposition Recomposition Network (EDRNet) to tackle the
Audio-Visual Event (AVE) localization problem in the supervised and weakly
supervised settings. AVEs in the real world exhibit common unravelling patterns
(termed as Event Progress Checkpoints (EPC)), which humans can perceive through
the cooperation of their auditory and visual senses. Unlike earlier methods
which attempt to recognize entire event sequences, the EDRNet models EPCs and
inter-EPC relationships using stacked temporal convolutions. Based on the
postulation that EPC representations are theoretically consistent for an event
category, we introduce the State Machine Based Video Fusion, a novel
augmentation technique that blends source videos using different EPC template
sequences. Additionally, we design a new loss function called the
Land-Shore-Sea loss to compactify continuous foreground and background
representations. Lastly, to alleviate the issue of confusing events during weak
supervision, we propose a prediction stabilization method called Bag to
Instance Label Correction. Experiments on the AVE dataset show that our
collective framework outperforms the state-of-the-art by a sizable margin.

    

### [[2112.11572] Practical Active Learning with Model Selection for Small Data](http://arxiv.org/abs/2112.11572)


  Active learning is of great interest for many practical applications,
especially in industry and the physical sciences, where there is a strong need
to minimize the number of costly experiments necessary to train predictive
models. However, there remain significant challenges for the adoption of active
learning methods in many practical applications. One important challenge is
that many methods assume a fixed model, where model hyperparameters are chosen
a priori. In practice, it is rarely true that a good model will be known in
advance. Existing methods for active learning with model selection typically
depend on a medium-sized labeling budget. In this work, we focus on the case of
having a very small labeling budget, on the order of a few dozen data points,
and develop a simple and fast method for practical active learning with model
selection. Our method is based on an underlying pool-based active learner for
binary classification using support vector classification with a radial basis
function kernel. First we show empirically that our method is able to find
hyperparameters that lead to the best performance compared to an oracle model
on less separable, difficult to classify datasets, and reasonable performance
on datasets that are more separable and easier to classify. Then, we
demonstrate that it is possible to refine our model selection method using a
weighted approach to trade-off between achieving optimal performance on
datasets that are easy to classify, versus datasets that are difficult to
classify, which can be tuned based on prior domain knowledge about the dataset.

    

### [[2112.11577] Learning Positional Embeddings for Coordinate-MLPs](http://arxiv.org/abs/2112.11577)


  We propose a novel method to enhance the performance of coordinate-MLPs by
learning instance-specific positional embeddings. End-to-end optimization of
positional embedding parameters along with network weights leads to poor
generalization performance. Instead, we develop a generic framework to learn
the positional embedding based on the classic graph-Laplacian regularization,
which can implicitly balance the trade-off between memorization and
generalization. This framework is then used to propose a novel positional
embedding scheme, where the hyperparameters are learned per coordinate (i.e,
instance) to deliver optimal performance. We show that the proposed embedding
achieves better performance with higher stability compared to the
well-established random Fourier features (RFF). Further, we demonstrate that
the proposed embedding scheme yields stable gradients, enabling seamless
integration into deep architectures as intermediate layers.

    

### [[2112.11592] Neural Echo State Network using oscillations of gas bubbles in water: Computational validation by Mackey-Glass time series forecasting](http://arxiv.org/abs/2112.11592)


  Physical reservoir computing (RC) is a computational framework, where machine
learning algorithms designed for digital computers are executed using analog
computer-like nonlinear physical systems that can provide high computational
power for predicting time-dependent quantities that can be found using
nonlinear differential equations. Here we suggest an RC system that combines
the nonlinearity of an acoustic response of a cluster of oscillating gas
bubbles in water with a standard Echo State Network (ESN) algorithm that is
well-suited to forecast nonlinear and chaotic time series. We computationally
confirm the plausibility of the proposed RC system by demonstrating its ability
to forecast a chaotic Mackey-Glass time series with the efficiency of ESN.

    

### [[2112.11594] GCoD: Graph Convolutional Network Acceleration via Dedicated Algorithm and Accelerator Co-Design](http://arxiv.org/abs/2112.11594)


  Graph Convolutional Networks (GCNs) have emerged as the state-of-the-art
graph learning model. However, it can be notoriously challenging to inference
GCNs over large graph datasets, limiting their application to large real-world
graphs and hindering the exploration of deeper and more sophisticated GCN
graphs. This is because real-world graphs can be extremely large and sparse.
Furthermore, the node degree of GCNs tends to follow the power-law distribution
and therefore have highly irregular adjacency matrices, resulting in
prohibitive inefficiencies in both data processing and movement and thus
substantially limiting the achievable GCN acceleration efficiency. To this end,
this paper proposes a GCN algorithm and accelerator Co-Design framework dubbed
GCoD which can largely alleviate the aforementioned GCN irregularity and boost
GCNs' inference efficiency. Specifically, on the algorithm level, GCoD
integrates a split and conquer GCN training strategy that polarizes the graphs
to be either denser or sparser in local neighborhoods without compromising the
model accuracy, resulting in graph adjacency matrices that (mostly) have merely
two levels of workload and enjoys largely enhanced regularity and thus ease of
acceleration. On the hardware level, we further develop a dedicated two-pronged
accelerator with a separated engine to process each of the aforementioned
denser and sparser workloads, further boosting the overall utilization and
acceleration efficiency. Extensive experiments and ablation studies validate
that our GCoD consistently reduces the number of off-chip accesses, leading to
speedups of 15286x, 294x, 7.8x, and 2.5x as compared to CPUs, GPUs, and
prior-art GCN accelerators including HyGCN and AWB-GCN, respectively, while
maintaining or even improving the task accuracy.

    

### [[2112.11600] Analytical Modelling of Exoplanet Transit Specroscopy with Dimensional Analysis and Symbolic Regression](http://arxiv.org/abs/2112.11600)


  The physical characteristics and atmospheric chemical composition of newly
discovered exoplanets are often inferred from their transit spectra which are
obtained from complex numerical models of radiative transfer. Alternatively,
simple analytical expressions provide insightful physical intuition into the
relevant atmospheric processes. The deep learning revolution has opened the
door for deriving such analytical results directly with a computer algorithm
fitting to the data. As a proof of concept, we successfully demonstrate the use
of symbolic regression on synthetic data for the transit radii of generic hot
Jupiter exoplanets to derive a corresponding analytical formula. As a
preprocessing step, we use dimensional analysis to identify the relevant
dimensionless combinations of variables and reduce the number of independent
inputs, which improves the performance of the symbolic regression. The
dimensional analysis also allowed us to mathematically derive and properly
parametrize the most general family of degeneracies among the input atmospheric
parameters which affect the characterization of an exoplanet atmosphere through
transit spectroscopy.

    

### [[2112.11602] Identifying Mixtures of Bayesian Network Distributions](http://arxiv.org/abs/2112.11602)


  A Bayesian Network is a directed acyclic graph (DAG) on a set of $n$ random
variables (identified with the vertices); a Bayesian Network Distribution (BND)
is a probability distribution on the rv's that is Markovian on the graph. A
finite mixture of such models is the projection on these variables of a BND on
the larger graph which has an additional "hidden" (or "latent") random variable
$U$, ranging in $\{1,\ldots,k\}$, and a directed edge from $U$ to every other
vertex.
Models of this type are fundamental to research in Causal Inference, where
$U$ models a confounding effect. One extremely special case has been of
longstanding interest in the theory literature: the empty graph. Such a
distribution is simply a mixture of $k$ product distributions. A longstanding
problem has been, given the joint distribution of a mixture of $k$ product
distributions, to identify each of the product distributions, and their mixture
weights. Our results are:
(1) We improve the sample complexity (and runtime) for identifying mixtures
of $k$ product distributions from $\exp(O(k^2))$ to $\exp(O(k \log k))$. This
is almost best possible in view of a known $\exp(\Omega(k))$ lower bound.
(2) We give the first algorithm for the case of non-empty graphs. The
complexity for a graph of maximum degree $\Delta$ is $\exp(O(k(\Delta^2 + \log
k)))$.
(The above complexities are approximate and suppress dependence on secondary
parameters.)

    

### [[2112.11619] A Convergent ADMM Framework for Efficient Neural Network Training](http://arxiv.org/abs/2112.11619)


  As a well-known optimization framework, the Alternating Direction Method of
Multipliers (ADMM) has achieved tremendous success in many classification and
regression applications. Recently, it has attracted the attention of deep
learning researchers and is considered to be a potential substitute to Gradient
Descent (GD). However, as an emerging domain, several challenges remain
unsolved, including 1) The lack of global convergence guarantees, 2) Slow
convergence towards solutions, and 3) Cubic time complexity with regard to
feature dimensions. In this paper, we propose a novel optimization framework to
solve a general neural network training problem via ADMM (dlADMM) to address
these challenges simultaneously. Specifically, the parameters in each layer are
updated backward and then forward so that parameter information in each layer
is exchanged efficiently. When the dlADMM is applied to specific architectures,
the time complexity of subproblems is reduced from cubic to quadratic via a
dedicated algorithm design utilizing quadratic approximations and backtracking
techniques. Last but not least, we provide the first proof of convergence to a
critical point sublinearly for an ADMM-type method (dlADMM) under mild
conditions. Experiments on seven benchmark datasets demonstrate the
convergence, efficiency, and effectiveness of our proposed dlADMM algorithm.

    

### [[2112.11622] An Alternate Policy Gradient Estimator for Softmax Policies](http://arxiv.org/abs/2112.11622)


  Policy gradient (PG) estimators for softmax policies are ineffective with
sub-optimally saturated initialization, which happens when the density
concentrates on a sub-optimal action. Sub-optimal policy saturation may arise
from bad policy initialization or sudden changes in the environment that occur
after the policy has already converged, and softmax PG estimators require a
large number of updates to recover an effective policy. This severe issue
causes high sample inefficiency and poor adaptability to new situations. To
mitigate this problem, we propose a novel policy gradient estimator for softmax
policies that utilizes the bias in the critic estimate and the noise present in
the reward signal to escape the saturated regions of the policy parameter
space. Our analysis and experiments, conducted on bandits and classical MDP
benchmarking tasks, show that our estimator is more robust to policy
saturation.

    

### [[2112.11628] SkipNode: On Alleviating Over-smoothing for Deep Graph Convolutional Networks](http://arxiv.org/abs/2112.11628)


  Over-smoothing is a challenging problem, which degrades the performance of
deep graph convolutional networks (GCNs). However, existing studies for
alleviating the over-smoothing problem lack either generality or effectiveness.
In this paper, we analyze the underlying issues behind the over-smoothing
problem, i.e., feature-diversity degeneration, gradient vanishing, and model
weights over-decaying. Inspired by this, we propose a simple yet effective
plug-and-play module, SkipNode, to alleviate over-smoothing. Specifically, for
each middle layer of a GCN model, SkipNode randomly (or based on node degree)
selects nodes to skip the convolutional operation by directly feeding their
input features to the nonlinear function. Analytically, 1) skipping the
convolutional operation prevents the features from losing diversity; and 2) the
"skipped" nodes enable gradients to be directly passed back, thus mitigating
the gradient vanishing and model weights over-decaying issues. To demonstrate
the superiority of SkipNode, we conduct extensive experiments on nine popular
datasets, including both homophilic and heterophilic graphs, with different
graph sizes on two typical tasks: node classification and link prediction.
Specifically, 1) SkipNode has strong generalizability of being applied to
various GCN-based models on different datasets and tasks; and 2) SkipNode
outperforms recent state-of-the-art anti-over-smoothing plug-and-play modules,
i.e., DropEdge and DropNode, in different settings. Code will be made publicly
available on GitHub.

    

### [[2112.11629] Convolutional neural network based on transfer learning for breast cancer screening](http://arxiv.org/abs/2112.11629)


  Breast cancer is the most common cancer in the world and the most prevalent
cause of death among women worldwide. Nevertheless, it is also one of the most
treatable malignancies if detected early. In this paper, a deep convolutional
neural network-based algorithm is proposed to aid in accurately identifying
breast cancer from ultrasonic images. In this algorithm, several neural
networks are fused in a parallel architecture to perform the classification
process and the voting criteria are applied in the final classification
decision between the candidate object classes where the output of each neural
network is representing a single vote. Several experiments were conducted on
the breast ultrasound dataset consisting of 537 Benign, 360 malignant, and 133
normal images. These experiments show an optimistic result and a capability of
the proposed model to outperform many state-of-the-art algorithms on several
measures. Using k-fold cross-validation and a bagging classifier ensemble, we
achieved an accuracy of 99.5% and a sensitivity of 99.6%.

    

### [[2112.11643] Exploring Credibility Scoring Metrics of Perception Systems for Autonomous Driving](http://arxiv.org/abs/2112.11643)


  Autonomous and semi-autonomous vehicles' perception algorithms can encounter
situations with erroneous object detection, such as misclassification of
objects on the road, which can lead to safety violations and potentially fatal
consequences. While there has been substantial work in the robustness of object
detection algorithms and online metric learning, there is little research on
benchmarking scoring metrics to determine any possible indicators of potential
misclassification. An emphasis is put on exploring the potential of taking
these scoring metrics online in order to allow the AV to make perception-based
decisions given real-time constraints. In this work, we explore which, if any,
metrics act as online indicators of when perception algorithms and object
detectors are failing. Our work provides insight on better design principles
and characteristics of online metrics to accurately evaluate the credibility of
object detectors. Our approach employs non-adversarial and realistic
perturbations to images, on which we evaluate various quantitative metrics. We
found that offline metrics can be designed to account for real-world
corruptions such as poor weather conditions and that the analysis of such
metrics can provide a segue into designing online metrics. This is a clear next
step as it can allow for error-free autonomous vehicle perception and safer
time-critical and safety-critical decision-making.

    

### [[2112.11656] Latent Space Simulation for Carbon Capture Design Optimization](http://arxiv.org/abs/2112.11656)


  The CO2 capture efficiency in solvent-based carbon capture systems (CCSs)
critically depends on the gas-solvent interfacial area (IA), making
maximization of IA a foundational challenge in CCS design. While the IA
associated with a particular CCS design can be estimated via a computational
fluid dynamics (CFD) simulation, using CFD to derive the IAs associated with
numerous CCS designs is prohibitively costly. Fortunately, previous works such
as Deep Fluids (DF) (Kim et al., 2019) show that large simulation speedups are
achievable by replacing CFD simulators with neural network (NN) surrogates that
faithfully mimic the CFD simulation process. This raises the possibility of a
fast, accurate replacement for a CFD simulator and therefore efficient
approximation of the IAs required by CCS design optimization. Thus, here, we
build on the DF approach to develop surrogates that can successfully be applied
to our complex carbon-capture CFD simulations. Our optimized DF-style
surrogates produce large speedups (4000x) while obtaining IA relative errors as
low as 4% on unseen CCS configurations that lie within the range of training
configurations. This hints at the promise of NN surrogates for our CCS design
optimization problem. Nonetheless, DF has inherent limitations with respect to
CCS design (e.g., limited transferability of trained models to new CCS
packings). We conclude with ideas to address these challenges.

    

### [[2112.11660] An Attention Score Based Attacker for Black-box NLP Classifier](http://arxiv.org/abs/2112.11660)


  Deep neural networks have a wide range of applications in solving various
real-world tasks and have achieved satisfactory results, in domains such as
computer vision, image classification, and natural language processing.
Meanwhile, the security and robustness of neural networks have become
imperative, as diverse researches have shown the vulnerable aspects of neural
networks. Case in point, in Natural language processing tasks, the neural
network may be fooled by an attentively modified text, which has a high
similarity to the original one. As per previous research, most of the studies
are focused on the image domain; Different from image adversarial attacks, the
text is represented in a discrete sequence, traditional image attack methods
are not applicable in the NLP field. In this paper, we propose a word-level NLP
sentiment classifier attack model, which includes a self-attention
mechanism-based word selection method and a greedy search algorithm for word
substitution. We experiment with our attack model by attacking GRU and 1D-CNN
victim models on IMDB datasets. Experimental results demonstrate that our model
achieves a higher attack success rate and more efficient than previous methods
due to the efficient word selection algorithms are employed and minimized the
word substitute number. Also, our model is transferable, which can be used in
the image domain with several modifications.

    

### [[2112.11663] Accelerated Proximal Alternating Gradient-Descent-Ascent for Nonconvex Minimax Machine Learning](http://arxiv.org/abs/2112.11663)


  Alternating gradient-descent-ascent (AltGDA) is an optimization algorithm
that has been widely used for model training in various machine learning
applications, which aim to solve a nonconvex minimax optimization problem.
However, the existing studies show that it suffers from a high computation
complexity in nonconvex minimax optimization. In this paper, we develop a
single-loop and fast AltGDA-type algorithm that leverages proximal gradient
updates and momentum acceleration to solve regularized nonconvex minimax
optimization problems. By identifying the intrinsic Lyapunov function of this
algorithm, we prove that it converges to a critical point of the nonconvex
minimax optimization problem and achieves a computation complexity
$\mathcal{O}(\kappa^{1.5}\epsilon^{-2})$, where $\epsilon$ is the desired level
of accuracy and $\kappa$ is the problem's condition number. Such a computation
complexity improves the state-of-the-art complexities of single-loop GDA and
AltGDA algorithms (see the summary of comparison in Table 1). We demonstrate
the effectiveness of our algorithm via an experiment on adversarial deep
learning.

    

### [[2112.11668] How Should Pre-Trained Language Models Be Fine-Tuned Towards Adversarial Robustness?](http://arxiv.org/abs/2112.11668)


  The fine-tuning of pre-trained language models has a great success in many
NLP fields. Yet, it is strikingly vulnerable to adversarial examples, e.g.,
word substitution attacks using only synonyms can easily fool a BERT-based
sentiment analysis model. In this paper, we demonstrate that adversarial
training, the prevalent defense technique, does not directly fit a conventional
fine-tuning scenario, because it suffers severely from catastrophic forgetting:
failing to retain the generic and robust linguistic features that have already
been captured by the pre-trained model. In this light, we propose Robust
Informative Fine-Tuning (RIFT), a novel adversarial fine-tuning method from an
information-theoretical perspective. In particular, RIFT encourages an
objective model to retain the features learned from the pre-trained model
throughout the entire fine-tuning process, whereas a conventional one only uses
the pre-trained weights for initialization. Experimental results show that RIFT
consistently outperforms the state-of-the-arts on two popular NLP tasks:
sentiment analysis and natural language inference, under different attacks
across various pre-trained language models.

    

### [[2112.11669] MECATS: Mixture-of-Experts for Quantile Forecasts of Aggregated Time Series](http://arxiv.org/abs/2112.11669)


  We introduce a mixture of heterogeneous experts framework called
\texttt{MECATS}, which simultaneously forecasts the values of a set of time
series that are related through an aggregation hierarchy. Different types of
forecasting models can be employed as individual experts so that the form of
each model can be tailored to the nature of the corresponding time series.
\texttt{MECATS} learns hierarchical relationships during the training stage to
help generalize better across all the time series being modeled and also
mitigates coherency issues that arise due to constraints imposed by the
hierarchy. We further build multiple quantile estimators on top of the point
forecasts. The resulting probabilistic forecasts are nearly coherent,
distribution-free, and independent of the choice of forecasting models. We
conduct a comprehensive evaluation on both point and probabilistic forecasts
and also formulate an extension for situations where change points exist in
sequential data. In general, our method is robust, adaptive to datasets with
different properties, and highly configurable and efficient for large-scale
forecasting pipelines.

    

### [[2112.11670] Domain Adaptation with Pre-trained Transformers for Query Focused Abstractive Text Summarization](http://arxiv.org/abs/2112.11670)


  The Query Focused Text Summarization (QFTS) task aims at building systems
that generate the summary of the text document(s) based on the given query. A
key challenge in addressing this task is the lack of large labeled data for
training the summarization model. In this paper, we address this challenge by
exploring a series of domain adaptation techniques. Given the recent success of
pre-trained transformer models in a wide range of natural language processing
tasks, we utilize such models to generate abstractive summaries for the QFTS
task for both single-document and multi-document scenarios. For domain
adaptation, we apply a variety of techniques using pre-trained
transformer-based summarization models including transfer learning, weakly
supervised learning, and distant supervision. Extensive experiments on six
datasets show that our proposed approach is very effective in generating
abstractive summaries for the QFTS task while setting a new state-of-the-art
result in several datasets across a set of automatic and human evaluation
metrics.

    

### [[2112.11687] Squareplus: A Softplus-Like Algebraic Rectifier](http://arxiv.org/abs/2112.11687)


  We present squareplus, an activation function that resembles softplus, but
which can be computed using only algebraic operations: addition,
multiplication, and square-root. Because squareplus is ~6x faster to evaluate
than softplus on a CPU and does not require access to transcendental functions,
it may have practical value in resource-limited deep learning applications.

    

### [[2112.11696] RepBin: Constraint-based Graph Representation Learning for Metagenomic Binning](http://arxiv.org/abs/2112.11696)


  Mixed communities of organisms are found in many environments (from the human
gut to marine ecosystems) and can have profound impact on human health and the
environment. Metagenomics studies the genomic material of such communities
through high-throughput sequencing that yields DNA subsequences for subsequent
analysis. A fundamental problem in the standard workflow, called binning, is to
discover clusters, of genomic subsequences, associated with the unknown
constituent organisms. Inherent noise in the subsequences, various biological
constraints that need to be imposed on them and the skewed cluster size
distribution exacerbate the difficulty of this unsupervised learning problem.
In this paper, we present a new formulation using a graph where the nodes are
subsequences and edges represent homophily information. In addition, we model
biological constraints providing heterophilous signal about nodes that cannot
be clustered together. We solve the binning problem by developing new
algorithms for (i) graph representation learning that preserves both homophily
relations and heterophily constraints (ii) constraint-based graph clustering
method that addresses the problems of skewed cluster size distribution.
Extensive experiments, on real and synthetic datasets, demonstrate that our
approach, called RepBin, outperforms a wide variety of competing methods. Our
constraint-based graph representation learning and clustering methods, that may
be useful in other domains as well, advance the state-of-the-art in both
metagenomics binning and graph representation learning.

    

### [[2112.11699] Few-Shot Object Detection: A Survey](http://arxiv.org/abs/2112.11699)


  Humans are able to learn to recognize new objects even from a few examples.
In contrast, training deep-learning-based object detectors requires huge
amounts of annotated data. To avoid the need to acquire and annotate these huge
amounts of data, few-shot object detection aims to learn from few object
instances of new categories in the target domain. In this survey, we provide an
overview of the state of the art in few-shot object detection. We categorize
approaches according to their training scheme and architectural layout. For
each type of approaches, we describe the general realization as well as
concepts to improve the performance on novel categories. Whenever appropriate,
we give short takeaways regarding these concepts in order to highlight the best
ideas. Eventually, we introduce commonly used datasets and their evaluation
protocols and analyze reported benchmark results. As a result, we emphasize
common challenges in evaluation and identify the most promising current trends
in this emerging field of few-shot object detection.

    

### [[2112.11721] Towards Malicious address identification in Bitcoin](http://arxiv.org/abs/2112.11721)


  The temporal aspect of blockchain transactions enables us to study the
address's behavior and detect if it is involved in any illicit activity.
However, due to the concept of change addresses (used to thwart replay
attacks), temporal aspects are not directly applicable in the Bitcoin
blockchain. Several pre-processing steps should be performed before such
temporal aspects are utilized. We are motivated to study the Bitcoin
transaction network and use the temporal features such as burst,
attractiveness, and inter-event time along with several graph-based properties
such as the degree of node and clustering coefficient to validate the
applicability of already existing approaches known for other cryptocurrency
blockchains on the Bitcoin blockchain. We generate the temporal and
non-temporal feature set and train the Machine Learning (ML) algorithm over
different temporal granularities to validate the state-of-the-art methods. We
study the behavior of the addresses over different time granularities of the
dataset. We identify that after applying change-address clustering, in Bitcoin,
existing temporal features can be extracted and ML approaches can be applied. A
comparative analysis of results show that the behavior of addresses in Ethereum
and Bitcoin is similar with respect to in-degree, out-degree and inter-event
time. Further, we identify 3 suspects that showed malicious behavior across
different temporal granularities. These suspects are not marked as malicious in
Bitcoin.

    

### [[2112.11731] Graph augmented Deep Reinforcement Learning in the GameRLand3D environment](http://arxiv.org/abs/2112.11731)


  We address planning and navigation in challenging 3D video games featuring
maps with disconnected regions reachable by agents using special actions. In
this setting, classical symbolic planners are not applicable or difficult to
adapt. We introduce a hybrid technique combining a low level policy trained
with reinforcement learning and a graph based high level classical planner. In
addition to providing human-interpretable paths, the approach improves the
generalization performance of an end-to-end approach in unseen maps, where it
achieves a 20% absolute increase in success rate over a recurrent end-to-end
agent on a point to point navigation task in yet unseen large-scale maps of
size 1km x 1km. In an in-depth experimental study, we quantify the limitations
of end-to-end Deep RL approaches in vast environments and we also introduce
"GameRLand3D", a new benchmark and soon to be released environment can generate
complex procedural 3D maps for navigation tasks.

    

### [[2112.11734] Investigating Neighborhood Modeling and Asymmetry Preservation in Digraph Representation Learning](http://arxiv.org/abs/2112.11734)


  Graph Neural Networks (GNNs) traditionally exhibit poor performance for
directed graphs (digraphs) due to notable challenges in 1) modeling
neighborhoods and 2) preserving asymmetry. In this paper, we address these
challenges in traditional GNNs by leveraging hyperbolic collaborative learning
from multi-ordered and partitioned neighborhoods, and regularizers inspired by
socio-psychological factors. Our resulting formalism, Digraph Hyperbolic
Network (D-HYPR) learns node representations in hyperbolic space to avoid
structural and semantic distortion of real-world digraphs. We conduct
comprehensive experimentation on 4 tasks: link prediction, node classification,
sign prediction, and embedding visualization. D-HYPR statistically
significantly outperforms the current state of the art on a majority of tasks
and datasets, while achieving competitive performance otherwise. Our code and
data will be available.

    

### [[2112.11739] A Survey of Natural Language Generation](http://arxiv.org/abs/2112.11739)


  This paper offers a comprehensive review of the research on Natural Language
Generation (NLG) over the past two decades, especially in relation to
data-to-text generation and text-to-text generation deep learning methods, as
well as new applications of NLG technology. This survey aims to (a) give the
latest synthesis of deep learning research on the NLG core tasks, as well as
the architectures adopted in the field; (b) detail meticulously and
comprehensively various NLG tasks and datasets, and draw attention to the
challenges in NLG evaluation, focusing on different evaluation methods and
their relationships; (c) highlight some future emphasis and relatively recent
research issues that arise due to the increasing synergy between NLG and other
artificial intelligence areas, such as computer vision, text and computational
creativity.

    

### [[2112.11743] Simple and Effective Balance of Contrastive Losses](http://arxiv.org/abs/2112.11743)


  Contrastive losses have long been a key ingredient of deep metric learning
and are now becoming more popular due to the success of self-supervised
learning. Recent research has shown the benefit of decomposing such losses into
two sub-losses which act in a complementary way when learning the
representation network: a positive term and an entropy term. Although the
overall loss is thus defined as a combination of two terms, the balance of
these two terms is often hidden behind implementation details and is largely
ignored and sub-optimal in practice. In this work, we approach the balance of
contrastive losses as a hyper-parameter optimization problem, and propose a
coordinate descent-based search method that efficiently find the
hyper-parameters that optimize evaluation performance. In the process, we
extend existing balance analyses to the contrastive margin loss, include batch
size in the balance, and explain how to aggregate loss elements from the batch
to maintain near-optimal performance over a larger range of batch sizes.
Extensive experiments with benchmarks from deep metric learning and
self-supervised learning show that optimal hyper-parameters are found faster
with our method than with other common search methods.

    

### [[2112.11760] On Asymptotic Linear Convergence of Projected Gradient Descent for Constrained Least Squares](http://arxiv.org/abs/2112.11760)


  Many recent problems in signal processing and machine learning such as
compressed sensing, image restoration, matrix/tensor recovery, and non-negative
matrix factorization can be cast as constrained optimization. Projected
gradient descent is a simple yet efficient method for solving such constrained
optimization problems. Local convergence analysis furthers our understanding of
its asymptotic behavior near the solution, offering sharper bounds on the
convergence rate compared to global convergence analysis. However, local
guarantees often appear scattered in problem-specific areas of machine learning
and signal processing. This manuscript presents a unified framework for the
local convergence analysis of projected gradient descent in the context of
constrained least squares. The proposed analysis offers insights into pivotal
local convergence properties such as the condition of linear convergence, the
region of convergence, the exact asymptotic rate of convergence, and the bound
on the number of iterations needed to reach a certain level of accuracy. To
demonstrate the applicability of the proposed approach, we present a recipe for
the convergence analysis of PGD and demonstrate it via a beginning-to-end
application of the recipe on four fundamental problems, namely, linearly
constrained least squares, sparse recovery, least squares with the unit norm
constraint, and matrix completion.

    

### [[2112.11768] Robust learning of data anomalies with analytically-solvable entropic outlier sparsification](http://arxiv.org/abs/2112.11768)


  Entropic Outlier Sparsification (EOS) is proposed as a robust computational
strategy for the detection of data anomalies in a broad class of learning
methods, including the unsupervised problems (like detection of non-Gaussian
outliers in mostly-Gaussian data) and in the supervised learning with
mislabeled data. EOS dwells on the derived analytic closed-form solution of the
(weighted) expected error minimization problem subject to the Shannon entropy
regularization. In contrast to common regularization strategies requiring
computational costs that scale polynomial with the data dimension, identified
closed-form solution is proven to impose additional iteration costs that depend
linearly on statistics size and are independent of data dimension. Obtained
analytic results also explain why the mixtures of spherically-symmetric
Gaussians - used heuristically in many popular data analysis algorithms -
represent an optimal choice for the non-parametric probability distributions
when working with squared Euclidean distances, combining expected error
minimality, maximal entropy/unbiasedness, and a linear cost scaling. The
performance of EOS is compared to a range of commonly-used tools on synthetic
problems and on partially-mislabeled supervised classification problems from
biomedicine.

    

### [[2112.11776] The Importance of the Current Input in Sequence Modeling](http://arxiv.org/abs/2112.11776)


  The last advances in sequence modeling are mainly based on deep learning
approaches. The current state of the art involves the use of variations of the
standard LSTM architecture, combined with several tricks that improve the final
prediction rates of the trained neural networks. However, in some cases, these
adaptations might be too much tuned to the particular problems being addressed.
In this article, we show that a very simple idea, to add a direct connection
between the input and the output, skipping the recurrent module, leads to an
increase of the prediction accuracy in sequence modeling problems related to
natural language processing. Experiments carried out on different problems show
that the addition of this kind of connection to a recurrent network always
improves the results, regardless of the architecture and training-specific
details. When this idea is introduced into the models that lead the field, the
resulting networks achieve a new state-of-the-art perplexity in language
modeling problems.

    

### [[2112.11789] DRF Codes: Deep SNR-Robust Feedback Codes](http://arxiv.org/abs/2112.11789)


  We present a new deep-neural-network (DNN) based error correction code for
fading channels with output feedback, called deep SNR-robust feedback (DRF)
code. At the encoder, parity symbols are generated by a long short term memory
(LSTM) network based on the message as well as the past forward channel outputs
observed by the transmitter in a noisy fashion. The decoder uses a
bi-directional LSTM architecture along with a signal to noise ratio (SNR)-aware
attention NN to decode the message. The proposed code overcomes two major
shortcomings of the previously proposed DNN-based codes over channels with
passive output feedback: (i) the SNR-aware attention mechanism at the decoder
enables reliable application of the same trained NN over a wide range of SNR
values; (ii) curriculum training with batch-size scheduling is used to speed up
and stabilize training while improving the SNR-robustness of the resulting
code. We show that the DRF codes significantly outperform state-of-the-art in
terms of both the SNR-robustness and the error rate in additive white Gaussian
noise (AWGN) channel with feedback. In fading channels with perfect phase
compensation at the receiver, DRF codes learn to efficiently exploit knowledge
of the instantaneous fading amplitude (which is available to the encoder
through feedback) to reduce the overhead and complexity associated with channel
estimation at the decoder. Finally, we show the effectiveness of DRF codes in
multicast channels with feedback, where linear feedback codes are known to be
strictly suboptimal.

    

### [[2112.11805] Neural-Symbolic Integration for Interactive Learning and Conceptual Grounding](http://arxiv.org/abs/2112.11805)


  We propose neural-symbolic integration for abstract concept explanation and
interactive learning. Neural-symbolic integration and explanation allow users
and domain-experts to learn about the data-driven decision making process of
large neural models. The models are queried using a symbolic logic language.
Interaction with the user then confirms or rejects a revision of the neural
model using logic-based constraints that can be distilled into the model
architecture. The approach is illustrated using the Logic Tensor Network
framework alongside Concept Activation Vectors and applied to a Convolutional
Neural Network.

    

### [[2112.11806] Lifting Symmetry Breaking Constraints with Inductive Logic Programming](http://arxiv.org/abs/2112.11806)


  Efficient omission of symmetric solution candidates is essential for
combinatorial problem-solving. Most of the existing approaches are
instance-specific and focus on the automatic computation of Symmetry Breaking
Constraints (SBCs) for each given problem instance. However, the application of
such approaches to large-scale instances or advanced problem encodings might be
problematic since the computed SBCs are propositional and, therefore, can
neither be meaningfully interpreted nor transferred to other instances. As a
result, a time-consuming recomputation of SBCs must be done before every
invocation of a solver.
To overcome these limitations, we introduce a new model-oriented approach for
Answer Set Programming that lifts the SBCs of small problem instances into a
set of interpretable first-order constraints using the Inductive Logic
Programming paradigm. Experiments demonstrate the ability of our framework to
learn general constraints from instance-specific SBCs for a collection of
combinatorial problems. The obtained results indicate that our approach
significantly outperforms a state-of-the-art instance-specific method as well
as the direct application of a solver.

    

### [[2112.11818] Decentralized Task Offloading in Edge Computing: A Multi-User Multi-Armed Bandit Approach](http://arxiv.org/abs/2112.11818)


  Mobile edge computing facilitates users to offload computation tasks to edge
servers for meeting their stringent delay requirements. Previous works mainly
explore task offloading when system-side information is given (e.g., server
processing speed, cellular data rate), or centralized offloading under system
uncertainty. But both generally fall short to handle task placement involving
many coexisting users in a dynamic and uncertain environment. In this paper, we
develop a multi-user offloading framework considering unknown yet stochastic
system-side information to enable a decentralized user-initiated service
placement. Specifically, we formulate the dynamic task placement as an online
multi-user multi-armed bandit process, and propose a decentralized epoch based
offloading (DEBO) to optimize user rewards which are subjected under network
delay. We show that DEBO can deduce the optimal user-server assignment, thereby
achieving a close-to-optimal service performance and tight O(log T) offloading
regret. Moreover, we generalize DEBO to various common scenarios such as
unknown reward gap, dynamic entering or leaving of clients, and fair reward
distribution, while further exploring when users' offloaded tasks require
heterogeneous computing resources. Particularly, we accomplish a sub-linear
regret for each of these instances. Real measurements based evaluations
corroborate the superiority of our offloading schemes over state-of-the-art
approaches in optimizing delay-sensitive rewards.

    

### [[2112.11832] Classifier Data Quality: A Geometric Complexity Based Method for Automated Baseline And Insights Generation](http://arxiv.org/abs/2112.11832)


  Testing Machine Learning (ML) models and AI-Infused Applications (AIIAs), or
systems that contain ML models, is highly challenging. In addition to the
challenges of testing classical software, it is acceptable and expected that
statistical ML models sometimes output incorrect results. A major challenge is
to determine when the level of incorrectness, e.g., model accuracy or F1 score
for classifiers, is acceptable and when it is not. In addition to business
requirements that should provide a threshold, it is a best practice to require
any proposed ML solution to out-perform simple baseline models, such as a
decision tree.
We have developed complexity measures, which quantify how difficult given
observations are to assign to their true class label; these measures can then
be used to automatically determine a baseline performance threshold. These
measures are superior to the best practice baseline in that, for a linear
computation cost, they also quantify each observation' classification
complexity in an explainable form, regardless of the classifier model used. Our
experiments with both numeric synthetic data and real natural language chatbot
data demonstrate that the complexity measures effectively highlight data
regions and observations that are likely to be misclassified.

    

### [[2112.11854] Movie Recommender System using critic consensus](http://arxiv.org/abs/2112.11854)


  Recommendation systems are perhaps one of the most important agents for
industry growth through the modern Internet world. Previous approaches on
recommendation systems include collaborative filtering and content based
filtering recommendation systems. These 2 methods are disjointed in nature and
require the continuous storage of user preferences for a better recommendation.
To provide better integration of the two processes, we propose a hybrid
recommendation system based on the integration of collaborative and
content-based content, taking into account the top critic consensus and movie
rating score. We would like to present a novel model that recommends movies
based on the combination of user preferences and critical consensus scores.

    

### [[2112.11858] End to End Software Engineering Research](http://arxiv.org/abs/2112.11858)


  End to end learning is machine learning starting in raw data and predicting a
desired concept, with all steps done automatically. In software engineering
context, we see it as starting from the source code and predicting process
metrics. This framework can be used for predicting defects, code quality,
productivity and more. End-to-end improves over features based machine learning
by not requiring domain experts and being able to extract new knowledge. We
describe a dataset of 5M files from 15k projects constructed for this goal. The
dataset is constructed in a way that enables not only predicting concepts but
also investigating their causes.

    

### [[2112.11873] FLoBC: A Decentralized Blockchain-Based Federated Learning Framework](http://arxiv.org/abs/2112.11873)


  The rapid expansion of data worldwide invites the need for more distributed
solutions in order to apply machine learning on a much wider scale. The
resultant distributed learning systems can have various degrees of
centralization. In this work, we demonstrate our solution FLoBC for building a
generic decentralized federated learning system using blockchain technology,
accommodating any machine learning model that is compatible with gradient
descent optimization. We present our system design comprising the two
decentralized actors: trainer and validator, alongside our methodology for
ensuring reliable and efficient operation of said system. Finally, we utilize
FLoBC as an experimental sandbox to compare and contrast the effects of
trainer-to-validator ratio, reward-penalty policy, and model synchronization
schemes on the overall system performance, ultimately showing by example that a
decentralized federated learning system is indeed a feasible alternative to
more centralized architectures.

    

### [[2112.11913] Trees in transformers: a theoretical analysis of the Transformer's ability to represent trees](http://arxiv.org/abs/2112.11913)


  Transformer networks are the de facto standard architecture in natural
language processing. To date, there are no theoretical analyses of the
Transformer's ability to capture tree structures. We focus on the ability of
Transformer networks to learn tree structures that are important for tree
transduction problems. We first analyze the theoretical capability of the
standard Transformer architecture to learn tree structures given enumeration of
all possible tree backbones, which we define as trees without labels. We then
prove that two linear layers with ReLU activation function can recover any tree
backbone from any two nonzero, linearly independent starting backbones. This
implies that a Transformer can learn tree structures well in theory. We conduct
experiments with synthetic data and find that the standard Transformer achieves
similar accuracy compared to a Transformer where tree position information is
explicitly encoded, albeit with slower convergence. This confirms empirically
that Transformers can learn tree structures.

    

### [[2112.11916] ALP: Data Augmentation using Lexicalized PCFGs for Few-Shot Text Classification](http://arxiv.org/abs/2112.11916)


  Data augmentation has been an important ingredient for boosting performances
of learned models. Prior data augmentation methods for few-shot text
classification have led to great performance boosts. However, they have not
been designed to capture the intricate compositional structure of natural
language. As a result, they fail to generate samples with plausible and diverse
sentence structures. Motivated by this, we present the data Augmentation using
Lexicalized Probabilistic context-free grammars (ALP) that generates augmented
samples with diverse syntactic structures with plausible grammar. The
lexicalized PCFG parse trees consider both the constituents and dependencies to
produce a syntactic frame that maximizes a variety of word choices in a
syntactically preservable manner without specific domain experts. Experiments
on few-shot text classification tasks demonstrate that ALP enhances many
state-of-the-art classification methods. As a second contribution, we delve
into the train-val splitting methodologies when a data augmentation method
comes into play. We argue empirically that the traditional splitting of
training and validation sets is sub-optimal compared to our novel
augmentation-based splitting strategies that further expand the training split
with the same number of labeled data. Taken together, our contributions on the
data augmentation strategies yield a strong training recipe for few-shot text
classification tasks.

    

### [[2112.11920] List Autoencoder: Towards Deep Learning Based Reliable Transmission Over Noisy Channels](http://arxiv.org/abs/2112.11920)


  There has been a growing interest in automating the design of channel
encoders and decoders in an auto-encoder(AE) framework in recent years for
reliable transmission of data over noisy channels. In this paper we present a
new framework for designing AEs for this purpose. In particular, we present an
AE framework, namely listAE, in which the decoder network outputs a list of
decoded message word candidates. A genie is assumed to be available at the
output of the decoder and specific loss functions are proposed to optimize the
performance of the genie-aided (GA)-listAE. The listAE is a general AE
framework and can be used with any network architecture. We propose a specific
end-to-end network architecture which decodes the received word on a sequence
of component codes with decreasing rates. The listAE based on the proposed
architecture, referred to as incremental redundancy listAE (IR-listAE),
improves the state-of-the-art AE performance by 1 dB at low block error rates
under GA decoding. We then employ cyclic redundancy check (CRC) codes to
replace the genie at the decoder, giving CRC-aided (CA)-listAE with negligible
performance loss compared to the GA-listAE. The CA-listAE shows meaningful
coding gain at the price of a slight decrease in the rate due to appending CRC
to the message word.

    

### [[2112.11921] Variational Quantum Soft Actor-Critic](http://arxiv.org/abs/2112.11921)


  Quantum computing has a superior advantage in tackling specific problems,
such as integer factorization and Simon's problem. For more general tasks in
machine learning, by applying variational quantum circuits, more and more
quantum algorithms have been proposed recently, especially in supervised
learning and unsupervised learning. However, little work has been done in
reinforcement learning, arguably more important and challenging. Previous work
in quantum reinforcement learning mainly focuses on discrete control tasks
where the action space is discrete. In this work, we develop a quantum
reinforcement learning algorithm based on soft actor-critic -- one of the
state-of-the-art methods for continuous control. Specifically, we use a hybrid
quantum-classical policy network consisting of a variational quantum circuit
and a classical artificial neural network. Tested in a standard reinforcement
learning benchmark, we show that this quantum version of soft actor-critic is
comparable with the original soft actor-critic, using much less adjustable
parameters. Furthermore, we analyze the effect of different hyper-parameters
and policy network architectures, pointing out the importance of architecture
design for quantum reinforcement learning.

    

### [[2112.11925] SOLIS -- The MLOps journey from data acquisition to actionable insights](http://arxiv.org/abs/2112.11925)


  Machine Learning operations is unarguably a very important and also one of
the hottest topics in Artificial Intelligence lately. Being able to define very
clear hypotheses for actual real-life problems that can be addressed by machine
learning models, collecting and curating large amounts of data for model
training and validation followed by model architecture search and actual
optimization and finally presenting the results fits very well the scenario of
Data Science experiments. This approach however does not supply the needed
procedures and pipelines for the actual deployment of machine learning
capabilities in real production grade systems. Automating live configuration
mechanisms, on the fly adapting to live or offline data capture and
consumption, serving multiple models in parallel either on edge or cloud
architectures, addressing specific limitations of GPU memory or compute power,
post-processing inference or prediction results and serving those either as
APIs or with IoT based communication stacks in the same end-to-end pipeline are
the real challenges that we try to address in this particular paper. In this
paper we present a unified deployment pipeline and freedom-to-operate approach
that supports all above requirements while using basic cross-platform tensor
framework and script language engines.

    

### [[2112.11929] Meta-Learning and Self-Supervised Pretraining for Real World Image Translation](http://arxiv.org/abs/2112.11929)


  Recent advances in deep learning, in particular enabled by hardware advances
and big data, have provided impressive results across a wide range of
computational problems such as computer vision, natural language, or
reinforcement learning. Many of these improvements are however constrained to
problems with large-scale curated data-sets which require a lot of human labor
to gather. Additionally, these models tend to generalize poorly under both
slight distributional shifts and low-data regimes. In recent years, emerging
fields such as meta-learning or self-supervised learning have been closing the
gap between proof-of-concept results and real-life applications of machine
learning by extending deep-learning to the semi-supervised and few-shot
domains. We follow this line of work and explore spatio-temporal structure in a
recently introduced image-to-image translation problem in order to: i)
formulate a novel multi-task few-shot image generation benchmark and ii)
explore data augmentations in contrastive pre-training for image translation
downstream tasks. We present several baselines for the few-shot problem and
discuss trade-offs between different approaches. Our code is available at
this https URL.

    

### [[2112.11944] Continual learning of longitudinal health records](http://arxiv.org/abs/2112.11944)


  Continual learning denotes machine learning methods which can adapt to new
environments while retaining and reusing knowledge gained from past
experiences. Such methods address two issues encountered by models in
non-stationary environments: ungeneralisability to new data, and the
catastrophic forgetting of previous knowledge when retrained. This is a
pervasive problem in clinical settings where patient data exhibits covariate
shift not only between populations, but also continuously over time. However,
while continual learning methods have seen nascent success in the imaging
domain, they have been little applied to the multi-variate sequential data
characteristic of critical care patient recordings.
Here we evaluate a variety of continual learning methods on longitudinal ICU
data in a series of representative healthcare scenarios. We find that while
several methods mitigate short-term forgetting, domain shift remains a
challenging problem over large series of tasks, with only replay based methods
achieving stable long-term performance.
Code for reproducing all experiments can be found at
this https URL


### [[2112.11947] Evaluating the Robustness of Deep Reinforcement Learning for Autonomous and Adversarial Policies in a Multi-agent Urban Driving Environment](http://arxiv.org/abs/2112.11947)


  Deep reinforcement learning is actively used for training autonomous driving
agents in a vision-based urban simulated environment. Due to the large
availability of various reinforcement learning algorithms, we are still unsure
of which one works better while training autonomous cars in single-agent as
well as multi-agent driving environments. A comparison of deep reinforcement
learning in vision-based autonomous driving will open up the possibilities for
training better autonomous car policies. Also, autonomous cars trained on deep
reinforcement learning-based algorithms are known for being vulnerable to
adversarial attacks, and we have less information on which algorithms would act
as a good adversarial agent. In this work, we provide a systematic evaluation
and comparative analysis of 6 deep reinforcement learning algorithms for
autonomous and adversarial driving in four-way intersection scenario.
Specifically, we first train autonomous cars using state-of-the-art deep
reinforcement learning algorithms. Second, we test driving capabilities of the
trained autonomous policies in single-agent as well as multi-agent scenarios.
Lastly, we use the same deep reinforcement learning algorithms to train
adversarial driving agents, in order to test the driving performance of
autonomous cars and look for possible collision and offroad driving scenarios.
We perform experiments by using vision-only high fidelity urban driving
simulated environments.

    

### [[2112.11989] FedLGA: Towards System-Heterogeneity of Federated Learning via Local Gradient Approximation](http://arxiv.org/abs/2112.11989)


  Federated Learning (FL) is a decentralized machine learning architecture,
which leverages a large number of remote devices to learn a joint model with
distributed training data. However, the system-heterogeneity is one major
challenge in a FL network to achieve robust distributed learning performance,
which is of two aspects: i) device-heterogeneity due to the diverse
computational capacity among devices; ii) data-heterogeneity due to the
non-identically distributed data across the network. Though there have been
benchmarks against the heterogeneous FL, e.g., FedProx, the prior studies lack
formalization and it remains an open problem. In this work, we formalize the
system-heterogeneous FL problem and propose a new algorithm, called FedLGA,
which addresses this problem by bridging the divergence of local model updates
via gradient approximation. To achieve this, FedLGA provides an alternated
Hessian estimation method, which only requires extra linear complexity on the
aggregator. Theoretically, we show that with a device-heterogeneous ratio
$\rho$, FedLGA achieves convergence rates on non-i.i.d distributed FL training
data against non-convex optimization problems for $\mathcal{O} \left(
\frac{(1+\rho)}{\sqrt{ENT}} + \frac{1}{T} \right)$ and $\mathcal{O} \left(
\frac{(1+\rho)\sqrt{E}}{\sqrt{TK}} + \frac{1}{T} \right)$ for full and partial
device participation respectively, where $E$ is the number of local learning
epoch, $T$ is the number of total communication round, $N$ is the total device
number and $K$ is the number of selected device in one communication round
under partially participation scheme. The results of comprehensive experiments
on multiple datasets show that FedLGA outperforms current FL benchmarks against
the system-heterogeneity.

    

### [[2112.11992] Automatic Estimation of Anthropometric Human Body Measurements](http://arxiv.org/abs/2112.11992)


  Research tasks related to human body analysis have been drawing a lot of
attention in computer vision area over the last few decades, considering its
potential benefits on our day-to-day life. Anthropometry is a field defining
physical measures of a human body size, form, and functional capacities.
Specifically, the accurate estimation of anthropometric body measurements from
visual human body data is one of the challenging problems, where the solution
would ease many different areas of applications, including ergonomics, garment
manufacturing, etc. This paper formulates a research in the field of deep
learning and neural networks, to tackle the challenge of body measurements
estimation from various types of visual input data (such as 2D images or 3D
point clouds). Also, we deal with the lack of real human data annotated with
ground truth body measurements required for training and evaluation, by
generating a synthetic dataset of various human body shapes and performing a
skeleton-driven annotation.

    

### [[2112.12006] Catch Me If You GAN: Using Artificial Intelligence for Fake Log Generation](http://arxiv.org/abs/2112.12006)


  With artificial intelligence (AI) becoming relevant in various parts of
everyday life, other technologies are already widely influenced by the new way
of handling large amounts of data. Although widespread already, AI has had only
punctual influences on the cybersecurity field specifically. Many techniques
and technologies used by cybersecurity experts function through manual labor
and barely draw on automation, e.g., logs are often reviewed manually by system
admins for potentially malicious keywords. This work evaluates the use of a
special type of AI called generative adversarial networks (GANs) for log
generation. More precisely, three different generative adversarial networks,
SeqGAN, MaliGAN, and CoT, are reviewed in this research regarding their
performance, focusing on generating new logs as a means of deceiving system
admins for red teams. Although static generators for fake logs have been around
for a while, their produces are usually easy to reveal as such. Using AI as an
approach to this problem has not been widely researched. Identified challenges
consist of formatting, dates and times, and overall consistency. Summing up the
results, GANs seem not to be a good fit for generating fake logs. Their
capability to detect fake logs, however, might be of use in practical
scenarios.

    

### [[2112.12021] Community Detection in Medical Image Datasets: Using Wavelets and Spectral Methods](http://arxiv.org/abs/2112.12021)


  Medical image datasets can have large number of images representing patients
with different health conditions and various disease severity. When dealing
with raw unlabeled image datasets, the large number of samples often makes it
hard for experts and non-experts to understand the variety of images present in
a dataset. Supervised learning methods rely on labeled images which requires a
considerable effort by medical experts to first understand the communities of
images present in the data and then labeling the images. Here, we propose an
algorithm to facilitate the automatic identification of communities in medical
image datasets. We further explain that such analysis can also be insightful in
a supervised setting, when the images are already labeled. Such insights are
useful because in reality, health and disease severity can be considered a
continuous spectrum, and within each class, there usually are finer communities
worthy of investigation, especially when they have similarities to communities
in other classes. In our approach, we use wavelet decomposition of images in
tandem with spectral methods. We show that the eigenvalues of a graph Laplacian
can reveal the number of notable communities in an image dataset. In our
experiments, we use a dataset of images labeled with different conditions for
COVID patients. We detect 25 communities in the dataset and then observe that
only 6 of those communities contain patients with pneumonia. We also
investigate the contents of a colorectal cancer histopathology dataset.

    

### [[2112.12024] Evaluating categorical encoding methods on a real credit card fraud detection database](http://arxiv.org/abs/2112.12024)


  Correctly dealing with categorical data in a supervised learning context is
still a major issue. Furthermore, though some machine learning methods embody
builtin methods to deal with categorical features, it is unclear whether they
bring some improvements and how do they compare with usual categorical encoding
methods. In this paper, we describe several well-known categorical encoding
methods that are based on target statistics and weight of evidence. We apply
them on a large and real credit card fraud detection database. Then, we train
the encoded databases using state-of-the-art gradient boosting methods and
evaluate their performances. We show that categorical encoding methods
generally bring substantial improvements with respect to the absence of
encoding. The contribution of this work is twofold: (1) we compare many
state-of-the-art "lite" categorical encoding methods on a large scale database
and (2) we use a real credit card fraud detection database.

    

### [[2112.12033] Encoding protein dynamic information in graph representation for functional residue identification](http://arxiv.org/abs/2112.12033)


  Recent advances in protein function prediction exploit graph-based deep
learning approaches to correlate the structural and topological features of
proteins with their molecular functions. However, proteins in vivo are not
static but dynamic molecules that alter conformation for functional purposes.
Here we apply normal mode analysis to native protein conformations and augment
protein graphs by connecting edges between dynamically correlated residue
pairs. In the multilabel function classification task, our method demonstrates
a remarkable performance gain based on this dynamics-informed representation.
The proposed graph neural network, ProDAR, increases the interpretability and
generalizability of residue-level annotations and robustly reflects structural
nuance in proteins. We elucidate the importance of dynamic information in graph
representation by comparing class activation maps for the hMTH1, nitrophorin,
and SARS-CoV-2 receptor binding domain. Our model successfully learns the
dynamic fingerprints of proteins and provides molecular insights into protein
functions, with vast untapped potential for broad biotechnology and
pharmaceutical applications.

    

### [[2112.12047] Generating Synthetic Mixed-type Longitudinal Electronic Health Records for Artificial Intelligent Applications](http://arxiv.org/abs/2112.12047)


  The recent availability of electronic health records (EHRs) have provided
enormous opportunities to develop artificial intelligence (AI) algorithms.
However, patient privacy has become a major concern that limits data sharing
across hospital settings and subsequently hinders the advances in AI.
\textit{Synthetic data}, which benefits from the development and proliferation
of generative models, has served as a promising substitute for real patient EHR
data. However, the current generative models are limited as they only generate
\textit{single type} of clinical data, i.e., either continuous-valued or
discrete-valued. In this paper, we propose a generative adversarial network
(GAN) entitled EHR-M-GAN which synthesizes \textit{mixed-type} timeseries EHR
data. EHR-M-GAN is capable of capturing the multidimensional, heterogeneous,
and correlated temporal dynamics in patient trajectories. We have validated
EHR-M-GAN on three publicly-available intensive care unit databases with
records from a total of 141,488 unique patients, and performed privacy risk
evaluation of the proposed model. EHR-M-GAN has demonstrated its superiority in
performance over state-of-the-art benchmarks for synthesizing clinical
timeseries with high fidelity. Notably, prediction models for outcomes of
intensive care performed significantly better when training data was augmented
with the addition of EHR-M-GAN-generated timeseries. EHR-M-GAN may have use in
developing AI algorithms in resource-limited settings, lowering the barrier for
data acquisition while preserving patient privacy.

    

### [[2112.12054] Machine Learning for Computational Science and Engineering -- a brief introduction and some critical questions](http://arxiv.org/abs/2112.12054)


  Artificial Intelligence (AI) is now entering every sub-field of science,
technology, engineering, arts, and management. Thanks to the hype and
availability of research funds, it is being adapted in many fields without much
thought. Computational Science and Engineering (CS&E) is one such sub-field. By
highlighting some critical questions around the issues and challenges in
adapting Machine Learning (ML) for CS&E, most of which are often overlooked in
journal papers, this contribution hopes to offer some insights into the
adaptation of ML for applications in CS\&E and related fields. This is a
general-purpose article written for a general audience and researchers new to
the fields of ML and/or CS\&E. This work focuses only on the forward problems
in computational science and engineering. Some basic equations and MATLAB code
are also provided to help the reader understand the basics.

    

### [[2112.12073] Two Stream Network for Stroke Detection in Table Tennis](http://arxiv.org/abs/2112.12073)


  This paper presents a table tennis stroke detection method from videos. The
method relies on a two-stream Convolutional Neural Network processing in
parallel the RGB Stream and its computed optical flow. The method has been
developed as part of the MediaEval 2021 benchmark for the Sport task. Our
contribution did not outperform the provided baseline on the test set but has
performed the best among the other participants with regard to the mAP metric.

    

### [[2112.12074] Spatio-Temporal CNN baseline method for the Sports Video Task of MediaEval 2021 benchmark](http://arxiv.org/abs/2112.12074)


  This paper presents the baseline method proposed for the Sports Video task
part of the MediaEval 2021 benchmark. This task proposes a stroke detection and
a stroke classification subtasks. This baseline addresses both subtasks. The
spatio-temporal CNN architecture and the training process of the model are
tailored according to the addressed subtask. The method has the purpose of
helping the participants to solve the task and is not meant to reach
stateof-the-art performance. Still, for the detection task, the baseline is
performing better than the other participants, which stresses the difficulty of
such a task.

    

### [[2112.12078] Deeper Learning with CoLU Activation](http://arxiv.org/abs/2112.12078)


  In neural networks, non-linearity is introduced by activation functions. One
commonly used activation function is Rectified Linear Unit (ReLU). ReLU has
been a popular choice as an activation but has flaws. State-of-the-art
functions like Swish and Mish are now gaining attention as a better choice as
they combat many flaws presented by other activation functions. CoLU is an
activation function similar to Swish and Mish in properties. It is defined as
f(x)=x/(1-xe^-(x+e^x)). It is smooth, continuously differentiable, unbounded
above, bounded below, non-saturating, and non-monotonic. Based on experiments
done with CoLU with different activation functions, it is observed that CoLU
usually performs better than other functions on deeper neural networks. While
training different neural networks on MNIST on an incrementally increasing
number of convolutional layers, CoLU retained the highest accuracy for more
layers. On a smaller network with 8 convolutional layers, CoLU had the highest
mean accuracy, closely followed by ReLU. On VGG-13 trained on Fashion-MNIST,
CoLU had a 4.20% higher accuracy than Mish and 3.31% higher accuracy than ReLU.
On ResNet-9 trained on Cifar-10, CoLU had 0.05% higher accuracy than Swish,
0.09% higher accuracy than Mish, and 0.29% higher accuracy than ReLU. It is
observed that activation functions may behave better than other activation
functions based on different factors including the number of layers, types of
layers, number of parameters, learning rate, optimizer, etc. Further research
can be done on these factors and activation functions for more optimal
activation functions and more knowledge on their behavior.

    

### [[2112.12083] Predicting treatment effects from observational studies using machine learning methods: A simulation study](http://arxiv.org/abs/2112.12083)


  Measuring treatment effects in observational studies is challenging because
of confounding bias. Confounding occurs when a variable affects both the
treatment and the outcome. Traditional methods such as propensity score
matching estimate treatment effects by conditioning on the confounders. Recent
literature has presented new methods that use machine learning to predict the
counterfactuals in observational studies which then allow for estimating
treatment effects. These studies however, have been applied to real world data
where the true treatment effects have not been known. This study aimed to study
the effectiveness of this counterfactual prediction method by simulating two
main scenarios: with and without confounding. Each type also included linear
and non-linear relationships between input and output data. The key item in the
simulations was that we generated known true causal effects. Linear regression,
lasso regression and random forest models were used to predict the
counterfactuals and treatment effects. These were compared these with the true
treatment effect as well as a naive treatment effect. The results show that the
most important factor in whether this machine learning method performs well, is
the degree of non-linearity in the data. Surprisingly, for both non-confounding
\textit{and} confounding, the machine learning models all performed well on the
linear dataset. However, when non-linearity was introduced, the models
performed very poorly. Therefore under the conditions of this simulation study,
the machine learning method performs well under conditions of linearity, even
if confounding is present, but at this stage should not be trusted when
non-linearity is introduced.

    

### [[2112.12095] Detect & Reject for Transferability of Black-box Adversarial Attacks Against Network Intrusion Detection Systems](http://arxiv.org/abs/2112.12095)


  In the last decade, the use of Machine Learning techniques in anomaly-based
intrusion detection systems has seen much success. However, recent studies have
shown that Machine learning in general and deep learning specifically are
vulnerable to adversarial attacks where the attacker attempts to fool models by
supplying deceptive input. Research in computer vision, where this
vulnerability was first discovered, has shown that adversarial images designed
to fool a specific model can deceive other machine learning models. In this
paper, we investigate the transferability of adversarial network traffic
against multiple machine learning-based intrusion detection systems.
Furthermore, we analyze the robustness of the ensemble intrusion detection
system, which is notorious for its better accuracy compared to a single model,
against the transferability of adversarial attacks. Finally, we examine Detect
& Reject as a defensive mechanism to limit the effect of the transferability
property of adversarial network traffic against machine learning-based
intrusion detection systems.

    

### [[2112.12124] Machine learning nonequilibrium electron forces for adiabatic spin dynamics](http://arxiv.org/abs/2112.12124)


  We present a generalized potential theory of nonequilibrium torques for the
Landau-Lifshitz equation. The general formulation of exchange forces in terms
of two potential energies allows for the implementation of accurate machine
learning models for adiabatic spin dynamics of out-of-equilibrium itinerant
magnetic systems. To demonstrate our approach, we develop a deep-learning
neural network that successfully learns the forces in a driven s-d model
computed from the nonequilibrium Green's function method. We show that the
Landau-Lifshitz dynamics simulations with forces predicted from the neural-net
model accurately reproduce the voltage-driven domain-wall propagation. Our work
opens a new avenue for multi-scale modeling of nonequilibrium dynamical
phenomena in itinerant magnets and spintronics based on machine-learning
models.

    

### [[2112.12134] A Unified Analysis Method for Online Optimization in Normed Vector Space](http://arxiv.org/abs/2112.12134)


  We present a unified analysis method that relies on the generalized cosine
rule and $\phi$-convex for online optimization in normed vector space using
dynamic regret as the performance metric. In combing the update rules, we start
with strategy $S$ (a two-parameter variant strategy covering Optimistic-FTRL
with surrogate linearized losses), and obtain $S$-I (type-I relaxation variant
form of $S$) and $S$-II (type-II relaxation variant form of $S$, which is
Optimistic-MD) by relaxation. Regret bounds for $S$-I and $S$-II are the
tightest possible. As instantiations, regret bounds of normalized exponentiated
subgradient and greedy/lazy projection are better than the currently known
optimal results. We extend online convex optimization to online monotone
optimization, by replacing losses of online game with monotone operators and
extending the definition of regret, namely regret$^n$, and expand the
application scope of $S$-I and $S$-II.

    

### [[1904.02345] Preference Neural Network](http://arxiv.org/abs/1904.02345)


  This paper proposes a preference neural network (PNN) to address the problem
of indifference preferences orders with new activation function. PNN also
solves the Multi-label ranking problem, where labels may have indifference
preference orders or subgroups are equally ranked. PNN follows a multi-layer
feedforward architecture with fully connected neurons. Each neuron contains a
novel smooth stairstep activation function based on the number of preference
orders. PNN inputs represent data features and output neurons represent label
indexes. The proposed PNN is evaluated using new preference mining dataset that
contains repeated label values which have not experimented before. PNN
outperforms five previously proposed methods for strict label ranking in terms
of accurate results with high computational efficiency.

    

### [[1907.06123] Preselection Bandits](http://arxiv.org/abs/1907.06123)


  In this paper, we introduce the Preselection Bandit problem, in which the
learner preselects a subset of arms (choice alternatives) for a user, which
then chooses the final arm from this subset. The learner is not aware of the
user's preferences, but can learn them from observed choices. In our concrete
setting, we allow these choices to be stochastic and model the user's actions
by means of the Plackett-Luce model. The learner's main task is to preselect
subsets that eventually lead to highly preferred choices. To formalize this
goal, we introduce a reasonable notion of regret and derive lower bounds on the
expected regret. Moreover, we propose algorithms for which the upper bound on
expected regret matches the lower bound up to a logarithmic term of the time
horizon.

    

### [[1909.07558] HAD-GAN: A Human-perception Auxiliary Defense GAN to Defend Adversarial Examples](http://arxiv.org/abs/1909.07558)


  Adversarial examples reveal the vulnerability and unexplained nature of
neural networks. Studying the defense of adversarial examples is of
considerable practical importance. Most adversarial examples that misclassify
networks are often undetectable by humans. In this paper, we propose a defense
model to train the classifier into a human-perception classification model with
shape preference. The proposed model comprising a texture transfer network
(TTN) and an auxiliary defense generative adversarial networks (GAN) is called
Human-perception Auxiliary Defense GAN (HAD-GAN). The TTN is used to extend the
texture samples of a clean image and helps classifiers focus on its shape. GAN
is utilized to form a training framework for the model and generate the
necessary images. A series of experiments conducted on MNIST, Fashion-MNIST and
CIFAR10 show that the proposed model outperforms the state-of-the-art defense
methods for network robustness. The model also demonstrates a significant
improvement on defense capability of adversarial examples.

    

### [[1910.07234] Aerial Images Processing for Car Detection using Convolutional Neural Networks: Comparison between Faster R-CNN and YoloV3](http://arxiv.org/abs/1910.07234)


  In this paper, we address the problem of car detection from aerial images
using Convolutional Neural Networks (CNN). This problem presents additional
challenges as compared to car (or any object) detection from ground images
because features of vehicles from aerial images are more difficult to discern.
To investigate this issue, we assess the performance of two state-of-the-art
CNN algorithms, namely Faster R-CNN, which is the most popular region-based
algorithm, and YOLOv3, which is known to be the fastest detection algorithm. We
analyze two datasets with different characteristics to check the impact of
various factors, such as UAV's altitude, camera resolution, and object size. A
total of 39 training experiments were conducted to account for the effect of
different hyperparameter values. The objective of this work is to conduct the
most robust and exhaustive comparison between these two cutting-edge algorithms
on the specific domain of aerial images. By using a variety of metrics, we show
that YOLOv3 yields better performance in most configurations, except that it
exhibits a lower recall and less confident detections when object sizes and
scales in the testing dataset differ largely from those in the training
dataset.

    

### [[2004.14174] Reevaluating Adversarial Examples in Natural Language](http://arxiv.org/abs/2004.14174)


  State-of-the-art attacks on NLP models lack a shared definition of a what
constitutes a successful attack. We distill ideas from past work into a unified
framework: a successful natural language adversarial example is a perturbation
that fools the model and follows some linguistic constraints. We then analyze
the outputs of two state-of-the-art synonym substitution attacks. We find that
their perturbations often do not preserve semantics, and 38% introduce
grammatical errors. Human surveys reveal that to successfully preserve
semantics, we need to significantly increase the minimum cosine similarities
between the embeddings of swapped words and between the sentence encodings of
original and perturbed sentences.With constraints adjusted to better preserve
semantics and grammaticality, the attack success rate drops by over 70
percentage points.

    

### [[2010.00042] Sampling possible reconstructions of undersampled acquisitions in MR imaging](http://arxiv.org/abs/2010.00042)


  Undersampling the k-space during MR acquisitions saves time, however results
in an ill-posed inversion problem, leading to an infinite set of images as
possible solutions. Traditionally, this is tackled as a reconstruction problem
by searching for a single "best" image out of this solution set according to
some chosen regularization or prior. This approach, however, misses the
possibility of other solutions and hence ignores the uncertainty in the
inversion process. In this paper, we propose a method that instead returns
multiple images which are possible under the acquisition model and the chosen
prior to capture the uncertainty in the inversion process. To this end, we
introduce a low dimensional latent space and model the posterior distribution
of the latent vectors given the acquisition data in k-space, from which we can
sample in the latent space and obtain the corresponding images. We use a
variational autoencoder for the latent model and the Metropolis adjusted
Langevin algorithm for the sampling. We evaluate our method on two datasets;
with images from the Human Connectome Project and in-house measured multi-coil
images. We compare to five alternative methods. Results indicate that the
proposed method produces images that match the measured k-space data better
than the alternatives, while showing realistic structural variability.
Furthermore, in contrast to the compared methods, the proposed method yields
higher uncertainty in the undersampled phase encoding direction, as expected.
Keywords: Magnetic Resonance image reconstruction, uncertainty estimation,
inverse problems, sampling, MCMC, deep learning, unsupervised learning.

    

### [[2010.01654] Probabilistic Feature Selection in Joint Quantile Time Series Analysis](http://arxiv.org/abs/2010.01654)


  Quantile feature selection over correlated multivariate time series data has
always been a methodological challenge and is an open problem. In this paper,
we propose a general probabilistic methodology for feature selection in joint
quantile time series analysis, under the name of quantile feature selection
time series (QFSTS) model. The QFSTS model is a general structural time series
model, where each component yields an additive contribution to the time series
modeling with direct interpretations. Its flexibility is compound in the sense
that users can add/deduct components for each times series and each time series
can have its own specific valued components of different sizes. Feature
selection is conducted in the quantile regression component, where each time
series has its own pool of contemporaneous external predictors allowing
"nowcasting". Creative probabilistic methodology in extending feature selection
to the quantile time series research area is developed by means of multivariate
asymmetric Laplace distribution, ``spike-and-slab" prior setup, the
Metropolis-Hastings algorithm, and the Bayesian model averaging technique, all
implemented consistently in the Bayesian paradigm. Different from most machine
learning algorithms, the QFSTS model requires small datasets to train,
converges fast, and is executable on ordinary personal computers. Extensive
examinations on simulated data and empirical data confirmed that the QFSTS
model has superior performance in feature selection, parameter estimation, and
forecast.

    

### [[2010.13356] Exploring the Security Boundary of Data Reconstruction via Neuron Exclusivity Analysis](http://arxiv.org/abs/2010.13356)


  Among existing privacy attacks on the gradient of neural networks, \emph{data
reconstruction attack}, which reverse engineers the training batch from the
gradient, poses a severe threat on the private training data. Despite its
empirical success on large architectures and small training batches, unstable
reconstruction accuracy is also observed when a smaller architecture or a
larger batch is under attack. Due to the weak interpretability of existing
learning-based attacks, there is little known on why, when and how data
reconstruction attack is feasible.
In our work, we perform the first analytic study on the security boundary of
data reconstruction from gradient via a microcosmic view on neural networks
with rectified linear units (ReLUs), the most popular activation function in
practice. For the first time, we characterize the insecure/secure boundary of
data reconstruction attack in terms of the \emph{neuron exclusivity state} of a
training batch, indexed by the number of \emph{\textbf{Ex}clusively
\textbf{A}ctivated \textbf{N}eurons} (ExANs, i.e., a ReLU activated by only one
sample in a batch). Intuitively, we show a training batch with more ExANs are
more vulnerable to data reconstruction attack and vice versa. On the one hand,
we construct a novel deterministic attack algorithm which substantially
outperforms previous attacks for reconstructing training batches lying in the
insecure boundary of a neural network. Meanwhile, for training batches lying in
the secure boundary, we prove the impossibility of unique reconstruction, based
on which an exclusivity reduction strategy is devised to enlarge the secure
boundary for mitigation purposes.

    

### [[2101.01163] SmartDeal: Re-Modeling Deep Network Weights for Efficient Inference and Training](http://arxiv.org/abs/2101.01163)


  The record-breaking performance of deep neural networks (DNNs) comes with
heavy parameterization, leading to external dynamic random-access memory (DRAM)
for storage. The prohibitive energy of DRAM accesses makes it non-trivial to
deploy DNN on resource-constrained devices, calling for minimizing the weight
and data movements to improve the energy efficiency. We present SmartDeal (SD),
an algorithm framework to trade higher-cost memory storage/access for
lower-cost computation, in order to aggressively boost the storage and energy
efficiency, for both inference and training. The core of SD is a novel weight
decomposition with structural constraints, carefully crafted to unleash the
hardware efficiency potential. Specifically, we decompose each weight tensor as
the product of a small basis matrix and a large structurally sparse coefficient
matrix whose non-zeros are quantized to power-of-2. The resulting sparse and
quantized DNNs enjoy greatly reduced energy for data movement and weight
storage, incurring minimal overhead to recover the original weights thanks to
the sparse bit-operations and cost-favorable computations. Beyond inference, we
take another leap to embrace energy-efficient training, introducing innovative
techniques to address the unique roadblocks arising in training while
preserving the SD structures. We also design a dedicated hardware accelerator
to fully utilize the SD structure to improve the real energy efficiency and
latency. We conduct experiments on both multiple tasks, models and datasets in
different settings. Results show that: 1) applied to inference, SD achieves up
to 2.44x energy efficiency as evaluated via real hardware implementations; 2)
applied to training, SD leads to 10.56x and 4.48x reduction in the storage and
training energy, with negligible accuracy loss compared to state-of-the-art
training baselines. Our source codes are available online.

    

### [[2102.07936] DFAC Framework: Factorizing the Value Function via Quantile Mixture for Multi-Agent Distributional Q-Learning](http://arxiv.org/abs/2102.07936)


  In fully cooperative multi-agent reinforcement learning (MARL) settings, the
environments are highly stochastic due to the partial observability of each
agent and the continuously changing policies of the other agents. To address
the above issues, we integrate distributional RL and value function
factorization methods by proposing a Distributional Value Function
Factorization (DFAC) framework to generalize expected value function
factorization methods to their DFAC variants. DFAC extends the individual
utility functions from deterministic variables to random variables, and models
the quantile function of the total return as a quantile mixture. To validate
DFAC, we demonstrate DFAC's ability to factorize a simple two-step matrix game
with stochastic rewards and perform experiments on all Super Hard tasks of
StarCraft Multi-Agent Challenge, showing that DFAC is able to outperform
expected value function factorization baselines.

    

### [[2103.04623] Consistency Regularization for Adversarial Robustness](http://arxiv.org/abs/2103.04623)


  Adversarial training (AT) is currently one of the most successful methods to
obtain the adversarial robustness of deep neural networks. However, the
phenomenon of robust overfitting, i.e., the robustness starts to decrease
significantly during AT, has been problematic, not only making practitioners
consider a bag of tricks for a successful training, e.g., early stopping, but
also incurring a significant generalization gap in the robustness. In this
paper, we propose an effective regularization technique that prevents robust
overfitting by optimizing an auxiliary `consistency' regularization loss during
AT. Specifically, we discover that data augmentation is a quite effective tool
to mitigate the overfitting in AT, and develop a regularization that forces the
predictive distributions after attacking from two different augmentations of
the same instance to be similar with each other. Our experimental results
demonstrate that such a simple regularization technique brings significant
improvements in the test robust accuracy of a wide range of AT methods. More
remarkably, we also show that our method could significantly help the model to
generalize its robustness against unseen adversaries, e.g., other types or
larger perturbations compared to those used during training. Code is available
at this https URL.

    

### [[2103.05324] A Simple Approach for Non-stationary Linear Bandits](http://arxiv.org/abs/2103.05324)


  This paper investigates the problem of non-stationary linear bandits, where
the unknown regression parameter is evolving over time. Existing studies
develop various algorithms and show that they enjoy an
$\widetilde{\mathcal{O}}(T^{2/3}P_T^{1/3})$ dynamic regret, where $T$ is the
time horizon and $P_T$ is the path-length that measures the fluctuation of the
evolving unknown parameter. In this paper, we discover that a serious technical
flaw makes their results ungrounded, and then present a fix, which gives an
$\widetilde{\mathcal{O}}(T^{3/4}P_T^{1/4})$ dynamic regret without modifying
original algorithms. Furthermore, we demonstrate that instead of using
sophisticated mechanisms, such as sliding window or weighted penalty, a simple
restarted strategy is sufficient to attain the same regret guarantee.
Specifically, we design an UCB-type algorithm to balance exploitation and
exploration, and restart it periodically to handle the drift of unknown
parameters. Our approach enjoys an $\widetilde{\mathcal{O}}(T^{3/4}P_T^{1/4})$
dynamic regret. Note that to achieve this bound, the algorithm requires an
oracle knowledge of the path-length $P_T$. Combining the bandits-over-bandits
mechanism by treating our algorithm as the base learner, we can further achieve
the same regret bound in a parameter-free way. Empirical studies also validate
the effectiveness of our approach.

    

### [[2104.01750] Optimal Sampling Gaps for Adaptive Submodular Maximization](http://arxiv.org/abs/2104.01750)


  Running machine learning algorithms on large and rapidly growing volumes of
data is often computationally expensive, one common trick to reduce the size of
a data set, and thus reduce the computational cost of machine learning
algorithms, is \emph{probability sampling}. It creates a sampled data set by
including each data point from the original data set with a known probability.
Although the benefit of running machine learning algorithms on the reduced data
set is obvious, one major concern is that the performance of the solution
obtained from samples might be much worse than that of the optimal solution
when using the full data set. In this paper, we examine the performance loss
caused by probability sampling in the context of adaptive submodular
maximization. We consider a simple probability sampling method which selects
each data point with probability $r\in[0,1]$. If we set the sampling rate
$r=1$, our problem reduces to finding a solution based on the original full
data set. We define sampling gap as the largest ratio between the optimal
solution obtained from the full data set and the optimal solution obtained from
the samples, over independence systems. %It captures the performance loss of
the optimal solution caused by the probability sampling. Our main contribution
is to show that if the utility function is policywise submodular, then for a
given sampling rate $r$, the sampling gap is both upper bounded and lower
bounded by $1/r$. One immediate implication of our result is that if we can
find an $\alpha$-approximation solution based on a sampled data set (which is
sampled at sampling rate $r$), then this solution achieves an $\alpha r$
approximation ratio against the optimal solution when using the full data set.

    

### [[2104.06461] Learning Log-Determinant Divergences for Positive Definite Matrices](http://arxiv.org/abs/2104.06461)


  Representations in the form of Symmetric Positive Definite (SPD) matrices
have been popularized in a variety of visual learning applications due to their
demonstrated ability to capture rich second-order statistics of visual data.
There exist several similarity measures for comparing SPD matrices with
documented benefits. However, selecting an appropriate measure for a given
problem remains a challenge and in most cases, is the result of a
trial-and-error process. In this paper, we propose to learn similarity measures
in a data-driven manner. To this end, we capitalize on the \alpha\beta-log-det
divergence, which is a meta-divergence parametrized by scalars \alpha and
\beta, subsuming a wide family of popular information divergences on SPD
matrices for distinct and discrete values of these parameters. Our key idea is
to cast these parameters in a continuum and learn them from data. We
systematically extend this idea to learn vector-valued parameters, thereby
increasing the expressiveness of the underlying non-linear measure. We conjoin
the divergence learning problem with several standard tasks in machine
learning, including supervised discriminative dictionary learning and
unsupervised SPD matrix clustering. We present Riemannian gradient descent
schemes for optimizing our formulations efficiently, and show the usefulness of
our method on eight standard computer vision tasks.

    

### [[2106.14813] Offline Planning and Online Learning under Recovering Rewards](http://arxiv.org/abs/2106.14813)


  Motivated by emerging applications such as live-streaming e-commerce,
promotions and recommendations, we introduce and solve a general class of
non-stationary multi-armed bandit problems that have the following two
features: (i) the decision maker can pull and collect rewards from up to
$K\,(\ge 1)$ out of $N$ different arms in each time period; (ii) the expected
reward of an arm immediately drops after it is pulled, and then
non-parametrically recovers as the arm's idle time increases. With the
objective of maximizing the expected cumulative reward over $T$ time periods,
we design a class of ``Purely Periodic Policies'' that jointly set a period to
pull each arm. For the proposed policies, we prove performance guarantees for
both the offline problem and the online problems. For the offline problem when
all model parameters are known, the proposed periodic policy obtains an
approximation ratio that is at the order of $1-\mathcal O(1/\sqrt{K})$, which
is asymptotically optimal when $K$ grows to infinity. For the online problem
when the model parameters are unknown and need to be dynamically learned, we
integrate the offline periodic policy with the upper confidence bound procedure
to construct on online policy. The proposed online policy is proved to
approximately have $\widetilde{\mathcal O}(N\sqrt{T})$ regret against the
offline benchmark. Our framework and policy design may shed light on broader
offline planning and online learning applications with non-stationary and
recovering rewards.

    

### [[2110.10927] SecureBoost+ : A High Performance Gradient Boosting Tree Framework for Large Scale Vertical Federated Learning](http://arxiv.org/abs/2110.10927)


  Gradient boosting decision tree (GBDT) is a widely used ensemble algorithm in
the industry. Its vertical federated learning version, SecureBoost, is one of
the most popular algorithms used in cross-silo privacy-preserving modeling. As
the area of privacy computation thrives in recent years, demands for
large-scale and high-performance federated learning have grown dramatically in
real-world applications. In this paper, to fulfill these requirements, we
propose SecureBoost+ that is both novel and improved from the prior work
SecureBoost. SecureBoost+ integrates several ciphertext calculation
optimizations and engineering optimizations. The experimental results
demonstrate that Secureboost+ has significant performance improvements on large
and high dimensional data sets compared to SecureBoost. It makes effective and
efficient large-scale vertical federated learning possible.

    

### [[2110.12506] Detecting model drift using polynomial relations](http://arxiv.org/abs/2110.12506)


  Machine learning models serve critical functions, such as classifying loan
applicants as good or bad risks. Each model is trained under the assumption
that the data used in training and in the field come from the same underlying
unknown distribution. Often, this assumption is broken in practice. It is
desirable to identify when this occurs, to minimize the impact on model
performance.
We suggest a new approach to detecting change in the data distribution by
identifying polynomial relations between the data features. We measure the
strength of each identified relation using its R-square value. A strong
polynomial relation captures a significant trait of the data which should
remain stable if the data distribution does not change. We thus use a set of
learned strong polynomial relations to identify drift. For a set of polynomial
relations that are stronger than a given threshold, we calculate the amount of
drift observed for that relation. The amount of drift is measured by
calculating the Bayes Factor for the polynomial relation likelihood of the
baseline data versus field data. We empirically validate the approach by
simulating a range of changes, and identify drift using the Bayes Factor of the
polynomial relation likelihood change.

    

### [[2111.04613] Variational Automatic Curriculum Learning for Sparse-Reward Cooperative Multi-Agent Problems](http://arxiv.org/abs/2111.04613)


  We introduce a curriculum learning algorithm, Variational Automatic
Curriculum Learning (VACL), for solving challenging goal-conditioned
cooperative multi-agent reinforcement learning problems. We motivate our
paradigm through a variational perspective, where the learning objective can be
decomposed into two terms: task learning on the current task distribution, and
curriculum update to a new task distribution. Local optimization over the
second term suggests that the curriculum should gradually expand the training
tasks from easy to hard. Our VACL algorithm implements this variational
paradigm with two practical components, task expansion and entity progression,
which produces training curricula over both the task configurations as well as
the number of entities in the task. Experiment results show that VACL solves a
collection of sparse-reward problems with a large number of agents.
Particularly, using a single desktop machine, VACL achieves 98% coverage rate
with 100 agents in the simple-spread benchmark and reproduces the ramp-use
behavior originally shown in OpenAI's hide-and-seek project. Our project
website is at this https URL.

    

### [[2112.10741] GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models](http://arxiv.org/abs/2112.10741)


  Diffusion models have recently been shown to generate high-quality synthetic
images, especially when paired with a guidance technique to trade off diversity
for fidelity. We explore diffusion models for the problem of text-conditional
image synthesis and compare two different guidance strategies: CLIP guidance
and classifier-free guidance. We find that the latter is preferred by human
evaluators for both photorealism and caption similarity, and often produces
photorealistic samples. Samples from a 3.5 billion parameter text-conditional
diffusion model using classifier-free guidance are favored by human evaluators
to those from DALL-E, even when the latter uses expensive CLIP reranking.
Additionally, we find that our models can be fine-tuned to perform image
inpainting, enabling powerful text-driven image editing. We train a smaller
model on a filtered dataset and release the code and weights at
this https URL.

    

### [[2011.07952] Multi-Coil MRI Reconstruction Challenge -- Assessing Brain MRI Reconstruction Models and their Generalizability to Varying Coil Configurations](http://arxiv.org/abs/2011.07952)


  Deep-learning-based brain magnetic resonance imaging (MRI) reconstruction
methods have the potential to accelerate the MRI acquisition process.
Nevertheless, the scientific community lacks appropriate benchmarks to assess
MRI reconstruction quality of high-resolution brain images, and evaluate how
these proposed algorithms will behave in the presence of small, but expected
data distribution shifts. The Multi-Coil Magnetic Resonance Image (MC-MRI)
Reconstruction Challenge provides a benchmark that aims at addressing these
issues, using a large dataset of high-resolution, three-dimensional,
T1-weighted MRI scans. The challenge has two primary goals: 1) to compare
different MRI reconstruction models on this dataset and 2) to assess the
generalizability of these models to data acquired with a different number of
receiver coils. In this paper, we describe the challenge experimental design,
and summarize the results of a set of baseline and state of the art brain MRI
reconstruction models. We provide relevant comparative information on the
current MRI reconstruction state-of-the-art and highlight the challenges of
obtaining generalizable models that are required prior to broader clinical
adoption. The MC-MRI benchmark data, evaluation code and current challenge
leaderboard are publicly available. They provide an objective performance
assessment for future developments in the field of brain MRI reconstruction.

    

### [[2112.10953] An Extension of InfoMap to Absorbing Random Walks](http://arxiv.org/abs/2112.10953)


  InfoMap is a popular approach for detecting densely connected "communities"
of nodes in networks. To detect such communities, it builds on the standard
type of Markov chain and ideas from information theory. Motivated by the
dynamics of disease spread on networks, whose nodes may have heterogeneous
disease-removal rates, we extend InfoMap to absorbing random walks. To do this,
we use absorption-scaled graphs, in which the edge weights are scaled according
to the absorption rates, along with Markov time sweeping. One of our extensions
of InfoMap converges to the standard version of InfoMap in the limit in which
the absorption rates approach $0$. We find that the community structure that
one detects using our extensions of InfoMap can differ markedly from the
community structure that one detects using methods that do not take
node-absorption rates into account. Additionally, we demonstrate that the
community structure that is induced by local dynamics can have important
implications for susceptible-infected-recovered (SIR) dynamics on ring-lattice
networks. For example, we find situations in which the outbreak duration is
maximized when a moderate number of nodes have large node-absorption rates. We
also use our extensions of InfoMap to study community structure in a
sexual-contact network. We consider the community structure that corresponds to
different absorption rates for homeless individuals in the network and the
associated impact on syphilis dynamics on the network. We observe that the
final outbreak size can be smaller when treatment rates are lower in the
homeless population than in other populations than when they are the same in
all populations.

    

### [[2112.11427] StyleSDF: High-Resolution 3D-Consistent Image and Geometry Generation](http://arxiv.org/abs/2112.11427)


  We introduce a high resolution, 3D-consistent image and shape generation
technique which we call StyleSDF. Our method is trained on single-view RGB data
only, and stands on the shoulders of StyleGAN2 for image generation, while
solving two main challenges in 3D-aware GANs: 1) high-resolution,
view-consistent generation of the RGB images, and 2) detailed 3D shape. We
achieve this by merging a SDF-based 3D representation with a style-based 2D
generator. Our 3D implicit network renders low-resolution feature maps, from
which the style-based network generates view-consistent, 1024x1024 images.
Notably, our SDF-based 3D modeling defines detailed 3D surfaces, leading to
consistent volume rendering. Our method shows higher quality results compared
to state of the art in terms of visual and geometric quality.

    

### [[2112.11587] DarkGates: A Hybrid Power-Gating Architecture to Mitigate the Performance Impact of Dark-Silicon in High Performance Processors](http://arxiv.org/abs/2112.11587)


  To reduce the leakage power of inactive (dark) silicon components, modern
processor systems shut-off these components' power supply using low-leakage
transistors, called power-gates. Unfortunately, power-gates increase the
system's power-delivery impedance and voltage guardband, limiting the system's
maximum attainable voltage (i.e., Vmax) and, thus, the CPU core's maximum
attainable frequency (i.e., Fmax). As a result, systems that are performance
constrained by the CPU frequency (i.e., Fmax-constrained), such as high-end
desktops, suffer significant performance loss due to power-gates.
To mitigate this performance loss, we propose DarkGates, a hybrid system
architecture that increases the performance of Fmax-constrained systems while
fulfilling their power efficiency requirements. DarkGates is based on three key
techniques: i) bypassing on-chip power-gates using package-level resources
(called bypass mode), ii) extending power management firmware to support
operation either in bypass mode or normal mode, and iii) introducing deeper
idle power states.
We implement DarkGates on an Intel Skylake microprocessor for client devices
and evaluate it using a wide variety of workloads. On a real 4-core Skylake
system with integrated graphics, DarkGates improves the average performance of
SPEC CPU2006 workloads across all thermal design power (TDP) levels (35W-91W)
between 4.2% and 5.3%. DarkGates maintains the performance of 3DMark workloads
for desktop systems with TDP greater than 45W while for a 35W-TDP (the lowest
TDP) desktop it experiences only a 2% degradation. In addition, DarkGates
fulfills the requirements of the ENERGY STAR and the Intel Ready Mode energy
efficiency benchmarks of desktop systems.

    

### [[2109.02379] QFlow: Quantitative Information Flow for Security-Aware Hardware Design in Verilog](http://arxiv.org/abs/2109.02379)


  The enormous amount of code required to design modern hardware
implementations often leads to critical vulnerabilities being overlooked.
Especially vulnerabilities that compromise the confidentiality of sensitive
data, such as cryptographic keys, have a major impact on the trustworthiness of
an entire system. Information flow analysis can elaborate whether information
from sensitive signals flows towards outputs or untrusted components of the
system. But most of these analytical strategies rely on the non-interference
property, stating that the untrusted targets must not be influenced by the
source's data, which is shown to be too inflexible for many applications. To
address this issue, there are approaches to quantify the information flow
between components such that insignificant leakage can be neglected. Due to the
high computational complexity of this quantification, approximations are
needed, which introduce mispredictions. To tackle those limitations, we
reformulate the approximations. Further, we propose a tool QFlow with a higher
detection rate than previous tools. It can be used by non-experienced users to
identify data leakages in hardware designs, thus facilitating a security-aware
design process.

    

### [[2112.11597] Rightsizing Clusters for Time-Limited Tasks](http://arxiv.org/abs/2112.11597)


  In conventional public clouds, designing a suitable initial cluster for a
given application workload is important in reducing the computational
foot-print during run-time. In edge or on-premise clouds, cold-start
rightsizing the cluster at the time of installation is crucial in avoiding the
recurrent capital expenditure. In both these cases, rightsizing has to balance
cost-performance trade-off for a given application with multiple tasks, where
each task can demand multiple resources, and the cloud offers nodes with
different capacity and cost. Multidimensional bin-packing can address this
cold-start rightsizing problem, but assumes that every task is always active.
In contrast, real-world tasks (e.g. load bursts, batch and dead-lined tasks
with time-limits) may be active only during specific time-periods or may have
dynamic load profiles. The cluster cost can be reduced by reusing resources via
time sharing and optimal packing. This motivates our generalized problem of
cold-start rightsizing for time-limited tasks: given a timeline, time-periods
and resource demands for tasks, the objective is to place the tasks on a
minimum cost cluster of nodes without violating node capacities at any time
instance. We design a baseline two-phase algorithm that performs penalty-based
mapping of task to node-type and then, solves each node-type independently. We
prove that the algorithm has an approximation ratio of O(D min(m, T)), where D,
m and T are the number of resources, node-types and timeslots, respectively. We
then present an improved linear programming based mapping strategy, enhanced
further with a cross-node-type filling mechanism. Our experiments on synthetic
and real-world cluster traces show significant cost reduction by LP-based
mapping compared to the baseline, and the filling mechanism improves further to
produce solutions within 20% of (a lower-bound to) the optimal solution.

    

### [[2112.11684] HP-GNN: Generating High Throughput GNN Training Implementation on CPU-FPGA Heterogeneous Platform](http://arxiv.org/abs/2112.11684)


  Graph Neural Networks (GNNs) have shown great success in many applications
such as recommendation systems, molecular property prediction, traffic
prediction, etc. Recently, CPU-FPGA heterogeneous platforms have been used to
accelerate many applications by exploiting customizable data path and abundant
user-controllable on-chip memory resources of FPGAs. Yet, accelerating and
deploying GNN training on such platforms requires not only expertise in
hardware design but also substantial development efforts.
We propose HP-GNN, a novel framework that generates high throughput GNN
training implementations on a given CPU-FPGA platform that can benefit both
application developers and machine learning researchers. HP-GNN takes GNN
training algorithms, GNN models as the inputs, and automatically performs
hardware mapping onto the target CPU-FPGA platform. HP-GNN consists of: (1)
data layout and internal representation that reduce the memory traffic and
random memory accesses; (2) optimized hardware templates that support various
GNN models; (3) a design space exploration engine for automatic hardware
mapping; (4) high-level application programming interfaces (APIs) that allows
users to specify GNN training with only a handful of lines of code. To evaluate
HP-GNN, we experiment with two well-known sampling-based GNN training
algorithms and two GNN models. For each training algorithm and model, HP-GNN
generates implementation on a state-of-the-art CPU-FPGA platform. Compared with
CPU-only and CPU-GPU platforms, experimental results show that the generated
implementations achieve $55.67\times$ and $2.17\times$ speedup on the average,
respectively. Compared with the state-of-the-art GNN training implementations,
HP-GNN achieves up to $4.45\times$ speedup.

    

### [[2112.11879] Lifting C Semantics for Dataflow Optimization](http://arxiv.org/abs/2112.11879)


  C is the lingua franca of programming and almost any device can be programmed
using C. However, programming mod-ern heterogeneous architectures such as
multi-core CPUs and GPUs requires explicitly expressing parallelism as well as
device-specific properties such as memory hierarchies. The resulting code is
often hard to understand, debug, and modify for different architectures. We
propose to lift C pro-grams to a parametric dataflow representation that lends
itself to static data-centric analysis and enables automatic high-performance
code generation. We separate writing code from optimizing for different
hardware: simple, portable C source code is used to generate efficient
specialized versions with a click of a button. Our approach can identify
parallelism when no other compiler can, and outperforms a bespoke parallelized
version of a scientific proxy application by up to21%.

    

### [[2112.11880] Iterative Krylov Methods for Acoustic Problems on Graphics Processing Unit](http://arxiv.org/abs/2112.11880)


  This paper deals with linear algebra operations on Graphics Processing Unit
(GPU) with complex number arithmetic using double precision. An analysis of
their uses within iterative Krylov methods is presented to solve acoustic
problems. Numerical experiments performed on a set of acoustic matrices arising
from the modelisation of acoustic phenomena inside a car compartment are
collected, and outline the performance, robustness and effectiveness of our
algorithms, with a speed-up up to 28x for dot product, 9.8x for sparse
matrix-vector product and solvers.

    

### [[2112.11978] Callback-based Completion Notification using MPI Continuations](http://arxiv.org/abs/2112.11978)


  Asynchronous programming models (APM) are gaining more and more traction,
allowing applications to expose the available concurrency to a runtime system
tasked with coordinating the execution. While MPI has long provided support for
multi-threaded communication and non-blocking operations, it falls short of
adequately supporting APMs as correctly and efficiently handling MPI
communication in different models is still a challenge. We have previously
proposed an extension to the MPI standard providing operation completion
notifications using callbacks, so-called MPI Continuations. This interface is
flexible enough to accommodate a wide range of different APMs.
In this paper, we present an extension to the previously described interface
that allows for finer control of the behavior of the MPI Continuations
interface. We then present some of our first experiences in using the interface
in the context of different applications, including the NAS parallel
benchmarks, the PaRSEC task-based runtime system, and a load-balancing scheme
within an adaptive mesh refinement solver called ExaHyPE. We show that the
interface, implemented inside Open MPI, enables low-latency, high-throughput
completion notifications that outperform solutions implemented in the
application space.

    

### [[2112.12142] Survey the storage systems used in HPC and BDA ecosystems](http://arxiv.org/abs/2112.12142)


  The advancement in HPC and BDA ecosystem demands a better understanding of
the storage systems to plan effective this http URL make applications access
data more efficiently for computation, HPC and BDA ecosystems adopt different
storage systems.Each storage system has its pros and cons.Therefore, it is
worthwhile and interesting to explore the storage systems used in HPC and BDA
respectively.Also, it's inquisitive to understand how such storage systems can
handle data consistency and fault tolerance at a massive this http URL this paper,
we're surveying four storage systems Lustre, Ceph, HDFS, and CockroachDB.Lustre
and HDFS are some of the most prominent file systems in HPC and BDA
ecosystem.Ceph is an upcoming filesystem and is being used by
supercomputers.CockroachDB is based on NewSQL systems a technique that is being
used in the industry for BDA applications.The study helps us to understand the
underlying architecture of these storage systems and the building blocks used
to create them.The protocols and mechanisms used for data storage, data access,
data consistency, fault tolerance, and recovery from failover are also
overviewed.The comparative study will help system designers to understand the
key features and architectural goals of these storage systems to select better
storage system solutions.

    

### [[2112.11481] Translating Human Mobility Forecasting through Natural Language Generation](http://arxiv.org/abs/2112.11481)


  Existing human mobility forecasting models follow the standard design of the
time-series prediction model which takes a series of numerical values as input
to generate a numerical value as a prediction. Although treating this as a
regression problem seems straightforward, incorporating various contextual
information such as the semantic category information of each Place-of-Interest
(POI) is a necessary step, and often the bottleneck, in designing an effective
mobility prediction model. As opposed to the typical approach, we treat
forecasting as a translation problem and propose a novel forecasting through a
language generation pipeline. The paper aims to address the human mobility
forecasting problem as a language translation task in a sequence-to-sequence
manner. A mobility-to-language template is first introduced to describe the
numerical mobility data as natural language sentences. The core intuition of
the human mobility forecasting translation task is to convert the input
mobility description sentences into a future mobility description from which
the prediction target can be obtained. Under this pipeline, a two-branch
network, SHIFT (Translating Human Mobility Forecasting), is designed.
Specifically, it consists of one main branch for language generation and one
auxiliary branch to directly learn mobility patterns. During the training, we
develop a momentum mode for better connecting and training the two branches.
Extensive experiments on three real-world datasets demonstrate that the
proposed SHIFT is effective and presents a new revolutionary approach to
forecasting human mobility.

    

### [[2112.11561] Explainable Artificial Intelligence for Autonomous Driving: A Comprehensive Overview and Field Guide for Future Research Directions](http://arxiv.org/abs/2112.11561)


  Autonomous driving has achieved a significant milestone in research and
development over the last decade. There is increasing interest in the field as
the deployment of self-operating vehicles on roads promises safer and more
ecologically friendly transportation systems. With the rise of computationally
powerful artificial intelligence (AI) techniques, autonomous vehicles can sense
their environment with high precision, make safe real-time decisions, and
operate more reliably without human interventions. However, intelligent
decision-making in autonomous cars is not generally understandable by humans in
the current state of the art, and such deficiency hinders this technology from
being socially acceptable. Hence, aside from making safe real-time decisions,
the AI systems of autonomous vehicles also need to explain how these decisions
are constructed in order to be regulatory compliant across many jurisdictions.
Our study sheds a comprehensive light on developing explainable artificial
intelligence (XAI) approaches for autonomous vehicles. In particular, we make
the following contributions. First, we provide a thorough overview of the
present gaps with respect to explanations in the state-of-the-art autonomous
vehicle industry. We then show the taxonomy of explanations and explanation
receivers in this field. Thirdly, we propose a framework for an architecture of
end-to-end autonomous driving systems and justify the role of XAI in both
debugging and regulating such systems. Finally, as future research directions,
we provide a field guide on XAI approaches for autonomous driving that can
improve operational safety and transparency towards achieving public approval
by regulators, manufacturers, and all engaged stakeholders.

    

### [[2112.11623] MOSAIC: Mobile Segmentation via decoding Aggregated Information and encoded Context](http://arxiv.org/abs/2112.11623)


  We present a next-generation neural network architecture, MOSAIC, for
efficient and accurate semantic image segmentation on mobile devices. MOSAIC is
designed using commonly supported neural operations by diverse mobile hardware
platforms for flexible deployment across various mobile platforms. With a
simple asymmetric encoder-decoder structure which consists of an efficient
multi-scale context encoder and a light-weight hybrid decoder to recover
spatial details from aggregated information, MOSAIC achieves new
state-of-the-art performance while balancing accuracy and computational cost.
Deployed on top of a tailored feature extraction backbone based on a searched
classification network, MOSAIC achieves a 5% absolute accuracy gain surpassing
the current industry standard MLPerf models and state-of-the-art architectures.

    

### [[2112.11632] Diformer: Directional Transformer for Neural Machine Translation](http://arxiv.org/abs/2112.11632)


  Autoregressive (AR) and Non-autoregressive (NAR) models have their own
superiority on the performance and latency, combining them into one model may
take advantage of both. Current combination frameworks focus more on the
integration of multiple decoding paradigms with a unified generative model,
e.g. Masked Language Model. However, the generalization can be harmful to the
performance due to the gap between training objective and inference. In this
paper, we aim to close the gap by preserving the original objective of AR and
NAR under a unified framework. Specifically, we propose the Directional
Transformer (Diformer) by jointly modelling AR and NAR into three generation
directions (left-to-right, right-to-left and straight) with a newly introduced
direction variable, which works by controlling the prediction of each token to
have specific dependencies under that direction. The unification achieved by
direction successfully preserves the original dependency assumption used in AR
and NAR, retaining both generalization and performance. Experiments on 4 WMT
benchmarks demonstrate that Diformer outperforms current united-modelling works
with more than 1.5 BLEU points for both AR and NAR decoding, and is also
competitive to the state-of-the-art independent AR and NAR models.

    

### [[2112.11640] Self-Distillation Mixup Training for Non-autoregressive Neural Machine Translation](http://arxiv.org/abs/2112.11640)


  Recently, non-autoregressive (NAT) models predict outputs in parallel,
achieving substantial improvements in generation speed compared to
autoregressive (AT) models. While performing worse on raw data, most NAT models
are trained as student models on distilled data generated by AT teacher models,
which is known as sequence-level Knowledge Distillation. An effective training
strategy to improve the performance of AT models is Self-Distillation Mixup
(SDM) Training, which pre-trains a model on raw data, generates distilled data
by the pre-trained model itself and finally re-trains a model on the
combination of raw data and distilled data. In this work, we aim to view SDM
for NAT models, but find directly adopting SDM to NAT models gains no
improvements in terms of translation quality. Through careful analysis, we
observe the invalidation is correlated to Modeling Diversity and Confirmation
Bias between the AT teacher model and the NAT student models. Based on these
findings, we propose an enhanced strategy named SDMRT by adding two stages to
classic SDM: one is Pre-Rerank on self-distilled data, the other is Fine-Tune
on Filtered teacher-distilled data. Our results outperform baselines by 0.6 to
1.2 BLEU on multiple NAT models. As another bonus, for Iterative Refinement NAT
models, our methods can outperform baselines within half iteration number,
which means 2X acceleration.

    

### [[2112.11642] Joint-training on Symbiosis Networks for Deep Nueral Machine Translation models](http://arxiv.org/abs/2112.11642)


  Deep encoders have been proven to be effective in improving neural machine
translation (NMT) systems, but it reaches the upper bound of translation
quality when the number of encoder layers exceeds 18. Worse still, deeper
networks consume a lot of memory, making it impossible to train efficiently. In
this paper, we present Symbiosis Networks, which include a full network as the
Symbiosis Main Network (M-Net) and another shared sub-network with the same
structure but less layers as the Symbiotic Sub Network (S-Net). We adopt
Symbiosis Networks on Transformer-deep (m-n) architecture and define a
particular regularization loss $\mathcal{L}_{\tau}$ between the M-Net and S-Net
in NMT. We apply joint-training on the Symbiosis Networks and aim to improve
the M-Net performance. Our proposed training strategy improves Transformer-deep
(12-6) by 0.61, 0.49 and 0.69 BLEU over the baselines under classic training on
WMT'14 EN->DE, DE->EN and EN->FR tasks. Furthermore, our Transformer-deep
(12-6) even outperforms classic Transformer-deep (18-6).

    

### [[2112.11701] Maximum Entropy Population Based Training for Zero-Shot Human-AI Coordination](http://arxiv.org/abs/2112.11701)


  An AI agent should be able to coordinate with humans to solve tasks. We
consider the problem of training a Reinforcement Learning (RL) agent without
using any human data, i.e., in a zero-shot setting, to make it capable of
collaborating with humans. Standard RL agents learn through self-play.
Unfortunately, these agents only know how to collaborate with themselves and
normally do not perform well with unseen partners, such as humans. The
methodology of how to train a robust agent in a zero-shot fashion is still
subject to research. Motivated from the maximum entropy RL, we derive a
centralized population entropy objective to facilitate learning of a diverse
population of agents, which is later used to train a robust agent to
collaborate with unseen partners. The proposed method shows its effectiveness
compared to baseline methods, including self-play PPO, the standard
Population-Based Training (PBT), and trajectory diversity-based PBT, in the
popular Overcooked game environment. We also conduct online experiments with
real humans and further demonstrate the efficacy of the method in the real
world. A supplementary video showing experimental results is available at
this https URL.

    

### [[2112.11718] Hybrid Curriculum Learning for Emotion Recognition in Conversation](http://arxiv.org/abs/2112.11718)


  Emotion recognition in conversation (ERC) aims to detect the emotion label
for each utterance. Motivated by recent studies which have proven that feeding
training examples in a meaningful order rather than considering them randomly
can boost the performance of models, we propose an ERC-oriented hybrid
curriculum learning framework. Our framework consists of two curricula: (1)
conversation-level curriculum (CC); and (2) utterance-level curriculum (UC). In
CC, we construct a difficulty measurer based on "emotion shift" frequency
within a conversation, then the conversations are scheduled in an "easy to
hard" schema according to the difficulty score returned by the difficulty
measurer. For UC, it is implemented from an emotion-similarity perspective,
which progressively strengthens the model's ability in identifying the
confusing emotions. With the proposed model-agnostic hybrid curriculum learning
strategy, we observe significant performance boosts over a wide range of
existing ERC models and we are able to achieve new state-of-the-art results on
four public ERC datasets.

    

### [[2112.11749] Class-aware Sounding Objects Localization via Audiovisual Correspondence](http://arxiv.org/abs/2112.11749)


  Audiovisual scenes are pervasive in our daily life. It is commonplace for
humans to discriminatively localize different sounding objects but quite
challenging for machines to achieve class-aware sounding objects localization
without category annotations, i.e., localizing the sounding object and
recognizing its category. To address this problem, we propose a two-stage
step-by-step learning framework to localize and recognize sounding objects in
complex audiovisual scenarios using only the correspondence between audio and
vision. First, we propose to determine the sounding area via coarse-grained
audiovisual correspondence in the single source cases. Then visual features in
the sounding area are leveraged as candidate object representations to
establish a category-representation object dictionary for expressive visual
character extraction. We generate class-aware object localization maps in
cocktail-party scenarios and use audiovisual correspondence to suppress silent
areas by referring to this dictionary. Finally, we employ category-level
audiovisual consistency as the supervision to achieve fine-grained audio and
sounding object distribution alignment. Experiments on both realistic and
synthesized videos show that our model is superior in localizing and
recognizing objects as well as filtering out silent ones. We also transfer the
learned audiovisual network into the unsupervised object detection task,
obtaining reasonable performance.

    

### [[2112.11796] Shape Fragments](http://arxiv.org/abs/2112.11796)


  In constraint languages for RDF graphs, such as ShEx and SHACL, constraints
on nodes and their properties in RDF graphs are known as "shapes". Schemas in
these languages list the various shapes that certain targeted nodes must
satisfy for the graph to conform to the schema. Using SHACL, we propose in this
paper a novel use of shapes, by which a set of shapes is used to extract a
subgraph from an RDF graph, the so-called shape fragment. Our proposed
mechanism fits in the framework of Linked Data Fragments. In this paper, (i) we
define our extraction mechanism formally, building on recently proposed SHACL
formalizations; (ii) we establish correctness properties, which relate shape
fragments to notions of provenance for database queries; (iii) we compare shape
fragments with SPARQL queries; (iv) we discuss implementation options; and (v)
we present initial experiments demonstrating that shape fragments are a
feasible new idea.

    

### [[2112.11834] Bottom-up approaches for multi-person pose estimation and it's applications: A brief review](http://arxiv.org/abs/2112.11834)


  Human Pose Estimation (HPE) is one of the fundamental problems in computer
vision. It has applications ranging from virtual reality, human behavior
analysis, video surveillance, anomaly detection, self-driving to medical
assistance. The main objective of HPE is to obtain the person's posture from
the given input. Among different paradigms for HPE, one paradigm is called
bottom-up multi-person pose estimation. In the bottom-up approach, initially,
all the key points of the targets are detected, and later in the optimization
stage, the detected key points are associated with the corresponding targets.
This review paper discussed the recent advancements in bottom-up approaches for
the HPE and listed the possible high-quality datasets used to train the models.
Additionally, a discussion of the prominent bottom-up approaches and their
quantitative results on the standard performance matrices are given. Finally,
the limitations of the existing methods are highlighted, and guidelines of the
future research directions are given.

    

### [[2112.11850] Multimodal Analysis of memes for sentiment extraction](http://arxiv.org/abs/2112.11850)


  Memes are one of the most ubiquitous forms of social media communication. The
study and processing of memes, which are intrinsically multimedia, is a popular
topic right now. The study presented in this research is based on the Memotion
dataset, which involves categorising memes based on irony, comedy, motivation,
and overall-sentiment. Three separate innovative transformer-based techniques
have been developed, and their outcomes have been thoroughly reviewed.The best
algorithm achieved a macro F1 score of 0.633 for humour classification, 0.55
for motivation classification, 0.61 for sarcasm classification, and 0.575 for
overall sentiment of the meme out of all our techniques.

    

### [[2112.11909] Few-shot Multi-hop Question Answering over Knowledge Base](http://arxiv.org/abs/2112.11909)


  Previous work on Chinese Knowledge Base Question Answering has been
restricted due to the lack of complex Chinese semantic parsing dataset and the
exponentially growth of searching space with the length of relation paths. This
paper proposes an efficient pipeline method equipped with a pre-trained
language model and a strategy to construct artificial training samples, which
only needs small amount of data but performs well on open-domain complex
Chinese Question Answering task. Besides, By adopting a Beam Search algorithm
based on a language model marking scores for candidate query tuples, we
decelerate the growing relation paths when generating multi-hop query paths.
Finally, we evaluate our model on CCKS2019 Complex Question Answering via
Knowledge Base task and achieves F1-score of 62.55\% on the test dataset.
Moreover when training with only 10\% data, our model can still achieves
F1-score of 58.54\%. The result shows the capability of our model to process
KBQA task and the advantage in few-shot learning.

    

### [[2112.11911] Towards Interactive Language Modeling](http://arxiv.org/abs/2112.11911)


  Interaction between caregivers and children plays a critical role in human
language acquisition and development. Given this observation, it is remarkable
that explicit interaction plays little to no role in artificial language
modeling -- which also targets the acquisition of human language, yet by
artificial models. Moreover, an interactive approach to language modeling has
the potential to make language models substantially more versatile and to
considerably impact downstream applications. Motivated by these considerations,
we pioneer the space of interactive language modeling. As a first contribution
we present a road map in which we detail the steps that need to be taken
towards interactive language modeling. We then lead by example and take the
first steps on this road map, showing the initial feasibility of our approach.
As such, this work aims to be the start of a larger research agenda on
interactive language modeling.

    

### [[2112.11914] Assisted Text Annotation Using Active Learning to Achieve High Quality with Little Effort](http://arxiv.org/abs/2112.11914)


  Large amounts of annotated data have become more important than ever,
especially since the rise of deep learning techniques. However, manual
annotations are costly. We propose a tool that enables researchers to create
large, high-quality, annotated datasets with only a few manual annotations,
thus strongly reducing annotation cost and effort. For this purpose, we combine
an active learning (AL) approach with a pre-trained language model to
semi-automatically identify annotation categories in the given text documents.
To highlight our research direction's potential, we evaluate the approach on
the task of identifying frames in news articles. Our preliminary results show
that employing AL strongly reduces the number of annotations for correct
classification of even these complex and subtle frames. On the framing dataset,
the AL approach needs only 16.3\% of the annotations to reach the same
performance as a model trained on the full dataset.

    

### [[2112.11915] Automatic Product Copywriting for E-Commerce](http://arxiv.org/abs/2112.11915)


  Product copywriting is a critical component of e-commerce recommendation
platforms. It aims to attract users' interest and improve user experience by
highlighting product characteristics with textual descriptions. In this paper,
we report our experience deploying the proposed Automatic Product Copywriting
Generation (APCG) system into the this http URL e-commerce product recommendation
platform. It consists of two main components: 1) natural language generation,
which is built from a transformer-pointer network and a pre-trained
sequence-to-sequence model based on millions of training data from our in-house
platform; and 2) copywriting quality control, which is based on both automatic
evaluation and human screening. For selected domains, the models are trained
and updated daily with the updated training data. In addition, the model is
also used as a real-time writing assistant tool on our live broadcast platform.
The APCG system has been deployed in this http URL since Feb 2021. By Sep 2021, it has
generated 2.53 million product descriptions, and improved the overall averaged
click-through rate (CTR) and the Conversion Rate (CVR) by 4.22% and 3.61%,
compared to baselines, respectively on a year-on-year basis. The accumulated
Gross Merchandise Volume (GMV) made by our system is improved by 213.42%,
compared to the number in Feb 2021.

    

### [[2112.11937] Adversarial Deep Reinforcement Learning for Trustworthy Autonomous Driving Policies](http://arxiv.org/abs/2112.11937)


  Deep reinforcement learning is widely used to train autonomous cars in a
simulated environment. Still, autonomous cars are well known for being
vulnerable when exposed to adversarial attacks. This raises the question of
whether we can train the adversary as a driving agent for finding failure
scenarios in autonomous cars, and then retrain autonomous cars with new
adversarial inputs to improve their robustness. In this work, we first train
and compare adversarial car policy on two custom reward functions to test the
driving control decision of autonomous cars in a multi-agent setting. Second,
we verify that adversarial examples can be used not only for finding unwanted
autonomous driving behavior, but also for helping autonomous driving cars in
improving their deep reinforcement learning policies. By using a high fidelity
urban driving simulation environment and vision-based driving agents, we
demonstrate that the autonomous cars retrained using the adversary player
noticeably increase the performance of their driving policies in terms of
reducing collision and offroad steering errors.

    

### [[2112.11939] Faster Convergence in Multi-Objective Optimization Algorithms Based on Decomposition](http://arxiv.org/abs/2112.11939)


  The Resource Allocation approach (RA) improves the performance of MOEA/D by
maintaining a big population and updating few solutions each generation.
However, most of the studies on RA generally focused on the properties of
different Resource Allocation metrics. Thus, it is still uncertain what the
main factors are that lead to increments in performance of MOEA/D with RA. This
study investigates the effects of MOEA/D with the Partial Update Strategy in an
extensive set of MOPs to generate insights into correspondences of MOEA/D with
the Partial Update and MOEA/D with small population size and big population
size. Our work undertakes an in-depth analysis of the populational dynamics
behaviour considering their final approximation Pareto sets, anytime
hypervolume performance, attained regions and number of unique non-dominated
solutions. Our results indicate that MOEA/D with Partial Update progresses with
the search as fast as MOEA/D with small population size and explores the search
space as MOEA/D with big population size. MOEA/D with Partial Update can
mitigate common problems related to population size choice with better
convergence speed in most MOPs, as shown by the results of hypervolume and
number of unique non-dominated solutions, the anytime performance and Empirical
Attainment Function indicates.

    

### [[2112.12028] VoiceMoji: A Novel On-Device Pipeline for Seamless Emoji Insertion in Dictation](http://arxiv.org/abs/2112.12028)


  Most of the speech recognition systems recover only words in the speech and
fail to capture emotions. Users have to manually add emoji(s) in text for
adding tone and making communication fun. Though there is much work done on
punctuation addition on transcribed speech, the area of emotion addition is
untouched. In this paper, we propose a novel on-device pipeline to enrich the
voice input experience. It involves, given a blob of transcribed text,
intelligently processing and identifying structure where emoji insertion makes
sense. Moreover, it includes semantic text analysis to predict emoji for each
of the sub-parts for which we propose a novel architecture Attention-based Char
Aware (ACA) LSTM which handles Out-Of-Vocabulary (OOV) words as well. All these
tasks are executed completely on-device and hence can aid on-device dictation
systems. To the best of our knowledge, this is the first work that shows how to
add emoji(s) in the transcribed text. We demonstrate that our components
achieve comparable results to previous neural approaches for punctuation
addition and emoji prediction with 80% fewer parameters. Overall, our proposed
model has a very small memory footprint of a mere 4MB to suit on-device
deployment.

    

### [[2112.12071] Activity-based and agent-based Transport model of Melbourne (AToM): an open multi-modal transport simulation model for Greater Melbourne](http://arxiv.org/abs/2112.12071)


  Agent-based and activity-based models for simulating transportation systems
have attracted significant attention in recent years. Few studies, however,
include a detailed representation of active modes of transportation - such as
walking and cycling - at a city-wide level, where dominating motorised modes
are often of primary concern. This paper presents an open workflow for creating
a multi-modal agent-based and activity-based transport simulation model,
focusing on Greater Melbourne, and including the process of mode choice
calibration for the four main travel modes of driving, public transport,
cycling and walking. The synthetic population generated and used as an input
for the simulation model represented Melbourne's population based on Census
2016, with daily activities and trips based on the Victoria's 2016-18 travel
survey data. The road network used in the simulation model includes all public
roads accessible via the included travel modes. We compared the output of the
simulation model with observations from the real world in terms of mode share,
road volume, travel time, and travel distance. Through these comparisons, we
showed that our model is suitable for studying mode choice and road usage
behaviour of travellers.

    

### [[2112.12072] Hierarchical Cross-Modality Semantic Correlation Learning Model for Multimodal Summarization](http://arxiv.org/abs/2112.12072)


  Multimodal summarization with multimodal output (MSMO) generates a summary
with both textual and visual content. Multimodal news report contains
heterogeneous contents, which makes MSMO nontrivial. Moreover, it is observed
that different modalities of data in the news report correlate hierarchically.
Traditional MSMO methods indistinguishably handle different modalities of data
by learning a representation for the whole data, which is not directly
adaptable to the heterogeneous contents and hierarchical correlation. In this
paper, we propose a hierarchical cross-modality semantic correlation learning
model (HCSCL) to learn the intra- and inter-modal correlation existing in the
multimodal data. HCSCL adopts a graph network to encode the intra-modal
correlation. Then, a hierarchical fusion framework is proposed to learn the
hierarchical correlation between text and images. Furthermore, we construct a
new dataset with relevant image annotation and image object label information
to provide the supervision information for the learning procedure. Extensive
experiments on the dataset show that HCSCL significantly outperforms the
baseline methods in automatic summarization metrics and fine-grained diversity
tests.

    

### [[2102.13490] Case Level Counterfactual Reasoning in Process Mining](http://arxiv.org/abs/2102.13490)


  Process mining is widely used to diagnose processes and uncover performance
and compliance problems. It is also possible to see relations between different
behavioral aspects, e.g., cases that deviate more at the beginning of the
process tend to get delayed in the later part of the process. However,
correlations do not necessarily reveal causalities. Moreover, standard process
mining diagnostics do not indicate how to improve the process. This is the
reason we advocate the use of structural equation models and counterfactual
reasoning. We use results from causal inference and adapt these to be able to
reason over event logs and process interventions. We have implemented the
approach as a ProM plug-in and have evaluated it on several data sets.

    

### [[2104.05703] Adversarial Open Domain Adaptation for Sketch-to-Photo Synthesis](http://arxiv.org/abs/2104.05703)


  In this paper, we explore open-domain sketch-to-photo translation, which aims
to synthesize a realistic photo from a freehand sketch with its class label,
even if the sketches of that class are missing in the training data. It is
challenging due to the lack of training supervision and the large geometric
distortion between the freehand sketch and photo domains. To synthesize the
absent freehand sketches from photos, we propose a framework that jointly
learns sketch-to-photo and photo-to-sketch generation. However, the generator
trained from fake sketches might lead to unsatisfying results when dealing with
sketches of missing classes, due to the domain gap between synthesized sketches
and real ones. To alleviate this issue, we further propose a simple yet
effective open-domain sampling and optimization strategy to "fool" the
generator into treating fake sketches as real ones. Our method takes advantage
of the learned sketch-to-photo and photo-to-sketch mapping of in-domain data
and generalizes it to the open-domain classes. We validate our method on the
Scribble and SketchyCOCO datasets. Compared with the recent competing methods,
our approach shows impressive results in synthesizing realistic color, texture,
and maintaining the geometric composition for various categories of open-domain
sketches. Our code is available at this https URL


### [[2106.08785] SEOVER: Sentence-level Emotion Orientation Vector based Conversation Emotion Recognition Model](http://arxiv.org/abs/2106.08785)


  For the task of conversation emotion recognition, recent works focus on
speaker relationship modeling but ignore the role of utterance's emotional
this http URL this paper, we propose a new expression paradigm of sentence-level
emotion orientation vector to model the potential correlation of emotions
between sentence vectors. Based on it, we design an emotion recognition model,
which extracts the sentence-level emotion orientation vectors from the language
model and jointly learns from the dialogue sentiment analysis model and
extracted sentence-level emotion orientation vectors to identify the speaker's
emotional orientation during the conversation. We conduct experiments on two
benchmark datasets and compare them with the five baseline models.The
experimental results show that our model has better performance on all data
sets.

    

### [[2110.12678] Nearly Tight Convergence Bounds for Semi-discrete Entropic Optimal Transport](http://arxiv.org/abs/2110.12678)


  We derive nearly tight and non-asymptotic convergence bounds for solutions of
entropic semi-discrete optimal transport. These bounds quantify the stability
of the dual solutions of the regularized problem (sometimes called Sinkhorn
potentials) w.r.t. the regularization parameter, for which we ensure a better
than Lipschitz dependence. Such facts may be a first step towards a
mathematical justification of annealing or $\varepsilon$-scaling heuristics for
the numerical resolution of regularized semi-discrete optimal transport. Our
results also entail a non-asymptotic and tight expansion of the difference
between the entropic and the unregularized costs.

    

### [[2112.06780] Explanation Container in Case-Based Biomedical Question-Answering](http://arxiv.org/abs/2112.06780)


  The National Center for Advancing Translational Sciences(NCATS) Biomedical
Data Translator (Translator) aims to attenuate problems faced by translational
scientists. Translator is a multi-agent architecture consisting of six
autonomous relay agents (ARAs) and eight knowledge providers (KPs). In this
paper, we present the design of the Explanatory Agent (xARA), a case-based ARA
that answers biomedical queries by accessing multiple KPs, ranking results, and
explaining the ranking of results. The Explanatory Agent is designed with five
knowledge containers that include the four original knowledge containers and
one additional container for explanation - the Explanation Container. The
Explanation Container is case-based and designed with its own knowledge
containers.

    

### [[2112.11767] Supporting RISC-V Performance Counters through Performance analysis tools for Linux (Perf)](http://arxiv.org/abs/2112.11767)


  Increased attention to RISC-V in Cloud, Data Center, Automotive and
Networking applications, has been fueling the move of RISC-V to the
high-performance computing scenario. However, lack of powerful performance
monitoring tools will result in poorly optimized applications and,
consequently, a limited computing performance. While the RISC-V ISA already
defines a hardware performance monitor (HPM), current software gives limited
support for monitoring performance. In this paper we introduce extensions and
modifications to the Performance analysis tools for Linux(perf/perf_events),
Linux kernel, and OpenSBI, aiming to achieve full support for the RISC-V
performance monitoring specification. Preliminary testing and evaluation was
carried out in Linux 5.7 running on a FPGA booted CVA6 CPU, formerly named
Ariane, showing a monitoring overhead of 0.283%.

    

### [[2112.11546] Model Sketching by Abstraction Refinement for Lifted Model Checking (Extended Version)](http://arxiv.org/abs/2112.11546)


  In this work, we show how the use of verification and analysis techniques for
model families (software product lines) with numerical features provides an
interesting technique to synthesize complete models from sketches (i.e.\
partial models with holes). In particular, we present an approach for
synthesizing Promela model sketches using variability-specific abstraction
refinement for lifted (family-based) model checking.

    

### [[2112.11745] Security Risks of Porting C Programs to WebAssembly](http://arxiv.org/abs/2112.11745)


  WebAssembly is a compilation target for cross-platform applications that is
increasingly being used. In this paper, we investigate whether one can
transparently cross-compile C programs to WebAssembly, and if not, what impact
porting can have on their security. We compile 17,802 programs that exhibit
common vulnerabilities to 64-bit x86 and to WebAssembly binaries, and we
observe that the execution of 4,911 binaries produces different results across
these platforms. Through manual inspection, we identify three classes of root
causes for such differences: the use of a different standard library
implementation, the lack of security measures in WebAssembly, and the different
semantics of the execution environments. We describe our observations and
discuss the ones that are critical from a security point of view and need most
attention from developers. We conclude that compiling an existing C program to
WebAssembly for cross-platform distribution may require source code
adaptations; otherwise, the security of the WebAssembly application may be at
risk.

    

### [[2112.11988] Reducing Programs to Objects](http://arxiv.org/abs/2112.11988)


  C++, Java, C#, Python, Ruby, JavaScript are the most powerful object-oriented
programming languages, if language power would be defined as the number of
features available for a programmer. EO, on the other hand, is an
object-oriented programming language with a reduced set of features: it has
nothing by objects and mechanisms of their composition and decoration. We are
trying to answer the following research question: "Which known features are
possible to implement using only objects?"

    