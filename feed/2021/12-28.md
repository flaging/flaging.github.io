
## 2021-12-28

### [<title>Variable importance using xgb.ggplot.shap.summary - XGBoost</title>](https://discuss.xgboost.ai/t/variable-importance-using-xgb-ggplot-shap-summary/2618/1)

### [[2112.12793] A Multi-View Framework for BGP Anomaly Detection via Graph Attention Network](http://arxiv.org/abs/2112.12793)


  As the default protocol for exchanging routing reachability information on
the Internet, the abnormal behavior in traffic of Border Gateway Protocols
(BGP) is closely related to Internet anomaly events. The BGP anomalous
detection model ensures stable routing services on the Internet through its
real-time monitoring and alerting capabilities. Previous studies either focused
on the feature selection problem or the memory characteristic in data, while
ignoring the relationship between features and the precise time correlation in
feature (whether it's long or short term dependence). In this paper, we propose
a multi-view model for capturing anomalous behaviors from BGP update traffic,
in which Seasonal and Trend decomposition using Loess (STL) method is used to
reduce the noise in the original time-series data, and Graph Attention Network
(GAT) is used to discover feature relationships and time correlations in
feature, respectively. Our results outperform the state-of-the-art methods at
the anomaly detection task, with the average F1 score up to 96.3% and 93.2% on
the balanced and imbalanced datasets respectively. Meanwhile, our model can be
extended to classify multiple anomalous and to detect unknown events.

    

### [[2112.12883] Backhaul-Aware Drone Base Station Placement and Resource Management for FSO based Drone Assisted Mobile Networks](http://arxiv.org/abs/2112.12883)


  In drone assisted mobile networks, drones mounted small cell base stations
(DBSs) are responsively and flexibly deployed over any Places of Interest
(PoI), such as sporadic hotspots and disaster-struck areas, where the existing
mobile network infrastructure is unable to provide wireless coverage. Here, a
DBS is a relay node to relay traffic between a nearby macro base station (MBS)
and the users. In addition, Free-space optics (FSO) is applied as the
backhauling solution to significantly increase the capacity of the backhaul
link between an MBS and a DBS in a drone assisted mobile network. Most of the
existing DBS placement solutions assume the FSO based backhaul link provides
sufficient link capacity, which may not be true, especially when a DBS is
placed far away from an MBS (e.g., > 10 km in disaster-struck areas) or in a
bad weather condition. In this paper, we formulate a problem to jointly
optimize bandwidth allocation and DBS placement by considering the FSO based
backhaul link capacity constraint. A Backhaul awaRe bandwidth allOcAtion and
DBS placement (BROAD) algorithm is designed to efficiently solve the problem,
and the performance of the algorithm is demonstrated via extensive simulations.

    

### [[2112.12985] DeepGANTT: A Scalable Deep Learning Scheduler for Backscatter Networks](http://arxiv.org/abs/2112.12985)


  Recent backscatter communication techniques enable ultra low power wireless
devices that operate without batteries while interoperating directly with
unmodified commodity wireless devices. Commodity devices cooperate in providing
the unmodulated carrier that the battery-free nodes need to communicate while
collecting energy from their environment to perform sensing, computation, and
communication tasks. The optimal provision of the unmodulated carrier limits
the size of the network because it is an NP-hard combinatorial optimization
problem. Consequently, previous works either ignore carrier optimization
altogether or resort to suboptimal heuristics, wasting valuable energy and
spectral resources. We present DeepGANTT, a deep learning scheduler for
battery-free devices interoperating with wireless commodity ones. DeepGANTT
leverages graph neural networks to overcome variable input and output size
challenges inherent to this problem. We train our deep learning scheduler with
optimal schedules of relatively small size obtained from a constraint
optimization solver. DeepGANTT not only outperforms a carefully crafted
heuristic solution but also performs within ~3% of the optimal scheduler on
trained problem sizes. Finally, DeepGANTT generalizes to problems more than
four times larger than the maximum used for training, therefore breaking the
scalability limitations of the optimal scheduler and paving the way for more
efficient backscatter networks.

    

### [[2112.13093] Multi-Provider NFV Network Service Delegation via Average Reward Reinforcement Learning](http://arxiv.org/abs/2112.13093)


  In multi-provider 5G/6G networks, service delegation enables administrative
domains to federate in provisioning NFV network services. Admission control is
fundamental in selecting the appropriate deployment domain to maximize average
profit without prior knowledge of service requests' statistical distributions.
This paper analyzes a general federation contract model for service delegation
in various ways. First, under the assumption of known system dynamics, we
obtain the theoretically optimal performance bound by formulating the admission
control problem as an infinite-horizon Markov decision process (MDP) and
solving it through dynamic programming. Second, we apply reinforcement learning
to practically tackle the problem when the arrival and departure rates are not
known. As Q-learning maximizes the discounted rewards, we prove it is not an
efficient solution due to its sensitivity to the discount factor. Then, we
propose the average reward reinforcement learning approach (R-Learning) to find
the policy that directly maximizes the average profit. Finally, we evaluate
different solutions through extensive simulations and experimentally using the
5Growth platform. Results confirm that the proposed R-Learning solution always
outperforms Q-Learning and the greedy policies. Furthermore, while there is at
most 9% optimality gap in the ideal simulation environment, it competes with
the MDP solution in the experimental assessment.

    

### [[2112.13161] BEAT: Blockchain-Enabled Accountable Infrastructure Sharing in 6G and Beyond](http://arxiv.org/abs/2112.13161)


  It is widely expected that future networks of 6G and beyond will deliver on
the unachieved goals set by 5G. Technologies such as Internet of Skills and
Industry 4.0 will become stable and viable, as a direct consequence of networks
that offer sustained and reliable mobile performance levels.
The primary challenges for future technologies are not just low-latency and
high-bandwidth. The more critical problem Mobile Service Providers (MSPs) will
face will be in balancing the inflated demands of network connections and
customers' trust in the network service, that is, being able to interconnect
billions of unique devices while adhering to the agreed terms of Service Level
Agreements (SLAs). To meet these targets, it is self-evident that MSPs cannot
operate in a solitary environment. They must enable cooperation among
themselves in a manner that ensures trust, both between themselves as well as
with customers.
In this study, we present the BEAT (Blockchain-Enabled Accountable and
Transparent) Infrastructure Sharing architecture. BEAT exploits the inherent
properties of permissioned type of distributed ledger technology (i.e.,
permissioned distributed ledgers) to deliver on accountability and transparency
metrics whenever infrastructure needs to be shared between providers. We also
propose a lightweight method that enables device-level accountability. BEAT has
been designed to be deployable directly as only minor software upgrades to
network devices such as routers. Our simulations on a resource-limited device
show that BEAT adds only a few seconds of overhead processing time -- with the
latest state-of-the-art network devices, we can reasonably anticipate much
lower overheads.

    

### [[2112.13170] On the Feasibility of 4.9 GHz Public Safety Band as Spectrum Option for Internet of Vehicles](http://arxiv.org/abs/2112.13170)


  There is an unprecedented impetus on the advancement of internet of vehicles
(IoV). The vehicle-to-everything (V2X) communication is well acknowledged as
the key technology in constitution of the IoV. Nevertheless, the spectrum for
V2X communication is undergoing a massive change in the United States: a
majority of the bandwidth has been reallocated to Wi-Fi leaving even less than
a half of the bandwidth for V2X. This motivates investigation of other
candidate spectrum bands for operation of V2X communication as an urgent effort
to guarantee efficient operations of IoV. To this line, this paper studies the
feasibility of sharing the 4.9 GHz public safety band between the incumbent
systems and V2X users.

    

### [[2112.13181] DeepMTL Pro: Deep Learning Based MultipleTransmitter Localization and Power Estimation](http://arxiv.org/abs/2112.13181)


  In this paper, we address the problem of Multiple Transmitter Localization
(MTL). MTL is to determine the locations of potential multiple transmitters in
a field, based on readings from a distributed set of sensors. In contrast to
the widely studied single transmitter localization problem, the MTL problem has
only been studied recently in a few works. MTL is of great significance in many
applications wherein intruders may be present. E.g., in shared spectrum
systems, detection of unauthorized transmitters and estimating their power are
imperative to efficient utilization of the shared spectrum.
In this paper, we present DeepMTL, a novel deep-learning approach to address
the MTL problem. In particular, we frame MTL as a sequence of two steps, each
of which is a computer vision problem: image-to-image translation and object
detection. The first step of image-to-image translation essentially maps an
input image representing sensor readings to an image representing the
distribution of transmitter locations, and the second object detection step
derives precise locations of transmitters from the image of transmitter
distributions. For the first step, we design our learning model Sen2Peak, while
for the second step, we customize a state-of-the-art object detection model
Yolo-cust. Using DeepMTL as a building block, we also develop techniques to
estimate transmit power of the localized transmitters. We demonstrate the
effectiveness of our approach via extensive large-scale simulations, and show
that our approach outperforms the previous approaches significantly (by 50% or
more) in accuracy performance metrics, and incurs an order of magnitude less
latency compared to other prior works. We also evaluate our techniques over a
small-scale area with real testbed data.

    

### [[2112.13222] Edge Robotics: Edge-Computing-Accelerated Multi-Robot Simultaneous Localization and Mapping](http://arxiv.org/abs/2112.13222)


  With the wide penetration of smart robots in multifarious fields,
Simultaneous Localization and Mapping (SLAM) technique in robotics has
attracted growing attention in the community. Yet collaborating SLAM over
multiple robots still remains challenging due to performance contradiction
between the intensive graphics computation of SLAM and the limited computing
capability of robots. While traditional solutions resort to the powerful cloud
servers acting as an external computation provider, we show by real-world
measurements that the significant communication overhead in data offloading
prevents its practicability to real deployment. To tackle these challenges,
this paper promotes the emerging edge computing paradigm into multi-robot SLAM
and proposes RecSLAM, a multi-robot laser SLAM system that focuses on
accelerating map construction process under the robot-edge-cloud architecture.
In contrast to conventional multi-robot SLAM that generates graphic maps on
robots and completely merges them on the cloud, RecSLAM develops a hierarchical
map fusion technique that directs robots' raw data to edge servers for
real-time fusion and then sends to the cloud for global merging. To optimize
the overall pipeline, an efficient multi-robot SLAM collaborative processing
framework is introduced to adaptively optimize robot-to-edge offloading
tailored to heterogeneous edge resource conditions, meanwhile ensuring the
workload balancing among the edge servers. Extensive evaluations show RecSLAM
can achieve up to 39% processing latency reduction over the state-of-the-art.
Besides, a proof-of-concept prototype is developed and deployed in real scenes
to demonstrate its effectiveness.

    

### [[2112.13603] Over-the-Air Multi-Task Federated Learning Over MIMO Interference Channel](http://arxiv.org/abs/2112.13603)


  With the explosive growth of data and wireless devices, federated learning
(FL) has emerged as a promising technology for large-scale intelligent systems.
Utilizing the analog superposition of electromagnetic waves, over-the-air
computation is an appealing approach to reduce the burden of communication in
the FL model aggregation. However, with the urgent demand for intelligent
systems, the training of multiple tasks with over-the-air computation further
aggravates the scarcity of communication resources. This issue can be
alleviated to some extent by training multiple tasks simultaneously with shared
communication resources, but the latter inevitably brings about the problem of
inter-task interference. In this paper, we study over-the-air multi-task FL
(OA-MTFL) over the multiple-input multiple-output (MIMO) interference channel.
We propose a novel model aggregation method for the alignment of local
gradients for different devices, which alleviates the straggler problem that
exists widely in over-the-air computation due to the channel heterogeneity. We
establish a unified communication-computation analysis framework for the
proposed OA-MTFL scheme by considering the spatial correlation between devices,
and formulate an optimization problem of designing transceiver beamforming and
device selection. We develop an algorithm by using alternating optimization
(AO) and fractional programming (FP) to solve this problem, which effectively
relieves the impact of inter-task interference on the FL learning performance.
We show that due to the use of the new model aggregation method, device
selection is no longer essential to our scheme, thereby avoiding the heavy
computational burden caused by implementing device selection. The numerical
results demonstrate the correctness of the analysis and the outstanding
performance of the proposed scheme.

    

### [[2104.01946] Machine Learning Applications in the Routing in Computer Networks](http://arxiv.org/abs/2104.01946)


  Development of routing algorithms is of clear importance as the volume of
Internet traffic continues to increase. In this survey, there is much research
into how Machine Learning techniques can be employed to improve the performance
and scalability of routing algorithms. We surveyed both centralized and
decentralized ML routing architectures and using a variety of ML techniques
broadly divided into supervised learning and reinforcement learning. Many of
the papers showed promise in their ability to optimize some aspect of network
routing. We also implemented two routing protocols within 14 surveyed routing
algorithms and verified the efficacy of their results. While the results of
most of the papers showed promise, many of them are based on simulations of
potentially unrealistic network configurations. To provide further efficacy to
the results, more real-world results are necessary.

    

### [[2112.12792] Understanding and Measuring Robustness of Multimodal Learning](http://arxiv.org/abs/2112.12792)


  The modern digital world is increasingly becoming multimodal. Although
multimodal learning has recently revolutionized the state-of-the-art
performance in multimodal tasks, relatively little is known about the
robustness of multimodal learning in an adversarial setting. In this paper, we
introduce a comprehensive measurement of the adversarial robustness of
multimodal learning by focusing on the fusion of input modalities in multimodal
models, via a framework called MUROAN (MUltimodal RObustness ANalyzer). We
first present a unified view of multimodal models in MUROAN and identify the
fusion mechanism of multimodal models as a key vulnerability. We then introduce
a new type of multimodal adversarial attacks called decoupling attack in MUROAN
that aims to compromise multimodal models by decoupling their fused modalities.
We leverage the decoupling attack of MUROAN to measure several state-of-the-art
multimodal models and find that the multimodal fusion mechanism in all these
models is vulnerable to decoupling attacks. We especially demonstrate that, in
the worst case, the decoupling attack of MUROAN achieves an attack success rate
of 100% by decoupling just 1.16% of the input space. Finally, we show that
traditional adversarial training is insufficient to improve the robustness of
multimodal models with respect to decoupling attacks. We hope our findings
encourage researchers to pursue improving the robustness of multimodal
learning.

    

### [[2112.12809] Bi-Directional Recurrent Neural Ordinary Differential Equations for Social Media Text Classification](http://arxiv.org/abs/2112.12809)


  Classification of posts in social media such as Twitter is difficult due to
the noisy and short nature of texts. Sequence classification models based on
recurrent neural networks (RNN) are popular for classifying posts that are
sequential in nature. RNNs assume the hidden representation dynamics to evolve
in a discrete manner and do not consider the exact time of the posting. In this
work, we propose to use recurrent neural ordinary differential equations
(RNODE) for social media post classification which consider the time of posting
and allow the computation of hidden representation to evolve in a
time-sensitive continuous manner. In addition, we propose a novel model,
Bi-directional RNODE (Bi-RNODE), which can consider the information flow in
both the forward and backward directions of posting times to predict the post
label. Our experiments demonstrate that RNODE and Bi-RNODE are effective for
the problem of stance classification of rumours in social media.

    

### [[2112.12819] Graph Few-shot Class-incremental Learning](http://arxiv.org/abs/2112.12819)


  The ability to incrementally learn new classes is vital to all real-world
artificial intelligence systems. A large portion of high-impact applications
like social media, recommendation systems, E-commerce platforms, etc. can be
represented by graph models. In this paper, we investigate the challenging yet
practical problem, Graph Few-shot Class-incremental (Graph FCL) problem, where
the graph model is tasked to classify both newly encountered classes and
previously learned classes. Towards that purpose, we put forward a Graph Pseudo
Incremental Learning paradigm by sampling tasks recurrently from the base
classes, so as to produce an arbitrary number of training episodes for our
model to practice the incremental learning skill. Furthermore, we design a
Hierarchical-Attention-based Graph Meta-learning framework, HAG-Meta. We
present a task-sensitive regularizer calculated from task-level attention and
node class prototypes to mitigate overfitting onto either novel or base
classes. To employ the topological knowledge, we add a node-level attention
module to adjust the prototype representation. Our model not only achieves
greater stability of old knowledge consolidation, but also acquires
advantageous adaptability to new knowledge with very limited data samples.
Extensive experiments on three real-world datasets, including Amazon-clothing,
Reddit, and DBLP, show that our framework demonstrates remarkable advantages in
comparison with the baseline and other related state-of-the-art methods.

    

### [[2112.12839] Faster Deep Ensemble Averaging for Quantification of DNA Damage from Comet Assay Images With Uncertainty Estimates](http://arxiv.org/abs/2112.12839)


  Several neurodegenerative diseases involve the accumulation of cellular DNA
damage. Comet assays are a popular way of estimating the extent of DNA damage.
Current literature on the use of deep learning to quantify DNA damage presents
an empirical approach to hyper-parameter optimization and does not include
uncertainty estimates. Deep ensemble averaging is a standard approach to
estimating uncertainty but it requires several iterations of network training,
which makes it time-consuming. Here we present an approach to quantify the
extent of DNA damage that combines deep learning with a rigorous and
comprehensive method to optimize the hyper-parameters with the help of
statistical tests. We also use an architecture that allows for a faster
computation of deep ensemble averaging and performs statistical tests
applicable to networks using transfer learning. We applied our approach to a
comet assay dataset with more than 1300 images and achieved an $R^2$ of 0.84,
where the output included the confidence interval for each prediction. The
proposed architecture is an improvement over the current approaches since it
speeds up the uncertainty estimation by 30X while being statistically more
rigorous.

    

### [[2112.12845] Reinforced Meta-path Selection for Recommendation on Heterogeneous Information Networks](http://arxiv.org/abs/2112.12845)


  Heterogeneous Information Networks (HINs) capture complex relations among
entities of various kinds and have been used extensively to improve the
effectiveness of various data mining tasks, such as in recommender systems.
Many existing HIN-based recommendation algorithms utilize hand-crafted
meta-paths to extract semantic information from the networks. These algorithms
rely on extensive domain knowledge with which the best set of meta-paths can be
selected. For applications where the HINs are highly complex with numerous node
and link types, the approach of hand-crafting a meta-path set is too tedious
and error-prone. To tackle this problem, we propose the Reinforcement
learning-based Meta-path Selection (RMS) framework to select effective
meta-paths and to incorporate them into existing meta-path-based recommenders.
To identify high-quality meta-paths, RMS trains a reinforcement learning (RL)
based policy network(agent), which gets rewards from the performance on the
downstream recommendation tasks. We design a HIN-based recommendation model,
HRec, that effectively uses the meta-path information. We further integrate
HRec with RMS and derive our recommendation solution, RMS-HRec, that
automatically utilizes the effective meta-paths. Experiments on real datasets
show that our algorithm can significantly improve the performance of
recommendation models by capturing important meta-paths automatically.

    

### [[2112.12855] SoK: Privacy-preserving Deep Learning with Homomorphic Encryption](http://arxiv.org/abs/2112.12855)


  Outsourced computation for neural networks allows users access to state of
the art models without needing to invest in specialized hardware and know-how.
The problem is that the users lose control over potentially privacy sensitive
data. With homomorphic encryption (HE) computation can be performed on
encrypted data without revealing its content. In this systematization of
knowledge, we take an in-depth look at approaches that combine neural networks
with HE for privacy preservation. We categorize the changes to neural network
models and architectures to make them computable over HE and how these changes
impact performance. We find numerous challenges to HE based privacy-preserving
deep learning such as computational overhead, usability, and limitations posed
by the encryption schemes.

    

### [[2112.12872] Sparsified Secure Aggregation for Privacy-Preserving Federated Learning](http://arxiv.org/abs/2112.12872)


  Secure aggregation is a popular protocol in privacy-preserving federated
learning, which allows model aggregation without revealing the individual
models in the clear. On the other hand, conventional secure aggregation
protocols incur a significant communication overhead, which can become a major
bottleneck in real-world bandwidth-limited applications. Towards addressing
this challenge, in this work we propose a lightweight gradient sparsification
framework for secure aggregation, in which the server learns the aggregate of
the sparsified local model updates from a large number of users, but without
learning the individual parameters. Our theoretical analysis demonstrates that
the proposed framework can significantly reduce the communication overhead of
secure aggregation while ensuring comparable computational complexity. We
further identify a trade-off between privacy and communication efficiency due
to sparsification. Our experiments demonstrate that our framework reduces the
communication overhead by up to 7.8x, while also speeding up the wall clock
training time by 1.13x, when compared to conventional secure aggregation
benchmarks.

    

### [[2112.12901] A machine learning analysis of the relationship between some underlying medical conditions and COVID-19 susceptibility](http://arxiv.org/abs/2112.12901)


  For the past couple years, the Coronavirus, commonly known as COVID-19, has
significantly affected the daily lives of all citizens residing in the United
States by imposing several, fatal health risks that cannot go unnoticed. In
response to the growing fear and danger COVID-19 inflicts upon societies in the
USA, several vaccines and boosters have been created as a permanent remedy for
individuals to take advantage of. In this paper, we investigate the
relationship between the COVID-19 vaccines and boosters and the total case
count for the Coronavirus across multiple states in the USA. Additionally, this
paper discusses the relationship between several, selected underlying health
conditions with COVID-19. To discuss these relationships effectively, this
paper will utilize statistical tests and machine learning methods for analysis
and discussion purposes. Furthermore, this paper reflects upon conclusions made
about the relationship between educational attainment, race, and COVID-19 and
the possible connections that can be established with underlying health
conditions, vaccination rates, and COVID-19 total case and death counts.

    

### [[2112.12909] Optimal Variable Clustering for High-Dimensional Matrix Valued Data](http://arxiv.org/abs/2112.12909)


  Matrix valued data has become increasingly prevalent in many applications.
Most of the existing clustering methods for this type of data are tailored to
the mean model and do not account for the dependence structure of the features,
which can be very informative, especially in high-dimensional settings. To
extract the information from the dependence structure for clustering, we
propose a new latent variable model for the features arranged in matrix form,
with some unknown membership matrices representing the clusters for the rows
and columns. Under this model, we further propose a class of hierarchical
clustering algorithms using the difference of a weighted covariance matrix as
the dissimilarity measure. Theoretically, we show that under mild conditions,
our algorithm attains clustering consistency in the high-dimensional setting.
While this consistency result holds for our algorithm with a broad class of
weighted covariance matrices, the conditions for this result depend on the
choice of the weight. To investigate how the weight affects the theoretical
performance of our algorithm, we establish the minimax lower bound for
clustering under our latent variable model. Given these results, we identify
the optimal weight in the sense that using this weight guarantees our algorithm
to be minimax rate-optimal in terms of the magnitude of some cluster separation
metric. The practical implementation of our algorithm with the optimal weight
is also discussed. Finally, we conduct simulation studies to evaluate the
finite sample performance of our algorithm and apply the method to a genomic
dataset.

    

### [[2112.12912] TSAX is Trending](http://arxiv.org/abs/2112.12912)


  Time series mining is an important branch of data mining, as time series data
is ubiquitous and has many applications in several domains. The main task in
time series mining is classification. Time series representation methods play
an important role in time series classification and other time series mining
tasks. One of the most popular representation methods of time series data is
the Symbolic Aggregate approXimation (SAX). The secret behind its popularity is
its simplicity and efficiency. SAX has however one major drawback, which is its
inability to represent trend information. Several methods have been proposed to
enable SAX to capture trend information, but this comes at the expense of
complex processing, preprocessing, or post-processing procedures. In this paper
we present a new modification of SAX that we call Trending SAX (TSAX), which
only adds minimal complexity to SAX, but substantially improves its performance
in time series classification. This is validated experimentally on 50 datasets.
The results show the superior performance of our method, as it gives a smaller
classification error on 39 datasets compared with SAX.

    

### [[2112.12913] Spoiler in a Textstack: How Much Can Transformers Help?](http://arxiv.org/abs/2112.12913)


  This paper presents our research regarding spoiler detection in reviews. In
this use case, we describe the method of fine-tuning and organizing the
available text-based model tasks with the latest deep learning achievements and
techniques to interpret the models' results.
Until now, spoiler research has been rarely described in the literature. We
tested the transfer learning approach and different latest transformer
architectures on two open datasets with annotated spoilers (ROC AUC above 81\%
on TV Tropes Movies dataset, and Goodreads dataset above 88\%). We also
collected data and assembled a new dataset with fine-grained annotations. To
that end, we employed interpretability techniques and measures to assess the
models' reliability and explain their results.

    

### [[2112.12933] Constrained tensor factorization for computational phenotyping and mortality prediction in patients with cancer](http://arxiv.org/abs/2112.12933)


  Background: The increasing adoption of electronic health records (EHR) across
the US has created troves of computable data, to which machine learning methods
have been applied to extract useful insights. EHR data, represented as a
three-dimensional analogue of a matrix (tensor), is decomposed into
two-dimensional factors that can be interpreted as computational phenotypes.
Methods: We apply constrained tensor factorization to derive computational
phenotypes and predict mortality in cohorts of patients with breast, prostate,
colorectal, or lung cancer in the Northwestern Medicine Enterprise Data
Warehouse from 2000 to 2015. In our experiments, we examined using a supervised
term in the factorization algorithm, filtering tensor co-occurrences by medical
indication, and incorporating additional social determinants of health (SDOH)
covariates in the factorization process. We evaluated the resulting
computational phenotypes qualitatively and by assessing their ability to
predict five-year mortality using the area under the curve (AUC) statistic.
Results: Filtering by medical indication led to more concise and interpretable
phenotypes. Mortality prediction performance (AUC) varied under the different
experimental conditions and by cancer type (breast: 0.623 - 0.694, prostate:
0.603 - 0.750, colorectal: 0.523 - 0.641, and lung: 0.517 - 0.623). Generally,
prediction performance improved with the use of a supervised term and the
incorporation of SDOH covariates. Conclusion: Constrained tensor factorization,
applied to sparse EHR data of patients with cancer, can discover computational
phenotypes predictive of five-year mortality. The incorporation of SDOH
variables into the factorization algorithm is an easy-to-implement and
effective way to improve prediction performance.

    

### [[2112.12938] Counterfactual Memorization in Neural Language Models](http://arxiv.org/abs/2112.12938)


  Modern neural language models widely used in tasks across NLP risk memorizing
sensitive information from their training data. As models continue to scale up
in parameters, training data, and compute, understanding memorization in
language models is both important from a learning-theoretical point of view,
and is practically crucial in real world applications. An open question in
previous studies of memorization in language models is how to filter out
"common" memorization. In fact, most memorization criteria strongly correlate
with the number of occurrences in the training set, capturing "common"
memorization such as familiar phrases, public knowledge or templated texts. In
this paper, we provide a principled perspective inspired by a taxonomy of human
memory in Psychology. From this perspective, we formulate a notion of
counterfactual memorization, which characterizes how a model's predictions
change if a particular document is omitted during training. We identify and
study counterfactually-memorized training examples in standard text datasets.
We further estimate the influence of each training example on the validation
set and on generated texts, and show that this can provide direct evidence of
the source of memorization at test time.

    

### [[2112.12953] Supraventricular Tachycardia Detection and Classification Model of ECG signal Using Machine Learning](http://arxiv.org/abs/2112.12953)


  Investigation on the electrocardiogram (ECG) signals is an essential way to
diagnose heart disease since the ECG process is noninvasive and easy to use.
This work presents a supraventricular arrhythmia prediction model consisting of
a few stages, including filtering of noise, a unique collection of ECG
characteristics, and automated learning classifying model to classify distinct
types, depending on their severity. We de-trend and de-noise a signal to reduce
noise to better determine functionality before extractions are performed. After
that, we present one R-peak detection method and Q-S detection method as a part
of necessary feature extraction. Next parameters are computed that correspond
to these features. Using these characteristics, we have developed a
classification model based on machine learning that can successfully categorize
different types of supraventricular tachycardia. Our findings suggest that
decision-tree-based models are the most efficient machine learning models for
supraventricular tachycardia arrhythmia. Among all the machine learning models,
this model most efficiently lowers the crucial signal misclassification of
supraventricular tachycardia. Experimental results indicate satisfactory
improvements and demonstrate a superior efficiency of the proposed approach
with 97% accuracy.

    

### [[2112.12956] Machine Learning-based Efficient Ventricular Tachycardia Detection Model of ECG Signal](http://arxiv.org/abs/2112.12956)


  In primary diagnosis and analysis of heart defects, an ECG signal plays a
significant role. This paper presents a model for the prediction of ventricular
tachycardia arrhythmia using noise filtering, a unique set of ECG features, and
a machine learning-based classifier model. Before signal feature extraction, we
detrend and denoise the signal to eliminate the noise for detecting features
properly. After that necessary features have been extracted and necessary
parameters related to these features are measured. Using these parameters, we
prepared one efficient multiclass classifier model using a machine learning
approach that can classify different types of ventricular tachycardia
arrhythmias efficiently. Our results indicate that Logistic regression and
Decision tree-based models are the most efficient machine learning models for
detecting ventricular tachycardia arrhythmia. In order to diagnose heart
diseases and find care for a patient, an early, reliable diagnosis of different
types of arrhythmia is necessary. By implementing our proposed method, this
work deals with the problem of reducing the misclassification of the critical
signal related to ventricular tachycardia very efficiently. Experimental
findings demonstrate satisfactory enhancements and demonstrate high resilience
to the algorithm that we have proposed. With this assistance, doctors can
assess this type of arrhythmia of a patient early and take the right decision
at the proper time.

    

### [[2112.12961] Optimal Model Averaging of Support Vector Machines in Diverging Model Spaces](http://arxiv.org/abs/2112.12961)


  Support vector machine (SVM) is a powerful classification method that has
achieved great success in many fields. Since its performance can be seriously
impaired by redundant covariates, model selection techniques are widely used
for SVM with high dimensional covariates. As an alternative to model selection,
significant progress has been made in the area of model averaging in the past
decades. Yet no frequentist model averaging method was considered for SVM. This
work aims to fill the gap and to propose a frequentist model averaging
procedure for SVM which selects the optimal weight by cross validation. Even
when the number of covariates diverges at an exponential rate of the sample
size, we show asymptotic optimality of the proposed method in the sense that
the ratio of its hinge loss to the lowest possible loss converges to one. We
also derive the convergence rate which provides more insights to model
averaging. Compared to model selection methods of SVM which require a tedious
but critical task of tuning parameter selection, the model averaging method
avoids the task and shows promising performances in the empirical studies.

    

### [[2112.12965] Error-bounded Approximate Time Series Joins using Compact Dictionary Representations of Time Series](http://arxiv.org/abs/2112.12965)


  The matrix profile is an effective data mining tool that provides similarity
join functionality for time series data. Users of the matrix profile can either
join a time series with itself using intra-similarity join (i.e., self-join) or
join a time series with another time series using inter-similarity join. By
invoking either or both types of joins, the matrix profile can help users
discover both conserved and anomalous structures in the data. Since the
introduction of the matrix profile five years ago, multiple efforts have been
made to speed up the computation with approximate joins; however, the majority
of these efforts only focus on self-joins. In this work, we show that it is
possible to efficiently perform approximate inter-time series similarity joins
with error bounded guarantees by creating a compact "dictionary" representation
of time series. Using the dictionary representation instead of the original
time series, we are able to improve the throughput of an anomaly mining system
by at least 20X, with essentially no decrease in accuracy. As a side effect,
the dictionaries also summarize the time series in a semantically meaningful
way and can provide intuitive and actionable insights. We demonstrate the
utility of our dictionary-based inter-time series similarity joins on domains
as diverse as medicine and transportation.

    

### [[2112.12966] Machine learning for Earth System Science (ESS): A survey, status and future directions for South Asia](http://arxiv.org/abs/2112.12966)


  This survey focuses on the current problems in Earth systems science where
machine learning algorithms can be applied. It provides an overview of previous
work, ongoing work at the Ministry of Earth Sciences, Gov. of India, and future
applications of ML algorithms to some significant earth science problems. We
provide a comparison of previous work with this survey, a mind map of
multidimensional areas related to machine learning and a Gartner's hype cycle
for machine learning in Earth system science (ESS). We mainly focus on the
critical components in Earth Sciences, including atmospheric, Ocean,
Seismology, and biosphere, and cover AI/ML applications to statistical
downscaling and forecasting problems.

    

### [[2112.12970] SGTR: End-to-end Scene Graph Generation with Transformer](http://arxiv.org/abs/2112.12970)


  Scene Graph Generation (SGG) remains a challenging visual understanding task
due to its complex compositional property. Most previous works adopt a
bottom-up two-stage or a point-based one-stage approach, which often suffers
from overhead time complexity or sub-optimal design assumption. In this work,
we propose a novel SGG method to address the aforementioned issues, which
formulates the task as a bipartite graph construction problem. To solve the
problem, we develop a transformer-based end-to-end framework that first
generates the entity and predicate proposal set, followed by inferring directed
edges to form the relation triplets. In particular, we develop a new
entity-aware predicate representation based on a structural predicate generator
to leverage the compositional property of relationships. Moreover, we design a
graph assembling module to infer the connectivity of the bipartite scene graph
based on our entity-aware structure, enabling us to generate the scene graph in
an end-to-end manner. Extensive experimental results show that our design is
able to achieve the state-of-the-art or comparable performance on two
challenging benchmarks, surpassing most of the existing approaches and enjoying
higher efficiency in inference. We hope our model can serve as a strong
baseline for the Transformer-based scene graph generation.

    

### [[2112.12979] Integrating Physics-Based Modeling with Machine Learning for Lithium-Ion Batteries](http://arxiv.org/abs/2112.12979)


  Mathematical modeling of lithium-ion batteries (LiBs) is a primary challenge
in advanced battery management. This paper proposes two new frameworks to
integrate a physics-based model with machine learning to achieve high-precision
modeling for LiBs. The frameworks are characterized by informing the machine
learning model of the state information of the physical model, enabling a deep
integration between physics and machine learning. Based on the frameworks, a
series of hybrid models are constructed, through combining an electrochemical
model and an equivalent circuit model, respectively, with a feedforward neural
network. The hybrid models are relatively parsimonious in structure and can
provide considerable predictive accuracy under a broad range of C-rates, as
shown by extensive simulations and experiments. The study further expands to
conduct aging-aware hybrid modeling, leading to the design of a hybrid model
conscious of the state-of-health to make prediction. Experiments show that the
model has high predictive accuracy throughout a LiB's cycle life.

    

### [[2112.12980] Disentanglement by Cyclic Reconstruction](http://arxiv.org/abs/2112.12980)


  Deep neural networks have demonstrated their ability to automatically extract
meaningful features from data. However, in supervised learning, information
specific to the dataset used for training, but irrelevant to the task at hand,
may remain encoded in the extracted representations. This remaining information
introduces a domain-specific bias, weakening the generalization performance. In
this work, we propose splitting the information into a task-related
representation and its complementary context representation. We propose an
original method, combining adversarial feature predictors and cyclic
reconstruction, to disentangle these two representations in the single-domain
supervised case. We then adapt this method to the unsupervised domain
adaptation problem, consisting of training a model capable of performing on
both a source and a target domain. In particular, our method promotes
disentanglement in the target domain, despite the absence of training labels.
This enables the isolation of task-specific information from both domains and a
projection into a common representation. The task-specific representation
allows efficient transfer of knowledge acquired from the source domain to the
target domain. In the single-domain case, we demonstrate the quality of our
representations on information retrieval tasks and the generalization benefits
induced by sharpened task-specific representations. We then validate the
proposed method on several classical domain adaptation benchmarks and
illustrate the benefits of disentanglement for domain adaptation.

    

### [[2112.12986] Is Importance Weighting Incompatible with Interpolating Classifiers?](http://arxiv.org/abs/2112.12986)


  Importance weighting is a classic technique to handle distribution shifts.
However, prior work has presented strong empirical and theoretical evidence
demonstrating that importance weights can have little to no effect on
overparameterized neural networks. Is importance weighting truly incompatible
with the training of overparameterized neural networks? Our paper answers this
in the negative. We show that importance weighting fails not because of the
overparameterization, but instead, as a result of using exponentially-tailed
losses like the logistic or cross-entropy loss. As a remedy, we show that
polynomially-tailed losses restore the effects of importance reweighting in
correcting distribution shift in overparameterized models. We characterize the
behavior of gradient descent on importance weighted polynomially-tailed losses
with overparameterized linear models, and theoretically demonstrate the
advantage of using polynomially-tailed losses in a label shift setting.
Surprisingly, our theory shows that using weights that are obtained by
exponentiating the classical unbiased importance weights can improve
performance. Finally, we demonstrate the practical value of our analysis with
neural network experiments on a subpopulation shift and a label shift dataset.
When reweighted, our loss function can outperform reweighted cross-entropy by
as much as 9% in test accuracy. Our loss function also gives test accuracies
comparable to, or even exceeding, well-tuned state-of-the-art methods for
correcting distribution shifts.

    

### [[2112.12989] Domain-Aware Continual Zero-Shot Learning](http://arxiv.org/abs/2112.12989)


  We introduce Domain Aware Continual Zero-Shot Learning (DACZSL), the task of
visually recognizing images of unseen categories in unseen domains
sequentially. We created DACZSL on top of the DomainNet dataset by dividing it
into a sequence of tasks, where classes are incrementally provided on seen
domains during training and evaluation is conducted on unseen domains for both
seen and unseen classes. We also proposed a novel Domain-Invariant CZSL Network
(DIN), which outperforms state-of-the-art baseline models that we adapted to
DACZSL setting. We adopt a structure-based approach to alleviate forgetting
knowledge from previous tasks with a small per-task private network in addition
to a global shared network. To encourage the private network to capture the
domain and task-specific representation, we train our model with a novel
adversarial knowledge disentanglement setting to make our global network
task-invariant and domain-invariant over all the tasks. Our method also learns
a class-wise learnable prompt to obtain better class-level text representation,
which is used to represent side information to enable zero-shot prediction of
future unseen classes. Our code and benchmarks will be made publicly available.

    

### [[2112.12994] Toeplitz Least Squares Problems, Fast Algorithms and Big Data](http://arxiv.org/abs/2112.12994)


  In time series analysis, when fitting an autoregressive model, one must solve
a Toeplitz ordinary least squares problem numerous times to find an appropriate
model, which can severely affect computational times with large data sets. Two
recent algorithms (LSAR and Repeated Halving) have applied randomized numerical
linear algebra (RandNLA) techniques to fitting an autoregressive model to big
time-series data. We investigate and compare the quality of these two
approximation algorithms on large-scale synthetic and real-world data. While
both algorithms display comparable results for synthetic datasets, the LSAR
algorithm appears to be more robust when applied to real-world time series
data. We conclude that RandNLA is effective in the context of big-data time
series.

    

### [[2112.12998] DP-UTIL: Comprehensive Utility Analysis of Differential Privacy in Machine Learning](http://arxiv.org/abs/2112.12998)


  Differential Privacy (DP) has emerged as a rigorous formalism to reason about
quantifiable privacy leakage. In machine learning (ML), DP has been employed to
limit inference/disclosure of training examples. Prior work leveraged DP across
the ML pipeline, albeit in isolation, often focusing on mechanisms such as
gradient perturbation. In this paper, we present, DP-UTIL, a holistic utility
analysis framework of DP across the ML pipeline with focus on input
perturbation, objective perturbation, gradient perturbation, output
perturbation, and prediction perturbation. Given an ML task on
privacy-sensitive data, DP-UTIL enables a ML privacy practitioner perform
holistic comparative analysis on the impact of DP in these five perturbation
spots, measured in terms of model utility loss, privacy leakage, and the number
of truly revealed training samples. We evaluate DP-UTIL over classification
tasks on vision, medical, and financial datasets, using two representative
learning algorithms (logistic regression and deep neural network) against
membership inference attack as a case study attack. One of the highlights of
our results is that prediction perturbation consistently achieves the lowest
utility loss on all models across all datasets. In logistic regression models,
objective perturbation results in lowest privacy leakage compared to other
perturbation techniques. For deep neural networks, gradient perturbation
results in lowest privacy leakage. Moreover, our results on true revealed
records suggest that as privacy leakage increases a differentially private
model reveals more number of member samples. Overall, our findings suggest that
to make informed decisions as to which perturbation mechanism to use, a ML
privacy practitioner needs to examine the dynamics between optimization
techniques (convex vs. non-convex), perturbation mechanisms, number of classes,
and privacy budget.

    

### [[2112.12999] Total Energy Shaping with Neural Interconnection and Damping Assignment -- Passivity Based Control](http://arxiv.org/abs/2112.12999)


  In this work we exploit the universal approximation property of Neural
Networks (NNs) to design interconnection and damping assignment (IDA)
passivity-based control (PBC) schemes for fully-actuated mechanical systems in
the port-Hamiltonian (pH) framework. To that end, we transform the IDA-PBC
method into a supervised learning problem that solves the partial differential
matching equations, and fulfills equilibrium assignment and Lyapunov stability
conditions. A main consequence of this, is that the output of the learning
algorithm has a clear control-theoretic interpretation in terms of passivity
and Lyapunov stability. The proposed control design methodology is validated
for mechanical systems of one and two degrees-of-freedom via numerical
simulations.

    

### [[2112.13006] Stochastic Learning Equation using Monotone Increasing Resolution of Quantization](http://arxiv.org/abs/2112.13006)


  In this paper, we propose a quantized learning equation with a monotone
increasing resolution of quantization and stochastic analysis for the proposed
algorithm. According to the white noise hypothesis for the quantization error
with dense and uniform distribution, we can regard the quantization error as
i.i.d.\ white noise. Based on this, we show that the learning equation with
monotonically increasing quantization resolution converges weakly as the
distribution viewpoint. The analysis of this paper shows that global
optimization is possible for a domain that satisfies the Lipschitz condition
instead of local convergence properties such as the Hessian constraint of the
objective function.

    

### [[2112.13011] A machine learning pipeline for autonomous numerical analytic continuation of Dyson-Schwinger equations](http://arxiv.org/abs/2112.13011)


  Dyson-Schwinger equations (DSEs) are a non-perturbative way to express
n-point functions in quantum field theory. Working in Euclidean space and in
Landau gauge, for example, one can study the quark propagator Dyson-Schwinger
equation in the real and complex domain, given that a suitable and tractable
truncation has been found. When aiming for solving these equations in the
complex domain, that is, for complex external momenta, one has to deform the
integration contour of the radial component in the complex plane of the loop
momentum expressed in hyper-spherical coordinates. This has to be done in order
to avoid poles and branch cuts in the integrand of the self-energy loop. Since
the nature of Dyson-Schwinger equations is such, that they have to be solved in
a self-consistent way, one cannot analyze the analytic properties of the
integrand after every iteration step, as this would not be feasible. In these
proceedings, we suggest a machine learning pipeline based on deep learning (DL)
approaches to computer vision (CV), as well as deep reinforcement learning
(DRL), that could solve this problem autonomously by detecting poles and branch
cuts in the numerical integrand after every iteration step and by suggesting
suitable integration contour deformations that avoid these obstructions. We
sketch out a proof of principle for both of these tasks, that is, the pole and
branch cut detection, as well as the contour deformation.

    

### [[2112.13021] Noninvasive Fetal Electrocardiography: Models, Technologies and Algorithms](http://arxiv.org/abs/2112.13021)


  The fetal electrocardiogram (fECG) was first recorded from the maternal
abdominal surface in the early 1900s. During the past fifty years, the most
advanced electronics technologies and signal processing algorithms have been
used to convert noninvasive fetal electrocardiography into a reliable
technology for fetal cardiac monitoring. In this chapter, the major signal
processing techniques, which have been developed for the modeling, extraction
and analysis of the fECG from noninvasive maternal abdominal recordings are
reviewed and compared with one another in detail. The major topics of the
chapter include: 1) the electrophysiology of the fECG from the signal
processing viewpoint, 2) the mathematical model of the maternal volume
conduction media and the waveform models of the fECG acquired from body surface
leads, 3) the signal acquisition requirements, 4) model-based techniques for
fECG noise and interference cancellation, including adaptive filters and
semi-blind source separation techniques, and 5) recent algorithmic advances for
fetal motion tracking and online fECG extraction from few number of channels.

    

### [[2112.13023] DARTS without a Validation Set: Optimizing the Marginal Likelihood](http://arxiv.org/abs/2112.13023)


  The success of neural architecture search (NAS) has historically been limited
by excessive compute requirements. While modern weight-sharing NAS methods such
as DARTS are able to finish the search in single-digit GPU days, extracting the
final best architecture from the shared weights is notoriously unreliable.
Training-Speed-Estimate (TSE), a recently developed generalization estimator
with a Bayesian marginal likelihood interpretation, has previously been used in
place of the validation loss for gradient-based optimization in DARTS. This
prevents the DARTS skip connection collapse, which significantly improves
performance on NASBench-201 and the original DARTS search space. We extend
those results by applying various DARTS diagnostics and show several unusual
behaviors arising from not using a validation set. Furthermore, our experiments
yield concrete examples of the depth gap and topology selection in DARTS having
a strongly negative impact on the search performance despite generally
receiving limited attention in the literature compared to the operations
selection.

    

### [[2112.13029] Gaussian Process Bandits with Aggregated Feedback](http://arxiv.org/abs/2112.13029)


  We consider the continuum-armed bandits problem, under a novel setting of
recommending the best arms within a fixed budget under aggregated feedback.
This is motivated by applications where the precise rewards are impossible or
expensive to obtain, while an aggregated reward or feedback, such as the
average over a subset, is available. We constrain the set of reward functions
by assuming that they are from a Gaussian Process and propose the Gaussian
Process Optimistic Optimisation (GPOO) algorithm. We adaptively construct a
tree with nodes as subsets of the arm space, where the feedback is the
aggregated reward of representatives of a node. We propose a new simple regret
notion with respect to aggregated feedback on the recommended arms. We provide
theoretical analysis for the proposed algorithm, and recover single point
feedback as a special case. We illustrate GPOO and compare it with related
algorithms on simulated data.

    

### [[2112.13047] Channel-Wise Attention-Based Network for Self-Supervised Monocular Depth Estimation](http://arxiv.org/abs/2112.13047)


  Self-supervised learning has shown very promising results for monocular depth
estimation. Scene structure and local details both are significant clues for
high-quality depth estimation. Recent works suffer from the lack of explicit
modeling of scene structure and proper handling of details information, which
leads to a performance bottleneck and blurry artefacts in predicted results. In
this paper, we propose the Channel-wise Attention-based Depth Estimation
Network (CADepth-Net) with two effective contributions: 1) The structure
perception module employs the self-attention mechanism to capture long-range
dependencies and aggregates discriminative features in channel dimensions,
explicitly enhances the perception of scene structure, obtains the better scene
understanding and rich feature representation. 2) The detail emphasis module
re-calibrates channel-wise feature maps and selectively emphasizes the
informative features, aiming to highlight crucial local details information and
fuse different level features more efficiently, resulting in more precise and
sharper depth prediction. Furthermore, the extensive experiments validate the
effectiveness of our method and show that our model achieves the
state-of-the-art results on the KITTI benchmark and Make3D datasets.

    

### [[2112.13058] Tri-Transformer Hawkes Process: Three Heads are better than one](http://arxiv.org/abs/2112.13058)


  Abstract. Most of the real world data we encounter are asynchronous event
sequence, so the last decades have been characterized by the implementation of
various point process into the field of social networks,electronic medical
records and financial transactions. At the beginning, Hawkes process and its
variants which can simulate simultaneously the self-triggering and mutual
triggering patterns between different events in complex sequences in a clear
and quantitative way are more popular.Later on, with the advances of neural
network, neural Hawkes process has been proposed one after another, and
gradually become a research hotspot. The proposal of the transformer Hawkes
process (THP) has gained a huge performance improvement, so a new upsurge of
the neural Hawkes process based on transformer is set off. However, THP does
not make full use of the information of occurrence time and type of event in
the asynchronous event sequence. It simply adds the encoding of event type
conversion and the location encoding of time conversion to the source encoding.
At the same time, the learner built from a single transformer will result in an
inescapable learning bias. In order to mitigate these problems, we propose a
tri-transformer Hawkes process (Tri-THP) model, in which the event and time
information are added to the dot-product attention as auxiliary information to
form a new multihead attention. The effectiveness of the Tri-THP is proved by a
series of well-designed experiments on both real world and synthetic data.

    

### [[2112.13078] Dual Hierarchical Attention Networks for Bi-typed Heterogeneous Graph Learning](http://arxiv.org/abs/2112.13078)


  Bi-typed heterogeneous graphs are applied in many real-world scenarios.
However, previous heterogeneous graph learning studies usually ignore the
complex interactions among the bi-typed entities in such heterogeneous graphs.
To address this issue, in this paper we propose a novel Dual Hierarchical
Attention Networks (DHAN) to learn comprehensive node representations on the
bi-typed heterogeneous graphs with intra-class and inter-class hierarchical
attention networks. Specifically, the intra-class attention aims to learn the
node representations from its same type of neighbors, while inter-class
attention is able to aggregate node representations from its different types of
neighbors. Hence, the dual attention operations enable DHAN to sufficiently
leverage not only the node intra-class neighboring information but also the
inter-class neighboring information in the bi-typed heterogeneous graph.
Experimental results on various tasks against the state-of-the-arts
sufficiently confirm the capability of DHAN in learning node comprehensive
representations on the bi-typed heterogeneous

    

### [[2112.13082] Multi-Scale Feature Fusion: Learning Better Semantic Segmentation for Road Pothole Detection](http://arxiv.org/abs/2112.13082)


  This paper presents a novel pothole detection approach based on single-modal
semantic segmentation. It first extracts visual features from input images
using a convolutional neural network. A channel attention module then reweighs
the channel features to enhance the consistency of different feature maps.
Subsequently, we employ an atrous spatial pyramid pooling module (comprising of
atrous convolutions in series, with progressive rates of dilation) to integrate
the spatial context information. This helps better distinguish between potholes
and undamaged road areas. Finally, the feature maps in the adjacent layers are
fused using our proposed multi-scale feature fusion module. This further
reduces the semantic gap between different feature channel layers. Extensive
experiments were carried out on the Pothole-600 dataset to demonstrate the
effectiveness of our proposed method. The quantitative comparisons suggest that
our method achieves the state-of-the-art (SoTA) performance on both RGB images
and transformed disparity images, outperforming three SoTA single-modal
semantic segmentation networks.

    

### [[2112.13097] Faster Rates for Compressed Federated Learning with Client-Variance Reduction](http://arxiv.org/abs/2112.13097)


  Due to the communication bottleneck in distributed and federated learning
applications, algorithms using communication compression have attracted
significant attention and are widely used in practice. Moreover, there exists
client-variance in federated learning due to the total number of heterogeneous
clients is usually very large and the server is unable to communicate with all
clients in each communication round. In this paper, we address these two issues
together by proposing compressed and client-variance reduced methods.
Concretely, we introduce COFIG and FRECON, which successfully enjoy
communication compression with client-variance reduction. The total
communication round of COFIG is
$O(\frac{(1+\omega)^{3/2}\sqrt{N}}{S\epsilon^2}+\frac{(1+\omega)N^{2/3}}{S\epsilon^2})$
in the nonconvex setting, where $N$ is the total number of clients, $S$ is the
number of communicated clients in each round, $\epsilon$ is the convergence
error, and $\omega$ is the parameter for the compression operator. Besides, our
FRECON can converge faster than COFIG in the nonconvex setting, and it
converges with $O(\frac{(1+\omega)\sqrt{N}}{S\epsilon^2})$ communication
rounds. In the convex setting, COFIG converges within the communication rounds
$O(\frac{(1+\omega)\sqrt{N}}{S\epsilon})$, which is also the first convergence
result for compression schemes that do not communicate with all the clients in
each round. In sum, both COFIG and FRECON do not need to communicate with all
the clients and provide first/faster convergence results for convex and
nonconvex federated learning, while previous works either require full clients
communication (thus not practical) or obtain worse convergence results.

    

### [[2112.13099] Fine-Tuning Data Structures for Analytical Query Processing](http://arxiv.org/abs/2112.13099)


  We introduce a framework for automatically choosing data structures to
support efficient computation of analytical workloads. Our contributions are
twofold. First, we introduce a novel low-level intermediate language that can
express the algorithms behind various query processing paradigms such as
classical joins, groupjoin, and in-database machine learning engines. This
language is designed around the notion of dictionaries, and allows for a more
fine-grained choice of its low-level implementation. Second, the cost model for
alternative implementations is automatically inferred by combining machine
learning and program reasoning. The dictionary cost model is learned using a
regression model trained over the profiling dataset of dictionary operations on
a given hardware architecture. The program cost model is inferred using static
program analysis.
Our experimental results show the effectiveness of the trained cost model on
micro benchmarks. Furthermore, we show that the performance of the code
generated by our framework either outperforms or is on par with the
state-of-the-art analytical query engines and a recent in-database machine
learning framework.

    

### [[2112.13109] Accelerated and instance-optimal policy evaluation with linear function approximation](http://arxiv.org/abs/2112.13109)


  We study the problem of policy evaluation with linear function approximation
and present efficient and practical algorithms that come with strong optimality
guarantees. We begin by proving lower bounds that establish baselines on both
the deterministic error and stochastic error in this problem. In particular, we
prove an oracle complexity lower bound on the deterministic error in an
instance-dependent norm associated with the stationary distribution of the
transition kernel, and use the local asymptotic minimax machinery to prove an
instance-dependent lower bound on the stochastic error in the i.i.d.
observation model. Existing algorithms fail to match at least one of these
lower bounds: To illustrate, we analyze a variance-reduced variant of temporal
difference learning, showing in particular that it fails to achieve the oracle
complexity lower bound. To remedy this issue, we develop an accelerated,
variance-reduced fast temporal difference algorithm (VRFTD) that simultaneously
matches both lower bounds and attains a strong notion of instance-optimality.
Finally, we extend the VRFTD algorithm to the setting with Markovian
observations, and provide instance-dependent convergence results that match
those in the i.i.d. setting up to a multiplicative factor that is proportional
to the mixing time of the chain. Our theoretical guarantees of optimality are
corroborated by numerical experiments.

    

### [[2112.13111] Measuring Quality of DNA Sequence Data via Degradation](http://arxiv.org/abs/2112.13111)


  We propose and apply a novel paradigm for characterization of genome data
quality, which quantifies the effects of intentional degradation of quality.
The rationale is that the higher the initial quality, the more fragile the
genome and the greater the effects of degradation. We demonstrate that this
phenomenon is ubiquitous, and that quantified measures of degradation can be
used for multiple purposes. We focus on identifying outliers that may be
problematic with respect to data quality, but might also be true anomalies or
even attempts to subvert the database.

    

### [[2112.13112] A Survey on Interpretable Reinforcement Learning](http://arxiv.org/abs/2112.13112)


  Although deep reinforcement learning has become a promising machine learning
approach for sequential decision-making problems, it is still not mature enough
for high-stake domains such as autonomous driving or medical applications. In
such contexts, a learned policy needs for instance to be interpretable, so that
it can be inspected before any deployment (e.g., for safety and verifiability
reasons). This survey provides an overview of various approaches to achieve
higher interpretability in reinforcement learning (RL). To that aim, we
distinguish interpretability (as a property of a model) and explainability (as
a post-hoc operation, with the intervention of a proxy) and discuss them in the
context of RL with an emphasis on the former notion. In particular, we argue
that interpretable RL may embrace different facets: interpretable inputs,
interpretable (transition/reward) models, and interpretable decision-making.
Based on this scheme, we summarize and analyze recent work related to
interpretable RL with an emphasis on papers published in the past 10 years. We
also discuss briefly some related research areas and point to some potential
promising research directions.

    

### [[2112.13117] Application of Markov Structure of Genomes to Outlier Identification and Read Classification](http://arxiv.org/abs/2112.13117)


  In this paper we apply the structure of genomes as second-order Markov
processes specified by the distributions of successive triplets of bases to two
bioinformatics problems: identification of outliers in genome databases and
read classification in metagenomics, using real coronavirus and adenovirus
data.

    

### [[2112.13121] The Curse of Zero Task Diversity: On the Failure of Transfer Learning to Outperform MAML and their Empirical Equivalence](http://arxiv.org/abs/2112.13121)


  It has been recently observed that a transfer learning solution might be all
we needed to solve many few-shot learning benchmarks. This raises important
questions about when and how meta-learning algorithms should be deployed. In
this paper, we make a first step in clarifying these questions by first
formulating a computable metric for a few-shot learning benchmark that we
hypothesize is predictive of whether meta-learning solutions will succeed or
not. We name this metric the diversity coefficient of a few-shot learning
benchmark. Using the diversity coefficient, we show that the MiniImagenet
benchmark has zero diversity - according to twenty-four different ways to
compute the diversity. We proceed to show that when making a fair comparison
between MAML learned solutions to transfer learning, both have identical
meta-test accuracy. This suggests that transfer learning fails to outperform
MAML - contrary to what previous work suggests. Together, these two facts
provide the first test of whether diversity correlates with meta-learning
success and therefore show that a diversity coefficient of zero correlates with
a high similarity between transfer learning and MAML learned solutions -
especially at meta-test time. We therefore conjecture meta-learned solutions
have the same meta-test performance as transfer learning when the diversity
coefficient is zero.

    

### [[2112.13137] Does MAML Only Work via Feature Re-use? A Data Centric Perspective](http://arxiv.org/abs/2112.13137)


  Recent work has suggested that a good embedding is all we need to solve many
few-shot learning benchmarks. Furthermore, other work has strongly suggested
that Model Agnostic Meta-Learning (MAML) also works via this same method - by
learning a good embedding. These observations highlight our lack of
understanding of what meta-learning algorithms are doing and when they work. In
this work, we provide empirical results that shed some light on how
meta-learned MAML representations function. In particular, we identify three
interesting properties: 1) In contrast to previous work, we show that it is
possible to define a family of synthetic benchmarks that result in a low degree
of feature re-use - suggesting that current few-shot learning benchmarks might
not have the properties needed for the success of meta-learning algorithms; 2)
meta-overfitting occurs when the number of classes (or concepts) are finite,
and this issue disappears once the task has an unbounded number of concepts
(e.g., online learning); 3) more adaptation at meta-test time with MAML does
not necessarily result in a significant representation change or even an
improvement in meta-test performance - even when training on our proposed
synthetic benchmarks. Finally, we suggest that to understand meta-learning
algorithms better, we must go beyond tracking only absolute performance and, in
addition, formally quantify the degree of meta-learning and track both metrics
together. Reporting results in future work this way will help us identify the
sources of meta-overfitting more accurately and help us design more flexible
meta-learning algorithms that learn beyond fixed feature re-use. Finally, we
conjecture the core challenge of re-thinking meta-learning is in the design of
few-shot learning data sets and benchmarks - rather than in the algorithms, as
suggested by previous work.

    

### [[2112.13141] On the Unreasonable Efficiency of State Space Clustering in Personalization Tasks](http://arxiv.org/abs/2112.13141)


  In this effort we consider a reinforcement learning (RL) technique for
solving personalization tasks with complex reward signals. In particular, our
approach is based on state space clustering with the use of a simplistic
$k$-means algorithm as well as conventional choices of the network
architectures and optimization algorithms. Numerical examples demonstrate the
efficiency of different RL procedures and are used to illustrate that this
technique accelerates the agent's ability to learn and does not restrict the
agent's performance.

    

### [[2112.13143] A Neural Framework for Learning Subgraph and Graph Similarity Measures](http://arxiv.org/abs/2112.13143)


  Subgraph similarity search is a fundamental operator in graph analysis. In
this framework, given a query graph and a graph database, the goal is to
identify subgraphs of the database graphs that are structurally similar to the
query. Subgraph edit distance (SED) is one of the most expressive measures for
subgraph similarity. In this work, we study the problem of learning SED from a
training set of graph pairs and their SED values. Towards that end, we design a
novel siamese graph neural network called NEUROSED, which learns an embedding
space with a rich structure reminiscent of SED. With the help of a specially
crafted inductive bias, NEUROSED not only enables high accuracy but also
ensures that the predicted SED, like true SED, satisfies triangle inequality.
The design is generic enough to also model graph edit distance (GED), while
ensuring that the predicted GED space is metric, like the true GED space.
Extensive experiments on real graph datasets, for both SED and GED, establish
that NEUROSED achieves approximately 2 times lower RMSE than the state of the
art and is approximately 18 times faster than the fastest baseline. Further,
owing to its pair-independent embeddings and theoretical properties, NEUROSED
allows approximately 3 orders of magnitude faster retrieval of graphs and
subgraphs.

    

### [[2112.13166] Cyberattack Detection in Large-Scale Smart Grids using Chebyshev Graph Convolutional Networks](http://arxiv.org/abs/2112.13166)


  As a highly complex and integrated cyber-physical system, modern power grids
are exposed to cyberattacks. False data injection attacks (FDIAs),
specifically, represent a major class of cyber threats to smart grids by
targeting the measurement data's integrity. Although various solutions have
been proposed to detect those cyberattacks, the vast majority of the works have
ignored the inherent graph structure of the power grid measurements and
validated their detectors only for small test systems with less than a few
hundred buses. To better exploit the spatial correlations of smart grid
measurements, this paper proposes a deep learning model for cyberattack
detection in large-scale AC power grids using Chebyshev Graph Convolutional
Networks (CGCN). By reducing the complexity of spectral graph filters and
making them localized, CGCN provides a fast and efficient convolution operation
to model the graph structural smart grid data. We numerically verify that the
proposed CGCN based detector surpasses the state-of-the-art model by 7.86 in
detection rate and 9.67 in false alarm rate for a large-scale power grid with
2848 buses. It is notable that the proposed approach detects cyberattacks under
4 milliseconds for a 2848-bus system, which makes it a good candidate for
real-time detection of cyberattacks in large systems.

    

### [[2112.13168] AI-Bind: Improving Binding Predictions for Novel Protein Targets and Ligands](http://arxiv.org/abs/2112.13168)


  Identifying novel drug-target interactions (DTI) is a critical and rate
limiting step in drug discovery. While deep learning models have been proposed
to accelerate the identification process, we show that state-of-the-art models
fail to generalize to novel (i.e., never-before-seen) structures. We first
unveil the mechanisms responsible for this shortcoming, demonstrating how
models rely on shortcuts that leverage the topology of the protein-ligand
bipartite network, rather than learning the node features. Then, we introduce
AI-Bind, a pipeline that combines network-based sampling strategies with
unsupervised pre-training, allowing us to limit the annotation imbalance and
improve binding predictions for novel proteins and ligands. We illustrate the
value of AI-Bind by predicting drugs and natural compounds with binding
affinity to SARS-CoV-2 viral proteins and the associated human proteins. We
also validate these predictions via auto-docking simulations and comparison
with recent experimental evidence. Overall, AI-Bind offers a powerful
high-throughput approach to identify drug-target combinations, with the
potential of becoming a powerful tool in drug discovery.

    

### [[2112.13178] Gradient Leakage Attack Resilient Deep Learning](http://arxiv.org/abs/2112.13178)


  Gradient leakage attacks are considered one of the wickedest privacy threats
in deep learning as attackers covertly spy gradient updates during iterative
training without compromising model training quality, and yet secretly
reconstruct sensitive training data using leaked gradients with high attack
success rate. Although deep learning with differential privacy is a defacto
standard for publishing deep learning models with differential privacy
guarantee, we show that differentially private algorithms with fixed privacy
parameters are vulnerable against gradient leakage attacks. This paper
investigates alternative approaches to gradient leakage resilient deep learning
with differential privacy (DP). First, we analyze existing implementation of
deep learning with differential privacy, which use fixed noise variance to
injects constant noise to the gradients in all layers using fixed privacy
parameters. Despite the DP guarantee provided, the method suffers from low
accuracy and is vulnerable to gradient leakage attacks. Second, we present a
gradient leakage resilient deep learning approach with differential privacy
guarantee by using dynamic privacy parameters. Unlike fixed-parameter
strategies that result in constant noise variance, different dynamic parameter
strategies present alternative techniques to introduce adaptive noise variance
and adaptive noise injection which are closely aligned to the trend of gradient
updates during differentially private model training. Finally, we describe four
complementary metrics to evaluate and compare alternative approaches.

    

### [[2112.13182] DBC-Forest: Deep forest with binning confidence screening](http://arxiv.org/abs/2112.13182)


  As a deep learning model, deep confidence screening forest (gcForestcs) has
achieved great success in various applications. Compared with the traditional
deep forest approach, gcForestcs effectively reduces the high time cost by
passing some instances in the high-confidence region directly to the final
stage. However, there is a group of instances with low accuracy in the
high-confidence region, which are called mis-partitioned instances. To find
these mis-partitioned instances, this paper proposes a deep binning confidence
screening forest (DBC-Forest) model, which packs all instances into bins based
on their confidences. In this way, more accurate instances can be passed to the
final stage, and the performance is improved. Experimental results show that
DBC-Forest achieves highly accurate predictions for the same hyperparameters
and is faster than other similar models to achieve the same accuracy.

    

### [[2112.13196] A comparative study on machine learning models combining with outlier detection and balanced sampling methods for credit scoring](http://arxiv.org/abs/2112.13196)


  Peer-to-peer (P2P) lending platforms have grown rapidly over the past decade
as the network infrastructure has improved and the demand for personal lending
has grown. Such platforms allow users to create peer-to-peer lending
relationships without the help of traditional financial institutions. Assessing
the borrowers' credit is crucial to reduce the default rate and benign
development of P2P platforms. Building a personal credit scoring machine
learning model can effectively predict whether users will repay loans on the
P2P platform. And the handling of data outliers and sample imbalance problems
can affect the final effect of machine learning models. There have been some
studies on balanced sampling methods, but the effect of outlier detection
methods and their combination with balanced sampling methods on the
effectiveness of machine learning models has not been fully studied. In this
paper, the influence of using different outlier detection methods and balanced
sampling methods on commonly used machine learning models is investigated.
Experiments on 44,487 Lending Club samples show that proper outlier detection
can improve the effectiveness of the machine learning model, and the balanced
sampling method only has a good effect on a few machine learning models, such
as MLP.

    

### [[2112.13199] A Spectral Method for Joint Community Detection and Orthogonal Group Synchronization](http://arxiv.org/abs/2112.13199)


  Community detection and orthogonal group synchronization are both fundamental
problems with a variety of important applications in science and engineering.
In this work, we consider the joint problem of community detection and
orthogonal group synchronization which aims to recover the communities and
perform synchronization simultaneously. To this end, we propose a simple
algorithm that consists of a spectral decomposition step followed by a
blockwise column pivoted QR factorization (CPQR). The proposed algorithm is
efficient and scales linearly with the number of data points. We also leverage
the recently developed `leave-one-out' technique to establish a near-optimal
guarantee for exact recovery of the cluster memberships and stable recovery of
the orthogonal transforms. Numerical experiments demonstrate the efficiency and
efficacy of our algorithm and confirm our theoretical characterization of it.

    

### [[2112.13208] Neural Network Module Decomposition and Recomposition](http://arxiv.org/abs/2112.13208)


  We propose a modularization method that decomposes a deep neural network
(DNN) into small modules from a functionality perspective and recomposes them
into a new model for some other task. Decomposed modules are expected to have
the advantages of interpretability and verifiability due to their small size.
In contrast to existing studies based on reusing models that involve
retraining, such as a transfer learning model, the proposed method does not
require retraining and has wide applicability as it can be easily combined with
existing functional modules. The proposed method extracts modules using weight
masks and can be applied to arbitrary DNNs. Unlike existing studies, it
requires no assumption about the network architecture. To extract modules, we
designed a learning method and a loss function to maximize shared weights among
modules. As a result, the extracted modules can be recomposed without a large
increase in the size. We demonstrate that the proposed method can decompose and
recompose DNNs with high compression ratio and high accuracy and is superior to
the existing method through sharing weights between modules.

    

### [[2112.13210] Explainable Artificial Intelligence for Pharmacovigilance: What Features Are Important When Predicting Adverse Outcomes?](http://arxiv.org/abs/2112.13210)


  Explainable Artificial Intelligence (XAI) has been identified as a viable
method for determining the importance of features when making predictions using
Machine Learning (ML) models. In this study, we created models that take an
individual's health information (e.g. their drug history and comorbidities) as
inputs, and predict the probability that the individual will have an Acute
Coronary Syndrome (ACS) adverse outcome. Using XAI, we quantified the
contribution that specific drugs had on these ACS predictions, thus creating an
XAI-based technique for pharmacovigilance monitoring, using ACS as an example
of the adverse outcome to detect. Individuals aged over 65 who were supplied
Musculo-skeletal system (anatomical therapeutic chemical (ATC) class M) or
Cardiovascular system (ATC class C) drugs between 1993 and 2009 were
identified, and their drug histories, comorbidities, and other key features
were extracted from linked Western Australian datasets. Multiple ML models were
trained to predict if these individuals would have an ACS related adverse
outcome (i.e., death or hospitalisation with a discharge diagnosis of ACS), and
a variety of ML and XAI techniques were used to calculate which features --
specifically which drugs -- led to these predictions. The drug dispensing
features for rofecoxib and celecoxib were found to have a greater than zero
contribution to ACS related adverse outcome predictions (on average), and it
was found that ACS related adverse outcomes can be predicted with 72% accuracy.
Furthermore, the XAI libraries LIME and SHAP were found to successfully
identify both important and unimportant features, with SHAP slightly
outperforming LIME. ML models trained on linked administrative health datasets
in tandem with XAI algorithms can successfully quantify feature importance, and
with further development, could potentially be used as pharmacovigilance
monitoring techniques.

    

### [[2112.13214] NeuronFair: Interpretable White-Box Fairness Testing through Biased Neuron Identification](http://arxiv.org/abs/2112.13214)


  Deep neural networks (DNNs) have demonstrated their outperformance in various
domains. However, it raises a social concern whether DNNs can produce reliable
and fair decisions especially when they are applied to sensitive domains
involving valuable resource allocation, such as education, loan, and
employment. It is crucial to conduct fairness testing before DNNs are reliably
deployed to such sensitive domains, i.e., generating as many instances as
possible to uncover fairness violations. However, the existing testing methods
are still limited from three aspects: interpretability, performance, and
generalizability. To overcome the challenges, we propose NeuronFair, a new DNN
fairness testing framework that differs from previous work in several key
aspects: (1) interpretable - it quantitatively interprets DNNs' fairness
violations for the biased decision; (2) effective - it uses the interpretation
results to guide the generation of more diverse instances in less time; (3)
generic - it can handle both structured and unstructured data. Extensive
evaluations across 7 datasets and the corresponding DNNs demonstrate
NeuronFair's superior performance. For instance, on structured datasets, it
generates much more instances (~x5.84) and saves more time (with an average
speedup of 534.56%) compared with the state-of-the-art methods. Besides, the
instances of NeuronFair can also be leveraged to improve the fairness of the
biased DNNs, which helps build more fair and trustworthy deep learning systems.

    

### [[2112.13215] Continual Learning for Unsupervised Anomaly Detection in Continuous Auditing of Financial Accounting Data](http://arxiv.org/abs/2112.13215)


  International audit standards require the direct assessment of a financial
statement's underlying accounting journal entries. Driven by advances in
artificial intelligence, deep-learning inspired audit techniques emerged to
examine vast quantities of journal entry data. However, in regular audits, most
of the proposed methods are applied to learn from a comparably stationary
journal entry population, e.g., of a financial quarter or year. Ignoring
situations where audit relevant distribution changes are not evident in the
training data or become incrementally available over time. In contrast, in
continuous auditing, deep-learning models are continually trained on a stream
of recorded journal entries, e.g., of the last hour. Resulting in situations
where previous knowledge interferes with new information and will be entirely
overwritten. This work proposes a continual anomaly detection framework to
overcome both challenges and designed to learn from a stream of journal entry
data experiences. The framework is evaluated based on deliberately designed
audit scenarios and two real-world datasets. Our experimental results provide
initial evidence that such a learning scheme offers the ability to reduce
false-positive alerts and false-negative decisions.

    

### [[2112.13230] N-Omniglot: a Large-scale Dataset for Spatio-Temporal Sparse Few-shot Learning](http://arxiv.org/abs/2112.13230)


  Few-shot learning (learning with a few samples) is one of the most important
capacities of the human brain. However, the current artificial intelligence
systems meet difficulties in achieving this ability, so as the biologically
plausible spiking neural networks (SNNs). Datasets for traditional few-shot
learning domains provide few amounts of temporal information. And the absence
of the neuromorphic datasets has hindered the development of few-shot learning
for SNNs. Here, we provide the first neuromorphic dataset: N-Omniglot, using
the Dynamic Vision Sensor (DVS). It contains 1623 categories of handwritten
characters, with only 20 samples per class. N-Omniglot eliminates the need for
a neuromorphic dataset for SNNs with high spareness and tremendous temporal
coherence. Additionally, the dataset provides a powerful challenge and a
suitable benchmark for developing SNNs algorithm in the few-shot learning
domain due to the chronological information of strokes. We also provide the
improved nearest neighbor, convolutional network, SiameseNet, and meta-learning
algorithm in spiking version for verification.

    

### [[2112.13236] An Ensemble of Pre-trained Transformer Models For Imbalanced Multiclass Malware Classification](http://arxiv.org/abs/2112.13236)


  Classification of malware families is crucial for a comprehensive
understanding of how they can infect devices, computers, or systems. Thus,
malware identification enables security researchers and incident responders to
take precautions against malware and accelerate mitigation. API call sequences
made by malware are widely utilized features by machine and deep learning
models for malware classification as these sequences represent the behavior of
malware. However, traditional machine and deep learning models remain incapable
of capturing sequence relationships between API calls. On the other hand, the
transformer-based models process sequences as a whole and learn relationships
between API calls due to multi-head attention mechanisms and positional
embeddings. Our experiments demonstrate that the transformer model with one
transformer block layer surpassed the widely used base architecture, LSTM.
Moreover, BERT or CANINE, pre-trained transformer models, outperformed in
classifying highly imbalanced malware families according to evaluation metrics,
F1-score, and AUC score. Furthermore, the proposed bagging-based random
transformer forest (RTF), an ensemble of BERT or CANINE, has reached the
state-of-the-art evaluation scores on three out of four datasets, particularly
state-of-the-art F1-score of 0.6149 on one of the commonly used benchmark
dataset.

    

### [[2112.13246] Towards Federated Learning on Time-Evolving Heterogeneous Data](http://arxiv.org/abs/2112.13246)


  Federated Learning (FL) is an emerging learning paradigm that preserves
privacy by ensuring client data locality on edge devices. The optimization of
FL is challenging in practice due to the diversity and heterogeneity of the
learning system. Despite recent research efforts on improving the optimization
of heterogeneous data, the impact of time-evolving heterogeneous data in
real-world scenarios, such as changing client data or intermittent clients
joining or leaving during training, has not been well studied. In this work, we
propose Continual Federated Learning (CFL), a flexible framework, to capture
the time-evolving heterogeneity of FL. CFL covers complex and realistic
scenarios -- which are challenging to evaluate in previous FL formulations --
by extracting the information of past local datasets and approximating the
local objective functions. Theoretically, we demonstrate that CFL methods
achieve a faster convergence rate than \fedavg in time-evolving scenarios, with
the benefit being dependent on approximation quality. In a series of
experiments, we show that the numerical findings match the convergence
analysis, and CFL methods significantly outperform the other SOTA FL baselines.

    

### [[2112.13251] Reactive Message Passing for Scalable Bayesian Inference](http://arxiv.org/abs/2112.13251)


  We introduce Reactive Message Passing (RMP) as a framework for executing
schedule-free, robust and scalable message passing-based inference in a factor
graph representation of a probabilistic model. RMP is based on the reactive
programming style that only describes how nodes in a factor graph react to
changes in connected nodes. The absence of a fixed message passing schedule
improves robustness, scalability and execution time of the inference procedure.
We also present ReactiveMP.jl, which is a Julia package for realizing RMP
through minimization of a constrained Bethe free energy. By user-defined
specification of local form and factorization constraints on the variational
posterior distribution, ReactiveMP.jl executes hybrid message passing
algorithms including belief propagation, variational message passing,
expectation propagation, and expectation maximisation update rules.
Experimental results demonstrate the improved performance of ReactiveMP-based
RMP in comparison to other Julia packages for Bayesian inference across a range
of probabilistic models. In particular, we show that the RMP framework is able
to run Bayesian inference for large-scale probabilistic state space models with
hundreds of thousands of random variables on a standard laptop computer.

    

### [[2112.13254] On Dynamic Pricing with Covariates](http://arxiv.org/abs/2112.13254)


  We consider the dynamic pricing problem with covariates under a generalized
linear demand model: a seller can dynamically adjust the price of a product
over a horizon of $T$ time periods, and at each time period $t$, the demand of
the product is jointly determined by the price and an observable covariate
vector $x_t\in\mathbb{R}^d$ through an unknown generalized linear model. Most
of the existing literature assumes the covariate vectors $x_t$'s are
independently and identically distributed (i.i.d.); the few papers that relax
this assumption either sacrifice model generality or yield sub-optimal regret
bounds. In this paper we show that a simple pricing algorithm has an
$O(d\sqrt{T}\log T)$ regret upper bound without assuming any statistical
structure on the covariates $x_t$ (which can even be arbitrarily chosen). The
upper bound on the regret matches the lower bound (even under the i.i.d.
assumption) up to logarithmic factors. Our paper thus shows that (i) the i.i.d.
assumption is not necessary for obtaining low regret, and (ii) the regret bound
can be independent of the (inverse) minimum eigenvalue of the covariance matrix
of the $x_t$'s, a quantity present in previous bounds. Furthermore, we discuss
a condition under which a better regret is achievable and how a Thompson
sampling algorithm can be applied to give an efficient computation of the
prices.

    

### [[2112.13259] Deeper Clinical Document Understanding Using Relation Extraction](http://arxiv.org/abs/2112.13259)


  The surging amount of biomedical literature & digital clinical records
presents a growing need for text mining techniques that can not only identify
but also semantically relate entities in unstructured data. In this paper we
propose a text mining framework comprising of Named Entity Recognition (NER)
and Relation Extraction (RE) models, which expands on previous work in three
main ways. First, we introduce two new RE model architectures -- an
accuracy-optimized one based on BioBERT and a speed-optimized one utilizing
crafted features over a Fully Connected Neural Network (FCNN). Second, we
evaluate both models on public benchmark datasets and obtain new
state-of-the-art F1 scores on the 2012 i2b2 Clinical Temporal Relations
challenge (F1 of 73.6, +1.2% over the previous SOTA), the 2010 i2b2 Clinical
Relations challenge (F1 of 69.1, +1.2%), the 2019 Phenotype-Gene Relations
dataset (F1 of 87.9, +8.5%), the 2012 Adverse Drug Events Drug-Reaction dataset
(F1 of 90.0, +6.3%), and the 2018 n2c2 Posology Relations dataset (F1 of 96.7,
+0.6%). Third, we show two practical applications of this framework -- for
building a biomedical knowledge graph and for improving the accuracy of mapping
entities to clinical codes. The system is built using the Spark NLP library
which provides a production-grade, natively scalable, hardware-optimized,
trainable & tunable NLP framework.

    

### [[2112.13264] Artifact Reduction in Fundus Imaging using Cycle Consistent Adversarial Neural Networks](http://arxiv.org/abs/2112.13264)


  Fundus images are very useful in identifying various ophthalmic disorders.
However, due to the presence of artifacts, the visibility of the retina is
severely affected. This may result in misdiagnosis of the disorder which may
lead to more complicated problems. Since deep learning is a powerful tool to
extract patterns from data without much human intervention, they can be applied
to image-to-image translation problems. An attempt has been made in this paper
to automatically rectify such artifacts present in the images of the fundus. We
use a CycleGAN based model which consists of residual blocks to reduce the
artifacts in the images. Significant improvements are seen when compared to the
existing techniques.

    

### [[2112.13267] Task and Model Agnostic Adversarial Attack on Graph Neural Networks](http://arxiv.org/abs/2112.13267)


  Graph neural networks (GNNs) have witnessed significant adoption in the
industry owing to impressive performance on various predictive tasks.
Performance alone, however, is not enough. Any widely deployed machine learning
algorithm must be robust to adversarial attacks. In this work, we investigate
this aspect for GNNs, identify vulnerabilities, and link them to graph
properties that may potentially lead to the development of more secure and
robust GNNs. Specifically, we formulate the problem of task and model agnostic
evasion attacks where adversaries modify the test graph to affect the
performance of any unknown downstream task. The proposed algorithm, GRAND
($Gr$aph $A$ttack via $N$eighborhood $D$istortion) shows that distortion of
node neighborhoods is effective in drastically compromising prediction
performance. Although neighborhood distortion is an NP-hard problem, GRAND
designs an effective heuristic through a novel combination of Graph Isomorphism
Network with deep $Q$-learning. Extensive experiments on real datasets show
that, on average, GRAND is up to $50\%$ more effective than state of the art
techniques, while being more than $100$ times faster.

    

### [[2112.13269] Over-Parametrized Matrix Factorization in the Presence of Spurious Stationary Points](http://arxiv.org/abs/2112.13269)


  Motivated by the emerging role of interpolating machines in signal processing
and machine learning, this work considers the computational aspects of
over-parametrized matrix factorization. In this context, the optimization
landscape may contain spurious stationary points (SSPs), which are proved to be
full-rank matrices. The presence of these SSPs means that it is impossible to
hope for any global guarantees in over-parametrized matrix factorization. For
example, when initialized at an SSP, the gradient flow will be trapped there
forever. Nevertheless, despite these SSPs, we establish in this work that the
gradient flow of the corresponding merit function converges to a global
minimizer, provided that its initialization is rank-deficient and sufficiently
close to the feasible set of the optimization problem. We numerically observe
that a heuristic discretization of the proposed gradient flow, inspired by
primal-dual algorithms, is successful when initialized randomly. Our result is
in sharp contrast with the local refinement methods which require an
initialization close to the optimal set of the optimization problem. More
specifically, we successfully avoid the traps set by the SSPs because the
gradient flow remains rank-deficient at all times, and not because there are no
SSPs nearby. The latter is the case for the local refinement methods. Moreover,
the widely-used restricted isometry property plays no role in our main result.

    

### [[2112.13284] Learning Linear Complementarity Systems](http://arxiv.org/abs/2112.13284)


  This paper investigates the learning, or system identification, of a class of
piecewise-affine dynamical systems known as linear complementarity systems
(LCSs). We propose a violation-based loss which enables efficient learning of
the LCS parameterization, without prior knowledge of the hybrid mode
boundaries, using gradient-based methods. The proposed violation-based loss
incorporates both dynamics prediction loss and a novel complementarity -
violation loss. We show several properties attained by this loss formulation,
including its differentiability, the efficient computation of first- and
second-order derivatives, and its relationship to the traditional prediction
loss, which strictly enforces complementarity. We apply this violation-based
loss formulation to learn LCSs with tens of thousands of (potentially stiff)
hybrid modes. The results demonstrate a state-of-the-art ability to identify
piecewise-affine dynamics, outperforming methods which must differentiate
through non-smooth linear complementarity problems.

    

### [[2112.13285] Pedagogical Rule Extraction for Learning Interpretable Models](http://arxiv.org/abs/2112.13285)


  Machine-learning models are ubiquitous. In some domains, for instance, in
medicine, the models' predictions must be interpretable. Decision trees,
classification rules, and subgroup discovery are three broad categories of
supervised machine-learning models presenting knowledge in the form of
interpretable rules. The accuracy of these models learned from small datasets
is usually low. Obtaining larger datasets is often hard to impossible. We
propose a framework dubbed PRELIM to learn better rules from small data. It
augments data using statistical models and employs it to learn a rulebased
model. In our extensive experiments, we identified PRELIM configurations that
outperform state-of-the-art.

    

### [[2112.13289] Prevalence Threshold and bounds in the Accuracy of Binary Classification Systems](http://arxiv.org/abs/2112.13289)


  The accuracy of binary classification systems is defined as the proportion of
correct predictions - both positive and negative - made by a classification
model or computational algorithm. A value between 0 (no accuracy) and 1
(perfect accuracy), the accuracy of a classification model is dependent on
several factors, notably: the classification rule or algorithm used, the
intrinsic characteristics of the tool used to do the classification, and the
relative frequency of the elements being classified. Several accuracy metrics
exist, each with its own advantages in different classification scenarios. In
this manuscript, we show that relative to a perfect accuracy of 1, the positive
prevalence threshold ($\phi_e$), a critical point of maximum curvature in the
precision-prevalence curve, bounds the $F{_{\beta}}$ score between 1 and
1.8/1.5/1.2 for $\beta$ values of 0.5/1.0/2.0, respectively; the $F_1$ score
between 1 and 1.5, and the Fowlkes-Mallows Index (FM) between 1 and $\sqrt{2}
\approx 1.414$. We likewise describe a novel $negative$ prevalence threshold
($\phi_n$), the level of sharpest curvature for the negative predictive
value-prevalence curve, such that $\phi_n$ $>$ $\phi_e$. The area between both
these thresholds bounds the Matthews Correlation Coefficient (MCC) between
$\sqrt{2}/2$ and $\sqrt{2}$. Conversely, the ratio of the maximum possible
accuracy to that at any point below the prevalence threshold, $\phi_e$, goes to
infinity with decreasing prevalence. Though applications are numerous, the
ideas herein discussed may be used in computational complexity theory,
artificial intelligence, and medical screening, amongst others. Where
computational time is a limiting resource, attaining the prevalence threshold
in binary classification systems may be sufficient to yield levels of accuracy
comparable to that under maximum prevalence.

    

### [[2112.13314] Silent Bugs in Deep Learning Frameworks: An Empirical Study of Keras and TensorFlow](http://arxiv.org/abs/2112.13314)


  Deep Learning (DL) frameworks are now widely used, simplifying the creation
of complex models as well as their integration to various applications even to
non DL experts. However, like any other programs, they are prone to bugs. This
paper deals with the subcategory of bugs named silent bugs: they lead to wrong
behavior but they do not cause system crashes or hangs, nor show an error
message to the user. Such bugs are even more dangerous in DL applications and
frameworks due to the "black-box" and stochastic nature of the systems (the end
user can not understand how the model makes decisions). This paper presents the
first empirical study of Keras and TensorFlow silent bugs, and their impact on
users' programs. We extracted closed issues related to Keras from the
TensorFlow GitHub repository. Out of the 1,168 issues that we gathered, 77 were
reproducible silent bugs affecting users' programs. We categorized the bugs
based on the effects on the users' programs and the components where the issues
occurred, using information from the issue reports. We then derived a threat
level for each of the issues, based on the impact they had on the users'
programs. To assess the relevance of identified categories and the impact
scale, we conducted an online survey with 103 DL developers. The participants
generally agreed with the significant impact of silent bugs in DL libraries and
acknowledged our findings (i.e., categories of silent bugs and the proposed
impact scale). Finally, leveraging our analysis, we provide a set of guidelines
to facilitate safeguarding against such bugs in DL frameworks.

    

### [[2112.13316] Efficient Diversity-Driven Ensemble for Deep Neural Networks](http://arxiv.org/abs/2112.13316)


  The ensemble of deep neural networks has been shown, both theoretically and
empirically, to improve generalization accuracy on the unseen test set.
However, the high training cost hinders its efficiency since we need a
sufficient number of base models and each one in the ensemble has to be
separately trained. Lots of methods are proposed to tackle this problem, and
most of them are based on the feature that a pre-trained network can transfer
its knowledge to the next base model and then accelerate the training process.
However, these methods suffer a severe problem that all of them transfer
knowledge without selection and thus lead to low diversity. As the effect of
ensemble learning is more pronounced if ensemble members are accurate and
diverse, we propose a method named Efficient Diversity-Driven Ensemble (EDDE)
to address both the diversity and the efficiency of an ensemble. To accelerate
the training process, we propose a novel knowledge transfer method which can
selectively transfer the previous generic knowledge. To enhance diversity, we
first propose a new diversity measure, then use it to define a diversity-driven
loss function for optimization. At last, we adopt a Boosting-based framework to
combine the above operations, such a method can also further improve diversity.
We evaluate EDDE on Computer Vision (CV) and Natural Language Processing (NLP)
tasks. Compared with other well-known ensemble methods, EDDE can get highest
ensemble accuracy with the lowest training cost, which means it is efficient in
the ensemble of neural networks.

    

### [[2112.13328] Continuous Offline Handwriting Recognition using Deep Learning Models](http://arxiv.org/abs/2112.13328)


  Handwritten text recognition is an open problem of great interest in the area
of automatic document image analysis. The transcription of handwritten content
present in digitized documents is significant in analyzing historical archives
or digitizing information from handwritten documents, forms, and
communications. In the last years, great advances have been made in this area
due to applying deep learning techniques to its resolution. This Thesis
addresses the offline continuous handwritten text recognition (HTR) problem,
consisting of developing algorithms and models capable of transcribing the text
present in an image without the need for the text to be segmented into
characters. For this purpose, we have proposed a new recognition model based on
integrating two types of deep learning architectures: convolutional neural
networks (CNN) and sequence-to-sequence (seq2seq) models, respectively. The
convolutional component of the model is oriented to identify relevant features
present in characters, and the seq2seq component builds the transcription of
the text by modeling the sequential nature of the text. For the design of this
new model, an extensive analysis of the capabilities of different convolutional
architectures in the simplified problem of isolated character recognition has
been carried out in order to identify the most suitable ones to be integrated
into the continuous model. Additionally, extensive experimentation of the
proposed model for the continuous problem has been carried out to determine its
robustness to changes in parameterization. The generalization capacity of the
model has also been validated by evaluating it on three handwritten text
databases using different languages: IAM in English, RIMES in French, and
Osborne in Spanish, respectively. The new proposed model provides competitive
results with those obtained with other well-established methodologies.

    

### [[2112.13338] MPCLeague: Robust MPC Platform for Privacy-Preserving Machine Learning](http://arxiv.org/abs/2112.13338)


  In the modern era of computing, machine learning tools have demonstrated
their potential in vital sectors, such as healthcare and finance, to derive
proper inferences. The sensitive and confidential nature of the data in such
sectors raises genuine concerns for data privacy. This motivated the area of
Privacy-preserving Machine Learning (PPML), where privacy of data is
guaranteed. In this thesis, we design an efficient platform, MPCLeague, for
PPML in the Secure Outsourced Computation (SOC) setting using Secure
Multi-party Computation (MPC) techniques.
MPC, the holy-grail problem of secure distributed computing, enables a set of
n mutually distrusting parties to perform joint computation on their private
inputs in a way that no coalition of t parties can learn more information than
the output (privacy) or affect the true output of the computation
(correctness). While MPC, in general, has been a subject of extensive research,
the area of MPC with a small number of parties has drawn popularity of late
mainly due to its application to real-time scenarios, efficiency and
simplicity. This thesis focuses on designing efficient MPC frameworks for 2, 3
and 4 parties, with at most one corruption and supports ring structures.
At the heart of this thesis are four frameworks - ASTRA, SWIFT, Tetrad,
ABY2.0 - catered to different settings. The practicality of our framework is
argued through improvements in the benchmarking of widely used ML algorithms --
Linear Regression, Logistic Regression, Neural Networks, and Support Vector
Machines. We propose two variants for each of our frameworks, with one variant
aiming to minimise the execution time while the other focuses on the monetary
cost. The concrete efficiency gains of our frameworks coupled with the stronger
security guarantee of robustness make our platform an ideal choice for a
real-time deployment of PPML techniques.

    

### [[2112.13339] It-Taylor Sampling Scheme for Denoising Diffusion Probabilistic Models using Ideal Derivatives](http://arxiv.org/abs/2112.13339)


  Denoising Diffusion Probabilistic Models (DDPMs) have been attracting
attention recently as a new challenger to popular deep neural generative models
including GAN, VAE, etc. However, DDPMs have a disadvantage that they often
require a huge number of refinement steps during the synthesis. To address this
problem, this paper proposes a new DDPM sampler based on a second-order
numerical scheme for stochastic differential equations (SDEs), while the
conventional sampler is based on a first-order numerical scheme. In general, it
is not easy to compute the derivatives that are required in higher-order
numerical schemes. However, in the case of DDPM, this difficulty is alleviated
by the trick which the authors call "ideal derivative substitution". The newly
derived higher-order sampler was applied to both image and speech generation
tasks, and it is experimentally observed that the proposed sampler could
synthesize plausible images and audio signals in relatively smaller number of
refinement steps.

    

### [[2112.13346] The Quantum Version of Prediction for Binary Classification Problem by Ensemble Methods](http://arxiv.org/abs/2112.13346)


  In this work, we consider the performance of using a quantum algorithm to
predict a result for a binary classification problem if a machine learning
model is an ensemble from any simple classifiers. Such an approach is faster
than classical prediction and uses quantum and classical computing, but it is
based on a probabilistic algorithm. Let $N$ be a number of classifiers from an
ensemble model and $O(T)$ be the running time of prediction on one classifier.
In classical case, an ensemble model gets answers from each classifier and
"averages" the result. The running time in classical case is $O\left( N \cdot T
\right)$. We propose an algorithm which works in $O\left(\sqrt{N} \cdot
T\right)$.

    

### [[2112.13350] Novel Dual-Channel Long Short-Term Memory Compressed Capsule Networks for Emotion Recognition](http://arxiv.org/abs/2112.13350)


  Recent analysis on speech emotion recognition has made considerable advances
with the use of MFCCs spectrogram features and the implementation of neural
network approaches such as convolutional neural networks (CNNs). Capsule
networks (CapsNet) have gained gratitude as alternatives to CNNs with their
larger capacities for hierarchical representation. To address these issues,
this research introduces a text-independent and speaker-independent SER novel
architecture, where a dual-channel long short-term memory compressed-CapsNet
(DC-LSTM COMP-CapsNet) algorithm is proposed based on the structural features
of CapsNet. Our proposed novel classifier can ensure the energy efficiency of
the model and adequate compression method in speech emotion recognition, which
is not delivered through the original structure of a CapsNet. Moreover, the
grid search approach is used to attain optimal solutions. Results witnessed an
improved performance and reduction in the training and testing running time.
The speech datasets used to evaluate our algorithm are: Arabic Emirati-accented
corpus, English speech under simulated and actual stress corpus, English
Ryerson audio-visual database of emotional speech and song corpus, and
crowd-sourced emotional multimodal actors dataset. This work reveals that the
optimum feature extraction method compared to other known methods is MFCCs
delta-delta. Using the four datasets and the MFCCs delta-delta, DC-LSTM
COMP-CapsNet surpasses all the state-of-the-art systems, classical classifiers,
CNN, and the original CapsNet. Using the Arabic Emirati-accented corpus, our
results demonstrate that the proposed work yields average emotion recognition
accuracy of 89.3% compared to 84.7%, 82.2%, 69.8%, 69.2%, 53.8%, 42.6%, and
31.9% based on CapsNet, CNN, support vector machine, multi-layer perceptron,
k-nearest neighbor, radial basis function, and naive Bayes, respectively.

    

### [[2112.13353] Novel Hybrid DNN Approaches for Speaker Verification in Emotional and Stressful Talking Environments](http://arxiv.org/abs/2112.13353)


  In this work, we conducted an empirical comparative study of the performance
of text-independent speaker verification in emotional and stressful
environments. This work combined deep models with shallow architecture, which
resulted in novel hybrid classifiers. Four distinct hybrid models were
utilized: deep neural network-hidden Markov model (DNN-HMM), deep neural
network-Gaussian mixture model (DNN-GMM), Gaussian mixture model-deep neural
network (GMM-DNN), and hidden Markov model-deep neural network (HMM-DNN). All
models were based on novel implemented architecture. The comparative study used
three distinct speech datasets: a private Arabic dataset and two public English
databases, namely, Speech Under Simulated and Actual Stress (SUSAS) and Ryerson
Audio-Visual Database of Emotional Speech and Song (RAVDESS). The test results
of the aforementioned hybrid models demonstrated that the proposed HMM-DNN
leveraged the verification performance in emotional and stressful environments.
Results also showed that HMM-DNN outperformed all other hybrid models in terms
of equal error rate (EER) and area under the curve (AUC) evaluation metrics.
The average resulting verification system based on the three datasets yielded
EERs of 7.19%, 16.85%, 11.51%, and 11.90% based on HMM-DNN, DNN-HMM, DNN-GMM,
and GMM-DNN, respectively. Furthermore, we found that the DNN-GMM model
demonstrated the least computational complexity compared to all other hybrid
models in both talking environments. Conversely, the HMM-DNN model required the
greatest amount of training time. Findings also demonstrated that EER and AUC
values depended on the database when comparing average emotional and stressful
performances.

    

### [[2112.13366] AIDA: An Active Inference-based Design Agent for Audio Processing Algorithms](http://arxiv.org/abs/2112.13366)


  In this paper we present AIDA, which is an active inference-based agent that
iteratively designs a personalized audio processing algorithm through situated
interactions with a human client. The target application of AIDA is to propose
on-the-spot the most interesting alternative values for the tuning parameters
of a hearing aid (HA) algorithm, whenever a HA client is not satisfied with
their HA performance. AIDA interprets searching for the "most interesting
alternative" as an issue of optimal (acoustic) context-aware Bayesian trial
design. In computational terms, AIDA is realized as an active inference-based
agent with an Expected Free Energy criterion for trial design. This type of
architecture is inspired by neuro-economic models on efficient (Bayesian) trial
design in brains and implies that AIDA comprises generative probabilistic
models for acoustic signals and user responses. We propose a novel generative
model for acoustic signals as a sum of time-varying auto-regressive filters and
a user response model based on a Gaussian Process Classifier. The full AIDA
agent has been implemented in a factor graph for the generative model and all
tasks (parameter learning, acoustic context classification, trial design, etc.)
are realized by variational message passing on the factor graph. All
verification and validation experiments and demonstrations are freely
accessible at our GitHub repository.

    

### [[2112.13367] A Trained Regularization Approach Based on Born Iterative Method for Electromagnetic Imaging](http://arxiv.org/abs/2112.13367)


  A trained-based Born iterative method (TBIM) is developed for electromagnetic
imaging (EMI) applications. The proposed TBIM consists of a nested loop; the
outer loop executes TBIM iteration steps, while the inner loop executes a
trained iterative shrinkage thresholding algorithm (TISTA). The applied TISTA
runs linear Landweber iterations implemented with a trained regularization
network designed based on U-net architecture. A normalization process was
imposed in TISTA that made TISTA training applicable within the proposed TBIM.
The iterative utilization of the regularization network in TISTA is a
bottleneck that demands high memory allocation through the training process.
Therefore TISTA within each TBIM step was trained separately. The TISTA
regularization network in each TBIM step was initialized using the weights from
the previous TBIM step. The above approach achieved high-quality image
restoration after running few TBIM steps while maintained low memory allocation
through the training process. The proposed framework can be extended to Newton
or quasi-Newton schemes, where within each Newton iteration, a linear ill-posed
problem is optimized that differs from one example to another. The numerical
results illustrated in this work show the superiority of the proposed TBIM
compared to the conventional sparse-based Born iterative method (SBIM).

    

### [[2112.13381] FRuDA: Framework for Distributed Adversarial Domain Adaptation](http://arxiv.org/abs/2112.13381)


  Breakthroughs in unsupervised domain adaptation (uDA) can help in adapting
models from a label-rich source domain to unlabeled target domains. Despite
these advancements, there is a lack of research on how uDA algorithms,
particularly those based on adversarial learning, can work in distributed
settings. In real-world applications, target domains are often distributed
across thousands of devices, and existing adversarial uDA algorithms -- which
are centralized in nature -- cannot be applied in these settings. To solve this
important problem, we introduce FRuDA: an end-to-end framework for distributed
adversarial uDA. Through a careful analysis of the uDA literature, we identify
the design goals for a distributed uDA system and propose two novel algorithms
to increase adaptation accuracy and training efficiency of adversarial uDA in
distributed settings. Our evaluation of FRuDA with five image and speech
datasets show that it can boost target domain accuracy by up to 50% and improve
the training efficiency of adversarial uDA by at least 11 times.

    

### [[2112.13384] Will You Dance To The Challenge? Predicting User Participation of TikTok Challenges](http://arxiv.org/abs/2112.13384)


  TikTok is a popular new social media, where users express themselves through
short video clips. A common form of interaction on the platform is
participating in "challenges", which are songs and dances for users to iterate
upon. Challenge contagion can be measured through replication reach, i.e.,
users uploading videos of their participation in the challenges. The uniqueness
of the TikTok platform where both challenge content and user preferences are
evolving requires the combination of challenge and user representation. This
paper investigates social contagion of TikTok challenges through predicting a
user's participation. We propose a novel deep learning model, deepChallenger,
to learn and combine latent user and challenge representations from past videos
to perform this user-challenge prediction task. We collect a dataset of over
7,000 videos from 12 trending challenges on the ForYouPage, the app's landing
page, and over 10,000 videos from 1303 users. Extensive experiments are
conducted and the results show that our proposed deepChallenger (F1=0.494)
outperforms baselines (F1=0.188) in the prediction task.

    

### [[2112.13386] Reducing Planning Complexity of General Reinforcement Learning with Non-Markovian Abstractions](http://arxiv.org/abs/2112.13386)


  The field of General Reinforcement Learning (GRL) formulates the problem of
sequential decision-making from ground up. The history of interaction
constitutes a "ground" state of the system, which never repeats. On the one
hand, this generality allows GRL to model almost every domain possible, e.g.\
Bandits, MDPs, POMDPs, PSRs, and history-based environments. On the other hand,
in general, the near-optimal policies in GRL are functions of complete history,
which hinders not only learning but also planning in GRL. The usual way around
for the planning part is that the agent is given a Markovian abstraction of the
underlying process. So, it can use any MDP planning algorithm to find a
near-optimal policy. The Extreme State Aggregation (ESA) framework has extended
this idea to non-Markovian abstractions without compromising on the possibility
of planning through a (surrogate) MDP. A distinguishing feature of ESA is that
it proves an upper bound of $O\left(\varepsilon^{-A} \cdot
(1-\gamma)^{-2A}\right)$ on the number of states required for the surrogate MDP
(where $A$ is the number of actions, $\gamma$ is the discount-factor, and
$\varepsilon$ is the optimality-gap) which holds \emph{uniformly} for
\emph{all} domains. While the possibility of a universal bound is quite
remarkable, we show that this bound is very loose. We propose a novel non-MDP
abstraction which allows for a much better upper bound of
$O\left(\varepsilon^{-1} \cdot (1-\gamma)^{-2} \cdot A \cdot 2^{A}\right)$.
Furthermore, we show that this bound can be improved further to
$O\left(\varepsilon^{-1} \cdot (1-\gamma)^{-2} \cdot \log^3 A \right)$ by using
an action-sequentialization method.

    

### [[2112.13398] Omitted Variable Bias in Machine Learned Causal Models](http://arxiv.org/abs/2112.13398)


  We derive general, yet simple, sharp bounds on the size of the omitted
variable bias for a broad class of causal parameters that can be identified as
linear functionals of the conditional expectation function of the outcome. Such
functionals encompass many of the traditional targets of investigation in
causal inference studies, such as, for example, (weighted) average of potential
outcomes, average treatment effects (including subgroup effects, such as the
effect on the treated), (weighted) average derivatives, and policy effects from
shifts in covariate distribution -- all for general, nonparametric causal
models. Our construction relies on the Riesz-Frechet representation of the
target functional. Specifically, we show how the bound on the bias depends only
on the additional variation that the latent variables create both in the
outcome and in the Riesz representer for the parameter of interest. Moreover,
in many important cases (e.g, average treatment effects in partially linear
models, or in nonseparable models with a binary treatment) the bound is shown
to depend on two easily interpretable quantities: the nonparametric partial
$R^2$ (Pearson's "correlation ratio") of the unobserved variables with the
treatment and with the outcome. Therefore, simple plausibility judgments on the
maximum explanatory power of omitted variables (in explaining treatment and
outcome variation) are sufficient to place overall bounds on the size of the
bias. Finally, leveraging debiased machine learning, we provide flexible and
efficient statistical inference methods to estimate the components of the
bounds that are identifiable from the observed distribution.

    

### [[2112.13404] Abstractions of General Reinforcement Learning](http://arxiv.org/abs/2112.13404)


  The field of artificial intelligence (AI) is devoted to the creation of
artificial decision-makers that can perform (at least) on par with the human
counterparts on a domain of interest. Unlike the agents in traditional AI, the
agents in artificial general intelligence (AGI) are required to replicate human
intelligence in almost every domain of interest. Moreover, an AGI agent should
be able to achieve this without (virtually any) further changes, retraining, or
fine-tuning of the parameters. The real world is non-stationary, non-ergodic,
and non-Markovian: we, humans, can neither revisit our past nor are the most
recent observations sufficient statistics. Yet, we excel at a variety of
complex tasks. Many of these tasks require longterm planning. We can associate
this success to our natural faculty to abstract away task-irrelevant
information from our overwhelming sensory experience. We make task-specific
mental models of the world without much effort. Due to this ability to
abstract, we can plan on a significantly compact representation of a task
without much loss of performance. Not only this, we also abstract our actions
to produce high-level plans: the level of action-abstraction can be anywhere
between small muscle movements to a mental notion of "doing an action". It is
natural to assume that any AGI agent competing with humans (at every plausible
domain) should also have these abilities to abstract its experiences and
actions. This thesis is an inquiry into the existence of such abstractions
which aid efficient planing for a wide range of domains, and most importantly,
these abstractions come with some optimality guarantees.

    

### [[2112.13408] Perlin Noise Improve Adversarial Robustness](http://arxiv.org/abs/2112.13408)


  Adversarial examples are some special input that can perturb the output of a
deep neural network, in order to make produce intentional errors in the
learning algorithms in the production environment. Most of the present methods
for generating adversarial examples require gradient information. Even
universal perturbations that are not relevant to the generative model rely to
some extent on gradient information. Procedural noise adversarial examples is a
new way of adversarial example generation, which uses computer graphics noise
to generate universal adversarial perturbations quickly while not relying on
gradient information. Combined with the defensive idea of adversarial training,
we use Perlin noise to train the neural network to obtain a model that can
defend against procedural noise adversarial examples. In combination with the
use of model fine-tuning methods based on pre-trained models, we obtain faster
training as well as higher accuracy. Our study shows that procedural noise
adversarial examples are defensible, but why procedural noise can generate
adversarial examples and how to defend against other kinds of procedural noise
adversarial examples that may emerge in the future remain to be investigated.

    

### [[2112.13410] Generative Kernel Continual learning](http://arxiv.org/abs/2112.13410)


  Kernel continual learning by \citet{derakhshani2021kernel} has recently
emerged as a strong continual learner due to its non-parametric ability to
tackle task interference and catastrophic forgetting. Unfortunately its success
comes at the expense of an explicit memory to store samples from past tasks,
which hampers scalability to continual learning settings with a large number of
tasks. In this paper, we introduce generative kernel continual learning, which
explores and exploits the synergies between generative models and kernels for
continual learning. The generative model is able to produce representative
samples for kernel learning, which removes the dependence on memory in kernel
continual learning. Moreover, as we replay only on the generative model, we
avoid task interference while being computationally more efficient compared to
previous methods that need replay on the entire model. We further introduce a
supervised contrastive regularization, which enables our model to generate even
more discriminative samples for better kernel-based classification performance.
We conduct extensive experiments on three widely-used continual learning
benchmarks that demonstrate the abilities and benefits of our contributions.
Most notably, on the challenging SplitCIFAR100 benchmark, with just a simple
linear kernel we obtain the same accuracy as kernel continual learning with
variational random features for one tenth of the memory, or a 10.1\% accuracy
gain for the same memory budget.

    

### [[2112.13414] Reinforcement Learning with Dynamic Convex Risk Measures](http://arxiv.org/abs/2112.13414)


  We develop an approach for solving time-consistent risk-sensitive stochastic
optimization problems using model-free reinforcement learning (RL).
Specifically, we assume agents assess the risk of a sequence of random
variables using dynamic convex risk measures. We employ a time-consistent
dynamic programming principle to determine the value of a particular policy,
and develop policy gradient update rules. We further develop an actor-critic
style algorithm using neural networks to optimize over policies. Finally, we
demonstrate the performance and flexibility of our approach by applying it to
optimization problems in statistical arbitrage trading and obstacle avoidance
robot control.

    

### [[2112.13416] Attribute Inference Attack of Speech Emotion Recognition in Federated Learning Settings](http://arxiv.org/abs/2112.13416)


  Speech emotion recognition (SER) processes speech signals to detect and
characterize expressed perceived emotions. Many SER application systems often
acquire and transmit speech data collected at the client-side to remote cloud
platforms for inference and decision making. However, speech data carry rich
information not only about emotions conveyed in vocal expressions, but also
other sensitive demographic traits such as gender, age and language background.
Consequently, it is desirable for SER systems to have the ability to classify
emotion constructs while preventing unintended/improper inferences of sensitive
and demographic information. Federated learning (FL) is a distributed machine
learning paradigm that coordinates clients to train a model collaboratively
without sharing their local data. This training approach appears secure and can
improve privacy for SER. However, recent works have demonstrated that FL
approaches are still vulnerable to various privacy attacks like reconstruction
attacks and membership inference attacks. Although most of these have focused
on computer vision applications, such information leakages exist in the SER
systems trained using the FL technique. To assess the information leakage of
SER systems trained using FL, we propose an attribute inference attack
framework that infers sensitive attribute information of the clients from
shared gradients or model parameters, corresponding to the FedSGD and the
FedAvg training algorithms, respectively. As a use case, we empirically
evaluate our approach for predicting the client's gender information using
three SER benchmark datasets: IEMOCAP, CREMA-D, and MSP-Improv. We show that
the attribute inference attack is achievable for SER systems trained using FL.
We further identify that most information leakage possibly comes from the first
layer in the SER model.

    

### [[2112.13418] Neuro-Symbolic Hierarchical Rule Induction](http://arxiv.org/abs/2112.13418)


  We propose an efficient interpretable neuro-symbolic model to solve Inductive
Logic Programming (ILP) problems. In this model, which is built from a set of
meta-rules organised in a hierarchical structure, first-order rules are
invented by learning embeddings to match facts and body predicates of a
meta-rule. To instantiate it, we specifically design an expressive set of
generic meta-rules, and demonstrate they generate a consequent fragment of Horn
clauses. During training, we inject a controlled \pw{Gumbel} noise to avoid
local optima and employ interpretability-regularization term to further guide
the convergence to interpretable rules. We empirically validate our model on
various tasks (ILP, visual genome, reinforcement learning) against several
state-of-the-art methods.

    

### [[2112.13432] New Methods & Metrics for LFQA tasks](http://arxiv.org/abs/2112.13432)


  Long-form question answering (LFQA) tasks require retrieving the documents
pertinent to a query, using them to form a paragraph-length answer. Despite
considerable progress in LFQA modeling, fundamental issues impede its progress:
i) train/validation/test dataset overlap, ii) absence of automatic metrics and
iii) generated answers not being "grounded" in retrieved documents. This work
addresses every one these critical bottlenecks, contributing natural language
inference/generation (NLI/NLG) methods and metrics that make significant
strides to their alleviation.

    

### [[2112.13443] Sinogram upsampling using Primal-Dual UNet for undersampled CT and radial MRI reconstruction](http://arxiv.org/abs/2112.13443)


  CT and MRI are two widely used clinical imaging modalities for non-invasive
diagnosis. However, both of these modalities come with certain problems. CT
uses harmful ionising radiation, and MRI suffers from slow acquisition speed.
Both problems can be tackled by undersampling, such as sparse sampling.
However, such undersampled data leads to lower resolution and introduces
artefacts. Several techniques, including deep learning based methods, have been
proposed to reconstruct such data. However, the undersampled reconstruction
problem for these two modalities was always considered as two different
problems and tackled separately by different research works. This paper
proposes a unified solution for both sparse CT and undersampled radial MRI
reconstruction, achieved by applying Fourier transform-based pre-processing on
the radial MRI and then reconstructing both modalities using sinogram
upsampling combined with filtered back-projection. The Primal-Dual network is a
deep learning based method for reconstructing sparsely-sampled CT data. This
paper introduces Primal-Dual UNet, which improves the Primal-Dual network in
terms of accuracy and reconstruction speed. The proposed method resulted in an
average SSIM of 0.932 while performing sparse CT reconstruction for fan-beam
geometry with a sparsity level of 16, achieving a statistically significant
improvement over the previous model, which resulted in 0.919. Furthermore, the
proposed model resulted in 0.903 and 0.957 average SSIM while reconstructing
undersampled brain and abdominal MRI data with an acceleration factor of 16 -
statistically significant improvements over the original model, which resulted
in 0.867 and 0.949. Finally, this paper shows that the proposed network not
only improves the overall image quality, but also improves the image quality
for the regions-of-interest; as well as generalises better in presence of a
needle.

    

### [[2112.13444] A CNN-BiLSTM Model with Attention Mechanism for Earthquake Prediction](http://arxiv.org/abs/2112.13444)


  Earthquakes, as natural phenomena, have continuously caused damage and loss
of human life historically. Earthquake prediction is an essential aspect of any
society's plans and can increase public preparedness and reduce damage to a
great extent. Nevertheless, due to the stochastic character of earthquakes and
the challenge of achieving an efficient and dependable model for earthquake
prediction, efforts have been insufficient thus far, and new methods are
required to solve this problem. Aware of these issues, this paper proposes a
novel prediction method based on attention mechanism (AM), convolution neural
network (CNN), and bi-directional long short-term memory (BiLSTM) models, which
can predict the number and maximum magnitude of earthquakes in each area of
mainland China-based on the earthquake catalog of the region. This model takes
advantage of LSTM and CNN with an attention mechanism to better focus on
effective earthquake characteristics and produce more accurate predictions.
Firstly, the zero-order hold technique is applied as pre-processing on
earthquake data, making the model's input data more proper. Secondly, to
effectively use spatial information and reduce dimensions of input data, the
CNN is used to capture the spatial dependencies between earthquake data.
Thirdly, the Bi-LSTM layer is employed to capture the temporal dependencies.
Fourthly, the AM layer is introduced to highlight its important features to
achieve better prediction performance. The results show that the proposed
method has better performance and generalize ability than other prediction
methods.

    

### [[2112.13450] Acoustic scene classification using auditory datasets](http://arxiv.org/abs/2112.13450)


  The approach used not only challenges some of the fundamental mathematical
techniques used so far in early experiments of the same trend but also
introduces new scopes and new horizons for interesting results. The physics
governing spectrograms have been optimized in the project along with exploring
how it handles the intense requirements of the problem at hand. Major
contributions and developments brought under the light, through this project
involve using better mathematical techniques and problem-specific machine
learning methods. Improvised data analysis and data augmentation for audio
datasets like frequency masking and random frequency-time stretching are used
in the project and hence are explained in this paper. In the used methodology,
the audio transforms principle were also tried and explored, and indeed the
insights gained were used constructively in the later stages of the project.
Using a deep learning principle is surely one of them. Also, in this paper, the
potential scopes and upcoming research openings in both short and long term
tunnel of time has been presented. Although much of the results gained are
domain-specific as of now, they are surely potent enough to produce novel
solutions in various different domains of diverse backgrounds.

    

### [[2112.13467] ToxTree: descriptor-based machine learning models for both hERG and Nav1.5 cardiotoxicity liability predictions](http://arxiv.org/abs/2112.13467)


  Drug-mediated blockade of the voltage-gated potassium channel(hERG) and the
voltage-gated sodium channel (Nav1.5) can lead to severe cardiovascular
complications. This rising concern has been reflected in the drug development
arena, as the frequent emergence of cardiotoxicity from many approved drugs led
to either discontinuing their use or, in some cases, their withdrawal from the
market. Predicting potential hERG and Nav1.5 blockers at the outset of the drug
discovery process can resolve this problem and can, therefore, decrease the
time and expensive cost of developing safe drugs. One fast and cost-effective
approach is to use in silico predictive methods to weed out potential hERG and
Nav1.5 blockers at the early stages of drug development. Here, we introduce two
robust 2D descriptor-based QSAR predictive models for both hERG and Nav1.5
liability predictions. The machine learning models were trained for both
regression, predicting the potency value of a drug, and multiclass
classification at three different potency cut-offs (i.e. 1{\mu}M, 10{\mu}M, and
30{\mu}M), where ToxTree-hERG Classifier, a pipeline of Random Forest models,
was trained on a large curated dataset of 8380 unique molecular compounds.
Whereas ToxTree-Nav1.5 Classifier, a pipeline of kernelized SVM models, was
trained on a large manually curated set of 1550 unique compounds retrieved from
both ChEMBL and PubChem publicly available bioactivity databases. The proposed
hERG inducer outperformed most metrics of the state-of-the-art published model
and other existing tools. Additionally, we are introducing the first Nav1.5
liability predictive model achieving a Q4 = 74.9% and a binary classification
of Q2 = 86.7% with MCC = 71.2% evaluated on an external test set of 173 unique
compounds. The curated datasets used in this project are made publicly
available to the research community.

    

### [[2112.13469] Learning Optimization Proxies for Large-Scale Security-Constrained Economic Dispatch](http://arxiv.org/abs/2112.13469)


  The Security-Constrained Economic Dispatch (SCED) is a fundamental
optimization model for Transmission System Operators (TSO) to clear real-time
energy markets while ensuring reliable operations of power grids. In a context
of growing operational uncertainty, due to increased penetration of renewable
generators and distributed energy resources, operators must continuously
monitor risk in real-time, i.e., they must quickly assess the system's behavior
under various changes in load and renewable production. Unfortunately,
systematically solving an optimization problem for each such scenario is not
practical given the tight constraints of real-time operations. To overcome this
limitation, this paper proposes to learn an optimization proxy for SCED, i.e.,
a Machine Learning (ML) model that can predict an optimal solution for SCED in
milliseconds. Motivated by a principled analysis of the market-clearing
optimizations of MISO, the paper proposes a novel ML pipeline that addresses
the main challenges of learning SCED solutions, i.e., the variability in load,
renewable output and production costs, as well as the combinatorial structure
of commitment decisions. A novel Classification-Then-Regression architecture is
also proposed, to further capture the behavior of SCED solutions. Numerical
experiments are reported on the French transmission system, and demonstrate the
approach's ability to produce, within a time frame that is compatible with
real-time operations, accurate optimization proxies that produce relative
errors below $0.6\%$.

    

### [[2112.13487] The Statistical Complexity of Interactive Decision Making](http://arxiv.org/abs/2112.13487)


  A fundamental challenge in interactive learning and decision making, ranging
from bandit problems to reinforcement learning, is to provide sample-efficient,
adaptive learning algorithms that achieve near-optimal regret. This question is
analogous to the classical problem of optimal (supervised) statistical
learning, where there are well-known complexity measures (e.g., VC dimension
and Rademacher complexity) that govern the statistical complexity of learning.
However, characterizing the statistical complexity of interactive learning is
substantially more challenging due to the adaptive nature of the problem. The
main result of this work provides a complexity measure, the Decision-Estimation
Coefficient, that is proven to be both necessary and sufficient for
sample-efficient interactive learning. In particular, we provide:
1. a lower bound on the optimal regret for any interactive decision making
problem, establishing the Decision-Estimation Coefficient as a fundamental
limit.
2. a unified algorithm design principle, Estimation-to-Decisions (E2D), which
transforms any algorithm for supervised estimation into an online algorithm for
decision making. E2D attains a regret bound matching our lower bound, thereby
achieving optimal sample-efficient learning as characterized by the
Decision-Estimation Coefficient.
Taken together, these results constitute a theory of learnability for
interactive decision making. When applied to reinforcement learning settings,
the Decision-Estimation Coefficient recovers essentially all existing hardness
results and lower bounds. More broadly, the approach can be viewed as a
decision-theoretic analogue of the classical Le Cam theory of statistical
estimation; it also unifies a number of existing approaches -- both Bayesian
and frequentist.

    

### [[2112.13502] Deep Treatment-Adaptive Network for Causal Inference](http://arxiv.org/abs/2112.13502)


  Causal inference is capable of estimating the treatment effect (i.e., the
causal effect of treatment on the outcome) to benefit the decision making in
various domains. One fundamental challenge in this research is that the
treatment assignment bias in observational data. To increase the validity of
observational studies on causal inference, representation based methods as the
state-of-the-art have demonstrated the superior performance of treatment effect
estimation. Most representation based methods assume all observed covariates
are pre-treatment (i.e., not affected by the treatment), and learn a balanced
representation from these observed covariates for estimating treatment effect.
Unfortunately, this assumption is often too strict a requirement in practice,
as some covariates are changed by doing an intervention on treatment (i.e.,
post-treatment). By contrast, the balanced representation learned from
unchanged covariates thus biases the treatment effect estimation.

    

### [[2112.13507] Block Modeling-Guided Graph Convolutional Neural Networks](http://arxiv.org/abs/2112.13507)


  Graph Convolutional Network (GCN) has shown remarkable potential of exploring
graph representation. However, the GCN aggregating mechanism fails to
generalize to networks with heterophily where most nodes have neighbors from
different classes, which commonly exists in real-world networks. In order to
make the propagation and aggregation mechanism of GCN suitable for both
homophily and heterophily (or even their mixture), we introduce block modeling
into the framework of GCN so that it can realize "block-guided classified
aggregation", and automatically learn the corresponding aggregation rules for
neighbors of different classes. By incorporating block modeling into the
aggregation process, GCN is able to aggregate information from homophilic and
heterophilic neighbors discriminately according to their homophily degree. We
compared our algorithm with state-of-art methods which deal with the
heterophily problem. Empirical results demonstrate the superiority of our new
approach over existing methods in heterophilic datasets while maintaining a
competitive performance in homophilic datasets.

    

### [[2112.13510] Mind the Gap: Cross-Lingual Information Retrieval with Hierarchical Knowledge Enhancement](http://arxiv.org/abs/2112.13510)


  Cross-Lingual Information Retrieval (CLIR) aims to rank the documents written
in a language different from the user's query. The intrinsic gap between
different languages is an essential challenge for CLIR. In this paper, we
introduce the multilingual knowledge graph (KG) to the CLIR task due to the
sufficient information of entities in multiple languages. It is regarded as a
"silver bullet" to simultaneously perform explicit alignment between queries
and documents and also broaden the representations of queries. And we propose a
model named CLIR with hierarchical knowledge enhancement (HIKE) for our task.
The proposed model encodes the textual information in queries, documents and
the KG with multilingual BERT, and incorporates the KG information in the
query-document matching process with a hierarchical information fusion
mechanism. Particularly, HIKE first integrates the entities and their
neighborhood in KG into query representations with a knowledge-level fusion,
then combines the knowledge from both source and target languages to further
mitigate the linguistic gap with a language-level fusion. Finally, experimental
results demonstrate that HIKE achieves substantial improvements over
state-of-the-art competitors.

    

### [[2112.13513] MSHT: Multi-stage Hybrid Transformer for the ROSE Image Analysis of Pancreatic Cancer](http://arxiv.org/abs/2112.13513)


  Pancreatic cancer is one of the most malignant cancers in the world, which
deteriorates rapidly with very high mortality. The rapid on-site evaluation
(ROSE) technique innovates the workflow by immediately analyzing the fast
stained cytopathological images with on-site pathologists, which enables faster
diagnosis in this time-pressured process. However, the wider expansion of ROSE
diagnosis has been hindered by the lack of experienced pathologists. To
overcome this problem, we propose a hybrid high-performance deep learning model
to enable the automated workflow, thus freeing the occupation of the valuable
time of pathologists. By firstly introducing the Transformer block into this
field with our particular multi-stage hybrid design, the spatial features
generated by the convolutional neural network (CNN) significantly enhance the
Transformer global modeling. Turning multi-stage spatial features as global
attention guidance, this design combines the robustness from the inductive bias
of CNN with the sophisticated global modeling power of Transformer. A dataset
of 4240 ROSE images is collected to evaluate the method in this unexplored
field. The proposed multi-stage hybrid Transformer (MSHT) achieves 95.68% in
classification accuracy, which is distinctively higher than the
state-of-the-art models. Facing the need for interpretability, MSHT outperforms
its counterparts with more accurate attention regions. The results demonstrate
that the MSHT can distinguish cancer samples accurately at an unprecedented
image scale, laying the foundation for deploying automatic decision systems and
enabling the expansion of ROSE in clinical practice. The code and records are
available at: this https URL.

    

### [[2112.13514] Anomaly Detection using Capsule Networks for High-dimensional Datasets](http://arxiv.org/abs/2112.13514)


  Anomaly detection is an essential problem in machine learning. Application
areas include network security, health care, fraud detection, etc., involving
high-dimensional datasets. A typical anomaly detection system always faces the
class-imbalance problem in the form of a vast difference in the sample sizes of
different classes. They usually have class overlap problems. This study used a
capsule network for the anomaly detection task. To the best of our knowledge,
this is the first instance where a capsule network is analyzed for the anomaly
detection task in a high-dimensional non-image complex data setting. We also
handle the related novelty and outlier detection problems. The architecture of
the capsule network was suitably modified for a binary classification task.
Capsule networks offer a good option for detecting anomalies due to the effect
of viewpoint invariance captured in its predictions and viewpoint equivariance
captured in internal capsule architecture. We used six-layered under-complete
autoencoder architecture with second and third layers containing capsules. The
capsules were trained using the dynamic routing algorithm. We created
$10$-imbalanced datasets from the original MNIST dataset and compared the
performance of the capsule network with $5$ baseline models. Our leading test
set measures are F1-score for minority class and area under the ROC curve. We
found that the capsule network outperformed every other baseline model on the
anomaly detection task by using only ten epochs for training and without using
any other data level and algorithm level approach. Thus, we conclude that
capsule networks are excellent in modeling complex high-dimensional imbalanced
datasets for the anomaly detection task.

    

### [[2112.13521] Can Reinforcement Learning Find Stackelberg-Nash Equilibria in General-Sum Markov Games with Myopic Followers?](http://arxiv.org/abs/2112.13521)


  We study multi-player general-sum Markov games with one of the players
designated as the leader and the other players regarded as followers. In
particular, we focus on the class of games where the followers are myopic,
i.e., they aim to maximize their instantaneous rewards. For such a game, our
goal is to find a Stackelberg-Nash equilibrium (SNE), which is a policy pair
$(\pi^*, \nu^*)$ such that (i) $\pi^*$ is the optimal policy for the leader
when the followers always play their best response, and (ii) $\nu^*$ is the
best response policy of the followers, which is a Nash equilibrium of the
followers' game induced by $\pi^*$. We develop sample-efficient reinforcement
learning (RL) algorithms for solving for an SNE in both online and offline
settings. Our algorithms are optimistic and pessimistic variants of
least-squares value iteration, and they are readily able to incorporate
function approximation tools in the setting of large state spaces. Furthermore,
for the case with linear function approximation, we prove that our algorithms
achieve sublinear regret and suboptimality under online and offline setups
respectively. To the best of our knowledge, we establish the first provably
efficient RL algorithms for solving for SNEs in general-sum Markov games with
myopic followers.

    

### [[2112.13530] Wasserstein Flow Meets Replicator Dynamics: A Mean-Field Analysis of Representation Learning in Actor-Critic](http://arxiv.org/abs/2112.13530)


  Actor-critic (AC) algorithms, empowered by neural networks, have had
significant empirical success in recent years. However, most of the existing
theoretical support for AC algorithms focuses on the case of linear function
approximations, or linearized neural networks, where the feature representation
is fixed throughout training. Such a limitation fails to capture the key aspect
of representation learning in neural AC, which is pivotal in practical
problems. In this work, we take a mean-field perspective on the evolution and
convergence of feature-based neural AC. Specifically, we consider a version of
AC where the actor and critic are represented by overparameterized two-layer
neural networks and are updated with two-timescale learning rates. The critic
is updated by temporal-difference (TD) learning with a larger stepsize while
the actor is updated via proximal policy optimization (PPO) with a smaller
stepsize. In the continuous-time and infinite-width limiting regime, when the
timescales are properly separated, we prove that neural AC finds the globally
optimal policy at a sublinear rate. Additionally, we prove that the feature
representation induced by the critic network is allowed to evolve within a
neighborhood of the initial one.

    

### [[2112.13542] Sparsest Univariate Learning Models Under Lipschitz Constraint](http://arxiv.org/abs/2112.13542)


  Beside the minimization of the prediction error, two of the most desirable
properties of a regression scheme are stability and interpretability. Driven by
these principles, we propose continuous-domain formulations for one-dimensional
regression problems. In our first approach, we use the Lipschitz constant as a
regularizer, which results in an implicit tuning of the overall robustness of
the learned mapping. In our second approach, we control the Lipschitz constant
explicitly using a user-defined upper-bound and make use of a
sparsity-promoting regularizer to favor simpler (and, hence, more
interpretable) solutions. The theoretical study of the latter formulation is
motivated in part by its equivalence, which we prove, with the training of a
Lipschitz-constrained two-layer univariate neural network with rectified linear
unit (ReLU) activations and weight decay. By proving representer theorems, we
show that both problems admit global minimizers that are continuous and
piecewise-linear (CPWL) functions. Moreover, we propose efficient algorithms
that find the sparsest solution of each problem: the CPWL mapping with the
least number of linear regions. Finally, we illustrate numerically the outcome
of our formulations.

    

### [[2112.13544] FitAct: Error Resilient Deep Neural Networks via Fine-Grained Post-Trainable Activation Functions](http://arxiv.org/abs/2112.13544)


  Deep neural networks (DNNs) are increasingly being deployed in
safety-critical systems such as personal healthcare devices and self-driving
cars. In such DNN-based systems, error resilience is a top priority since
faults in DNN inference could lead to mispredictions and safety hazards. For
latency-critical DNN inference on resource-constrained edge devices, it is
nontrivial to apply conventional redundancy-based fault tolerance techniques.
In this paper, we propose FitAct, a low-cost approach to enhance the error
resilience of DNNs by deploying fine-grained post-trainable activation
functions. The main idea is to precisely bound the activation value of each
individual neuron via neuron-wise bounded activation functions so that it could
prevent fault propagation in the network. To avoid complex DNN model
re-training, we propose to decouple the accuracy training and resilience
training and develop a lightweight post-training phase to learn these
activation functions with precise bound values. Experimental results on widely
used DNN models such as AlexNet, VGG16, and ResNet50 demonstrate that FitAct
outperforms state-of-the-art studies such as Clip-Act and Ranger in enhancing
the DNN error resilience for a wide range of fault rates while adding
manageable runtime and memory space overheads.

    

### [[2112.13545] ViR:the Vision Reservoir](http://arxiv.org/abs/2112.13545)


  The most recent year has witnessed the success of applying the Vision
Transformer (ViT) for image classification. However, there are still evidences
indicating that ViT often suffers following two aspects, i) the high
computation and the memory burden from applying the multiple Transformer layers
for pre-training on a large-scale dataset, ii) the over-fitting when training
on small datasets from scratch. To address these problems, a novel method,
namely, Vision Reservoir computing (ViR), is proposed here for image
classification, as a parallel to ViT. By splitting each image into a sequence
of tokens with fixed length, the ViR constructs a pure reservoir with a nearly
fully connected topology to replace the Transformer module in ViT. Two kinds of
deep ViR models are subsequently proposed to enhance the network performance.
Comparative experiments between the ViR and the ViT are carried out on several
image classification benchmarks. Without any pre-training process, the ViR
outperforms the ViT in terms of both model and computational complexity.
Specifically, the number of parameters of the ViR is about 15% even 5% of the
ViT, and the memory footprint is about 20% to 40% of the ViT. The superiority
of the ViR performance is explained by Small-World characteristics, Lyapunov
exponents, and memory capacity.

    

### [[2112.13547] PRIME: A Few Primitives Can Boost Robustness to Common Corruptions](http://arxiv.org/abs/2112.13547)


  Despite their impressive performance on image classification tasks, deep
networks have a hard time generalizing to many common corruptions of their
data. To fix this vulnerability, prior works have mostly focused on increasing
the complexity of their training pipelines, combining multiple methods, in the
name of diversity. However, in this work, we take a step back and follow a
principled approach to achieve robustness to common corruptions. We propose
PRIME, a general data augmentation scheme that consists of simple families of
max-entropy image transformations. We show that PRIME outperforms the prior art
for corruption robustness, while its simplicity and plug-and-play nature
enables it to be combined with other methods to further boost their robustness.
Furthermore, we analyze PRIME to shed light on the importance of the mixing
strategy on synthesizing corrupted images, and to reveal the
robustness-accuracy trade-offs arising in the context of common corruptions.
Finally, we show that the computational efficiency of our method allows it to
be easily used in both on-line and off-line data augmentation schemes.

    

### [[2112.13562] Powerful Graph Convolutioal Networks with Adaptive Propagation Mechanism for Homophily and Heterophily](http://arxiv.org/abs/2112.13562)


  Graph Convolutional Networks (GCNs) have been widely applied in various
fields due to their significant power on processing graph-structured data.
Typical GCN and its variants work under a homophily assumption (i.e., nodes
with same class are prone to connect to each other), while ignoring the
heterophily which exists in many real-world networks (i.e., nodes with
different classes tend to form edges). Existing methods deal with heterophily
by mainly aggregating higher-order neighborhoods or combing the immediate
representations, which leads to noise and irrelevant information in the result.
But these methods did not change the propagation mechanism which works under
homophily assumption (that is a fundamental part of GCNs). This makes it
difficult to distinguish the representation of nodes from different classes. To
address this problem, in this paper we design a novel propagation mechanism,
which can automatically change the propagation and aggregation process
according to homophily or heterophily between node pairs. To adaptively learn
the propagation process, we introduce two measurements of homophily degree
between node pairs, which is learned based on topological and attribute
information, respectively. Then we incorporate the learnable homophily degree
into the graph convolution framework, which is trained in an end-to-end schema,
enabling it to go beyond the assumption of homophily. More importantly, we
theoretically prove that our model can constrain the similarity of
representations between nodes according to their homophily degree. Experiments
on seven real-world datasets demonstrate that this new approach outperforms the
state-of-the-art methods under heterophily or low homophily, and gains
competitive performance under homophily.

    

### [[2112.13578] A probabilistic model for fast-to-evaluate 2D crack path prediction in heterogeneous materials](http://arxiv.org/abs/2112.13578)


  This paper is devoted to the construction of a new fast-to-evaluate model for
the prediction of 2D crack paths in concrete-like microstructures. The model
generates piecewise linear cracks paths with segmentation points selected using
a Markov chain model. The Markov chain kernel involves local indicators of
mechanical interest and its parameters are learnt from numerical full-field 2D
simulations of craking using a cohesive-volumetric finite element solver called
XPER. The resulting model exhibits a drastic improvement of CPU time in
comparison to simulations from XPER.

    

### [[2112.13581] Survival Analysis of the Compressor Station Based on Hawkes Process with Weibull Base Intensity](http://arxiv.org/abs/2112.13581)


  In this paper, we use the Hawkes process to model the sequence of failure,
i.e., events of compressor station and conduct survival analysis on various
failure events of the compressor station. However, until now, nearly all
relevant literatures of the Hawkes point processes assume that the base
intensity of the conditional intensity function is time-invariant. This
assumption is apparently too harsh to be verified. For example, in the
practical application, including financial analysis, reliability analysis,
survival analysis and social network analysis, the base intensity of the truth
conditional intensity function is very likely to be time-varying. The constant
base intensity will not reflect the base probability of the failure occurring
over time. Thus, in order to solve this problem, in this paper, we propose a
new time-varying base intensity, for example, which is from Weibull
distribution. First, we introduce the base intensity from the Weibull
distribution, and then we propose an effective learning algorithm by maximum
likelihood estimator. Experiments on the constant base intensity synthetic
data, time-varying base intensity synthetic data, and real-world data show that
our method can learn the triggering patterns of the Hawkes processes and the
time-varying base intensity simultaneously and robustly. Experiments on the
real-world data reveal the Granger causality of different kinds of failures and
the base probability of failure varying over time.

    

### [[2112.13585] Learn Layer-wise Connections in Graph Neural Networks](http://arxiv.org/abs/2112.13585)


  In recent years, Graph Neural Networks (GNNs) have shown superior performance
on diverse applications on real-world datasets. To improve the model capacity
and alleviate the over-smoothing problem, several methods proposed to
incorporate the intermediate layers by layer-wise connections. However, due to
the highly diverse graph types, the performance of existing methods vary on
diverse graphs, leading to a need for data-specific layer-wise connection
methods. To address this problem, we propose a novel framework LLC (Learn
Layer-wise Connections) based on neural architecture search (NAS) to learn
adaptive connections among intermediate layers in GNNs. LLC contains one novel
search space which consists of 3 types of blocks and learnable connections, and
one differentiable search algorithm to enable the efficient search process.
Extensive experiments on five real-world datasets are conducted, and the
results show that the searched layer-wise connections can not only improve the
performance but also alleviate the over-smoothing problem.

    

### [[2112.13593] Multi-modal Attention Network for Stock Movements Prediction](http://arxiv.org/abs/2112.13593)


  Stock prices move as piece-wise trending fluctuation rather than a purely
random walk. Traditionally, the prediction of future stock movements is based
on the historical trading record. Nowadays, with the development of social
media, many active participants in the market choose to publicize their
strategies, which provides a window to glimpse over the whole market's attitude
towards future movements by extracting the semantics behind social media.
However, social media contains conflicting information and cannot replace
historical records completely. In this work, we propose a multi-modality
attention network to reduce conflicts and integrate semantic and numeric
features to predict future stock movements comprehensively. Specifically, we
first extract semantic information from social media and estimate their
credibility based on posters' identity and public reputation. Then we
incorporate the semantic from online posts and numeric features from historical
records to make the trading strategy. Experimental results show that our
approach outperforms previous methods by a significant margin in both
prediction accuracy (61.20\%) and trading profits (9.13\%). It demonstrates
that our method improves the performance of stock movements prediction and
informs future research on multi-modality fusion towards stock prediction.

    

### [[2112.13642] Extracting knowledge from features with multilevel abstraction](http://arxiv.org/abs/2112.13642)


  Knowledge distillation aims at transferring the knowledge from a large
teacher model to a small student model with great improvements of the
performance of the student model. Therefore, the student network can replace
the teacher network to deploy on low-resource devices since the higher
performance, lower number of parameters and shorter inference time.
Self-knowledge distillation (SKD) attracts a great attention recently that a
student model itself is a teacher model distilling knowledge from. To the best
of our knowledge, self knowledge distillation can be divided into two main
streams: data augmentation and refined knowledge auxiliary. In this paper, we
purpose a novel SKD method in a different way from the main stream methods. Our
method distills knowledge from multilevel abstraction features. Experiments and
ablation studies show its great effectiveness and generalization on various
kinds of tasks with various kinds of model structures. Our codes have been
released on GitHub.

    

### [[2112.13647] Move As You Like: Image Animation in E-Commerce Scenario](http://arxiv.org/abs/2112.13647)


  Creative image animations are attractive in e-commerce applications, where
motion transfer is one of the import ways to generate animations from static
images. However, existing methods rarely transfer motion to objects other than
human body or human face, and even fewer apply motion transfer in practical
scenarios. In this work, we apply motion transfer on the Taobao product images
in real e-commerce scenario to generate creative animations, which are more
attractive than static images and they will bring more benefits. We animate the
Taobao products of dolls, copper running horses and toy dinosaurs based on
motion transfer method for demonstration.

    

### [[1812.06638] AI-Aided Online Adaptive OFDM Receiver: Design and Experimental Results](http://arxiv.org/abs/1812.06638)


  Orthogonal frequency division multiplexing (OFDM) has been widely applied in
current communication systems. The artificial intelligence (AI)-aided OFDM
receivers are currently brought to the forefront to replace and improve the
traditional OFDM receivers. In this study, we first compare two AI-aided OFDM
receivers, namely, data-driven fully connected deep neural network and
model-driven ComNet, through extensive simulation and real-time video
transmission using a 5G rapid prototyping system for an over-the-air (OTA)
test. We find a performance gap between the simulation and the OTA test caused
by the discrepancy between the channel model for offline training and the real
environment. We develop a novel online training system, which is called
SwitchNet receiver, to address this issue. This receiver has a flexible and
extendable architecture and can adapt to real channels by training only several
parameters online. From the OTA test, the AI-aided OFDM receivers, especially
the SwitchNet receiver, are robust to real environments and promising for
future communication systems. We discuss potential challenges and future
research inspired by our initial study in this paper.

    

### [[1910.12241] Pre-train and Learn: Preserve Global Information for Graph Neural Networks](http://arxiv.org/abs/1910.12241)


  Graph neural networks (GNNs) have shown great power in learning on attributed
graphs. However, it is still a challenge for GNNs to utilize information
faraway from the source node. Moreover, general GNNs require graph attributes
as input, so they cannot be appled to plain graphs. In the paper, we propose
new models named G-GNNs (Global information for GNNs) to address the above
limitations. First, the global structure and attribute features for each node
are obtained via unsupervised pre-training, which preserve the global
information associated to the node. Then, using the global features and the raw
network attributes, we propose a parallel framework of GNNs to learn different
aspects from these features. The proposed learning methods can be applied to
both plain graphs and attributed graphs. Extensive experiments have shown that
G-GNNs can outperform other state-of-the-art models on three standard
evaluation graphs. Specially, our methods establish new benchmark records on
Cora (84.31\%) and Pubmed (80.95\%) when learning on attributed graphs.

    

### [[1911.01208] Higher Criticism for Discriminating Word-Frequency Tables and Testing Authorship](http://arxiv.org/abs/1911.01208)


  We adapt the Higher Criticism (HC) goodness-of-fit test to measure the
closeness between word-frequency tables. We apply this measure to authorship
attribution challenges, where the goal is to identify the author of a document
using other documents whose authorship is known. The method is simple yet
performs well without handcrafting and tuning; reporting accuracy at the state
of the art level in various current challenges. As an inherent side effect, the
HC calculation identifies a subset of discriminating words. In practice, the
identified words have low variance across documents belonging to a corpus of
homogeneous authorship. We conclude that in comparing the similarity of a new
document and a corpus of a single author, HC is mostly affected by words
characteristic of the author and is relatively unaffected by topic structure.

    

### [[1912.06875] Natural Actor-Critic Converges Globally for Hierarchical Linear Quadratic Regulator](http://arxiv.org/abs/1912.06875)


  Multi-agent reinforcement learning has been successfully applied to a number
of challenging problems. Despite these empirical successes, theoretical
understanding of different algorithms is lacking, primarily due to the curse of
dimensionality caused by the exponential growth of the state-action space with
the number of agents. We study a fundamental problem of multi-agent linear
quadratic regulator (LQR) in a setting where the agents are partially
exchangeable. In this setting, we develop a hierarchical actor-critic
algorithm, whose computational complexity is independent of the total number of
agents, and prove its global linear convergence to the optimal policy. As LQRs
are often used to approximate general dynamic systems, this paper provides an
important step towards a better understanding of general hierarchical
mean-field multi-agent reinforcement learning.

    

### [[1912.10762] Learning Variable Ordering Heuristics for Solving Constraint Satisfaction Problems](http://arxiv.org/abs/1912.10762)


  Backtracking search algorithms are often used to solve the Constraint
Satisfaction Problem (CSP). The efficiency of backtracking search depends
greatly on the variable ordering heuristics. Currently, the most commonly used
heuristics are hand-crafted based on expert knowledge. In this paper, we
propose a deep reinforcement learning based approach to automatically discover
new variable ordering heuristics that are better adapted for a given class of
CSP instances. We show that directly optimizing the search cost is hard for
bootstrapping, and propose to optimize the expected cost of reaching a leaf
node in the search tree. To capture the complex relations among the variables
and constraints, we design a representation scheme based on Graph Neural
Network that can process CSP instances with different sizes and constraint
arities. Experimental results on random CSP instances show that the learned
policies outperform classical hand-crafted heuristics in terms of minimizing
the search tree size, and can effectively generalize to instances that are
larger than those used in training.

    

### [[2002.12410] On Biased Compression for Distributed Learning](http://arxiv.org/abs/2002.12410)


  In the last few years, various communication compression techniques have
emerged as an indispensable tool helping to alleviate the communication
bottleneck in distributed learning. However, despite the fact {\em biased}
compressors often show superior performance in practice when compared to the
much more studied and understood {\em unbiased} compressors, very little is
known about them. In this work we study three classes of biased compression
operators, two of which are new, and their performance when applied to
(stochastic) gradient descent and distributed (stochastic) gradient descent. We
show for the first time that biased compressors can lead to linear convergence
rates both in the single node and distributed settings. We prove that
distributed compressed SGD method, employed with error feedback mechanism,
enjoys the ergodic rate $\mathcal{O}\left( \delta L \exp[-\frac{\mu K}{\delta
L}] + \frac{(C + \delta D)}{K\mu}\right)$, where $\delta\ge1$ is a compression
parameter which grows when more compression is applied, $L$ and $\mu$ are the
smoothness and strong convexity constants, $C$ captures stochastic gradient
noise ($C=0$ if full gradients are computed on each node) and $D$ captures the
variance of the gradients at the optimum ($D=0$ for over-parameterized models).
Further, via a theoretical study of several synthetic and empirical
distributions of communicated gradients, we shed light on why and by how much
biased compressors outperform their unbiased variants. Finally, we propose
several new biased compressors with promising theoretical guarantees and
practical performance.

    

### [[2003.06112] A Graph Convolutional Topic Model for Short and Noisy Text Streams](http://arxiv.org/abs/2003.06112)


  Learning hidden topics from data streams has become absolutely necessary but
posed challenging problems such as concept drift as well as short and noisy
data. Using prior knowledge to enrich a topic model is one of potential
solutions to cope with these challenges. Prior knowledge that is derived from
human knowledge (e.g. Wordnet) or a pre-trained model (e.g. Word2vec) is very
valuable and useful to help topic models work better. However, in a streaming
environment where data arrives continually and infinitely, existing studies are
limited to exploiting these resources effectively. Especially, a knowledge
graph, that contains meaningful word relations, is ignored. In this paper, to
aim at exploiting a knowledge graph effectively, we propose a novel graph
convolutional topic model (GCTM) which integrates graph convolutional networks
(GCN) into a topic model and a learning method which learns the networks and
the topic model simultaneously for data streams. In each minibatch, our method
not only can exploit an external knowledge graph but also can balance the
external and old knowledge to perform well on new data. We conduct extensive
experiments to evaluate our method with both a human knowledge graph (Wordnet)
and a graph built from pre-trained word embeddings (Word2vec). The experimental
results show that our method achieves significantly better performances than
state-of-the-art baselines in terms of probabilistic predictive measure and
topic coherence. In particular, our method can work well when dealing with
short texts as well as concept drift. The implementation of GCTM is available
at \url{this https URL}.

    

### [[2003.06123] Dynamic transformation of prior knowledge into Bayesian models for data streams](http://arxiv.org/abs/2003.06123)


  We consider how to effectively use prior knowledge when learning a Bayesian
model from streaming environments where the data come infinitely and
sequentially. This problem is highly important in the era of data explosion and
rich sources of precious external knowledge such as pre-trained models,
ontologies, Wikipedia, etc. We show that some existing approaches can forget
any knowledge very fast. We then propose a novel framework that enables to
incorporate the prior knowledge of different forms into a base Bayesian model
for data streams. Our framework subsumes some existing popular models for
time-series/dynamic data. Extensive experiments show that our framework
outperforms existing methods with a large margin. In particular, our framework
can help Bayesian models generalize well on extremely short text while other
methods overfit. The implementation of our framework is available at
this https URL.

    

### [[2003.08051] Unsupervised Domain Adaptation Through Transferring both the Source-Knowledge and Target-Relatedness Simultaneously](http://arxiv.org/abs/2003.08051)


  Unsupervised domain adaptation (UDA) is an emerging research topic in the
field of machine learning and pattern recognition, which aims to help the
learning of unlabeled target domain by transferring knowledge from the source
domain.

    

### [[2005.04544] Unified Models of Human Behavioral Agents in Bandits, Contextual Bandits and RL](http://arxiv.org/abs/2005.04544)


  Artificial behavioral agents are often evaluated based on their consistent
behaviors and performance to take sequential actions in an environment to
maximize some notion of cumulative reward. However, human decision making in
real life usually involves different strategies and behavioral trajectories
that lead to the same empirical outcome. Motivated by clinical literature of a
wide range of neurological and psychiatric disorders, we propose here a more
general and flexible parametric framework for sequential decision making that
involves a two-stream reward processing mechanism. We demonstrated that this
framework is flexible and unified enough to incorporate a family of problems
spanning multi-armed bandits (MAB), contextual bandits (CB) and reinforcement
learning (RL), which decompose the sequential decision making process in
different levels. Inspired by the known reward processing abnormalities of many
mental disorders, our clinically-inspired agents demonstrated interesting
behavioral trajectories and comparable performance on simulated tasks with
particular reward distributions, a real-world dataset capturing human
decision-making in gambling tasks, and the PacMan game across different reward
stationarities in a lifelong learning setting.

    

### [[2005.10624] Greedy Algorithm almost Dominates in Smoothed Contextual Bandits](http://arxiv.org/abs/2005.10624)


  Online learning algorithms, widely used to power search and content
optimization on the web, must balance exploration and exploitation, potentially
sacrificing the experience of current users in order to gain information that
will lead to better decisions in the future. While necessary in the worst case,
explicit exploration has a number of disadvantages compared to the greedy
algorithm that always "exploits" by choosing an action that currently looks
optimal. We ask under what conditions inherent diversity in the data makes
explicit exploration unnecessary. We build on a recent line of work on the
smoothed analysis of the greedy algorithm in the linear contextual bandits
model. We improve on prior results to show that a greedy approach almost
matches the best possible Bayesian regret rate of any other algorithm on the
same problem instance whenever the diversity conditions hold, and that this
regret is at most $\tilde O(T^{1/3})$.

    

### [[2005.12900] Breaking the Sample Size Barrier in Model-Based Reinforcement Learning with a Generative Model](http://arxiv.org/abs/2005.12900)


  This paper is concerned with the sample efficiency of reinforcement learning,
assuming access to a generative model (or simulator). We first consider
$\gamma$-discounted infinite-horizon Markov decision processes (MDPs) with
state space $\mathcal{S}$ and action space $\mathcal{A}$. Despite a number of
prior works tackling this problem, a complete picture of the trade-offs between
sample complexity and statistical accuracy is yet to be determined. In
particular, all prior results suffer from a severe sample size barrier, in the
sense that their claimed statistical guarantees hold only when the sample size
exceeds at least $\frac{|\mathcal{S}||\mathcal{A}|}{(1-\gamma)^2}$. The current
paper overcomes this barrier by certifying the minimax optimality of two
algorithms -- a perturbed model-based algorithm and a conservative model-based
algorithm -- as soon as the sample size exceeds the order of
$\frac{|\mathcal{S}||\mathcal{A}|}{1-\gamma}$ (modulo some log factor). Moving
beyond infinite-horizon MDPs, we further study time-inhomogeneous
finite-horizon MDPs, and prove that a plain model-based planning algorithm
suffices to achieve minimax-optimal sample complexity given any target accuracy
level. To the best of our knowledge, this work delivers the first
minimax-optimal guarantees that accommodate the entire range of sample sizes
(beyond which finding a meaningful policy is information theoretically
infeasible).

    

### [[2006.03134] Tensor Completion Made Practical](http://arxiv.org/abs/2006.03134)


  Tensor completion is a natural higher-order generalization of matrix
completion where the goal is to recover a low-rank tensor from sparse
observations of its entries. Existing algorithms are either heuristic without
provable guarantees, based on solving large semidefinite programs which are
impractical to run, or make strong assumptions such as requiring the factors to
be nearly orthogonal. In this paper we introduce a new variant of alternating
minimization, which in turn is inspired by understanding how the progress
measures that guide convergence of alternating minimization in the matrix
setting need to be adapted to the tensor setting. We show strong provable
guarantees, including showing that our algorithm converges linearly to the true
tensors even when the factors are highly correlated and can be implemented in
nearly linear time. Moreover our algorithm is also highly practical and we show
that we can complete third order tensors with a thousand dimensions from
observing a tiny fraction of its entries. In contrast, and somewhat
surprisingly, we show that the standard version of alternating minimization,
without our new twist, can converge at a drastically slower rate in practice.

    

### [[2006.03258] Learned Factor Graphs for Inference from Stationary Time Sequences](http://arxiv.org/abs/2006.03258)


  The design of methods for inference from time sequences has traditionally
relied on statistical models that describe the relation between a latent
desired sequence and the observed one. A broad family of model-based algorithms
have been derived to carry out inference at controllable complexity using
recursive computations over the factor graph representing the underlying
distribution. An alternative model-agnostic approach utilizes machine learning
(ML) methods. Here we propose a framework that combines model-based algorithms
and data-driven ML tools for stationary time sequences. In the proposed
approach, neural networks are developed to separately learn specific components
of a factor graph describing the distribution of the time sequence, rather than
the complete inference task. By exploiting stationary properties of this
distribution, the resulting approach can be applied to sequences of varying
temporal duration. Learned factor graph can be realized using compact neural
networks that are trainable using small training sets, or alternatively, be
used to improve upon existing deep inference systems. We present an inference
algorithm based on learned stationary factor graphs, which learns to implement
the sum-product scheme from labeled data, and can be applied to sequences of
different lengths. Our experimental results demonstrate the ability of the
proposed learned factor graphs to learn to carry out accurate inference from
small training sets for sleep stage detection using the Sleep-EDF dataset, as
well as for symbol detection in digital communications with unknown channels.

    

### [[2006.07046] Disentangled Representation Learning and Generation with Manifold Optimization](http://arxiv.org/abs/2006.07046)


  Disentanglement is a useful property in representation learning which
increases the interpretability of generative models such as Variational
Auto-Encoders (VAE), Generative Adversarial Models, and their many variants.
Typically in such models, an increase in disentanglement performance is
traded-off with generation quality. In the context of latent space models, this
work presents a representation learning framework that explicitly promotes
disentanglement by encouraging orthogonal directions of variations. The
proposed objective is the sum of an auto-encoder error term along with a
Principal Component Analysis reconstruction error in the feature space. This
has an interpretation of a Restricted Kernel Machine with the eigenvector
matrix valued on the Stiefel manifold. Our analysis shows that such a
construction promotes disentanglement by matching the principal directions in
the latent space with the directions of orthogonal variation in data space. In
an alternating minimization scheme, we use Cayley ADAM algorithm -- a
stochastic optimization method on the Stiefel manifold along with the ADAM
optimizer. Our theoretical discussion and various experiments show that the
proposed model improves over many VAE variants in terms of both generation
quality and disentangled representation learning.

    

### [[2006.09347] Understanding and Mitigating Exploding Inverses in Invertible Neural Networks](http://arxiv.org/abs/2006.09347)


  Invertible neural networks (INNs) have been used to design generative models,
implement memory-saving gradient computation, and solve inverse problems. In
this work, we show that commonly-used INN architectures suffer from exploding
inverses and are thus prone to becoming numerically non-invertible. Across a
wide range of INN use-cases, we reveal failures including the non-applicability
of the change-of-variables formula on in- and out-of-distribution (OOD) data,
incorrect gradients for memory-saving backprop, and the inability to sample
from normalizing flow models. We further derive bi-Lipschitz properties of
atomic building blocks of common architectures. These insights into the
stability of INNs then provide ways forward to remedy these failures. For tasks
where local invertibility is sufficient, like memory-saving backprop, we
propose a flexible and efficient regularizer. For problems where global
invertibility is necessary, such as applying normalizing flows on OOD data, we
show the importance of designing stable INN building blocks.

    

### [[2009.00804] Architectural Implications of Graph Neural Networks](http://arxiv.org/abs/2009.00804)


  Graph neural networks (GNN) represent an emerging line of deep learning
models that operate on graph structures. It is becoming more and more popular
due to its high accuracy achieved in many graph-related tasks. However, GNN is
not as well understood in the system and architecture community as its
counterparts such as multi-layer perceptrons and convolutional neural networks.
This work tries to introduce the GNN to our community. In contrast to prior
work that only presents characterizations of GCNs, our work covers a large
portion of the varieties for GNN workloads based on a general GNN description
framework. By constructing the models on top of two widely-used libraries, we
characterize the GNN computation at inference stage concerning general-purpose
and application-specific architectures and hope our work can foster more system
and architecture research for GNNs.

    

### [[2009.08474] Hierarchical Multi-Grained Generative Model for Expressive Speech Synthesis](http://arxiv.org/abs/2009.08474)


  This paper proposes a hierarchical generative model with a multi-grained
latent variable to synthesize expressive speech. In recent years, fine-grained
latent variables are introduced into the text-to-speech synthesis that enable
the fine control of the prosody and speaking styles of synthesized speech.
However, the naturalness of speech degrades when these latent variables are
obtained by sampling from the standard Gaussian prior. To solve this problem,
we propose a novel framework for modeling the fine-grained latent variables,
considering the dependence on an input text, a hierarchical linguistic
structure, and a temporal structure of latent variables. This framework
consists of a multi-grained variational autoencoder, a conditional prior, and a
multi-level auto-regressive latent converter to obtain the different
time-resolution latent variables and sample the finer-level latent variables
from the coarser-level ones by taking into account the input text. Experimental
results indicate an appropriate method of sampling fine-grained latent
variables without the reference signal at the synthesis stage. Our proposed
framework also provides the controllability of speaking style in an entire
utterance.

    

### [[2010.05234] A Practical Tutorial on Graph Neural Networks](http://arxiv.org/abs/2010.05234)


  Graph neural networks (GNNs) have recently grown in popularity in the field
of artificial intelligence (AI) due to their unique ability to ingest
relatively unstructured data types as input data. Although some elements of the
GNN architecture are conceptually similar in operation to traditional neural
networks (and neural network variants), other elements represent a departure
from traditional deep learning techniques. This tutorial exposes the power and
novelty of GNNs to AI practitioners by collating and presenting details
regarding the motivations, concepts, mathematics, and applications of the most
common and performant variants of GNNs. Importantly, we present this tutorial
concisely, alongside practical examples, thus providing a practical and
accessible tutorial on the topic of GNNs.

    

### [[2010.12870] Efficient Learning in Non-Stationary Linear Markov Decision Processes](http://arxiv.org/abs/2010.12870)


  We study episodic reinforcement learning in non-stationary linear (a.k.a.
low-rank) Markov Decision Processes (MDPs), i.e, both the reward and transition
kernel are linear with respect to a given feature map and are allowed to evolve
either slowly or abruptly over time. For this problem setting, we propose
OPT-WLSVI an optimistic model-free algorithm based on weighted least squares
value iteration which uses exponential weights to smoothly forget data that are
far in the past. We show that our algorithm, when competing against the best
policy at each time, achieves a regret that is upper bounded by
$\widetilde{\mathcal{O}}(d^{5/4}H^2 \Delta^{1/4} K^{3/4})$ where $d$ is the
dimension of the feature space, $H$ is the planning horizon, $K$ is the number
of episodes and $\Delta$ is a suitable measure of non-stationarity of the MDP.
Moreover, we point out technical gaps in the study of forgetting strategies in
non-stationary linear bandits setting made by previous works and we propose a
fix to their regret analysis.

    

### [[2011.01417] Non-Equilibrium Skewness, Market Crises, and Option Pricing: Non-Linear Langevin Model of Markets with Supersymmetry](http://arxiv.org/abs/2011.01417)


  This paper presents a tractable model of non-linear dynamics of market
returns using a Langevin approach. Due to non-linearity of an interaction
potential, the model admits regimes of both small and large return
fluctuations. Langevin dynamics are mapped onto an equivalent quantum
mechanical (QM) system. Borrowing ideas from supersymmetric quantum mechanics
(SUSY QM), a parameterized ground state wave function (WF) of this QM system is
used as a direct input to the model, which also fixes a non-linear Langevin
potential. Using a two-component Gaussian mixture as a ground state WF with an
asymmetric double well potential produces a tractable low-parametric model with
interpretable parameters, referred to as the NES (Non-Equilibrium Skew) model.
Supersymmetry (SUSY) is then used to find time-dependent solutions of the model
in an analytically tractable way. Additional approximations give rise to a
final practical version of the NES model, where real-measure and risk-neutral
return distributions are given by three component Gaussian mixtures. This
produces a closed-form approximation for option pricing in the NES model by a
mixture of three Black-Scholes prices, providing accurate calibration to option
prices for either benign or distressed market environments, while using only a
single volatility parameter. These results stand in stark contrast to the most
of other option pricing models such as local, stochastic, or rough volatility
models that need more complex specifications of noise to fit the market data.

    

### [[2012.14936] Learning Energy-Based Model with Variational Auto-Encoder as Amortized Sampler](http://arxiv.org/abs/2012.14936)


  Due to the intractable partition function, training energy-based models
(EBMs) by maximum likelihood requires Markov chain Monte Carlo (MCMC) sampling
to approximate the gradient of the Kullback-Leibler divergence between data and
model distributions. However, it is non-trivial to sample from an EBM because
of the difficulty of mixing between modes. In this paper, we propose to learn a
variational auto-encoder (VAE) to initialize the finite-step MCMC, such as
Langevin dynamics that is derived from the energy function, for efficient
amortized sampling of the EBM. With these amortized MCMC samples, the EBM can
be trained by maximum likelihood, which follows an "analysis by synthesis"
scheme; while the VAE learns from these MCMC samples via variational Bayes. We
call this joint training algorithm the variational MCMC teaching, in which the
VAE chases the EBM toward data distribution. We interpret the learning
algorithm as a dynamic alternating projection in the context of information
geometry. Our proposed models can generate samples comparable to GANs and EBMs.
Additionally, we demonstrate that our model can learn effective probabilistic
distribution toward supervised conditional learning tasks.

    

### [[2101.08732] Self-Adaptive Training: Bridging Supervised and Self-Supervised Learning](http://arxiv.org/abs/2101.08732)


  We propose self-adaptive training -- a unified training algorithm that
dynamically calibrates and enhances training processes by model predictions
without incurring an extra computational cost -- to advance both supervised and
self-supervised learning of deep neural networks. We analyze the training
dynamics of deep networks on training data that are corrupted by, e.g., random
noise and adversarial examples. Our analysis shows that model predictions are
able to magnify useful underlying information in data and this phenomenon
occurs broadly even in the absence of any label information, highlighting that
model predictions could substantially benefit the training processes:
self-adaptive training improves the generalization of deep networks under noise
and enhances the self-supervised representation learning. The analysis also
sheds light on understanding deep learning, e.g., a potential explanation of
the recently-discovered double-descent phenomenon in empirical risk
minimization and the collapsing issue of the state-of-the-art self-supervised
learning algorithms. Experiments on the CIFAR, STL, and ImageNet datasets
verify the effectiveness of our approach in three applications: classification
with label noise, selective classification, and linear evaluation. To
facilitate future research, the code has been made publicly available at
this https URL.

    

### [[2102.12680] Confidence Calibration with Bounded Error Using Transformations](http://arxiv.org/abs/2102.12680)


  As machine learning techniques become widely adopted in new domains,
especially in safety-critical systems such as autonomous vehicles, it is
crucial to provide accurate output uncertainty estimation. As a result, many
approaches have been proposed to calibrate neural networks to accurately
estimate the likelihood of misclassification. However, while these methods
achieve low calibration error, there is space for further improvement,
especially in large-dimensional settings such as ImageNet. In this paper, we
introduce a calibration algorithm, named Hoki, that works by applying random
transformations to the neural network logits. We provide a sufficient condition
for perfect calibration based on the number of label prediction changes
observed after applying the transformations. We perform experiments on multiple
datasets and show that the proposed approach generally outperforms
state-of-the-art calibration algorithms across multiple datasets and models,
especially on the challenging ImageNet dataset. Finally, Hoki is scalable as
well, as it requires comparable execution time to that of temperature scaling.

    

### [[2103.08017] Transient growth of accelerated optimization algorithms](http://arxiv.org/abs/2103.08017)


  Optimization algorithms are increasingly being used in applications with
limited time budgets. In many real-time and embedded scenarios, only a few
iterations can be performed and traditional convergence metrics cannot be used
to evaluate performance in these non-asymptotic regimes. In this paper, we
examine the transient behavior of accelerated first-order optimization
algorithms. For convex quadratic problems, we employ tools from linear systems
theory to show that transient growth arises from the presence of non-normal
dynamics. We identify the existence of modes that yield an algebraic growth in
early iterations and quantify the transient excursion from the optimal solution
caused by these modes. For strongly convex smooth optimization problems, we
utilize the theory of integral quadratic constraints (IQCs) to establish an
upper bound on the magnitude of the transient response of Nesterov's
accelerated algorithm. We show that both the Euclidean distance between the
optimization variable and the global minimizer and the rise time to the
transient peak are proportional to the square root of the condition number of
the problem. Finally, for problems with large condition numbers, we demonstrate
tightness of the bounds that we derive up to constant factors.

    

### [[2104.14210] FairDrop: Biased Edge Dropout for Enhancing Fairness in Graph Representation Learning](http://arxiv.org/abs/2104.14210)


  Graph representation learning has become a ubiquitous component in many
scenarios, ranging from social network analysis to energy forecasting in smart
grids. In several applications, ensuring the fairness of the node (or graph)
representations with respect to some protected attributes is crucial for their
correct deployment. Yet, fairness in graph deep learning remains
under-explored, with few solutions available. In particular, the tendency of
similar nodes to cluster on several real-world graphs (i.e., homophily) can
dramatically worsen the fairness of these procedures. In this paper, we propose
a novel biased edge dropout algorithm (FairDrop) to counter-act homophily and
improve fairness in graph representation learning. FairDrop can be plugged in
easily on many existing algorithms, is efficient, adaptable, and can be
combined with other fairness-inducing solutions. After describing the general
algorithm, we demonstrate its application on two benchmark tasks, specifically,
as a random walk model for producing node embeddings, and to a graph
convolutional network for link prediction. We prove that the proposed algorithm
can successfully improve the fairness of all models up to a small or negligible
drop in accuracy, and compares favourably with existing state-of-the-art
solutions. In an ablation study, we demonstrate that our algorithm can flexibly
interpolate between biasing towards fairness and an unbiased edge dropout.
Furthermore, to better evaluate the gains, we propose a new dyadic group
definition to measure the bias of a link prediction task when paired with
group-based fairness metrics. In particular, we extend the metric used to
measure the bias in the node embeddings to take into account the graph
structure.

    

### [[2104.14537] Towards Fair Classifiers Without Sensitive Attributes: Exploring Biases in Related Features](http://arxiv.org/abs/2104.14537)


  Despite the rapid development and great success of machine learning models,
extensive studies have exposed their disadvantage of inheriting latent
discrimination and societal bias from the training data. This phenomenon
hinders their adoption on high-stake applications. Thus, many efforts have been
taken for developing fair machine learning models. Most of them require that
sensitive attributes are available during training to learn fair models.
However, in many real-world applications, it is usually infeasible to obtain
the sensitive attributes due to privacy or legal issues, which challenges
existing fair-ensuring strategies. Though the sensitive attribute of each data
sample is unknown, we observe that there are usually some non-sensitive
features in the training data that are highly correlated with sensitive
attributes, which can be used to alleviate the bias. Therefore, in this paper,
we study a novel problem of exploring features that are highly correlated with
sensitive attributes for learning fair and accurate classifiers. We
theoretically show that by minimizing the correlation between these related
features and model prediction, we can learn a fair classifier. Based on this
motivation, we propose a novel framework which simultaneously uses these
related features for accurate prediction and enforces fairness. In addition,
the model can dynamically adjust the regularization weight of each related
feature to balance its contribution on model classification and fairness.
Experimental results on real-world datasets demonstrate the effectiveness of
the proposed model for learning fair models with high classification accuracy.

    

### [[2105.04663] GSPMD: General and Scalable Parallelization for ML Computation Graphs](http://arxiv.org/abs/2105.04663)


  We present GSPMD, an automatic, compiler-based parallelization system for
common machine learning computations. It allows users to write programs in the
same way as for a single device, then give hints through a few annotations on
how to distribute tensors, based on which GSPMD will parallelize the
computation. Its representation of partitioning is simple yet general, allowing
it to express different or mixed paradigms of parallelism on a wide variety of
models.
GSPMD infers the partitioning for every operator based on limited user
annotations, making it convenient to scale existing single-device programs. It
solves several technical challenges for production usage, allowing GSPMD to
achieve 50% to 62% compute utilization on up to 2048 Cloud TPUv3 cores for
models with up to one trillion parameters.

    

### [[2105.10909] Killing One Bird with Two Stones: Model Extraction and Attribute Inference Attacks against BERT-based APIs](http://arxiv.org/abs/2105.10909)


  The collection and availability of big data, combined with advances in
pre-trained models (e.g., BERT, XLNET, etc), have revolutionized the predictive
performance of modern natural language processing tasks, ranging from text
classification to text generation. This allows corporations to provide machine
learning as a service (MLaaS) by encapsulating fine-tuned BERT-based models as
APIs. However, BERT-based APIs have exhibited a series of security and privacy
vulnerabilities. For example, prior work has exploited the security issues of
the BERT-based APIs through the adversarial examples crafted by the extracted
model. However, the privacy leakage problems of the BERT-based APIs through the
extracted model have not been well studied. On the other hand, due to the high
capacity of BERT-based APIs, the fine-tuned model is easy to be overlearned,
but what kind of information can be leaked from the extracted model remains
unknown. In this work, we bridge this gap by first presenting an effective
model extraction attack, where the adversary can practically steal a BERT-based
API (the target/victim model) by only querying a limited number of queries. We
further develop an effective attribute inference attack which can infer the
sensitive attribute of the training data used by the BERT-based APIs. Our
extensive experiments on benchmark datasets under various realistic settings
validate the potential vulnerabilities of BERT-based APIs. Moreover, we
demonstrate that two promising defense methods become ineffective against our
attacks, which calls for more effective defense methods.

    

### [[2105.14016] Sample-Efficient Reinforcement Learning for Linearly-Parameterized MDPs with a Generative Model](http://arxiv.org/abs/2105.14016)


  The curse of dimensionality is a widely known issue in reinforcement learning
(RL). In the tabular setting where the state space $\mathcal{S}$ and the action
space $\mathcal{A}$ are both finite, to obtain a nearly optimal policy with
sampling access to a generative model, the minimax optimal sample complexity
scales linearly with $|\mathcal{S}|\times|\mathcal{A}|$, which can be
prohibitively large when $\mathcal{S}$ or $\mathcal{A}$ is large. This paper
considers a Markov decision process (MDP) that admits a set of state-action
features, which can linearly express (or approximate) its probability
transition kernel. We show that a model-based approach (resp.$~$Q-learning)
provably learns an $\varepsilon$-optimal policy (resp.$~$Q-function) with high
probability as soon as the sample size exceeds the order of
$\frac{K}{(1-\gamma)^{3}\varepsilon^{2}}$
(resp.$~$$\frac{K}{(1-\gamma)^{4}\varepsilon^{2}}$), up to some logarithmic
factor. Here $K$ is the feature dimension and $\gamma\in(0,1)$ is the discount
factor of the MDP. Both sample complexity bounds are provably tight, and our
result for the model-based approach matches the minimax lower bound. Our
results show that for arbitrarily large-scale MDP, both the model-based
approach and Q-learning are sample-efficient when $K$ is relatively small, and
hence the title of this paper.

    

### [[2106.15853] Understanding and Improving Early Stopping for Learning with Noisy Labels](http://arxiv.org/abs/2106.15853)


  The memorization effect of deep neural network (DNN) plays a pivotal role in
many state-of-the-art label-noise learning methods. To exploit this property,
the early stopping trick, which stops the optimization at the early stage of
training, is usually adopted. Current methods generally decide the early
stopping point by considering a DNN as a whole. However, a DNN can be
considered as a composition of a series of layers, and we find that the latter
layers in a DNN are much more sensitive to label noise, while their former
counterparts are quite robust. Therefore, selecting a stopping point for the
whole network may make different DNN layers antagonistically affected each
other, thus degrading the final performance. In this paper, we propose to
separate a DNN into different parts and progressively train them to address
this problem. Instead of the early stopping, which trains a whole DNN all at
once, we initially train former DNN layers by optimizing the DNN with a
relatively large number of epochs. During training, we progressively train the
latter DNN layers by using a smaller number of epochs with the preceding layers
fixed to counteract the impact of noisy labels. We term the proposed method as
progressive early stopping (PES). Despite its simplicity, compared with the
early stopping, PES can help to obtain more promising and stable results.
Furthermore, by combining PES with existing approaches on noisy label training,
we achieve state-of-the-art performance on image classification benchmarks.

    

### [[2106.15910] Graph Signal Restoration Using Nested Deep Algorithm Unrolling](http://arxiv.org/abs/2106.15910)


  Graph signal processing is a ubiquitous task in many applications such as
sensor, social, transportation and brain networks, point cloud processing, and
graph neural networks. Often, graph signals are corrupted in the sensing
process, thus requiring restoration. In this paper, we propose two graph signal
restoration methods based on deep algorithm unrolling (DAU). First, we present
a graph signal denoiser by unrolling iterations of the alternating direction
method of multiplier (ADMM). We then suggest a general restoration method for
linear degradation by unrolling iterations of Plug-and-Play ADMM (PnP-ADMM). In
the second approach, the unrolled ADMM-based denoiser is incorporated as a
submodule, leading to a nested DAU structure. The parameters in the proposed
denoising/restoration methods are trainable in an end-to-end manner. Our
approach is interpretable and keeps the number of parameters small since we
only tune graph-independent regularization parameters. We overcome two main
challenges in existing graph signal restoration methods: 1) limited performance
of convex optimization algorithms due to fixed parameters which are often
determined manually. 2) large number of parameters of graph neural networks
that result in difficulty of training. Several experiments for graph signal
denoising and interpolation are performed on synthetic and real-world data. The
proposed methods show performance improvements over several existing techniques
in terms of root mean squared error in both tasks.

    

### [[2108.00813] The CirCor DigiScope Dataset: From Murmur Detection to Murmur Classification](http://arxiv.org/abs/2108.00813)


  Cardiac auscultation is one of the most cost-effective techniques used to
detect and identify many heart conditions. Computer-assisted decision systems
based on auscultation can support physicians in their decisions. Unfortunately,
the application of such systems in clinical trials is still minimal since most
of them only aim to detect the presence of extra or abnormal waves in the
phonocardiogram signal, i.e., only a binary ground truth variable (normal vs
abnormal) is provided. This is mainly due to the lack of large publicly
available datasets, where a more detailed description of such abnormal waves
(e.g., cardiac murmurs) exists.
To pave the way to more effective research on healthcare recommendation
systems based on auscultation, our team has prepared the currently largest
pediatric heart sound dataset. A total of 5282 recordings have been collected
from the four main auscultation locations of 1568 patients, in the process,
215780 heart sounds have been manually annotated. Furthermore, and for the
first time, each cardiac murmur has been manually annotated by an expert
annotator according to its timing, shape, pitch, grading, and quality. In
addition, the auscultation locations where the murmur is present were
identified as well as the auscultation location where the murmur is detected
more intensively. Such detailed description for a relatively large number of
heart sounds may pave the way for new machine learning algorithms with a
real-world application for the detection and analysis of murmur waves for
diagnostic purposes.

    

### [[2110.11208] User-Level Private Learning via Correlated Sampling](http://arxiv.org/abs/2110.11208)


  Most works in learning with differential privacy (DP) have focused on the
setting where each user has a single sample. In this work, we consider the
setting where each user holds $m$ samples and the privacy protection is
enforced at the level of each user's data. We show that, in this setting, we
may learn with a much fewer number of users. Specifically, we show that, as
long as each user receives sufficiently many samples, we can learn any
privately learnable class via an $(\epsilon, \delta)$-DP algorithm using only
$O(\log(1/\delta)/\epsilon)$ users. For $\epsilon$-DP algorithms, we show that
we can learn using only $O_{\epsilon}(d)$ users even in the local model, where
$d$ is the probabilistic representation dimension. In both cases, we show a
nearly-matching lower bound on the number of users required.
A crucial component of our results is a generalization of global stability
[Bun et al., FOCS 2020] that allows the use of public randomness. Under this
relaxed notion, we employ a correlated sampling strategy to show that the
global stability can be boosted to be arbitrarily close to one, at a polynomial
expense in the number of samples.

    

### [[2111.01097] Code2Snapshot: Using Code Snapshots for Learning Representations of Source Code](http://arxiv.org/abs/2111.01097)


  There are several approaches to encode source code in the input vectors of
neural models. These approaches attempt to include various syntactic and
semantic features of input programs in their encoding. In this paper, we
investigate Code2Snapshot, a novel representation of the source code that is
based on the snapshots of input programs. We evaluate several variations of
this representation and compare its performance with state-of-the-art
representations that utilize the rich syntactic and semantic features of input
programs. Our preliminary study on the utility of Code2Snapshot in the code
summarization task suggests that simple snapshots of input programs have
comparable performance to the state-of-the-art representations. Interestingly,
obscuring the input programs have insignificant impacts on the Code2Snapshot
performance, suggesting that, for some tasks, neural models may provide high
performance by relying merely on the structure of input programs.

    

### [[2111.14609] An Investigation on Learning, Polluting, and Unlearning the Spam Emails for Lifelong Learning](http://arxiv.org/abs/2111.14609)


  Machine unlearning for security is studied in this context. Several spam
email detection methods exist, each of which employs a different algorithm to
detect undesired spam emails. But these models are vulnerable to attacks. Many
attackers exploit the model by polluting the data, which are trained to the
model in various ways. So to act deftly in such situations model needs to
readily unlearn the polluted data without the need for retraining. Retraining
is impractical in most cases as there is already a massive amount of data
trained to the model in the past, which needs to be trained again just for
removing a small amount of polluted data, which is often significantly less
than 1%. This problem can be solved by developing unlearning frameworks for all
spam detection models. In this research, unlearning module is integrated into
spam detection models that are based on Naive Bayes, Decision trees, and Random
Forests algorithms. To assess the benefits of unlearning over retraining, three
spam detection models are polluted and exploited by taking attackers' positions
and proving models' vulnerability. Reduction in accuracy and true positive
rates are shown in each case showing the effect of pollution on models. Then
unlearning modules are integrated into the models, and polluted data is
unlearned; on testing the models after unlearning, restoration of performance
is seen. Also, unlearning and retraining times are compared with different
pollution data sizes on all models. On analyzing the findings, it can be
concluded that unlearning is considerably superior to retraining. Results show
that unlearning is fast, easy to implement, easy to use, and effective.

    

### [[2112.02962] DANets: Deep Abstract Networks for Tabular Data Classification and Regression](http://arxiv.org/abs/2112.02962)


  Tabular data are ubiquitous in real world applications. Although many
commonly-used neural components (e.g., convolution) and extensible neural
networks (e.g., ResNet) have been developed by the machine learning community,
few of them were effective for tabular data and few designs were adequately
tailored for tabular data structures. In this paper, we propose a novel and
flexible neural component for tabular data, called Abstract Layer (AbstLay),
which learns to explicitly group correlative input features and generate
higher-level features for semantics abstraction. Also, we design a structure
re-parameterization method to compress AbstLay, thus reducing the computational
complexity by a clear margin in the reference phase. A special basic block is
built using AbstLays, and we construct a family of Deep Abstract Networks
(DANets) for tabular data classification and regression by stacking such
blocks. In DANets, a special shortcut path is introduced to fetch information
from raw tabular features, assisting feature interactions across different
levels. Comprehensive experiments on seven real-world tabular datasets show
that our AbstLay and DANets are effective for tabular data classification and
regression, and the computational complexity is superior to competitive
methods. Besides, we evaluate the performance gains of DANet as it goes deep,
verifying the extendibility of our method. Our code is available at
this https URL.

    

### [[2112.03126] Label-Efficient Semantic Segmentation with Diffusion Models](http://arxiv.org/abs/2112.03126)


  Denoising diffusion probabilistic models have recently received much research
attention since they outperform alternative approaches, such as GANs, and
currently provide state-of-the-art generative performance. The superior
performance of diffusion models has made them an appealing tool in several
applications, including inpainting, super-resolution, and semantic editing. In
this paper, we demonstrate that diffusion models can also serve as an
instrument for semantic segmentation, especially in the setup when labeled data
is scarce. In particular, for several pretrained diffusion models, we
investigate the intermediate activations from the networks that perform the
Markov step of the reverse diffusion process. We show that these activations
effectively capture the semantic information from an input image and appear to
be excellent pixel-level representations for the segmentation problem. Based on
these observations, we describe a simple segmentation method, which can work
even if only a few training images are provided. Our approach significantly
outperforms the existing alternatives on several datasets for the same amount
of human supervision.

    

### [[2112.06554] Ensemble CNN Networks for GBM Tumors Segmentation using Multi-parametric MRI](http://arxiv.org/abs/2112.06554)


  Glioblastomas are the most aggressive fast-growing primary brain cancer which
originate in the glial cells of the brain. Accurate identification of the
malignant brain tumor and its sub-regions is still one of the most challenging
problems in medical image segmentation. The Brain Tumor Segmentation Challenge
(BraTS) has been a popular benchmark for automatic brain glioblastomas
segmentation algorithms since its initiation. In this year, BraTS 2021
challenge provides the largest multi-parametric (mpMRI) dataset of 2,000
pre-operative patients. In this paper, we propose a new aggregation of two deep
learning frameworks namely, DeepSeg and nnU-Net for automatic glioblastoma
recognition in pre-operative mpMRI. Our ensemble method obtains Dice similarity
scores of 92.00, 87.33, and 84.10 and Hausdorff Distances of 3.81, 8.91, and
16.02 for the enhancing tumor, tumor core, and whole tumor regions,
respectively, on the BraTS 2021 validation set, ranking us among the top ten
teams. These experimental findings provide evidence that it can be readily
applied clinically and thereby aiding in the brain cancer prognosis, therapy
planning, and therapy response monitoring. A docker image for reproducing our
segmentation results is available online at
(this https URL).

    

### [[2104.02012] Graph Neural Networks Based Detection of Stealth False Data Injection Attacks in Smart Grids](http://arxiv.org/abs/2104.02012)


  False data injection attacks (FDIAs) represent a major class of attacks that
aim to break the integrity of measurements by injecting false data into the
smart metering devices in power grids. To the best of authors' knowledge, no
study has attempted to design a detector that automatically models the
underlying graph topology and spatially correlated measurement data of the
smart grids to better detect cyber attacks. The contributions of this paper to
detect and mitigate FDIAs are twofold. First, we present a generic, localized,
and stealth (unobservable) attack generation methodology and publicly
accessible datasets for researchers to develop and test their algorithms.
Second, we propose a Graph Neural Network (GNN) based, scalable and real-time
detector of FDIAs that efficiently combines model-driven and data-driven
approaches by incorporating the inherent physical connections of modern AC
power grids and exploiting the spatial correlations of the measurement. It is
experimentally verified by comparing the proposed GNN based detector with the
currently available FDIA detectors in the literature that our algorithm
outperforms the best available solutions by 3.14%, 4.25%, and 4.41% in F1 score
for standard IEEE testbeds with 14, 118, and 300 buses, respectively.

    

### [[2112.12836] Hardware Support for FPGA Resource Elasticity](http://arxiv.org/abs/2112.12836)


  FPGAs are increasingly being deployed in the cloud to accelerate diverse
applications. They are to be shared among multiple tenants to improve the total
cost of ownership. Partial reconfiguration technology enables multi-tenancy on
FPGA by partitioning it into regions, each hosting a specific application's
accelerator. However, the region's size can not be changed once they are
defined, resulting in the underutilization of FPGA resources. This paper argues
to divide the acceleration requirements of an application into multiple small
computation modules. The devised FPGA shell can reconfigure the available PR
regions with those modules and enable them to communicate with each other over
Crossbar interconnect with the Wishbone bus interface. For each PR region being
reconfigured, it updates the register file with the valid destination addresses
and the bandwidth allocation of the interconnect. Any invalid communication
request originating from the Wishbone master interface is masked in the
corresponding master port of the crossbar. The allocated bandwidth for the PR
region is ensured by the weighted round-robin arbiter in the slave port of the
crossbar. Finally, the envisioned resource manager can increase or decrease the
number of PR regions allocated to an application based on its acceleration
requirements and PR regions' availability.

    

### [[2112.13149] Fast and Scalable Computation of the Forward and Inverse Discrete Periodic Radon Transform](http://arxiv.org/abs/2112.13149)


  The Discrete Periodic Radon Transform (DPRT) has been extensively used in
applications that involve image reconstructions from projections. This
manuscript introduces a fast and scalable approach for computing the forward
and inverse DPRT that is based on the use of: (i) a parallel array of
fixed-point adder trees, (ii) circular shift registers to remove the need for
accessing external memory components when selecting the input data for the
adder trees, (iii) an image block-based approach to DPRT computation that can
fit the proposed architecture to available resources, and (iv) fast
transpositions that are computed in one or a few clock cycles that do not
depend on the size of the input image. As a result, for an $N\times N$ image
($N$ prime), the proposed approach can compute up to $N^{2}$ additions per
clock cycle. Compared to previous approaches, the scalable approach provides
the fastest known implementations for different amounts of computational
resources. For example, for a $251\times 251$ image, for approximately $25\%$
fewer flip-flops than required for a systolic implementation, we have that the
scalable DPRT is computed 36 times faster. For the fastest case, we introduce
optimized architectures that can compute the DPRT and its inverse in just
$2N+\left\lceil \log_{2}N\right\rceil+1$ and $2N+3\left\lceil
\log_{2}N\right\rceil+B+2$ cycles respectively, where $B$ is the number of bits
used to represent each input pixel. On the other hand, the scalable DPRT
approach requires more 1-bit additions than for the systolic implementation and
provides a trade-off between speed and additional 1-bit additions. All of the
proposed DPRT architectures were implemented in VHDL and validated using an
FPGA implementation.

    

### [[2112.13150] Fast 2D Convolutions and Cross-Correlations Using Scalable Architectures](http://arxiv.org/abs/2112.13150)


  The manuscript describes fast and scalable architectures and associated
algorithms for computing convolutions and cross-correlations. The basic idea is
to map 2D convolutions and cross-correlations to a collection of 1D
convolutions and cross-correlations in the transform domain. This is
accomplished through the use of the Discrete Periodic Radon Transform (DPRT)
for general kernels and the use of SVD-LU decompositions for low-rank kernels.
The approach uses scalable architectures that can be fitted into modern FPGA
and Zynq-SOC devices. Based on different types of available resources, for
$P\times P$ blocks, 2D convolutions and cross-correlations can be computed in
just $O(P)$ clock cycles up to $O(P^2)$ clock cycles. Thus, there is a
trade-off between performance and required numbers and types of resources. We
provide implementations of the proposed architectures using modern programmable
devices (Virtex-7 and Zynq-SOC). Based on the amounts and types of required
resources, we show that the proposed approaches significantly outperform
current methods.

    

### [[2112.13157] A Parallel SystemC Virtual Platform for Neuromorphic Architectures](http://arxiv.org/abs/2112.13157)


  With the increasing interest in neuromorphic computing, designers of embedded
systems face the challenge of efficiently simulating such platforms to enable
architecture design exploration early in the development cycle. Executing
artificial neural network applications on neuromorphic systems which are being
simulated on virtual platforms (VPs) is an extremely demanding computational
task. Nevertheless, it is a vital benchmarking task for comparing different
possible architectures. Therefore, exploiting the multicore capabilities of the
VP's host system is essential to achieve faster simulations. Hence, this paper
presents a parallel SystemC based VP for RISC-V multicore platforms integrating
multiple computing-in-memory neuromorphic accelerators. In this paper,
different VP segmentation architectures are explored for the integration of
neuromorphic accelerators and are shown their corresponding speedup simulations
compared to conventional sequential SystemC execution.

    

### [[2112.13306] Asynchronous Memory Access Unit for General Purpose Processors](http://arxiv.org/abs/2112.13306)


  In future data centers, applications will make heavy use of far memory
(including disaggregated memory pools and NVM). The access latency of far
memory is more widely distributed than that of local memory accesses. This
makes the efficiency of traditional blocking load/store in most general-purpose
processors decrease in this scenario. Therefore, this work proposes an in-core
asynchronous memory access unit.

    

### [[2112.13451] A Linear-Time Algorithm for Steady-State Analysis of Electromigration in General Interconnects](http://arxiv.org/abs/2112.13451)


  Electromigration (EM) is a key reliability issue in deeply scaled technology
nodes. Traditional EM methods first filter immortal wires using the Blech
criterion, and then perform EM analysis based on Black's equation on the
remaining wires. The Blech criterion is based on finding the steady-state
stress in a two-terminal wire segment, but most on-chip structures are
considerably more complex. Current-density-based assessment methodologies,
i.e., Black's equation and the Blech criterion, which are predominantly used to
detect EM-susceptible wires, do not capture the physics of EM, but alternative
physics-based methods involve the solution of differential equations and are
slow. This paper uses first principles, based on solving fundamental stress
equations that relate electron wind and back-stress forces to the stress
evolution in an interconnect, and devises a technique that analyzes any general
tree or mesh interconnect structure to test for immortality. The resulting
solution is extremely computationally efficient and its computation time is
linear in the number of metal segments. Two variants of the method are
proposed: a current-density-based method that requires traversals of the
interconnect graph, and a voltage-based formulation negates the need for any
traversals. The methods are applied to large interconnect networks for
determining the steady-state stress at all nodes and test all segments of each
network for immortality. The proposed model is applied to a variety of tree and
mesh structures and is demonstrated to be fast. By construction, it is an exact
solution and it is demonstrated to match much more computationally expensive
numerical simulations.

    

### [[2106.08167] ShortcutFusion: From Tensorflow to FPGA-based accelerator with reuse-aware memory allocation for shortcut data](http://arxiv.org/abs/2106.08167)


  Residual block is a very common component in recent state-of-the art CNNs
such as EfficientNet or EfficientDet. Shortcut data accounts for nearly 40% of
feature-maps access in ResNet152 [8]. Most of the previous DNN compilers,
accelerators ignore the shortcut data optimization. This paper presents
ShortcutFusion, an optimization tool for FPGA-based accelerator with a
reuse-aware static memory allocation for shortcut data, to maximize on-chip
data reuse given resource constraints. From TensorFlow DNN models, the proposed
design generates instruction sets for a group of nodes which uses an optimized
data reuse for each residual block. The accelerator design implemented on the
Xilinx KCU1500 FPGA card 2.8x faster and 9.9x more power efficient than NVIDIA
RTX 2080 Ti for 256x256 input size. . Compared to the result from baseline, in
which the weights, inputs, and outputs are accessed from the off-chip memory
exactly once per each layer, ShortcutFusion reduces the DRAM access by
47.8-84.8% for RetinaNet, Yolov3, ResNet152, and EfficientNet. Given a similar
buffer size to ShortcutMining [8], which also mine the shortcut data in
hardware, the proposed work reduces off-chip access for feature-maps 5.27x
while accessing weight from off-chip memory exactly once.

    

### [[2112.12921] The Serverless Computing Survey: A Technical Primer for Design Architecture](http://arxiv.org/abs/2112.12921)


  The development of cloud infrastructures inspires the emergence of
cloud-native computing. As the most promising architecture for deploying
microservices, serverless computing has recently attracted more and more
attention in both industry and academia. Due to its inherent scalability and
flexibility, serverless computing becomes attractive and more pervasive for
ever-growing Internet services. Despite the momentum in the cloud-native
community, the existing challenges and compromises still wait for more advanced
research and solutions to further explore the potentials of the serverless
computing model. As a contribution to this knowledge, this article surveys and
elaborates the research domains in the serverless context by decoupling the
architecture into four stack layers: Virtualization, Encapsule, System
Orchestration, and System Coordination. Inspired by the security model, we
highlight the key implications and limitations of these works in each layer,
and make suggestions for potential challenges to the field of future serverless
computing.

    

### [[2112.12946] Redy: Remote Dynamic Memory Cache](http://arxiv.org/abs/2112.12946)


  Redy is a cloud service that provides high performance caches using
RDMA-accessible remote memory. An application can customize the performance of
each cache with a service level objective (SLO) for latency and throughput. By
using remote memory, it can leverage stranded memory and spot VM instances to
reduce the cost of its caches and improve data center resource utilization.
Redy automatically customizes the resource configuration for the given SLO,
handles the dynamics of remote memory regions, and recovers from failures. The
experimental evaluation shows that Redy can deliver its promised performance
and robustness under remote memory dynamics in the cloud. We augment a
production key-value store, FASTER, with a Redy cache. When the working set
exceeds local memory, using Redy is significantly faster than spilling to SSDs.

    

### [[2112.13354] Large-scale Machine Learning Cluster Scheduling via Multi-agent Graph Reinforcement Learning](http://arxiv.org/abs/2112.13354)


  Efficient scheduling of distributed deep learning (DL) jobs in large GPU
clusters is crucial for resource efficiency and job performance. While server
sharing among jobs improves resource utilization, interference among co-located
DL jobs occurs due to resource contention. Interference-aware job placement has
been studied, with white-box approaches based on explicit interference modeling
and black-box schedulers with reinforcement learning. In today's clusters
containing thousands of GPU servers, running a single scheduler to manage all
arrival jobs in a timely and effective manner is challenging, due to the large
workload scale. We adopt multiple schedulers in a large-scale cluster/data
center, and propose a multi-agent reinforcement learning (MARL) scheduling
framework to cooperatively learn fine-grained job placement policies, towards
the objective of minimizing job completion time (JCT). To achieve
topology-aware placements, our proposed framework uses hierarchical graph
neural networks to encode the data center topology and server architecture. In
view of a common lack of precise reward samples corresponding to different
placements, a job interference model is further devised to predict interference
levels in face of various co-locations, for training of the MARL schedulers.
Testbed and trace-driven evaluations show that our scheduler framework
outperforms representative scheduling schemes by more than 20% in terms of
average JCT, and is adaptive to various machine learning cluster topologies.

    

### [[2112.13449] Tree exploration in dual-memory model](http://arxiv.org/abs/2112.13449)


  We study the problem of online tree exploration by a deterministic mobile
agent. Our main objective is to establish what features of the model of the
mobile agent and the environment allow linear exploration time. We study agents
that, upon entering to a node, do not receive as input the edge via which they
entered. In such a model, deterministic memoryless exploration is infeasible,
hence the agent needs to be allowed to use some memory. The memory can be
located at the agent or at each node. The existing lower bounds show that if
the memory is either only at the agent or only at the nodes, then the
exploration needs superlinear time. We show that tree exploration in
dual-memory model, with constant memory at the agent and logarithmic at each
node is possible in linear time when one of two additional features is present:
fixed initial state of the memory at each node (so called clean memory) or a
single movable token. We present two algorithms working in linear time for
arbitrary trees in these two models. On the other hand, in our lower bound we
show that if the agent has a single bit of memory and one bit is present at
each node, then exploration may require quadratic time on paths, if the initial
memory at nodes could be set arbitrarily (so called dirty memory). This shows
that having clean node memory or a token allows linear exploration of trees in
the model with two types of memory, but having neither of those features may
lead to quadratic exploration time even on a simple path.

    

### [[2112.13509] Automatic Configuration for Optimal Communication Scheduling in DNN Training](http://arxiv.org/abs/2112.13509)


  ByteScheduler partitions and rearranges tensor transmissions to improve the
communication efficiency of distributed Deep Neural Network (DNN) training. The
configuration of hyper-parameters (i.e., the partition size and the credit
size) is critical to the effectiveness of partitioning and rearrangement.
Currently, ByteScheduler adopts Bayesian Optimization (BO) to find the optimal
configuration for the hyper-parameters beforehand. In practice, however,
various runtime factors (e.g., worker node status and network conditions)
change over time, making the statically-determined one-shot configuration
result suboptimal for real-world DNN training. To address this problem, we
present a real-time configuration method (called AutoByte) that automatically
and timely searches the optimal hyper-parameters as the training systems
dynamically change. AutoByte extends the ByteScheduler framework with a
meta-network, which takes the system's runtime statistics as its input and
outputs predictions for speedups under specific configurations. Evaluation
results on various DNN models show that AutoByte can dynamically tune the
hyper-parameters with low resource usage, and deliver up to 33.2\% higher
performance than the best static configuration in ByteScheduler.

    

### [[2112.13594] Universal Randomized Guessing Subjected to Distortion](http://arxiv.org/abs/2112.13594)


  In this paper, we consider the problem of guessing a sequence subject to a
distortion constraint. Specifically, we assume the following game between Alice
and Bob: Alice has a sequence $\bx$ of length $n$. Bob wishes to guess $\bx$,
yet he is satisfied with finding any sequence $\hat{\bx}$ which is within a
given distortion $D$ from $\bx$. Thus, he successively submits queries to
Alice, until receiving an affirmative answer, stating that his guess was within
the required distortion.
Finding guessing strategies which minimize the number of guesses (the
\emph{guesswork}), and analyzing its properties (e.g., its $\rho$--th moment)
has several applications in information security, source and channel coding.
Guessing subject to a distortion constraint is especially useful when
considering contemporary biometrically--secured systems, where the "password"
which protects the data is not a single, fixed vector but rather a \emph{ball
of feature vectors} centered at some $\bx$, and any feature vector within the
ball results in acceptance.
We formally define the guessing problem under distortion in \emph{four
different setups}: memoryless sources, guessing through a noisy channel,
sources with memory and individual sequences. We suggest a randomized guessing
strategy which is asymptotically optimal for all setups and is \emph{five--fold
universal}, as it is independent of the source statistics, the channel, the
moment to be optimized, the distortion measure and the distortion level.

    

### [[2112.13650] Multiagent Transition Systems: Protocol-Stack Mathematics for Distributed Computing](http://arxiv.org/abs/2112.13650)


  Presently, the practice of distributed computing is such that problems exist
in a mathematical realm different from their solutions. Here, we present a
novel mathematical realm, termed multiagent transition systems, that aims to
accommodate both distributed computing problems and their solutions. A problem
is presented as a specification -- a multiagent transition system -- and a
solution as an implementation of the specification by another, lower-level
multiagent transition systems.
This duality of roles of a multiagent transition system can be exploited all
the way from a high-level distributed computing problem description down to an
agreed-upon base layer, say TCP/IP, resulting in a mathematical protocol stack
where each protocol is implemented by the one below it. Correct implementations
are compositional and thus provide also an implementation of a protocol stack
as a whole. The framework also offers a formal, yet natural, notion of faults
and their resilience. Several applications of this mathematical framework are
underway.
As an illustration of the power of the approach, we provide multiagent
transition systems specifying a centralized single-chain protocol and a
distributed longest-chain protocol, show that the single-chain protocol is
universal in that it can implement any centralized multiagent transition
system, show an implementation of this protocol by the longest-chain protocol,
and conclude -- via the compositionality of correct implementations -- that the
distributed longest-chain protocol is universal for centralized multiagent
transition systems.

    

### [[1701.00854] Is Parallel Programming Hard, And, If So, What Can You Do About It? (Release v2021.12.22a)](http://arxiv.org/abs/1701.00854)


  The purpose of this book is to help you program shared-memory parallel
systems without risking your sanity. Nevertheless, you should think of the
information in this book as a foundation on which to build, rather than as a
completed cathedral. Your mission, if you choose to accept, is to help make
further progress in the exciting field of parallel programming-progress that
will in time render this book obsolete.
Parallel programming in the 21st century is no longer focused solely on
science, research, and grand-challenge projects. And this is all to the good,
because it means that parallel programming is becoming an engineering
discipline. Therefore, as befits an engineering discipline, this book examines
specific parallel-programming tasks and describes how to approach them. In some
surprisingly common cases, these tasks can be automated.
This book is written in the hope that presenting the engineering discipline
underlying successful parallel-programming projects will free a new generation
of parallel hackers from the need to slowly and painstakingly reinvent old
wheels, enabling them to instead focus their energy and creativity on new
frontiers. However, what you get from this book will be determined by what you
put into it. It is hoped that simply reading this book will be helpful, and
that working the Quick Quizzes will be even more helpful. However, the best
results come from applying the techniques taught in this book to real-life
problems. As always, practice makes perfect.
But no matter how you approach it, we sincerely hope that parallel
programming brings you at least as much fun, excitement, and challenge that it
has brought to us!

    

### [[2112.12808] MISO hierarchical inference engine with fuzzy implication satisfying I(A(x, y), z) = I(x, I(y, z))](http://arxiv.org/abs/2112.12808)


  Fuzzy inference engine, as one of the most important components of fuzzy
systems, can obtain some meaningful outputs from fuzzy sets on input space and
fuzzy rule base using fuzzy logic inference methods. In order to enhance the
computational efficiency of fuzzy inference engine in multi-input-single-output
(MISO) fuzzy systems, this paper aims mainly to investigate three MISO fuzzy
hierarchial inference engines based on fuzzy implications satisfying the law of
importation with aggregation functions (LIA). We firstly find some aggregation
functions for well-known fuzzy implications such that they satisfy (LIA) with
them. For a given aggregation function, the fuzzy implication which satisfies
(LIA) with this aggregation function is then characterized. Finally, we
construct three fuzzy hierarchical inference engines in MISO fuzzy systems
applying aforementioned theoretical developments.

    

### [[2112.12876] Learning to Walk with Dual Agents for Knowledge Graph Reasoning](http://arxiv.org/abs/2112.12876)


  Graph walking based on reinforcement learning (RL) has shown great success in
navigating an agent to automatically complete various reasoning tasks over an
incomplete knowledge graph (KG) by exploring multi-hop relational paths.
However, existing multi-hop reasoning approaches only work well on short
reasoning paths and tend to miss the target entity with the increasing path
length. This is undesirable for many reason-ing tasks in real-world scenarios,
where short paths connecting the source and target entities are not available
in incomplete KGs, and thus the reasoning performances drop drastically unless
the agent is able to seek out more clues from longer paths. To address the
above challenge, in this paper, we propose a dual-agent reinforcement learning
framework, which trains two agents (GIANT and DWARF) to walk over a KG jointly
and search for the answer collaboratively. Our approach tackles the reasoning
challenge in long paths by assigning one of the agents (GIANT) searching on
cluster-level paths quickly and providing stage-wise hints for another agent
(DWARF). Finally, experimental results on several KG reasoning benchmarks show
that our approach can search answers more accurately and efficiently, and
outperforms existing RL-based methods for long path queries by a large margin.

    

### [[2112.12886] Rediscovering Affordance: A Reinforcement Learning Perspective](http://arxiv.org/abs/2112.12886)


  Affordance refers to the perception of possible actions allowed by an object.
Despite its relevance to human-computer interaction, no existing theory
explains the mechanisms that underpin affordance-formation; that is, how
affordances are discovered and adapted via interaction. We propose an
integrative theory of affordance-formation based on the theory of reinforcement
learning in cognitive sciences. The key assumption is that users learn to
associate promising motor actions to percepts via experience when reinforcement
signals (success/failure) are present. They also learn to categorize actions
(e.g., ``rotating'' a dial), giving them the ability to name and reason about
affordance. Upon encountering novel widgets, their ability to generalize these
actions determines their ability to perceive affordances. We implement this
theory in a virtual robot model, which demonstrates human-like adaptation of
affordance in interactive widgets tasks. While its predictions align with
trends in human data, humans are able to adapt affordances faster, suggesting
the existence of additional mechanisms.

    

### [[2112.12910] Towards Understanding Human Functional Brain Development with Explainable Artificial Intelligence: Challenges and Perspectives](http://arxiv.org/abs/2112.12910)


  The last decades have seen significant advancements in non-invasive
neuroimaging technologies that have been increasingly adopted to examine human
brain development. However, these improvements have not necessarily been
followed by more sophisticated data analysis measures that are able to explain
the mechanisms underlying functional brain development. For example, the shift
from univariate (single area in the brain) to multivariate (multiple areas in
brain) analysis paradigms is of significance as it allows investigations into
the interactions between different brain regions. However, despite the
potential of multivariate analysis to shed light on the interactions between
developing brain regions, artificial intelligence (AI) techniques applied
render the analysis non-explainable. The purpose of this paper is to understand
the extent to which current state-of-the-art AI techniques can inform
functional brain development. In addition, a review of which AI techniques are
more likely to explain their learning based on the processes of brain
development as defined by developmental cognitive neuroscience (DCN) frameworks
is also undertaken. This work also proposes that eXplainable AI (XAI) may
provide viable methods to investigate functional brain development as
hypothesised by DCN frameworks.

    

### [[2112.12916] Visual Semantics Allow for Textual Reasoning Better in Scene Text Recognition](http://arxiv.org/abs/2112.12916)


  Existing Scene Text Recognition (STR) methods typically use a language model
to optimize the joint probability of the 1D character sequence predicted by a
visual recognition (VR) model, which ignore the 2D spatial context of visual
semantics within and between character instances, making them not generalize
well to arbitrary shape scene text. To address this issue, we make the first
attempt to perform textual reasoning based on visual semantics in this paper.
Technically, given the character segmentation maps predicted by a VR model, we
construct a subgraph for each instance, where nodes represent the pixels in it
and edges are added between nodes based on their spatial similarity. Then,
these subgraphs are sequentially connected by their root nodes and merged into
a complete graph. Based on this graph, we devise a graph convolutional network
for textual reasoning (GTR) by supervising it with a cross-entropy loss. GTR
can be easily plugged in representative STR models to improve their performance
owing to better textual reasoning. Specifically, we construct our model, namely
S-GTR, by paralleling GTR to the language model in a segmentation-based STR
baseline, which can effectively exploit the visual-linguistic complementarity
via mutual learning. S-GTR sets new state-of-the-art on six challenging STR
benchmarks and generalizes well to multi-linguistic datasets. Code is available
at this https URL.

    

### [[2112.12926] nvBench: A Large-Scale Synthesized Dataset for Cross-Domain Natural Language to Visualization Task](http://arxiv.org/abs/2112.12926)


  NL2VIS - which translates natural language (NL) queries to corresponding
visualizations (VIS) - has attracted more and more attention both in commercial
visualization vendors and academic researchers. In the last few years, the
advanced deep learning-based models have achieved human-like abilities in many
natural language processing (NLP) tasks, which clearly tells us that the deep
learning-based technique is a good choice to push the field of NL2VIS. However,
a big balk is the lack of benchmarks with lots of (NL, VIS) pairs. We present
nvBench, the first large-scale NL2VIS benchmark, containing 25,750 (NL, VIS)
pairs from 750 tables over 105 domains, synthesized from (NL, SQL) benchmarks
to support cross-domain NL2VIS task. The quality of nvBench has been
extensively validated by 23 experts and 300+ crowd workers. Deep learning-based
models training using nvBench demonstrate that nvBench can push the field of
NL2VIS.

    

### [[2112.12940] Analyzing Scientific Publications using Domain-Specific Word Embedding and Topic Modelling](http://arxiv.org/abs/2112.12940)


  The scientific world is changing at a rapid pace, with new technology being
developed and new trends being set at an increasing frequency. This paper
presents a framework for conducting scientific analyses of academic
publications, which is crucial to monitor research trends and identify
potential innovations. This framework adopts and combines various techniques of
Natural Language Processing, such as word embedding and topic modelling. Word
embedding is used to capture semantic meanings of domain-specific words. We
propose two novel scientific publication embedding, i.e., PUB-G and PUB-W,
which are capable of learning semantic meanings of general as well as
domain-specific words in various research fields. Thereafter, topic modelling
is used to identify clusters of research topics within these larger research
fields. We curated a publication dataset consisting of two conferences and two
journals from 1995 to 2020 from two research domains. Experimental results show
that our PUB-G and PUB-W embeddings are superior in comparison to other
baseline embeddings by a margin of ~0.18-1.03 based on topic coherence.

    

### [[2112.12955] Deep ensembles in bioimage segmentation](http://arxiv.org/abs/2112.12955)


  Semantic segmentation consists in classifying each pixel of an image by
assigning it to a specific label chosen from a set of all the available ones.
During the last few years, a lot of attention shifted to this kind of task.
Many computer vision researchers tried to apply autoencoder structures to
develop models that can learn the semantics of the image as well as a low-level
representation of it. In an autoencoder architecture, given an input, an
encoder computes a low dimensional representation of the input that is then
used by a decoder to reconstruct the original data. In this work, we propose an
ensemble of convolutional neural networks (CNNs). In ensemble methods, many
different models are trained and then used for classification, the ensemble
aggregates the outputs of the single classifiers. The approach leverages on
differences of various classifiers to improve the performance of the whole
system. Diversity among the single classifiers is enforced by using different
loss functions. In particular, we present a new loss function that results from
the combination of Dice and Structural Similarity Index. The proposed ensemble
is implemented by combining different backbone networks using the DeepLabV3+
and HarDNet environment. The proposal is evaluated through an extensive
empirical evaluation on two real-world scenarios: polyp and skin segmentation.
All the code is available online at this https URL.

    

### [[2112.12990] Deep Neuroevolution Squeezes More out of Small Neural Networks and Small Training Sets: Sample Application to MRI Brain Sequence Classification](http://arxiv.org/abs/2112.12990)


  Purpose: Deep Neuroevolution (DNE) holds the promise of providing radiology
artificial intelligence (AI) that performs well with small neural networks and
small training sets. We seek to realize this potential via a proof-of-principle
application to MRI brain sequence classification.
Methods: We analyzed a training set of 20 patients, each with four
sequences/weightings: T1, T1 post-contrast, T2, and T2-FLAIR. We trained the
parameters of a relatively small convolutional neural network (CNN) as follows:
First, we randomly mutated the CNN weights. We then measured the CNN training
set accuracy, using the latter as the fitness evaluation metric. The fittest
child CNNs were identified. We incorporated their mutations into the parent
CNN. This selectively mutated parent became the next generation's parent CNN.
We repeated this process for approximately 50,000 generations.
Results: DNE achieved monotonic convergence to 100% training set accuracy.
DNE also converged monotonically to 100% testing set accuracy.
Conclusions: DNE can achieve perfect accuracy with small training sets and
small CNNs. Particularly when combined with Deep Reinforcement Learning, DNE
may provide a path forward in the quest to make radiology AI more human-like in
its ability to learn. DNE may very well turn out to be a key component of the
much-anticipated meta-learning regime of radiology AI algorithms that can adapt
to new tasks and new image types, similar to human radiologists.

    

### [[2112.13060] NIP: Neuron-level Inverse Perturbation Against Adversarial Attacks](http://arxiv.org/abs/2112.13060)


  Although deep learning models have achieved unprecedented success, their
vulnerabilities towards adversarial attacks have attracted increasing
attention, especially when deployed in security-critical domains. To address
the challenge, numerous defense strategies, including reactive and proactive
ones, have been proposed for robustness improvement. From the perspective of
image feature space, some of them cannot reach satisfying results due to the
shift of features. Besides, features learned by models are not directly related
to classification results. Different from them, We consider defense method
essentially from model inside and investigated the neuron behaviors before and
after attacks. We observed that attacks mislead the model by dramatically
changing the neurons that contribute most and least to the correct label.
Motivated by it, we introduce the concept of neuron influence and further
divide neurons into front, middle and tail part. Based on it, we propose
neuron-level inverse perturbation(NIP), the first neuron-level reactive defense
method against adversarial attacks. By strengthening front neurons and
weakening those in the tail part, NIP can eliminate nearly all adversarial
perturbations while still maintaining high benign accuracy. Besides, it can
cope with different sizes of perturbations via adaptivity, especially larger
ones. Comprehensive experiments conducted on three datasets and six models show
that NIP outperforms the state-of-the-art baselines against eleven adversarial
attacks. We further provide interpretable proofs via neuron activation and
visualization for better understanding.

    

### [[2112.13064] CatchBackdoor: Backdoor Testing by Critical Trojan Neural Path Identification via Differential Fuzzing](http://arxiv.org/abs/2112.13064)


  The success of deep neural networks (DNNs) in real-world applications has
benefited from abundant pre-trained models. However, the backdoored pre-trained
models can pose a significant trojan threat to the deployment of downstream
DNNs. Existing DNN testing methods are mainly designed to find incorrect corner
case behaviors in adversarial settings but fail to discover the backdoors
crafted by strong trojan attacks. Observing the trojan network behaviors shows
that they are not just reflected by a single compromised neuron as proposed by
previous work but attributed to the critical neural paths in the activation
intensity and frequency of multiple neurons. This work formulates the DNN
backdoor testing and proposes the CatchBackdoor framework. Via differential
fuzzing of critical neurons from a small number of benign examples, we identify
the trojan paths and particularly the critical ones, and generate backdoor
testing examples by simulating the critical neurons in the identified paths.
Extensive experiments demonstrate the superiority of CatchBackdoor, with higher
detection performance than existing methods. CatchBackdoor works better on
detecting backdoors by stealthy blending and adaptive attacks, which existing
methods fail to detect. Moreover, our experiments show that CatchBackdoor may
reveal the potential backdoors of models in Model Zoo.

    

### [[2112.13076] Virtuoso: Video-based Intelligence for real-time tuning on SOCs](http://arxiv.org/abs/2112.13076)


  Efficient and adaptive computer vision systems have been proposed to make
computer vision tasks, such as image classification and object detection,
optimized for embedded or mobile devices. These solutions, quite recent in
their origin, focus on optimizing the model (a deep neural network, DNN) or the
system by designing an adaptive system with approximation knobs. In spite of
several recent efforts, we show that existing solutions suffer from two major
drawbacks. First, the system does not consider energy consumption of the models
while making a decision on which model to run. Second, the evaluation does not
consider the practical scenario of contention on the device, due to other
co-resident workloads. In this work, we propose an efficient and adaptive video
object detection system, Virtuoso, which is jointly optimized for accuracy,
energy efficiency, and latency. Underlying Virtuoso is a multi-branch execution
kernel that is capable of running at different operating points in the
accuracy-energy-latency axes, and a lightweight runtime scheduler to select the
best fit execution branch to satisfy the user requirement. To fairly compare
with Virtuoso, we benchmark 15 state-of-the-art or widely used protocols,
including Faster R-CNN (FRCNN), YOLO v3, SSD, EfficientDet, SELSA, MEGA, REPP,
FastAdapt, and our in-house adaptive variants of FRCNN+, YOLO+, SSD+, and
EfficientDet+ (our variants have enhanced efficiency for mobiles). With this
comprehensive benchmark, Virtuoso has shown superiority to all the above
protocols, leading the accuracy frontier at every efficiency level on NVIDIA
Jetson mobile GPUs. Specifically, Virtuoso has achieved an accuracy of 63.9%,
which is more than 10% higher than some of the popular object detection models,
FRCNN at 51.1%, and YOLO at 49.5%.

    

### [[2112.13144] SoK: A Study of the Security on Voice Processing Systems](http://arxiv.org/abs/2112.13144)


  As the use of Voice Processing Systems (VPS) continues to become more
prevalent in our daily lives through the increased reliance on applications
such as commercial voice recognition devices as well as major text-to-speech
software, the attacks on these systems are increasingly complex, varied, and
constantly evolving. With the use cases for VPS rapidly growing into new spaces
and purposes, the potential consequences regarding privacy are increasingly
more dangerous. In addition, the growing number and increased practicality of
over-the-air attacks have made system failures much more probable. In this
paper, we will identify and classify an arrangement of unique attacks on voice
processing systems. Over the years research has been moving from specialized,
untargeted attacks that result in the malfunction of systems and the denial of
services to more general, targeted attacks that can force an outcome controlled
by an adversary. The current and most frequently used machine learning systems
and deep neural networks, which are at the core of modern voice processing
systems, were built with a focus on performance and scalability rather than
security. Therefore, it is critical for us to reassess the developing voice
processing landscape and to identify the state of current attacks and defenses
so that we may suggest future developments and theoretical improvements.

    

### [[2112.13175] Practical Fixed-Parameter Algorithms for Defending Active Directory Style Attack Graphs](http://arxiv.org/abs/2112.13175)


  Active Directory is the default security management system for Windows domain
networks. We study the shortest path edge interdiction problem for defending
Active Directory style attack graphs. The problem is formulated as a
Stackelberg game between one defender and one attacker. The attack graph
contains one destination node and multiple entry nodes. The attacker's entry
node is chosen by nature. The defender chooses to block a set of edges limited
by his budget. The attacker then picks the shortest unblocked attack path. The
defender aims to maximize the expected shortest path length for the attacker,
where the expectation is taken over entry nodes.
We observe that practical Active Directory attack graphs have small maximum
attack path lengths and are structurally close to trees. We first show that
even if the maximum attack path length is a constant, the problem is still
$W[1]$-hard with respect to the defender's budget. Having a small maximum
attack path length and a small budget is not enough to design fixed-parameter
algorithms. If we further assume that the number of entry nodes is small, then
we derive a fixed-parameter tractable algorithm.
We then propose two other fixed-parameter algorithms by exploiting the
tree-like features. One is based on tree decomposition and requires a small
tree width. The other assumes a small number of splitting nodes (nodes with
multiple out-going edges). Finally, the last algorithm is converted into a
graph convolutional neural network based heuristic, which scales to larger
graphs with more splitting nodes.

    

### [[2112.13237] CABACE: Injecting Character Sequence Information and Domain Knowledge for Enhanced Acronym and Long-Form Extraction](http://arxiv.org/abs/2112.13237)


  Acronyms and long-forms are commonly found in research documents, more so in
documents from scientific and legal domains. Many acronyms used in such
documents are domain-specific and are very rarely found in normal text corpora.
Owing to this, transformer-based NLP models often detect OOV (Out of
Vocabulary) for acronym tokens, especially for non-English languages, and their
performance suffers while linking acronyms to their long forms during
extraction. Moreover, pretrained transformer models like BERT are not
specialized to handle scientific and legal documents. With these points being
the overarching motivation behind this work, we propose a novel framework
CABACE: Character-Aware BERT for ACronym Extraction, which takes into account
character sequences in text and is adapted to scientific and legal domains by
masked language modelling. We further use an objective with an augmented loss
function, adding the max loss and mask loss terms to the standard cross-entropy
loss for training CABACE. We further leverage pseudo labelling and adversarial
data generation to improve the generalizability of the framework. Experimental
results prove the superiority of the proposed framework in comparison to
various baselines. Additionally, we show that the proposed framework is better
suited than baseline models for zero-shot generalization to non-English
languages, thus reinforcing the effectiveness of our approach. Our team
BacKGProp secured the highest scores on the French dataset, second-highest on
Danish and Vietnamese, and third-highest in the English-Legal dataset on the
global leaderboard for the acronym extraction (AE) shared task at SDU AAAI-22.

    

### [[2112.13241] A Preliminary Study for Literary Rhyme Generation based on Neuronal Representation, Semantics and Shallow Parsing](http://arxiv.org/abs/2112.13241)


  In recent years, researchers in the area of Computational Creativity have
studied the human creative process proposing different approaches to reproduce
it with a formal procedure. In this paper, we introduce a model for the
generation of literary rhymes in Spanish, combining structures of language and
neural network models %(\textit{Word2vec}).%, into a structure for semantic
assimilation. The results obtained with a manual evaluation of the texts
generated by our algorithm are encouraging.

    

### [[2112.13243] Evolutionary Generation of Visual Motion Illusions](http://arxiv.org/abs/2112.13243)


  Why do we sometimes perceive static images as if they were moving? Visual
motion illusions enjoy a sustained popularity, yet there is no definitive
answer to the question of why they work. We present a generative model, the
Evolutionary Illusion GENerator (EIGen), that creates new visual motion
illusions. The structure of EIGen supports the hypothesis that illusory motion
might be the result of perceiving the brain's own predictions rather than
perceiving raw visual input from the eyes. The scientific motivation of this
paper is to demonstrate that the perception of illusory motion could be a side
effect of the predictive abilities of the brain. The philosophical motivation
of this paper is to call attention to the untapped potential of "motivated
failures", ways for artificial systems to fail as biological systems fail, as a
worthy outlet for Artificial Intelligence and Artificial Life research.

    

### [[2112.13308] Unsupervised Clustering Active Learning for Person Re-identification](http://arxiv.org/abs/2112.13308)


  Supervised person re-identification (re-id) approaches require a large amount
of pairwise manual labeled data, which is not applicable in most real-world
scenarios for re-id deployment. On the other hand, unsupervised re-id methods
rely on unlabeled data to train models but performs poorly compared with
supervised re-id methods. In this work, we aim to combine unsupervised re-id
learning with a small number of human annotations to achieve a competitive
performance. Towards this goal, we present a Unsupervised Clustering Active
Learning (UCAL) re-id deep learning approach. It is capable of incrementally
discovering the representative centroid-pairs and requiring human annotate
them. These few labeled representative pairwise data can improve the
unsupervised representation learning model with other large amounts of
unlabeled data. More importantly, because the representative centroid-pairs are
selected for annotation, UCAL can work with very low-cost human effort.
Extensive experiments demonstrate the superiority of the proposed model over
state-of-the-art active learning methods on three re-id benchmark datasets.

    

### [[2112.13320] Budget Sensitive Reannotation of Noisy Relation Classification Data Using Label Hierarchy](http://arxiv.org/abs/2112.13320)


  Large crowd-sourced datasets are often noisy and relation classification (RC)
datasets are no exception. Reannotating the entire dataset is one probable
solution however it is not always viable due to time and budget constraints.
This paper addresses the problem of efficient reannotation of a large noisy
dataset for the RC. Our goal is to catch more annotation errors in the dataset
while reannotating fewer instances. Existing work on RC dataset reannotation
lacks the flexibility about how much data to reannotate. We introduce the
concept of a reannotation budget to overcome this limitation. The immediate
follow-up problem is: Given a specific reannotation budget, which subset of the
data should we reannotate? To address this problem, we present two strategies
to selectively reannotate RC datasets. Our strategies utilize the taxonomic
hierarchy of relation labels. The intuition of our work is to rely on the graph
distance between actual and predicted relation labels in the label hierarchy
graph. We evaluate our reannotation strategies on the well-known TACRED
dataset. We design our experiments to answer three specific research questions.
First, does our strategy select novel candidates for reannotation? Second, for
a given reannotation budget is our reannotation strategy more efficient at
catching annotation errors? Third, what is the impact of data reannotation on
RC model performance measurement? Experimental results show that our both
reannotation strategies are novel and efficient. Our analysis indicates that
the current reported performance of RC models on noisy TACRED data is inflated.

    

### [[2112.13372] Delivery Issues Identification from Customer Feedback Data](http://arxiv.org/abs/2112.13372)


  Millions of packages are delivered successfully by online and local retail
stores across the world every day. The proper delivery of packages is needed to
ensure high customer satisfaction and repeat purchases. These deliveries suffer
various problems despite the best efforts from the stores. These issues happen
not only due to the large volume and high demand for low turnaround time but
also due to mechanical operations and natural factors. These issues range from
receiving wrong items in the package to delayed shipment to damaged packages
because of mishandling during transportation. Finding solutions to various
delivery issues faced by both sending and receiving parties plays a vital role
in increasing the efficiency of the entire process. This paper shows how to
find these issues using customer feedback from the text comments and uploaded
images. We used transfer learning for both Text and Image models to minimize
the demand for thousands of labeled examples. The results show that the model
can find different issues. Furthermore, it can also be used for tasks like
bottleneck identification, process improvement, automating refunds, etc.
Compared with the existing process, the ensemble of text and image models
proposed in this paper ensures the identification of several types of delivery
issues, which is more suitable for the real-life scenarios of delivery of items
in retail businesses. This method can supply a new idea of issue detection for
the delivery of packages in similar industries.

    

### [[2112.13388] The brain as a probabilistic transducer: an evolutionarily plausible network architecture for knowledge representation, computation, and behavior](http://arxiv.org/abs/2112.13388)


  We offer a general theoretical framework for brain and behavior that is
evolutionarily and computationally plausible. The brain in our abstract model
is a network of nodes and edges. Although it has some similarities to standard
neural network models, as we show, there are some significant differences. Both
nodes and edges in our network have weights and activation levels. They act as
probabilistic transducers that use a set of relatively simple rules to
determine how activation levels and weights are affected by input, generate
output, and affect each other. We show that these simple rules enable a
learning process that allows the network to represent increasingly complex
knowledge, and simultaneously to act as a computing device that facilitates
planning, decision-making, and the execution of behavior. By specifying the
innate (genetic) components of the network, we show how evolution could endow
the network with initial adaptive rules and goals that are then enriched
through learning. We demonstrate how the developing structure of the network
(which determines what the brain can do and how well) is critically affected by
the co-evolved coordination between the mechanisms affecting the distribution
of data input and those determining the learning parameters (used in the
programs run by nodes and edges). Finally, we consider how the model accounts
for various findings in the field of learning and decision making, how it can
address some challenging problems in mind and behavior, such as those related
to setting goals and self-control, and how it can help understand some
cognitive disorders.

    

### [[2112.13428] ArT: All-round Thinker for Unsupervised Commonsense Question-Answering](http://arxiv.org/abs/2112.13428)


  Without labeled question-answer pairs for necessary training, unsupervised
commonsense question-answering (QA) appears to be extremely challenging due to
its indispensable unique prerequisite on commonsense source like knowledge
bases (KBs), which are usually highly resource consuming in construction.
Recently pre-trained language models (PrLMs) show effectiveness as an
alternative for commonsense clues when they play a role of knowledge generator.
However, existing work simply generates hundreds of pseudo-answers, or roughly
performs knowledge generation according to templates once for all, which may
result in much noise and thus hinders the quality of generated knowledge.
Motivated by human thinking experience, we propose an approach of All-round
Thinker (ArT) by fully taking association during knowledge generating. In
detail, our model first focuses on key parts in the given context, and then
generates highly related knowledge on such a basis in an association way like
human thinking. Besides, for casual reasoning, a reverse thinking mechanism is
proposed to conduct bidirectional inferring between cause and effect. ArT is
totally unsupervised and KBs-free. We evaluate it on three commonsense QA
benchmarks: COPA, SocialIQA and SCT. On all scales of PrLM backbones, ArT shows
its brilliant performance and outperforms previous advanced unsupervised
models.

    

### [[2112.13465] PreDisM: Pre-Disaster Modelling With CNN Ensembles for At-Risk Communities](http://arxiv.org/abs/2112.13465)


  The machine learning community has recently had increased interest in the
climate and disaster damage domain due to a marked increased occurrences of
natural hazards (e.g., hurricanes, forest fires, floods, earthquakes). However,
not enough attention has been devoted to mitigating probable destruction from
impending natural hazards. We explore this crucial space by predicting
building-level damages on a before-the-fact basis that would allow state actors
and non-governmental organizations to be best equipped with resource
distribution to minimize or preempt losses. We introduce PreDisM that employs
an ensemble of ResNets and fully connected layers over decision trees to
capture image-level and meta-level information to accurately estimate weakness
of man-made structures to disaster-occurrences. Our model performs well and is
responsive to tuning across types of disasters and highlights the space of
preemptive hazard damage modelling.

    

### [[2112.13477] A Brief History of Updates of Answer-Set Programs](http://arxiv.org/abs/2112.13477)


  Over the last couple of decades, there has been a considerable effort devoted
to the problem of updating logic programs under the stable model semantics
(a.k.a. answer-set programs) or, in other words, the problem of characterising
the result of bringing up-to-date a logic program when the world it describes
changes. Whereas the state-of-the-art approaches are guided by the same basic
intuitions and aspirations as belief updates in the context of classical logic,
they build upon fundamentally different principles and methods, which have
prevented a unifying framework that could embrace both belief and rule updates.
In this paper, we will overview some of the main approaches and results related
to answer-set programming updates, while pointing out some of the main
challenges that research in this topic has faced.

    

### [[2112.13508] Duck swarm algorithm: a novel swarm intelligence algorithm](http://arxiv.org/abs/2112.13508)


  A swarm intelligence-based optimization algorithm, named Duck Swarm Algorithm
(DSA), is proposed in this paper. This algorithm is inspired by the searching
for food sources and foraging behaviors of the duck swarm. The performance of
DSA is verified by using eighteen benchmark functions, where it is statistical
(best, mean, standard deviation, and average running time) results are compared
with seven well-known algorithms like Particle swarm optimization (PSO),
Firefly algorithm (FA), Chicken swarm optimization (CSO), Grey wolf optimizer
(GWO), Sine cosine algorithm (SCA), and Marine-predators algorithm (MPA), and
Archimedes optimization algorithm (AOA). Moreover, the Wilcoxon rank-sum test,
Friedman test, and convergence curves of the comparison results are used to
prove the superiority of the DSA against other algorithms. The results
demonstrate that DSA is a high-performance optimization method in terms of
convergence speed and exploration-exploitation balance for solving
high-dimension optimization functions. Also, DSA is applied for the optimal
design of two constrained engineering problems (the Three-bar truss problem,
and the Sawmill operation problem). Additionally, four engineering constraint
problems have also been used to analyze the performance of the proposed DSA.
Overall, the comparison results revealed that the DSA is a promising and very
competitive algorithm for solving different optimization problems.

    

### [[2112.13523] Interpreting Dynamical Systems as Bayesian Reasoners](http://arxiv.org/abs/2112.13523)


  A central concept in active inference is that the internal states of a
physical system parametrise probability measures over states of the external
world. These can be seen as an agent's beliefs, expressed as a Bayesian prior
or posterior. Here we begin the development of a general theory that would tell
us when it is appropriate to interpret states as representing beliefs in this
way. We focus on the case in which a system can be interpreted as performing
either Bayesian filtering or Bayesian inference. We provide formal definitions
of what it means for such an interpretation to exist, using techniques from
category theory.

    

### [[2112.13534] Adversarial Attack for Asynchronous Event-based Data](http://arxiv.org/abs/2112.13534)


  Deep neural networks (DNNs) are vulnerable to adversarial examples that are
carefully designed to cause the deep learning model to make mistakes.
Adversarial examples of 2D images and 3D point clouds have been extensively
studied, but studies on event-based data are limited. Event-based data can be
an alternative to a 2D image under high-speed movements, such as autonomous
driving. However, the given adversarial events make the current deep learning
model vulnerable to safety issues. In this work, we generate adversarial
examples and then train the robust models for event-based data, for the first
time. Our algorithm shifts the time of the original events and generates
additional adversarial events. Additional adversarial events are generated in
two stages. First, null events are added to the event-based data to generate
additional adversarial events. The perturbation size can be controlled with the
number of null events. Second, the location and time of additional adversarial
events are set to mislead DNNs in a gradient-based attack. Our algorithm
achieves an attack success rate of 97.95\% on the N-Caltech101 dataset.
Furthermore, the adversarial training model improves robustness on the
adversarial event data compared to the original model.

    

### [[2112.13557] Semantic Characterizations of General Belief Base Revision](http://arxiv.org/abs/2112.13557)


  The AGM postulates by Alchourrn, Grdenfors, and Makinson continue to
represent a cornerstone in research related to belief change. Katsuno and
Mendelzon (K&M) adopted the AGM postulates for changing belief bases and
characterized AGM belief base revision in propositional logic over finite
signatures. We generalize K&M's approach to the setting of (multiple) base
revision in arbitrary Tarskian logics, covering all logics with a classical
model-theoretic semantics and hence a wide variety of logics used in knowledge
representation and beyond. Our generic formulation applies to various notions
of "base" (such as belief sets, arbitrary or finite sets of sentences, or
single sentences). The core result is a representation theorem showing a
two-way correspondence between AGM base revision operators and certain
"assignments": functions mapping belief bases to total - yet not transitive -
"preference" relations between interpretations. Alongside, we present a
companion result for the case when the AGM postulate of syntax-independence is
abandoned. We also provide a characterization of all logics for which our
result can be strengthened to assignments producing transitive preference
relations (as in K&M's original work), giving rise to two more representation
theorems for such logics, according to syntax dependence vs. independence.

    

### [[2112.13597] HeteroQA: Learning towards Question-and-Answering through Multiple Information Sources via Heterogeneous Graph Modeling](http://arxiv.org/abs/2112.13597)


  Community Question Answering (CQA) is a well-defined task that can be used in
many scenarios, such as E-Commerce and online user community for special
interests.
In these communities, users can post articles, give comment, raise a question
and answer it.
These data form the heterogeneous information sources where each information
source have their own special structure and context (comments attached to an
article or related question with answers).
Most of the CQA methods only incorporate articles or Wikipedia to extract
knowledge and answer the user's question.
However, various types of information sources in the community are not fully
explored by these CQA methods and these multiple information sources (MIS) can
provide more related knowledge to user's questions.
Thus, we propose a question-aware heterogeneous graph transformer to
incorporate the MIS in the user community to automatically generate the answer.
To evaluate our proposed method, we conduct the experiments on two datasets:
$\text{MSM}^{\text{plus}}$ the modified version of benchmark dataset MS-MARCO
and the AntQA dataset which is the first large-scale CQA dataset with four
types of MIS.
Extensive experiments on two datasets show that our model outperforms all the
baselines in terms of all the metrics.

    

### [[2112.13646] Personalized Lane Change Decision Algorithm Using Deep Reinforcement Learning Approach](http://arxiv.org/abs/2112.13646)


  To develop driving automation technologies for human, a human-centered
methodology should be adopted for ensured safety and satisfactory user
experience. Automated lane change decision in dense highway traffic is
challenging, especially when considering the personalized preferences of
different drivers. To fulfill human driver centered decision algorithm
development, we carry out driver-in-the-loop experiments on a
6-Degree-of-Freedom driving simulator. Based on the analysis of the lane change
data by drivers of three specific styles,personalization indicators are
selected to describe the driver preferences in lane change decision. Then a
deep reinforcement learning (RL) approach is applied to design human-like
agents for automated lane change decision, with refined reward and loss
functions to capture the driver preferences.The trained RL agents and benchmark
agents are tested in a two-lane highway driving scenario, and by comparing the
agents with the specific drivers at the same initial states of lane change, the
statistics show that the proposed algorithm can guarantee higher consistency of
lane change decision preferences. The driver personalization indicators and the
proposed RL-based lane change decision algorithm are promising to contribute in
automated lane change system developing.

    

### [[1412.1862] Knowledge, Justification, and Adequate Reasons](http://arxiv.org/abs/1412.1862)


  Is knowledge definable as justified true belief ("JTB")? We argue that one
can legitimately answer positively or negatively, depending on whether or not
one's true belief is justified by what we call adequate reasons. To facilitate
our argument we introduce a simple propositional logic of reason-based belief,
and give an axiomatic characterization of the notion of adequacy for reasons.
We show that this logic is sufficiently flexible to accommodate various useful
features, including quantification over reasons. We use our framework to
contrast two notions of JTB: one internalist, the other externalist. We argue
that Gettier cases essentially challenge the internalist notion but not the
externalist one. Our approach commits us to a form of infallibilism about
knowledge, but it also leaves us with a puzzle, namely whether knowledge
involves the possession of only adequate reasons, or leaves room for some
inadequate reasons. We favor the latter position, which reflects a milder and
more realistic version of infallibilism.

    

### [[1906.08487] Generating Empathetic Responses by Looking Ahead the User's Sentiment](http://arxiv.org/abs/1906.08487)


  An important aspect of human conversation difficult for machines is
conversing with empathy, which is to understand the user's emotion and respond
appropriately. Recent neural conversation models that attempted to generate
empathetic responses either focused on conditioning the output to a given
emotion, or incorporating the current user emotional state. However, these
approaches do not factor in how the user would feel towards the generated
response. Hence, in this paper, we propose Sentiment Look-ahead, which is a
novel perspective for empathy that models the future user emotional state. In
short, Sentiment Look-ahead is a reward function under a reinforcement learning
framework that provides a higher reward to the generative model when the
generated utterance improves the user's sentiment. We implement and evaluate
three different possible implementations of sentiment look-ahead and
empirically show that our proposed approach can generate significantly more
empathetic, relevant, and fluent responses than other competitive baselines
such as multitask learning.

    

### [[2112.12823] A Rationale-Based Classification of MISRA C Guidelines](http://arxiv.org/abs/2112.12823)


  MISRA C is the most authoritative language subset for the C programming
language that is a de facto standard in several industry sectors where safety
and security are of paramount importance. While MISRA C is currently encoded in
175 guidelines (coding rules and directives), it does not coincide with them:
proper adoption of MISRA C requires embracing its preventive approach (as
opposed to the "bug finding" approach) and a documented development process
where justifiable non-compliances are authorized and recorded as deviations.
MISRA C guidelines are classified along several axes in the official MISRA
documents. In this paper, we add to these an orthogonal classification that
associates guidelines with their main rationale. The advantages of this new
classification are illustrated for different kinds of projects, including those
not (yet) having MISRA compliance among their objectives.

    

### [[2112.12869] A Lightweight Approach to Computing Message Races with an Application to Causal-Consistent Reversible Debugging](http://arxiv.org/abs/2112.12869)


  Message-passing concurrency is a popular model that underlies several
programming languages like, e.g., Erlang, Akka or (to some extent) Rust. At
runtime, an application is seen as a collection of processes that can only
interact through message sending and receiving. In this paper, we present a
lightweight formalism (a trace) to model a message-passing concurrent execution
that is appropriate to deal with so-called selective receives and allows one to
identify common problems like lost or delayed messages, some forms of deadlock,
etc. We then provide constructive definitions to identify (potential) message
races and to explore alternative executions. Finally, we apply our approach in
order to extend a reversible debugging framework for Erlang programs.

    

### [[2101.11320] Tutorial implementation of Hoare logic in Haskell](http://arxiv.org/abs/2101.11320)


  Using the programming language Haskell, we introduce an implementation of
propositional calculus, number theory, and a simple imperative language that
can evaluate arithmetic and boolean expressions. Finally, we provide an
implementation of Hoare's logic which will allow us to deduce facts about
programs without the need for a full evaluation.

    