
## 2021-12-8

### [<title>【央视新闻】广州会务费发票【手机搜狐网】 - DockOne.io</title>](http://dockone.io/question/1759505)

### [<title>「今日发布」上海烟酒发票「手机腾讯网」 - DockOne.io</title>](http://dockone.io/question/1759504)

### [<title>「央视新闻」集安开医院疾病诊断证明「医院假条单《手机搜狐网》 - DockOne.io</title>](http://dockone.io/question/1759503)

### [<title>「今日爆料」湘潭开三甲医院诊断证明「医院假条单[长津湖」 - DockOne.io</title>](http://dockone.io/question/1759502)

### [<title>【央视新闻】深圳会务费发票【手机搜狐网】 - DockOne.io</title>](http://dockone.io/question/1759501)

### [<title>「央视新闻」梅河口开医院疾病诊断证明「医院假条单《手机搜狐网》 - DockOne.io</title>](http://dockone.io/question/1759500)

### [<title>「今日发布」武汉服务费发票「手机腾讯网」 - DockOne.io</title>](http://dockone.io/question/1759499)

### [<title>「今日爆料」株洲开三甲医院诊断证明「医院假条单[长津湖」 - DockOne.io</title>](http://dockone.io/question/1759498)

### [<title>【央视新闻】北京会务费发票【手机搜狐网】 - DockOne.io</title>](http://dockone.io/question/1759497)

### [<title>「央视新闻」公主岭开医院疾病诊断证明「医院假条单《手机搜狐网》 - DockOne.io</title>](http://dockone.io/question/1759496)

### [<title>「今日发布」南京服务费发票「手机腾讯网」 - DockOne.io</title>](http://dockone.io/question/1759495)

### [<title>「今日爆料」岳阳开三甲医院诊断证明「医院假条单[长津湖」 - DockOne.io</title>](http://dockone.io/question/1759494)

### [<title>「央视新闻」双辽开医院疾病诊断证明「医院假条单《手机搜狐网》 - DockOne.io</title>](http://dockone.io/question/1759493)

### [<title>【央视新闻】上海会务费发票【手机搜狐网】 - DockOne.io</title>](http://dockone.io/question/1759492)

### [<title>「今日发布」苏州服务费发票「手机腾讯网」 - DockOne.io</title>](http://dockone.io/question/1759491)

### [<title>「央视新闻」大安开医院疾病诊断证明「医院假条单《手机搜狐网》 - DockOne.io</title>](http://dockone.io/question/1759490)

### [<title>「今日爆料」益阳开三甲医院诊断证明「医院假条单[长津湖」 - DockOne.io</title>](http://dockone.io/question/1759489)

### [<title>「今日爆料」常德开三甲医院诊断证明「医院假条单[长津湖」 - DockOne.io</title>](http://dockone.io/question/1759488)

### [<title>【央视新闻】武汉会议费发票【手机搜狐网】 - DockOne.io</title>](http://dockone.io/question/1759487)

### [<title>「今日发布」宁波服务费发票「手机腾讯网」 - DockOne.io</title>](http://dockone.io/question/1759486)

### [[2112.03315] Adversarial Machine Learning In Network Intrusion Detection Domain: A Systematic Review](http://arxiv.org/abs/2112.03315)


  Due to their massive success in various domains, deep learning techniques are
increasingly used to design network intrusion detection solutions that detect
and mitigate unknown and known attacks with high accuracy detection rates and
minimal feature engineering. However, it has been found that deep learning
models are vulnerable to data instances that can mislead the model to make
incorrect classification decisions so-called (adversarial examples). Such
vulnerability allows attackers to target NIDSs by adding small crafty
perturbations to the malicious traffic to evade detection and disrupt the
system's critical functionalities. The problem of deep adversarial learning has
been extensively studied in the computer vision domain; however, it is still an
area of open research in network security applications. Therefore, this survey
explores the researches that employ different aspects of adversarial machine
learning in the area of network intrusion detection in order to provide
directions for potential solutions. First, the surveyed studies are categorized
based on their contribution to generating adversarial examples, evaluating the
robustness of ML-based NIDs towards adversarial examples, and defending these
models against such attacks. Second, we highlight the characteristics
identified in the surveyed research. Furthermore, we discuss the applicability
of the existing generic adversarial attacks for the NIDS domain, the
feasibility of launching the proposed attacks in real-world scenarios, and the
limitations of the existing mitigation solutions.

    

### [[2112.03683] In-Network Processing for Low-Latency Industrial Anomaly Detection in Softwarized Networks](http://arxiv.org/abs/2112.03683)


  Modern manufacturers are currently undertaking the integration of novel
digital technologies - such as 5G-based wireless networks, the Internet of
Things (IoT), and cloud computing - to elevate their production process to a
brand new level, the level of smart factories. In the setting of a modern smart
factory, time-critical applications are increasingly important to facilitate
efficient and safe production. However, these applications suffer from delays
in data transmission and processing due to the high density of wireless sensors
and the large volumes of data that they generate. As the advent of
next-generation networks has made network nodes intelligent and capable of
handling multiple network functions, the increased computational power of the
nodes makes it possible to offload some of the computational overhead. In this
paper, we show for the first time our IA-Net-Lite industrial anomaly detection
system with the novel capability of in-network data processing. IA-Net-Lite
utilizes intelligent network devices to combine data transmission and
processing, as well as to progressively filter redundant data in order to
optimize service latency. By testing in a practical network emulator, we showed
that the proposed approach can reduce the service latency by up to 40%.
Moreover, the benefits of our approach could potentially be exploited in other
large-volume and artificial intelligence applications.

    

### [[2112.03723] Shrub Ensembles for Online Classification](http://arxiv.org/abs/2112.03723)


  Online learning algorithms have become a ubiquitous tool in the machine
learning toolbox and are frequently used in small, resource-constraint
environments. Among the most successful online learning methods are Decision
Tree (DT) ensembles. DT ensembles provide excellent performance while adapting
to changes in the data, but they are not resource efficient. Incremental tree
learners keep adding new nodes to the tree but never remove old ones increasing
the memory consumption over time. Gradient-based tree learning, on the other
hand, requires the computation of gradients over the entire tree which is
costly for even moderately sized trees. In this paper, we propose a novel
memory-efficient online classification ensemble called shrub ensembles for
resource-constraint systems. Our algorithm trains small to medium-sized
decision trees on small windows and uses stochastic proximal gradient descent
to learn the ensemble weights of these `shrubs'. We provide a theoretical
analysis of our algorithm and include an extensive discussion on the behavior
of our approach in the online setting. In a series of 2~959 experiments on 12
different datasets, we compare our method against 8 state-of-the-art methods.
Our Shrub Ensembles retain an excellent performance even when only little
memory is available. We show that SE offers a better accuracy-memory trade-off
in 7 of 12 cases, while having a statistically significant better performance
than most other methods. Our implementation is available under
this https URL .

    

### [[2112.03835] Attention-Based Model and Deep Reinforcement Learning for Distribution of Event Processing Tasks](http://arxiv.org/abs/2112.03835)


  Event processing is the cornerstone of the dynamic and responsive Internet of
Things (IoT). Recent approaches in this area are based on representational
state transfer (REST) principles, which allow event processing tasks to be
placed at any device that follows the same principles. However, the tasks
should be properly distributed among edge devices to ensure fair resources
utilization and guarantee seamless execution. This article investigates the use
of deep learning to fairly distribute the tasks. An attention-based neural
network model is proposed to generate efficient load balancing solutions under
different scenarios. The proposed model is based on the Transformer and Pointer
Network architectures, and is trained by an advantage actor-critic
reinforcement learning algorithm. The model is designed to scale to the number
of event processing tasks and the number of edge devices, with no need for
hyperparameters re-tuning or even retraining. Extensive experimental results
show that the proposed model outperforms conventional heuristics in many key
performance indicators. The generic design and the obtained results show that
the proposed model can potentially be applied to several other load balancing
problem variations, which makes the proposal an attractive option to be used in
real-world scenarios due to its scalability and efficiency.

    

### [[2104.00352] Decentralized and Model-Free Federated Learning: Consensus-Based Distillation in Function Space](http://arxiv.org/abs/2104.00352)


  This paper proposes a fully decentralized federated learning (FL) scheme for
Internet of Everything (IoE) devices that are connected via multi-hop networks.
Because FL algorithms hardly converge the parameters of machine learning (ML)
models, this paper focuses on the convergence of ML models in function spaces.
Considering that the representative loss functions of ML tasks e.g, mean
squared error (MSE) and Kullback-Leibler (KL) divergence, are convex
functionals, algorithms that directly update functions in function spaces could
converge to the optimal solution. The key concept of this paper is to tailor a
consensus-based optimization algorithm to work in the function space and
achieve the global optimum in a distributed manner. This paper first analyzes
the convergence of the proposed algorithm in a function space, which is
referred to as a meta-algorithm, and shows that the spectral graph theory can
be applied to the function space in a manner similar to that of numerical
vectors. Then, consensus-based multi-hop federated distillation (CMFD) is
developed for a neural network (NN) to implement the meta-algorithm. CMFD
leverages knowledge distillation to realize function aggregation among adjacent
devices without parameter averaging. An advantage of CMFD is that it works even
with different NN models among the distributed learners. Although CMFD does not
perfectly reflect the behavior of the meta-algorithm, the discussion of the
meta-algorithm's convergence property promotes an intuitive understanding of
CMFD, and simulation evaluations show that NN models converge using CMFD for
several tasks. The simulation results also show that CMFD achieves higher
accuracy than parameter aggregation for weakly connected networks, and CMFD is
more stable than parameter aggregation methods.

    

### [[2112.03262] Multi-scale Graph Convolutional Networks with Self-Attention](http://arxiv.org/abs/2112.03262)


  Graph convolutional networks (GCNs) have achieved remarkable learning ability
for dealing with various graph structural data recently. In general, deep GCNs
do not work well since graph convolution in conventional GCNs is a special form
of Laplacian smoothing, which makes the representation of different nodes
indistinguishable. In the literature, multi-scale information was employed in
GCNs to enhance the expressive power of GCNs. However, over-smoothing
phenomenon as a crucial issue of GCNs remains to be solved and investigated. In
this paper, we propose two novel multi-scale GCN frameworks by incorporating
self-attention mechanism and multi-scale information into the design of GCNs.
Our methods greatly improve the computational efficiency and prediction
accuracy of the GCNs model. Extensive experiments on both node classification
and graph classification demonstrate the effectiveness over several
state-of-the-art GCNs. Notably, the proposed two architectures can efficiently
mitigate the over-smoothing problem of GCNs, and the layer of our model can
even be increased to $64$.

    

### [[2112.03265] A Deep-Learning Intelligent System Incorporating Data Augmentation for Short-Term Voltage Stability Assessment of Power Systems](http://arxiv.org/abs/2112.03265)


  Facing the difficulty of expensive and trivial data collection and
annotation, how to make a deep learning-based short-term voltage stability
assessment (STVSA) model work well on a small training dataset is a challenging
and urgent problem. Although a big enough dataset can be directly generated by
contingency simulation, this data generation process is usually cumbersome and
inefficient; while data augmentation provides a low-cost and efficient way to
artificially inflate the representative and diversified training datasets with
label preserving transformations. In this respect, this paper proposes a novel
deep-learning intelligent system incorporating data augmentation for STVSA of
power systems. First, due to the unavailability of reliable quantitative
criteria to judge the stability status for a specific power system,
semi-supervised cluster learning is leveraged to obtain labeled samples in an
original small dataset. Second, to make deep learning applicable to the small
dataset, conditional least squares generative adversarial networks
(LSGAN)-based data augmentation is introduced to expand the original dataset
via artificially creating additional valid samples. Third, to extract temporal
dependencies from the post-disturbance dynamic trajectories of a system, a
bi-directional gated recurrent unit with attention mechanism based assessment
model is established, which bi-directionally learns the significant time
dependencies and automatically allocates attention weights. The test results
demonstrate the presented approach manages to achieve better accuracy and a
faster response time with original small datasets. Besides classification
accuracy, this work employs statistical measures to comprehensively examine the
performance of the proposal.

    

### [[2112.03266] Contrastive Cycle Adversarial Autoencoders for Single-cell Multi-omics Alignment and Integration](http://arxiv.org/abs/2112.03266)


  Muilti-modality data are ubiquitous in biology, especially that we have
entered the multi-omics era, when we can measure the same biological object
(cell) from different aspects (omics) to provide a more comprehensive insight
into the cellular system. When dealing with such multi-omics data, the first
step is to determine the correspondence among different modalities. In other
words, we should match data from different spaces corresponding to the same
object. This problem is particularly challenging in the single-cell multi-omics
scenario because such data are very sparse with extremely high dimensions.
Secondly, matched single-cell multi-omics data are rare and hard to collect.
Furthermore, due to the limitations of the experimental environment, the data
are usually highly noisy. To promote the single-cell multi-omics research, we
overcome the above challenges, proposing a novel framework to align and
integrate single-cell RNA-seq data and single-cell ATAC-seq data. Our approach
can efficiently map the above data with high sparsity and noise from different
spaces to a low-dimensional manifold in a unified space, making the downstream
alignment and integration straightforward. Compared with the other
state-of-the-art methods, our method performs better in both simulated and real
single-cell data. The proposed method is helpful for the single-cell
multi-omics research. The improvement for integration on the simulated data is
significant.

    

### [[2112.03267] Communication and Energy Efficient Slimmable Federated Learning via Superposition Coding and Successive Decoding](http://arxiv.org/abs/2112.03267)


  Mobile devices are indispensable sources of big data. Federated learning (FL)
has a great potential in exploiting these private data by exchanging locally
trained models instead of their raw data. However, mobile devices are often
energy limited and wirelessly connected, and FL cannot cope flexibly with their
heterogeneous and time-varying energy capacity and communication throughput,
limiting the adoption. Motivated by these issues, we propose a novel energy and
communication efficient FL framework, coined SlimFL. To resolve the
heterogeneous energy capacity problem, each device in SlimFL runs a
width-adjustable slimmable neural network (SNN). To address the heterogeneous
communication throughput problem, each full-width (1.0x) SNN model and its
half-width ($0.5$x) model are superposition-coded before transmission, and
successively decoded after reception as the 0.5x or $1.0$x model depending on
the channel quality. Simulation results show that SlimFL can simultaneously
train both $0.5$x and $1.0$x models with reasonable accuracy and convergence
speed, compared to its vanilla FL counterpart separately training the two
models using $2$x more communication resources. Surprisingly, SlimFL achieves
even higher accuracy with lower energy footprints than vanilla FL for poor
channels and non-IID data distributions, under which vanilla FL converges
slowly.

    

### [[2112.03268] Synthetic ECG Signal Generation Using Generative Neural Networks](http://arxiv.org/abs/2112.03268)


  Electrocardiogram (ECG) datasets tend to be highly imbalanced due to the
scarcity of abnormal cases. Additionally, the use of real patients' ECG is
highly regulated due to privacy issues. Therefore, there is always a need for
more ECG data, especially for the training of automatic diagnosis machine
learning models, which perform better when trained on a balanced dataset. We
studied the synthetic ECG generation capability of 5 different models from the
generative adversarial network (GAN) family and compared their performances,
the focus being only on Normal cardiac cycles. Dynamic Time Warping (DTW),
Fréchet, and Euclidean distance functions were employed to quantitatively
measure performance. Five different methods for evaluating generated beats were
proposed and applied. We also proposed 3 new concepts (threshold, accepted beat
and productivity rate) and employed them along with the aforementioned methods
as a systematic way for comparison between models. The results show that all
the tested models can to an extent successfully mass-generate acceptable
heartbeats with high similarity in morphological features, and potentially all
of them can be used to augment imbalanced datasets. However, visual inspections
of generated beats favor BiLSTM-DC GAN and WGAN, as they produce statistically
more acceptable beats. Also, with regards to productivity rate, the Classic GAN
is superior with a 72% productivity rate.

    

### [[2112.03270] Toward a Taxonomy of Trust for Probabilistic Machine Learning](http://arxiv.org/abs/2112.03270)


  Probabilistic machine learning increasingly informs critical decisions in
medicine, economics, politics, and beyond. We need evidence to support that the
resulting decisions are well-founded. To aid development of trust in these
decisions, we develop a taxonomy delineating where trust in an analysis can
break down: (1) in the translation of real-world goals to goals on a particular
set of available training data, (2) in the translation of abstract goals on the
training data to a concrete mathematical problem, (3) in the use of an
algorithm to solve the stated mathematical problem, and (4) in the use of a
particular code implementation of the chosen algorithm. We detail how trust can
fail at each step and illustrate our taxonomy with two case studies: an
analysis of the efficacy of microcredit and The Economist's predictions of the
2020 US presidential election. Finally, we describe a wide variety of methods
that can be used to increase trust at each step of our taxonomy. The use of our
taxonomy highlights steps where existing research work on trust tends to
concentrate and also steps where establishing trust is particularly
challenging.

    

### [[2112.03273] Dynamic Graph Learning-Neural Network for Multivariate Time Series Modeling](http://arxiv.org/abs/2112.03273)


  Multivariate time series forecasting is a challenging task because the data
involves a mixture of long- and short-term patterns, with dynamic
spatio-temporal dependencies among variables. Existing graph neural networks
(GNN) typically model multivariate relationships with a pre-defined spatial
graph or learned fixed adjacency graph. It limits the application of GNN and
fails to handle the above challenges. In this paper, we propose a novel
framework, namely static- and dynamic-graph learning-neural network (SDGL). The
model acquires static and dynamic graph matrices from data to model long- and
short-term patterns respectively. Static matric is developed to capture the
fixed long-term association pattern via node embeddings, and we leverage graph
regularity for controlling the quality of the learned static graph. To capture
dynamic dependencies among variables, we propose dynamic graphs learning method
to generate time-varying matrices based on changing node features and static
node embeddings. And in the method, we integrate the learned static graph
information as inductive bias to construct dynamic graphs and local
spatio-temporal patterns better. Extensive experiments are conducted on two
traffic datasets with extra structural information and four time series
datasets, which show that our approach achieves state-of-the-art performance on
almost all datasets. If the paper is accepted, I will open the source code on
github.

    

### [[2112.03275] Smart Metering System Capable of Anomaly Detection by Bi-directional LSTM Autoencoder](http://arxiv.org/abs/2112.03275)


  Anomaly detection is concerned with a wide range of applications such as
fault detection, system monitoring, and event detection. Identifying anomalies
from metering data obtained from smart metering system is a critical task to
enhance reliability, stability, and efficiency of the power system. This paper
presents an anomaly detection process to find outliers observed in the smart
metering system. In the proposed approach, bi-directional long short-term
memory (BiLSTM) based autoencoder is used and finds the anomalous data point.
It calculates the reconstruction error through autoencoder with the
non-anomalous data, and the outliers to be classified as anomalies are
separated from the non-anomalous data by predefined threshold. Anomaly
detection method based on the BiLSTM autoencoder is tested with the metering
data corresponding to 4 types of energy sources electricity/water/heating/hot
water collected from 985 households.

    

### [[2112.03276] Organ localisation using supervised and semi supervised approaches combining reinforcement learning with imitation learning](http://arxiv.org/abs/2112.03276)


  Computer aided diagnostics often requires analysis of a region of interest
(ROI) within a radiology scan, and the ROI may be an organ or a suborgan.
Although deep learning algorithms have the ability to outperform other methods,
they rely on the availability of a large amount of annotated data. Motivated by
the need to address this limitation, an approach to localisation and detection
of multiple organs based on supervised and semi-supervised learning is
presented here. It draws upon previous work by the authors on localising the
thoracic and lumbar spine region in CT images. The method generates six
bounding boxes of organs of interest, which are then fused to a single bounding
box. The results of experiments on localisation of the Spleen, Left and Right
Kidneys in CT Images using supervised and semi supervised learning (SSL)
demonstrate the ability to address data limitations with a much smaller data
set and fewer annotations, compared to other state-of-the-art methods. The SSL
performance was evaluated using three different mixes of labelled and
unlabelled data (i.e.30:70,35:65,40:60) for each of lumbar spine, spleen left
and right kidneys respectively. The results indicate that SSL provides a
workable alternative especially in medical imaging where it is difficult to
obtain annotated data.

    

### [[2112.03298] Automation Of Transiting Exoplanet Detection, Identification and Habitability Assessment Using Machine Learning Approaches](http://arxiv.org/abs/2112.03298)


  We are at a unique timeline in the history of human evolution where we may be
able to discover earth-like planets around stars outside our solar system where
conditions can support life or even find evidence of life on those planets.
With the launch of several satellites in recent years by NASA, ESA, and other
major space agencies, an ample amount of datasets are at our disposal which can
be utilized to train machine learning models that can automate the arduous
tasks of exoplanet detection, its identification, and habitability
determination. Automating these tasks can save a considerable amount of time
and minimize human errors due to manual intervention. To achieve this aim, we
first analyze the light intensity curves from stars captured by the Kepler
telescope to detect the potential curves that exhibit the characteristics of an
existence of a possible planetary system. For this detection, along with
training conventional models, we propose a stacked GBDT model that can be
trained on multiple representations of the light signals simultaneously.
Subsequently, we address the automation of exoplanet identification and
habitability determination by leveraging several state-of-art machine learning
and ensemble approaches. The identification of exoplanets aims to distinguish
false positive instances from the actual instances of exoplanets whereas the
habitability assessment groups the exoplanet instances into different clusters
based on their habitable characteristics. Additionally, we propose a new metric
called Adequate Thermal Adequacy (ATA) score to establish a potential linear
relationship between habitable and non-habitable instances. Experimental
results suggest that the proposed stacked GBDT model outperformed the
conventional models in detecting transiting exoplanets. Furthermore, the
incorporation of ATA scores in habitability classification enhanced the
performance of models.

    

### [[2112.03321] Noether Networks: Meta-Learning Useful Conserved Quantities](http://arxiv.org/abs/2112.03321)


  Progress in machine learning (ML) stems from a combination of data
availability, computational resources, and an appropriate encoding of inductive
biases. Useful biases often exploit symmetries in the prediction problem, such
as convolutional networks relying on translation equivariance. Automatically
discovering these useful symmetries holds the potential to greatly improve the
performance of ML systems, but still remains a challenge. In this work, we
focus on sequential prediction problems and take inspiration from Noether's
theorem to reduce the problem of finding inductive biases to meta-learning
useful conserved quantities. We propose Noether Networks: a new type of
architecture where a meta-learned conservation loss is optimized inside the
prediction function. We show, theoretically and experimentally, that Noether
Networks improve prediction quality, providing a general framework for
discovering inductive biases in sequential problems.

    

### [[2112.03324] Neuro-Symbolic Inductive Logic Programming with Logical Neural Networks](http://arxiv.org/abs/2112.03324)


  Recent work on neuro-symbolic inductive logic programming has led to
promising approaches that can learn explanatory rules from noisy, real-world
data. While some proposals approximate logical operators with differentiable
operators from fuzzy or real-valued logic that are parameter-free thus
diminishing their capacity to fit the data, other approaches are only loosely
based on logic making it difficult to interpret the learned "rules". In this
paper, we propose learning rules with the recently proposed logical neural
networks (LNN). Compared to others, LNNs offer strong connection to classical
Boolean logic thus allowing for precise interpretation of learned rules while
harboring parameters that can be trained with gradient-based optimization to
effectively fit the data. We extend LNNs to induce rules in first-order logic.
Our experiments on standard benchmarking tasks confirm that LNN rules are
highly interpretable and can achieve comparable or higher accuracy due to their
flexible parameterization.

    

### [[2112.03340] Label Hallucination for Few-Shot Classification](http://arxiv.org/abs/2112.03340)


  Few-shot classification requires adapting knowledge learned from a large
annotated base dataset to recognize novel unseen classes, each represented by
few labeled examples. In such a scenario, pretraining a network with high
capacity on the large dataset and then finetuning it on the few examples causes
severe overfitting. At the same time, training a simple linear classifier on
top of "frozen" features learned from the large labeled dataset fails to adapt
the model to the properties of the novel classes, effectively inducing
underfitting. In this paper we propose an alternative approach to both of these
two popular strategies. First, our method pseudo-labels the entire large
dataset using the linear classifier trained on the novel classes. This
effectively "hallucinates" the novel classes in the large dataset, despite the
novel categories not being present in the base database (novel and base classes
are disjoint). Then, it finetunes the entire model with a distillation loss on
the pseudo-labeled base examples, in addition to the standard cross-entropy
loss on the novel dataset. This step effectively trains the network to
recognize contextual and appearance cues that are useful for the novel-category
recognition but using the entire large-scale base dataset and thus overcoming
the inherent data-scarcity problem of few-shot learning. Despite the simplicity
of the approach, we show that that our method outperforms the state-of-the-art
on four well-established few-shot classification benchmarks.

    

### [[2112.03348] Grain segmentation in atomistic simulations using orientation-based iterative self-organizing data analysis](http://arxiv.org/abs/2112.03348)


  Atomistic simulations have now established themselves as an indispensable
tool in understanding deformation mechanisms of materials at the atomic scale.
Large scale simulations are regularly used to study the behavior of
polycrystalline materials at the nanoscale. In this work, we propose a method
for grain segmentation of an atomistic configuration using an unsupervised
machine learning algorithm that clusters atoms into individual grains based on
their orientation. The proposed method, called the Orisodata algorithm, is
based on the iterative self-organizing data analysis technique and is modified
to work in the orientation space. The working of the algorithm is demonstrated
on a 122 grain nanocrystalline thin film sample in both undeformed and deformed
states. The Orisodata algorithm is also compared with two other grain
segmentation algorithms available in the open-source visualization tool Ovito.
The results show that the Orisodata algorithm is able to correctly identify
deformation twins as well as regions separated by low angle grain boundaries.
The model parameters have intuitive physical meaning and relate to similar
thresholds used in experiments, which not only helps obtain optimal values but
also facilitates easy interpretation and validation of results.

    

### [[2112.03350] Test-Time Detection of Backdoor Triggers for Poisoned Deep Neural Networks](http://arxiv.org/abs/2112.03350)


  Backdoor (Trojan) attacks are emerging threats against deep neural networks
(DNN). A DNN being attacked will predict to an attacker-desired target class
whenever a test sample from any source class is embedded with a backdoor
pattern; while correctly classifying clean (attack-free) test samples. Existing
backdoor defenses have shown success in detecting whether a DNN is attacked and
in reverse-engineering the backdoor pattern in a "post-training" regime: the
defender has access to the DNN to be inspected and a small, clean dataset
collected independently, but has no access to the (possibly poisoned) training
set of the DNN. However, these defenses neither catch culprits in the act of
triggering the backdoor mapping, nor mitigate the backdoor attack at test-time.
In this paper, we propose an "in-flight" defense against backdoor attacks on
image classification that 1) detects use of a backdoor trigger at test-time;
and 2) infers the class of origin (source class) for a detected trigger
example. The effectiveness of our defense is demonstrated experimentally
against different strong backdoor attacks.

    

### [[2112.03358] Associative Memories Using Complex-Valued Hopfield Networks Based on Spin-Torque Oscillator Arrays](http://arxiv.org/abs/2112.03358)


  Simulations of complex-valued Hopfield networks based on spin-torque
oscillators can recover phase-encoded images. Sequences of memristor-augmented
inverters provide tunable delay elements that implement complex weights by
phase shifting the oscillatory output of the oscillators. Pseudo-inverse
training suffices to store at least 12 images in a set of 192 oscillators,
representing 16$\times$12 pixel images. The energy required to recover an image
depends on the desired error level. For the oscillators and circuitry
considered here, 5 % root mean square deviations from the ideal image require
approximately 5 $\mu$s and consume roughly 130 nJ. Simulations show that the
network functions well when the resonant frequency of the oscillators can be
tuned to have a fractional spread less than $10^{-3}$, depending on the
strength of the feedback.

    

### [[2112.03360] Cadence: A Practical Time-series Partitioning Algorithm for Unlabeled IoT Sensor Streams](http://arxiv.org/abs/2112.03360)


  Timeseries partitioning is an essential step in most machine-learning driven,
sensor-based IoT applications. This paper introduces a sample-efficient,
robust, time-series segmentation model and algorithm. We show that by learning
a representation specifically with the segmentation objective based on maximum
mean discrepancy (MMD), our algorithm can robustly detect time-series events
across different applications. Our loss function allows us to infer whether
consecutive sequences of samples are drawn from the same distribution (null
hypothesis) and determines the change-point between pairs that reject the null
hypothesis (i.e., come from different distributions). We demonstrate its
applicability in a real-world IoT deployment for ambient-sensing based activity
recognition. Moreover, while many works on change-point detection exist in the
literature, our model is significantly simpler and matches or outperforms
state-of-the-art methods. We can fully train our model in 9-93 seconds on
average with little variation in hyperparameters for data across different
applications.

    

### [[2112.03364] Scalable Geometric Deep Learning on Molecular Graphs](http://arxiv.org/abs/2112.03364)


  Deep learning in molecular and materials sciences is limited by the lack of
integration between applied science, artificial intelligence, and
high-performance computing. Bottlenecks with respect to the amount of training
data, the size and complexity of model architectures, and the scale of the
compute infrastructure are all key factors limiting the scaling of deep
learning for molecules and materials. Here, we present $\textit{LitMatter}$, a
lightweight framework for scaling molecular deep learning methods. We train
four graph neural network architectures on over 400 GPUs and investigate the
scaling behavior of these methods. Depending on the model architecture,
training time speedups up to $60\times$ are seen. Empirical neural scaling
relations quantify the model-dependent scaling and enable optimal compute
resource allocation and the identification of scalable molecular geometric deep
learning model implementations.

    

### [[2112.03371] Graphical Models with Attention for Context-Specific Independence and an Application to Perceptual Grouping](http://arxiv.org/abs/2112.03371)


  Discrete undirected graphical models, also known as Markov Random Fields
(MRFs), can flexibly encode probabilistic interactions of multiple variables,
and have enjoyed successful applications to a wide range of problems. However,
a well-known yet little studied limitation of discrete MRFs is that they cannot
capture context-specific independence (CSI). Existing methods require carefully
developed theories and purpose-built inference methods, which limit their
applications to only small-scale problems. In this paper, we propose the Markov
Attention Model (MAM), a family of discrete MRFs that incorporates an attention
mechanism. The attention mechanism allows variables to dynamically attend to
some other variables while ignoring the rest, and enables capturing of CSIs in
MRFs. A MAM is formulated as an MRF, allowing it to benefit from the rich set
of existing MRF inference methods and scale to large models and datasets. To
demonstrate MAM's capabilities to capture CSIs at scale, we apply MAMs to
capture an important type of CSI that is present in a symbolic approach to
recurrent computations in perceptual grouping. Experiments on two recently
proposed synthetic perceptual grouping tasks and on realistic images
demonstrate the advantages of MAMs in sample-efficiency, interpretability and
generalizability when compared with strong recurrent neural network baselines,
and validate MAM's capabilities to efficiently capture CSIs at scale.

    

### [[2112.03376] Convergence Guarantees for Deep Epsilon Greedy Policy Learning](http://arxiv.org/abs/2112.03376)


  Policy learning is a quickly growing area. As robotics and computers control
day-to-day life, their error rate needs to be minimized and controlled. There
are many policy learning methods and provable error rates that accompany them.
We show an error or regret bound and convergence of the Deep Epsilon Greedy
method which chooses actions with a neural network's prediction. In experiments
with the real-world dataset MNIST, we construct a nonlinear reinforcement
learning problem. We witness how with either high or low noise, some methods do
and some do not converge which agrees with our proof of convergence.

    

### [[2112.03377] RafterNet: Probabilistic predictions in multi-response regression](http://arxiv.org/abs/2112.03377)


  A fully nonparametric approach for making probabilistic predictions in
multi-response regression problems is introduced. Random forests are used as
marginal models for each response variable and, as novel contribution of the
present work, the dependence between the multiple response variables is modeled
by a generative neural network. This combined modeling approach of random
forests, corresponding empirical marginal residual distributions and a
generative neural network is referred to as RafterNet. Multiple datasets serve
as examples to demonstrate the flexibility of the approach and its impact for
making probabilistic forecasts.

    

### [[2112.03378] Differentiable Generalised Predictive Coding](http://arxiv.org/abs/2112.03378)


  This paper deals with differentiable dynamical models congruent with neural
process theories in neuroscience that cast brain function as hierarchical
filtering aiming at the refinement of an internal generative model explaining
observations. Our work extends existing implementations of predictive coding
with exact gradients and allows integration with deep neural networks for
non-linear latent state parameterization. In contrast to Gradient Descent in
combination with error backpropagation, such gradient based predictive coding
optimises neural networks locally in each layer by optimising
precision-weighted prediction errors that propagate from data towards latent
states. Predictions flow backwards, from latent states towards lower layers.
The model suggested here, GPC, uses exact gradients to learn hierarchical and
dynamical predictions of lower latent states. Hierarchical predictions encode
the perceived content and its structure. Dynamical predictions address changes
in the encoded content. As a result, hierarchical and dynamical predictions
address different aspects of the same latent states. Since changes in latent
states are influenced by the content they represent and vice versa, both
pathways interact and allow to encode representations of content-dynamics
dependencies across spatio-temporal scales and even backwards in time. We apply
GPC to various perception tasks on sequential data with adaptive sampling
rates. We discuss possibilities to relax the assumption of linearly
hierarchical model layout in favour of arbitrary graph structure. Finally, we
sketch out ideas for efficient perception and planning in nested
spatio-temporal hierarchies and discuss the connection to Markov Blankets in
the brain.

    

### [[2112.03379] Efficient Continuous Manifold Learning for Time Series Modeling](http://arxiv.org/abs/2112.03379)


  Modeling non-Euclidean data is drawing attention along with the unprecedented
successes of deep neural networks in diverse fields. In particular, symmetric
positive definite (SPD) matrix is being actively studied in computer vision,
signal processing, and medical image analysis, thanks to its ability to learn
appropriate statistical representations. However, due to its strong
constraints, it remains challenging for optimization problems or inefficient
computation costs, especially, within a deep learning framework. In this paper,
we propose to exploit a diffeomorphism mapping between Riemannian manifolds and
a Cholesky space, by which it becomes feasible not only to efficiently solve
optimization problems but also to reduce computation costs greatly. Further, in
order for dynamics modeling in time series data, we devise a continuous
manifold learning method by integrating a manifold ordinary differential
equation and a gated recurrent neural network in a systematic manner. It is
noteworthy that because of the nice parameterization of matrices in a Cholesky
space, it is straightforward to train our proposed network with Riemannian
geometric metrics equipped. We demonstrate through experiments that the
proposed model can be efficiently and reliably trained as well as outperform
existing manifold methods and state-of-the-art methods in two classification
tasks: action recognition and sleep staging classification.

    

### [[2112.03383] Graph Neural Networks Accelerated Molecular Dynamics](http://arxiv.org/abs/2112.03383)


  Molecular Dynamics (MD) simulation is a powerful tool for understanding the
dynamics and structure of matter. Since the resolution of MD is atomic-scale,
achieving long time-scale simulations with femtosecond integration is very
expensive. In each MD step, numerous redundant computations are performed which
can be learnt and avoided. These redundant computations can be surrogated and
modeled by a deep learning model like a Graph Neural Network (GNN). In this
work, we developed a GNN Accelerated Molecular Dynamics (GAMD) model that
achieves fast and accurate force predictions and generates trajectories
consistent with the classical MD simulations. Our results show that GAMD can
accurately predict the dynamics of two typical molecular systems, Lennard-Jones
(LJ) particles and Water (LJ+Electrostatics). GAMD's learning and inference are
agnostic to the scale, where it can scale to much larger systems at test time.
We also performed a comprehensive benchmark test comparing our implementation
of GAMD to production-level MD softwares, where we showed GAMD is competitive
with them on the large-scale simulation.

    

### [[2112.03386] Guided Imitation of Task and Motion Planning](http://arxiv.org/abs/2112.03386)


  While modern policy optimization methods can do complex manipulation from
sensory data, they struggle on problems with extended time horizons and
multiple sub-goals. On the other hand, task and motion planning (TAMP) methods
scale to long horizons but they are computationally expensive and need to
precisely track world state. We propose a method that draws on the strength of
both methods: we train a policy to imitate a TAMP solver's output. This
produces a feed-forward policy that can accomplish multi-step tasks from
sensory data. First, we build an asynchronous distributed TAMP solver that can
produce supervision data fast enough for imitation learning. Then, we propose a
hierarchical policy architecture that lets us use partially trained control
policies to speed up the TAMP solver. In robotic manipulation tasks with 7-DoF
joint control, the partially trained policies reduce the time needed for
planning by a factor of up to 2.6. Among these tasks, we can learn a policy
that solves the RoboSuite 4-object pick-place task 88% of the time from object
pose observations and a policy that solves the RoboDesk 9-goal benchmark 79% of
the time from RGB images (averaged across the 9 disparate tasks).

    

### [[2112.03395] Manas: Mining Software Repositories to Assist AutoML](http://arxiv.org/abs/2112.03395)


  Today deep learning is widely used for building software. A software
engineering problem with deep learning is that finding an appropriate
convolutional neural network (CNN) model for the task can be a challenge for
developers. Recent work on AutoML, more precisely neural architecture search
(NAS), embodied by tools like Auto-Keras aims to solve this problem by
essentially viewing it as a search problem where the starting point is a
default CNN model, and mutation of this CNN model allows exploration of the
space of CNN models to find a CNN model that will work best for the problem.
These works have had significant success in producing high-accuracy CNN models.
There are two problems, however. First, NAS can be very costly, often taking
several hours to complete. Second, CNN models produced by NAS can be very
complex that makes it harder to understand them and costlier to train them. We
propose a novel approach for NAS, where instead of starting from a default CNN
model, the initial model is selected from a repository of models extracted from
GitHub. The intuition being that developers solving a similar problem may have
developed a better starting point compared to the default model. We also
analyze common layer patterns of CNN models in the wild to understand changes
that the developers make to improve their models. Our approach uses commonly
occurring changes as mutation operators in NAS. We have extended Auto-Keras to
implement our approach. Our evaluation using 8 top voted problems from Kaggle
for tasks including image classification and image regression shows that given
the same search time, without loss of accuracy, Manas produces models with
42.9% to 99.6% fewer number of parameters than Auto-Keras' models. Benchmarked
on GPU, Manas' models train 30.3% to 641.6% faster than Auto-Keras' models.

    

### [[2112.03398] Top-Down Deep Clustering with Multi-generator GANs](http://arxiv.org/abs/2112.03398)


  Deep clustering (DC) leverages the representation power of deep architectures
to learn embedding spaces that are optimal for cluster analysis. This approach
filters out low-level information irrelevant for clustering and has proven
remarkably successful for high dimensional data spaces. Some DC methods employ
Generative Adversarial Networks (GANs), motivated by the powerful latent
representations these models are able to learn implicitly. In this work, we
propose HC-MGAN, a new technique based on GANs with multiple generators
(MGANs), which have not been explored for clustering. Our method is inspired by
the observation that each generator of a MGAN tends to generate data that
correlates with a sub-region of the real data distribution. We use this
clustered generation to train a classifier for inferring from which generator a
given image came from, thus providing a semantically meaningful clustering for
the real distribution. Additionally, we design our method so that it is
performed in a top-down hierarchical clustering tree, thus proposing the first
hierarchical DC method, to the best of our knowledge. We conduct several
experiments to evaluate the proposed method against recent DC methods,
obtaining competitive results. Last, we perform an exploratory analysis of the
hierarchical clustering tree that highlights how accurately it organizes the
data in a hierarchy of semantically coherent patterns.

    

### [[2112.03402] Nested Hyperbolic Spaces for Dimensionality Reduction and Hyperbolic NN Design](http://arxiv.org/abs/2112.03402)


  Hyperbolic neural networks have been popular in the recent past due to their
ability to represent hierarchical data sets effectively and efficiently. The
challenge in developing these networks lies in the nonlinearity of the
embedding space namely, the Hyperbolic space. Hyperbolic space is a homogeneous
Riemannian manifold of the Lorentz group. Most existing methods (with some
exceptions) use local linearization to define a variety of operations
paralleling those used in traditional deep neural networks in Euclidean spaces.
In this paper, we present a novel fully hyperbolic neural network which uses
the concept of projections (embeddings) followed by an intrinsic aggregation
and a nonlinearity all within the hyperbolic space. The novelty here lies in
the projection which is designed to project data on to a lower-dimensional
embedded hyperbolic space and hence leads to a nested hyperbolic space
representation independently useful for dimensionality reduction. The main
theoretical contribution is that the proposed embedding is proved to be
isometric and equivariant under the Lorentz transformations. This projection is
computationally efficient since it can be expressed by simple linear
operations, and, due to the aforementioned equivariance property, it allows for
weight sharing. The nested hyperbolic space representation is the core
component of our network and therefore, we first compare this ensuing nested
hyperbolic space representation with other dimensionality reduction methods
such as tangent PCA, principal geodesic analysis (PGA) and HoroPCA. Based on
this equivariant embedding, we develop a novel fully hyperbolic graph
convolutional neural network architecture to learn the parameters of the
projection. Finally, we present experiments demonstrating comparative
performance of our network on several publicly available data sets.

    

### [[2112.03404] Feature Importance-aware Graph Attention Network and Dueling Double Deep Q-Network Combined Approach for Critical Node Detection Problems](http://arxiv.org/abs/2112.03404)


  Detecting critical nodes in sparse networks is important in a variety of
application domains. A Critical Node Problem (CNP) aims to find a set of
critical nodes from a network whose deletion maximally degrades the pairwise
connectivity of the residual network. Due to its general NP-hard nature,
state-of-the-art CNP solutions are based on heuristic approaches. Domain
knowledge and trial-and-error are usually required when designing such
approaches, thus consuming considerable effort and time. This work proposes a
feature importance-aware graph attention network for node representation and
combines it with dueling double deep Q-network to create an end-to-end
algorithm to solve CNP for the first time. It does not need any
problem-specific knowledge or labeled datasets as required by most of existing
methods. Once the model is trained, it can be generalized to cope with various
types of CNPs (with different sizes and topological structures) without
re-training. Extensive experiments on 28 real-world networks show that the
proposed method is highly comparable to state-of-the-art methods. It does not
require any problem-specific knowledge and, hence, can be applicable to many
applications including those impossible ones by using the existing approaches.
It can be combined with some local search methods to further improve its
solution quality. Extensive comparison results are given to show its
effectiveness in solving CNP.

    

### [[2112.03405] A Novel Deep Parallel Time-series Relation Network for Fault Diagnosis](http://arxiv.org/abs/2112.03405)


  Considering the models that apply the contextual information of time-series
data could improve the fault diagnosis performance, some neural network
structures such as RNN, LSTM, and GRU were proposed to model the industrial
process effectively. However, these models are restricted by their serial
computation and hence cannot achieve high diagnostic efficiency. Also the
parallel CNN is difficult to implement fault diagnosis in an efficient way
because it requires larger convolution kernels or deep structure to achieve
long-term feature extraction capabilities. Besides, BERT model applies absolute
position embedding to introduce contextual information to the model, which
would bring noise to the raw data and therefore cannot be applied to fault
diagnosis directly. In order to address the above problems, a fault diagnosis
model named deep parallel time-series relation network(\textit{DPTRN}) has been
proposed in this paper. There are mainly three advantages for DPTRN: (1) Our
proposed time relationship unit is based on full multilayer
perceptron(\textit{MLP}) structure, therefore, DPTRN performs fault diagnosis
in a parallel way and improves computing efficiency significantly. (2) By
improving the absolute position embedding, our novel decoupling position
embedding unit could be applied on the fault diagnosis directly and learn
contextual information. (3) Our proposed DPTRN has obvious advantage in feature
interpretability. Our model outperforms other methods on both TE and KDD-CUP99
datasets which confirms the effectiveness, efficiency and interpretability of
the proposed DPTRN model.

    

### [[2112.03406] Equal Bits: Enforcing Equally Distributed Binary Network Weights](http://arxiv.org/abs/2112.03406)


  Binary networks are extremely efficient as they use only two symbols to
define the network: $\{+1,-1\}$. One can make the prior distribution of these
symbols a design choice. The recent IR-Net of Qin et al. argues that imposing a
Bernoulli distribution with equal priors (equal bit ratios) over the binary
weights leads to maximum entropy and thus minimizes information loss. However,
prior work cannot precisely control the binary weight distribution during
training, and therefore cannot guarantee maximum entropy. Here, we show that
quantizing using optimal transport can guarantee any bit ratio, including equal
ratios. We investigate experimentally that equal bit ratios are indeed
preferable and show that our method leads to optimization benefits. We show
that our quantization method is effective when compared to state-of-the-art
binarization methods, even when using binary weight pruning.

    

### [[2112.03407] Causal Analysis and Classification of Traffic Crash Injury Severity Using Machine Learning Algorithms](http://arxiv.org/abs/2112.03407)


  Causal analysis and classification of injury severity applying non-parametric
methods for traffic crashes has received limited attention. This study presents
a methodological framework for causal inference, using Granger causality
analysis, and injury severity classification of traffic crashes, occurring on
interstates, with different machine learning techniques including decision
trees (DT), random forest (RF), extreme gradient boosting (XGBoost), and deep
neural network (DNN). The data used in this study were obtained for traffic
crashes on all interstates across the state of Texas from a period of six years
between 2014 and 2019. The output of the proposed severity classification
approach includes three classes for fatal and severe injury (KA) crashes,
non-severe and possible injury (BC) crashes, and property damage only (PDO)
crashes. While Granger Causality helped identify the most influential factors
affecting crash severity, the learning-based models predicted the severity
classes with varying performance. The results of Granger causality analysis
identified the speed limit, surface and weather conditions, traffic volume,
presence of workzones, workers in workzones, and high occupancy vehicle (HOV)
lanes, among others, as the most important factors affecting crash severity.
The prediction performance of the classifiers yielded varying results across
the different classes. Specifically, while decision tree and random forest
classifiers provided the greatest performance for PDO and BC severities,
respectively, for the KA class, the rarest class in the data, deep neural net
classifier performed superior than all other algorithms, most likely due to its
capability of approximating nonlinear models. This study contributes to the
limited body of knowledge pertaining to causal analysis and classification
prediction of traffic crash injury severity using non-parametric approaches.

    

### [[2112.03411] Extrapolation Frameworks in Cognitive Psychology Suitable for Study of Image Classification Models](http://arxiv.org/abs/2112.03411)


  We study the functional task of deep learning image classification models and
show that image classification requires extrapolation capabilities. This
suggests that new theories have to be developed for the understanding of deep
learning as the current theory assumes models are solely interpolating, leaving
many questions about them unanswered. We investigate the pixel space and also
the feature spaces extracted from images by trained models (in their hidden
layers, including the 64-dimensional feature space in the last hidden layer of
pre-trained residual neural networks), and also the feature space extracted by
wavelets/shearlets. In all these domains, testing samples considerably fall
outside the convex hull of training sets, and image classification requires
extrapolation. In contrast to the deep learning literature, in cognitive
science, psychology, and neuroscience, extrapolation and learning are often
studied in tandem. Moreover, many aspects of human visual cognition and
behavior are reported to involve extrapolation. We propose a novel
extrapolation framework for the mathematical study of deep learning models. In
our framework, we use the term extrapolation in this specific way of
extrapolating outside the convex hull of training set (in the pixel space or
feature space) but within the specific scope defined by the training data, the
same way extrapolation is defined in many studies in cognitive science. We
explain that our extrapolation framework can provide novel answers to open
research problems about deep learning including their over-parameterization,
their training regime, out-of-distribution detection, etc. We also see that the
extent of extrapolation is negligible in learning tasks where deep learning is
reported to have no advantage over simple models.

    

### [[2112.03419] Using Image Transformations to Learn Network Structure](http://arxiv.org/abs/2112.03419)


  Many learning tasks require observing a sequence of images and making a
decision. In a transportation problem of designing and planning for shipping
boxes between nodes, we show how to treat the network of nodes and the flows
between them as images. These images have useful structural information that
can be statistically summarized. Using image compression techniques, we reduce
an image down to a set of numbers that contain interpretable geographic
information that we call geographic signatures. Using geographic signatures, we
learn network structure that can be utilized to recommend future network
connectivity. We develop a Bayesian reinforcement algorithm that takes
advantage of statistically summarized network information as priors and
user-decisions to reinforce an agent's probabilistic decision.

    

### [[2112.03421] Virtual Replay Cache](http://arxiv.org/abs/2112.03421)


  Return caching is a recent strategy that enables efficient minibatch training
with multistep estimators (e.g. the {\lambda}-return) for deep reinforcement
learning. By precomputing return estimates in sequential batches and then
storing the results in an auxiliary data structure for later sampling, the
average computation spent per estimate can be greatly reduced. Still, the
efficiency of return caching could be improved, particularly with regard to its
large memory usage and repetitive data copies. We propose a new data structure,
the Virtual Replay Cache (VRC), to address these shortcomings. When learning to
play Atari 2600 games, the VRC nearly eliminates DQN({\lambda})'s cache memory
footprint and slightly reduces the total training time on our hardware.

    

### [[2112.03432] First-Order Regret in Reinforcement Learning with Linear Function Approximation: A Robust Estimation Approach](http://arxiv.org/abs/2112.03432)


  Obtaining first-order regret bounds -- regret bounds scaling not as the
worst-case but with some measure of the performance of the optimal policy on a
given instance -- is a core question in sequential decision-making. While such
bounds exist in many settings, they have proven elusive in reinforcement
learning with large state spaces. In this work we address this gap, and show
that it is possible to obtain regret scaling as $\mathcal{O}(\sqrt{V_1^\star
K})$ in reinforcement learning with large state spaces, namely the linear MDP
setting. Here $V_1^\star$ is the value of the optimal policy and $K$ is the
number of episodes. We demonstrate that existing techniques based on least
squares estimation are insufficient to obtain this result, and instead develop
a novel robust self-normalized concentration bound based on the robust Catoni
mean estimator, which may be of independent interest.

    

### [[2112.03440] A Unified Framework for Multi-distribution Density Ratio Estimation](http://arxiv.org/abs/2112.03440)


  Binary density ratio estimation (DRE), the problem of estimating the ratio
$p_1/p_2$ given their empirical samples, provides the foundation for many
state-of-the-art machine learning algorithms such as contrastive representation
learning and covariate shift adaptation. In this work, we consider a
generalized setting where given samples from multiple distributions $p_1,
\ldots, p_k$ (for $k > 2$), we aim to efficiently estimate the density ratios
between all pairs of distributions. Such a generalization leads to important
new applications such as estimating statistical discrepancy among multiple
random variables like multi-distribution $f$-divergence, and bias correction
via multiple importance sampling. We then develop a general framework from the
perspective of Bregman divergence minimization, where each strictly convex
multivariate function induces a proper loss for multi-distribution DRE.
Moreover, we rederive the theoretical connection between multi-distribution
density ratio estimation and class probability estimation, justifying the use
of any strictly proper scoring rule composite with a link function for
multi-distribution DRE. We show that our framework leads to methods that
strictly generalize their counterparts in binary DRE, as well as new methods
that show comparable or superior performance on various downstream tasks.

    

### [[2112.03452] Location Leakage in Federated Signal Maps](http://arxiv.org/abs/2112.03452)


  We consider the problem of predicting cellular network performance (signal
maps) from measurements collected by several mobile devices. We formulate the
problem within the online federated learning framework: (i) federated learning
(FL) enables users to collaboratively train a model, while keeping their
training data on their devices; (ii) measurements are collected as users move
around over time and are used for local training in an online fashion. We
consider an honest-but-curious server, who observes the updates from target
users participating in FL and infers their location using a deep leakage from
gradients (DLG) type of attack, originally developed to reconstruct training
data of DNN image classifiers. We make the key observation that a DLG attack,
applied to our setting, infers the average location of a batch of local data,
and can thus be used to reconstruct the target users' trajectory at a coarse
granularity. We show that a moderate level of privacy protection is already
offered by the averaging of gradients, which is inherent to Federated
Averaging. Furthermore, we propose an algorithm that devices can apply locally
to curate the batches used for local updates, so as to effectively protect
their location privacy without hurting utility. Finally, we show that the
effect of multiple users participating in FL depends on the similarity of their
trajectories. To the best of our knowledge, this is the first study of DLG
attacks in the setting of FL from crowdsourced spatio-temporal data.

    

### [[2112.03455] Hybrid guiding: A multi-resolution refinement approach for semantic segmentation of gigapixel histopathological images](http://arxiv.org/abs/2112.03455)


  Histopathological cancer diagnostics has become more complex, and the
increasing number of biopsies is a challenge for most pathology laboratories.
Thus, development of automatic methods for evaluation of histopathological
cancer sections would be of value. In this study, we used 624 whole slide
images (WSIs) of breast cancer from a Norwegian cohort. We propose a cascaded
convolutional neural network design, called H2G-Net, for semantic segmentation
of gigapixel histopathological images. The design involves a detection stage
using a patch-wise method, and a refinement stage using a convolutional
autoencoder. To validate the design, we conducted an ablation study to assess
the impact of selected components in the pipeline on tumour segmentation.
Guiding segmentation, using hierarchical sampling and deep heatmap refinement,
proved to be beneficial when segmenting the histopathological images. We found
a significant improvement when using a refinement network for postprocessing
the generated tumour segmentation heatmaps. The overall best design achieved a
Dice score of 0.933 on an independent test set of 90 WSIs. The design
outperformed single-resolution approaches, such as cluster-guided, patch-wise
high-resolution classification using MobileNetV2 (0.872) and a low-resolution
U-Net (0.874). In addition, segmentation on a representative x400 WSI took ~58
seconds, using only the CPU. The findings demonstrate the potential of
utilizing a refinement network to improve patch-wise predictions. The solution
is efficient and does not require overlapping patch inference or ensembling.
Furthermore, we showed that deep neural networks can be trained using a random
sampling scheme that balances on multiple different labels simultaneously,
without the need of storing patches on disk. Future work should involve more
efficient patch generation and sampling, as well as improved clustering.

    

### [[2112.03459] A Novel Convergence Analysis for Algorithms of the Adam Family](http://arxiv.org/abs/2112.03459)


  Since its invention in 2014, the Adam optimizer has received tremendous
attention. On one hand, it has been widely used in deep learning and many
variants have been proposed, while on the other hand their theoretical
convergence property remains to be a mystery. It is far from satisfactory in
the sense that some studies require strong assumptions about the updates, which
are not necessarily applicable in practice, while other studies still follow
the original problematic convergence analysis of Adam, which was shown to be
not sufficient to ensure convergence. Although rigorous convergence analysis
exists for Adam, they impose specific requirements on the update of the
adaptive step size, which are not generic enough to cover many other variants
of Adam. To address theses issues, in this extended abstract, we present a
simple and generic proof of convergence for a family of Adam-style methods
(including Adam, AMSGrad, Adabound, etc.). Our analysis only requires an
increasing or large "momentum" parameter for the first-order moment, which is
indeed the case used in practice, and a boundness condition on the adaptive
factor of the step size, which applies to all variants of Adam under mild
conditions of stochastic gradients. We also establish a variance diminishing
result for the used stochastic gradient estimators. Indeed, our analysis of
Adam is so simple and generic that it can be leveraged to establish the
convergence for solving a broader family of non-convex optimization problems,
including min-max, compositional, and bilevel optimization problems. For the
full (earlier) version of this extended abstract, please refer to
arXiv:2104.14840.

    

### [[2112.03461] GraphPAS: Parallel Architecture Search for Graph Neural Networks](http://arxiv.org/abs/2112.03461)


  Graph neural architecture search has received a lot of attention as Graph
Neural Networks (GNNs) has been successfully applied on the non-Euclidean data
recently. However, exploring all possible GNNs architectures in the huge search
space is too time-consuming or impossible for big graph data. In this paper, we
propose a parallel graph architecture search (GraphPAS) framework for graph
neural networks. In GraphPAS, we explore the search space in parallel by
designing a sharing-based evolution learning, which can improve the search
efficiency without losing the accuracy. Additionally, architecture information
entropy is adopted dynamically for mutation selection probability, which can
reduce space exploration. The experimental result shows that GraphPAS
outperforms state-of-art models with efficiency and accuracy simultaneously.

    

### [[2112.03465] Federated Deep Reinforcement Learning for the Distributed Control of NextG Wireless Networks](http://arxiv.org/abs/2112.03465)


  Next Generation (NextG) networks are expected to support demanding tactile
internet applications such as augmented reality and connected autonomous
vehicles. Whereas recent innovations bring the promise of larger link capacity,
their sensitivity to the environment and erratic performance defy traditional
model-based control rationales. Zero-touch data-driven approaches can improve
the ability of the network to adapt to the current operating conditions. Tools
such as reinforcement learning (RL) algorithms can build optimal control policy
solely based on a history of observations. Specifically, deep RL (DRL), which
uses a deep neural network (DNN) as a predictor, has been shown to achieve good
performance even in complex environments and with high dimensional inputs.
However, the training of DRL models require a large amount of data, which may
limit its adaptability to ever-evolving statistics of the underlying
environment. Moreover, wireless networks are inherently distributed systems,
where centralized DRL approaches would require excessive data exchange, while
fully distributed approaches may result in slower convergence rates and
performance degradation. In this paper, to address these challenges, we propose
a federated learning (FL) approach to DRL, which we refer to federated DRL
(F-DRL), where base stations (BS) collaboratively train the embedded DNN by
only sharing models' weights rather than training data. We evaluate two
distinct versions of F-DRL, value and policy based, and show the superior
performance they achieve compared to distributed and centralized DRL.

    

### [[2112.03467] Spectral Complexity-scaled Generalization Bound of Complex-valued Neural Networks](http://arxiv.org/abs/2112.03467)


  Complex-valued neural networks (CVNNs) have been widely applied to various
fields, especially signal processing and image recognition. However, few works
focus on the generalization of CVNNs, albeit it is vital to ensure the
performance of CVNNs on unseen data. This paper is the first work that proves a
generalization bound for the complex-valued neural network. The bound scales
with the spectral complexity, the dominant factor of which is the spectral norm
product of weight matrices. Further, our work provides a generalization bound
for CVNNs when training data is sequential, which is also affected by the
spectral complexity. Theoretically, these bounds are derived via Maurey
Sparsification Lemma and Dudley Entropy Integral. Empirically, we conduct
experiments by training complex-valued convolutional neural networks on
different datasets: MNIST, FashionMNIST, CIFAR-10, CIFAR-100, Tiny ImageNet,
and IMDB. Spearman's rank-order correlation coefficients and the corresponding
p values on these datasets give strong proof that the spectral complexity of
the network, measured by the weight matrices spectral norm product, has a
statistically significant correlation with the generalization ability.

    

### [[2112.03469] Emulating Spatio-Temporal Realizations of Three-Dimensional Isotropic Turbulence via Deep Sequence Learning Models](http://arxiv.org/abs/2112.03469)


  We use a data-driven approach to model a three-dimensional turbulent flow
using cutting-edge Deep Learning techniques. The deep learning framework
incorporates physical constraints on the flow, such as preserving
incompressibility and global statistical invariants of velocity gradient
tensor. The accuracy of the model is assessed using statistical and
physics-based metrics. The data set comes from Direct Numerical Simulation of
an incompressible, statistically stationary, isotropic turbulent flow in a
cubic box. Since the size of the dataset is memory intensive, we first generate
a low-dimensional representation of the velocity data, and then pass it to a
sequence prediction network that learns the spatial and temporal correlations
of the underlying data. The dimensionality reduction is performed via
extraction using Vector-Quantized Autoencoder (VQ-AE), which learns the
discrete latent variables. For the sequence forecasting, the idea of
Transformer architecture from natural language processing is used, and its
performance compared against more standard Recurrent Networks (such as
Convolutional LSTM). These architectures are designed and trained to perform a
sequence to sequence multi-class classification task in which they take an
input sequence with a fixed length (k) and predict a sequence with a fixed
length (p), representing the future time instants of the flow. Our results for
the short-term predictions show that the accuracy of results for both models
deteriorates across predicted snapshots due to autoregressive nature of the
predictions. Based on our diagnostics tests, the trained Conv-Transformer model
outperforms the Conv-LSTM one and can accurately, both quantitatively and
qualitatively, retain the large scales and capture well the inertial scales of
flow but fails at recovering the small and intermittent fluid motions.

    

### [[2112.03476] Defending against Model Stealing via Verifying Embedded External Features](http://arxiv.org/abs/2112.03476)


  Obtaining a well-trained model involves expensive data collection and
training procedures, therefore the model is a valuable intellectual property.
Recent studies revealed that adversaries can `steal' deployed models even when
they have no training samples and can not get access to the model parameters or
structures. Currently, there were some defense methods to alleviate this
threat, mostly by increasing the cost of model stealing. In this paper, we
explore the defense from another angle by verifying whether a suspicious model
contains the knowledge of defender-specified \emph{external features}.
Specifically, we embed the external features by tempering a few training
samples with style transfer. We then train a meta-classifier to determine
whether a model is stolen from the victim. This approach is inspired by the
understanding that the stolen models should contain the knowledge of features
learned by the victim model. We examine our method on both CIFAR-10 and
ImageNet datasets. Experimental results demonstrate that our method is
effective in detecting different types of model stealing simultaneously, even
if the stolen model is obtained via a multi-stage stealing process. The codes
for reproducing main results are available at Github
(this https URL).

    

### [[2112.03477] BDFA: A Blind Data Adversarial Bit-flip Attack on Deep Neural Networks](http://arxiv.org/abs/2112.03477)


  Adversarial bit-flip attack (BFA) on Neural Network weights can result in
catastrophic accuracy degradation by flipping a very small number of bits. A
major drawback of prior bit flip attack techniques is their reliance on test
data. This is frequently not possible for applications that contain sensitive
or proprietary data. In this paper, we propose Blind Data Adversarial Bit-flip
Attack (BDFA), a novel technique to enable BFA without any access to the
training or testing data. This is achieved by optimizing for a synthetic
dataset, which is engineered to match the statistics of batch normalization
across different layers of the network and the targeted label. Experimental
results show that BDFA could decrease the accuracy of ResNet50 significantly
from 75.96\% to 13.94\% with only 4 bits flips.

    

### [[2112.03478] Generative Adversarial Networks for Labeled Data Creation for Structural Damage Detection](http://arxiv.org/abs/2112.03478)


  There has been a drastic progression in the field of Data Science in the last
few decades and other disciplines have been continuously benefitting from it.
Structural Health Monitoring (SHM) is one of those fields that use Artificial
Intelligence (AI) such as Machine Learning (ML) and Deep Learning (DL)
algorithms for condition assessment of civil structures based on the collected
data. The ML and DL methods require plenty of data for training procedures;
however, in SHM, data collection from civil structures is very exhaustive;
particularly getting useful data (damage associated data) can be very
challenging. This paper uses 1-D Wasserstein Deep Convolutional Generative
Adversarial Networks using Gradient Penalty (1-D WDCGAN-GP) for synthetic
labeled vibration data generation. Then, implements structural damage detection
on different levels of synthetically enhanced vibration datasets by using 1-D
Deep Convolutional Neural Network (1-D DCNN). The damage detection results show
that the 1-D WDCGAN-GP can be successfully utilized to tackle data scarcity in
vibration-based damage diagnostics of civil structures. Keywords: Structural
Health Monitoring (SHM), Structural Damage Diagnostics, Structural Damage
Detection, 1-D Deep Convolutional Neural Networks (1-D DCNN), 1-D Generative
Adversarial Networks (1-D GAN), Deep Convolutional Generative Adversarial
Networks (DCGAN), Wasserstein Generative Adversarial Networks with Gradient
Penalty (WGAN-GP)

    

### [[2112.03482] Combining Learning from Human Feedback and Knowledge Engineering to Solve Hierarchical Tasks in Minecraft](http://arxiv.org/abs/2112.03482)


  Real-world tasks of interest are generally poorly defined by human-readable
descriptions and have no pre-defined reward signals unless it is defined by a
human designer. Conversely, data-driven algorithms are often designed to solve
a specific, narrowly defined, task with performance metrics that drives the
agent's learning. In this work, we present the solution that won first place
and was awarded the most human-like agent in the 2021 NeurIPS Competition
MineRL BASALT Challenge: Learning from Human Feedback in Minecraft, which
challenged participants to use human data to solve four tasks defined only by a
natural language description and no reward function. Our approach uses the
available human demonstration data to train an imitation learning policy for
navigation and additional human feedback to train an image classifier. These
modules, together with an estimated odometry map, are then combined into a
state-machine designed based on human knowledge of the tasks that breaks them
down in a natural hierarchy and controls which macro behavior the learning
agent should follow at any instant. We compare this hybrid intelligence
approach to both end-to-end machine learning and pure engineered solutions,
which are then judged by human evaluators. Codebase is available at
this https URL.

    

### [[2112.03487] Enhanced Exploration in Neural Feature Selection for Deep Click-Through Rate Prediction Models via Ensemble of Gating Layers](http://arxiv.org/abs/2112.03487)


  Feature selection has been an essential step in developing industry-scale
deep Click-Through Rate (CTR) prediction systems. The goal of neural feature
selection (NFS) is to choose a relatively small subset of features with the
best explanatory power as a means to remove redundant features and reduce
computational cost. Inspired by gradient-based neural architecture search (NAS)
and network pruning methods, people have tackled the NFS problem with Gating
approach that inserts a set of differentiable binary gates to drop less
informative features. The binary gates are optimized along with the network
parameters in an efficient end-to-end manner. In this paper, we analyze the
gradient-based solution from an exploration-exploitation perspective and use
empirical results to show that Gating approach might suffer from insufficient
exploration. To improve the exploration capacity of gradient-based solutions,
we propose a simple but effective ensemble learning approach, named Ensemble
Gating. We choose two public datasets, namely Avazu and Criteo, to evaluate
this approach. Our experiments show that, without adding any computational
overhead or introducing any hyper-parameter (except the size of the ensemble),
our method is able to consistently improve Gating approach and find a better
subset of features on the two datasets with three different underlying deep CTR
prediction models.

    

### [[2112.03491] Explicitly antisymmetrized neural network layers for variational Monte Carlo simulation](http://arxiv.org/abs/2112.03491)


  The combination of neural networks and quantum Monte Carlo methods has arisen
as a path forward for highly accurate electronic structure calculations.
Previous proposals have combined equivariant neural network layers with an
antisymmetric layer to satisfy the antisymmetry requirements of the electronic
wavefunction. However, to date it is unclear if one can represent antisymmetric
functions of physical interest, and it is difficult to measure the
expressiveness of the antisymmetric layer. This work attempts to address this
problem by introducing explicitly antisymmetrized universal neural network
layers as a diagnostic tool. We first introduce a generic antisymmetric (GA)
layer, which we use to replace the entire antisymmetric layer of the highly
accurate ansatz known as the FermiNet. We demonstrate that the resulting
FermiNet-GA architecture can yield effectively the exact ground state energy
for small systems. We then consider a factorized antisymmetric (FA) layer which
more directly generalizes the FermiNet by replacing products of determinants
with products of antisymmetrized neural networks. Interestingly, the resulting
FermiNet-FA architecture does not outperform the FermiNet. This suggests that
the sum of products of antisymmetries is a key limiting aspect of the FermiNet
architecture. To explore this further, we investigate a slight modification of
the FermiNet called the full determinant mode, which replaces each product of
determinants with a single combined determinant. The full single-determinant
FermiNet closes a large part of the gap between the standard single-determinant
FermiNet and FermiNet-GA. Surprisingly, on the nitrogen molecule at a
dissociating bond length of 4.0 Bohr, the full single-determinant FermiNet can
significantly outperform the standard 64-determinant FermiNet, yielding an
energy within 0.4 kcal/mol of the best available computational benchmark.

    

### [[2112.03499] A Piece-wise Polynomial Filtering Approach for Graph Neural Networks](http://arxiv.org/abs/2112.03499)


  Graph Neural Networks (GNNs) exploit signals from node features and the input
graph topology to improve node classification task performance. However, these
models tend to perform poorly on heterophilic graphs, where connected nodes
have different labels. Recently proposed GNNs work across graphs having varying
levels of homophily. Among these, models relying on polynomial graph filters
have shown promise. We observe that solutions to these polynomial graph filter
models are also solutions to an overdetermined system of equations. It suggests
that in some instances, the model needs to learn a reasonably high order
polynomial. On investigation, we find the proposed models ineffective at
learning such polynomials due to their designs. To mitigate this issue, we
perform an eigendecomposition of the graph and propose to learn multiple
adaptive polynomial filters acting on different subsets of the spectrum. We
theoretically and empirically show that our proposed model learns a better
filter, thereby improving classification accuracy. We study various aspects of
our proposed model including, dependency on the number of eigencomponents
utilized, latent polynomial filters learned, and performance of the individual
polynomials on the node classification task. We further show that our model is
scalable by evaluating over large graphs. Our model achieves performance gains
of up to 5% over the state-of-the-art models and outperforms existing
polynomial filter-based approaches in general.

    

### [[2112.03502] A Generic Approach for Enhancing GANs by Regularized Latent Optimization](http://arxiv.org/abs/2112.03502)


  With the rapidly growing model complexity and data volume, training deep
generative models (DGMs) for better performance has becoming an increasingly
more important challenge. Previous research on this problem has mainly focused
on improving DGMs by either introducing new objective functions or designing
more expressive model architectures. However, such approaches often introduce
significantly more computational and/or designing overhead. To resolve such
issues, we introduce in this paper a generic framework called {\em
generative-model inference} that is capable of enhancing pre-trained GANs
effectively and seamlessly in a variety of application scenarios. Our basic
idea is to efficiently infer the optimal latent distribution for the given
requirements using Wasserstein gradient flow techniques, instead of re-training
or fine-tuning pre-trained model parameters. Extensive experimental results on
applications like image generation, image translation, text-to-image
generation, image inpainting, and text-guided image editing suggest the
effectiveness and superiority of our proposed framework.

    

### [[2112.03508] Training Deep Models to be Explained with Fewer Examples](http://arxiv.org/abs/2112.03508)


  Although deep models achieve high predictive performance, it is difficult for
humans to understand the predictions they made. Explainability is important for
real-world applications to justify their reliability. Many example-based
explanation methods have been proposed, such as representer point selection,
where an explanation model defined by a set of training examples is used for
explaining a prediction model. For improving the interpretability, reducing the
number of examples in the explanation model is important. However, the
explanations with fewer examples can be unfaithful since it is difficult to
approximate prediction models well by such example-based explanation models.
The unfaithful explanations mean that the predictions by the explainable model
are different from those by the prediction model. We propose a method for
training deep models such that their predictions are faithfully explained by
explanation models with a small number of examples. We train the prediction and
explanation models simultaneously with a sparse regularizer for reducing the
number of examples. The proposed method can be incorporated into any neural
network-based prediction models. Experiments using several datasets demonstrate
that the proposed method improves faithfulness while keeping the predictive
performance.

    

### [[2112.03518] Genetic Algorithm for Constrained Molecular Inverse Design](http://arxiv.org/abs/2112.03518)


  A genetic algorithm is suitable for exploring large search spaces as it finds
an approximate solution. Because of this advantage, genetic algorithm is
effective in exploring vast and unknown space such as molecular search space.
Though the algorithm is suitable for searching vast chemical space, it is
difficult to optimize pharmacological properties while maintaining molecular
substructure. To solve this issue, we introduce a genetic algorithm featuring a
constrained molecular inverse design. The proposed algorithm successfully
produces valid molecules for crossover and mutation. Furthermore, it optimizes
specific properties while adhering to structural constraints using a two-phase
optimization. Experiments prove that our algorithm effectively finds molecules
that satisfy specific properties while maintaining structural constraints.

    

### [[2112.03528] Physics guided deep learning generative models for crystal materials discovery](http://arxiv.org/abs/2112.03528)


  Deep learning based generative models such as deepfake have been able to
generate amazing images and videos. However, these models may need significant
transformation when applied to generate crystal materials structures in which
the building blocks, the physical atoms are very different from the pixels.
Naively transferred generative models tend to generate a large portion of
physically infeasible crystal structures that are not stable or synthesizable.
Herein we show that by exploiting and adding physically oriented data
augmentation, loss function terms, and post processing, our deep adversarial
network (GAN) based generative models can now generate crystal structures with
higher physical feasibility and expand our previous models which can only
create cubic structures.

    

### [[2112.03541] Predicting the Travel Distance of Patients to Access Healthcare using Deep Neural Networks](http://arxiv.org/abs/2112.03541)


  Objective: Improving geographical access remains a key issue in determining
the sufficiency of regional medical resources during health policy design.
However, patient choices can be the result of the complex interactivity of
various factors. The aim of this study is to propose a deep neural network
approach to model the complex decision of patient choice in travel distance to
access care, which is an important indicator for policymaking in allocating
resources. Method: We used the 4-year nationwide insurance data of Taiwan and
accumulated the possible features discussed in earlier literature. This study
proposes the use of a convolutional neural network (CNN)-based framework to
make predictions. The model performance was tested against other machine
learning methods. The proposed framework was further interpreted using
Integrated Gradients (IG) to analyze the feature weights. Results: We
successfully demonstrated the effectiveness of using a CNN-based framework to
predict the travel distance of patients, achieving an accuracy of 0.968, AUC of
0.969, sensitivity of 0.960, and specificity of 0.989. The CNN-based framework
outperformed all other methods. In this research, the IG weights are
potentially explainable; however, the relationship does not correspond to known
indicators in public health, similar to common consensus. Conclusions: Our
results demonstrate the feasibility of the deep learning-based travel distance
prediction model. It has the potential to guide policymaking in resource
allocation.

    

### [[2112.03547] Self-Organized Polynomial-Time Coordination Graphs](http://arxiv.org/abs/2112.03547)


  Coordination graph is a promising approach to model agent collaboration in
multi-agent reinforcement learning. It factorizes a large multi-agent system
into a suite of overlapping groups that represent the underlying coordination
dependencies. One critical challenge in this paradigm is the complexity of
computing maximum-value actions for a graph-based value factorization. It
refers to the decentralized constraint optimization problem (DCOP), which and
whose constant-ratio approximation are NP-hard problems. To bypass this
fundamental hardness, this paper proposes a novel method, named Self-Organized
Polynomial-time Coordination Graphs (SOP-CG), which uses structured graph
classes to guarantee the optimality of the induced DCOPs with sufficient
function expressiveness. We extend the graph topology to be state-dependent,
formulate the graph selection as an imaginary agent, and finally derive an
end-to-end learning paradigm from the unified Bellman optimality equation. In
experiments, we show that our approach learns interpretable graph topologies,
induces effective coordination, and improves performance across a variety of
cooperative multi-agent tasks.

    

### [[2112.03548] Private Robust Estimation by Stabilizing Convex Relaxations](http://arxiv.org/abs/2112.03548)


  We give the first polynomial time and sample $(\epsilon,
\delta)$-differentially private (DP) algorithm to estimate the mean, covariance
and higher moments in the presence of a constant fraction of adversarial
outliers. Our algorithm succeeds for families of distributions that satisfy two
well-studied properties in prior works on robust estimation: certifiable
subgaussianity of directional moments and certifiable hypercontractivity of
degree 2 polynomials. Our recovery guarantees hold in the "right
affine-invariant norms": Mahalanobis distance for mean, multiplicative spectral
and relative Frobenius distance guarantees for covariance and injective norms
for higher moments. Prior works obtained private robust algorithms for mean
estimation of subgaussian distributions with bounded covariance. For covariance
estimation, ours is the first efficient algorithm (even in the absence of
outliers) that succeeds without any condition-number assumptions.
Our algorithms arise from a new framework that provides a general blueprint
for modifying convex relaxations for robust estimation to satisfy strong
worst-case stability guarantees in the appropriate parameter norms whenever the
algorithms produce witnesses of correctness in their run. We verify such
guarantees for a modification of standard sum-of-squares (SoS) semidefinite
programming relaxations for robust estimation. Our privacy guarantees are
obtained by combining stability guarantees with a new "estimate dependent"
noise injection mechanism in which noise scales with the eigenvalues of the
estimated covariance. We believe this framework will be useful more generally
in obtaining DP counterparts of robust estimators.
Independently of our work, Ashtiani and Liaw [AL21] also obtained a
polynomial time and sample private robust estimation algorithm for Gaussian
distributions.

    

### [[2112.03555] Federated Causal Discovery](http://arxiv.org/abs/2112.03555)


  Causal discovery aims to learn a causal graph from observational data. To
date, most causal discovery methods require data to be stored in a central
server. However, data owners gradually refuse to share their personalized data
to avoid privacy leakage, making this task more troublesome by cutting off the
first step. A puzzle arises: $\textit{how do we infer causal relations from
decentralized data?}$ In this paper, with the additive noise model assumption
of data, we take the first step in developing a gradient-based learning
framework named DAG-Shared Federated Causal Discovery (DS-FCD), which can learn
the causal graph without directly touching local data and naturally handle the
data heterogeneity. DS-FCD benefits from a two-level structure of each local
model. The first level learns the causal graph and communicates with the server
to get model information from other clients, while the second level
approximates causal mechanisms and personally updates from its own data to
accommodate the data heterogeneity. Moreover, DS-FCD formulates the overall
learning task as a continuous optimization problem by taking advantage of an
equality acyclicity constraint, which can be naturally solved by gradient
descent methods. Extensive experiments on both synthetic and real-world
datasets verify the efficacy of the proposed method.

    

### [[2112.03558] Graph Neural Controlled Differential Equations for Traffic Forecasting](http://arxiv.org/abs/2112.03558)


  Traffic forecasting is one of the most popular spatio-temporal tasks in the
field of machine learning. A prevalent approach in the field is to combine
graph convolutional networks and recurrent neural networks for the
spatio-temporal processing. There has been fierce competition and many novel
methods have been proposed. In this paper, we present the method of
spatio-temporal graph neural controlled differential equation (STG-NCDE).
Neural controlled differential equations (NCDEs) are a breakthrough concept for
processing sequential data. We extend the concept and design two NCDEs: one for
the temporal processing and the other for the spatial processing. After that,
we combine them into a single framework. We conduct experiments with 6
benchmark datasets and 20 baselines. STG-NCDE shows the best accuracy in all
cases, outperforming all those 20 baselines by non-trivial margins.

    

### [[2112.03562] CMA-CLIP: Cross-Modality Attention CLIP for Image-Text Classification](http://arxiv.org/abs/2112.03562)


  Modern Web systems such as social media and e-commerce contain rich contents
expressed in images and text. Leveraging information from multi-modalities can
improve the performance of machine learning tasks such as classification and
recommendation. In this paper, we propose the Cross-Modality Attention
Contrastive Language-Image Pre-training (CMA-CLIP), a new framework which
unifies two types of cross-modality attentions, sequence-wise attention and
modality-wise attention, to effectively fuse information from image and text
pairs. The sequence-wise attention enables the framework to capture the
fine-grained relationship between image patches and text tokens, while the
modality-wise attention weighs each modality by its relevance to the downstream
tasks. In addition, by adding task specific modality-wise attentions and
multilayer perceptrons, our proposed framework is capable of performing
multi-task classification with multi-modalities.
We conduct experiments on a Major Retail Website Product Attribute (MRWPA)
dataset and two public datasets, Food101 and Fashion-Gen. The results show that
CMA-CLIP outperforms the pre-trained and fine-tuned CLIP by an average of 11.9%
in recall at the same level of precision on the MRWPA dataset for multi-task
classification. It also surpasses the state-of-the-art method on Fashion-Gen
Dataset by 5.5% in accuracy and achieves competitive performance on Food101
Dataset. Through detailed ablation studies, we further demonstrate the
effectiveness of both cross-modality attention modules and our method's
robustness against noise in image and text inputs, which is a common challenge
in practice.

    

### [[2112.03566] More layers! End-to-end regression and uncertainty on tabular data with deep learning](http://arxiv.org/abs/2112.03566)


  This paper attempts to analyze the effectiveness of deep learning for tabular
data processing. It is believed that decision trees and their ensembles is the
leading method in this domain, and deep neural networks must be content with
computer vision and so on. But the deep neural network is a framework for
building gradient-based hierarchical representations, and this key feature
should be able to provide the best processing of generic structured (tabular)
data, not just image matrices and audio spectrograms. This problem is
considered through the prism of the Weather Prediction track in the Yandex
Shifts challenge (in other words, the Yandex Shifts Weather task). This task is
a variant of the classical tabular data regression problem. It is also
connected with another important problem: generalization and uncertainty in
machine learning. This paper proposes an end-to-end algorithm for solving the
problem of regression with uncertainty on tabular data, which is based on the
combination of four ideas: 1) deep ensemble of self-normalizing neural
networks, 2) regression as parameter estimation of the Gaussian target error
distribution, 3) hierarchical multitask learning, and 4) simple data
preprocessing. Three modifications of the proposed algorithm form the top-3
leaderboard of the Yandex Shifts Weather challenge respectively. This paper
considers that this success has occurred due to the fundamental properties of
the deep learning algorithm, and tries to prove this.

    

### [[2112.03568] Unsupervised Learning of Compositional Scene Representations from Multiple Unspecified Viewpoints](http://arxiv.org/abs/2112.03568)


  Visual scenes are extremely rich in diversity, not only because there are
infinite combinations of objects and background, but also because the
observations of the same scene may vary greatly with the change of viewpoints.
When observing a visual scene that contains multiple objects from multiple
viewpoints, humans are able to perceive the scene in a compositional way from
each viewpoint, while achieving the so-called "object constancy" across
different viewpoints, even though the exact viewpoints are untold. This ability
is essential for humans to identify the same object while moving and to learn
from vision efficiently. It is intriguing to design models that have the
similar ability. In this paper, we consider a novel problem of learning
compositional scene representations from multiple unspecified viewpoints
without using any supervision, and propose a deep generative model which
separates latent representations into a viewpoint-independent part and a
viewpoint-dependent part to solve this problem. To infer latent
representations, the information contained in different viewpoints is
iteratively integrated by neural networks. Experiments on several specifically
designed synthetic datasets have shown that the proposed method is able to
effectively learn from multiple unspecified viewpoints.

    

### [[2112.03570] Membership Inference Attacks From First Principles](http://arxiv.org/abs/2112.03570)


  A membership inference attack allows an adversary to query a trained machine
learning model to predict whether or not a particular example was contained in
the model's training dataset. These attacks are currently evaluated using
average-case "accuracy" metrics that fail to characterize whether the attack
can confidently identify any members of the training set. We argue that attacks
should instead be evaluated by computing their true-positive rate at low (e.g.,
<0.1%) false-positive rates, and find most prior attacks perform poorly when
evaluated in this way. To address this we develop a Likelihood Ratio Attack
(LiRA) that carefully combines multiple ideas from the literature. Our attack
is 10x more powerful at low false-positive rates, and also strictly dominates
prior attacks on existing metrics.

    

### [[2112.03571] Neural Networks for Infectious Diseases Detection: Prospects and Challenges](http://arxiv.org/abs/2112.03571)


  Artificial neural network (ANN) ability to learn, correct errors, and
transform a large amount of raw data into useful medical decisions for
treatment and care have increased its popularity for enhanced patient safety
and quality of care. Therefore, this paper reviews the critical role of ANNs in
providing valuable insights for patients' healthcare decisions and efficient
disease diagnosis. We thoroughly review different types of ANNs presented in
the existing literature that advanced ANNs adaptation for complex applications.
Moreover, we also investigate ANN's advances for various disease diagnoses and
treatments such as viral, skin, cancer, and COVID-19. Furthermore, we propose a
novel deep Convolutional Neural Network (CNN) model called ConXNet for
improving the detection accuracy of COVID-19 disease. ConXNet is trained and
tested using different datasets, and it achieves more than 97% detection
accuracy and precision, which is significantly better than existing models.
Finally, we highlight future research directions and challenges such as
complexity of the algorithms, insufficient available data, privacy and
security, and integration of biosensing with ANNs. These research directions
require considerable attention for improving the scope of ANNs for medical
diagnostic and treatment applications.

    

### [[2112.03572] Question Answering Survey: Directions, Challenges, Datasets, Evaluation Matrices](http://arxiv.org/abs/2112.03572)


  The usage and amount of information available on the internet increase over
the past decade. This digitization leads to the need for automated answering
system to extract fruitful information from redundant and transitional
knowledge sources. Such systems are designed to cater the most prominent answer
from this giant knowledge source to the user query using natural language
understanding (NLU) and thus eminently depends on the Question-answering(QA)
field.
Question answering involves but not limited to the steps like mapping of user
question to pertinent query, retrieval of relevant information, finding the
best suitable answer from the retrieved information etc. The current
improvement of deep learning models evince compelling performance improvement
in all these tasks.
In this review work, the research directions of QA field are analyzed based
on the type of question, answer type, source of evidence-answer, and modeling
approach. This detailing followed by open challenges of the field like
automatic question generation, similarity detection and, low resource
availability for a language. In the end, a survey of available datasets and
evaluation measures is presented.

    

### [[2112.03575] MESA: Offline Meta-RL for Safe Adaptation and Fault Tolerance](http://arxiv.org/abs/2112.03575)


  Safe exploration is critical for using reinforcement learning (RL) in
risk-sensitive environments. Recent work learns risk measures which measure the
probability of violating constraints, which can then be used to enable safety.
However, learning such risk measures requires significant interaction with the
environment, resulting in excessive constraint violations during learning.
Furthermore, these measures are not easily transferable to new environments. We
cast safe exploration as an offline meta-RL problem, where the objective is to
leverage examples of safe and unsafe behavior across a range of environments to
quickly adapt learned risk measures to a new environment with previously unseen
dynamics. We then propose MEta-learning for Safe Adaptation (MESA), an approach
for meta-learning a risk measure for safe RL. Simulation experiments across 5
continuous control domains suggest that MESA can leverage offline data from a
range of different environments to reduce constraint violations in unseen
environments by up to a factor of 2 while maintaining task performance. See
this https URL for code and supplementary material.

    

### [[2112.03588] A deep language model to predict metabolic network equilibria](http://arxiv.org/abs/2112.03588)


  We show that deep learning models, and especially architectures like the
Transformer, originally intended for natural language, can be trained on
randomly generated datasets to predict to very high accuracy both the
qualitative and quantitative features of metabolic networks. Using standard
mathematical techniques, we create large sets (40 million elements) of random
networks that can be used to train our models. These trained models can predict
network equilibrium on random graphs in more than 99% of cases. They can also
generalize to graphs with different structure than those encountered at
training. Finally, they can predict almost perfectly the equilibria of a small
set of known biological networks. Our approach is both very economical in
experimental data and uses only small and shallow deep-learning model, far from
the large architectures commonly used in machine translation. Such results pave
the way for larger use of deep learning models for problems related to
biological networks in key areas such as quantitative systems pharmacology,
systems biology, and synthetic biology.

    

### [[2112.03595] State-of-the-art predictive and prescriptive analytics for IEEE CIS 3rd Technical Challenge](http://arxiv.org/abs/2112.03595)


  In this paper, we describe our proposed methodology to approach the
predict+optimise challenge introduced in the IEEE CIS 3rd Technical Challenge.
The predictive model employs an ensemble of LightGBM models and the
prescriptive analysis employs mathematical optimisation to efficiently
prescribe solutions that minimise the average cost over multiple scenarios. Our
solutions ranked 1st in the optimisation and 2nd in the prediction challenge of
the competition.

    

### [[2112.03603] Handwritten Mathematical Expression Recognition via Attention Aggregation based Bi-directional Mutual Learning](http://arxiv.org/abs/2112.03603)


  Handwritten mathematical expression recognition aims to automatically
generate LaTeX sequences from given images. Currently, attention-based
encoder-decoder models are widely used in this task. They typically generate
target sequences in a left-to-right (L2R) manner, leaving the right-to-left
(R2L) contexts unexploited. In this paper, we propose an Attention aggregation
based Bi-directional Mutual learning Network (ABM) which consists of one shared
encoder and two parallel inverse decoders (L2R and R2L). The two decoders are
enhanced via mutual distillation, which involves one-to-one knowledge transfer
at each training step, making full use of the complementary information from
two inverse directions. Moreover, in order to deal with mathematical symbols in
diverse scales, an Attention Aggregation Module (AAM) is proposed to
effectively integrate multi-scale coverage attentions. Notably, in the
inference phase, given that the model already learns knowledge from two inverse
directions, we only use the L2R branch for inference, keeping the original
parameter size and inference speed. Extensive experiments demonstrate that our
proposed approach achieves the recognition accuracy of 56.85 % on CROHME 2014,
52.92 % on CROHME 2016, and 53.96 % on CROHME 2019 without data augmentation
and model ensembling, substantially outperforming the state-of-the-art methods.
The source code is available in the supplementary materials.

    

### [[2112.03609] Predict and Optimize: Through the Lens of Learning to Rank](http://arxiv.org/abs/2112.03609)


  In the last years predict-and-optimize approaches (Elmachtoub and Grigas
2021; Wilder, Dilkina, and Tambe 2019) have received increasing attention.
These problems have the settings where the predictions of predictive machine
learning (ML) models are fed to downstream optimization problems for decision
making. Predict-and-optimize approaches propose to train the ML models, often
neural network models, by directly optimizing the quality of decisions made by
the optimization solvers. However, one major bottleneck of predict-and-optimize
approaches is solving the optimization problem for each training instance at
every epoch. To address this challenge, Mulamba et al. (2021) propose noise
contrastive estimation by caching feasible solutions. In this work, we show the
noise contrastive estimation can be considered a case of learning to rank the
solution cache. We also develop pairwise and listwise ranking loss functions,
which can be differentiated in closed form without the need of solving the
optimization problem. By training with respect to these surrogate loss
function, we empirically show that we are able to minimize the regret of the
predictions.

    

### [[2112.03621] Permutation Equivariant Generative Adversarial Networks for Graphs](http://arxiv.org/abs/2112.03621)


  One of the most discussed issues in graph generative modeling is the ordering
of the representation. One solution consists of using equivariant generative
functions, which ensure the ordering invariance. After having discussed some
properties of such functions, we propose 3G-GAN, a 3-stages model relying on
GANs and equivariant functions. The model is still under development. However,
we present some encouraging exploratory experiments and discuss the issues
still to be addressed.

    

### [[2112.03626] Bless and curse of smoothness and phase transitions in nonparametric regressions: a nonasymptotic perspective](http://arxiv.org/abs/2112.03626)


  When the regression function belongs to the standard smooth classes
consisting of univariate functions with derivatives up to the $(\gamma+1)$th
order bounded by a common constant everywhere or a.e., it is well known that
the minimax optimal rate of convergence in mean squared error (MSE) is
$\left(\frac{\sigma^{2}}{n}\right)^{\frac{2\gamma+2}{2\gamma+3}}$ when $\gamma$
is finite and the sample size $n\rightarrow\infty$. From a nonasymptotic
viewpoint that considers finite $n$, this paper shows that: for the standard
Hölder and Sobolev classes, the minimax optimal rate is
$\frac{\sigma^{2}\left(\gamma\vee1\right)}{n}$ when
$\frac{n}{\sigma^{2}}\precsim\left(\gamma\vee1\right)^{2\gamma+3}$ and
$\left(\frac{\sigma^{2}}{n}\right)^{\frac{2\gamma+2}{2\gamma+3}}$ when
$\frac{n}{\sigma^{2}}\succsim\left(\gamma\vee1\right)^{2\gamma+3}$. To
establish these results, we derive upper and lower bounds on the covering and
packing numbers for the generalized Hölder class where the $k$th
($k=0,...,\gamma$) derivative is bounded from above by a parameter $R_{k}$ and
the $\gamma$th derivative is $R_{\gamma+1}-$Lipschitz (and also for the
generalized ellipsoid class of smooth functions). Our bounds sharpen the
classical metric entropy results for the standard classes, and give the general
dependence on $\gamma$ and $R_{k}$. By deriving the minimax optimal MSE rates
under $R_{k}=1$, $R_{k}\leq\left(k-1\right)!$ and $R_{k}=k!$ (with the latter
two cases motivated in our introduction) with the help of our new entropy
bounds, we show a couple of interesting results that cannot be shown with the
existing entropy bounds in the literature. For the Hölder class of
$d-$variate functions, our result suggests that the classical asymptotic rate
$\left(\frac{\sigma^{2}}{n}\right)^{\frac{2\gamma+2}{2\gamma+2+d}}$ could be an
underestimate of the MSE in finite samples.

    

### [[2112.03632] Generation of Non-Deterministic Synthetic Face Datasets Guided by Identity Priors](http://arxiv.org/abs/2112.03632)


  Enabling highly secure applications (such as border crossing) with face
recognition requires extensive biometric performance tests through large scale
data. However, using real face images raises concerns about privacy as the laws
do not allow the images to be used for other purposes than originally intended.
Using representative and subsets of face data can also lead to unwanted
demographic biases and cause an imbalance in datasets. One possible solution to
overcome these issues is to replace real face images with synthetically
generated samples. While generating synthetic images has benefited from recent
advancements in computer vision, generating multiple samples of the same
synthetic identity resembling real-world variations is still unaddressed, i.e.,
mated samples. This work proposes a non-deterministic method for generating
mated face images by exploiting the well-structured latent space of StyleGAN.
Mated samples are generated by manipulating latent vectors, and more precisely,
we exploit Principal Component Analysis (PCA) to define semantically meaningful
directions in the latent space and control the similarity between the original
and the mated samples using a pre-trained face recognition system. We create a
new dataset of synthetic face images (SymFace) consisting of 77,034 samples
including 25,919 synthetic IDs. Through our analysis using well-established
face image quality metrics, we demonstrate the differences in the biometric
quality of synthetic samples mimicking characteristics of real biometric data.
The analysis and results thereof indicate the use of synthetic samples created
using the proposed approach as a viable alternative to replacing real biometric
data.

    

### [[2112.03636] Godot Reinforcement Learning Agents](http://arxiv.org/abs/2112.03636)


  We present Godot Reinforcement Learning (RL) Agents, an open-source interface
for developing environments and agents in the Godot Game Engine. The Godot RL
Agents interface allows the design, creation and learning of agent behaviors in
challenging 2D and 3D environments with various on-policy and off-policy Deep
RL algorithms. We provide a standard Gym interface, with wrappers for learning
in the Ray RLlib and Stable Baselines RL frameworks. This allows users access
to over 20 state of the art on-policy, off-policy and multi-agent RL
algorithms. The framework is a versatile tool that allows researchers and game
designers the ability to create environments with discrete, continuous and
mixed action spaces. The interface is relatively performant, with 12k
interactions per second on a high end laptop computer, when parallized on 4 CPU
cores. An overview video is available here: this https URL


### [[2112.03638] Scaling Structured Inference with Randomization](http://arxiv.org/abs/2112.03638)


  The scale of the state space of discrete graphical models is crucial for
model capacity in the era of deep learning. Existing dynamic programming (DP)
based inference typically works with a small number of states (usually less
than hundreds). In this work, we propose a family of randomized dynamic
programming (RDP) algorithms for scaling structured models to tens of thousands
of latent states. Our method is widely applicable to classical DP-based
inference (partition, marginal, reparameterization, entropy, .etc) and
different graph structures (chains, trees, and more general hypergraphs). It is
also compatible with automatic differentiation so can be integrated with neural
networks seamlessly and learned with gradient-based optimizers. Our core
technique is randomization, which is to restrict and reweight DP on a small
selected subset of nodes, leading to computation reduction by orders of
magnitudes. We further achieve low bias and variance with Rao-Blackwellization
and importance sampling. Experiments on different inferences over different
graphs demonstrate the accuracy and efficiency of our methods. Furthermore,
when using RDP to train a scaled structured VAE, it outperforms baselines in
terms of test likelihood and successfully prevents posterior collapse.

    

### [[2112.03644] CCasGNN: Collaborative Cascade Prediction Based on Graph Neural Networks](http://arxiv.org/abs/2112.03644)


  Cascade prediction aims at modeling information diffusion in the network.
Most previous methods concentrate on mining either structural or sequential
features from the network and the propagation path. Recent efforts devoted to
combining network structure and sequence features by graph neural networks and
recurrent neural networks. Nevertheless, the limitation of spectral or spatial
methods restricts the improvement of prediction performance. Moreover,
recurrent neural networks are time-consuming and computation-expensive, which
causes the inefficiency of prediction. Here, we propose a novel method CCasGNN
considering the individual profile, structural features, and sequence
information. The method benefits from using a collaborative framework of GAT
and GCN and stacking positional encoding into the layers of graph neural
networks, which is different from all existing ones and demonstrates good
performance. The experiments conducted on two real-world datasets confirm that
our method significantly improves the prediction accuracy compared to
state-of-the-art approaches. What's more, the ablation study investigates the
contribution of each component in our method.

    

### [[2112.03657] Understanding Square Loss in Training Overparametrized Neural Network Classifiers](http://arxiv.org/abs/2112.03657)


  Deep learning has achieved many breakthroughs in modern classification tasks.
Numerous architectures have been proposed for different data structures but
when it comes to the loss function, the cross-entropy loss is the predominant
choice. Recently, several alternative losses have seen revived interests for
deep classifiers. In particular, empirical evidence seems to promote square
loss but a theoretical justification is still lacking. In this work, we
contribute to the theoretical understanding of square loss in classification by
systematically investigating how it performs for overparametrized neural
networks in the neural tangent kernel (NTK) regime. Interesting properties
regarding the generalization error, robustness, and calibration error are
revealed. We consider two cases, according to whether classes are separable or
not. In the general non-separable case, fast convergence rate is established
for both misclassification rate and calibration error. When classes are
separable, the misclassification rate improves to be exponentially fast.
Further, the resulting margin is proven to be lower bounded away from zero,
providing theoretical guarantees for robustness. We expect our findings to hold
beyond the NTK regime and translate to practical settings. To this end, we
conduct extensive empirical studies on practical neural networks, demonstrating
the effectiveness of square loss in both synthetic low-dimensional data and
real image data. Comparing to cross-entropy, square loss has comparable
generalization error but noticeable advantages in robustness and model
calibration.

    

### [[2112.03660] A generalization gap estimation for overparameterized models via Langevin functional variance](http://arxiv.org/abs/2112.03660)


  This paper discusses estimating the generalization gap, a difference between
a generalization gap and an empirical error, for overparameterized models
(e.g., neural networks). We first show that a functional variance, a key
concept in defining a widely-applicable information criterion, characterizes
the generalization gap even in overparameterized settings, where a conventional
theory cannot be applied. We next propose a computationally efficient
approximation of the function variance, a Langevin approximation of the
functional variance~(Langevin FV). This method leverages the 1st-order but not
the 2nd-order gradient of the squared loss function; so, it can be computed
efficiently and implemented consistently with gradient-based optimization
algorithms. We demonstrate the Langevin FV numerically in estimating
generalization gaps of overparameterized linear regression and non-linear
neural network models.

    

### [[2112.03667] Cross-domain User Preference Learning for Cold-start Recommendation](http://arxiv.org/abs/2112.03667)


  Cross-domain cold-start recommendation is an increasingly emerging issue for
recommender systems. Existing works mainly focus on solving either cross-domain
user recommendation or cold-start content recommendation. However, when a new
domain evolves at its early stage, it has potential users similar to the source
domain but with much fewer interactions. It is critical to learn a user's
preference from the source domain and transfer it into the target domain,
especially on the newly arriving contents with limited user feedback. To bridge
this gap, we propose a self-trained Cross-dOmain User Preference LEarning
(COUPLE) framework, targeting cold-start recommendation with various semantic
tags, such as attributes of items or genres of videos. More specifically, we
consider three levels of preferences, including user history, user content and
user group to provide reliable recommendation. With user history represented by
a domain-aware sequential model, a frequency encoder is applied to the
underlying tags for user content preference learning. Then, a hierarchical
memory tree with orthogonal node representation is proposed to further
generalize user group preference across domains. The whole framework updates in
a contrastive way with a First-In-First-Out (FIFO) queue to obtain more
distinctive representations. Extensive experiments on two datasets demonstrate
the efficiency of COUPLE in both user and content cold-start situations. By
deploying an online A/B test for a week, we show that the Click-Through-Rate
(CTR) of COUPLE is superior to other baselines used on Taobao APP. Now the
method is serving online for the cross-domain cold micro-video recommendation.

    

### [[2112.03676] Domain Generalization via Progressive Layer-wise and Channel-wise Dropout](http://arxiv.org/abs/2112.03676)


  By training a model on multiple observed source domains, domain
generalization aims to generalize well to arbitrary unseen target domains
without further training. Existing works mainly focus on learning
domain-invariant features to improve the generalization ability. However, since
target domain is not available during training, previous methods inevitably
suffer from overfitting in source domains. To tackle this issue, we develop an
effective dropout-based framework to enlarge the region of the model's
attention, which can effectively mitigate the overfitting problem.
Particularly, different from the typical dropout scheme, which normally
conducts the dropout on the fixed layer, first, we randomly select one layer,
and then we randomly select its channels to conduct dropout. Besides, we
leverage the progressive scheme to add the ratio of the dropout during
training, which can gradually boost the difficulty of training model to enhance
the robustness of the model. Moreover, to further alleviate the impact of the
overfitting issue, we leverage the augmentation schemes on image-level and
feature-level to yield a strong baseline model. We conduct extensive
experiments on multiple benchmark datasets, which show our method can
outperform the state-of-the-art methods.

    

### [[2112.03678] Does Proprietary Software Still Offer Protection of Intellectual Property in the Age of Machine Learning? -- A Case Study using Dual Energy CT Data](http://arxiv.org/abs/2112.03678)


  In the domain of medical image processing, medical device manufacturers
protect their intellectual property in many cases by shipping only compiled
software, i.e. binary code which can be executed but is difficult to be
understood by a potential attacker. In this paper, we investigate how well this
procedure is able to protect image processing algorithms. In particular, we
investigate whether the computation of mono-energetic images and iodine maps
from dual energy CT data can be reverse-engineered by machine learning methods.
Our results indicate that both can be approximated using only one single slice
image as training data at a very high accuracy with structural similarity
greater than 0.98 in all investigated cases.

    

### [[2112.03690] Low-rank Tensor Decomposition for Compression of Convolutional Neural Networks Using Funnel Regularization](http://arxiv.org/abs/2112.03690)


  Tensor decomposition is one of the fundamental technique for model
compression of deep convolution neural networks owing to its ability to reveal
the latent relations among complex structures. However, most existing methods
compress the networks layer by layer, which cannot provide a satisfactory
solution to achieve global optimization. In this paper, we proposed a model
reduction method to compress the pre-trained networks using low-rank tensor
decomposition of the convolution layers. Our method is based on the
optimization techniques to select the proper ranks of decomposed network
layers. A new regularization method, called funnel function, is proposed to
suppress the unimportant factors during the compression, so the proper ranks
can be revealed much easier. The experimental results show that our algorithm
can reduce more model parameters than other tensor compression methods. For
ResNet18 with ImageNet2012, our reduced model can reach more than twi times
speed up in terms of GMAC with merely 0.7% Top-1 accuracy drop, which
outperforms most existing methods in both metrics.

    

### [[2112.03694] Hard Sample Aware Noise Robust Learning for Histopathology Image Classification](http://arxiv.org/abs/2112.03694)


  Deep learning-based histopathology image classification is a key technique to
help physicians in improving the accuracy and promptness of cancer diagnosis.
However, the noisy labels are often inevitable in the complex manual annotation
process, and thus mislead the training of the classification model. In this
work, we introduce a novel hard sample aware noise robust learning method for
histopathology image classification. To distinguish the informative hard
samples from the harmful noisy ones, we build an easy/hard/noisy (EHN)
detection model by using the sample training history. Then we integrate the EHN
into a self-training architecture to lower the noise rate through gradually
label correction. With the obtained almost clean dataset, we further propose a
noise suppressing and hard enhancing (NSHE) scheme to train the noise robust
model. Compared with the previous works, our method can save more clean samples
and can be directly applied to the real-world noisy dataset scenario without
using a clean subset. Experimental results demonstrate that the proposed scheme
outperforms the current state-of-the-art methods in both the synthetic and
real-world noisy datasets. The source code and data are available at
this https URL.

    

### [[2112.03695] Safe Distillation Box](http://arxiv.org/abs/2112.03695)


  Knowledge distillation (KD) has recently emerged as a powerful strategy to
transfer knowledge from a pre-trained teacher model to a lightweight student,
and has demonstrated its unprecedented success over a wide spectrum of
applications. In spite of the encouraging results, the KD process per se poses
a potential threat to network ownership protection, since the knowledge
contained in network can be effortlessly distilled and hence exposed to a
malicious user. In this paper, we propose a novel framework, termed as Safe
Distillation Box (SDB), that allows us to wrap a pre-trained model in a virtual
box for intellectual property protection. Specifically, SDB preserves the
inference capability of the wrapped model to all users, but precludes KD from
unauthorized users. For authorized users, on the other hand, SDB carries out a
knowledge augmentation scheme to strengthen the KD performances and the results
of the student model. In other words, all users may employ a model in SDB for
inference, but only authorized users get access to KD from the model. The
proposed SDB imposes no constraints over the model architecture, and may
readily serve as a plug-and-play solution to protect the ownership of a
pre-trained network. Experiments across various datasets and architectures
demonstrate that, with SDB, the performance of an unauthorized KD drops
significantly while that of an authorized gets enhanced, demonstrating the
effectiveness of SDB.

    

### [[2112.03696] Noise Distribution Adaptive Self-Supervised Image Denoising using Tweedie Distribution and Score Matching](http://arxiv.org/abs/2112.03696)


  Tweedie distributions are a special case of exponential dispersion models,
which are often used in classical statistics as distributions for generalized
linear models. Here, we reveal that Tweedie distributions also play key roles
in modern deep learning era, leading to a distribution independent
self-supervised image denoising formula without clean reference images.
Specifically, by combining with the recent Noise2Score self-supervised image
denoising approach and the saddle point approximation of Tweedie distribution,
we can provide a general closed-form denoising formula that can be used for
large classes of noise distributions without ever knowing the underlying noise
distribution. Similar to the original Noise2Score, the new approach is composed
of two successive steps: score matching using perturbed noisy images, followed
by a closed form image denoising formula via distribution-independent Tweedie's
formula. This also suggests a systematic algorithm to estimate the noise model
and noise parameters for a given noisy image data set. Through extensive
experiments, we demonstrate that the proposed method can accurately estimate
noise models and parameters, and provide the state-of-the-art self-supervised
image denoising performance in the benchmark dataset and real-world dataset.

    

### [[2112.03698] Modeling and Predicting Blood Flow Characteristics through Double Stenosed Artery from CFD simulation using Deep Learning Models](http://arxiv.org/abs/2112.03698)


  Establishing patient-specific finite element analysis (FEA) models for
computational fluid dynamics (CFD) of double stenosed artery models involves
time and effort, restricting physicians' ability to respond quickly in
time-critical medical applications. Such issues might be addressed by training
deep learning (DL) models to learn and predict blood flow characteristics using
a dataset generated by CFD simulations of simplified double stenosed artery
models with different configurations. When blood flow patterns are compared
through an actual double stenosed artery model, derived from IVUS imaging, it
is revealed that the sinusoidal approximation of stenosed neck geometry, which
has been widely used in previous research works, fails to effectively represent
the effects of a real constriction. As a result, a novel geometric
representation of the constricted neck is proposed which, in terms of a
generalized simplified model, outperforms the former assumption. The sequential
change in artery lumen diameter and flow parameters along the length of the
vessel presented opportunities for the use of LSTM and GRU DL models. However,
with the small dataset of short lengths of doubly constricted blood arteries,
the basic neural network model outperforms the specialized RNNs for most flow
properties. LSTM, on the other hand, performs better for predicting flow
properties with large fluctuations, such as varying blood pressure over the
length of the vessels. Despite having good overall accuracies in training and
testing across all the properties for the vessels in the dataset, the GRU model
underperforms for an individual vessel flow prediction in all cases. The
results also point to the need of individually optimized hyperparameters for
each property in any model rather than aiming to achieve overall good
performance across all outputs with a single set of hyperparameters.

    

### [[2112.03703] Construction de variables à l'aide de classifieurs comme aide à la régression](http://arxiv.org/abs/2112.03703)


  This paper proposes a method for the automatic creation of variables (in the
case of regression) that complement the information contained in the initial
input vector. The method works as a pre-processing step in which the continuous
values of the variable to be regressed are discretized into a set of intervals
which are then used to define value thresholds. Then classifiers are trained to
predict whether the value to be regressed is less than or equal to each of
these thresholds. The different outputs of the classifiers are then
concatenated in the form of an additional vector of variables that enriches the
initial vector of the regression problem. The implemented system can thus be
considered as a generic pre-processing tool. We tested the proposed enrichment
method with 5 types of regressors and evaluated it in 33 regression datasets.
Our experimental results confirm the interest of the approach.

    

### [[2112.03704] Two-stage Deep Stacked Autoencoder with Shallow Learning for Network Intrusion Detection System](http://arxiv.org/abs/2112.03704)


  Sparse events, such as malign attacks in real-time network traffic, have
caused big organisations an immense hike in revenue loss. This is due to the
excessive growth of the network and its exposure to a plethora of people. The
standard methods used to detect intrusions are not promising and have
significant failure to identify new malware. Moreover, the challenges in
handling high volume data with sparsity, high false positives, fewer detection
rates in minor class, training time and feature engineering of the
dimensionality of data has promoted deep learning to take over the task with
less time and great results. The existing system needs improvement in solving
real-time network traffic issues along with feature engineering. Our proposed
work overcomes these challenges by giving promising results using deep-stacked
autoencoders in two stages. The two-stage deep learning combines with shallow
learning using the random forest for classification in the second stage. This
made the model get well with the latest Canadian Institute for Cybersecurity -
Intrusion Detection System 2017 (CICIDS-2017) dataset. Zero false positives
with admirable detection accuracy were achieved.

    

### [[2112.03705] Correlation Based Feature Subset Selection for Multivariate Time-Series Data](http://arxiv.org/abs/2112.03705)


  Correlations in streams of multivariate time series data means that
typically, only a small subset of the features are required for a given data
mining task. In this paper, we propose a technique which we call Merit Score
for Time-Series data (MSTS) that does feature subset selection based on the
correlation patterns of single feature classifier outputs. We assign a Merit
Score to the feature subsets which is used as the basis for selecting 'good'
feature subsets. The proposed technique is evaluated on datasets from the UEA
multivariate time series archive and is compared against a Wrapper approach for
feature subset selection. MSTS is shown to be effective for feature subset
selection and is in particular effective as a data reduction technique. MSTS is
shown here to be computationally more efficient than the Wrapper strategy in
selecting a suitable feature subset, being more than 100 times faster for some
larger datasets while also maintaining a good classification accuracy.

    

### [[2112.03710] CapsProm: A Capsule Network For Promoter Prediction](http://arxiv.org/abs/2112.03710)


  Locating the promoter region in DNA sequences is of paramount importance in
the field of bioinformatics. This is a problem widely studied in the
literature, however, not yet fully resolved. Some researchers have presented
remarkable results using convolution networks, that allowed the automatic
extraction of features from a DNA chain. However, a universal architecture that
could generalize to several organisms has not yet been achieved, and thus,
requiring researchers to seek new architectures and hyperparameters for each
new organism evaluated. In this work, we propose a versatile architecture,
based on capsule network, that can accurately identify promoter sequences in
raw DNA data from seven different organisms, eukaryotic, and prokaryotic. Our
model, the CapsProm, could assist in the transfer of learning between organisms
and expand its applicability. Furthermore the CapsProm showed competitive
results, overcoming the baseline method in five out of seven of the tested
datasets (F1-score). The models and source code are made available at
this https URL.

    

### [[2112.03728] Flexible Networks for Learning Physical Dynamics of Deformable Objects](http://arxiv.org/abs/2112.03728)


  Learning the physical dynamics of deformable objects with particle-based
representation has been the objective of many computational models in machine
learning. While several state-of-the-art models have achieved this objective in
simulated environments, most existing models impose a precondition, such that
the input is a sequence of ordered point sets - i.e., the order of the points
in each point set must be the same across the entire input sequence. This
restrains the model to generalize to real-world data, which is considered to be
a sequence of unordered point sets. In this paper, we propose a model named
time-wise PointNet (TP-Net) that solves this problem by directly consuming a
sequence of unordered point sets to infer the future state of a deformable
object with particle-based representation. Our model consists of a shared
feature extractor that extracts global features from each input point set in
parallel and a prediction network that aggregates and reasons on these features
for future prediction. The key concept of our approach is that we use global
features rather than local features to achieve invariance to input permutations
and ensure the stability and scalability of our model. Experiments demonstrate
that our model achieves state-of-the-art performance in both synthetic dataset
and in real-world dataset, with real-time prediction speed. We provide
quantitative and qualitative analysis on why our approach is more effective and
efficient than existing approaches.

    

### [[2112.03732] A coarse space acceleration of deep-DDM](http://arxiv.org/abs/2112.03732)


  The use of deep learning methods for solving PDEs is a field in full
expansion. In particular, Physical Informed Neural Networks, that implement a
sampling of the physical domain and use a loss function that penalizes the
violation of the partial differential equation, have shown their great
potential. Yet, to address large scale problems encountered in real
applications and compete with existing numerical methods for PDEs, it is
important to design parallel algorithms with good scalability properties. In
the vein of traditional domain decomposition methods (DDM), we consider the
recently proposed deep-ddm approach. We present an extension of this method
that relies on the use of a coarse space correction, similarly to what is done
in traditional DDM solvers. Our investigations shows that the coarse correction
is able to alleviate the deterioration of the convergence of the solver when
the number of subdomains is increased thanks to an instantaneous information
exchange between subdomains at each iteration. Experimental results demonstrate
that our approach induces a remarkable acceleration of the original deep-ddm
method, at a reduced additional computational cost.

    

### [[2112.03734] Towards Modeling and Resolving Singular Parameter Spaces using Stratifolds](http://arxiv.org/abs/2112.03734)


  When analyzing parametric statistical models, a useful approach consists in
modeling geometrically the parameter space. However, even for very simple and
commonly used hierarchical models like statistical mixtures or stochastic deep
neural networks, the smoothness assumption of manifolds is violated at singular
points which exhibit non-smooth neighborhoods in the parameter space. These
singular models have been analyzed in the context of learning dynamics, where
singularities can act as attractors on the learning trajectory and, therefore,
negatively influence the convergence speed of models. We propose a general
approach to circumvent the problem arising from singularities by using
stratifolds, a concept from algebraic topology, to formally model singular
parameter spaces. We use the property that specific stratifolds are equipped
with a resolution method to construct a smooth manifold approximation of the
singular space. We empirically show that using (natural) gradient descent on
the smooth manifold approximation instead of the singular space allows us to
avoid the attractor behavior and therefore improve the convergence speed in
learning.

    

### [[2112.03736] Gaussian map predictions for 3D surface feature localisation and counting](http://arxiv.org/abs/2112.03736)


  In this paper, we propose to employ a Gaussian map representation to estimate
precise location and count of 3D surface features, addressing the limitations
of state-of-the-art methods based on density estimation which struggle in
presence of local disturbances. Gaussian maps indicate probable object location
and can be generated directly from keypoint annotations avoiding laborious and
costly per-pixel annotations. We apply this method to the 3D spheroidal class
of objects which can be projected into 2D shape representation enabling
efficient processing by a neural network GNet, an improved UNet architecture,
which generates the likely locations of surface features and their precise
count. We demonstrate a practical use of this technique for counting strawberry
achenes which is used as a fruit quality measure in phenotyping applications.
The results of training the proposed system on several hundreds of 3D scans of
strawberries from a publicly available dataset demonstrate the accuracy and
precision of the system which outperforms the state-of-the-art density-based
methods for this application.

    

### [[2112.03750] Wild ToFu: Improving Range and Quality of Indirect Time-of-Flight Depth with RGB Fusion in Challenging Environments](http://arxiv.org/abs/2112.03750)


  Indirect Time-of-Flight (I-ToF) imaging is a widespread way of depth
estimation for mobile devices due to its small size and affordable price.
Previous works have mainly focused on quality improvement for I-ToF imaging
especially curing the effect of Multi Path Interference (MPI). These
investigations are typically done in specifically constrained scenarios at
close distance, indoors and under little ambient light. Surprisingly little
work has investigated I-ToF quality improvement in real-life scenarios where
strong ambient light and far distances pose difficulties due to an extreme
amount of induced shot noise and signal sparsity, caused by the attenuation
with limited sensor power and light scattering. In this work, we propose a new
learning based end-to-end depth prediction network which takes noisy raw I-ToF
signals as well as an RGB image and fuses their latent representation based on
a multi step approach involving both implicit and explicit alignment to predict
a high quality long range depth map aligned to the RGB viewpoint. We test our
approach on challenging real-world scenes and show more than 40% RMSE
improvement on the final depth map compared to the baseline approach.

    

### [[2112.03753] Tell me why! -- Explanations support learning of relational and causal structure](http://arxiv.org/abs/2112.03753)


  Explanations play a considerable role in human learning, especially in areas
thatremain major challenges for AI -- forming abstractions, and learning about
the re-lational and causal structure of the world. Here, we explore whether
reinforcement learning agents might likewise benefit from explanations. We
outline a family of relational tasks that involve selecting an object that is
the odd one out in a set (i.e., unique along one of many possible feature
dimensions). Odd-one-out tasks require agents to reason over multi-dimensional
relationships among a set of objects. We show that agents do not learn these
tasks well from reward alone, but achieve >90% performance when they are also
trained to generate language explaining object properties or why a choice is
correct or incorrect. In further experiments, we show how predicting
explanations enables agents to generalize appropriately from ambiguous,
causally-confounded training, and even to meta-learn to perform experimental
interventions to identify causal structure. We show that explanations help
overcome the tendency of agents to fixate on simple features, and explore which
aspects of explanations make them most beneficial. Our results suggest that
learning from explanations is a powerful principle that could offer a promising
path towards training more robust and general machine learning systems.

    

### [[2112.03754] A Continuous-time Stochastic Gradient Descent Method for Continuous Data](http://arxiv.org/abs/2112.03754)


  Optimization problems with continuous data appear in, e.g., robust machine
learning, functional data analysis, and variational inference. Here, the target
function is given as an integral over a family of (continuously) indexed target
functions - integrated with respect to a probability measure. Such problems can
often be solved by stochastic optimization methods: performing optimization
steps with respect to the indexed target function with randomly switched
indices. In this work, we study a continuous-time variant of the stochastic
gradient descent algorithm for optimization problems with continuous data. This
so-called stochastic gradient process consists in a gradient flow minimizing an
indexed target function that is coupled with a continuous-time index process
determining the index. Index processes are, e.g., reflected diffusions, pure
jump processes, or other Lévy processes on compact spaces. Thus, we study
multiple sampling patterns for the continuous data space and allow for data
simulated or streamed at runtime of the algorithm. We analyze the approximation
properties of the stochastic gradient process and study its longtime behavior
and ergodicity under constant and decreasing learning rates. We end with
illustrating the applicability of the stochastic gradient process in a
polynomial regression problem with noisy functional data, as well as in a
physics-informed neural network.

    

### [[2112.03763] Creating Multimodal Interactive Agents with Imitation and Self-Supervised Learning](http://arxiv.org/abs/2112.03763)


  A common vision from science fiction is that robots will one day inhabit our
physical spaces, sense the world as we do, assist our physical labours, and
communicate with us through natural language. Here we study how to design
artificial agents that can interact naturally with humans using the
simplification of a virtual environment. We show that imitation learning of
human-human interactions in a simulated world, in conjunction with
self-supervised learning, is sufficient to produce a multimodal interactive
agent, which we call MIA, that successfully interacts with non-adversarial
humans 75% of the time. We further identify architectural and algorithmic
techniques that improve performance, such as hierarchical action selection.
Altogether, our results demonstrate that imitation of multi-modal, real-time
human behaviour may provide a straightforward and surprisingly effective means
of imbuing agents with a rich behavioural prior from which agents might then be
fine-tuned for specific purposes, thus laying a foundation for training capable
agents for interactive robots or digital assistants. A video of MIA's behaviour
may be found at this https URL


### [[2112.03765] In-flight Novelty Detection with Convolutional Neural Networks](http://arxiv.org/abs/2112.03765)


  Gas turbine engines are complex machines that typically generate a vast
amount of data, and require careful monitoring to allow for cost-effective
preventative maintenance. In aerospace applications, returning all measured
data to ground is prohibitively expensive, often causing useful, high value,
data to be discarded. The ability to detect, prioritise, and return useful data
in real-time is therefore vital. This paper proposes that system output
measurements, described by a convolutional neural network model of normality,
are prioritised in real-time for the attention of preventative maintenance
decision makers.
Due to the complexity of gas turbine engine time-varying behaviours, deriving
accurate physical models is difficult, and often leads to models with low
prediction accuracy and incompatibility with real-time execution. Data-driven
modelling is a desirable alternative producing high accuracy, asset specific
models without the need for derivation from first principles.
We present a data-driven system for online detection and prioritisation of
anomalous data. Biased data assessment deriving from novel operating conditions
is avoided by uncertainty management integrated into the deep neural predictive
model. Testing is performed on real and synthetic data, showing sensitivity to
both real and synthetic faults. The system is capable of running in real-time
on low-power embedded hardware and is currently in deployment on the
Rolls-Royce Pearl 15 engine flight trials.

    

### [[2112.03773] On the Effectiveness of Mode Exploration in Bayesian Model Averaging for Neural Networks](http://arxiv.org/abs/2112.03773)


  Multiple techniques for producing calibrated predictive probabilities using
deep neural networks in supervised learning settings have emerged that leverage
approaches to ensemble diverse solutions discovered during cyclic training or
training from multiple random starting points (deep ensembles). However, only a
limited amount of work has investigated the utility of exploring the local
region around each diverse solution (posterior mode). Using three well-known
deep architectures on the CIFAR-10 dataset, we evaluate several simple methods
for exploring local regions of the weight space with respect to Brier score,
accuracy, and expected calibration error. We consider both Bayesian inference
techniques (variational inference and Hamiltonian Monte Carlo applied to the
softmax output layer) as well as utilizing the stochastic gradient descent
trajectory near optima. While adding separate modes to the ensemble uniformly
improves performance, we show that the simple mode exploration methods
considered here produce little to no improvement over ensembles without mode
exploration.

    

### [[2112.03777] Variance-Aware Weight Initialization for Point Convolutional Neural Networks](http://arxiv.org/abs/2112.03777)


  Appropriate weight initialization has been of key importance to successfully
train neural networks. Recently, batch normalization has diminished the role of
weight initialization by simply normalizing each layer based on batch
statistics. Unfortunately, batch normalization has several drawbacks when
applied to small batch sizes, as they are required to cope with memory
limitations when learning on point clouds. While well-founded weight
initialization strategies can render batch normalization unnecessary and thus
avoid these drawbacks, no such approaches have been proposed for point
convolutional networks. To fill this gap, we propose a framework to unify the
multitude of continuous convolutions. This enables our main contribution,
variance-aware weight initialization. We show that this initialization can
avoid batch normalization while achieving similar and, in some cases, better
performance.

    

### [[2112.03798] PTR-PPO: Proximal Policy Optimization with Prioritized Trajectory Replay](http://arxiv.org/abs/2112.03798)


  On-policy deep reinforcement learning algorithms have low data utilization
and require significant experience for policy improvement. This paper proposes
a proximal policy optimization algorithm with prioritized trajectory replay
(PTR-PPO) that combines on-policy and off-policy methods to improve sampling
efficiency by prioritizing the replay of trajectories generated by old
policies. We first design three trajectory priorities based on the
characteristics of trajectories: the first two being max and mean trajectory
priorities based on one-step empirical generalized advantage estimation (GAE)
values and the last being reward trajectory priorities based on normalized
undiscounted cumulative reward. Then, we incorporate the prioritized trajectory
replay into the PPO algorithm, propose a truncated importance weight method to
overcome the high variance caused by large importance weights under multistep
experience, and design a policy improvement loss function for PPO under
off-policy conditions. We evaluate the performance of PTR-PPO in a set of Atari
discrete control tasks, achieving state-of-the-art performance. In addition, by
analyzing the heatmap of priority changes at various locations in the priority
memory during training, we find that memory size and rollout length can have a
significant impact on the distribution of trajectory priorities and, hence, on
the performance of the algorithm.

    

### [[2112.03806] OOD-GNN: Out-of-Distribution Generalized Graph Neural Network](http://arxiv.org/abs/2112.03806)


  Graph neural networks (GNNs) have achieved impressive performance when
testing and training graph data come from identical distribution. However,
existing GNNs lack out-of-distribution generalization abilities so that their
performance substantially degrades when there exist distribution shifts between
testing and training graph data. To solve this problem, in this work, we
propose an out-of-distribution generalized graph neural network (OOD-GNN) for
achieving satisfactory performance on unseen testing graphs that have different
distributions with training graphs. Our proposed OOD-GNN employs a novel
nonlinear graph representation decorrelation method utilizing random Fourier
features, which encourages the model to eliminate the statistical dependence
between relevant and irrelevant graph representations through iteratively
optimizing the sample graph weights and graph encoder. We further design a
global weight estimator to learn weights for training graphs such that
variables in graph representations are forced to be independent. The learned
weights help the graph encoder to get rid of spurious correlations and, in
turn, concentrate more on the true connection between learned discriminative
graph representations and their ground-truth labels. We conduct extensive
experiments to validate the out-of-distribution generalization abilities on two
synthetic and 12 real-world datasets with distribution shifts. The results
demonstrate that our proposed OOD-GNN significantly outperforms
state-of-the-art baselines.

    

### [[2112.03807] raceBERT -- A Transformer-based Model for Predicting Race from Names](http://arxiv.org/abs/2112.03807)


  This paper presents raceBERT -- a transformer-based model for predicting race
from character sequences in names, and an accompanying python package. Using a
transformer-based model trained on a U.S. Florida voter registration dataset,
the model predicts the likelihood of a name belonging to 5 U.S. census race
categories (White, Black, Hispanic, Asian & Pacific Islander, American Indian &
Alaskan Native). I build on Sood and Laohaprapanon (2018) by replacing their
LSTM model with transformer-based models (pre-trained BERT model, and a roBERTa
model trained from scratch), and compare the results. To the best of my
knowledge, raceBERT achieves state-of-the-art results in race prediction using
names, with an average f1-score of 0.86 -- a 4.\1% improvement over the
previous state-of-the-art, and improvements between 15-17\% for non-white
names.

    

### [[2112.03811] Disentangled Counterfactual Recurrent Networks for Treatment Effect Inference over Time](http://arxiv.org/abs/2112.03811)


  Choosing the best treatment-plan for each individual patient requires
accurate forecasts of their outcome trajectories as a function of the
treatment, over time. While large observational data sets constitute rich
sources of information to learn from, they also contain biases as treatments
are rarely assigned randomly in practice. To provide accurate and unbiased
forecasts, we introduce the Disentangled Counterfactual Recurrent Network
(DCRN), a novel sequence-to-sequence architecture that estimates treatment
outcomes over time by learning representations of patient histories that are
disentangled into three separate latent factors: a treatment factor,
influencing only treatment selection; an outcome factor, influencing only the
outcome; and a confounding factor, influencing both. With an architecture that
is completely inspired by the causal structure of treatment influence over
time, we advance forecast accuracy and disease understanding, as our
architecture allows for practitioners to infer which patient features influence
which part in a patient's trajectory, contrasting other approaches in this
domain. We demonstrate that DCRN outperforms current state-of-the-art methods
in forecasting treatment responses, on both real and simulated data.

    

### [[2112.03815] Accurate parameter estimation using scan-specific unsupervised deep learning for relaxometry and MR fingerprinting](http://arxiv.org/abs/2112.03815)


  We propose an unsupervised convolutional neural network (CNN) for relaxation
parameter estimation. This network incorporates signal relaxation and Bloch
simulations while taking advantage of residual learning and spatial relations
across neighboring voxels. Quantification accuracy and robustness to noise is
shown to be significantly improved compared to standard parameter estimation
methods in numerical simulations and in vivo data for multi-echo T2 and T2*
mapping. The combination of the proposed network with subspace modeling and MR
fingerprinting (MRF) from highly undersampled data permits high quality T1 and
T2 mapping.

    

### [[2112.03837] Augment & Valuate : A Data Enhancement Pipeline for Data-Centric AI](http://arxiv.org/abs/2112.03837)


  Data scarcity and noise are important issues in industrial applications of
machine learning. However, it is often challenging to devise a scalable and
generalized approach to address the fundamental distributional and semantic
properties of dataset with black box models. For this reason, data-centric
approaches are crucial for the automation of machine learning operation
pipeline. In order to serve as the basis for this automation, we suggest a
domain-agnostic pipeline for refining the quality of data in image
classification problems. This pipeline contains data valuation, cleansing, and
augmentation. With an appropriate combination of these methods, we could
achieve 84.711% test accuracy (ranked #6, Honorable Mention in the Most
Innovative) in the Data-Centric AI competition only with the provided dataset.

    

### [[2112.03857] Grounded Language-Image Pre-training](http://arxiv.org/abs/2112.03857)


  This paper presents a grounded language-image pre-training (GLIP) model for
learning object-level, language-aware, and semantic-rich visual
representations. GLIP unifies object detection and phrase grounding for
pre-training. The unification brings two benefits: 1) it allows GLIP to learn
from both detection and grounding data to improve both tasks and bootstrap a
good grounding model; 2) GLIP can leverage massive image-text pairs by
generating grounding boxes in a self-training fashion, making the learned
representation semantic-rich. In our experiments, we pre-train GLIP on 27M
grounding data, including 3M human-annotated and 24M web-crawled image-text
pairs. The learned representations demonstrate strong zero-shot and few-shot
transferability to various object-level recognition tasks. 1) When directly
evaluated on COCO and LVIS (without seeing any images in COCO during
pre-training), GLIP achieves 49.8 AP and 26.9 AP, respectively, surpassing many
supervised baselines. 2) After fine-tuned on COCO, GLIP achieves 60.8 AP on val
and 61.5 AP on test-dev, surpassing prior SoTA. 3) When transferred to 13
downstream object detection tasks, a 1-shot GLIP rivals with a fully-supervised
Dynamic Head. Code will be released at this https URL.

    

### [[2112.03860] Traversing within the Gaussian Typical Set: Differentiable Gaussianization Layers for Inverse Problems Augmented by Normalizing Flows](http://arxiv.org/abs/2112.03860)


  Generative networks such as normalizing flows can serve as a learning-based
prior to augment inverse problems to achieve high-quality results. However, the
latent space vector may not remain a typical sample from the desired
high-dimensional standard Gaussian distribution when traversing the latent
space during an inversion. As a result, it can be challenging to attain a
high-fidelity solution, particularly in the presence of noise and inaccurate
physics-based models. To address this issue, we propose to re-parameterize and
Gaussianize the latent vector using novel differentiable data-dependent layers
wherein custom operators are defined by solving optimization problems. These
proposed layers enforce an inversion to find a feasible solution within a
Gaussian typical set of the latent space. We tested and validated our technique
on an image deblurring task and eikonal tomography -- a PDE-constrained inverse
problem and achieved high-fidelity results.

    

### [[2112.03865] Universalizing Weak Supervision](http://arxiv.org/abs/2112.03865)


  Weak supervision (WS) frameworks are a popular way to bypass hand-labeling
large datasets for training data-hungry models. These approaches synthesize
multiple noisy but cheaply-acquired estimates of labels into a set of
high-quality pseudolabels for downstream training. However, the synthesis
technique is specific to a particular kind of label, such as binary labels or
sequences, and each new label type requires manually designing a new synthesis
algorithm. Instead, we propose a universal technique that enables weak
supervision over any label type while still offering desirable properties,
including practical flexibility, computational efficiency, and theoretical
guarantees. We apply this technique to important problems previously not
tackled by WS frameworks including learning to rank, regression, and learning
in hyperbolic manifolds. Theoretically, our synthesis approach produces a
consistent estimator for learning a challenging but important generalization of
the exponential family model. Experimentally, we validate our framework and
show improvement over baselines in diverse settings including real-world
learning-to-rank and regression problems along with learning on hyperbolic
manifolds.

    

### [[2112.03867] Towards a Shared Rubric for Dataset Annotation](http://arxiv.org/abs/2112.03867)


  When arranging for third-party data annotation, it can be hard to compare how
well the competing providers apply best practices to create high-quality
datasets. This leads to a "race to the bottom," where competition based solely
on price makes it hard for vendors to charge for high-quality annotation. We
propose a voluntary rubric which can be used (a) as a scorecard to compare
vendors' offerings, (b) to communicate our expectations of the vendors more
clearly and consistently than today, (c) to justify the expense of choosing
someone other than the lowest bidder, and (d) to encourage annotation providers
to improve their practices.

    

### [[2112.03874] Efficient Calibration of Multi-Agent Market Simulators from Time Series with Bayesian Optimization](http://arxiv.org/abs/2112.03874)


  Multi-agent market simulation is commonly used to create an environment for
downstream machine learning or reinforcement learning tasks, such as training
or testing trading strategies before deploying them to real-time trading. In
electronic trading markets only the price or volume time series, that result
from interaction of multiple market participants, are typically directly
observable. Therefore, multi-agent market environments need to be calibrated so
that the time series that result from interaction of simulated agents resemble
historical -- which amounts to solving a highly complex large-scale
optimization problem. In this paper, we propose a simple and efficient
framework for calibrating multi-agent market simulator parameters from
historical time series observations. First, we consider a novel concept of
eligibility set to bypass the potential non-identifiability issue. Second, we
generalize the two-sample Kolmogorov-Smirnov (K-S) test with Bonferroni
correction to test the similarity between two high-dimensional time series
distributions, which gives a simple yet effective distance metric between the
time series sample sets. Third, we suggest using Bayesian optimization (BO) and
trust-region BO (TuRBO) to minimize the aforementioned distance metric.
Finally, we demonstrate the efficiency of our framework using numerical
experiments.

    

### [[2112.03888] Image Enhancement via Bilateral Learning](http://arxiv.org/abs/2112.03888)


  Nowadays, due to advanced digital imaging technologies and internet
accessibility to the public, the number of generated digital images has
increased dramatically. Thus, the need for automatic image enhancement
techniques is quite apparent. In recent years, deep learning has been used
effectively. Here, after introducing some recently developed works on image
enhancement, an image enhancement system based on convolutional neural networks
is presented. Our goal is to make an effective use of two available approaches,
convolutional neural network and bilateral grid. In our approach, we increase
the training data and the model dimensions and propose a variable rate during
the training process. The enhancement results produced by our proposed method,
while incorporating 5 different experts, show both quantitative and qualitative
improvements as compared to other available methods.

    

### [[2112.03898] Lattice-Based Methods Surpass Sum-of-Squares in Clustering](http://arxiv.org/abs/2112.03898)


  Clustering is a fundamental primitive in unsupervised learning which gives
rise to a rich class of computationally-challenging inference tasks. In this
work, we focus on the canonical task of clustering $d$-dimensional Gaussian
mixtures with unknown (and possibly degenerate) covariance. Recent works (Ghosh
et al.\ '20; Mao, Wein '21; Davis, Diaz, Wang '21) have established lower
bounds against the class of low-degree polynomial methods and the
sum-of-squares (SoS) hierarchy for recovering certain hidden structures planted
in Gaussian clustering instances. Prior work on many similar inference tasks
portends that such lower bounds strongly suggest the presence of an inherent
statistical-to-computational gap for clustering, that is, a parameter regime
where the clustering task is \textit{statistically} possible but no
\textit{polynomial-time} algorithm succeeds.
One special case of the clustering task we consider is equivalent to the
problem of finding a planted hypercube vector in an otherwise random subspace.
We show that, perhaps surprisingly, this particular clustering model
\textit{does not exhibit} a statistical-to-computational gap, even though the
aforementioned low-degree and SoS lower bounds continue to apply in this case.
To achieve this, we give a polynomial-time algorithm based on the
Lenstra--Lenstra--Lovasz lattice basis reduction method which achieves the
statistically-optimal sample complexity of $d+1$ samples. This result extends
the class of problems whose conjectured statistical-to-computational gaps can
be "closed" by "brittle" polynomial-time algorithms, highlighting the crucial
but subtle role of noise in the onset of statistical-to-computational gaps.

    

### [[2112.03899] Information is Power: Intrinsic Control via Information Capture](http://arxiv.org/abs/2112.03899)


  Humans and animals explore their environment and acquire useful skills even
in the absence of clear goals, exhibiting intrinsic motivation. The study of
intrinsic motivation in artificial agents is concerned with the following
question: what is a good general-purpose objective for an agent? We study this
question in dynamic partially-observed environments, and argue that a compact
and general learning objective is to minimize the entropy of the agent's state
visitation estimated using a latent state-space model. This objective induces
an agent to both gather information about its environment, corresponding to
reducing uncertainty, and to gain control over its environment, corresponding
to reducing the unpredictability of future world states. We instantiate this
approach as a deep reinforcement learning agent equipped with a deep
variational Bayes filter. We find that our agent learns to discover, represent,
and exercise control of dynamic objects in a variety of partially-observed
environments sensed with visual observations without extrinsic reward.

    

### [[1906.07801] Safe Testing](http://arxiv.org/abs/1906.07801)


  We develop the theory of hypothesis testing based on the E-value, a notion of
evidence that, unlike the p-value, allows for effortlessly combining results
from several studies in the common scenario where the decision to perform a new
study may depend on previous outcomes. Tests based on E-values are safe, i.e.
they preserve Type-I error guarantees, under such optional continuation. We
define growth-rate optimality (GRO) as an analogue of power in an optional
continuation context, and we show how to construct GRO E-variables for general
testing problems with composite null and alternative, emphasizing models with
nuisance parameters. GRO E-values take the form of Bayes factors with special
priors. We illustrate the theory using several classic examples including a
one-sample safe t-test (in which the right Haar prior turns out to be GRO) and
the 2x2 contingency table (in which the GRO prior is different from standard
priors). Sharing Fisherian, Neymanian and Jeffreys-Bayesian interpretations,
E-values and the corresponding tests may provide a methodology acceptable to
adherents of all three schools.

    

### [[2005.14707] Towards Context-Agnostic Learning Using Synthetic Data](http://arxiv.org/abs/2005.14707)


  We propose a novel setting for learning, where the input domain is the image
of a map defined on the product of two sets, one of which completely determines
the labels. We derive a new risk bound for this setting that decomposes into a
bias and an error term, and exhibits a surprisingly weak dependence on the true
labels. Inspired by these results, we present an algorithm aimed at minimizing
the bias term by exploiting the ability to sample from each set independently.
We apply our setting to visual classification tasks, where our approach enables
us to train classifiers on datasets that consist entirely of a single synthetic
example of each class. On several standard benchmarks for real-world image
classification, we achieve robust performance in the context-agnostic setting,
with good generalization to real world domains, whereas training directly on
real world data without our techniques yields classifiers that are brittle to
perturbations of the background.

    

### [[2008.07912] Inductive logic programming at 30: a new introduction](http://arxiv.org/abs/2008.07912)


  Inductive logic programming (ILP) is a form of machine learning. The goal of
ILP is to induce a hypothesis (a set of logical rules) that generalises
training examples. As ILP turns 30, we provide a new introduction to the field.
We introduce the necessary logical notation and the main learning settings;
describe the building blocks of an ILP system; compare several systems on
several dimensions; describe four systems (Aleph, TILDE, ASPAL, and Metagol);
highlight key application areas; and, finally, summarise current limitations
and directions for future research.

    

### [[2008.12595] Dynamical Variational Autoencoders: A Comprehensive Review](http://arxiv.org/abs/2008.12595)


  The Variational Autoencoder (VAE) is a powerful deep generative model that is
now extensively used to represent high-dimensional complex data via a
low-dimensional latent space learned in an unsupervised manner. In the original
VAE model, input data vectors are processed independently. In recent years, a
series of papers have presented different extensions of the VAE to process
sequential data, that not only model the latent space, but also model the
temporal dependencies within a sequence of data vectors and corresponding
latent vectors, relying on recurrent neural networks or state space models. In
this paper we perform an extensive literature review of these models.
Importantly, we introduce and discuss a general class of models called
Dynamical Variational Autoencoders (DVAEs) that encompasses a large subset of
these temporal VAE extensions. Then we present in detail seven different
instances of DVAE that were recently proposed in the literature, with an effort
to homogenize the notations and presentation lines, as well as to relate these
models with existing classical temporal models. We reimplemented those seven
DVAE models and we present the results of an experimental benchmark conducted
on the speech analysis-resynthesis task (the PyTorch code is made publicly
available). The paper is concluded with an extensive discussion on important
issues concerning the DVAE class of models and future research guidelines.

    

### [[2009.04796] XCM: An Explainable Convolutional Neural Network for Multivariate Time Series Classification](http://arxiv.org/abs/2009.04796)


  Multivariate Time Series (MTS) classification has gained importance over the
past decade with the increase in the number of temporal datasets in multiple
domains. The current state-of-the-art MTS classifier is a heavyweight deep
learning approach, which outperforms the second-best MTS classifier only on
large datasets. Moreover, this deep learning approach cannot provide faithful
explanations as it relies on post hoc model-agnostic explainability methods,
which could prevent its use in numerous applications. In this paper, we present
XCM, an eXplainable Convolutional neural network for MTS classification. XCM is
a new compact convolutional neural network which extracts information relative
to the observed variables and time directly from the input data. Thus, XCM
architecture enables a good generalization ability on both large and small
datasets, while allowing the full exploitation of a faithful post hoc
model-specific explainability method (Gradient-weighted Class Activation
Mapping) by precisely identifying the observed variables and timestamps of the
input data that are important for predictions. We first show that XCM
outperforms the state-of-the-art MTS classifiers on both the large and small
public UEA datasets. Then, we illustrate how XCM reconciles performance and
explainability on a synthetic dataset and show that XCM enables a more precise
identification of the regions of the input data that are important for
predictions compared to the current deep learning MTS classifier also providing
faithful explainability. Finally, we present how XCM can outperform the current
most accurate state-of-the-art algorithm on a real-world application while
enhancing explainability by providing faithful and more informative
explanations.

    

### [[2009.08427] Discovering Dynamic Salient Regions for Spatio-Temporal Graph Neural Networks](http://arxiv.org/abs/2009.08427)


  Graph Neural Networks are perfectly suited to capture latent interactions
between various entities in the spatio-temporal domain (e.g. videos). However,
when an explicit structure is not available, it is not obvious what atomic
elements should be represented as nodes. Current works generally use
pre-trained object detectors or fixed, predefined regions to extract graph
nodes. Improving upon this, our proposed model learns nodes that dynamically
attach to well-delimited salient regions, which are relevant for a higher-level
task, without using any object-level supervision. Constructing these localized,
adaptive nodes gives our model inductive bias towards object-centric
representations and we show that it discovers regions that are well correlated
with objects in the video. In extensive ablation studies and experiments on two
challenging datasets, we show superior performance to previous graph neural
networks models for video classification.

    

### [[2010.08222] Towards Tight Communication Lower Bounds for Distributed Optimisation](http://arxiv.org/abs/2010.08222)


  We consider a standard distributed optimisation setting where $N$ machines,
each holding a $d$-dimensional function $f_i$, aim to jointly minimise the sum
of the functions $\sum_{i = 1}^N f_i (x)$. This problem arises naturally in
large-scale distributed optimisation, where a standard solution is to apply
variants of (stochastic) gradient descent. We focus on the communication
complexity of this problem: our main result provides the first fully
unconditional bounds on total number of bits which need to be sent and received
by the $N$ machines to solve this problem under point-to-point communication,
within a given error-tolerance. Specifically, we show that $\Omega( Nd \log d /
N\varepsilon)$ total bits need to be communicated between the machines to find
an additive $\epsilon$-approximation to the minimum of $\sum_{i = 1}^N f_i
(x)$. The result holds for both deterministic and randomised algorithms, and,
importantly, requires no assumptions on the algorithm structure. The lower
bound is tight under certain restrictions on parameter values, and is matched
within constant factors for quadratic objectives by a new variant of quantised
gradient descent, which we describe and analyse. Our results bring over tools
from communication complexity to distributed optimisation, which has potential
for further applications.

    

### [[2011.01805] HeLayers: A Tile Tensors Framework for Large Neural Networks on Encrypted Data](http://arxiv.org/abs/2011.01805)


  Privacy-preserving solutions enable companies to offload confidential data to
third-party services while fulfilling their government regulations. To
accomplish this, they leverage various cryptographic techniques such as
Homomorphic Encryption (HE), which allows performing computation on encrypted
data. Most HE schemes work in a SIMD fashion, and the data packing method can
dramatically affect the running time and memory costs. Finding a packing method
that leads to an optimal performant implementation is a hard task.
We present a simple and intuitive framework that abstracts the packing
decision for the user. We explain its underlying data structures and optimizer,
and propose a novel algorithm for performing 2D convolution operations. We used
this framework to implement an HE-friendly version of AlexNet, which runs in
three minutes, several orders of magnitude faster than other state-of-the-art
solutions that only use HE.

    

### [[2011.12193] xFraud: Explainable Fraud Transaction Detection](http://arxiv.org/abs/2011.12193)


  At online retail platforms, it is crucial to actively detect the risks of
transactions to improve customer experience and minimize financial loss. In
this work, we propose xFraud, an explainable fraud transaction prediction
framework which is mainly composed of a detector and an explainer. The xFraud
detector can effectively and efficiently predict the legitimacy of incoming
transactions. Specifically, it utilizes a heterogeneous graph neural network to
learn expressive representations from the informative heterogeneously typed
entities in the transaction logs. The explainer in xFraud can generate
meaningful and human-understandable explanations from graphs to facilitate
further processes in the business unit. In our experiments with xFraud on real
transaction networks with up to 1.1 billion nodes and 3.7 billion edges, xFraud
is able to outperform various baseline models in many evaluation metrics while
remaining scalable in distributed settings. In addition, we show that xFraud
explainer can generate reasonable explanations to significantly assist the
business analysis via both quantitative and qualitative evaluations.

    

### [[2012.09090] You Are What You Tweet: Profiling Users by Past Tweets to Improve Hate Speech Detection](http://arxiv.org/abs/2012.09090)


  Hate speech detection research has predominantly focused on purely
content-based methods, without exploiting any additional context. We briefly
critique pros and cons of this task formulation. We then investigate profiling
users by their past utterances as an informative prior to better predict
whether new utterances constitute hate speech. To evaluate this, we augment
three Twitter hate speech datasets with additional timeline data, then embed
this additional context into a strong baseline model. Promising results suggest
merit for further investigation, though analysis is complicated by differences
in annotation schemes and processes, as well as Twitter API limitations and
data sharing policies.

    

### [[2101.06482] A Renormalization Group Approach to Connect Discrete- and Continuous-Time Descriptions of Gaussian Processes](http://arxiv.org/abs/2101.06482)


  Discretization of continuous stochastic processes is needed to numerically
simulate them or to infer models from experimental time series. However,
depending on the nature of the process, the same discretization scheme, if not
accurate enough, may perform very differently for the two tasks. Exact
discretizations, which work equally well at any scale, are characterized by the
property of invariance under coarse-graining. Motivated by this observation, we
build an explicit Renormalization Group approach for Gaussian time series
generated by auto-regressive models. We show that the RG fixed points
correspond to discretizations of linear SDEs, and only come in the form of
first order Markov processes or non-Markovian ones. This fact provides an
alternative explanation of why standard delay-vector embedding procedures fail
in reconstructing partially observed noise-driven systems. We also suggest a
possible effective Markovian discretization for the inference of partially
observed underdamped equilibrium processes based on the exploitation of the
Einstein relation.

    

### [[2101.11805] Chronological age estimation of lateral cephalometric radiographs with deep learning](http://arxiv.org/abs/2101.11805)


  The traditional manual age estimation method is crucial labor based on many
kinds of the X-Ray image. Some current studies have shown that lateral
cephalometric(LC) images can be used to estimate age. However, these methods
are based on manually measuring some image features and making age estimates
based on experience or scoring. Therefore, these methods are time-consuming and
labor-intensive, and the effect will be affected by subjective opinions. In
this work, we propose a saliency map-enhanced age estimation method, which can
automatically perform age estimation based on LC images. Meanwhile, it can also
show the importance of each region in the image for age estimation, which
undoubtedly increases the method's Interpretability. Our method was tested on
3014 LC images from 4 to 40 years old. The MEA of the experimental result is
1.250, which is less than the result of the state-of-the-art benchmark because
it performs significantly better in the age group with fewer data. Besides, our
model is trained in each area with a high contribution to age estimation in LC
images, so the effect of these different areas on the age estimation task was
verified. Consequently, we conclude that the proposed saliency map enhancements
chronological age estimation method of lateral cephalometric radiographs can
work well in chronological age estimation task, especially when the amount of
data is small. Besides, compared with traditional deep learning, our method is
also interpretable.

    

### [[2102.13276] Spectral Top-Down Recovery of Latent Tree Models](http://arxiv.org/abs/2102.13276)


  Modeling the distribution of high dimensional data by a latent tree graphical
model is a prevalent approach in multiple scientific domains. A common task is
to infer the underlying tree structure, given only observations of its terminal
nodes. Many algorithms for tree recovery are computationally intensive, which
limits their applicability to trees of moderate size. For large trees, a common
approach, termed divide-and-conquer, is to recover the tree structure in two
steps. First, recover the structure separately of multiple, possibly random
subsets of the terminal nodes. Second, merge the resulting subtrees to form a
full tree. Here, we develop Spectral Top-Down Recovery (STDR), a deterministic
divide-and-conquer approach to infer large latent tree models. Unlike previous
methods, STDR partitions the terminal nodes in a non random way, based on the
Fiedler vector of a suitable Laplacian matrix related to the observed nodes. We
prove that under certain conditions, this partitioning is consistent with the
tree structure. This, in turn, leads to a significantly simpler merging
procedure of the small subtrees. We prove that STDR is statistically consistent
and bound the number of samples required to accurately recover the tree with
high probability. Using simulated data from several common tree models in
phylogenetics, we demonstrate that STDR has a significant advantage in terms of
runtime, with improved or similar accuracy.

    

### [[2103.16355] Nonlinear Weighted Directed Acyclic Graph and A Priori Estimates for Neural Networks](http://arxiv.org/abs/2103.16355)


  In an attempt to better understand structural benefits and generalization
power of deep neural networks, we firstly present a novel graph theoretical
formulation of neural network models, including fully connected, residual
network (ResNet) and densely connected networks (DenseNet). Secondly, we extend
the error analysis of the population risk for two layer network
\cite{ew2019prioriTwo} and ResNet \cite{e2019prioriRes} to DenseNet, and show
further that for neural networks satisfying certain mild conditions, similar
estimates can be obtained. These estimates are a priori in nature since they
depend sorely on the information prior to the training process, in particular,
the bounds for the estimation errors are independent of the input dimension.

    

### [[2104.01714] Urysohn Forest for Aleatoric Uncertainty Quantification](http://arxiv.org/abs/2104.01714)


  This paper focuses on building models of stochastic systems with aleatoric
uncertainty. The main novelty is an algorithm of boosted ensemble training of
multiple models for obtaining a probability distribution of an individual
output as a function of the system input. The second novel contribution is a
new regression model to be used in the ensemble. The model is a multi-layered
tree of hierarchically-connected discrete Urysohn operators (or generalised
additive models, which are mathematically equivalent to the discrete Urysohn
operators in this case). Since multiple models (trees) are trained in the
ensemble, the authors refer them as an Urysohn forest. The source code is
freely available online.

    

### [[2104.09455] Arithmetic-Intensity-Guided Fault Tolerance for Neural Network Inference on GPUs](http://arxiv.org/abs/2104.09455)


  Neural networks (NNs) are increasingly employed in safety-critical domains
and in environments prone to unreliability (e.g., soft errors), such as on
spacecraft. Therefore, it is critical to impart fault tolerance to NN
inference. Algorithm-based fault tolerance (ABFT) is emerging as an efficient
approach for fault tolerance in NNs.
We propose an adaptive approach to ABFT for NN inference that exploits
untapped opportunities in emerging deployment scenarios. GPUs have high
compute-to-memory-bandwidth ratios, while NN layers have a wide range of
arithmetic intensities. This leaves some layers compute bound and others
memory-bandwidth bound, but current approaches to ABFT do not consider these
differences. We first investigate ABFT schemes best suited for each of these
scenarios. We then propose intensity-guided ABFT, an adaptive,
arithmetic-intensity-guided approach that selects the most efficient ABFT
scheme for each NN layer. Intensity-guided ABFT reduces execution-time overhead
by 1.09--5.3$\times$ across many NNs compared to traditional approaches to
ABFT.

    

### [[2105.06903] Posterior Regularization on Bayesian Hierarchical Mixture Clustering](http://arxiv.org/abs/2105.06903)


  Bayesian hierarchical mixture clustering (BHMC) is an interesting model that
improves on the traditional Bayesian hierarchical clustering approaches.
Regarding the parent-to-node diffusion in the generative process, BHMC replaces
the conventional Gaussian-to-Gaussian (G2G) kernels with a Hierarchical
Dirichlet Process Mixture Model (HDPMM). However, the drawback of the BHMC lies
in that it might obtain comparatively high nodal variance in the higher levels
(i.e., those closer to the root node). This can be interpreted as that the
separation between the nodes, in particular those in the higher levels, might
be weak. Attempting to overcome this drawback, we consider a recent inferential
framework named posterior regularization, which facilitates a simple manner to
impose extra constraints on a Bayesian model to address some weakness of the
original model. Hence, to enhance the separation of clusters, we apply
posterior regularization to impose max-margin constraints on the nodes at every
level of the hierarchy. In this paper, we illustrate how the framework
integrates with the BHMC and achieves the desired improvements over the
original model.

    

### [[2105.07562] Power-grid stability predictions using transferable machine learning](http://arxiv.org/abs/2105.07562)


  Complex network analyses have provided clues to improve power-grid stability
with the help of numerical models. The high computational cost of numerical
simulations, however, has inhibited the approach, especially when it deals with
the dynamic properties of power grids such as frequency synchronization. In
this study, we investigate machine learning techniques to estimate the
stability of power-grid synchronization. We test three different machine
learning algorithms -- random forest, support vector machine, and artificial
neural network -- training them with two different types of synthetic power
grids consisting of homogeneous and heterogeneous input-power distribution,
respectively. We find that the three machine learning models better predict the
synchronization stability of power-grid nodes when they are trained with the
heterogeneous input-power distribution than the homogeneous one. With the
real-world power grids of Great Britain, Spain, France, and Germany, we also
demonstrate that the machine learning algorithms trained on synthetic power
grids are transferable to the stability prediction of the real-world power
grids, which implies the prospective applicability of machine learning
techniques on power-grid studies.

    

### [[2106.00886] Partial Wasserstein Covering](http://arxiv.org/abs/2106.00886)


  We consider a general task called partial Wasserstein covering with the goal
of providing information on what patterns are not being taken into account in a
dataset (e.g., dataset used during development) compared with another
dataset(e.g., dataset obtained from actual applications). We model this task as
a discrete optimization problem with partial Wasserstein divergence as an
objective function. Although this problem is NP-hard, we prove that it
satisfies the submodular property, allowing us to use a greedy algorithm with a
0.63 approximation. However, the greedy algorithm is still inefficient because
it requires solving linear programming for each objective function evaluation.
To overcome this inefficiency, we propose quasi-greedy algorithms that consist
of a series of acceleration techniques, such as sensitivity analysis based on
strong duality and the so-called C-transform in the optimal transport field.
Experimentally, we demonstrate that we can efficiently fill in the gaps between
the two datasets and find missing scene in real driving scenes datasets.

    

### [[2106.03907] Deep Proxy Causal Learning and its Application to Confounded Bandit Policy Evaluation](http://arxiv.org/abs/2106.03907)


  Proxy causal learning (PCL) is a method for estimating the causal effect of
treatments on outcomes in the presence of unobserved confounding, using proxies
(structured side information) for the confounder. This is achieved via
two-stage regression: in the first stage, we model relations among the
treatment and proxies; in the second stage, we use this model to learn the
effect of treatment on the outcome, given the context provided by the proxies.
PCL guarantees recovery of the true causal effect, subject to identifiability
conditions. We propose a novel method for PCL, the deep feature proxy variable
method (DFPV), to address the case where the proxies, treatments, and outcomes
are high-dimensional and have nonlinear complex relationships, as represented
by deep neural network features. We show that DFPV outperforms recent
state-of-the-art PCL methods on challenging synthetic benchmarks, including
settings involving high dimensional image data. Furthermore, we show that PCL
can be applied to off-policy evaluation for the confounded bandit problem, in
which DFPV also exhibits competitive performance.

    

### [[2106.04015] Uncertainty Baselines: Benchmarks for Uncertainty & Robustness in Deep Learning](http://arxiv.org/abs/2106.04015)


  High-quality estimates of uncertainty and robustness are crucial for numerous
real-world applications, especially for deep learning which underlies many
deployed ML systems. The ability to compare techniques for improving these
estimates is therefore very important for research and practice alike. Yet,
competitive comparisons of methods are often lacking due to a range of reasons,
including: compute availability for extensive tuning, incorporation of
sufficiently many baselines, and concrete documentation for reproducibility. In
this paper we introduce Uncertainty Baselines: high-quality implementations of
standard and state-of-the-art deep learning methods on a variety of tasks. As
of this writing, the collection spans 19 methods across 9 tasks, each with at
least 5 metrics. Each baseline is a self-contained experiment pipeline with
easily reusable and extendable components. Our goal is to provide immediate
starting points for experimentation with new methods or applications.
Additionally we provide model checkpoints, experiment outputs as Python
notebooks, and leaderboards for comparing results. Code available at
this https URL.

    

### [[2107.00783] Reinforcement Learning for Feedback-Enabled Cyber Resilience](http://arxiv.org/abs/2107.00783)


  Digitization and remote connectivity have enlarged the attack surface and
made cyber systems more vulnerable. As attackers become increasingly
sophisticated and resourceful, mere reliance on traditional cyber protection,
such as intrusion detection, firewalls, and encryption, is insufficient to
secure the cyber systems. Cyber resilience provides a new security paradigm
that complements inadequate protection with resilience mechanisms. A
Cyber-Resilient Mechanism (CRM) adapts to the known or zero-day threats and
uncertainties in real-time and strategically responds to them to maintain
critical functions of the cyber systems in the event of successful attacks.
Feedback architectures play a pivotal role in enabling the online sensing,
reasoning, and actuation process of the CRM. Reinforcement Learning (RL) is an
essential tool that epitomizes the feedback architectures for cyber resilience.
It allows the CRM to provide sequential responses to attacks with limited or
without prior knowledge of the environment and the attacker. In this work, we
review the literature on RL for cyber resilience and discuss cyber resilience
against three major types of vulnerabilities, i.e., posture-related,
information-related, and human-related vulnerabilities. We introduce three
application domains of CRMs: moving target defense, defensive cyber deception,
and assistive human security technologies. The RL algorithms also have
vulnerabilities themselves. We explain the three vulnerabilities of RL and
present attack models where the attacker targets the information exchanged
between the environment and the agent: the rewards, the state observations, and
the action commands. We show that the attacker can trick the RL agent into
learning a nefarious policy with minimum attacking effort. Lastly, we discuss
the future challenges of RL for cyber security and resilience and emerging
applications of RL-based CRMs.

    

### [[2110.11027] FedGEMS: Federated Learning of Larger Server Models via Selective Knowledge Fusion](http://arxiv.org/abs/2110.11027)


  Today data is often scattered among billions of resource-constrained edge
devices with security and privacy constraints. Federated Learning (FL) has
emerged as a viable solution to learn a global model while keeping data
private, but the model complexity of FL is impeded by the computation resources
of edge nodes. In this work, we investigate a novel paradigm to take advantage
of a powerful server model to break through model capacity in FL. By
selectively learning from multiple teacher clients and itself, a server model
develops in-depth knowledge and transfers its knowledge back to clients in
return to boost their respective performance. Our proposed framework achieves
superior performance on both server and client models and provides several
advantages in a unified framework, including flexibility for heterogeneous
client architectures, robustness to poisoning attacks, and communication
efficiency between clients and server on various image classification tasks.

    

### [[2110.12544] Online estimation and control with optimal pathlength regret](http://arxiv.org/abs/2110.12544)


  A natural goal when designing online learning algorithms for non-stationary
environments is to bound the regret of the algorithm in terms of the temporal
variation of the input sequence. Intuitively, when the variation is small, it
should be easier for the algorithm to achieve low regret, since past
observations are predictive of future inputs. Such data-dependent "pathlength"
regret bounds have recently been obtained for a wide variety of online learning
problems, including OCO and bandits. We obtain the first pathlength regret
bounds for online control and estimation (e.g. Kalman filtering) in linear
dynamical systems. The key idea in our derivation is to reduce
pathlength-optimal filtering and control to certain variational problems in
robust estimation and control; these reductions may be of independent interest.
Numerical simulations confirm that our pathlength-optimal algorithms outperform
traditional $H_2$ and $H_{\infty}$ algorithms when the environment varies over
time.

    

### [[2112.02936] Pairwise Learning for Neural Link Prediction](http://arxiv.org/abs/2112.02936)


  In this paper, we aim at providing an effective Pairwise Learning Neural Link
Prediction (PLNLP) framework. The framework treats link prediction as a
pairwise learning to rank problem and consists of four main components, i.e.,
neighborhood encoder, link predictor, negative sampler and objective function.
The framework is flexible that any generic graph neural convolution or link
prediction specific neural architecture could be employed as neighborhood
encoder. For link predictor, we design different scoring functions, which could
be selected based on different types of graphs. In negative sampler, we provide
several sampling strategies, which are problem specific. As for objective
function, we propose to use an effective ranking loss, which approximately
maximizes the standard ranking metric AUC. We evaluate the proposed PLNLP
framework on 4 link property prediction datasets of Open Graph Benchmark,
including ogbl-ddi, ogbl-collab, ogbl-ppa and ogbl-ciation2. PLNLP achieves Top
1 performance on ogbl-ddi, and Top 2 performance on ogbl-collab and
ogbl-ciation2 only with basic neural architecture. The performance demonstrates
the effectiveness of PLNLP.

    

### [[2112.03662] Lightning: Striking the Secure Isolation on GPU Clouds with Transient Hardware Faults](http://arxiv.org/abs/2112.03662)


  GPU clouds have become a popular computing platform because of the cost of
owning and maintaining high-performance computing clusters. Many cloud
architectures have also been proposed to ensure a secure execution environment
for guest applications by enforcing strong security policies to isolate the
untrusted hypervisor from the guest virtual machines (VMs). In this paper, we
study the impact of GPU chip's hardware faults on the security of cloud
"trusted" execution environment using Deep Neural Network (DNN) as the
underlying application. We show that transient hardware faults of GPUs can be
generated by exploiting the Dynamic Voltage and Frequency Scaling (DVFS)
technology, and these faults may cause computation errors, but they have
limited impact on the inference accuracy of DNN due to the robustness and
fault-tolerant nature of well-developed DNN models. To take full advantage of
these transient hardware faults, we propose the Lightning attack to locate the
fault injection targets of DNNs and to control the fault injection precision in
terms of timing and position. We conduct experiments on three commodity GPUs to
attack four widely-used DNNs. Experimental results show that the proposed
attack can reduce the inference accuracy of the models by as high as 78.3\% and
64.5\% on average. More importantly, 67.9\% of the targeted attacks have
successfully misled the models to give our desired incorrect inference result.
This demonstrates that the secure isolation on GPU clouds is vulnerable against
transient hardware faults and the computation results may not be trusted.

    

### [[2112.03426] A Survey of Verification, Validation and Testing Solutions for Smart Contracts](http://arxiv.org/abs/2112.03426)


  Smart contracts are programs stored on a blockchain that run when
predetermined conditions are met. However, designing and implementing a smart
contract is not trivial since upon deployment on a blockchain, it is no longer
possible to modify it (neither for improving nor for bug fixing). It is only
possible by deploying a new version of the smart contract which is costly
(deployment cost for the new contract and destruction cost for the old
contract). To this end, there are many solutions for testing the smart
contracts before their deployment. Since realizing bug-free smart contracts
increase the reliability, as well as reduce the cost, testing is an essential
activity. In this paper, we group the existing solutions that attempt to tackle
smart contract testing into following categories: public test networks,
security analysis tools, blockchain emulators and blockchain simulators. Then,
we analyze these solutions, categorize them and show what their pros and cons
are.

    

### [[2112.03435] Campaign Knowledge Network: Building Knowledge for Campaign Efficiency](http://arxiv.org/abs/2112.03435)


  In the landscape of exascale computing collaborative research campaigns are
conducted as co-design activities of loosely coordinated experiments. But the
higher level context and the knowledge of individual experimental activity is
lost over time. We undertook a knowledge capture and representation aid called
Campaign Knowledge Network(CKN), a co-design design and analysis tool. We
demonstrate that CKN can satisfy the Hoarde abstraction and can distill
campaign context from runtime information thereby creating a knowledge resource
upon which analysis tools can run to provide more efficient experimentation

    

### [[2112.03504] Improving Dynamic Regret in Distributed Online Mirror Descent Using Primal and Dual Information](http://arxiv.org/abs/2112.03504)


  We consider the problem of distributed online optimization, with a group of
learners connected via a dynamic communication graph. The goal of the learners
is to track the global minimizer of a sum of time-varying loss functions in a
distributed manner. We propose a novel algorithm, termed Distributed Online
Mirror Descent with Multiple Averaging Decision and Gradient Consensus
(DOMD-MADGC), which is based on mirror descent but incorporates multiple
consensus averaging iterations over local gradients as well as local decisions.
The key idea is to allow the local learners to collect a sufficient amount of
global information, which enables them to more accurately approximation the
time-varying global loss, so that they can closely track the dynamic global
minimizer over time. We show that the dynamic regret of DOMD-MADGC is upper
bounded by the path length, which is defined as the cumulative distance between
successive minimizers. The resulting bound improves upon the bounds of existing
distributed online algorithms and removes the explicit dependence on $T$.

    

### [[2112.03543] Phase Transition of the 3-Majority Dynamics with Uniform Communication Noise](http://arxiv.org/abs/2112.03543)


  Communication noise is a common feature in several real-world scenarios where
systems of agents need to communicate in order to pursue some collective task.
In particular, many biologically inspired systems that try to achieve
agreements on some opinion must implement resilient dynamics that are not
strongly affected by noisy communications. In this work, we study the popular
3-Majority dynamics, an opinion dynamics which has been proved to be an
efficient protocol for the majority consensus problem, in which we introduce a
simple feature of uniform communication noise, following (d'Amore et al. 2020).
We prove that in the fully connected communication network of n agents and in
the binary opinion case, the process induced by the 3-Majority dynamics
exhibits a phase transition. For a noise probability $p < 1/3$, the dynamics
reaches in logarithmic time an almost-consensus metastable phase which lasts
for a polynomial number of rounds with high probability. Furthermore, departing
from previous analyses, we further characterize this phase by showing that
there exists an attractive equilibrium value $s_{\text{eq}} \in [n]$ for the
bias of the system, i.e. the difference between the majority community size and
the minority one. Moreover, the agreement opinion turns out to be the initial
majority one if the bias towards it is of magnitude $\Omega(\sqrt{n\log n})$ in
the initial configuration. If, instead, $p > 1/3$, no form of consensus is
possible, and any information regarding the initial majority opinion is lost in
logarithmic time with high probability. Despite more communications per-round
are allowed, the 3-Majority dynamics surprisingly turns out to be less
resilient to noise than the Undecided-State dynamics (d'Amore et al. 2020),
whose noise threshold value is $p = 1/2$.

    

### [[2112.03692] Blockchain Synchronous Trust Consensus Model](http://arxiv.org/abs/2112.03692)


  This work introduces a novel approach for the governance of a blockchain
containing social constructs and technical viability for widescale applications
for the next generation of distributed ledgers. Functional requirements for
this new blockchain distributed ledger (BDL) were garnered from an analysis of
the needs for large-scale applications. Applied research was employed as part
of this endeavor to test the practicality and scalability of the solution
outline. Novel features in this application draw together controls and
enforcement for cybersecurity, digital content management, licensing, and
configuration management. The Synchronous Trust Consensus Model applied
research project named Project Philos was sponsored by the BlockChain
Development Community (BCDC) with support from the University of Colorado.
Research has followed both theorized conceptual and theory-to-practice models
to prove the scientific soundness and the viability of incentive for community
engagement. Results show that this new model proves the feasibility of an
indefinitely expandable blockchain distributed ledger capability, while also
providing a new participant incentive that is highly effective in engaging a
community of practitioners.

    

### [[2112.03851] Stochastic Optimized Schwarz Methods for the Gravity Equations on Graphics Processing Unit](http://arxiv.org/abs/2112.03851)


  Low order, sequential or non-massively parallel finite elements are generaly
used for three-dimensional gravity modelling. In this paper, in order to obtain
better gravity anomaly solutions in heterogeneous media, we solve the
gravimetry problem using massively parallel high order finite elements on
hybrid multi-CPU/GPU clusters. Parallel algorithms well suited for such hybrid
architectures have to be designed. A new stochastic-based optimization
procedure for the optimized Schwarz method is here presented, implemented and
tuned to graphical cards processors units. Numerical experiments performed on a
reallistic test case, demonstrates the robustness and efficiency of the
proposed method and of its implementation on massive multi-CPU/GPU
architectures.

    

### [[2112.03896] Gradient and Projection Free Distributed Online Min-Max Resource Optimization](http://arxiv.org/abs/2112.03896)


  We consider distributed online min-max resource allocation with a set of
parallel agents and a parameter server. Our goal is to minimize the pointwise
maximum over a set of time-varying convex and decreasing cost functions,
without a priori information about these functions. We propose a novel online
algorithm, termed Distributed Online resource Re-Allocation (DORA), where
non-stragglers learn to relinquish resource and share resource with stragglers.
A notable feature of DORA is that it does not require gradient calculation or
projection operation, unlike most existing online optimization strategies. This
allows it to substantially reduce the computation overhead in large-scale and
distributed networks. We show that the dynamic regret of the proposed algorithm
is upper bounded by $O\left(T^{\frac{3}{4}}(1+P_T)^{\frac{1}{4}}\right)$, where
$T$ is the total number of rounds and $P_T$ is the path-length of the
instantaneous minimizers. We further consider an application to the bandwidth
allocation problem in distributed online machine learning. Our numerical study
demonstrates the efficacy of the proposed solution and its performance
advantage over gradient- and/or projection-based resource allocation algorithms
in reducing wall-clock time.

    

### [[2112.03277] Quality control for more reliable integration of deep learning-based image segmentation into medical workflows](http://arxiv.org/abs/2112.03277)


  Machine learning algorithms underpin modern diagnostic-aiding software, which
has proved valuable in clinical practice, particularly in radiology. However,
inaccuracies, mainly due to the limited availability of clinical samples for
training these algorithms, hamper their wider applicability, acceptance, and
recognition amongst clinicians. We present an analysis of state-of-the-art
automatic quality control (QC) approaches that can be implemented within these
algorithms to estimate the certainty of their outputs. We validated the most
promising approaches on a brain image segmentation task identifying white
matter hyperintensities (WMH) in magnetic resonance imaging data. WMH are a
correlate of small vessel disease common in mid-to-late adulthood and are
particularly challenging to segment due to their varied size, and
distributional patterns. Our results show that the aggregation of uncertainty
and Dice prediction were most effective in failure detection for this task.
Both methods independently improved mean Dice from 0.82 to 0.84. Our work
reveals how QC methods can help to detect failed segmentation cases and
therefore make automatic segmentation more reliable and suitable for clinical
practice.

    

### [[2112.03346] Multidimensional Assignment Problem for multipartite entity resolution](http://arxiv.org/abs/2112.03346)


  Multipartite entity resolution aims at integrating records from multiple
datasets into one entity. We derive a mathematical formulation for a general
class of record linkage problems in multipartite entity resolution across many
datasets as a combinatorial optimization problem known as the multidimensional
assignment problem. As a motivation for our approach, we illustrate the
advantage of multipartite entity resolution over sequential bipartite matching.
Because the optimization problem is NP-hard, we apply two heuristic procedures,
a Greedy algorithm and very large scale neighborhood search, to solve the
assignment problem and find the most likely matching of records from multiple
datasets into a single entity. We evaluate and compare the performance of these
algorithms and their modifications on synthetically generated data. We perform
computational experiments to compare performance of recent heuristic, the very
large-scale neighborhood search, with a Greedy algorithm, another heuristic for
the MAP, as well as with two versions of genetic algorithm, a general
metaheuristic. Importantly, we perform experiments to compare two alternative
methods of re-starting the search for the former heuristic, specifically a
random-sampling multi-start and a deterministic design-based multi-start. We
find evidence that design-based multi-start can be more efficient as the size
of databases grow large. In addition, we show that very large scale search,
especially its multi-start version, outperforms simple Greedy heuristic.
Hybridization of Greedy search with very large scale neighborhood search
improves the performance. Using multi-start with as few as three additional
runs of very large scale search offers some improvement in the performance of
the very large scale search procedure. Last, we propose an approach to
evaluating complexity of the very large-scale neighborhood search.

    

### [[2112.03351] Audio Deepfake Perceptions in College Going Populations](http://arxiv.org/abs/2112.03351)


  Deepfake is content or material that is generated or manipulated using AI
methods, to pass off as real. There are four different deepfake types: audio,
video, image and text. In this research we focus on audio deepfakes and how
people perceive it. There are several audio deepfake generation frameworks, but
we chose MelGAN which is a non-autoregressive and fast audio deepfake
generating framework, requiring fewer parameters. This study tries to assess
audio deepfake perceptions among college students from different majors. This
study also answers the question of how their background and major can affect
their perception towards AI generated deepfakes. We also analyzed the results
based on different aspects of: grade level, complexity of the grammar used in
the audio clips, length of the audio clips, those who knew the term deepfakes
and those who did not, as well as the political angle. It is interesting that
the results show when an audio clip has a political connotation, it can affect
what people think about whether it is real or fake, even if the content is
fairly similar. This study also explores the question of how background and
major can affect perception towards deepfakes.

    

### [[2112.03414] JUSTICE: A Benchmark Dataset for Supreme Court's Judgment Prediction](http://arxiv.org/abs/2112.03414)


  Artificial intelligence is being utilized in many domains as of late, and the
legal system is no exception. However, as it stands now, the number of
well-annotated datasets pertaining to legal documents from the Supreme Court of
the United States (SCOTUS) is very limited for public use. Even though the
Supreme Court rulings are public domain knowledge, trying to do meaningful work
with them becomes a much greater task due to the need to manually gather and
process that data from scratch each time. Hence, our goal is to create a
high-quality dataset of SCOTUS court cases so that they may be readily used in
natural language processing (NLP) research and other data-driven applications.
Additionally, recent advances in NLP provide us with the tools to build
predictive models that can be used to reveal patterns that influence court
decisions. By using advanced NLP algorithms to analyze previous court cases,
the trained models are able to predict and classify a court's judgment given
the case's facts from the plaintiff and the defendant in textual format; in
other words, the model is emulating a human jury by generating a final verdict.

    

### [[2112.03415] Producing augmentation-invariant embeddings from real-life imagery](http://arxiv.org/abs/2112.03415)


  This article presents an efficient way to produce feature-rich,
high-dimensionality embedding spaces from real-life images. The features
produced are designed to be independent from augmentations used in real-life
cases which appear on social media. Our approach uses convolutional neural
networks (CNN) to produce an embedding space. An ArcFace head was used to train
the model by employing automatically produced augmentations. Additionally, we
present a way to make an ensemble out of different embeddings containing the
same semantic information, a way to normalize the resulting embedding using an
external dataset, and a novel way to perform quick training of these models
with a high number of classes in the ArcFace head. Using this approach we
achieved the 2nd place in the 2021 Facebook AI Image Similarity Challenge:
Descriptor Track.

    

### [[2112.03456] RSBNet: One-Shot Neural Architecture Search for A Backbone Network in Remote Sensing Image Recognition](http://arxiv.org/abs/2112.03456)


  Recently, a massive number of deep learning based approaches have been
successfully applied to various remote sensing image (RSI) recognition tasks.
However, most existing advances of deep learning methods in the RSI field
heavily rely on the features extracted by the manually designed backbone
network, which severely hinders the potential of deep learning models due the
complexity of RSI and the limitation of prior knowledge. In this paper, we
research a new design paradigm for the backbone architecture in RSI recognition
tasks, including scene classification, land-cover classification and object
detection. A novel one-shot architecture search framework based on
weight-sharing strategy and evolutionary algorithm is proposed, called RSBNet,
which consists of three stages: Firstly, a supernet constructed in a layer-wise
search space is pretrained on a self-assembled large-scale RSI dataset based on
an ensemble single-path training strategy. Next, the pre-trained supernet is
equipped with different recognition heads through the switchable recognition
module and respectively fine-tuned on the target dataset to obtain
task-specific supernet. Finally, we search the optimal backbone architecture
for different recognition tasks based on the evolutionary algorithm without any
network training. Extensive experiments have been conducted on five benchmark
datasets for different recognition tasks, the results show the effectiveness of
the proposed search paradigm and demonstrate that the searched backbone is able
to flexibly adapt different RSI recognition tasks and achieve impressive
performance.

    

### [[2112.03458] Glue: Adaptively Merging Single Table Cardinality to Estimate Join Query Size](http://arxiv.org/abs/2112.03458)


  Cardinality estimation (CardEst), a central component of the query optimizer,
plays a significant role in generating high-quality query plans in DBMS. The
CardEst problem has been extensively studied in the last several decades, using
both traditional and ML-enhanced methods. Whereas, the hardest problem in
CardEst, i.e., how to estimate the join query size on multiple tables, has not
been extensively solved. Current methods either reply on independence
assumptions or apply techniques with heavy burden, whose performance is still
far from satisfactory. Even worse, existing CardEst methods are often designed
to optimize one goal, i.e., inference speed or estimation accuracy, which can
not adapt to different occasions.
In this paper, we propose a very general framework, called Glue, to tackle
with these challenges. Its key idea is to elegantly decouple the correlations
across different tables and losslessly merge single table CardEst results to
estimate the join query size. Glue supports obtaining the single table-wise
CardEst results using any existing CardEst method and can process any complex
join schema. Therefore, it easily adapts to different scenarios having
different performance requirements, i.e., OLTP with fast estimation time or
OLAP with high estimation accuracy. Meanwhile, we show that Glue can be
seamlessly integrated into the plan search process and is able to support
counting distinct number of values. All these properties exhibit the potential
advances of deploying Glue in real-world DBMS.

    

### [[2112.03521] UNITER-Based Situated Coreference Resolution with Rich Multimodal Input](http://arxiv.org/abs/2112.03521)


  We present our work on the multimodal coreference resolution task of the
Situated and Interactive Multimodal Conversation 2.0 (SIMMC 2.0) dataset as a
part of the tenth Dialog System Technology Challenge (DSTC10). We propose a
UNITER-based model utilizing rich multimodal context such as textual dialog
history, object knowledge base and visual dialog scenes to determine whether
each object in the current scene is mentioned in the current dialog turn.
Results show that the proposed approach outperforms the official DSTC10
baseline substantially, with the object F1 score boosted from 36.6% to 77.3% on
the development set, demonstrating the effectiveness of the proposed object
representations from rich multimodal input. Our model ranks second in the
official evaluation on the object coreference resolution task with an F1 score
of 73.3% after model ensembling.

    

### [[2112.03530] A Conditional Point Diffusion-Refinement Paradigm for 3D Point Cloud Completion](http://arxiv.org/abs/2112.03530)


  3D point cloud is an important 3D representation for capturing real world 3D
objects. However, real-scanned 3D point clouds are often incomplete, and it is
important to recover complete point clouds for downstream applications. Most
existing point cloud completion methods use Chamfer Distance (CD) loss for
training. The CD loss estimates correspondences between two point clouds by
searching nearest neighbors, which does not capture the overall point density
distribution on the generated shape, and therefore likely leads to non-uniform
point cloud generation. To tackle this problem, we propose a novel Point
Diffusion-Refinement (PDR) paradigm for point cloud completion. PDR consists of
a Conditional Generation Network (CGNet) and a ReFinement Network (RFNet). The
CGNet uses a conditional generative model called the denoising diffusion
probabilistic model (DDPM) to generate a coarse completion conditioned on the
partial observation. DDPM establishes a one-to-one pointwise mapping between
the generated point cloud and the uniform ground truth, and then optimizes the
mean squared error loss to realize uniform generation. The RFNet refines the
coarse output of the CGNet and further improves quality of the completed point
cloud. Furthermore, we develop a novel dual-path architecture for both
networks. The architecture can (1) effectively and efficiently extract
multi-level features from partially observed point clouds to guide completion,
and (2) accurately manipulate spatial locations of 3D points to obtain smooth
surfaces and sharp details. Extensive experimental results on various benchmark
datasets show that our PDR paradigm outperforms previous state-of-the-art
methods for point cloud completion. Remarkably, with the help of the RFNet, we
can accelerate the iterative generation process of the DDPM by up to 50 times
without much performance drop.

    

### [[2112.03533] A Time-domain Generalized Wiener Filter for Multi-channel Speech Separation](http://arxiv.org/abs/2112.03533)


  Frequency-domain neural beamformers are the mainstream methods for recent
multi-channel speech separation models. Despite their well-defined behaviors
and the effectiveness, such frequency-domain beamformers still have the
limitations of a bounded oracle performance and the difficulties of designing
proper networks for the complex-valued operations. In this paper, we propose a
time-domain generalized Wiener filter (TD-GWF), an extension to the
conventional frequency-domain beamformers that has higher oracle performance
and only involves real-valued operations. We also provide discussions on how
TD-GWF can be connected to conventional frequency-domain beamformers.
Experiment results show that a significant performance improvement can be
achieved by replacing frequency-domain beamformers by the TD-GWF in the
recently proposed sequential neural beamforming pipelines.

    

### [[2112.03552] Bootstrapping ViTs: Towards Liberating Vision Transformers from Pre-training](http://arxiv.org/abs/2112.03552)


  Recently, vision Transformers (ViTs) are developing rapidly and starting to
challenge the domination of convolutional neural networks (CNNs) in the realm
of computer vision (CV). With the general-purpose Transformer architecture for
replacing the hard-coded inductive biases of convolution, ViTs have surpassed
CNNs, especially in data-sufficient circumstances. However, ViTs are prone to
over-fit on small datasets and thus rely on large-scale pre-training, which
expends enormous time. In this paper, we strive to liberate ViTs from
pre-training by introducing CNNs' inductive biases back to ViTs while
preserving their network architectures for higher upper bound and setting up
more suitable optimization objectives. To begin with, an agent CNN is designed
based on the given ViT with inductive biases. Then a bootstrapping training
algorithm is proposed to jointly optimize the agent and ViT with weight
sharing, during which the ViT learns inductive biases from the intermediate
features of the agent. Extensive experiments on CIFAR-10/100 and ImageNet-1k
with limited training data have shown encouraging results that the inductive
biases help ViTs converge significantly faster and outperform conventional CNNs
with even fewer parameters.

    

### [[2112.03557] Multi-speaker Emotional Text-to-speech Synthesizer](http://arxiv.org/abs/2112.03557)


  We present a methodology to train our multi-speaker emotional text-to-speech
synthesizer that can express speech for 10 speakers' 7 different emotions. All
silences from audio samples are removed prior to learning. This results in fast
learning by our model. Curriculum learning is applied to train our model
efficiently. Our model is first trained with a large single-speaker neutral
dataset, and then trained with neutral speech from all speakers. Finally, our
model is trained using datasets of emotional speech from all speakers. In each
stage, training samples of each speaker-emotion pair have equal probability to
appear in mini-batches. Through this procedure, our model can synthesize speech
for all targeted speakers and emotions. Our synthesized audio sets are
available on our web page.

    

### [[2112.03577] Pragmatic Implementation of Reinforcement Algorithms For Path Finding On Raspberry Pi](http://arxiv.org/abs/2112.03577)


  In this paper, pragmatic implementation of an indoor autonomous delivery
system that exploits Reinforcement Learning algorithms for path planning and
collision avoidance is audited. The proposed system is a cost-efficient
approach that is implemented to facilitate a Raspberry Pi controlled
four-wheel-drive non-holonomic robot map a grid. This approach computes and
navigates the shortest path from a source key point to a destination key point
to carry out the desired delivery. Q learning and Deep-Q learning are used to
find the optimal path while avoiding collision with static obstacles. This work
defines an approach to deploy these two algorithms on a robot. A novel
algorithm to decode an array of directions into accurate movements in a certain
action space is also proposed. The procedure followed to dispatch this system
with the said requirements is described, ergo presenting our proof of concept
for indoor autonomous delivery vehicles.

    

### [[2112.03615] Saliency Diversified Deep Ensemble for Robustness to Adversaries](http://arxiv.org/abs/2112.03615)


  Deep learning models have shown incredible performance on numerous image
recognition, classification, and reconstruction tasks. Although very appealing
and valuable due to their predictive capabilities, one common threat remains
challenging to resolve. A specifically trained attacker can introduce malicious
input perturbations to fool the network, thus causing potentially harmful
mispredictions. Moreover, these attacks can succeed when the adversary has full
access to the target model (white-box) and even when such access is limited
(black-box setting). The ensemble of models can protect against such attacks
but might be brittle under shared vulnerabilities in its members (attack
transferability). To that end, this work proposes a novel diversity-promoting
learning approach for the deep ensembles. The idea is to promote saliency map
diversity (SMD) on ensemble members to prevent the attacker from targeting all
ensemble members at once by introducing an additional term in our learning
objective. During training, this helps us minimize the alignment between model
saliencies to reduce shared member vulnerabilities and, thus, increase ensemble
robustness to adversaries. We empirically show a reduced transferability
between ensemble members and improved performance compared to the
state-of-the-art ensemble defense against medium and high strength white-box
attacks. In addition, we demonstrate that our approach combined with existing
methods outperforms state-of-the-art ensemble algorithms for defense under
white-box and black-box attacks.

    

### [[2112.03643] QKSA: Quantum Knowledge Seeking Agent -- resource-optimized reinforcement learning using quantum process tomography](http://arxiv.org/abs/2112.03643)


  In this research, we extend the universal reinforcement learning (URL) agent
models of artificial general intelligence to quantum environments. The utility
function of a classical exploratory stochastic Knowledge Seeking Agent, KL-KSA,
is generalized to distance measures from quantum information theory on density
matrices. Quantum process tomography (QPT) algorithms form the tractable subset
of programs for modeling environmental dynamics. The optimal QPT policy is
selected based on a mutable cost function based on algorithmic complexity as
well as computational resource complexity. Instead of Turing machines, we
estimate the cost metrics on a high-level language to allow realistic
experimentation. The entire agent design is encapsulated in a self-replicating
quine which mutates the cost function based on the predictive value of the
optimal policy choosing scheme. Thus, multiple agents with pareto-optimal QPT
policies evolve using genetic programming, mimicking the development of
physical theories each with different resource trade-offs. This formal
framework is termed Quantum Knowledge Seeking Agent (QKSA).
Despite its importance, few quantum reinforcement learning models exist in
contrast to the current thrust in quantum machine learning. QKSA is the first
proposal for a framework that resembles the classical URL models. Similar to
how AIXI-tl is a resource-bounded active version of Solomonoff universal
induction, QKSA is a resource-bounded participatory observer framework to the
recently proposed algorithmic information-based reconstruction of quantum
mechanics. QKSA can be applied for simulating and studying aspects of quantum
information theory. Specifically, we demonstrate that it can be used to
accelerate quantum variational algorithms which include tomographic
reconstruction as its integral subroutine.

    

### [[2112.03670] Hybrid Self-Attention NEAT: A novel evolutionary approach to improve the NEAT algorithm](http://arxiv.org/abs/2112.03670)


  This article presents a "Hybrid Self-Attention NEAT" method to improve the
original NeuroEvolution of Augmenting Topologies (NEAT) algorithm in
high-dimensional inputs. Although the NEAT algorithm has shown a significant
result in different challenging tasks, as input representations are high
dimensional, it cannot create a well-tuned network. Our study addresses this
limitation by using self-attention as an indirect encoding method to select the
most important parts of the input. In addition, we improve its overall
performance with the help of a hybrid method to evolve the final network
weights. The main conclusion is that Hybrid Self- Attention NEAT can eliminate
the restriction of the original NEAT. The results indicate that in comparison
with evolutionary algorithms, our model can get comparable scores in Atari
games with raw pixels input with a much lower number of parameters.

    

### [[2112.03727] RFGAN: RF-Based Human Synthesis](http://arxiv.org/abs/2112.03727)


  This paper demonstrates human synthesis based on the Radio Frequency (RF)
signals, which leverages the fact that RF signals can record human movements
with the signal reflections off the human body. Different from existing RF
sensing works that can only perceive humans roughly, this paper aims to
generate fine-grained optical human images by introducing a novel cross-modal
RFGAN model. Specifically, we first build a radio system equipped with
horizontal and vertical antenna arrays to transceive RF signals. Since the
reflected RF signals are processed as obscure signal projection heatmaps on the
horizontal and vertical planes, we design a RF-Extractor with RNN in RFGAN for
RF heatmap encoding and combining to obtain the human activity information.
Then we inject the information extracted by the RF-Extractor and RNN as the
condition into GAN using the proposed RF-based adaptive normalizations.
Finally, we train the whole model in an end-to-end manner. To evaluate our
proposed model, we create two cross-modal datasets (RF-Walk & RF-Activity) that
contain thousands of optical human activity frames and corresponding RF
signals. Experimental results show that the RFGAN can generate target human
activity frames using RF signals. To the best of our knowledge, this is the
first work to generate optical images based on RF signals.

    

### [[2112.03740] Dilated convolution with learnable spacings](http://arxiv.org/abs/2112.03740)


  Dilated convolution is basically a convolution with a wider kernel created by
regularly inserting spaces between the kernel elements. In this article, we
present a new version of the dilated convolution in which the spacings are made
learnable via backpropagation through an interpolation technique. We call this
method "Dilated Convolution with Learnable Spacings" (DCLS) and we generalize
its approach to the n-dimensional convolution case. However, our main focus
here will be the 2D case for which we developed two implementations: a naive
one that constructs the dilated kernel, suitable for small dilation rates, and
a more time/memory efficient one that uses a modified version of the "im2col"
algorithm. We then illustrate how this technique improves the accuracy of
existing architectures on semantic segmentation task on Pascal Voc 2012 dataset
via a simple drop-in replacement of the classical dilated convolutional layers
by DCLS ones. Furthermore, we show that DCLS allows to reduce the number of
learnable parameters of the depthwise convolutions used in the recent ConvMixer
architecture by a factor 3 with no or very low reduction in accuracy and that
by replacing large dense kernels with sparse DCLS ones. The code of the method
is based on Pytorch and available at:
this https URL.

    

### [[2112.03756] Bridging the Model-Reality Gap with Lipschitz Network Adaptation](http://arxiv.org/abs/2112.03756)


  As robots venture into the real world, they are subject to unmodeled dynamics
and disturbances. Traditional model-based control approaches have been proven
successful in relatively static and known operating environments. However, when
an accurate model of the robot is not available, model-based design can lead to
suboptimal and even unsafe behaviour. In this work, we propose a method that
bridges the model-reality gap and enables the application of model-based
approaches even if dynamic uncertainties are present. In particular, we present
a learning-based model reference adaptation approach that makes a robot system,
with possibly uncertain dynamics, behave as a predefined reference model. In
turn, the reference model can be used for model-based controller design. In
contrast to typical model reference adaptation control approaches, we leverage
the representative power of neural networks to capture highly nonlinear
dynamics uncertainties and guarantee stability by encoding a certifying
Lipschitz condition in the architectural design of a special type of neural
network called the Lipschitz network. Our approach applies to a general class
of nonlinear control-affine systems even when our prior knowledge about the
true robot system is limited. We demonstrate our approach in flying inverted
pendulum experiments, where an off-the-shelf quadrotor is challenged to balance
an inverted pendulum while hovering or tracking circular trajectories.

    

### [[2112.03759] Learning a Robust Multiagent Driving Policy for Traffic Congestion Reduction](http://arxiv.org/abs/2112.03759)


  The advent of automated and autonomous vehicles (AVs) creates opportunities
to achieve system-level goals using multiple AVs, such as traffic congestion
reduction. Past research has shown that multiagent congestion-reducing driving
policies can be learned in a variety of simulated scenarios. While initial
proofs of concept were in small, closed traffic networks with a centralized
controller, recently successful results have been demonstrated in more
realistic settings with distributed control policies operating in open road
networks where vehicles enter and leave. However, these driving policies were
mostly tested under the same conditions they were trained on, and have not been
thoroughly tested for robustness to different traffic conditions, which is a
critical requirement in real-world scenarios. This paper presents a learned
multiagent driving policy that is robust to a variety of open-network traffic
conditions, including vehicle flows, the fraction of AVs in traffic, AV
placement, and different merging road geometries. A thorough empirical analysis
investigates the sensitivity of such a policy to the amount of AVs in both a
simple merge network and a more complex road with two merging ramps. It shows
that the learned policy achieves significant improvement over simulated
human-driven policies even with AV penetration as low as 2%. The same policy is
also shown to be capable of reducing traffic congestion in more complex roads
with two merging ramps.

    

### [[2112.03816] A Deep Learning Driven Algorithmic Pipeline for Autonomous Navigation in Row-Based Crops](http://arxiv.org/abs/2112.03816)


  Expensive sensors and inefficient algorithmic pipelines significantly affect
the overall cost of autonomous machines. However, affordable robotic solutions
are essential to practical usage, and their financial impact constitutes a
fundamental requirement to employ service robotics in most fields of
application. Among all, researchers in the precision agriculture domain strive
to devise robust and cost-effective autonomous platforms in order to provide
genuinely large-scale competitive solutions. In this article, we present a
complete algorithmic pipeline for row-based crops autonomous navigation,
specifically designed to cope with low-range sensors and seasonal variations.
Firstly, we build on a robust data-driven methodology to generate a viable path
for the autonomous machine, covering the full extension of the crop with only
the occupancy grid map information of the field. Moreover, our solution
leverages on latest advancement of deep learning optimization techniques and
synthetic generation of data to provide an affordable solution that efficiently
tackles the well-known Global Navigation Satellite System unreliability and
degradation due to vegetation growing inside rows. Extensive experimentation
and simulations against computer-generated environments and real-world crops
demonstrated the robustness and intrinsic generalizability of our methodology
that opens the possibility of highly affordable and fully autonomous machines.

    

### [[2112.03849] Natural Answer Generation: From Factoid Answer to Full-length Answer using Grammar Correction](http://arxiv.org/abs/2112.03849)


  Question Answering systems these days typically use template-based language
generation. Though adequate for a domain-specific task, these systems are too
restrictive and predefined for domain-independent systems. This paper proposes
a system that outputs a full-length answer given a question and the extracted
factoid answer (short spans such as named entities) as the input. Our system
uses constituency and dependency parse trees of questions. A transformer-based
Grammar Error Correction model GECToR (2020), is used as a post-processing step
for better fluency. We compare our system with (i) Modified Pointer Generator
(SOTA) and (ii) Fine-tuned DialoGPT for factoid questions. We also test our
approach on existential (yes-no) questions with better results. Our model
generates accurate and fluent answers than the state-of-the-art (SOTA)
approaches. The evaluation is done on NewsQA and SqUAD datasets with an
increment of 0.4 and 0.9 percentage points in ROUGE-1 score respectively. Also
the inference time is reduced by 85\% as compared to the SOTA. The improved
datasets used for our evaluation will be released as part of the research
contribution.

    

### [[2112.03850] Policy Search for Model Predictive Control with Application to Agile Drone Flight](http://arxiv.org/abs/2112.03850)


  Policy Search and Model Predictive Control~(MPC) are two different paradigms
for robot control: policy search has the strength of automatically learning
complex policies using experienced data, while MPC can offer optimal control
performance using models and trajectory optimization. An open research question
is how to leverage and combine the advantages of both approaches. In this work,
we provide an answer by using policy search for automatically choosing
high-level decision variables for MPC, which leads to a novel
policy-search-for-model-predictive-control framework. Specifically, we
formulate the MPC as a parameterized controller, where the hard-to-optimize
decision variables are represented as high-level policies. Such a formulation
allows optimizing policies in a self-supervised fashion. We validate this
framework by focusing on a challenging problem in agile drone flight: flying a
quadrotor through fast-moving gates. Experiments show that our controller
achieves robust and real-time control performance in both simulation and the
real world. The proposed framework offers a new perspective for merging
learning and control.

    

### [[2112.03877] Is Complexity Important for Philosophy of Mind?](http://arxiv.org/abs/2112.03877)


  Computational complexity has often been ignored in philosophy of mind, in
philosophical artificial intelligence studies. The purpose of this paper is
threefold. First and foremost, to show the importance of complexity rather than
computability in philosophical and AI problems. Second, to rephrase the notion
of computability in terms of solvability, i.e. treating computability as
non-sufficient for establishing intelligence. The Church-Turing thesis is
therefore revisited and rephrased in order to capture the ontological
background of spatial and temporal complexity. Third, to emphasize ontological
differences between different time complexities, which seem to provide a solid
base towards better understanding of artificial intelligence in general.

    

### [[1704.01014] An Ontological Architecture for Orbital Debris Data](http://arxiv.org/abs/1704.01014)


  The orbital debris problem presents an opportunity for inter-agency and
international cooperation toward the mutually beneficial goals of debris
prevention, mitigation, remediation, and improved space situational awareness
(SSA). Achieving these goals requires sharing orbital debris and other SSA
data. Toward this, I present an ontological architecture for the orbital debris
domain, taking steps in the creation of an orbital debris ontology (ODO). The
purpose of this ontological system is to (I) represent general orbital debris
and SSA domain knowledge, (II) structure, and standardize where needed, orbital
data and terminology, and (III) foster semantic interoperability and
data-sharing. In doing so I hope to (IV) contribute to solving the orbital
debris problem, improving peaceful global SSA, and ensuring safe space travel
for future generations.

    

### [[2003.00400] Learn Task First or Learn Human Partner First: A Hierarchical Task Decomposition Method for Human-Robot Cooperation](http://arxiv.org/abs/2003.00400)


  Applying Deep Reinforcement Learning (DRL) to Human-Robot Cooperation (HRC)
in dynamic control problems is promising yet challenging as the robot needs to
learn the dynamics of the controlled system and dynamics of the human partner.
In existing research, the robot powered by DRL adopts coupled observation of
the environment and the human partner to learn both dynamics simultaneously.
However, such a learning strategy is limited in terms of learning efficiency
and team performance. This work proposes a novel task decomposition method with
a hierarchical reward mechanism that enables the robot to learn the
hierarchical dynamic control task separately from learning the human partner's
behavior. The method is validated with a hierarchical control task in a
simulated environment with human subject experiments. Our method also provides
insight into the design of the learning strategy for HRC. The results show that
the robot should learn the task first to achieve higher team performance and
learn the human first to achieve higher learning efficiency.

    

### [[2012.10773] Forming Human-Robot Cooperation for Tasks with General Goal using Evolutionary Value Learning](http://arxiv.org/abs/2012.10773)


  In Human-Robot Cooperation (HRC), the robot cooperates with humans to
accomplish the task together. Existing approaches assume the human has a
specific goal during the cooperation, and the robot infers and acts toward it.
However, in real-world environments, a human usually only has a general goal
(e.g., general direction or area in motion planning) at the beginning of the
cooperation, which needs to be clarified to a specific goal (e.g., an exact
position) during cooperation. The specification process is interactive and
dynamic, which depends on the environment and the partners' behavior. The robot
that does not consider the goal specification process may cause frustration to
the human partner, elongate the time to come to an agreement, and compromise or
fail team performance. We present the Evolutionary Value Learning (EVL)
approach, which uses a State-based Multivariate Bayesian Inference method to
model the dynamics of the goal specification process in HRC. EVL can actively
enhance the process of goal specification and cooperation formation. This
enables the robot to simultaneously help the human specify the goal and learn a
cooperative policy in a Deep Reinforcement Learning (DRL) manner. In a dynamic
ball balancing task with real human subjects, the robot equipped with EVL
outperforms existing methods with faster goal specification processes and
better team performance.

    

### [[2106.01686] AliCG: Fine-grained and Evolvable Conceptual Graph Construction for Semantic Search at Alibaba](http://arxiv.org/abs/2106.01686)


  Conceptual graphs, which is a particular type of Knowledge Graphs, play an
essential role in semantic search. Prior conceptual graph construction
approaches typically extract high-frequent, coarse-grained, and time-invariant
concepts from formal texts. In real applications, however, it is necessary to
extract less-frequent, fine-grained, and time-varying conceptual knowledge and
build taxonomy in an evolving manner. In this paper, we introduce an approach
to implementing and deploying the conceptual graph at Alibaba. Specifically, We
propose a framework called AliCG which is capable of a) extracting fine-grained
concepts by a novel bootstrapping with alignment consensus approach, b) mining
long-tail concepts with a novel low-resource phrase mining approach, c)
updating the graph dynamically via a concept distribution estimation method
based on implicit and explicit user behaviors. We have deployed the framework
at Alibaba UC Browser. Extensive offline evaluation as well as online A/B
testing demonstrate the efficacy of our approach.

    

### [[2112.03592] Parallel Discrete Convolutions on Adaptive Particle Representations of Images](http://arxiv.org/abs/2112.03592)


  We present data structures and algorithms for native implementations of
discrete convolution operators over Adaptive Particle Representations (APR) of
images on parallel computer architectures. The APR is a content-adaptive image
representation that locally adapts the sampling resolution to the image signal.
It has been developed as an alternative to pixel representations for large,
sparse images as they typically occur in fluorescence microscopy. It has been
shown to reduce the memory and runtime costs of storing, visualizing, and
processing such images. This, however, requires that image processing natively
operates on APRs, without intermediately reverting to pixels. Designing
efficient and scalable APR-native image processing primitives, however, is
complicated by the APR's irregular memory structure. Here, we provide the
algorithmic building blocks required to efficiently and natively process APR
images using a wide range of algorithms that can be formulated in terms of
discrete convolutions. We show that APR convolution naturally leads to
scale-adaptive algorithms that efficiently parallelize on multi-core CPU and
GPU architectures. We quantify the speedups in comparison to pixel-based
algorithms and convolutions on evenly sampled data. We achieve pixel-equivalent
throughputs of up to 1 TB/s on a single Nvidia GeForce RTX 2080 gaming GPU,
requiring up to two orders of magnitude less memory than a pixel-based
implementation.

    

### [[2112.03653] A Specification for Typed Template Haskell](http://arxiv.org/abs/2112.03653)


  Multi-stage programming is a proven technique that provides predictable
performance characteristics by controlling code generation. We propose a core
semantics for Typed Template Haskell, an extension of Haskell that supports
multi staged programming that interacts well with polymorphism and qualified
types. Our semantics relates a declarative source language with qualified types
to a core language based on the the polymorphic lambda calculus augmented with
multi-stage constructs.

    

### [[2010.08261] Relating Functional and Imperative Session Types](http://arxiv.org/abs/2010.08261)


  Imperative session types provide an imperative interface to session-typed
communication. In such an interface, channel references are first-class objects
with operations that change the typestate of the channel. Compared to
functional session type APIs, the program structure is simpler at the surface,
but typestate is required to model the current state of communication
throughout.
Following an early work that explored the imperative approach, a significant
body of work on session types has neglected the imperative approach and opts
for a functional approach that uses linear types to manage channel references
soundly. We demonstrate that the functional approach subsumes the early work on
imperative session types by exhibiting a typing and semantics preserving
translation into a system of linear functional session types.
We further show that the untyped backwards translation from the functional to
the imperative calculus is semantics preserving. We restrict the type system of
the functional calculus such that the backwards translation becomes type
preserving. Thus, we precisely capture the difference in expressiveness of the
two calculi and conclude that the lack of expressiveness in the imperative
calculus is largely due to restrictions imposed by its type system.

    

### [[2105.03389] GADTs, Functoriality, Parametricity: Pick Two](http://arxiv.org/abs/2105.03389)


  GADTs can be represented either as their Church encodings à la Atkey, or as
fixpoints à la Johann and Polonsky. While a GADT represented as its Church
encoding need not support a map function satisfying the functor laws, the
fixpoint representation of a GADT must support such a map function even to be
well-defined. The two representations of a GADT thus need not be the same in
general. This observation forces a choice of representation of data types in
languages supporting GADTs. In this paper we show that choosing whether to
represent data types as their Church encodings or as fixpoints determines
whether or not a language supporting GADTs can have parametric models. This
choice thus has important consequences for how we can program with, and reason
about, these advanced data types.

    