
## 2021-7-21

### [<title>关于哪里可以住宿电子发票-ZOL问答财税网 - DockOne.io</title>](http://dockone.io/question/723520)

### [<title>关于贵阳哪里可以开广告设计费发票-贵阳本地宝tr - DockOne.io</title>](http://dockone.io/question/723519)

### [<title>关于兰州哪里可以开广告设计费发票-兰州本地宝eo - DockOne.io</title>](http://dockone.io/question/723518)

### [<title>关于呼和浩特餐饮电子发票-呼和浩特财税网 - DockOne.io</title>](http://dockone.io/question/723517)

### [<title>关于温州餐饮电子发票-温州财税网 - DockOne.io</title>](http://dockone.io/question/723516)

### [<title>关于长沙哪里可以开广告设计费发票-长沙本地宝oh - DockOne.io</title>](http://dockone.io/question/723515)

### [<title>关于昆明餐饮电子发票-昆明财税网 - DockOne.io</title>](http://dockone.io/question/723514)

### [<title>关于哈尔滨哪里可以开广告设计费发票-哈尔滨本地宝sg - DockOne.io</title>](http://dockone.io/question/723513)

### [<title>关于南昌餐饮电子发票-南昌财税网 - DockOne.io</title>](http://dockone.io/question/723512)

### [<title>关于石家庄哪里可以开广告设计费发票-石家庄本地宝zx - DockOne.io</title>](http://dockone.io/question/723511)

### [<title>关于福州餐饮电子发票-福州财税网 - DockOne.io</title>](http://dockone.io/question/723510)

### [<title>关于济南哪里可以开广告设计费发票-济南本地宝as - DockOne.io</title>](http://dockone.io/question/723509)

### [<title>关于惠州餐饮电子发票-惠州财税网 - DockOne.io</title>](http://dockone.io/question/723508)

### [<title>关于沈阳哪里可以开广告设计费发票-沈阳本地宝cc - DockOne.io</title>](http://dockone.io/question/723507)

### [<title>关于佛山餐饮电子发票-佛山财税网 - DockOne.io</title>](http://dockone.io/question/723506)

### [<title>关于长春哪里可以开广告设计费发票-长春本地宝sw - DockOne.io</title>](http://dockone.io/question/723505)

### [<title>关于海口哪里可以开广告设计费发票-海口本地宝zm - DockOne.io</title>](http://dockone.io/question/723504)

### [<title>关于东莞餐饮电子发票-东莞财税网 - DockOne.io</title>](http://dockone.io/question/723503)

### [<title>关于三亚哪里可以开广告设计费发票-三亚本地宝bf - DockOne.io</title>](http://dockone.io/question/723502)

### [<title>关于青岛餐饮电子发票-青岛财税网 - DockOne.io</title>](http://dockone.io/question/723501)

### [[2107.09518] Relay-Assisted Cooperative Federated Learning](http://arxiv.org/abs/2107.09518)


  Federated learning (FL) has recently emerged as a promising technology to
enable artificial intelligence (AI) at the network edge, where distributed
mobile devices collaboratively train a shared AI model under the coordination
of an edge server. To significantly improve the communication efficiency of FL,
over-the-air computation allows a large number of mobile devices to
concurrently upload their local models by exploiting the superposition property
of wireless multi-access channels. Due to wireless channel fading, the model
aggregation error at the edge server is dominated by the weakest channel among
all devices, causing severe straggler issues. In this paper, we propose a
relay-assisted cooperative FL scheme to effectively address the straggler
issue. In particular, we deploy multiple half-duplex relays to cooperatively
assist the devices in uploading the local model updates to the edge server. The
nature of the over-the-air computation poses system objectives and constraints
that are distinct from those in traditional relay communication systems.
Moreover, the strong coupling between the design variables renders the
optimization of such a system challenging. To tackle the issue, we propose an
alternating-optimization-based algorithm to optimize the transceiver and relay
operation with low complexity. Then, we analyze the model aggregation error in
a single-relay case and show that our relay-assisted scheme achieves a smaller
error than the one without relays provided that the relay transmit power and
the relay channel gains are sufficiently large. The analysis provides critical
insights on relay deployment in the implementation of cooperative FL. Extensive
numerical results show that our design achieves faster convergence compared
with state-of-the-art schemes.

    

### [[2107.09558] Into Summarization Techniques for IoT Data Discovery Routing](http://arxiv.org/abs/2107.09558)


  In this paper, we consider the IoT data discovery data objects to specific
nodes in the network. They are very problem in very large and growing scale
networks. Specifically, we investigate in depth the routing table summarization
techniques to support effective and space-efficient IoT data discovery routing.
Novel summarization algorithms, including alphabetical based, hash based, and
meaning based summarization and their corresponding coding schemes are
proposed. The issue of potentially misleading routing due to summarization is
also investigated. Subsequently, we analyze the strategy of when to summarize
in order to balance the tradeoff especially in handling MAA based lookups.
between the routing table compression rate and the chance of Unstructured
discovery routing approaches, such as [4] [5], causing misleading routing. For
experimental study, we have collected 100K IoT data streams from various IoT
databases as the input dataset. Experimental results show that our
summarization solution can reduce the routing table size by 20 to 30 folds with
2-5% increase in latency when compared with similar peer-to-peer discovery
routing algorithms without summarization. Also, our approach outperforms DHT
based approaches by 2 to 6 folds in terms of latency and traffic.

    

### [[2103.08234] Wireless Backhaul in 5G and Beyond: Issues, Challenges and Opportunities](http://arxiv.org/abs/2103.08234)


  With the introduction of new technologies such as Unmanned Aerial Vehicle
(UAV), High Altitude Platform Station (HAPS), Millimeter Wave (mmWave)
frequencies, Massive Multiple-Input Multiple-Output (mMIMO), and beamforming,
wireless backhaul is expected to be an integral part of the 5G networks. While
this concept is nothing new, it was shortcoming in terms of performance
compared to the fiber backhauling. However, with these new technologies, fiber
is no longer the foremost technology for backhauling. With the projected
densification of networks, wireless backhaul has become mandatory to use. There
are still challenges to be tackled if wireless backhaul is to be used
efficiently. Resource allocation, deployment, scheduling, power management and
energy efficiency are some of these problems. Wireless backhaul also acts as an
enabler for new technologies and improves some of the existing ones
significantly. To name a few, rural connectivity, satellite communication, and
mobile edge computing are some concepts for which wireless backhauling acts as
an enabler. Small cell usage with wireless backhaul presents different security
challenges. Governing bodies of cellular networks have standardization efforts
going on especially for the Integrated Acces-Backhaul (IAB) concept, and this
is briefly mentioned. Finally, wireless backhaul is also projected to be an
important part of the beyond 5G networks, and newly developed concepts such as
cell-free networking, ultra-massive MIMO, and extremely dense network show this
trend as well. In this survey, we present the aforementioned issues,
challenges, opportunities, and applications of wireless backhaul in 5G, while
briefly mentioning concepts related to wireless backhaul beyond 5G alongside
with security and standardization issues.

    

### [[2104.00508] Integrated optimization of heterogeneous-network management and the elusive role of macrocells](http://arxiv.org/abs/2104.00508)


  We consider heterogeneous wireless networks in the physical interference
model and introduce a new formulation of the mixed-integer nonlinear
programming problem that addresses base-station activation and many-to-many
associations while minimizing power consumption. We also introduce HetNetGA, a
genetic algorithm that can tackle the problem without any approximations.
Though unsuitable for practical deployment, HetNetGA enables the investigation
of such networks' true possibilities. Results for scenarios involving both
macrocells and picocells often align with what is expected, but sometimes are
unexpected and essentially point to the need to better understand the role of
macrocells in helping provide capacity while remaining energetically
advantageous.

    

### [[2104.08651] SMS Goes Nuclear: Fortifying SMS-Based MFA in Online Account Ecosystem](http://arxiv.org/abs/2104.08651)


  With the rapid growth of online services, the number of online accounts
proliferates. The security of a single user account no longer depends merely on
its own service provider but also the accounts on other service platforms(We
refer to this online account environment as Online Account Ecosystem). In this
paper, we first uncover the vulnerability of Online Account Ecosystem, which
stems from the defective multi-factor authentication (MFA), specifically the
ones with SMS-based verification, and dependencies among accounts on different
platforms. We propose Chain Reaction Attack that exploits the weakest point in
Online Account Ecosystem and can ultimately compromise the most secure
platform. Furthermore, we design and implement ActFort, a systematic approach
to detect the vulnerability of Online Account Ecosystem by analyzing the
authentication credential factors and sensitive personal information as well as
evaluating the dependency relationships among online accounts. We evaluate our
system on hundreds of representative online services listed in Alexa in
diversified fields. Based on the analysis from ActFort, we provide several
pragmatic insights into the current Online Account Ecosystem and propose
several feasible countermeasures including the online account exposed
information protection mechanism and the built-in authentication to fortify the
security of Online Account Ecosystem.

    

### [[2107.09051] AI in Finance: Challenges, Techniques and Opportunities](http://arxiv.org/abs/2107.09051)


  AI in finance broadly refers to the applications of AI techniques in
financial businesses. This area has been lasting for decades with both classic
and modern AI techniques applied to increasingly broader areas of finance,
economy and society. In contrast to either discussing the problems, aspects and
opportunities of finance that have benefited from specific AI techniques and in
particular some new-generation AI and data science (AIDS) areas or reviewing
the progress of applying specific techniques to resolving certain financial
problems, this review offers a comprehensive and dense roadmap of the
overwhelming challenges, techniques and opportunities of AI research in finance
over the past decades. The landscapes and challenges of financial businesses
and data are firstly outlined, followed by a comprehensive categorization and a
dense overview of the decades of AI research in finance. We then structure and
illustrate the data-driven analytics and learning of financial businesses and
data. The comparison, criticism and discussion of classic vs. modern AI
techniques for finance are followed. Lastly, open issues and opportunities
address future AI-empowered finance and finance-motivated AI research.

    

### [[2107.09055] Stock price prediction using BERT and GAN](http://arxiv.org/abs/2107.09055)


  The stock market has been a popular topic of interest in the recent past. The
growth in the inflation rate has compelled people to invest in the stock and
commodity markets and other areas rather than saving. Further, the ability of
Deep Learning models to make predictions on the time series data has been
proven time and again. Technical analysis on the stock market with the help of
technical indicators has been the most common practice among traders and
investors. One more aspect is the sentiment analysis - the emotion of the
investors that shows the willingness to invest. A variety of techniques have
been used by people around the globe involving basic Machine Learning and
Neural Networks. Ranging from the basic linear regression to the advanced
neural networks people have experimented with all possible techniques to
predict the stock market. It's evident from recent events how news and
headlines affect the stock markets and cryptocurrencies. This paper proposes an
ensemble of state-of-the-art methods for predicting stock prices. Firstly
sentiment analysis of the news and the headlines for the company Apple Inc,
listed on the NASDAQ is performed using a version of BERT, which is a
pre-trained transformer model by Google for Natural Language Processing (NLP).
Afterward, a Generative Adversarial Network (GAN) predicts the stock price for
Apple Inc using the technical indicators, stock indexes of various countries,
some commodities, and historical prices along with the sentiment scores.
Comparison is done with baseline models like - Long Short Term Memory (LSTM),
Gated Recurrent Units (GRU), vanilla GAN, and Auto-Regressive Integrated Moving
Average (ARIMA) model.

    

### [[2107.09060] LAPNet: Non-rigid Registration derived in k-space for Magnetic Resonance Imaging](http://arxiv.org/abs/2107.09060)


  Physiological motion, such as cardiac and respiratory motion, during Magnetic
Resonance (MR) image acquisition can cause image artifacts. Motion correction
techniques have been proposed to compensate for these types of motion during
thoracic scans, relying on accurate motion estimation from undersampled
motion-resolved reconstruction. A particular interest and challenge lie in the
derivation of reliable non-rigid motion fields from the undersampled
motion-resolved data. Motion estimation is usually formulated in image space
via diffusion, parametric-spline, or optical flow methods. However, image-based
registration can be impaired by remaining aliasing artifacts due to the
undersampled motion-resolved reconstruction. In this work, we describe a
formalism to perform non-rigid registration directly in the sampled Fourier
space, i.e. k-space. We propose a deep-learning based approach to perform fast
and accurate non-rigid registration from the undersampled k-space data. The
basic working principle originates from the Local All-Pass (LAP) technique, a
recently introduced optical flow-based registration. The proposed LAPNet is
compared against traditional and deep learning image-based registrations and
tested on fully-sampled and highly-accelerated (with two undersampling
strategies) 3D respiratory motion-resolved MR images in a cohort of 40 patients
with suspected liver or lung metastases and 25 healthy subjects. The proposed
LAPNet provided consistent and superior performance to image-based approaches
throughout different sampling trajectories and acceleration factors.

    

### [[2107.09070] Dim but not entirely dark: Extracting the Galactic Center Excess' source-count distribution with neural nets](http://arxiv.org/abs/2107.09070)


  The two leading hypotheses for the Galactic Center Excess (GCE) in the
$\textit{Fermi}$ data are an unresolved population of faint millisecond pulsars
(MSPs) and dark-matter (DM) annihilation. The dichotomy between these
explanations is typically reflected by modeling them as two separate emission
components. However, point-sources (PSs) such as MSPs become statistically
degenerate with smooth Poisson emission in the ultra-faint limit (formally
where each source is expected to contribute much less than one photon on
average), leading to an ambiguity that can render questions such as whether the
emission is PS-like or Poissonian in nature ill-defined. We present a
conceptually new approach that describes the PS and Poisson emission in a
unified manner and only afterwards derives constraints on the Poissonian
component from the so obtained results. For the implementation of this
approach, we leverage deep learning techniques, centered around a neural
network-based method for histogram regression that expresses uncertainties in
terms of quantiles. We demonstrate that our method is robust against a number
of systematics that have plagued previous approaches, in particular DM / PS
misattribution. In the $\textit{Fermi}$ data, we find a faint GCE described by
a median source-count distribution (SCD) peaked at a flux of $\sim4 \times
10^{-11} \ \text{counts} \ \text{cm}^{-2} \ \text{s}^{-1}$ (corresponding to
$\sim3 - 4$ expected counts per PS), which would require $N \sim
\mathcal{O}(10^4)$ sources to explain the entire excess (median value $N =
\text{29,300}$ across the sky). Although faint, this SCD allows us to derive
the constraint $\eta_P \leq 66\%$ for the Poissonian fraction of the GCE flux
$\eta_P$ at 95% confidence, suggesting that a substantial amount of the GCE
flux is due to PSs.

    

### [[2107.09078] Sample Complexity of Learning Quantum Circuits](http://arxiv.org/abs/2107.09078)


  Quantum computers hold unprecedented potentials for machine learning
applications. Here, we prove that physical quantum circuits are PAC (probably
approximately correct) learnable on a quantum computer via empirical risk
minimization: to learn a quantum circuit with at most $n^c$ gates and each gate
acting on a constant number of qubits, the sample complexity is bounded by
$\tilde{O}(n^{c+1})$. In particular, we explicitly construct a family of
variational quantum circuits with $O(n^{c+1})$ elementary gates arranged in a
fixed pattern, which can represent all physical quantum circuits consisting of
at most $n^c$ elementary gates. Our results provide a valuable guide for
quantum machine learning in both theory and experiment.

    

### [[2107.09082] Reconstruction of the Density Power Spectrum from Quasar Spectra using Machine Learning](http://arxiv.org/abs/2107.09082)


  We describe a novel end-to-end approach using Machine Learning to reconstruct
the power spectrum of cosmological density perturbations at high redshift from
observed quasar spectra. State-of-the-art cosmological simulations of structure
formation are used to generate a large synthetic dataset of line-of-sight
absorption spectra paired with 1-dimensional fluid quantities along the same
line-of-sight, such as the total density of matter and the density of neutral
atomic hydrogen. With this dataset, we build a series of data-driven models to
predict the power spectrum of total matter density. We are able to produce
models which yield reconstruction to accuracy of about 1% for wavelengths $k
\leq 2 h Mpc^{-1}$, while the error increases at larger $k$. We show the size
of data sample required to reach a particular error rate, giving a sense of how
much data is necessary to reach a desired accuracy. This work provides a
foundation for developing methods to analyse very large upcoming datasets with
the next-generation observational facilities.

    

### [[2107.09086] DPNNet-2.0 Part I: Finding hidden planets from simulated images of protoplanetary disk gaps](http://arxiv.org/abs/2107.09086)


  The observed sub-structures, like annular gaps, in dust emissions from
protoplanetary disk, are often interpreted as signatures of embedded planets.
Fitting a model of planetary gaps to these observed features using customized
simulations or empirical relations can reveal the characteristics of the hidden
planets. However, customized fitting is often impractical owing to the
increasing sample size and the complexity of disk-planet interaction. In this
paper we introduce the architecture of DPNNet-2.0, second in the series after
DPNNet \citep{aud20}, designed using a Convolutional Neural Network ( CNN, here
specifically ResNet50) for predicting exoplanet masses directly from simulated
images of protoplanetary disks hosting a single planet. DPNNet-2.0 additionally
consists of a multi-input framework that uses both a CNN and multi-layer
perceptron (a class of artificial neural network) for processing image and disk
parameters simultaneously. This enables DPNNet-2.0 to be trained using images
directly, with the added option of considering disk parameters (disk
viscosities, disk temperatures, disk surface density profiles, dust abundances,
and particle Stokes numbers) generated from disk-planet hydrodynamic
simulations as inputs. This work provides the required framework and is the
first step towards the use of computer vision (implementing CNN) to directly
extract mass of an exoplanet from planetary gaps observed in dust-surface
density maps by telescopes such as the Atacama Large (sub-)Millimeter Array.

    

### [[2107.09088] Reward-Weighted Regression Converges to a Global Optimum](http://arxiv.org/abs/2107.09088)


  Reward-Weighted Regression (RWR) belongs to a family of widely known
iterative Reinforcement Learning algorithms based on the
Expectation-Maximization framework. In this family, learning at each iteration
consists of sampling a batch of trajectories using the current policy and
fitting a new policy to maximize a return-weighted log-likelihood of actions.
Although RWR is known to yield monotonic improvement of the policy under
certain circumstances, whether and under which conditions RWR converges to the
optimal policy have remained open questions. In this paper, we provide for the
first time a proof that RWR converges to a global optimum when no function
approximation is used.

    

### [[2107.09091] Support Recovery in Universal One-bit Compressed Sensing](http://arxiv.org/abs/2107.09091)


  One-bit compressed sensing (1bCS) is an extreme-quantized signal acquisition
method that has been widely studied in the past decade. In 1bCS, linear samples
of a high dimensional signal are quantized to only one bit per sample (sign of
the measurement). Assuming the original signal vector to be sparse, existing
results either aim to find the support of the vector, or approximate the signal
within an $\epsilon$-ball. The focus of this paper is support recovery, which
often also computationally facilitates approximate signal recovery. A universal
measurement matrix for 1bCS refers to one set of measurements that work for all
sparse signals. With universality, it is known that $\tilde{\Theta}(k^2)$ 1bCS
measurements are necessary and sufficient for support recovery (where $k$
denotes the sparsity). In this work, we show that it is possible to universally
recover the support with a small number of false positives with
$\tilde{O}(k^{3/2})$ measurements. If the dynamic range of the signal vector is
known, then with a different technique, this result can be improved to only
$\tilde{O}(k)$ measurements. Further results on support recovery are also
provided.

    

### [[2107.09095] A New Clustering-Based Technique for the Acceleration of Deep Convolutional Networks](http://arxiv.org/abs/2107.09095)


  Deep learning and especially the use of Deep Neural Networks (DNNs) provides
impressive results in various regression and classification tasks. However, to
achieve these results, there is a high demand for computing and storing
resources. This becomes problematic when, for instance, real-time, mobile
applications are considered, in which the involved (embedded) devices have
limited resources. A common way of addressing this problem is to transform the
original large pre-trained networks into new smaller models, by utilizing Model
Compression and Acceleration (MCA) techniques. Within the MCA framework, we
propose a clustering-based approach that is able to increase the number of
employed centroids/representatives, while at the same time, have an
acceleration gain compared to conventional, $k$-means based approaches. This is
achieved by imposing a special structure to the employed representatives, which
is enabled by the particularities of the problem at hand. Moreover, the
theoretical acceleration gains are presented and the key system
hyper-parameters that affect that gain, are identified. Extensive evaluation
studies carried out using various state-of-the-art DNN models trained in image
classification, validate the superiority of the proposed method as compared for
its use in MCA tasks.

    

### [[2107.09099] Token-Level Supervised Contrastive Learning for Punctuation Restoration](http://arxiv.org/abs/2107.09099)


  Punctuation is critical in understanding natural language text. Currently,
most automatic speech recognition (ASR) systems do not generate punctuation,
which affects the performance of downstream tasks, such as intent detection and
slot filling. This gives rise to the need for punctuation restoration. Recent
work in punctuation restoration heavily utilizes pre-trained language models
without considering data imbalance when predicting punctuation classes. In this
work, we address this problem by proposing a token-level supervised contrastive
learning method that aims at maximizing the distance of representation of
different punctuation marks in the embedding space. The result shows that
training with token-level supervised contrastive learning obtains up to 3.2%
absolute F1 improvement on the test set.

    

### [[2107.09101] Accelerating deep neural networks for efficient scene understanding in automotive cyber-physical systems](http://arxiv.org/abs/2107.09101)


  Automotive Cyber-Physical Systems (ACPS) have attracted a significant amount
of interest in the past few decades, while one of the most critical operations
in these systems is the perception of the environment. Deep learning and,
especially, the use of Deep Neural Networks (DNNs) provides impressive results
in analyzing and understanding complex and dynamic scenes from visual data. The
prediction horizons for those perception systems are very short and inference
must often be performed in real time, stressing the need of transforming the
original large pre-trained networks into new smaller models, by utilizing Model
Compression and Acceleration (MCA) techniques. Our goal in this work is to
investigate best practices for appropriately applying novel weight sharing
techniques, optimizing the available variables and the training procedures
towards the significant acceleration of widely adopted DNNs. Extensive
evaluation studies carried out using various state-of-the-art DNN models in
object detection and tracking experiments, provide details about the type of
errors that manifest after the application of weight sharing techniques,
resulting in significant acceleration gains with negligible accuracy losses.

    

### [[2107.09106] Separating Skills and Concepts for Novel Visual Question Answering](http://arxiv.org/abs/2107.09106)


  Generalization to out-of-distribution data has been a problem for Visual
Question Answering (VQA) models. To measure generalization to novel questions,
we propose to separate them into "skills" and "concepts". "Skills" are visual
tasks, such as counting or attribute recognition, and are applied to "concepts"
mentioned in the question, such as objects and people. VQA methods should be
able to compose skills and concepts in novel ways, regardless of whether the
specific composition has been seen in training, yet we demonstrate that
existing models have much to improve upon towards handling new compositions. We
present a novel method for learning to compose skills and concepts that
separates these two factors implicitly within a model by learning grounded
concept representations and disentangling the encoding of skills from that of
concepts. We enforce these properties with a novel contrastive learning
procedure that does not rely on external annotations and can be learned from
unlabeled image-question pairs. Experiments demonstrate the effectiveness of
our approach for improving compositional and grounding performance.

    

### [[2107.09110] OnlineSTL: Scaling Time Series Decomposition by 100x](http://arxiv.org/abs/2107.09110)


  Decomposing a complex time series into trend, seasonality, and remainder
components is an important primitive that facilitates time series anomaly
detection, change point detection and forecasting. Although numerous batch
algorithms are known for time series decomposition, none operate well in an
online scalable setting where high throughput and real-time response are
paramount. In this paper, we propose OnlineSTL, a novel online algorithm for
time series decomposition which solves the scalability problem and is deployed
for real-time metrics monitoring on high resolution, high ingest rate data.
Experiments on different synthetic and real world time series datasets
demonstrate that OnlineSTL achieves orders of magnitude speedups while
maintaining quality of decomposition.

    

### [[2107.09118] Confidence Aware Neural Networks for Skin Cancer Detection](http://arxiv.org/abs/2107.09118)


  Deep learning (DL) models have received particular attention in medical
imaging due to their promising pattern recognition capabilities. However, Deep
Neural Networks (DNNs) require a huge amount of data, and because of the lack
of sufficient data in this field, transfer learning can be a great solution.
DNNs used for disease diagnosis meticulously concentrate on improving the
accuracy of predictions without providing a figure about their confidence of
predictions. Knowing how much a DNN model is confident in a computer-aided
diagnosis model is necessary for gaining clinicians' confidence and trust in
DL-based solutions. To address this issue, this work presents three different
methods for quantifying uncertainties for skin cancer detection from images. It
also comprehensively evaluates and compares performance of these DNNs using
novel uncertainty-related metrics. The obtained results reveal that the
predictive uncertainty estimation methods are capable of flagging risky and
erroneous predictions with a high uncertainty estimate. We also demonstrate
that ensemble approaches are more reliable in capturing uncertainties through
inference.

    

### [[2107.09123] Latency-Memory Optimized Splitting of Convolution Neural Networks for Resource Constrained Edge Devices](http://arxiv.org/abs/2107.09123)


  With the increasing reliance of users on smart devices, bringing essential
computation at the edge has become a crucial requirement for any type of
business. Many such computations utilize Convolution Neural Networks (CNNs) to
perform AI tasks, having high resource and computation requirements, that are
infeasible for edge devices. Splitting the CNN architecture to perform part of
the computation on edge and remaining on the cloud is an area of research that
has seen increasing interest in the field. In this paper, we assert that
running CNNs between an edge device and the cloud is synonymous to solving a
resource-constrained optimization problem that minimizes the latency and
maximizes resource utilization at the edge. We formulate a multi-objective
optimization problem and propose the LMOS algorithm to achieve a Pareto
efficient solution. Experiments done on real-world edge devices show that, LMOS
ensures feasible execution of different CNN models at the edge and also
improves upon existing state-of-the-art approaches.

    

### [[2107.09130] GNN4IP: Graph Neural Network for Hardware Intellectual Property Piracy Detection](http://arxiv.org/abs/2107.09130)


  Aggressive time-to-market constraints and enormous hardware design and
fabrication costs have pushed the semiconductor industry toward hardware
Intellectual Properties (IP) core design. However, the globalization of the
integrated circuits (IC) supply chain exposes IP providers to theft and illegal
redistribution of IPs. Watermarking and fingerprinting are proposed to detect
IP piracy. Nevertheless, they come with additional hardware overhead and cannot
guarantee IP security as advanced attacks are reported to remove the watermark,
forge, or bypass it. In this work, we propose a novel methodology, GNN4IP, to
assess similarities between circuits and detect IP piracy. We model the
hardware design as a graph and construct a graph neural network model to learn
its behavior using the comprehensive dataset of register transfer level codes
and gate-level netlists that we have gathered. GNN4IP detects IP piracy with
96% accuracy in our dataset and recognizes the original IP in its obfuscated
version with 100% accuracy.

    

### [[2107.09133] Rethinking the limiting dynamics of SGD: modified loss, phase space oscillations, and anomalous diffusion](http://arxiv.org/abs/2107.09133)


  In this work we explore the limiting dynamics of deep neural networks trained
with stochastic gradient descent (SGD). We find empirically that long after
performance has converged, networks continue to move through parameter space by
a process of anomalous diffusion in which distance travelled grows as a power
law in the number of gradient updates with a nontrivial exponent. We reveal an
intricate interaction between the hyperparameters of optimization, the
structure in the gradient noise, and the Hessian matrix at the end of training
that explains this anomalous diffusion. To build this understanding, we first
derive a continuous-time model for SGD with finite learning rates and batch
sizes as an underdamped Langevin equation. We study this equation in the
setting of linear regression, where we can derive exact, analytic expressions
for the phase space dynamics of the parameters and their instantaneous
velocities from initialization to stationarity. Using the Fokker-Planck
equation, we show that the key ingredient driving these dynamics is not the
original training loss, but rather the combination of a modified loss, which
implicitly regularizes the velocity, and probability currents, which cause
oscillations in phase space. We identify qualitative and quantitative
predictions of this theory in the dynamics of a ResNet-18 model trained on
ImageNet. Through the lens of statistical physics, we uncover a mechanistic
origin for the anomalous limiting dynamics of deep neural networks trained with
SGD.

    

### [[2107.09139] Constrained Policy Gradient Method for Safe and Fast Reinforcement Learning: a Neural Tangent Kernel Based Approach](http://arxiv.org/abs/2107.09139)


  This paper presents a constrained policy gradient algorithm. We introduce
constraints for safe learning with the following steps. First, learning is
slowed down (lazy learning) so that the episodic policy change can be computed
with the help of the policy gradient theorem and the neural tangent kernel.
Then, this enables us the evaluation of the policy at arbitrary states too. In
the same spirit, learning can be guided, ensuring safety via augmenting episode
batches with states where the desired action probabilities are prescribed.
Finally, exogenous discounted sum of future rewards (returns) can be computed
at these specific state-action pairs such that the policy network satisfies
constraints. Computing the returns is based on solving a system of linear
equations (equality constraints) or a constrained quadratic program (inequality
constraints). Simulation results suggest that adding constraints (external
information) to the learning can improve learning in terms of speed and safety
reasonably if constraints are appropriately selected. The efficiency of the
constrained learning was demonstrated with a shallow and wide ReLU network in
the Cartpole and Lunar Lander OpenAI gym environments. The main novelty of the
paper is giving a practical use of the neural tangent kernel in reinforcement
learning.

    

### [[2107.09142] Sequence-to-Sequence Piano Transcription with Transformers](http://arxiv.org/abs/2107.09142)


  Automatic Music Transcription has seen significant progress in recent years
by training custom deep neural networks on large datasets. However, these
models have required extensive domain-specific design of network architectures,
input/output representations, and complex decoding schemes. In this work, we
show that equivalent performance can be achieved using a generic
encoder-decoder Transformer with standard decoding methods. We demonstrate that
the model can learn to translate spectrogram inputs directly to MIDI-like
output events for several transcription tasks. This sequence-to-sequence
approach simplifies transcription by jointly modeling audio features and
language-like output dependencies, thus removing the need for task-specific
architectures. These results point toward possibilities for creating new Music
Information Retrieval models by focusing on dataset creation and labeling
rather than custom model design.

    

### [[2107.09144] Wave-Informed Matrix Factorization withGlobal Optimality Guarantees](http://arxiv.org/abs/2107.09144)


  With the recent success of representation learning methods, which includes
deep learning as a special case, there has been considerable interest in
developing representation learning techniques that can incorporate known
physical constraints into the learned representation. As one example, in many
applications that involve a signal propagating through physical media (e.g.,
optics, acoustics, fluid dynamics, etc), it is known that the dynamics of the
signal must satisfy constraints imposed by the wave equation. Here we propose a
matrix factorization technique that decomposes such signals into a sum of
components, where each component is regularized to ensure that it satisfies
wave equation constraints. Although our proposed formulation is non-convex, we
prove that our model can be efficiently solved to global optimality in
polynomial time. We demonstrate the benefits of our work by applications in
structural health monitoring, where prior work has attempted to solve this
problem using sparse dictionary learning approaches that do not come with any
theoretical guarantees regarding convergence to global optimality and employ
heuristics to capture desired physical constraints.

    

### [[2107.09145] Adaptive wavelet distillation from neural networks through interpretations](http://arxiv.org/abs/2107.09145)


  Recent deep-learning models have achieved impressive prediction performance,
but often sacrifice interpretability and computational efficiency.
Interpretability is crucial in many disciplines, such as science and medicine,
where models must be carefully vetted or where interpretation is the goal
itself. Moreover, interpretable models are concise and often yield
computational efficiency. Here, we propose adaptive wavelet distillation (AWD),
a method which aims to distill information from a trained neural network into a
wavelet transform. Specifically, AWD penalizes feature attributions of a neural
network in the wavelet domain to learn an effective multi-resolution wavelet
transform. The resulting model is highly predictive, concise, computationally
efficient, and has properties (such as a multi-scale structure) which make it
easy to interpret. In close collaboration with domain experts, we showcase how
AWD addresses challenges in two real-world settings: cosmological parameter
inference and molecular-partner prediction. In both cases, AWD yields a
scientifically interpretable and concise model which gives predictive
performance better than state-of-the-art neural networks. Moreover, AWD
identifies predictive features that are scientifically meaningful in the
context of respective domains. All code and models are released in a
full-fledged package available on Github
(this https URL).

    

### [[2107.09158] Improving exploration in policy gradient search: Application to symbolic optimization](http://arxiv.org/abs/2107.09158)


  Many machine learning strategies designed to automate mathematical tasks
leverage neural networks to search large combinatorial spaces of mathematical
symbols. In contrast to traditional evolutionary approaches, using a neural
network at the core of the search allows learning higher-level symbolic
patterns, providing an informed direction to guide the search. When no labeled
data is available, such networks can still be trained using reinforcement
learning. However, we demonstrate that this approach can suffer from an early
commitment phenomenon and from initialization bias, both of which limit
exploration. We present two exploration methods to tackle these issues,
building upon ideas of entropy regularization and distribution initialization.
We show that these techniques can improve the performance, increase sample
efficiency, and lower the complexity of solutions for the task of symbolic
regression.

    

### [[2107.09170] DeepSocNav: Social Navigation by Imitating Human Behaviors](http://arxiv.org/abs/2107.09170)


  Current datasets to train social behaviors are usually borrowed from
surveillance applications that capture visual data from a bird's-eye
perspective. This leaves aside precious relationships and visual cues that
could be captured through a first-person view of a scene. In this work, we
propose a strategy to exploit the power of current game engines, such as Unity,
to transform pre-existing bird's-eye view datasets into a first-person view, in
particular, a depth view. Using this strategy, we are able to generate large
volumes of synthetic data that can be used to pre-train a social navigation
model. To test our ideas, we present DeepSocNav, a deep learning based model
that takes advantage of the proposed approach to generate synthetic data.
Furthermore, DeepSocNav includes a self-supervised strategy that is included as
an auxiliary task. This consists of predicting the next depth frame that the
agent will face. Our experiments show the benefits of the proposed model that
is able to outperform relevant baselines in terms of social navigation scores.

    

### [[2107.09182] Incorporating domain knowledge into neural-guided search](http://arxiv.org/abs/2107.09182)


  Many AutoML problems involve optimizing discrete objects under a black-box
reward. Neural-guided search provides a flexible means of searching these
combinatorial spaces using an autoregressive recurrent neural network. A major
benefit of this approach is that builds up objects sequentially--this provides
an opportunity to incorporate domain knowledge into the search by directly
modifying the logits emitted during sampling. In this work, we formalize a
framework for incorporating such in situ priors and constraints into
neural-guided search, and provide sufficient conditions for enforcing
constraints. We integrate several priors and constraints from existing works
into this framework, propose several new ones, and demonstrate their efficacy
in informing the task of symbolic regression.

    

### [[2107.09194] Can we globally optimize cross-validation loss? Quasiconvexity in ridge regression](http://arxiv.org/abs/2107.09194)


  Models like LASSO and ridge regression are extensively used in practice due
to their interpretability, ease of use, and strong theoretical guarantees.
Cross-validation (CV) is widely used for hyperparameter tuning in these models,
but do practical optimization methods minimize the true out-of-sample loss? A
recent line of research promises to show that the optimum of the CV loss
matches the optimum of the out-of-sample loss (possibly after simple
corrections). It remains to show how tractable it is to minimize the CV loss.
In the present paper, we show that, in the case of ridge regression, the CV
loss may fail to be quasiconvex and thus may have multiple local optima. We can
guarantee that the CV loss is quasiconvex in at least one case: when the
spectrum of the covariate matrix is nearly flat and the noise in the observed
responses is not too high. More generally, we show that quasiconvexity status
is independent of many properties of the observed data (response norm,
covariate-matrix right singular vectors and singular-value scaling) and has a
complex dependence on the few that remain. We empirically confirm our theory
using simulated experiments.

    

### [[2107.09200] A quantum algorithm for training wide and deep classical neural networks](http://arxiv.org/abs/2107.09200)


  Given the success of deep learning in classical machine learning, quantum
algorithms for traditional neural network architectures may provide one of the
most promising settings for quantum machine learning. Considering a
fully-connected feedforward neural network, we show that conditions amenable to
classical trainability via gradient descent coincide with those necessary for
efficiently solving quantum linear systems. We propose a quantum algorithm to
approximately train a wide and deep neural network up to $O(1/n)$ error for a
training set of size $n$ by performing sparse matrix inversion in $O(\log n)$
time. To achieve an end-to-end exponential speedup over gradient descent, the
data distribution must permit efficient state preparation and readout. We
numerically demonstrate that the MNIST image dataset satisfies such conditions;
moreover, the quantum algorithm matches the accuracy of the fully-connected
network. Beyond the proven architecture, we provide empirical evidence for
$O(\log n)$ training of a convolutional neural network with pooling.

    

### [[2107.09202] Compressing Multisets with Large Alphabets](http://arxiv.org/abs/2107.09202)


  Current methods that optimally compress multisets are not suitable for
high-dimensional symbols, as their compute time scales linearly with alphabet
size. Compressing a multiset as an ordered sequence with off-the-shelf codecs
is computationally more efficient, but has a sub-optimal compression rate, as
bits are wasted encoding the order between symbols. We present a method that
can recover those bits, assuming symbols are i.i.d., at the cost of an
additional $\mathcal{O}(|\mathcal{M}|\log M)$ in average time complexity, where
$|\mathcal{M}|$ and $M$ are the total and unique number of symbols in the
multiset. Our method is compatible with any prefix-free code. Experiments show
that, when paired with efficient coders, our method can efficiently compress
high-dimensional sources such as multisets of images and collections of JSON
files.

    

### [[2107.09203] Wide and Deep Graph Neural Network with Distributed Online Learning](http://arxiv.org/abs/2107.09203)


  Graph neural networks (GNNs) are naturally distributed architectures for
learning representations from network data. This renders them suitable
candidates for decentralized tasks. In these scenarios, the underlying graph
often changes with time due to link failures or topology variations, creating a
mismatch between the graphs on which GNNs were trained and the ones on which
they are tested. Online learning can be leveraged to retrain GNNs at testing
time to overcome this issue. However, most online algorithms are centralized
and usually offer guarantees only on convex problems, which GNNs rarely lead
to. This paper develops the Wide and Deep GNN (WD-GNN), a novel architecture
that can be updated with distributed online learning mechanisms. The WD-GNN
consists of two components: the wide part is a linear graph filter and the deep
part is a nonlinear GNN. At training time, the joint wide and deep architecture
learns nonlinear representations from data. At testing time, the wide, linear
part is retrained, while the deep, nonlinear one remains fixed. This often
leads to a convex formulation. We further propose a distributed online learning
algorithm that can be implemented in a decentralized setting. We also show the
stability of the WD-GNN to changes of the underlying graph and analyze the
convergence of the proposed online learning procedure. Experiments on movie
recommendation, source localization and robot swarm control corroborate
theoretical findings and show the potential of the WD-GNN for distributed
online learning.

    

### [[2107.09204] A Comparison of Supervised and Unsupervised Deep Learning Methods for Anomaly Detection in Images](http://arxiv.org/abs/2107.09204)


  Anomaly detection in images plays a significant role for many applications
across all industries, such as disease diagnosis in healthcare or quality
assurance in manufacturing. Manual inspection of images, when extended over a
monotonously repetitive period of time is very time consuming and can lead to
anomalies being overlooked.Artificial neural networks have proven themselves
very successful on simple, repetitive tasks, in some cases even outperforming
humans. Therefore, in this paper we investigate different methods of deep
learning, including supervised and unsupervised learning, for anomaly detection
applied to a quality assurance use case. We utilize the MVTec anomaly dataset
and develop three different models, a CNN for supervised anomaly detection,
KD-CAE for autoencoder anomaly detection, NI-CAE for noise induced anomaly
detection and a DCGAN for generating reconstructed images. By experiments, we
found that KD-CAE performs better on the anomaly datasets compared to CNN and
NI-CAE, with NI-CAE performing the best on the Transistor dataset. We also
implemented a DCGAN for the creation of new training data but due to
computational limitation and lack of extrapolating the mechanics of AnoGAN, we
restricted ourselves just to the generation of GAN based images. We conclude
that unsupervised methods are more powerful for anomaly detection in images,
especially in a setting where only a small amount of anomalous data is
available, or the data is unlabeled.

    

### [[2107.09207] Asymptotic Escape of Spurious Critical Points on the Low-rank Matrix Manifold](http://arxiv.org/abs/2107.09207)


  We show that the Riemannian gradient descent algorithm on the low-rank matrix
manifold almost surely escapes some spurious critical points on the boundary of
the manifold. Given that the low-rank matrix manifold is an incomplete set,
this result is the first to overcome this difficulty and partially justify the
global use of the Riemannian gradient descent on the manifold. The spurious
critical points are some rank-deficient matrices that capture only part of the
SVD components of the ground truth. They exhibit very singular behavior and
evade the classical analysis of strict saddle points. We show that using the
dynamical low-rank approximation and a rescaled gradient flow, some of the
spurious critical points can be converted to classical strict saddle points,
which leads to the desired result. Numerical experiments are provided to
support our theoretical findings.

    

### [[2107.09208] Music Tempo Estimation via Neural Networks -- A Comparative Analysis](http://arxiv.org/abs/2107.09208)


  This paper presents a comparative analysis on two artificial neural networks
(with different architectures) for the task of tempo estimation. For this
purpose, it also proposes the modeling, training and evaluation of a B-RNN
(Bidirectional Recurrent Neural Network) model capable of estimating tempo in
bpm (beats per minutes) of musical pieces, without using external auxiliary
modules. An extensive database (12,550 pieces in total) was curated to conduct
a quantitative and qualitative analysis over the experiment. Percussion-only
tracks were also included in the dataset. The performance of the B-RNN is
compared to that of state-of-the-art models. For further comparison, a
state-of-the-art CNN was also retrained with the same datasets used for the
B-RNN training. Evaluation results for each model and datasets are presented
and discussed, as well as observations and ideas for future research. Tempo
estimation was more accurate for the percussion only dataset, suggesting that
the estimation can be more accurate for percussion-only tracks, although
further experiments (with more of such datasets) should be made to gather
stronger evidence.

    

### [[2107.09217] Heterogeneous network-based drug repurposing for COVID-19](http://arxiv.org/abs/2107.09217)


  The Corona Virus Disease 2019 (COVID-19) belongs to human coronaviruses
(HCoVs), which spreads rapidly around the world. Compared with new drug
development, drug repurposing may be the best shortcut for treating COVID-19.
Therefore, we constructed a comprehensive heterogeneous network based on the
HCoVs-related target proteins and use the previously proposed deepDTnet, to
discover potential drug candidates for COVID-19. We obtain high performance in
predicting the possible drugs effective for COVID-19 related proteins. In
summary, this work utilizes a powerful heterogeneous network-based deep
learning method, which may be beneficial to quickly identify candidate
repurposable drugs toward future clinical trials for COVID-19. The code and
data are available at this https URL.

    

### [[2107.09224] Evaluating Probabilistic Inference in Deep Learning: Beyond Marginal Predictions](http://arxiv.org/abs/2107.09224)


  A fundamental challenge for any intelligent system is prediction: given some
inputs $X_1,..,X_\tau$ can you predict outcomes $Y_1,.., Y_\tau$. The KL
divergence $\mathbf{d}_{\mathrm{KL}}$ provides a natural measure of prediction
quality, but the majority of deep learning research looks only at the marginal
predictions per input $X_t$. In this technical report we propose a scoring rule
$\mathbf{d}_{\mathrm{KL}}^\tau$, parameterized by $\tau \in \mathcal{N}$ that
evaluates the joint predictions at $\tau$ inputs simultaneously. We show that
the commonly-used $\tau=1$ can be insufficient to drive good decisions in many
settings of interest. We also show that, as $\tau$ grows, performing well
according to $\mathbf{d}_{\mathrm{KL}}^\tau$ recovers universal guarantees for
any possible decision. Finally, we provide problem-dependent guidance on the
scale of $\tau$ for which our score provides sufficient guarantees for good
performance.

    

### [[2107.09232] Reinforcement learning autonomously identifying the source of errors for agents in a group mission](http://arxiv.org/abs/2107.09232)


  When agents are swarmed to carry out a mission, there is often a sudden
failure of some of the agents observed from the command base. It is generally
difficult to distinguish whether the failure is caused by actuators
(hypothesis, $h_a$) or sensors (hypothesis, $h_s$) solely by the communication
between the command base and the concerning agent. By making a collision to the
agent by another, we would be able to distinguish which hypothesis is likely:
For $h_a$, we expect to detect corresponding displacements while for $h_a$ we
do not. Such swarm strategies to grasp the situation are preferably to be
generated autonomously by artificial intelligence (AI). Preferable actions
($e.g.$, the collision) for the distinction would be those maximizing the
difference between the expected behaviors for each hypothesis, as a value
function. Such actions exist, however, only very sparsely in the whole
possibilities, for which the conventional search based on gradient methods does
not make sense. Instead, we have successfully applied the reinforcement
learning technique, achieving the maximization of such a sparse value function.
The machine learning actually concluded autonomously the colliding action to
distinguish the hypothesises. Getting recognized an agent with actuator error
by the action, the agents behave as if other ones want to assist the
malfunctioning one to achieve a given mission.

    

### [[2107.09234] Shared Interest: Large-Scale Visual Analysis of Model Behavior by Measuring Human-AI Alignment](http://arxiv.org/abs/2107.09234)


  Saliency methods -- techniques to identify the importance of input features
on a model's output -- are a common first step in understanding neural network
behavior. However, interpreting saliency requires tedious manual inspection to
identify and aggregate patterns in model behavior, resulting in ad hoc or
cherry-picked analysis. To address these concerns, we present Shared Interest:
a set of metrics for comparing saliency with human annotated ground truths. By
providing quantitative descriptors, Shared Interest allows ranking, sorting,
and aggregation of inputs thereby facilitating large-scale systematic analysis
of model behavior. We use Shared Interest to identify eight recurring patterns
in model behavior including focusing on a sufficient subset of ground truth
features or being distracted by contextual features. Working with
representative real-world users, we show how Shared Interest can be used to
rapidly develop or lose trust in a model's reliability, uncover issues that are
missed in manual analyses, and enable interactive probing of model behavior.

    

### [[2107.09240] Generative Video Transformer: Can Objects be the Words?](http://arxiv.org/abs/2107.09240)


  Transformers have been successful for many natural language processing tasks.
However, applying transformers to the video domain for tasks such as long-term
video generation and scene understanding has remained elusive due to the high
computational complexity and the lack of natural tokenization. In this paper,
we propose the Object-Centric Video Transformer (OCVT) which utilizes an
object-centric approach for decomposing scenes into tokens suitable for use in
a generative video transformer. By factoring the video into objects, our fully
unsupervised model is able to learn complex spatio-temporal dynamics of
multiple interacting objects in a scene and generate future frames of the
video. Our model is also significantly more memory-efficient than pixel-based
models and thus able to train on videos of length up to 70 frames with a single
48GB GPU. We compare our model with previous RNN-based approaches as well as
other possible video transformer baselines. We demonstrate OCVT performs well
when compared to baselines in generating future frames. OCVT also develops
useful representations for video reasoning, achieving start-of-the-art
performance on the CATER task.

    

### [[2107.09251] OPAL: Offline Preference-Based Apprenticeship Learning](http://arxiv.org/abs/2107.09251)


  We study how an offline dataset of prior (possibly random) experience can be
used to address two challenges that autonomous systems face when they endeavor
to learn from, adapt to, and collaborate with humans : (1) identifying the
human's intent and (2) safely optimizing the autonomous system's behavior to
achieve this inferred intent. First, we use the offline dataset to efficiently
infer the human's reward function via pool-based active preference learning.
Second, given this learned reward function, we perform offline reinforcement
learning to optimize a policy based on the inferred human intent. Crucially,
our proposed approach does not require actual physical rollouts or an accurate
simulator for either the reward learning or policy optimization steps, enabling
both safe and efficient apprenticeship learning. We identify and evaluate our
approach on a subset of existing offline RL benchmarks that are well suited for
offline reward learning and also evaluate extensions of these benchmarks which
allow more open-ended behaviors. Our experiments show that offline
preference-based reward learning followed by offline reinforcement learning
enables efficient and high-performing policies, while only requiring small
numbers of preference queries. Videos available at
this https URL.

    

### [[2107.09256] Active operator inference for learning low-dimensional dynamical-system models from noisy data](http://arxiv.org/abs/2107.09256)


  Noise poses a challenge for learning dynamical-system models because already
small variations can distort the dynamics described by trajectory data. This
work builds on operator inference from scientific machine learning to infer
low-dimensional models from high-dimensional state trajectories polluted with
noise. The presented analysis shows that, under certain conditions, the
inferred operators are unbiased estimators of the well-studied projection-based
reduced operators from traditional model reduction. Furthermore, the connection
between operator inference and projection-based model reduction enables
bounding the mean-squared errors of predictions made with the learned models
with respect to traditional reduced models. The analysis also motivates an
active operator inference approach that judiciously samples high-dimensional
trajectories with the aim of achieving a low mean-squared error by reducing the
effect of noise. Numerical experiments with high-dimensional linear and
nonlinear state dynamics demonstrate that predictions obtained with active
operator inference have orders of magnitude lower mean-squared errors than
operator inference with traditional, equidistantly sampled trajectory data.

    

### [[2107.09262] FoleyGAN: Visually Guided Generative Adversarial Network-Based Synchronous Sound Generation in Silent Videos](http://arxiv.org/abs/2107.09262)


  Deep learning based visual to sound generation systems essentially need to be
developed particularly considering the synchronicity aspects of visual and
audio features with time. In this research we introduce a novel task of guiding
a class conditioned generative adversarial network with the temporal visual
information of a video input for visual to sound generation task adapting the
synchronicity traits between audio-visual modalities. Our proposed FoleyGAN
model is capable of conditioning action sequences of visual events leading
towards generating visually aligned realistic sound tracks. We expand our
previously proposed Automatic Foley dataset to train with FoleyGAN and evaluate
our synthesized sound through human survey that shows noteworthy (on average
81\%) audio-visual synchronicity performance. Our approach also outperforms in
statistical experiments compared with other baseline models and audio-visual
datasets.

    

### [[2107.09282] ReSSL: Relational Self-Supervised Learning with Weak Augmentation](http://arxiv.org/abs/2107.09282)


  Self-supervised Learning (SSL) including the mainstream contrastive learning
has achieved great success in learning visual representations without data
annotations. However, most of methods mainly focus on the instance level
information (\ie, the different augmented images of the same instance should
have the same feature or cluster into the same class), but there is a lack of
attention on the relationships between different instances. In this paper, we
introduced a novel SSL paradigm, which we term as relational self-supervised
learning (ReSSL) framework that learns representations by modeling the
relationship between different instances. Specifically, our proposed method
employs sharpened distribution of pairwise similarities among different
instances as \textit{relation} metric, which is thus utilized to match the
feature embeddings of different augmentations. Moreover, to boost the
performance, we argue that weak augmentations matter to represent a more
reliable relation, and leverage momentum strategy for practical efficiency.
Experimental results show that our proposed ReSSL significantly outperforms the
previous state-of-the-art algorithms in terms of both performance and training
efficiency. Code is available at \url{this https URL}.

    

### [[2107.09286] ByPE-VAE: Bayesian Pseudocoresets Exemplar VAE](http://arxiv.org/abs/2107.09286)


  Recent studies show that advanced priors play a major role in deep generative
models. Exemplar VAE, as a variant of VAE with an exemplar-based prior, has
achieved impressive results. However, due to the nature of model design, an
exemplar-based model usually requires vast amounts of data to participate in
training, which leads to huge computational complexity. To address this issue,
we propose Bayesian Pseudocoresets Exemplar VAE (ByPE-VAE), a new variant of
VAE with a prior based on Bayesian pseudocoreset. The proposed prior is
conditioned on a small-scale pseudocoreset rather than the whole dataset for
reducing the computational cost and avoiding overfitting. Simultaneously, we
obtain the optimal pseudocoreset via a stochastic optimization algorithm during
VAE training aiming to minimize the Kullback-Leibler divergence between the
prior based on the pseudocoreset and that based on the whole dataset.
Experimental results show that ByPE-VAE can achieve competitive improvements
over the state-of-the-art VAEs in the tasks of density estimation,
representation learning, and generative data augmentation. Particularly, on a
basic VAE architecture, ByPE-VAE is up to 3 times faster than Exemplar VAE
while almost holding the performance. Code is available at our supplementary
materials.

    

### [[2107.09301] A Bayesian Approach to Invariant Deep Neural Networks](http://arxiv.org/abs/2107.09301)


  We propose a novel Bayesian neural network architecture that can learn
invariances from data alone by inferring a posterior distribution over
different weight-sharing schemes. We show that our model outperforms other
non-invariant architectures, when trained on datasets that contain specific
invariances. The same holds true when no data augmentation is performed.

    

### [[2107.09305] Follow Your Path: a Progressive Method for Knowledge Distillation](http://arxiv.org/abs/2107.09305)


  Deep neural networks often have a huge number of parameters, which posts
challenges in deployment in application scenarios with limited memory and
computation capacity. Knowledge distillation is one approach to derive compact
models from bigger ones. However, it has been observed that a converged heavy
teacher model is strongly constrained for learning a compact student network
and could make the optimization subject to poor local optima. In this paper, we
propose ProKT, a new model-agnostic method by projecting the supervision
signals of a teacher model into the student's parameter space. Such projection
is implemented by decomposing the training objective into local intermediate
targets with an approximate mirror descent technique. The proposed method could
be less sensitive with the quirks during optimization which could result in a
better local optimum. Experiments on both image and text datasets show that our
proposed ProKT consistently achieves superior performance compared to other
existing knowledge distillation methods.

    

### [[2107.09309] LENS: Layer Distribution Enabled Neural Architecture Search in Edge-Cloud Hierarchies](http://arxiv.org/abs/2107.09309)


  Edge-Cloud hierarchical systems employing intelligence through Deep Neural
Networks (DNNs) endure the dilemma of workload distribution within them.
Previous solutions proposed to distribute workloads at runtime according to the
state of the surroundings, like the wireless conditions. However, such
conditions are usually overlooked at design time. This paper addresses this
issue for DNN architectural design by presenting a novel methodology, LENS,
which administers multi-objective Neural Architecture Search (NAS) for
two-tiered systems, where the performance objectives are refashioned to
consider the wireless communication parameters. From our experimental search
space, we demonstrate that LENS improves upon the traditional solution's Pareto
set by 76.47% and 75% with respect to the energy and latency metrics,
respectively.

    

### [[2107.09321] A Real-time Speaker Diarization System Based on Spatial Spectrum](http://arxiv.org/abs/2107.09321)


  In this paper we describe a speaker diarization system that enables
localization and identification of all speakers present in a conversation or
meeting. We propose a novel systematic approach to tackle several long-standing
challenges in speaker diarization tasks: (1) to segment and separate
overlapping speech from two speakers; (2) to estimate the number of speakers
when participants may enter or leave the conversation at any time; (3) to
provide accurate speaker identification on short text-independent utterances;
(4) to track down speakers movement during the conversation; (5) to detect
speaker change incidence real-time. First, a differential directional
microphone array-based approach is exploited to capture the target speakers'
voice in far-field adverse environment. Second, an online speaker-location
joint clustering approach is proposed to keep track of speaker location. Third,
an instant speaker number detector is developed to trigger the mechanism that
separates overlapped speech. The results suggest that our system effectively
incorporates spatial information and achieves significant gains.

    

### [[2107.09323] Transfer Learning for Credit Card Fraud Detection: A Journey from Research to Production](http://arxiv.org/abs/2107.09323)


  The dark face of digital commerce generalization is the increase of fraud
attempts. To prevent any type of attacks, state of the art fraud detection
systems are now embedding Machine Learning (ML) modules. The conception of such
modules is only communicated at the level of research and papers mostly focus
on results for isolated benchmark datasets and metrics. But research is only a
part of the journey, preceded by the right formulation of the business problem
and collection of data, and followed by a practical integration. In this paper,
we give a wider vision of the process, on a case study of transfer learning for
fraud detection, from business to research, and back to business.

    

### [[2107.09338] Kernel Selection for Stein Variational Gradient Descent](http://arxiv.org/abs/2107.09338)


  Stein variational gradient descent (SVGD) and its variants have shown
promising successes in approximate inference for complex distributions.
However, their empirical performance depends crucially on the choice of optimal
kernel. Unfortunately, RBF kernel with median heuristics is a common choice in
previous approaches which has been proved sub-optimal. Inspired by the paradigm
of multiple kernel learning, our solution to this issue is using a combination
of multiple kernels to approximate the optimal kernel instead of a single one
which may limit the performance and flexibility. To do so, we extend Kernelized
Stein Discrepancy (KSD) to its multiple kernel view called Multiple Kernelized
Stein Discrepancy (MKSD). Further, we leverage MKSD to construct a general
algorithm based on SVGD, which be called Multiple Kernel SVGD (MK-SVGD).
Besides, we automatically assign a weight to each kernel without any other
parameters. The proposed method not only gets rid of optimal kernel dependence
but also maintains computational effectiveness. Experiments on various tasks
and models show the effectiveness of our method.

    

### [[2107.09355] Approximation Theory of Convolutional Architectures for Time Series Modelling](http://arxiv.org/abs/2107.09355)


  We study the approximation properties of convolutional architectures applied
to time series modelling, which can be formulated mathematically as a
functional approximation problem. In the recurrent setting, recent results
reveal an intricate connection between approximation efficiency and memory
structures in the data generation process. In this paper, we derive parallel
results for convolutional architectures, with WaveNet being a prime example.
Our results reveal that in this new setting, approximation efficiency is not
only characterised by memory, but also additional fine structures in the target
relationship. This leads to a novel definition of spectrum-based regularity
that measures the complexity of temporal relationships under the convolutional
approximation scheme. These analyses provide a foundation to understand the
differences between architectural choices for time series modelling and can
give theoretically grounded guidance for practical applications.

    

### [[2107.09356] Toward Collaborative Reinforcement Learning Agents that Communicate Through Text-Based Natural Language](http://arxiv.org/abs/2107.09356)


  Communication between agents in collaborative multi-agent settings is in
general implicit or a direct data stream. This paper considers text-based
natural language as a novel form of communication between multiple agents
trained with reinforcement learning. This could be considered first steps
toward a truly autonomous communication without the need to define a limited
set of instructions, and natural collaboration between humans and robots.
Inspired by the game of Blind Leads, we propose an environment where one agent
uses natural language instructions to guide another through a maze. We test the
ability of reinforcement learning agents to effectively communicate through
discrete word-level symbols and show that the agents are able to sufficiently
communicate through natural language with a limited vocabulary. Although the
communication is not always perfect English, the agents are still able to
navigate the maze. We achieve a BLEU score of 0.85, which is an improvement of
0.61 over randomly generated sequences while maintaining a 100% maze completion
rate. This is a 3.5 times the performance of the random baseline using our
reference set.

    

### [[2107.09359] An Empirical Analysis of Measure-Valued Derivatives for Policy Gradients](http://arxiv.org/abs/2107.09359)


  Reinforcement learning methods for robotics are increasingly successful due
to the constant development of better policy gradient techniques. A precise
(low variance) and accurate (low bias) gradient estimator is crucial to face
increasingly complex tasks. Traditional policy gradient algorithms use the
likelihood-ratio trick, which is known to produce unbiased but high variance
estimates. More modern approaches exploit the reparametrization trick, which
gives lower variance gradient estimates but requires differentiable value
function approximators. In this work, we study a different type of stochastic
gradient estimator: the Measure-Valued Derivative. This estimator is unbiased,
has low variance, and can be used with differentiable and non-differentiable
function approximators. We empirically evaluate this estimator in the
actor-critic policy gradient setting and show that it can reach comparable
performance with methods based on the likelihood-ratio or reparametrization
tricks, both in low and high-dimensional action spaces.

    

### [[2107.09362] Protecting Semantic Segmentation Models by Using Block-wise Image Encryption with Secret Key from Unauthorized Access](http://arxiv.org/abs/2107.09362)


  Since production-level trained deep neural networks (DNNs) are of a great
business value, protecting such DNN models against copyright infringement and
unauthorized access is in a rising demand. However, conventional model
protection methods focused only the image classification task, and these
protection methods were never applied to semantic segmentation although it has
an increasing number of applications. In this paper, we propose to protect
semantic segmentation models from unauthorized access by utilizing block-wise
transformation with a secret key for the first time. Protected models are
trained by using transformed images. Experiment results show that the proposed
protection method allows rightful users with the correct key to access the
model to full capacity and deteriorate the performance for unauthorized users.
However, protected models slightly drop the segmentation performance compared
to non-protected models.

    

### [[2107.09366] Positive/Negative Approximate Multipliers for DNN Accelerators](http://arxiv.org/abs/2107.09366)


  Recent Deep Neural Networks (DNNs) managed to deliver superhuman accuracy
levels on many AI tasks. Several applications rely more and more on DNNs to
deliver sophisticated services and DNN accelerators are becoming integral
components of modern systems-on-chips. DNNs perform millions of arithmetic
operations per inference and DNN accelerators integrate thousands of
multiply-accumulate units leading to increased energy requirements. Approximate
computing principles are employed to significantly lower the energy consumption
of DNN accelerators at the cost of some accuracy loss. Nevertheless, recent
research demonstrated that complex DNNs are increasingly sensitive to
approximation. Hence, the obtained energy savings are often limited when
targeting tight accuracy constraints. In this work, we present a dynamically
configurable approximate multiplier that supports three operation modes, i.e.,
exact, positive error, and negative error. In addition, we propose a
filter-oriented approximation method to map the weights to the appropriate
modes of the approximate multiplier. Our mapping algorithm balances the
positive with the negative errors due to the approximate multiplications,
aiming at maximizing the energy reduction while minimizing the overall
convolution error. We evaluate our approach on multiple DNNs and datasets
against state-of-the-art approaches, where our method achieves 18.33% energy
gains on average across 7 NNs on 4 different datasets for a maximum accuracy
drop of only 1%.

    

### [[2107.09370] An Embedding of ReLU Networks and an Analysis of their Identifiability](http://arxiv.org/abs/2107.09370)


  Neural networks with the Rectified Linear Unit (ReLU) nonlinearity are
described by a vector of parameters $\theta$, and realized as a piecewise
linear continuous function $R_{\theta}: x \in \mathbb R^{d} \mapsto
R_{\theta}(x) \in \mathbb R^{k}$. Natural scalings and permutations operations
on the parameters $\theta$ leave the realization unchanged, leading to
equivalence classes of parameters that yield the same realization. These
considerations in turn lead to the notion of identifiability -- the ability to
recover (the equivalence class of) $\theta$ from the sole knowledge of its
realization $R_{\theta}$. The overall objective of this paper is to introduce
an embedding for ReLU neural networks of any depth, $\Phi(\theta)$, that is
invariant to scalings and that provides a locally linear parameterization of
the realization of the network. Leveraging these two key properties, we derive
some conditions under which a deep ReLU network is indeed locally identifiable
from the knowledge of the realization on a finite set of samples $x_{i} \in
\mathbb R^{d}$. We study the shallow case in more depth, establishing necessary
and sufficient conditions for the network to be identifiable from a bounded
subset $\mathcal X \subseteq \mathbb R^{d}$.

    

### [[2107.09384] An induction proof of the backpropagation algorithm in matrix notation](http://arxiv.org/abs/2107.09384)


  Backpropagation (BP) is a core component of the contemporary deep learning
incarnation of neural networks. Briefly, BP is an algorithm that exploits the
computational architecture of neural networks to efficiently evaluate the
gradient of a cost function during neural network parameter optimization. The
validity of BP rests on the application of a multivariate chain rule to the
computational architecture of neural networks and their associated objective
functions. Introductions to deep learning theory commonly present the
computational architecture of neural networks in matrix form, but eschew a
parallel formulation and justification of BP in the framework of matrix
differential calculus. This entails several drawbacks for the theory and
didactics of deep learning. In this work, we overcome these limitations by
providing a full induction proof of the BP algorithm in matrix notation.
Specifically, we situate the BP algorithm in the framework of matrix
differential calculus, encompass affine-linear potential functions, prove the
validity of the BP algorithm in inductive form, and exemplify the
implementation of the matrix form BP algorithm in computer code.

    

### [[2107.09391] Built-in Elastic Transformations for Improved Robustness](http://arxiv.org/abs/2107.09391)


  We focus on building robustness in the convolutions of neural visual
classifiers, especially against natural perturbations like elastic
deformations, occlusions and Gaussian noise. Existing CNNs show outstanding
performance on clean images, but fail to tackle naturally occurring
perturbations. In this paper, we start from elastic perturbations, which
approximate (local) view-point changes of the object. We present
elastically-augmented convolutions (EAConv) by parameterizing filters as a
combination of fixed elastically-perturbed bases functions and trainable
weights for the purpose of integrating unseen viewpoints in the CNN. We show on
CIFAR-10 and STL-10 datasets that the general robustness of our method on
unseen occlusion and Gaussian perturbations improves, while even improving the
performance on clean images slightly without performing any data augmentation.

    

### [[2107.09392] SVSNet: An End-to-end Speaker Voice Similarity Assessment Model](http://arxiv.org/abs/2107.09392)


  Neural evaluation metrics derived for numerous speech generation tasks have
recently attracted great attention. In this paper, we propose SVSNet, the first
end-to-end neural network model to assess the speaker voice similarity between
natural speech and synthesized speech. Unlike most neural evaluation metrics
that use hand-crafted features, SVSNet directly takes the raw waveform as input
to more completely utilize speech information for prediction. SVSNet consists
of encoder, co-attention, distance calculation, and prediction modules and is
trained in an end-to-end manner. The experimental results on the Voice
Conversion Challenge 2018 and 2020 (VCC2018 and VCC2020) datasets show that
SVSNet notably outperforms well-known baseline systems in the assessment of
speaker similarity at the utterance and system levels.

    

### [[2107.09402] Establishing process-structure linkages using Generative Adversarial Networks](http://arxiv.org/abs/2107.09402)


  The microstructure of material strongly influences its mechanical properties
and the microstructure itself is influenced by the processing conditions. Thus,
establishing a Process-Structure-Property relationship is a crucial task in
material design and is of interest in many engineering applications. We develop
a GAN (Generative Adversarial Network) to synthesize microstructures based on
given processing conditions. This approach is devoid of feature engineering,
needs little domain awareness, and can be applied to a wide variety of material
systems. Results show that our GAN model can produce high-fidelity multi-phase
microstructures which have a good correlation with the given processing
conditions.

    

### [[2107.09408] CREW: Computation Reuse and Efficient Weight Storage for Hardware-accelerated MLPs and RNNs](http://arxiv.org/abs/2107.09408)


  Deep Neural Networks (DNNs) have achieved tremendous success for cognitive
applications. The core operation in a DNN is the dot product between quantized
inputs and weights. Prior works exploit the weight/input repetition that arises
due to quantization to avoid redundant computations in Convolutional Neural
Networks (CNNs). However, in this paper we show that their effectiveness is
severely limited when applied to Fully-Connected (FC) layers, which are
commonly used in state-of-the-art DNNs, as it is the case of modern Recurrent
Neural Networks (RNNs) and Transformer models.
To improve energy-efficiency of FC computation we present CREW, a hardware
accelerator that implements Computation Reuse and an Efficient Weight Storage
mechanism to exploit the large number of repeated weights in FC layers. CREW
first performs the multiplications of the unique weights by their respective
inputs and stores the results in an on-chip buffer. The storage requirements
are modest due to the small number of unique weights and the relatively small
size of the input compared to convolutional layers. Next, CREW computes each
output by fetching and adding its required products. To this end, each weight
is replaced offline by an index in the buffer of unique products. Indices are
typically smaller than the quantized weights, since the number of unique
weights for each input tends to be much lower than the range of quantized
weights, which reduces storage and memory bandwidth requirements.
Overall, CREW greatly reduces the number of multiplications and provides
significant savings in model memory footprint and memory bandwidth usage. We
evaluate CREW on a diverse set of modern DNNs. On average, CREW provides 2.61x
speedup and 2.42x energy savings over a TPU-like accelerator. Compared to UCNN,
a state-of-art computation reuse technique, CREW achieves 2.10x speedup and
2.08x energy savings on average.

    

### [[2107.09414] Algorithm Selection on a Meta Level](http://arxiv.org/abs/2107.09414)


  The problem of selecting an algorithm that appears most suitable for a
specific instance of an algorithmic problem class, such as the Boolean
satisfiability problem, is called instance-specific algorithm selection. Over
the past decade, the problem has received considerable attention, resulting in
a number of different methods for algorithm selection. Although most of these
methods are based on machine learning, surprisingly little work has been done
on meta learning, that is, on taking advantage of the complementarity of
existing algorithm selection methods in order to combine them into a single
superior algorithm selector. In this paper, we introduce the problem of meta
algorithm selection, which essentially asks for the best way to combine a given
set of algorithm selectors. We present a general methodological framework for
meta algorithm selection as well as several concrete learning methods as
instantiations of this framework, essentially combining ideas of meta learning
and ensemble learning. In an extensive experimental evaluation, we demonstrate
that ensembles of algorithm selectors can significantly outperform single
algorithm selectors and have the potential to form the new state of the art in
algorithm selection.

    

### [[2107.09422] Large-scale graph representation learning with very deep GNNs and self-supervision](http://arxiv.org/abs/2107.09422)


  Effectively and efficiently deploying graph neural networks (GNNs) at scale
remains one of the most challenging aspects of graph representation learning.
Many powerful solutions have only ever been validated on comparatively small
datasets, often with counter-intuitive outcomes -- a barrier which has been
broken by the Open Graph Benchmark Large-Scale Challenge (OGB-LSC). We entered
the OGB-LSC with two large-scale GNNs: a deep transductive node classifier
powered by bootstrapping, and a very deep (up to 50-layer) inductive graph
regressor regularised by denoising objectives. Our models achieved an
award-level (top-3) performance on both the MAG240M and PCQM4M benchmarks. In
doing so, we demonstrate evidence of scalable self-supervised graph
representation learning, and utility of very deep GNNs -- both very important
open issues. Our code is publicly available at:
this https URL.

    

### [[2107.09428] Streaming End-to-End ASR based on Blockwise Non-Autoregressive Models](http://arxiv.org/abs/2107.09428)


  Non-autoregressive (NAR) modeling has gained more and more attention in
speech processing. With recent state-of-the-art attention-based automatic
speech recognition (ASR) structure, NAR can realize promising real-time factor
(RTF) improvement with only small degradation of accuracy compared to the
autoregressive (AR) models. However, the recognition inference needs to wait
for the completion of a full speech utterance, which limits their applications
on low latency scenarios. To address this issue, we propose a novel end-to-end
streaming NAR speech recognition system by combining blockwise-attention and
connectionist temporal classification with mask-predict (Mask-CTC) NAR. During
inference, the input audio is separated into small blocks and then processed in
a blockwise streaming way. To address the insertion and deletion error at the
edge of the output of each block, we apply an overlapping decoding strategy
with a dynamic mapping trick that can produce more coherent sentences.
Experimental results show that the proposed method improves online ASR
recognition in low latency conditions compared to vanilla Mask-CTC. Moreover,
it can achieve a much faster inference speed compared to the AR attention-based
models. All of our codes will be publicly available at
this https URL.

    

### [[2107.09437] Edge of chaos as a guiding principle for modern neural network training](http://arxiv.org/abs/2107.09437)


  The success of deep neural networks in real-world problems has prompted many
attempts to explain their training dynamics and generalization performance, but
more guiding principles for the training of neural networks are still needed.
Motivated by the edge of chaos principle behind the optimal performance of
neural networks, we study the role of various hyperparameters in modern neural
network training algorithms in terms of the order-chaos phase diagram. In
particular, we study a fully analytical feedforward neural network trained on
the widely adopted Fashion-MNIST dataset, and study the dynamics associated
with the hyperparameters in back-propagation during the training process. We
find that for the basic algorithm of stochastic gradient descent with momentum,
in the range around the commonly used hyperparameter values, clear scaling
relations are present with respect to the training time during the ordered
phase in the phase diagram, and the model's optimal generalization power at the
edge of chaos is similar across different training parameter combinations. In
the chaotic phase, the same scaling no longer exists. The scaling allows us to
choose the training parameters to achieve faster training without sacrificing
performance. In addition, we find that the commonly used model regularization
method - weight decay - effectively pushes the model towards the ordered phase
to achieve better performance. Leveraging on this fact and the scaling
relations in the other hyperparameters, we derived a principled guideline for
hyperparameter determination, such that the model can achieve optimal
performance by saturating it at the edge of chaos. Demonstrated on this simple
neural network model and training algorithm, our work improves the
understanding of neural network training dynamics, and can potentially be
extended to guiding principles of more complex model architectures and
algorithms.

    

### [[2107.09461] CANITA: Faster Rates for Distributed Convex Optimization with Communication Compression](http://arxiv.org/abs/2107.09461)


  Due to the high communication cost in distributed and federated learning,
methods relying on compressed communication are becoming increasingly popular.
Besides, the best theoretically and practically performing gradient-type
methods invariably rely on some form of acceleration/momentum to reduce the
number of communications (faster convergence), e.g., Nesterov's accelerated
gradient descent (Nesterov, 2004) and Adam (Kingma and Ba, 2014). In order to
combine the benefits of communication compression and convergence acceleration,
we propose a \emph{compressed and accelerated} gradient method for distributed
optimization, which we call CANITA. Our CANITA achieves the \emph{first
accelerated rate}
$O\bigg(\sqrt{\Big(1+\sqrt{\frac{\omega^3}{n}}\Big)\frac{L}{\epsilon}} +
\omega\big(\frac{1}{\epsilon}\big)^{\frac{1}{3}}\bigg)$, which improves upon
the state-of-the-art non-accelerated rate
$O\left((1+\frac{\omega}{n})\frac{L}{\epsilon} +
\frac{\omega^2+n}{\omega+n}\frac{1}{\epsilon}\right)$ of DIANA (Khaled et al.,
2020b) for distributed general convex problems, where $\epsilon$ is the target
error, $L$ is the smooth parameter of the objective, $n$ is the number of
machines/devices, and $\omega$ is the compression parameter (larger $\omega$
means more compression can be applied, and no compression implies $\omega=0$).
Our results show that as long as the number of devices $n$ is large (often true
in distributed/federated learning), or the compression $\omega$ is not very
high, CANITA achieves the faster convergence rate
$O\Big(\sqrt{\frac{L}{\epsilon}}\Big)$, i.e., the number of communication
rounds is $O\Big(\sqrt{\frac{L}{\epsilon}}\Big)$ (vs.
$O\big(\frac{L}{\epsilon}\big)$ achieved by previous works). As a result,
CANITA enjoys the advantages of both compression (compressed communication in
each round) and acceleration (much fewer communication rounds).

    

### [[2107.09480] Learned Sorted Table Search and Static Indexes in Small Space: Methodological and Practical Insights via an Experimental Study](http://arxiv.org/abs/2107.09480)


  Sorted Table Search Procedures are the quintessential query-answering tool,
still very useful, e.g, Search Engines (Google Chrome). Speeding them up, in
small additional space with respect to the table being searched into, is still
a quite significant achievement. Static Learned Indexes have been very
successful in achieving such a speed-up, but leave open a major question: To
what extent one can enjoy the speed-up of Learned Indexes while using constant
or nearly constant additional space. By generalizing the experimental
methodology of a recent benchmarking study on Learned Indexes, we shed light on
this question, by considering two scenarios. The first, quite elementary, i.e.,
textbook code, and the second using advanced Learned Indexing algorithms and
the supporting sophisticated software platforms. Although in both cases one
would expect a positive answer, its achievement is not as simple as it seems.
Indeed, our extensive set of experiments reveal a complex relationship between
query time and model space. The findings regarding this relationship and the
corresponding quantitative estimates, across memory levels, can be of interest
to algorithm designers and of use to practitioners as well. As an essential
part of our research, we introduce two new models that are of interest in their
own right. The first is a constant space model that can be seen as a
generalization of $k$-ary search, while the second is a synoptic {\bf RMI}, in
which we can control model space usage.

    

### [[2107.09483] Significant Wave Height Prediction based on Wavelet Graph Neural Network](http://arxiv.org/abs/2107.09483)


  Computational intelligence-based ocean characteristics forecasting
applications, such as Significant Wave Height (SWH) prediction, are crucial for
avoiding social and economic loss in coastal cities. Compared to the
traditional empirical-based or numerical-based forecasting models, "soft
computing" approaches, including machine learning and deep learning models,
have shown numerous success in recent years. In this paper, we focus on
enabling the deep learning model to learn both short-term and long-term
spatial-temporal dependencies for SWH prediction. A Wavelet Graph Neural
Network (WGNN) approach is proposed to integrate the advantages of wavelet
transform and graph neural network. Several parallel graph neural networks are
separately trained on wavelet decomposed data, and the reconstruction of each
model's prediction forms the final SWH prediction. Experimental results show
that the proposed WGNN approach outperforms other models, including the
numerical models, the machine learning models, and several deep learning
models.

    

### [[2107.09484] Predicting Friction System Performance with Symbolic Regression and Genetic Programming with Factor Variables](http://arxiv.org/abs/2107.09484)


  Friction systems are mechanical systems wherein friction is used for force
transmission (e.g. mechanical braking systems or automatic gearboxes). For
finding optimal and safe design parameters, engineers have to predict friction
system performance. This is especially difficult in real-world applications,
because it is affected by many parameters. We have used symbolic regression and
genetic programming for finding accurate and trustworthy prediction models for
this task. However, it is not straight-forward how nominal variables can be
included. In particular, a one-hot-encoding is unsatisfactory because genetic
programming tends to remove such indicator variables. We have therefore used
so-called factor variables for representing nominal variables in symbolic
regression models. Our results show that GP is able to produce symbolic
regression models for predicting friction performance with predictive accuracy
that is comparable to artificial neural networks. The symbolic regression
models with factor variables are less complex than models using a one-hot
encoding.

    

### [[2107.09502] Feature-Filter: Detecting Adversarial Examples through Filtering off Recessive Features](http://arxiv.org/abs/2107.09502)


  Deep neural networks (DNNs) are under threat from adversarial example
attacks. The adversary can easily change the outputs of DNNs by adding small
well-designed perturbations to inputs. Adversarial example detection is a
fundamental work for robust DNNs-based service. Adversarial examples show the
difference between humans and DNNs in image recognition. From a human-centric
perspective, image features could be divided into dominant features that are
comprehensible to humans, and recessive features that are incomprehensible to
humans, yet are exploited by DNNs. In this paper, we reveal that imperceptible
adversarial examples are the product of recessive features misleading neural
networks, and an adversarial attack is essentially a kind of method to enrich
these recessive features in the image. The imperceptibility of the adversarial
examples indicates that the perturbations enrich recessive features, yet hardly
affect dominant features. Therefore, adversarial examples are sensitive to
filtering off recessive features, while benign examples are immune to such
operation. Inspired by this idea, we propose a label-only adversarial detection
approach that is referred to as feature-filter. Feature-filter utilizes
discrete cosine transform to approximately separate recessive features from
dominant features, and gets a mutant image that is filtered off recessive
features. By only comparing DNN's prediction labels on the input and its
mutant, feature-filter can real-time detect imperceptible adversarial examples
at high accuracy and few false positives.

    

### [[2107.09507] EEG-based Cross-Subject Driver Drowsiness Recognition with Interpretable CNN](http://arxiv.org/abs/2107.09507)


  In the context of electroencephalogram (EEG)-based driver drowsiness
recognition, it is still a challenging task to design a calibration-free
system, since there exists a significant variability of EEG signals among
different subjects and recording sessions. As deep learning has received much
research attention in recent years, many efforts have been made to use deep
learning methods for EEG signal recognition. However, existing works mostly
treat deep learning models as blackbox classifiers, while what have been
learned by the models and to which extent they are affected by the noise from
EEG data are still underexplored. In this paper, we develop a novel
convolutional neural network that can explain its decision by highlighting the
local areas of the input sample that contain important information for the
classification. The network has a compact structure for ease of interpretation
and takes advantage of separable convolutions to process the EEG signals in a
spatial-temporal sequence. Results show that the model achieves an average
accuracy of 78.35% on 11 subjects for leave-one-out cross-subject drowsiness
recognition, which is higher than the conventional baseline methods of
53.4%-72.68% and state-of-art deep learning methods of 63.90%-65.61%.
Visualization results show that the model has learned to recognize biologically
explainable features from EEG signals, e.g., Alpha spindles, as strong
indicators of drowsiness across different subjects. In addition, we also
explore reasons behind some wrongly classified samples and how the model is
affected by artifacts and noise in the data. Our work illustrates a promising
direction on using interpretable deep learning models to discover meaning
patterns related to different mental states from complex EEG signals.

    

### [[2107.09509] Wearable Health Monitoring System for Older Adults in a Smart Home Environment](http://arxiv.org/abs/2107.09509)


  The advent of IoT has enabled the design of connected and integrated smart
health monitoring systems. These smart health monitoring systems could be
realized in a smart home context to render long-term care to the elderly
population. In this paper, we present the design of a wearable health
monitoring system suitable for older adults in a smart home context. The
proposed system offers solutions to monitor the stress, blood pressure, and
location of an individual within a smart home environment. The stress detection
model proposed in this work uses Electrodermal Activity (EDA),
Photoplethysmogram (PPG), and Skin Temperature (ST) sensors embedded in a smart
wristband for detecting physiological stress. The stress detection model is
trained and tested using stress labels obtained from salivary cortisol which is
a clinically established biomarker for physiological stress. A voice-based
prototype is also implemented and the feasibility of the proposed system for
integration in a smart home environment is analyzed by simulating a data
acquisition and streaming scenario. We have also proposed a blood pressure
estimation model using PPG signal and advanced regression techniques for
integration with the stress detection model in the wearable health monitoring
system. Finally, the design of a voice-assisted indoor location system is
proposed for integration with the proposed system within a smart home
environment. The proposed wearable health monitoring system is an important
direction to realize a smart home environment with extensive diagnostic
capabilities so that such a system could be useful for rendering long-term and
personalized care to the aging population in the comfort of their home.

    

### [[2107.09510] Modality Fusion Network and Personalized Attention in Momentary Stress Detection in the Wild](http://arxiv.org/abs/2107.09510)


  Multimodal wearable physiological data in daily life settings have been used
to estimate self-reported stress labels.However, missing data modalities in
data collection make it challenging to leverage all the collected samples.
Besides, heterogeneous sensor data and labels among individuals add challenges
in building robust stress detection models. In this paper, we proposed a
modality fusion network (MFN) to train models and infer self-reported binary
stress labels under both complete and incomplete modality condition. In
addition, we applied a personalized attention (PA) strategy to leverage
personalized representation along with the generalized one-size-fits-all model.
We evaluated our methods on a multimodal wearable sensor dataset (N=41)
including galvanic skin response (GSR) and electrocardiogram (ECG). Compared to
the baseline method using the samples with complete modalities, the performance
of the MFN improved by 1.6\% in f1-scores. On the other hand, the proposed PA
strategy showed a 2.3\% higher stress detection f1-score and approximately up
to 70\% reduction in personalized model parameter size (9.1 MB) compared to the
previous state-of-the-art transfer learning strategy (29.3 MB).

    

### [[2107.09519] Canonical Polyadic Decomposition and Deep Learning for Machine Fault Detection](http://arxiv.org/abs/2107.09519)


  Acoustic monitoring for machine fault detection is a recent and expanding
research path that has already provided promising results for industries.
However, it is impossible to collect enough data to learn all types of faults
from a machine. Thus, new algorithms, trained using data from healthy
conditions only, were developed to perform unsupervised anomaly detection. A
key issue in the development of these algorithms is the noise in the signals,
as it impacts the anomaly detection performance. In this work, we propose a
powerful data-driven and quasi non-parametric denoising strategy for spectral
data based on a tensor decomposition: the Non-negative Canonical Polyadic (CP)
decomposition. This method is particularly adapted for machine emitting
stationary sound. We demonstrate in a case study, the Malfunctioning Industrial
Machine Investigation and Inspection (MIMII) baseline, how the use of our
denoising strategy leads to a sensible improvement of the unsupervised anomaly
detection. Such approaches are capable to make sound-based monitoring of
industrial processes more reliable.

    

### [[2107.09539] Parametric Scattering Networks](http://arxiv.org/abs/2107.09539)


  The wavelet scattering transform creates geometric invariants and deformation
stability from an initial structured signal. In multiple signal domains it has
been shown to yield more discriminative representations compared to other
non-learned representations, and to outperform learned representations in
certain tasks, particularly on limited labeled data and highly structured
signals. The wavelet filters used in the scattering transform are typically
selected to create a tight frame via a parameterized mother wavelet. Focusing
on Morlet wavelets, we propose to instead adapt the scales, orientations, and
slants of the filters to produce problem-specific parametrizations of the
scattering transform. We show that our learned versions of the scattering
transform yield significant performance gains over the standard scattering
transform in the small sample classification settings, and our empirical
results suggest that tight frames may not always be necessary for scattering
transforms to extract effective representations.

    

### [[2107.09542] Open Problem: Is There an Online Learning Algorithm That Learns Whenever Online Learning Is Possible?](http://arxiv.org/abs/2107.09542)


  This open problem asks whether there exists an online learning algorithm for
binary classification that guarantees, for all target concepts, to make a
sublinear number of mistakes, under only the assumption that the (possibly
random) sequence of points X allows that such a learning algorithm can exist
for that sequence. As a secondary problem, it also asks whether a specific
concise condition completely determines whether a given (possibly random)
sequence of points X admits the existence of online learning algorithms
guaranteeing a sublinear number of mistakes for all target concepts.

    

### [[2107.09543] A Review of Generative Adversarial Networks in Cancer Imaging: New Applications, New Solutions](http://arxiv.org/abs/2107.09543)


  Despite technological and medical advances, the detection, interpretation,
and treatment of cancer based on imaging data continue to pose significant
challenges. These include high inter-observer variability, difficulty of
small-sized lesion detection, nodule interpretation and malignancy
determination, inter- and intra-tumour heterogeneity, class imbalance,
segmentation inaccuracies, and treatment effect uncertainty. The recent
advancements in Generative Adversarial Networks (GANs) in computer vision as
well as in medical imaging may provide a basis for enhanced capabilities in
cancer detection and analysis. In this review, we assess the potential of GANs
to address a number of key challenges of cancer imaging, including data
scarcity and imbalance, domain and dataset shifts, data access and privacy,
data annotation and quantification, as well as cancer detection, tumour
profiling and treatment planning. We provide a critical appraisal of the
existing literature of GANs applied to cancer imagery, together with
suggestions on future research directions to address these challenges. We
analyse and discuss 163 papers that apply adversarial training techniques in
the context of cancer imaging and elaborate their methodologies, advantages and
limitations. With this work, we strive to bridge the gap between the needs of
the clinical cancer imaging community and the current and prospective research
on GANs in the artificial intelligence community.

    

### [[2107.09545] Predicting Driver Takeover Time in Conditionally Automated Driving](http://arxiv.org/abs/2107.09545)


  It is extremely important to ensure a safe takeover transition in
conditionally automated driving. One of the critical factors that quantifies
the safe takeover transition is takeover time. Previous studies identified the
effects of many factors on takeover time, such as takeover lead time,
non-driving tasks, modalities of the takeover requests (TORs), and scenario
urgency. However, there is a lack of research to predict takeover time by
considering these factors all at the same time. Toward this end, we used
eXtreme Gradient Boosting (XGBoost) to predict the takeover time using a
dataset from a meta-analysis study [1]. In addition, we used SHAP (SHapley
Additive exPlanation) to analyze and explain the effects of the predictors on
takeover time. We identified seven most critical predictors that resulted in
the best prediction performance. Their main effects and interaction effects on
takeover time were examined. The results showed that the proposed approach
provided both good performance and explainability. Our findings have
implications on the design of in-vehicle monitoring and alert systems to
facilitate the interaction between the drivers and the automated vehicle.

    

### [[2107.09546] Responsible and Regulatory Conform Machine Learning for Medicine: A Survey of Technical Challenges and Solutions](http://arxiv.org/abs/2107.09546)


  Machine learning is expected to fuel significant improvements in medical
care. To ensure that fundamental principles such as beneficence, respect for
human autonomy, prevention of harm, justice, privacy, and transparency are
respected, medical machine learning applications must be developed responsibly.
In this paper, we survey the technical challenges involved in creating medical
machine learning systems responsibly and in conformity with existing
regulations, as well as possible solutions to address these challenges. We
begin by providing a brief overview of existing regulations affecting medical
machine learning, showing that properties such as safety, robustness,
reliability, privacy, security, transparency, explainability, and
nondiscrimination are all demanded already by existing law and regulations -
albeit, in many cases, to an uncertain degree. Next, we discuss the underlying
technical challenges, possible ways for addressing them, and their respective
merits and drawbacks. We notice that distribution shift, spurious correlations,
model underspecification, and data scarcity represent severe challenges in the
medical context (and others) that are very difficult to solve with classical
black-box deep neural networks. Important measures that may help to address
these challenges include the use of large and representative datasets and
federated learning as a means to that end, the careful exploitation of domain
knowledge wherever feasible, the use of inherently transparent models,
comprehensive model testing and verification, as well as stakeholder inclusion.

    

### [[2107.09554] Mining Topological Dependencies of Recurrent Congestion in Road Networks](http://arxiv.org/abs/2107.09554)


  The discovery of spatio-temporal dependencies within urban road networks that
cause Recurrent Congestion (RC) patterns is crucial for numerous real-world
applications, including urban planning and scheduling of public transportation
services. While most existing studies investigate temporal patterns of RC
phenomena, the influence of the road network topology on RC is often
overlooked. This article proposes the ST-Discovery algorithm, a novel
unsupervised spatio-temporal data mining algorithm that facilitates the
effective data-driven discovery of RC dependencies induced by the road network
topology using real-world traffic data. We factor out regularly reoccurring
traffic phenomena, such as rush hours, mainly induced by the daytime, by
modelling and systematically exploiting temporal traffic load outliers. We
present an algorithm that first constructs connected subgraphs of the road
network based on the traffic speed outliers. Second, the algorithm identifies
pairs of subgraphs that indicate spatio-temporal correlations in their traffic
load behaviour to identify topological dependencies within the road network.
Finally, we rank the identified subgraph pairs based on the dependency score
determined by our algorithm. Our experimental results demonstrate that
ST-Discovery can effectively reveal topological dependencies in urban road
networks.

    

### [[2107.09562] Characterizing Generalization under Out-Of-Distribution Shifts in Deep Metric Learning](http://arxiv.org/abs/2107.09562)


  Deep Metric Learning (DML) aims to find representations suitable for
zero-shot transfer to a priori unknown test distributions. However, common
evaluation protocols only test a single, fixed data split in which train and
test classes are assigned randomly. More realistic evaluations should consider
a broad spectrum of distribution shifts with potentially varying degree and
difficulty. In this work, we systematically construct train-test splits of
increasing difficulty and present the ooDML benchmark to characterize
generalization under out-of-distribution shifts in DML. ooDML is designed to
probe the generalization performance on much more challenging, diverse
train-to-test distribution shifts. Based on our new benchmark, we conduct a
thorough empirical analysis of state-of-the-art DML methods. We find that while
generalization tends to consistently degrade with difficulty, some methods are
better at retaining performance as the distribution shift increases. Finally,
we propose few-shot DML as an efficient way to consistently improve
generalization in response to unknown test shifts presented in ooDML. Code
available here:
this https URL.

    

### [[2107.09572] Best-of-All-Worlds Bounds for Online Learning with Feedback Graphs](http://arxiv.org/abs/2107.09572)


  We study the online learning with feedback graphs framework introduced by
Mannor and Shamir (2011), in which the feedback received by the online learner
is specified by a graph $G$ over the available actions. We develop an algorithm
that simultaneously achieves regret bounds of the form:
$\smash{\mathcal{O}(\sqrt{\theta(G) T})}$ with adversarial losses;
$\mathcal{O}(\theta(G)\operatorname{polylog}{T})$ with stochastic losses; and
$\mathcal{O}(\theta(G)\operatorname{polylog}{T} + \smash{\sqrt{\theta(G) C})}$
with stochastic losses subject to $C$ adversarial corruptions. Here,
$\theta(G)$ is the clique covering number of the graph $G$. Our algorithm is an
instantiation of Follow-the-Regularized-Leader with a novel regularization that
can be seen as a product of a Tsallis entropy component (inspired by Zimmert
and Seldin (2019)) and a Shannon entropy component (analyzed in the corrupted
stochastic case by Amir et al. (2020)), thus subtly interpolating between the
two forms of entropies. One of our key technical contributions is in
establishing the convexity of this regularizer and controlling its inverse
Hessian, despite its complex product structure.

    

### [[2107.09577] How Does Cell-Free Massive MIMO Support Multiple Federated Learning Groups?](http://arxiv.org/abs/2107.09577)


  Federated learning (FL) has been considered as a promising learning framework
for future machine learning systems due to its privacy preservation and
communication efficiency. In beyond-5G/6G systems, it is likely to have
multiple FL groups with different learning purposes. This scenario leads to a
question: How does a wireless network support multiple FL groups? As an answer,
we first propose to use a cell-free massive multiple-input multiple-output
(MIMO) network to guarantee the stable operation of multiple FL processes by
letting the iterations of these FL processes be executed together within a
large-scale coherence time. We then develop a novel scheme that asynchronously
executes the iterations of FL processes under multicasting downlink and
conventional uplink transmission protocols. Finally, we propose a
simple/low-complexity resource allocation algorithm which optimally chooses the
power and computation resources to minimize the execution time of each
iteration of each FL process.

    

### [[2107.09597] Positively Weighted Kernel Quadrature via Subsampling](http://arxiv.org/abs/2107.09597)


  We study kernel quadrature rules with positive weights for probability
measures on general domains. Our theoretical analysis combines the spectral
properties of the kernel with random sampling of points. This results in
effective algorithms to construct kernel quadrature rules with positive weights
and small worst-case error. Besides additional robustness, our numerical
experiments indicate that this can achieve fast convergence rates that compete
with the optimal bounds in well-known examples.

    

### [[2107.09598] Learning Altruistic Behaviours in Reinforcement Learning without External Rewards](http://arxiv.org/abs/2107.09598)


  Can artificial agents learn to assist others in achieving their goals without
knowing what those goals are? Generic reinforcement learning agents could be
trained to behave altruistically towards others by rewarding them for
altruistic behaviour, i.e., rewarding them for benefiting other agents in a
given situation. Such an approach assumes that other agents' goals are known so
that the altruistic agent can cooperate in achieving those goals. However,
explicit knowledge of other agents' goals is often difficult to acquire. Even
assuming such knowledge to be given, training of altruistic agents would
require manually-tuned external rewards for each new environment. Thus, it is
beneficial to develop agents that do not depend on external supervision and can
learn altruistic behaviour in a task-agnostic manner. Assuming that other
agents rationally pursue their goals, we hypothesize that giving them more
choices will allow them to pursue those goals better. Some concrete examples
include opening a door for others or safeguarding them to pursue their
objectives without interference. We formalize this concept and propose an
altruistic agent that learns to increase the choices another agent has by
maximizing the number of states that the other agent can reach in its future.
We evaluate our approach on three different multi-agent environments where
another agent's success depends on the altruistic agent's behaviour. Finally,
we show that our unsupervised agents can perform comparably to agents
explicitly trained to work cooperatively. In some cases, our agents can even
outperform the supervised ones.

    

### [[2107.09602] Medical Imaging with Deep Learning for COVID- 19 Diagnosis: A Comprehensive Review](http://arxiv.org/abs/2107.09602)


  The outbreak of novel coronavirus disease (COVID- 19) has claimed millions of
lives and has affected all aspects of human life. This paper focuses on the
application of deep learning (DL) models to medical imaging and drug discovery
for managing COVID-19 disease. In this article, we detail various medical
imaging-based studies such as X-rays and computed tomography (CT) images along
with DL methods for classifying COVID-19 affected versus pneumonia. The
applications of DL techniques to medical images are further described in terms
of image localization, segmentation, registration, and classification leading
to COVID-19 detection. The reviews of recent papers indicate that the highest
classification accuracy of 99.80% is obtained when InstaCovNet-19 DL method is
applied to an X-ray dataset of 361 COVID-19 patients, 362 pneumonia patients
and 365 normal people. Furthermore, it can be seen that the best classification
accuracy of 99.054% can be achieved when EDL_COVID DL method is applied to a CT
image dataset of 7500 samples where COVID-19 patients, lung tumor patients and
normal people are equal in number. Moreover, we illustrate the potential DL
techniques in drug or vaccine discovery in combating the coronavirus. Finally,
we address a number of problems, concerns and future research directions
relevant to DL applications for COVID-19.

    

### [[2107.09621] Rethinking the Tradeoff in Integrated Sensing and Communication: Recognition Accuracy versus Communication Rate](http://arxiv.org/abs/2107.09621)


  Integrated sensing and communication (ISAC) is a promising technology to
improve the band-utilization efficiency via spectrum sharing or hardware
sharing between radar and communication systems. Since a common radio resource
budget is shared by both functionalities, there exists a tradeoff between the
sensing and communication performance. However, this tradeoff curve is
currently unknown in ISAC systems with human motion recognition tasks based on
deep learning. To fill this gap, this paper formulates and solves a
multi-objective optimization problem which simultaneously maximizes the
recognition accuracy and the communication data rate. The key ingredient of
this new formulation is a nonlinear recognition accuracy model with respect to
the wireless resources, where the model is derived from power function
regression of the system performance of the deep spectrogram network. To avoid
cost-expensive data collection procedures, a primitive-based autoregressive
hybrid (PBAH) channel model is developed, which facilitates efficient training
and testing dataset generation for human motion recognition in a virtual
environment. Extensive results demonstrate that the proposed wireless
recognition accuracy and PBAH channel models match the actual experimental data
very well. Moreover, it is found that the accuracy-rate region consists of a
communication saturation zone, a sensing saturation zone, and a
communication-sensing adversarial zone, of which the third zone achieves the
desirable balanced performance for ISAC systems.

    

### [[2107.09627] Precision-Weighted Federated Learning](http://arxiv.org/abs/2107.09627)


  Federated Learning using the Federated Averaging algorithm has shown great
advantages for large-scale applications that rely on collaborative learning,
especially when the training data is either unbalanced or inaccessible due to
privacy constraints. We hypothesize that Federated Averaging underestimates the
full extent of heterogeneity of data when the aggregation is performed. We
propose Precision-weighted Federated Learning a novel algorithm that takes into
account the variance of the stochastic gradients when computing the weighted
average of the parameters of models trained in a Federated Learning setting.
With Precision-weighted Federated Learning, we provide an alternate averaging
scheme that leverages the heterogeneity of the data when it has a large
diversity of features in its composition. Our method was evaluated using
standard image classification datasets with two different data partitioning
strategies (IID/non-IID) to measure the performance and speed of our method in
resource-constrained environments, such as mobile and IoT devices. We obtained
a good balance between computational efficiency and convergence rates with
Precision-weighted Federated Learning. Our performance evaluations show 9%
better predictions with MNIST, 18% with Fashion-MNIST, and 5% with CIFAR-10 in
the non-IID setting. Further reliability evaluations ratify the stability in
our method by reaching a 99% reliability index with IID partitions and 96% with
non-IID partitions. In addition, we obtained a 20x speedup on Fashion-MNIST
with only 10 clients and up to 37x with 100 clients participating in the
aggregation concurrently per communication round. The results indicate that
Precision-weighted Federated Learning is an effective and faster alternative
approach for aggregating private data, especially in domains where data is
highly heterogeneous.

    

### [[2107.09640] Predicting the 2020 US Presidential Election with Twitter](http://arxiv.org/abs/2107.09640)


  One major sub-domain in the subject of polling public opinion with social
media data is electoral prediction. Electoral prediction utilizing social media
data potentially would significantly affect campaign strategies, complementing
traditional polling methods and providing cheaper polling in real-time. First,
this paper explores past successful methods from research for analysis and
prediction of the 2020 US Presidential Election using Twitter data. Then, this
research proposes a new method for electoral prediction which combines
sentiment, from NLP on the text of tweets, and structural data with aggregate
polling, a time series analysis, and a special focus on Twitter users critical
to the election. Though this method performed worse than its baseline of
polling predictions, it is inconclusive whether this is an accurate method for
predicting elections due to scarcity of data. More research and more data are
needed to accurately measure this method's overall effectiveness.

    

### [[2107.09645] Mastering Visual Continuous Control: Improved Data-Augmented Reinforcement Learning](http://arxiv.org/abs/2107.09645)


  We present DrQ-v2, a model-free reinforcement learning (RL) algorithm for
visual continuous control. DrQ-v2 builds on DrQ, an off-policy actor-critic
approach that uses data augmentation to learn directly from pixels. We
introduce several improvements that yield state-of-the-art results on the
DeepMind Control Suite. Notably, DrQ-v2 is able to solve complex humanoid
locomotion tasks directly from pixel observations, previously unattained by
model-free RL. DrQ-v2 is conceptually simple, easy to implement, and provides
significantly better computational footprint compared to prior work, with the
majority of tasks taking just 8 hours to train on a single GPU. Finally, we
publicly release DrQ-v2's implementation to provide RL practitioners with a
strong and computationally efficient baseline.

    

### [[2107.09647] Proximal Policy Optimization for Tracking Control Exploiting Future Reference Information](http://arxiv.org/abs/2107.09647)


  In recent years, reinforcement learning (RL) has gained increasing attention
in control engineering. Especially, policy gradient methods are widely used. In
this work, we improve the tracking performance of proximal policy optimization
(PPO) for arbitrary reference signals by incorporating information about future
reference values. Two variants of extending the argument of the actor and the
critic taking future reference values into account are presented. In the first
variant, global future reference values are added to the argument. For the
second variant, a novel kind of residual space with future reference values
applicable to model-free reinforcement learning is introduced. Our approach is
evaluated against a PI controller on a simple drive train model. We expect our
method to generalize to arbitrary references better than previous approaches,
pointing towards the applicability of RL to control real systems.

    

### [[2107.09648] Different kinds of cognitive plausibility: why are transformers better than RNNs at predicting N400 amplitude?](http://arxiv.org/abs/2107.09648)


  Despite being designed for performance rather than cognitive plausibility,
transformer language models have been found to be better at predicting metrics
used to assess human language comprehension than language models with other
architectures, such as recurrent neural networks. Based on how well they
predict the N400, a neural signal associated with processing difficulty, we
propose and provide evidence for one possible explanation - their predictions
are affected by the preceding context in a way analogous to the effect of
semantic facilitation in humans.

    

### [[2107.09661] Learn2Hop: Learned Optimization on Rough Landscapes](http://arxiv.org/abs/2107.09661)


  Optimization of non-convex loss surfaces containing many local minima remains
a critical problem in a variety of domains, including operations research,
informatics, and material design. Yet, current techniques either require
extremely high iteration counts or a large number of random restarts for good
performance. In this work, we propose adapting recent developments in
meta-learning to these many-minima problems by learning the optimization
algorithm for various loss landscapes. We focus on problems from atomic
structural optimization--finding low energy configurations of many-atom
systems--including widely studied models such as bimetallic clusters and
disordered silicon. We find that our optimizer learns a 'hopping' behavior
which enables efficient exploration and improves the rate of low energy minima
discovery. Finally, our learned optimizers show promising generalization with
efficiency gains on never before seen tasks (e.g. new elements or
compositions). Code will be made available shortly.

    

### [[2002.09067] Incremental Sampling Without Replacement for Sequence Models](http://arxiv.org/abs/2002.09067)


  Sampling is a fundamental technique, and sampling without replacement is
often desirable when duplicate samples are not beneficial. Within machine
learning, sampling is useful for generating diverse outputs from a trained
model. We present an elegant procedure for sampling without replacement from a
broad class of randomized programs, including generative neural models that
construct outputs sequentially. Our procedure is efficient even for
exponentially-large output spaces. Unlike prior work, our approach is
incremental, i.e., samples can be drawn one at a time, allowing for increased
flexibility. We also present a new estimator for computing expectations from
samples drawn without replacement. We show that incremental sampling without
replacement is applicable to many domains, e.g., program synthesis and
combinatorial optimization.

    

### [[2007.03814] Variational Representations and Neural Network Estimation of Rényi Divergences](http://arxiv.org/abs/2007.03814)


  We derive a new variational formula for the Rényi family of divergences,
$R_\alpha(Q\|P)$, between probability measures $Q$ and $P$. Our result
generalizes the classical Donsker-Varadhan variational formula for the
Kullback-Leibler divergence. We further show that this Rényi variational
formula holds over a range of function spaces; this leads to a formula for the
optimizer under very weak assumptions and is also key in our development of a
consistency theory for Rényi divergence estimators. By applying this theory
to neural-network estimators, we show that if a neural network family satisfies
one of several strengthened versions of the universal approximation property
then the corresponding Rényi divergence estimator is consistent. In contrast
to density-estimator based methods, our estimators involve only expectations
under $Q$ and $P$ and hence are more effective in high dimensional systems. We
illustrate this via several numerical examples of neural network estimation in
systems of up to 5000 dimensions.

    

### [[2007.05675] Towards Cross-Granularity Few-Shot Learning: Coarse-to-Fine Pseudo-Labeling with Visual-Semantic Meta-Embedding](http://arxiv.org/abs/2007.05675)


  Few-shot learning aims at rapidly adapting to novel categories with only a
handful of samples at test time, which has been predominantly tackled with the
idea of meta-learning. However, meta-learning approaches essentially learn
across a variety of few-shot tasks and thus still require large-scale training
data with fine-grained supervision to derive a generalized model, thereby
involving prohibitive annotation cost. In this paper, we advance the few-shot
classification paradigm towards a more challenging scenario, i.e.,
cross-granularity few-shot classification, where the model observes only coarse
labels during training while is expected to perform fine-grained classification
during testing. This task largely relieves the annotation cost since
fine-grained labeling usually requires strong domain-specific expertise. To
bridge the cross-granularity gap, we approximate the fine-grained data
distribution by greedy clustering of each coarse-class into pseudo-fine-classes
according to the similarity of image embeddings. We then propose a
meta-embedder that jointly optimizes the visual- and semantic-discrimination,
in both instance-wise and coarse class-wise, to obtain a good feature space for
this coarse-to-fine pseudo-labeling process. Extensive experiments and ablation
studies are conducted to demonstrate the effectiveness and robustness of our
approach on three representative datasets.

    

### [[2009.09176] Causal Discovery with Multi-Domain LiNGAM for Latent Factors](http://arxiv.org/abs/2009.09176)


  Discovering causal structures among latent factors from observed data is a
particularly challenging problem. Despite some efforts for this problem,
existing methods focus on the single-domain data only. In this paper, we
propose Multi-Domain Linear Non-Gaussian Acyclic Models for Latent Factors
(MD-LiNA), where the causal structure among latent factors of interest is
shared for all domains, and we provide its identification results. The model
enriches the causal representation for multi-domain data. We propose an
integrated two-phase algorithm to estimate the model. In particular, we first
locate the latent factors and estimate the factor loading matrix. Then to
uncover the causal structure among shared latent factors of interest, we derive
a score function based on the characterization of independence relations
between external influences and the dependence relations between multi-domain
latent factors and latent factors of interest. We show that the proposed method
provides locally consistent estimators. Experimental results on both synthetic
and real-world data demonstrate the efficacy and robustness of our approach.

    

### [[2010.13677] Deep Low-rank plus Sparse Network for Dynamic MR Imaging](http://arxiv.org/abs/2010.13677)


  In dynamic magnetic resonance (MR) imaging, low-rank plus sparse (L+S)
decomposition, or robust principal component analysis (PCA), has achieved
stunning performance. However, the selection of the parameters of L+S is
empirical, and the acceleration rate is limited, which are common failings of
iterative compressed sensing MR imaging (CS-MRI) reconstruction methods. Many
deep learning approaches have been proposed to address these issues, but few of
them use a low-rank prior. In this paper, a model-based low-rank plus sparse
network, dubbed L+S-Net, is proposed for dynamic MR reconstruction. In
particular, we use an alternating linearized minimization method to solve the
optimization problem with low-rank and sparse regularization. Learned soft
singular value thresholding is introduced to ensure the clear separation of the
L component and S component. Then, the iterative steps are unrolled into a
network in which the regularization parameters are learnable. We prove that the
proposed L+S-Net achieves global convergence under two standard assumptions.
Experiments on retrospective and prospective cardiac cine datasets show that
the proposed model outperforms state-of-the-art CS and existing deep learning
methods and has great potential for extremely high acceleration factors (up to
24x).

    

### [[2010.13972] GPUTreeShap: Massively Parallel Exact Calculation of SHAP Scores for Tree Ensembles](http://arxiv.org/abs/2010.13972)


  SHAP (SHapley Additive exPlanation) values provide a game theoretic
interpretation of the predictions of machine learning models based on Shapley
values. While exact calculation of SHAP values is computationally intractable
in general, a recursive polynomial-time algorithm called TreeShap is available
for decision tree models. However, despite its polynomial time complexity,
TreeShap can become a significant bottleneck in practical machine learning
pipelines when applied to large decision tree ensembles. We present
GPUTreeShap, a modified TreeShap algorithm suitable for massively parallel
computation on graphics processing units. Our approach first preprocesses each
decision tree to isolate variable sized sub-problems from the original
recursive algorithm, then solves a bin packing problem, and finally maps
sub-problems to single-instruction, multiple-thread (SIMT) tasks for parallel
execution with specialised hardware instructions. With a single NVIDIA Tesla
V100-32 GPU, we achieve speedups of up to 19x for SHAP values, and speedups of
up to 340x for SHAP interaction values, over a state-of-the-art multi-core CPU
implementation executed on two 20-core Xeon E5-2698 v4 2.2 GHz CPUs. We also
experiment with multi-GPU computing using eight V100 GPUs, demonstrating
throughput of 1.2M rows per second -- equivalent CPU-based performance is
estimated to require 6850 CPU cores.

    

### [[2010.15390] Multitask Bandit Learning Through Heterogeneous Feedback Aggregation](http://arxiv.org/abs/2010.15390)


  In many real-world applications, multiple agents seek to learn how to perform
highly related yet slightly different tasks in an online bandit learning
protocol. We formulate this problem as the $\epsilon$-multi-player multi-armed
bandit problem, in which a set of players concurrently interact with a set of
arms, and for each arm, the reward distributions for all players are similar
but not necessarily identical. We develop an upper confidence bound-based
algorithm, RobustAgg$(\epsilon)$, that adaptively aggregates rewards collected
by different players. In the setting where an upper bound on the pairwise
similarities of reward distributions between players is known, we achieve
instance-dependent regret guarantees that depend on the amenability of
information sharing across players. We complement these upper bounds with
nearly matching lower bounds. In the setting where pairwise similarities are
unknown, we provide a lower bound, as well as an algorithm that trades off
minimax regret guarantees for adaptivity to unknown similarity structure.

    

### [[2011.04125] Quantum-Inspired Algorithms from Randomized Numerical Linear Algebra](http://arxiv.org/abs/2011.04125)


  We create classical (non-quantum) dynamic data structures supporting queries
for recommender systems and least-squares regression that are comparable to
their quantum analogues. De-quantizing such algorithms has received a flurry of
attention in recent years; we obtain sharper bounds for these problems. More
significantly, we achieve these improvements by arguing that the previous
quantum-inspired algorithms for these problems are doing leverage or
ridge-leverage score sampling in disguise; these are powerful and standard
techniques in randomized numerical linear algebra. With this recognition, we
are able to employ the large body of work in numerical linear algebra to obtain
algorithms for these problems that are simpler or faster (or both) than
existing approaches.

    

### [[2011.04419] Towards Domain-Agnostic Contrastive Learning](http://arxiv.org/abs/2011.04419)


  Despite recent success, most contrastive self-supervised learning methods are
domain-specific, relying heavily on data augmentation techniques that require
knowledge about a particular domain, such as image cropping and rotation. To
overcome such limitation, we propose a novel domain-agnostic approach to
contrastive learning, named DACL, that is applicable to domains where
invariances, and thus, data augmentation techniques, are not readily available.
Key to our approach is the use of Mixup noise to create similar and dissimilar
examples by mixing data samples differently either at the input or hidden-state
levels. To demonstrate the effectiveness of DACL, we conduct experiments across
various domains such as tabular data, images, and graphs. Our results show that
DACL not only outperforms other domain-agnostic noising methods, such as
Gaussian-noise, but also combines well with domain-specific methods, such as
SimCLR, to improve self-supervised visual representation learning. Finally, we
theoretically analyze our method and show advantages over the Gaussian-noise
based contrastive learning approach.

    

### [[2011.05041] Sparse within Sparse Gaussian Processes using Neighbor Information](http://arxiv.org/abs/2011.05041)


  Approximations to Gaussian processes based on inducing variables, combined
with variational inference techniques, enable state-of-the-art sparse
approaches to infer GPs at scale through mini batch-based learning. In this
work, we address one limitation of sparse GPs, which is due to the challenge in
dealing with a large number of inducing variables without imposing a special
structure on the inducing inputs. In particular, we introduce a novel
hierarchical prior, which imposes sparsity on the set of inducing variables. We
treat our model variationally, and we experimentally show considerable
computational gains compared to standard sparse GPs when sparsity on the
inducing variables is realized considering the nearest inducing inputs of a
random mini-batch of the data. We perform an extensive experimental validation
that demonstrates the effectiveness of our approach compared to the
state-of-the-art. Our approach enables the possibility to use sparse GPs using
a large number of inducing points without incurring a prohibitive computational
cost.

    

### [[2012.02684] Model-Agnostic Learning to Meta-Learn](http://arxiv.org/abs/2012.02684)


  In this paper, we propose a learning algorithm that enables a model to
quickly exploit commonalities among related tasks from an unseen task
distribution, before quickly adapting to specific tasks from that same
distribution. We investigate how learning with different task distributions can
first improve adaptability by meta-finetuning on related tasks before improving
goal task generalization with finetuning. Synthetic regression experiments
validate the intuition that learning to meta-learn improves adaptability and
consecutively generalization. Experiments on more complex image classification,
continual regression, and reinforcement learning tasks demonstrate that
learning to meta-learn generally improves task-specific adaptation. The
methodology, setup, and hypotheses in this proposal were positively evaluated
by peer review before conclusive experiments were carried out.

    

### [[2101.05206] Deep Learning Assisted Calibrated Beam Training for Millimeter-Wave Communication Systems](http://arxiv.org/abs/2101.05206)


  Huge overhead of beam training imposes a significant challenge in
millimeter-wave (mmWave) wireless communications. To address this issue, in
this paper, we propose a wide beam based training approach to calibrate the
narrow beam direction according to the channel power leakage. To handle the
complex nonlinear properties of the channel power leakage, deep learning is
utilized to predict the optimal narrow beam directly. Specifically, three deep
learning assisted calibrated beam training schemes are proposed. The first
scheme adopts convolution neural network to implement the prediction based on
the instantaneous received signals of wide beam training. We also perform the
additional narrow beam training based on the predicted probabilities for
further beam direction calibrations. However, the first scheme only depends on
one wide beam training, which lacks the robustness to noise. To tackle this
problem, the second scheme adopts long-short term memory (LSTM) network for
tracking the movement of users and calibrating the beam direction according to
the received signals of prior beam training, in order to enhance the robustness
to noise. To further reduce the overhead of wide beam training, our third
scheme, an adaptive beam training strategy, selects partial wide beams to be
trained based on the prior received signals. Two criteria, namely, optimal
neighboring criterion and maximum probability criterion, are designed for the
selection. Furthermore, to handle mobile scenarios, auxiliary LSTM is
introduced to calibrate the directions of the selected wide beams more
precisely. Simulation results demonstrate that our proposed schemes achieve
significantly higher beamforming gain with smaller beam training overhead
compared with the conventional and existing deep-learning based counterparts.

    

### [[2101.08032] Riemannian Manifold Optimization for Discriminant Subspace Learning](http://arxiv.org/abs/2101.08032)


  Linear discriminant analysis (LDA) is a widely used algorithm in machine
learning to extract a low-dimensional representation of high-dimensional data,
it features to find the orthogonal discriminant projection subspace by using
the Fisher discriminant criterion. However, the traditional Euclidean-based
methods for solving LDA are easily convergent to spurious local minima and
hardly obtain an optimal solution. To address such a problem, in this paper, we
propose a novel algorithm namely Riemannian-based discriminant analysis (RDA)
for subspace learning. In order to obtain an explicit solution, we transform
the traditional Euclidean-based methods to the Riemannian manifold space and
use the trust-region method to learn the discriminant projection subspace. We
compare the proposed algorithm to existing variants of LDA, as well as the
unsupervised tensor decomposition methods on image classification tasks. The
numerical results suggest that RDA achieves state-of-the-art performance in
classification accuracy.

    

### [[2102.01936] A Bayesian Federated Learning Framework with Online Laplace Approximation](http://arxiv.org/abs/2102.01936)


  Federated learning (FL) allows multiple clients to collaboratively learn a
globally shared model through cycles of model aggregation and local model
training, without the need to share data. Most existing FL methods train local
models separately on different clients, and then simply average their
parameters to obtain a centralized model on the server side. However, these
approaches generally suffer from large aggregation errors and severe local
forgetting, which are particularly bad in heterogeneous data settings. To
tackle these issues, in this paper, we propose a novel FL framework that uses
online Laplace approximation to approximate posteriors on both the client and
server side. On the server side, a multivariate Gaussian product mechanism is
employed to construct and maximize a global posterior, largely reducing the
aggregation errors induced by large discrepancies between local models. On the
client side, a prior loss that uses the global posterior probabilistic
parameters delivered from the server is designed to guide the local training.
Binding such learning constraints from other clients enables our method to
mitigate local forgetting. Finally, we achieve state-of-the-art results on
several benchmarks, clearly demonstrating the advantages of the proposed
method.

    

### [[2102.07856] Don't Just Blame Over-parametrization for Over-confidence: Theoretical Analysis of Calibration in Binary Classification](http://arxiv.org/abs/2102.07856)


  Modern machine learning models with high accuracy are often miscalibrated --
the predicted top probability does not reflect the actual accuracy, and tends
to be over-confident. It is commonly believed that such over-confidence is
mainly due to over-parametrization, in particular when the model is large
enough to memorize the training data and maximize the confidence.
In this paper, we show theoretically that over-parametrization is not the
only reason for over-confidence. We prove that logistic regression is
inherently over-confident, in the realizable, under-parametrized setting where
the data is generated from the logistic model, and the sample size is much
larger than the number of parameters. Further, this over-confidence happens for
general well-specified binary classification problems as long as the activation
is symmetric and concave on the positive part. Perhaps surprisingly, we also
show that over-confidence is not always the case -- there exists another
activation function (and a suitable loss function) under which the learned
classifier is under-confident at some probability values. Overall, our theory
provides a precise characterization of calibration in realizable binary
classification, which we verify on simulations and real data experiments.

    

### [[2102.08898] Few-shot Conformal Prediction with Auxiliary Tasks](http://arxiv.org/abs/2102.08898)


  We develop a novel approach to conformal prediction when the target task has
limited data available for training. Conformal prediction identifies a small
set of promising output candidates in place of a single prediction, with
guarantees that the set contains the correct answer with high probability. When
training data is limited, however, the predicted set can easily become unusably
large. In this work, we obtain substantially tighter prediction sets while
maintaining desirable marginal guarantees by casting conformal prediction as a
meta-learning paradigm over exchangeable collections of auxiliary tasks. Our
conformalization algorithm is simple, fast, and agnostic to the choice of
underlying model, learning algorithm, or dataset. We demonstrate the
effectiveness of this approach across a number of few-shot classification and
regression tasks in natural language processing, computer vision, and
computational chemistry for drug discovery.

    

### [[2102.11203] A Theory of Label Propagation for Subpopulation Shift](http://arxiv.org/abs/2102.11203)


  One of the central problems in machine learning is domain adaptation. Unlike
past theoretical work, we consider a new model for subpopulation shift in the
input or representation space. In this work, we propose a provably effective
framework for domain adaptation based on label propagation. In our analysis, we
use a simple but realistic expansion assumption, proposed in
\citet{wei2021theoretical}. Using a teacher classifier trained on the source
domain, our algorithm not only propagates to the target domain but also
improves upon the teacher. By leveraging existing generalization bounds, we
also obtain end-to-end finite-sample guarantees on the entire algorithm. In
addition, we extend our theoretical framework to a more general setting of
source-to-target transfer based on a third unlabeled dataset, which can be
easily applied in various learning scenarios. Inspired by our theory, we adapt
consistency-based semi-supervised learning methods to domain adaptation
settings and gain significant improvements.

    

### [[2102.11271] Reinforcement Learning with Prototypical Representations](http://arxiv.org/abs/2102.11271)


  Learning effective representations in image-based environments is crucial for
sample efficient Reinforcement Learning (RL). Unfortunately, in RL,
representation learning is confounded with the exploratory experience of the
agent -- learning a useful representation requires diverse data, while
effective exploration is only possible with coherent representations.
Furthermore, we would like to learn representations that not only generalize
across tasks but also accelerate downstream exploration for efficient
task-specific training. To address these challenges we propose Proto-RL, a
self-supervised framework that ties representation learning with exploration
through prototypical representations. These prototypes simultaneously serve as
a summarization of the exploratory experience of an agent as well as a basis
for representing observations. We pre-train these task-agnostic representations
and prototypes on environments without downstream task information. This
enables state-of-the-art downstream policy learning on a set of difficult
continuous control tasks.

    

### [[2102.11289] Ps and Qs: Quantization-aware pruning for efficient low latency neural network inference](http://arxiv.org/abs/2102.11289)


  Efficient machine learning implementations optimized for inference in
hardware have wide-ranging benefits, depending on the application, from lower
inference latency to higher data throughput and reduced energy consumption. Two
popular techniques for reducing computation in neural networks are pruning,
removing insignificant synapses, and quantization, reducing the precision of
the calculations. In this work, we explore the interplay between pruning and
quantization during the training of neural networks for ultra low latency
applications targeting high energy physics use cases. Techniques developed for
this study have potential applications across many other domains. We study
various configurations of pruning during quantization-aware training, which we
term quantization-aware pruning, and the effect of techniques like
regularization, batch normalization, and different pruning schemes on
performance, computational complexity, and information content metrics. We find
that quantization-aware pruning yields more computationally efficient models
than either pruning or quantization alone for our task. Further,
quantization-aware pruning typically performs similar to or better in terms of
computational efficiency compared to other neural architecture search
techniques like Bayesian optimization. Surprisingly, while networks with
different training configurations can have similar performance for the
benchmark application, the information content in the network can vary
significantly, affecting its generalizability.

    

### [[2103.00222] Variational Laplace for Bayesian neural networks](http://arxiv.org/abs/2103.00222)


  We develop variational Laplace for Bayesian neural networks (BNNs) which
exploits a local approximation of the curvature of the likelihood to estimate
the ELBO without the need for stochastic sampling of the neural-network
weights. The Variational Laplace objective is simple to evaluate, as it is (in
essence) the log-likelihood, plus weight-decay, plus a squared-gradient
regularizer. Variational Laplace gave better test performance and expected
calibration errors than maximum a-posteriori inference and standard
sampling-based variational inference, despite using the same variational
approximate posterior. Finally, we emphasise care needed in benchmarking
standard VI as there is a risk of stopping before the variance parameters have
converged. We show that early-stopping can be avoided by increasing the
learning rate for the variance parameters.

    

### [[2103.00755] Adaptive Sampling for Minimax Fair Classification](http://arxiv.org/abs/2103.00755)


  Machine learning models trained on uncurated datasets can often end up
adversely affecting inputs belonging to underrepresented groups. To address
this issue, we consider the problem of adaptively constructing training sets
which allow us to learn classifiers that are fair in a minimax sense. We first
propose an adaptive sampling algorithm based on the principle of optimism, and
derive theoretical bounds on its performance. We also propose heuristic
extensions of this algorithm suitable for application to large scale, practical
problems. Next, by deriving algorithm independent lower-bounds for a specific
class of problems, we show that the performance achieved by our adaptive scheme
cannot be improved in general. We then validate the benefits of adaptively
constructing training sets via experiments on synthetic tasks with logistic
regression classifiers, as well as on several real-world tasks using
convolutional neural networks (CNNs).

    

### [[2103.01124] Automated data-driven approach for gap filling in the time series using evolutionary learning](http://arxiv.org/abs/2103.01124)


  In the paper, we propose an adaptive data-driven model-based approach for
filling the gaps in time series. The approach is based on the automated
evolutionary identification of the optimal structure for a composite
data-driven model. It allows adapting the model for the effective gap-filling
in a specific dataset without the involvement of the data scientist. As a case
study, both synthetic and real datasets from different fields (environmental,
economic, etc) are used. The experiments confirm that the proposed approach
allows achieving the higher quality of the gap restoration and improve the
effectiveness of forecasting models.

    

### [[2103.02913] Quantifying identifiability to choose and audit $ε$ in differentially private deep learning](http://arxiv.org/abs/2103.02913)


  Differential privacy allows bounding the influence that training data records
have on a machine learning model. To use differential privacy in machine
learning, data scientists must choose privacy parameters $(\epsilon,\delta)$.
Choosing meaningful privacy parameters is key, since models trained with weak
privacy parameters might result in excessive privacy leakage, while strong
privacy parameters might overly degrade model utility. However, privacy
parameter values are difficult to choose for two main reasons. First, the
theoretical upper bound on privacy loss $(\epsilon,\delta)$ might be loose,
depending on the chosen sensitivity and data distribution of practical
datasets. Second, legal requirements and societal norms for anonymization often
refer to individual identifiability, to which $(\epsilon,\delta)$ are only
indirectly related.
We transform $(\epsilon,\delta)$ to a bound on the Bayesian posterior belief
of the adversary assumed by differential privacy concerning the presence of any
record in the training dataset. The bound holds for multidimensional queries
under composition, and we show that it can be tight in practice. Furthermore,
we derive an identifiability bound, which relates the adversary assumed in
differential privacy to previous work on membership inference adversaries. We
formulate an implementation of this differential privacy adversary that allows
data scientists to audit model training and compute empirical identifiability
scores and empirical $(\epsilon,\delta)$.

    

### [[2103.05108] Believe The HiPe: Hierarchical Perturbation for Fast, Robust and Model-Agnostic Explanations](http://arxiv.org/abs/2103.05108)


  Understanding the predictions made by Artificial Intelligence (AI) systems is
becoming more and more important as deep learning models are used for
increasingly complex and high-stakes tasks. Saliency mapping - an easily
interpretable visual attribution method - is one important tool for this, but
existing formulations are limited by either computational cost or architectural
constraints. We therefore propose Hierarchical Perturbation, a very fast and
completely model-agnostic method for explaining model predictions with robust
saliency maps. Using standard benchmarks and datasets, we show that our
saliency maps are of competitive or superior quality to those generated by
existing model-agnostic methods - and are over 20X faster to compute.

    

### [[2103.08001] Claim Verification using a Multi-GAN based Model](http://arxiv.org/abs/2103.08001)


  This article describes research on claim verification carried out using a
multiple GAN-based model. The proposed model consists of three pairs of
generators and discriminators. The generator and discriminator pairs are
responsible for generating synthetic data for supported and refuted claims and
claim labels. A theoretical discussion about the proposed model is provided to
validate the equilibrium state of the model. The proposed model is applied to
the FEVER dataset, and a pre-trained language model is used for the input text
data. The synthetically generated data helps to gain information which helps
the model to perform better than state of the art models and other standard
classifiers.

    

### [[2103.09957] CheXbreak: Misclassification Identification for Deep Learning Models Interpreting Chest X-rays](http://arxiv.org/abs/2103.09957)


  A major obstacle to the integration of deep learning models for chest x-ray
interpretation into clinical settings is the lack of understanding of their
failure modes. In this work, we first investigate whether there are patient
subgroups that chest x-ray models are likely to misclassify. We find that
patient age and the radiographic finding of lung lesion, pneumothorax or
support devices are statistically relevant features for predicting
misclassification for some chest x-ray models. Second, we develop
misclassification predictors on chest x-ray models using their outputs and
clinical features. We find that our best performing misclassification
identifier achieves an AUROC close to 0.9 for most diseases. Third, employing
our misclassification identifiers, we develop a corrective algorithm to
selectively flip model predictions that have high likelihood of
misclassification at inference time. We observe F1 improvement on the
prediction of Consolidation (0.008 [95% CI 0.005, 0.010]) and Edema (0.003,
[95% CI 0.001, 0.006]). By carrying out our investigation on ten distinct and
high-performing chest x-ray models, we are able to derive insights across model
architectures and offer a generalizable framework applicable to other medical
imaging tasks.

    

### [[2103.10790] Quality Evolvability ES: Evolving Individuals With a Distribution of Well Performing and Diverse Offspring](http://arxiv.org/abs/2103.10790)


  One of the most important lessons from the success of deep learning is that
learned representations tend to perform much better at any task compared to
representations we design by hand. Yet evolution of evolvability algorithms,
which aim to automatically learn good genetic representations, have received
relatively little attention, perhaps because of the large amount of
computational power they require. The recent method Evolvability ES allows
direct selection for evolvability with little computation. However, it can only
be used to solve problems where evolvability and task performance are aligned.
We propose Quality Evolvability ES, a method that simultaneously optimizes for
task performance and evolvability and without this restriction. Our proposed
approach Quality Evolvability has similar motivation to Quality Diversity
algorithms, but with some important differences. While Quality Diversity aims
to find an archive of diverse and well-performing, but potentially genetically
distant individuals, Quality Evolvability aims to find a single individual with
a diverse and well-performing distribution of offspring. By doing so Quality
Evolvability is forced to discover more evolvable representations. We
demonstrate on robotic locomotion control tasks that Quality Evolvability ES,
similarly to Quality Diversity methods, can learn faster than objective-based
methods and can handle deceptive problems.

    

### [[2103.12177] Energy Disaggregation using Variational Autoencoders](http://arxiv.org/abs/2103.12177)


  Non-intrusive load monitoring (NILM) is a technique that uses a single sensor
to measure the total power consumption of a building. Using an energy
disaggregation method, the consumption of individual appliances can be
estimated from the aggregate measurement. Recent disaggregation algorithms have
significantly improved the performance of NILM systems. However, the
generalization capability of these methods to different houses as well as the
disaggregation of multi-state appliances are still major challenges. In this
paper we address these issues and propose an energy disaggregation approach
based on the variational autoencoders framework. The probabilistic encoder
makes this approach an efficient model for encoding information relevant to the
reconstruction of the target appliance consumption. In particular, the proposed
model accurately generates more complex load profiles, thus improving the power
signal reconstruction of multi-state appliances. Moreover, its regularized
latent space improves the generalization capabilities of the model across
different houses. The proposed model is compared to state-of-the-art NILM
approaches on the UK-DALE and REFIT datasets, and yields competitive results.
The mean absolute error reduces by 18% on average across all appliances
compared to the state-of-the-art. The F1-Score increases by more than 11%,
showing improvements for the detection of the target appliance in the aggregate
measurement.

    

### [[2103.13416] Mixture Density Network Estimation of Continuous Variable Maximum Likelihood Using Discrete Training Samples](http://arxiv.org/abs/2103.13416)


  Mixture Density Networks (MDNs) can be used to generate probability density
functions of model parameters $\boldsymbol{\theta}$ given a set of observables
$\mathbf{x}$. In some applications, training data are available only for
discrete values of a continuous parameter $\boldsymbol{\theta}$. In such
situations a number of performance-limiting issues arise which can result in
biased estimates. We demonstrate the usage of MDNs for parameter estimation,
discuss the origins of the biases, and propose a corrective method for each
issue.

    

### [[2104.01188] Scan Specific Artifact Reduction in K-space (SPARK) Neural Networks Synergize with Physics-based Reconstruction to Accelerate MRI](http://arxiv.org/abs/2104.01188)


  Purpose: To develop a scan-specific model that estimates and corrects k-space
errors made when reconstructing accelerated Magnetic Resonance Imaging (MRI)
data.
Methods: Scan-Specific Artifact Reduction in k-space (SPARK) trains a
convolutional-neural-network to estimate and correct k-space errors made by an
input reconstruction technique by back-propagating from the mean-squared-error
loss between an auto-calibration signal (ACS) and the input technique's
reconstructed ACS. First, SPARK is applied to GRAPPA and demonstrates improved
robustness over other scan-specific models, such as RAKI and residual-RAKI.
Subsequent experiments demonstrate that SPARK synergizes with residual-RAKI to
improve reconstruction performance. SPARK also improves reconstruction quality
when applied to advanced acquisition and reconstruction techniques like 2D
virtual coil (VC-) GRAPPA, 2D LORAKS, 3D GRAPPA without an integrated ACS
region, and 2D/3D wave-encoded images.
Results: SPARK yields 1.5x - 2x RMSE reduction when applied to GRAPPA and
improves robustness to ACS size for various acceleration rates in comparison to
other scan-specific techniques. When applied to advanced reconstruction
techniques such as residual-RAKI, 2D VC-GRAPPA and LORAKS, SPARK achieves up to
20% RMSE improvement. SPARK with 3D GRAPPA also improves performance by ~2x and
perceived image quality without a fully sampled ACS region. Finally, SPARK
synergizes with non-cartesian 2D and 3D wave-encoding imaging by reducing RMSE
between 20-25% and providing qualitative improvements.
Conclusion: SPARK synergizes with physics-based acquisition and
reconstruction techniques to improve accelerated MRI by training scan-specific
models to estimate and correct reconstruction errors in k-space.

    

### [[2104.10586] Mixture of Robust Experts (MoRE):A Robust Denoising Method towards multiple perturbations](http://arxiv.org/abs/2104.10586)


  To tackle the susceptibility of deep neural networks to examples, the
adversarial training has been proposed which provides a notion of robust
through an inner maximization problem presenting the first-order embedded
within the outer minimization of the training loss. To generalize the
adversarial robustness over different perturbation types, the adversarial
training method has been augmented with the improved inner maximization
presenting a union of multiple perturbations e.g., various $\ell_p$
norm-bounded perturbations.

    

### [[2104.12922] One Billion Audio Sounds from GPU-enabled Modular Synthesis](http://arxiv.org/abs/2104.12922)


  We release synth1B1, a multi-modal audio corpus consisting of 1 billion
4-second synthesized sounds, paired with the synthesis parameters used to
generate them. The dataset is 100x larger than any audio dataset in the
literature. We also introduce torchsynth, an open source modular synthesizer
that generates the synth1B1 samples on-the-fly at 16200x faster than real-time
(714MHz) on a single GPU. Finally, we release two new audio datasets: FM synth
timbre and subtractive synth pitch. Using these datasets, we demonstrate new
rank-based evaluation criteria for existing audio representations. Finally, we
propose a novel approach to synthesizer hyperparameter optimization.

    

### [[2105.00243] FedProto: Federated Prototype Learning over Heterogeneous Devices](http://arxiv.org/abs/2105.00243)


  The heterogeneity across devices usually hinders the optimization convergence
and generalization performance of federated learning (FL) when the aggregation
of devices' knowledge occurs in the gradient space. For example, devices may
differ in terms of data distribution, network latency, input/output space,
and/or model architecture, which can easily lead to the misalignment of their
local gradients. To improve the tolerance to heterogeneity, we propose a novel
federated prototype learning (FedProto) framework in which the devices and
server communicate the class prototypes instead of the gradients. FedProto
aggregates the local prototypes collected from different devices, and then
sends the global prototypes back to all devices to regularize the training of
local models. The training on each device aims to minimize the classification
error on the local data while keeping the resulting local prototypes
sufficiently close to the corresponding global ones. Through experiments, we
propose a benchmark setting tailored for heterogeneous FL, with FedProto
outperforming several recent FL approaches on multiple datasets.

    

### [[2105.00304] SpookyNet: Learning Force Fields with Electronic Degrees of Freedom and Nonlocal Effects](http://arxiv.org/abs/2105.00304)


  Machine-learned force fields (ML-FFs) combine the accuracy of ab initio
methods with the efficiency of conventional force fields. However, current
ML-FFs typically ignore electronic degrees of freedom, such as the total charge
or spin state, and assume chemical locality, which is problematic when
molecules have inconsistent electronic states, or when nonlocal effects play a
significant role. This work introduces SpookyNet, a deep neural network for
constructing ML-FFs with explicit treatment of electronic degrees of freedom
and quantum nonlocality. Chemically meaningful inductive biases and analytical
corrections built into the network architecture allow it to properly model
physical limits. SpookyNet improves upon the current state-of-the-art (or
achieves similar performance) on popular quantum chemistry data sets. Notably,
it is able to generalize across chemical and conformational space and can
leverage the learned chemical insights, e.g. by predicting unknown spin states,
thus helping to close a further important remaining gap for today's machine
learning models in quantum chemistry.

    

### [[2105.04656] Distribution-free calibration guarantees for histogram binning without sample splitting](http://arxiv.org/abs/2105.04656)


  We prove calibration guarantees for the popular histogram binning (also
called uniform-mass binning) method of Zadrozny and Elkan [2001]. Histogram
binning has displayed strong practical performance, but theoretical guarantees
have only been shown for sample split versions that avoid 'double dipping' the
data. We demonstrate that the statistical cost of sample splitting is
practically significant on a credit default dataset. We then prove calibration
guarantees for the original method that double dips the data, using a certain
Markov property of order statistics. Based on our results, we make practical
recommendations for choosing the number of bins in histogram binning. In our
illustrative simulations, we propose a new tool for assessing calibration --
validity plots -- which provide more information than an ECE estimate. Code for
this work will be made publicly available at
this https URL.

    

### [[2105.04754] Non-Parametric Estimation of Manifolds from Noisy Data](http://arxiv.org/abs/2105.04754)


  A common observation in data-driven applications is that high dimensional
data has a low intrinsic dimension, at least locally. In this work, we consider
the problem of estimating a $d$ dimensional sub-manifold of $\mathbb{R}^D$ from
a finite set of noisy samples. Assuming that the data was sampled uniformly
from a tubular neighborhood of $\mathcal{M}\in \mathcal{C}^k$, a compact
manifold without boundary, we present an algorithm that takes a point $r$ from
the tubular neighborhood and outputs $\hat p_n\in \mathbb{R}^D$, and
$\widehat{T_{\hat p_n}\mathcal{M}}$ an element in the Grassmanian $Gr(d, D)$.
We prove that as the number of samples $n\to\infty$ the point $\hat p_n$
converges to $p\in \mathcal{M}$ and $\widehat{T_{\hat p_n}\mathcal{M}}$
converges to $T_p\mathcal{M}$ (the tangent space at that point) with high
probability. Furthermore, we show that the estimation yields asymptotic rates
of convergence of $n^{-\frac{k}{2k + d}}$ for the point estimation and
$n^{-\frac{k-1}{2k + d}}$ for the estimation of the tangent space. These rates
are known to be optimal for the case of function estimation.

    

### [[2105.14686] Fully Hyperbolic Neural Networks](http://arxiv.org/abs/2105.14686)


  Hyperbolic neural networks have shown great potential for modeling complex
data. However, existing hyperbolic networks are not completely hyperbolic, as
they encode features in a hyperbolic space yet formalize most of their
operations in the tangent space (a Euclidean subspace) at the origin of the
hyperbolic space. This hybrid method greatly limits the modeling ability of
networks. In this paper, we propose a fully hyperbolic framework to build
hyperbolic networks based on the Lorentz model by adapting the Lorentz
transformations (including boost and rotation) to formalize essential
operations of neural networks. Moreover, we also prove that linear
transformation in tangent spaces used by existing hyperbolic networks is a
relaxation of the Lorentz rotation and does not include the boost, implicitly
limiting the capabilities of existing hyperbolic networks. The experimental
results on four NLP tasks show that our method has better performance for
building both shallow and deep networks. Our code will be released to
facilitate follow-up research.

    

### [[2106.02081] Solving Schrödinger Bridges via Maximum Likelihood](http://arxiv.org/abs/2106.02081)


  The Schrödinger bridge problem (SBP) finds the most likely stochastic
evolution between two probability distributions given a prior stochastic
evolution. As well as applications in the natural sciences, problems of this
kind have important applications in machine learning such as dataset alignment
and hypothesis testing. Whilst the theory behind this problem is relatively
mature, scalable numerical recipes to estimate the Schrödinger bridge remain
an active area of research. We prove an equivalence between the SBP and maximum
likelihood estimation enabling direct application of successful machine
learning techniques. We propose a numerical procedure to estimate SBPs using
Gaussian process and demonstrate the practical usage of our approach in
numerical simulations and experiments.

    

### [[2106.04426] Hash Layers For Large Sparse Models](http://arxiv.org/abs/2106.04426)


  We investigate the training of sparse layers that use different parameters
for different inputs based on hashing in large Transformer models.
Specifically, we modify the feedforward layer to hash to different sets of
weights depending on the current token, over all tokens in the sequence. We
show that this procedure either outperforms or is competitive with
learning-to-route mixture-of-expert methods such as Switch Transformers and
BASE Layers, while requiring no routing parameters or extra terms in the
objective function such as a load balancing loss, and no sophisticated
assignment algorithm. We study the performance of different hashing techniques,
hash sizes and input features, and show that balanced and random hashes focused
on the most local features work best, compared to either learning clusters or
using longer-range context. We show our approach works well both on large
language modeling and dialogue tasks, and on downstream fine-tuning tasks.

    

### [[2106.05319] Stein Latent Optimization for GANs](http://arxiv.org/abs/2106.05319)


  Generative adversarial networks (GANs) with clustered latent spaces can
perform conditional generation in a completely unsupervised manner. However,
the salient attributes of unlabeled data in the real-world are mostly
imbalanced. Existing unsupervised conditional GANs cannot properly cluster the
attributes in their latent spaces because they assume uniform distributions of
the attributes. To address this problem, we theoretically derive Stein latent
optimization that provides reparameterizable gradient estimations of the latent
distribution parameters assuming a Gaussian mixture prior in a continuous
latent space. Structurally, we introduce an encoder network and a novel
contrastive loss to help generated data from a single mixture component to
represent a single attribute. We confirm that the proposed method, named Stein
Latent Optimization for GANs (SLOGAN), successfully learns the balanced or
imbalanced attributes and performs unsupervised tasks such as unsupervised
conditional generation, unconditional generation, and cluster assignment even
in the absence of information of the attributes (e.g. the imbalance ratio).
Moreover, we demonstrate that the attributes to be learned can be manipulated
using a small amount of probe data.

    

### [[2106.06639] Federated Learning with Buffered Asynchronous Aggregation](http://arxiv.org/abs/2106.06639)


  Federated Learning (FL) trains a shared model across distributed devices
while keeping the training data on the devices. Most FL schemes are
synchronous: they perform a synchronized aggregation of model updates from
individual devices. Synchronous training can be slow because of late-arriving
devices (stragglers). On the other hand, completely asynchronous training makes
FL less private because of incompatibility with secure aggregation. In this
work, we propose a model aggregation scheme, FedBuff, that combines the best
properties of synchronous and asynchronous FL. Similar to synchronous FL,
FedBuff is compatible with secure aggregation. Similar to asynchronous FL,
FedBuff is robust to stragglers. In FedBuff, clients trains asynchronously and
send updates to the server. The server aggregates client updates in a private
buffer until updates have been received, at which point a server model update
is immediately performed. We provide theoretical convergence guarantees for
FedBuff in a non-convex setting. Empirically, FedBuff converges up to 3.8x
faster than previous proposals for synchronous FL (e.g., FedAvgM), and up to
2.5x faster than previous proposals for asynchronous FL (e.g., FedAsync). We
show that FedBuff is robust to different staleness distributions and is more
scalable than synchronous FL techniques.

    

### [[2106.08285] Multi-StyleGAN: Towards Image-Based Simulation of Time-Lapse Live-Cell Microscopy](http://arxiv.org/abs/2106.08285)


  Time-lapse fluorescent microscopy (TLFM) combined with predictive
mathematical modelling is a powerful tool to study the inherently dynamic
processes of life on the single-cell level. Such experiments are costly,
complex and labour intensive. A complimentary approach and a step towards in
silico experimentation, is to synthesise the imagery itself. Here, we propose
Multi-StyleGAN as a descriptive approach to simulate time-lapse fluorescence
microscopy imagery of living cells, based on a past experiment. This novel
generative adversarial network synthesises a multi-domain sequence of
consecutive timesteps. We showcase Multi-StyleGAN on imagery of multiple live
yeast cells in microstructured environments and train on a dataset recorded in
our laboratory. The simulation captures underlying biophysical factors and time
dependencies, such as cell morphology, growth, physical interactions, as well
as the intensity of a fluorescent reporter protein. An immediate application is
to generate additional training and validation data for feature extraction
algorithms or to aid and expedite development of advanced experimental
techniques such as online monitoring or control of cells.
Code and dataset is available at
this https URL.

    

### [[2106.08909] Offline RL Without Off-Policy Evaluation](http://arxiv.org/abs/2106.08909)


  Most prior approaches to offline reinforcement learning (RL) have taken an
iterative actor-critic approach involving off-policy evaluation. In this paper
we show that simply doing one step of constrained/regularized policy
improvement using an on-policy Q estimate of the behavior policy performs
surprisingly well. This one-step algorithm beats the previously reported
results of iterative algorithms on a large portion of the D4RL benchmark. The
simple one-step baseline achieves this strong performance without many of the
tricks used by previously proposed iterative algorithms and is more robust to
hyperparameters. We argue that the relatively poor performance of iterative
approaches is a result of the high variance inherent in doing off-policy
evaluation and magnified by the repeated optimization of policies against those
high-variance estimates. In addition, we hypothesize that the strong
performance of the one-step algorithm is due to a combination of favorable
structure in the environment and behavior policy.

    

### [[2106.11858] MEAL: Manifold Embedding-based Active Learning](http://arxiv.org/abs/2106.11858)


  Image segmentation is a common and challenging task in autonomous driving.
Availability of sufficient pixel-level annotations for the training data is a
hurdle. Active learning helps learning from small amounts of data by suggesting
the most promising samples for labeling. In this work, we propose a new
pool-based method for active learning, which proposes promising patches
extracted from full image, in each acquisition step. The problem is framed in
an exploration-exploitation framework by combining an embedding based on
Uniform Manifold Approximation to model representativeness with entropy as
uncertainty measure to model informativeness. We applied our proposed method to
the autonomous driving datasets CamVid and Cityscapes and performed a
quantitative comparison with state-of-the-art baselines. We find that our
active learning method achieves better performance compared to previous
methods.

    

### [[2106.13792] Proxy Convexity: A Unified Framework for the Analysis of Neural Networks Trained by Gradient Descent](http://arxiv.org/abs/2106.13792)


  Although the optimization objectives for learning neural networks are highly
non-convex, gradient-based methods have been wildly successful at learning
neural networks in practice. This juxtaposition has led to a number of recent
studies on provable guarantees for neural networks trained by gradient descent.
Unfortunately, the techniques in these works are often highly specific to the
problem studied in each setting, relying on different assumptions on the
distribution, optimization parameters, and network architectures, making it
difficult to generalize across different settings. In this work, we propose a
unified non-convex optimization framework for the analysis of neural network
training. We introduce the notions of proxy convexity and proxy
Polyak-Lojasiewicz (PL) inequalities, which are satisfied if the original
objective function induces a proxy objective function that is implicitly
minimized when using gradient methods. We show that stochastic gradient descent
(SGD) on objectives satisfying proxy convexity or the proxy PL inequality leads
to efficient guarantees for proxy objective functions. We further show that
many existing guarantees for neural networks trained by gradient descent can be
unified through proxy convexity and proxy PL inequalities.

    

### [[2107.09178] Compute RAMs: Adaptable Compute and Storage Blocks for DL-Optimized FPGAs](http://arxiv.org/abs/2107.09178)


  The configurable building blocks of current FPGAs -- Logic blocks (LBs),
Digital Signal Processing (DSP) slices, and Block RAMs (BRAMs) -- make them
efficient hardware accelerators for the rapid-changing world of Deep Learning
(DL). Communication between these blocks happens through an interconnect fabric
consisting of switching elements spread throughout the FPGA. In this paper, a
new block, Compute RAM, is proposed. Compute RAMs provide highly-parallel
processing-in-memory (PIM) by combining computation and storage capabilities in
one block. Compute RAMs can be integrated in the FPGA fabric just like the
existing FPGA blocks and provide two modes of operation (storage or compute)
that can be dynamically chosen. They reduce power consumption by reducing data
movement, provide adaptable precision support, and increase the effective
on-chip memory bandwidth. Compute RAMs also help increase the compute density
of FPGAs. In our evaluation of addition, multiplication and dot-product
operations across multiple data precisions (int4, int8 and bfloat16), we
observe an average savings of 80% in energy consumption, and an improvement in
execution time ranging from 20% to 80%. Adding Compute RAMs can benefit non-DL
applications as well, and make FPGAs more efficient, flexible, and performant
accelerators.

    

### [[2107.09245] MUSE: Multi-Use Error Correcting Codes](http://arxiv.org/abs/2107.09245)


  In this work we present a new set of error correcting codes -- Multi-Use
Error Correcting Codes (MUSE ECC) -- that have the ability to match reliability
guarantees of all commodity, conventional state-of-the-art ECC with fewer bits
of storage. MUSE ECC derives its power by building on arithmetic coding methods
(first used in an experimental system in 1960s). We show that our MUSE
construction can be used as a "drop in" replacement within error correction
frameworks used widely today. Further, we show how MUSE is a promising fit for
emerging technologies such as a DDR5 memories. Concretely, all instantiations
of MUSE we show in this paper offer 100% Single Error Correction, and multi-bit
error detection between 70% and 95% while using fewer check bits. MUSE ECC
corrects failure of a single chip on a DIMM with check bit space savings of
12.5% compared to conventional techniques. The performance overheads, if any,
are negligible. Our results open the possibility of reusing ECC storage for
things beyond reliability without compromising reliability, thus solving a
40-year-old puzzle.

    

### [[2107.09333] StreamBlocks: A compiler for heterogeneous dataflow computing (technical report)](http://arxiv.org/abs/2107.09333)


  To increase performance and efficiency, systems use FPGAs as reconfigurable
accelerators. A key challenge in designing these systems is partitioning
computation between processors and an FPGA. An appropriate division of labor
may be difficult to predict in advance and require experiments and
measurements. When an investigation requires rewriting part of the system in a
new language or with a new programming model, its high cost can retard the
study of different configurations. A single-language system with an appropriate
programming model and compiler that targets both platforms simplifies this
exploration to a simple recompile with new compiler directives.
This work introduces StreamBlocks, an open-source compiler and runtime that
uses the CAL dataflow programming language to partition computations across
heterogeneous (CPU/accelerator) platforms. Because of the dataflow model's
semantics and the CAL language, StreamBlocks can exploit both thread
parallelism in multi-core CPUs and the inherent parallelism of FPGAs.
StreamBlocks supports exploring the design space with a profile-guided tool
that helps identify the best hardware-software partitions.

    

### [[2107.09448] DNN is not all you need: Parallelizing Non-Neural ML Algorithms on Ultra-Low-Power IoT Processors](http://arxiv.org/abs/2107.09448)


  Machine Learning (ML) functions are becoming ubiquitous in latency- and
privacy-sensitive IoT applications, prompting for a shift toward near-sensor
processing at the extreme edge and the consequent increasing adoption of
Parallel Ultra-Low Power (PULP) IoT processors. These compute- and
memory-constrained parallel architectures need to run efficiently a wide range
of algorithms, including key Non-Neural ML kernels that compete favorably with
Deep Neural Networks (DNNs) in terms of accuracy under severe resource
constraints. In this paper, we focus on enabling efficient parallel execution
of Non-Neural ML algorithms on two RISCV-based PULP platforms, namely GAP8, a
commercial chip, and PULP-OPEN, a research platform running on an FPGA
emulator. We optimized the parallel algorithms through a fine-grained analysis
and intensive optimization to maximize the speedup, considering two alternative
Floating-Point (FP) emulation libraries on GAP8 and the native FPU support on
PULP-OPEN. Experimental results show that a target-optimized emulation library
can lead to an average 1.61x runtime improvement compared to a standard
emulation library, while the native FPU support reaches up to 32.09x. In terms
of parallel speedup, our design improves the sequential execution by 7.04x on
average on the targeted octa-core platforms. Lastly, we present a comparison
with the ARM Cortex-M4 microcontroller (MCU), a widely adopted commercial
solution for edge deployments, which is 12.87$x slower than PULP-OPEN.

    

### [[2107.09500] Domino: A Tailored Network-on-Chip Architecture to Enable Highly Localized Inter- and Intra-Memory DNN Computing](http://arxiv.org/abs/2107.09500)


  The ever-increasing computation complexity of fast-growing Deep Neural
Networks (DNNs) has requested new computing paradigms to overcome the memory
wall in conventional Von Neumann computing architectures. The emerging
Computing-In-Memory (CIM) architecture has been a promising candidate to
accelerate neural network computing. However, the data movement between CIM
arrays may still dominate the total power consumption in conventional designs.
This paper proposes a flexible CIM processor architecture named Domino to
enable stream computing and local data access to significantly reduce the data
movement energy. Meanwhile, Domino employs tailored distributed instruction
scheduling within Network-on-Chip (NoC) to implement inter-memory-computing and
attain mapping flexibility. The evaluation with prevailing CNN models shows
that Domino achieves 1.15-to-9.49$\times$ power efficiency over several
state-of-the-art CIM accelerators and improves the throughput by
1.57-to-12.96$\times$.

    

### [[2107.09308] Online Deployment Algorithms for Microservice Systems with Complex Dependencies](http://arxiv.org/abs/2107.09308)


  Cloud and edge computing have been widely adopted in many application
scenarios. With the increasing demand of fast iteration and complexity of
business logic, it is challenging to achieve rapid development and continuous
delivery in such highly distributed cloud and edge computing environment. At
present, microservice-based architecture has been the dominant deployment
style, and a microservice system has to evolve agilely to offer stable Quality
of Service (QoS) in the situation where user requirement changes frequently.
Many research have been conducted to optimally re-deploy microservices to adapt
to changing requirements. Nevertheless, complex dependencies between
microservices and the existence of multiple instances of one single
microservice in a microservice system have not been fully considered in
existing works. This paper defines SPPMS, the Service Placement Problem in
Microservice Systems that feature complex dependencies and multiple instances,
as a Fractional Polynomial Problem (FPP) . Considering the high computation
complexity of FPP, it is then transformed into a Quadratic Sum-of-Ratios
Fractional Problem (QSRFP) which is further solved by the proposed greedy-based
algorithms. Experiments demonstrate that our models and algorithms outperform
existing approaches in both quality and computation speed.

    

### [[2107.09657] A New Design Framework for Heterogeneous Uncoded Storage Elastic Computing](http://arxiv.org/abs/2107.09657)


  Elasticity is one important feature in modern cloud computing systems and can
result in computation failure or significantly increase computing time. Such
elasticity means that virtual machines over the cloud can be preempted under a
short notice (e.g., hours or minutes) if a high-priority job appears; on the
other hand, new virtual machines may become available over time to compensate
the computing resources. Coded Storage Elastic Computing (CSEC) introduced by
Yang et al. in 2018 is an effective and efficient approach to overcome the
elasticity and it costs relatively less storage and computation load. However,
one of the limitations of the CSEC is that it may only be applied to certain
types of computations (e.g., linear) and may be challenging to be applied to
more involved computations because the coded data storage and approximation are
often needed. Hence, it may be preferred to use uncoded storage by directly
copying data into the virtual machines. In addition, based on our own
measurement, virtual machines on Amazon EC2 clusters often have heterogeneous
computation speed even if they have exactly the same configurations (e.g., CPU,
RAM, I/O cost). In this paper, we introduce a new optimization framework on
Uncoded Storage Elastic Computing (USEC) systems with heterogeneous computing
speed to minimize the overall computation time. Under this framework, we
propose optimal solutions of USEC systems with or without straggler tolerance
using different storage placements. Our proposed algorithms are evaluated using
power iteration applications on Amazon EC2.

    

### [[1901.02303] PMU-based Distributed Non-iterative Algorithm for Real-time Voltage Stability Monitoring](http://arxiv.org/abs/1901.02303)


  The Phasor measurement unit (PMU) measurements are mandatory to monitor the
power system's voltage stability margin in an online manner. Monitoring is key
to the secure operation of the grid. Traditionally, online monitoring of
voltage stability using synchrophasors required a centralized communication
architecture, which leads to high investment cost and cyber-security concerns.
The increasing importance of cyber-security and low investment cost have
recently led to the development of distributed algorithms for online monitoring
of the grid that are inherently less prone to malicious attacks. In this work,
a novel distributed non-iterative voltage stability index (VSI) is proposed by
recasting the power flow equations as circles. The online computations of VSI
are simultaneously performed by the processors embedded at each bus in the
smart grid with the help of PMUs and communication of voltage phasors between
neighboring buses. The distributed nature of the index enables the real-time
identification of the critical bus of the system with minimal communication
infrastructure. The effectiveness of the proposed distributed index is
demonstrated on IEEE test systems and contrasted with existing methods to show
the benefits of the proposed method in speed, interpretability, identification
of outage location, and low sensitivity to noisy measurements.

    

### [[1908.06394] Nakamoto Consensus with Verifiable Delay Puzzle](http://arxiv.org/abs/1908.06394)


  This paper presents a new consensus protocol based on verifiable delay
function. First, we introduce the concept of verifiable delay puzzle (VDP),
which resembles the hashing puzzle used in the PoW mechanism but can only be
solved sequentially. We then present a VDP implementation based on the
continuous verifiable delay function. Further, we show that VDP can be combined
with the Nakamoto consensus in a proof-of-stake/proof-of-delay hybrid protocol.
We analyze the persistence and liveness of the protocol, and show that compared
to PoW, our proposal consumes much less energy; compared to BFT leader-election
based consensus algorithms, our proposal achieves better resistance to
long-range attacks and DoS attacks targeting the block proposers.

    

### [[2010.05447] Scalable Consensus Protocols for PoW based Blockchain and blockDAG](http://arxiv.org/abs/2010.05447)


  In this paper, we propose two models for scaling the transaction throughput
in Proof-of-Work (PoW) based blockchain networks. In the first approach, a
mathematical model has derived for optimal transaction throughput for PoW based
longest chain rule blockchain. In this approach, the blockchain Peer-to-Peer
(P2P) network is considered as Erdös-Rényi random network topology. This
approach is however limited by the block creation rate, the results suggest
that the rate beyond an optimal point can result in unfairness in the system.
The second approach is a new consensus protocol proposed by considering the
ledger as a Directed Acyclic Graph (DAG) called blockDAG instead of a chain of
blocks. In this framework, we follow a two-step strategy that makes the system
robust enough to handle the double-spend attacks. The first step involves the
development of an unsupervised learning graph clustering algorithm for
separating the blocks created by an attacker. In the second step, the attackers
blocks are eliminated and the remaining blocks are arranged in topological
order by honest clients which makes the blockDAG system suitable for smart
contract applications found in Internet of Things (IoT) services. The
Simulation results demonstrate a significant improvement in the transaction
throughput compared to bitcoin.

    

### [[2107.09119] Rational Verification for Probabilistic Systems](http://arxiv.org/abs/2107.09119)


  Rational verification is the problem of determining which temporal logic
properties will hold in a multi-agent system, under the assumption that agents
in the system act rationally, by choosing strategies that collectively form a
game-theoretic equilibrium. Previous work in this area has largely focussed on
deterministic systems. In this paper, we develop the theory and algorithms for
rational verification in probabilistic systems. We focus on concurrent
stochastic games (CSGs), which can be used to model uncertainty and randomness
in complex multi-agent environments. We study the rational verification problem
for both non-cooperative games and cooperative games in the qualitative
probabilistic setting. In the former case, we consider LTL properties satisfied
by the Nash equilibria of the game and in the latter case LTL properties
satisfied by the core. In both cases, we show that the problem is
2EXPTIME-complete, thus not harder than the much simpler verification problem
of model checking LTL properties of systems modelled as Markov decision
processes (MDPs).

    

### [[2107.09129] ThingFO v1.2's Terms, Properties, Relationships and Axioms -- Foundational Ontology for Things](http://arxiv.org/abs/2107.09129)


  The present preprint specifies and defines all Terms, Properties,
Relationships and Axioms of ThingFO (Thing Foundational Ontology) v1.2, which
is a slightly updated version of its predecessor, ThingFO v1.1. It is an
ontology for particular and universal Things placed at the foundational level
in the context of a four-layered ontological architecture named FCD-OntoArch
(Foundational, Core, and Domain Ontological Architecture for Sciences). This is
a five-layered ontological architecture, which considers Foundational, Core,
Domain and Instance levels. In turn, the domain level is split down in two
sub-levels, namely: Top-domain and Low-domain. Ontologies at the same level can
be related to each other, except for the foundational level where only the
ThingFO ontology is. In addition, ontologies' terms and relationships at lower
levels can be semantically enriched by ontologies' terms and relationships from
the higher levels. ThingFO and ontologies at the core level such as
SituationCO, ProcessCO, ProjectCO, among others, are domain independent.
ThingFO is made up of three main concepts, namely: Thing with the semantics of
Particular, Thing Category with the semantics of Universal, and Assertion that
represents human statements about different aspects of Particulars and
Universals. Note that annotations of updates from the previous version (v1.1)
to the current one (v1.2) can be found in Appendix A.

    

### [[2107.09134] Convolutional module for heart localization and segmentation in MRI](http://arxiv.org/abs/2107.09134)


  Magnetic resonance imaging (MRI) is a widely known medical imaging technique
used to assess the heart function. Deep learning (DL) models perform several
tasks in cardiac MRI (CMR) images with good efficacy, such as segmentation,
estimation, and detection of diseases. Many DL models based on convolutional
neural networks (CNN) were improved by detecting regions-of-interest (ROI)
either automatically or by hand. In this paper we describe Visual-Motion-Focus
(VMF), a module that detects the heart motion in the 4D MRI sequence, and
highlights ROIs by focusing a Radial Basis Function (RBF) on the estimated
motion field. We experimented and evaluated VMF on three CMR datasets,
observing that the proposed ROIs cover 99.7% of data labels (Recall score),
improved the CNN segmentation (mean Dice score) by 1.7 (p < .001) after the ROI
extraction, and improved the overall training speed by 2.5 times (+150%).

    

### [[2107.09244] S2Looking: A Satellite Side-Looking Dataset for Building Change Detection](http://arxiv.org/abs/2107.09244)


  Collecting large-scale annotated satellite imagery datasets is essential for
deep-learning-based global building change surveillance. In particular, the
scroll imaging mode of optical satellites enables larger observation ranges and
shorter revisit periods, facilitating efficient global surveillance. However,
the images in recent satellite change detection datasets are mainly captured at
near-nadir viewing angles. In this paper, we introduce S2Looking, a building
change detection dataset that contains large-scale side-looking satellite
images captured at varying off-nadir angles. Our S2Looking dataset consists of
5000 registered bitemporal image pairs (size of 1024*1024, 0.5 ~ 0.8 m/pixel)
of rural areas throughout the world and more than 65,920 annotated change
instances. We provide two label maps to separately indicate the newly built and
demolished building regions for each sample in the dataset. We establish a
benchmark task based on this dataset, i.e., identifying the pixel-level
building changes in the bi-temporal images. We test several state-of-the-art
methods on both the S2Looking dataset and the (near-nadir) LEVIR-CD+ dataset.
The experimental results show that recent change detection methods exhibit much
poorer performance on the S2Looking than on LEVIR-CD+. The proposed S2Looking
dataset presents three main challenges: 1) large viewing angle changes, 2)
large illumination variances and 3) various complex scene characteristics
encountered in rural areas. Our proposed dataset may promote the development of
algorithms for satellite image change detection and registration under
conditions of large off-nadir angles. The dataset is available at
this https URL.

    

### [[2107.09285] Neural Abstructions: Abstractions that Support Construction for Grounded Language Learning](http://arxiv.org/abs/2107.09285)


  Although virtual agents are increasingly situated in environments where
natural language is the most effective mode of interaction with humans, these
exchanges are rarely used as an opportunity for learning. Leveraging language
interactions effectively requires addressing limitations in the two most common
approaches to language grounding: semantic parsers built on top of fixed object
categories are precise but inflexible and end-to-end models are maximally
expressive, but fickle and opaque. Our goal is to develop a system that
balances the strengths of each approach so that users can teach agents new
instructions that generalize broadly from a single example. We introduce the
idea of neural abstructions: a set of constraints on the inference procedure of
a label-conditioned generative model that can affect the meaning of the label
in context. Starting from a core programming language that operates over
abstructions, users can define increasingly complex mappings from natural
language to actions. We show that with this method a user population is able to
build a semantic parser for an open-ended house modification task in Minecraft.
The semantic parser that results is both flexible and expressive: the
percentage of utterances sourced from redefinitions increases steadily over the
course of 191 total exchanges, achieving a final value of 28%.

    

### [[2107.09288] MIMO: Mutual Integration of Patient Journey and Medical Ontology for Healthcare Representation Learning](http://arxiv.org/abs/2107.09288)


  Healthcare representation learning on the Electronic Health Record (EHR) is
seen as crucial for predictive analytics in the medical field. Many natural
language processing techniques, such as word2vec, RNN and self-attention, have
been adapted for use in hierarchical and time stamped EHR data, but fail when
they lack either general or task-specific data. Hence, some recent works train
healthcare representations by incorporating medical ontology (a.k.a. knowledge
graph), by self-supervised tasks like diagnosis prediction, but (1) the
small-scale, monotonous ontology is insufficient for robust learning, and (2)
critical contexts or dependencies underlying patient journeys are never
exploited to enhance ontology learning. To address this, we propose an
end-to-end robust Transformer-based solution, Mutual Integration of patient
journey and Medical Ontology (MIMO) for healthcare representation learning and
predictive analytics. Specifically, it consists of task-specific representation
learning and graph-embedding modules to learn both patient journey and medical
ontology interactively. Consequently, this creates a mutual integration to
benefit both healthcare representation learning and medical ontology embedding.
Moreover, such integration is achieved by a joint training of both
task-specific predictive and ontology-based disease typing tasks based on fused
embeddings of the two modules. Experiments conducted on two real-world
diagnosis prediction datasets show that, our healthcare representation model
MIMO not only achieves better predictive results than previous state-of-the-art
approaches regardless of sufficient or insufficient training data, but also
derives more interpretable embeddings of diagnoses.

    

### [[2107.09332] Improving Sentence-Level Relation Extraction through Curriculum Learning](http://arxiv.org/abs/2107.09332)


  The sentence-level relation extraction mainly aims to classify the relation
between two entities in a sentence. The sentence-level relation extraction
corpus is often containing data of difficulty for the model to infer or noise
data. In this paper, we propose a curriculum learning-based relation extraction
model that split data by difficulty and utilize it for learning. In the
experiments with the representative sentence-level relation extraction
datasets, TACRED and Re-TACRED, the proposed method showed good performances.

    

### [[2107.09352] Similarity metrics for Different Market Scenarios in Abides](http://arxiv.org/abs/2107.09352)


  Markov Decision Processes (MDPs) are an effective way to formally describe
many Machine Learning problems. In fact, recently MDPs have also emerged as a
powerful framework to model financial trading tasks. For example, financial
MDPs can model different market scenarios. However, the learning of a
(near-)optimal policy for each of these financial MDPs can be a very
time-consuming process, especially when nothing is known about the policy to
begin with. An alternative approach is to find a similar financial MDP for
which we have already learned its policy, and then reuse such policy in the
learning of a new policy for a new financial MDP. Such a knowledge transfer
between market scenarios raises several issues. On the one hand, how to measure
the similarity between financial MDPs. On the other hand, how to use this
similarity measurement to effectively transfer the knowledge between financial
MDPs. This paper addresses both of these issues. Regarding the first one, this
paper analyzes the use of three similarity metrics based on conceptual,
structural and performance aspects of the financial MDPs. Regarding the second
one, this paper uses Probabilistic Policy Reuse to balance the
exploitation/exploration in the learning of a new financial MDP according to
the similarity of the previous financial MDPs whose knowledge is reused.

    

### [[2107.09540] Critic Guided Segmentation of Rewarding Objects in First-Person Views](http://arxiv.org/abs/2107.09540)


  This work discusses a learning approach to mask rewarding objects in images
using sparse reward signals from an imitation learning dataset. For that, we
train an Hourglass network using only feedback from a critic model. The
Hourglass network learns to produce a mask to decrease the critic's score of a
high score image and increase the critic's score of a low score image by
swapping the masked areas between these two images. We trained the model on an
imitation learning dataset from the NeurIPS 2020 MineRL Competition Track,
where our model learned to mask rewarding objects in a complex interactive 3D
environment with a sparse reward signal. This approach was part of the 1st
place winning solution in this competition. Video demonstration and code:
this https URL


### [[2107.09556] WikiGraphs: A Wikipedia Text - Knowledge Graph Paired Dataset](http://arxiv.org/abs/2107.09556)


  We present a new dataset of Wikipedia articles each paired with a knowledge
graph, to facilitate the research in conditional text generation, graph
generation and graph representation learning. Existing graph-text paired
datasets typically contain small graphs and short text (1 or few sentences),
thus limiting the capabilities of the models that can be learned on the data.
Our new dataset WikiGraphs is collected by pairing each Wikipedia article from
the established WikiText-103 benchmark (Merity et al., 2016) with a subgraph
from the Freebase knowledge graph (Bollacker et al., 2008). This makes it easy
to benchmark against other state-of-the-art text generative models that are
capable of generating long paragraphs of coherent text. Both the graphs and the
text data are of significantly larger scale compared to prior graph-text paired
datasets. We present baseline graph neural network and transformer model
results on our dataset for 3 tasks: graph -> text generation, graph -> text
retrieval and text -> graph retrieval. We show that better conditioning on the
graph provides gains in generation and retrieval quality but there is still
large room for improvement.

    

### [[2107.09574] Accelerating Edge Intelligence via Integrated Sensing and Communication](http://arxiv.org/abs/2107.09574)


  Realizing edge intelligence consists of sensing, communication, training, and
inference stages. Conventionally, the sensing and communication stages are
executed sequentially, which results in excessive amount of dataset generation
and uploading time. This paper proposes to accelerate edge intelligence via
integrated sensing and communication (ISAC). As such, the sensing and
communication stages are merged so as to make the best use of the wireless
signals for the dual purpose of dataset generation and uploading. However, ISAC
also introduces additional interference between sensing and communication
functionalities. To address this challenge, this paper proposes a
classification error minimization formulation to design the ISAC beamforming
and time allocation. Globally optimal solution is derived via the rank-1
guaranteed semidefinite relaxation, and performance analysis is performed to
quantify the ISAC gain. Simulation results are provided to verify the
effectiveness of the proposed ISAC scheme. Interestingly, it is found that when
the sensing time dominates the communication time, ISAC is always beneficial.
However, when the communication time dominates, the edge intelligence with ISAC
scheme may not be better than that with the conventional scheme, since ISAC
introduces harmful interference between the sensing and communication signals.

    

### [[2107.09579] Semantic Reasoning with Differentiable Graph Transformations](http://arxiv.org/abs/2107.09579)


  This paper introduces a differentiable semantic reasoner, where rules are
presented as a relevant set of graph transformations. These rules can be
written manually or inferred by a set of facts and goals presented as a
training set. While the internal representation uses embeddings in a latent
space, each rule can be expressed as a set of predicates conforming to a subset
of Description Logic.

    

### [[2107.09591] Hybrid neural network reduced order modelling for turbulent flows with geometric parameters](http://arxiv.org/abs/2107.09591)


  Geometrically parametrized Partial Differential Equations are nowadays widely
used in many different fields as, for example, shape optimization processes or
patient specific surgery studies. The focus of this work is on some advances
for this topic, capable of increasing the accuracy with respect to previous
approaches while relying on a high cost-benefit ratio performance. The main
scope of this paper is the introduction of a new technique mixing up a
classical Galerkin-projection approach together with a data-driven method to
obtain a versatile and accurate algorithm for the resolution of geometrically
parametrized incompressible turbulent Navier-Stokes problems. The effectiveness
of this procedure is demonstrated on two different test cases: a classical
academic back step problem and a shape deformation Ahmed body application. The
results show into details the properties of the architecture we developed while
exposing possible future perspectives for this work.

    

### [[2107.09600] DSP: Dual Soft-Paste for Unsupervised Domain Adaptive Semantic Segmentation](http://arxiv.org/abs/2107.09600)


  Unsupervised domain adaptation (UDA) for semantic segmentation aims to adapt
a segmentation model trained on the labeled source domain to the unlabeled
target domain. Existing methods try to learn domain invariant features while
suffering from large domain gaps that make it difficult to correctly align
discrepant features, especially in the initial training phase. To address this
issue, we propose a novel Dual Soft-Paste (DSP) method in this paper.
Specifically, DSP selects some classes from a source domain image using a
long-tail class first sampling strategy and softly pastes the corresponding
image patch on both the source and target training images with a fusion weight.
Technically, we adopt the mean teacher framework for domain adaptation, where
the pasted source and target images go through the student network while the
original target image goes through the teacher network. Output-level alignment
is carried out by aligning the probability maps of the target fused image from
both networks using a weighted cross-entropy loss. In addition, feature-level
alignment is carried out by aligning the feature maps of the source and target
images from student network using a weighted maximum mean discrepancy loss. DSP
facilitates the model learning domain-invariant features from the intermediate
domains, leading to faster convergence and better performance. Experiments on
two challenging benchmarks demonstrate the superiority of DSP over
state-of-the-art methods. Code is available at
\url{this https URL}.

    

### [[2107.09609] QVHighlights: Detecting Moments and Highlights in Videos via Natural Language Queries](http://arxiv.org/abs/2107.09609)


  Detecting customized moments and highlights from videos given natural
language (NL) user queries is an important but under-studied topic. One of the
challenges in pursuing this direction is the lack of annotated data. To address
this issue, we present the Query-based Video Highlights (QVHighlights) dataset.
It consists of over 10,000 YouTube videos, covering a wide range of topics,
from everyday activities and travel in lifestyle vlog videos to social and
political activities in news videos. Each video in the dataset is annotated
with: (1) a human-written free-form NL query, (2) relevant moments in the video
w.r.t. the query, and (3) five-point scale saliency scores for all
query-relevant clips. This comprehensive annotation enables us to develop and
evaluate systems that detect relevant moments as well as salient highlights for
diverse, flexible user queries. We also present a strong baseline for this
task, Moment-DETR, a transformer encoder-decoder model that views moment
retrieval as a direct set prediction problem, taking extracted video and query
representations as inputs and predicting moment coordinates and saliency scores
end-to-end. While our model does not utilize any human prior, we show that it
performs competitively when compared to well-engineered architectures. With
weakly supervised pretraining using ASR captions, Moment-DETR substantially
outperforms previous methods. Lastly, we present several ablations and
visualizations of Moment-DETR. Data and code is publicly available at
this https URL


### [[2006.04734] Reinforcement Learning Under Moral Uncertainty](http://arxiv.org/abs/2006.04734)


  An ambitious goal for machine learning is to create agents that behave
ethically: The capacity to abide by human moral norms would greatly expand the
context in which autonomous agents could be practically and safely deployed,
e.g. fully autonomous vehicles will encounter charged moral decisions that
complicate their deployment. While ethical agents could be trained by rewarding
correct behavior under a specific moral theory (e.g. utilitarianism), there
remains widespread disagreement about the nature of morality. Acknowledging
such disagreement, recent work in moral philosophy proposes that ethical
behavior requires acting under moral uncertainty, i.e. to take into account
when acting that one's credence is split across several plausible ethical
theories. This paper translates such insights to the field of reinforcement
learning, proposes two training methods that realize different points among
competing desiderata, and trains agents in simple environments to act under
moral uncertainty. The results illustrate (1) how such uncertainty can help
curb extreme behavior from commitment to single theories and (2) several
technical complications arising from attempting to ground moral philosophy in
RL (e.g. how can a principled trade-off between two competing but incomparable
reward functions be reached). The aim is to catalyze progress towards
morally-competent agents and highlight the potential of RL to contribute
towards the computational grounding of moral philosophy.

    

### [[2007.08855] A Biologically Plausible Audio-Visual Integration Model for Continual Learning](http://arxiv.org/abs/2007.08855)


  The problem of catastrophic forgetting has a history of more than 30 years
and has not been completely solved yet. Since the human brain has natural
ability to perform continual lifelong learning, learning from the brain may
provide solutions to this problem. In this paper, we propose a novel
biologically plausible audio-visual integration model (AVIM) based on the
assumption that the integration of audio and visual perceptual information in
the medial temporal lobe during learning is crucial to form concepts and make
continual learning possible. Specifically, we use multi-compartment
Hodgkin-Huxley neurons to build the model and adopt the calcium-based synaptic
tagging and capture as the model's learning rule. Furthermore, we define a new
continual learning paradigm to simulate the possible continual learning process
in the human brain. We then test our model under this new paradigm. Our
experimental results show that the proposed AVIM can achieve state-of-the-art
continual learning performance compared with other advanced methods such as
OWM, iCaRL and GEM. Moreover, it can generate stable representations of objects
during learning. These results support our assumption that concept formation is
essential for continuous lifelong learning and suggest the proposed AVIM is a
possible concept formation mechanism.

    

### [[2007.12681] Data science and AI in FinTech: An overview](http://arxiv.org/abs/2007.12681)


  Financial technology (FinTech) has been playing an increasingly critical role
in driving modern economies, society, technology, and many other areas. Smart
FinTech is the new-generation FinTech, largely inspired and empowered by data
science and new-generation AI and (DSAI) techniques. Smart FinTech synthesizes
broad DSAI and transforms finance and economies to drive intelligent,
automated, whole-of-business and personalized economic and financial
businesses, services and systems. The research on data science and AI in
FinTech involves many latest progress made in smart FinTech for BankingTech,
TradeTech, LendTech, InsurTech, WealthTech, PayTech, RiskTech,
cryptocurrencies, and blockchain, and the DSAI techniques including complex
system methods, quantitative methods, intelligent interactions, recognition and
responses, data analytics, deep learning, federated learning,
privacy-preserving processing, augmentation, optimization, and system
intelligence enhancement. Here, we present a highly dense research overview of
smart financial businesses and their challenges, the smart FinTech ecosystem,
the DSAI techniques to enable smart FinTech, and some research directions of
smart FinTech futures to the DSAI communities.

    

### [[2008.10518] ScrewNet: Category-Independent Articulation Model Estimation From Depth Images Using Screw Theory](http://arxiv.org/abs/2008.10518)


  Robots in human environments will need to interact with a wide variety of
articulated objects such as cabinets, drawers, and dishwashers while assisting
humans in performing day-to-day tasks. Existing methods either require objects
to be textured or need to know the articulation model category a priori for
estimating the model parameters for an articulated object. We propose ScrewNet,
a novel approach that estimates an object's articulation model directly from
depth images without requiring a priori knowledge of the articulation model
category. ScrewNet uses screw theory to unify the representation of different
articulation types and perform category-independent articulation model
estimation. We evaluate our approach on two benchmarking datasets and compare
its performance with a current state-of-the-art method. Results demonstrate
that ScrewNet can successfully estimate the articulation models and their
parameters for novel objects across articulation model categories with better
on average accuracy than the prior state-of-the-art method. Project webpage:
this https URL


### [[2010.12537] TUTA: Tree-based Transformers for Generally Structured Table Pre-training](http://arxiv.org/abs/2010.12537)


  Tables are widely used with various structures to organize and present data.
Recent attempts on table understanding mainly focus on relational tables, yet
overlook to other common table structures. In this paper, we propose TUTA, a
unified pre-training architecture for understanding generally structured
tables. Noticing that understanding a table requires spatial, hierarchical, and
semantic information, we enhance transformers with three novel structure-aware
mechanisms. First, we devise a unified tree-based structure, called a
bi-dimensional coordinate tree, to describe both the spatial and hierarchical
information of generally structured tables. Upon this, we propose tree-based
attention and position embedding to better capture the spatial and hierarchical
information. Moreover, we devise three progressive pre-training objectives to
enable representations at the token, cell, and table levels. We pre-train TUTA
on a wide range of unlabeled web and spreadsheet tables and fine-tune it on two
critical tasks in the field of table structure understanding: cell type
classification and table type classification. Experiments show that TUTA is
highly effective, achieving state-of-the-art on five widely-studied datasets.

    

### [[2102.02295] Variational Bayes survival analysis for unemployment modelling](http://arxiv.org/abs/2102.02295)


  Mathematical modelling of unemployment dynamics attempts to predict the
probability of a job seeker finding a job as a function of time. This is
typically achieved by using information in unemployment records. These records
are right censored, making survival analysis a suitable approach for parameter
estimation. The proposed model uses a deep artificial neural network (ANN) as a
non-linear hazard function. Through embedding, high-cardinality categorical
features are analysed efficiently. The posterior distribution of the ANN
parameters are estimated using a variational Bayes method. The model is
evaluated on a time-to-employment data set spanning from 2011 to 2020 provided
by the Slovenian public employment service. It is used to determine the
employment probability over time for each individual on the record. Similar
models could be applied to other questions with multi-dimensional,
high-cardinality categorical data including censored records. Such data is
often encountered in personal records, for example in medical records.

    

### [[2103.15571] Enhancing the Transferability of Adversarial Attacks through Variance Tuning](http://arxiv.org/abs/2103.15571)


  Deep neural networks are vulnerable to adversarial examples that mislead the
models with imperceptible perturbations. Though adversarial attacks have
achieved incredible success rates in the white-box setting, most existing
adversaries often exhibit weak transferability in the black-box setting,
especially under the scenario of attacking models with defense mechanisms. In
this work, we propose a new method called variance tuning to enhance the class
of iterative gradient based attack methods and improve their attack
transferability. Specifically, at each iteration for the gradient calculation,
instead of directly using the current gradient for the momentum accumulation,
we further consider the gradient variance of the previous iteration to tune the
current gradient so as to stabilize the update direction and escape from poor
local optima. Empirical results on the standard ImageNet dataset demonstrate
that our method could significantly improve the transferability of
gradient-based adversarial attacks. Besides, our method could be used to attack
ensemble models or be integrated with various input transformations.
Incorporating variance tuning with input transformations on iterative
gradient-based attacks in the multi-model setting, the integrated method could
achieve an average success rate of 90.1% against nine advanced defense methods,
improving the current best attack performance significantly by 85.1% . Code is
available at this https URL.

    

### [[2104.10567] SOGAN: 3D-Aware Shadow and Occlusion Robust GAN for Makeup Transfer](http://arxiv.org/abs/2104.10567)


  In recent years, virtual makeup applications have become more and more
popular. However, it is still challenging to propose a robust makeup transfer
method in the real-world environment. Current makeup transfer methods mostly
work well on good-conditioned clean makeup images, but transferring makeup that
exhibits shadow and occlusion is not satisfying. To alleviate it, we propose a
novel makeup transfer method, called 3D-Aware Shadow and Occlusion Robust GAN
(SOGAN). Given the source and the reference faces, we first fit a 3D face model
and then disentangle the faces into shape and texture. In the texture branch,
we map the texture to the UV space and design a UV texture generator to
transfer the makeup. Since human faces are symmetrical in the UV space, we can
conveniently remove the undesired shadow and occlusion from the reference image
by carefully designing a Flip Attention Module (FAM). After obtaining cleaner
makeup features from the reference image, a Makeup Transfer Module (MTM) is
introduced to perform accurate makeup transfer. The qualitative and
quantitative experiments demonstrate that our SOGAN not only achieves superior
results in shadow and occlusion situations but also performs well in large pose
and expression variations.

    

### [[2106.09009] End-to-End Spoken Language Understanding for Generalized Voice Assistants](http://arxiv.org/abs/2106.09009)


  End-to-end (E2E) spoken language understanding (SLU) systems predict
utterance semantics directly from speech using a single model. Previous work in
this area has focused on targeted tasks in fixed domains, where the output
semantic structure is assumed a priori and the input speech is of limited
complexity. In this work we present our approach to developing an E2E model for
generalized SLU in commercial voice assistants (VAs). We propose a fully
differentiable, transformer-based, hierarchical system that can be pretrained
at both the ASR and NLU levels. This is then fine-tuned on both transcription
and semantic classification losses to handle a diverse set of intent and
argument combinations. This leads to an SLU system that achieves significant
improvements over baselines on a complex internal generalized VA dataset with a
43% improvement in accuracy, while still meeting the 99% accuracy benchmark on
the popular Fluent Speech Commands dataset. We further evaluate our model on a
hard test set, exclusively containing slot arguments unseen in training, and
demonstrate a nearly 20% improvement, showing the efficacy of our approach in
truly demanding VA scenarios.

    

### [[2107.01151] Collaborative Visual Navigation](http://arxiv.org/abs/2107.01151)


  As a fundamental problem for Artificial Intelligence, multi-agent system
(MAS) is making rapid progress, mainly driven by multi-agent reinforcement
learning (MARL) techniques. However, previous MARL methods largely focused on
grid-world like or game environments; MAS in visually rich environments has
remained less explored. To narrow this gap and emphasize the crucial role of
perception in MAS, we propose a large-scale 3D dataset, CollaVN, for
multi-agent visual navigation (MAVN). In CollaVN, multiple agents are entailed
to cooperatively navigate across photo-realistic environments to reach target
locations. Diverse MAVN variants are explored to make our problem more general.
Moreover, a memory-augmented communication framework is proposed. Each agent is
equipped with a private, external memory to persistently store communication
information. This allows agents to make better use of their past communication
information, enabling more efficient collaboration and robust long-term
planning. In our experiments, several baselines and evaluation metrics are
designed. We also empirically verify the efficacy of our proposed MARL approach
across different MAVN task settings.

    

### [[2007.00337] Determining Sequence of Image Processing Technique (IPT) to Detect Adversarial Attacks](http://arxiv.org/abs/2007.00337)


  Developing secure machine learning models from adversarial examples is
challenging as various methods are continually being developed to generate
adversarial attacks. In this work, we propose an evolutionary approach to
automatically determine Image Processing Techniques Sequence (IPTS) for
detecting malicious inputs. Accordingly, we first used a diverse set of attack
methods including adaptive attack methods (on our defense) to generate
adversarial samples from the clean dataset. A detection framework based on a
genetic algorithm (GA) is developed to find the optimal IPTS, where the
optimality is estimated by different fitness measures such as Euclidean
distance, entropy loss, average histogram, local binary pattern and loss
functions. The "image difference" between the original and processed images is
used to extract the features, which are then fed to a classification scheme in
order to determine whether the input sample is adversarial or clean. This paper
described our methodology and performed experiments using multiple data-sets
tested with several adversarial attacks. For each attack-type and dataset, it
generates unique IPTS. A set of IPTS selected dynamically in testing time which
works as a filter for the adversarial attack. Our empirical experiments
exhibited promising results indicating the approach can efficiently be used as
processing for any AI model.

    

### [[2107.09351] IoTDataBench: Extending TPCx-IoT for Compression and Scalability](http://arxiv.org/abs/2107.09351)


  We present a record-breaking result and lessons learned in practicing
TPCx-IoT benchmarking for a real-world use case. We find that more system
characteristics need to be benchmarked for its application to real-world use
cases. We introduce an extension to the TPCx-IoT benchmark, covering
fundamental requirements of time-series data management for IoT infrastructure.
We characterize them as data compression and system scalability. To evaluate
these two important features of IoT databases, we propose IoTDataBench and
update four aspects of TPCx-IoT, i.e., data generation, workloads, metrics and
test procedures. Preliminary evaluation results show systems that fail to
effectively compress data or flexibly scale can negatively affect the
redesigned metrics, while systems with high compression ratios and linear
scalability are rewarded in the final metrics. Such systems have the ability to
scale up computing resources on demand and can thus save dollar costs.

    

### [[2107.09472] Verified Functional Programming of an Abstract Interpreter](http://arxiv.org/abs/2107.09472)


  Abstract interpreters are complex pieces of software: even if the abstract
interpretation theory and companion algorithms are well understood, their
implementations are subject to bugs, that might question the soundness of their
computations.
While some formally verified abstract interpreters have been written in the
past, writing and understanding them requires expertise in the use of proof
assistants, and requires a non-trivial amount of interactive proofs.
This paper presents a formally verified abstract interpreter fully programmed
and proved correct in the F* verified programming environment. Thanks to F*
refinement types and SMT prover capabilities we demonstrate a substantial
saving in proof effort compared to previous works based on interactive proof
assistants.
Almost all the code of our implementation, proofs included, written in a
functional style, are presented directly in the paper.

    

### [[2006.13635] ReLoC Reloaded: A Mechanized Relational Logic for Fine-Grained Concurrency and Logical Atomicity](http://arxiv.org/abs/2006.13635)


  We present a new version of ReLoC: a relational separation logic for proving
refinements of programs with higher-order state, fine-grained concurrency,
polymorphism and recursive types. The core of ReLoC is its refinement judgment
$e \precsim e' : \tau$, which states that a program $e$ refines a program $e'$
at type $\tau$. ReLoC provides type-directed structural rules and symbolic
execution rules in separation-logic style for manipulating the judgment,
whereas in prior work on refinements for languages with higher-order state and
concurrency, such proofs were carried out by unfolding the judgment into its
definition in the model. ReLoC's abstract proof rules make it simpler to carry
out refinement proofs, and enable us to generalize the notion of logically
atomic specifications to the relational case, which we call logically atomic
relational specifications.
We build ReLoC on top of the Iris framework for separation logic in Coq,
allowing us to leverage features of Iris to prove soundness of ReLoC, and to
carry out refinement proofs in ReLoC. We implement tactics for interactive
proofs in ReLoC, allowing us to mechanize several case studies in Coq, and
thereby demonstrate the practicality of ReLoC.
ReLoC Reloaded extends ReLoC (LICS'18) with various technical improvements, a
new Coq mechanization, and support for Iris's prophecy variables. The latter
allows us to carry out refinement proofs that involve reasoning about the
program's future. We also expand ReLoC's notion of logically atomic relational
specifications with a new flavor based on the HOCAP pattern by Svendsen et al.

    

### [<title>补办|罗瑞尔大学本科毕业证/成绩单样本-g - DockOne.io</title>](http://dockone.io/question/746097)

### [<title>补办|温莎大学本科毕业证/成绩单样本-i - DockOne.io</title>](http://dockone.io/question/746096)

### [<title>关于）盐城个人电商执照怎么申请-爱上本地宝 - DockOne.io</title>](http://dockone.io/question/746095)

### [<title>关于）南通个人电商执照怎么申请-爱上本地宝 - DockOne.io</title>](http://dockone.io/question/746094)

### [<title>补办|西安大略大学本科毕业证/成绩单样本-o - DockOne.io</title>](http://dockone.io/question/746093)

### [<title>补办|滑铁卢大学本科毕业证/成绩单样本-v - DockOne.io</title>](http://dockone.io/question/746092)

### [<title>关于）扬州个人电商执照怎么申请-爱上本地宝 - DockOne.io</title>](http://dockone.io/question/746091)

### [<title>关于）合肥个人电商执照怎么申请-爱上本地宝 - DockOne.io</title>](http://dockone.io/question/746090)

### [<title>补办|多伦多大学本科毕业证/成绩单样本-s - DockOne.io</title>](http://dockone.io/question/746089)

### [<title>补办|萨德伯里大学本科毕业证/成绩单样本-e - DockOne.io</title>](http://dockone.io/question/746088)

### [<title>关于）荆门个人电商执照怎么申请-爱上本地宝 - DockOne.io</title>](http://dockone.io/question/746087)

### [<title>关于）兰州个人电商执照怎么申请-爱上本地宝 - DockOne.io</title>](http://dockone.io/question/746086)

### [<title>补办|安大略理工大学本科毕业证/成绩单样本-a - DockOne.io</title>](http://dockone.io/question/746085)

### [<title>补办|渥太华大学本科毕业证/成绩单样本-e - DockOne.io</title>](http://dockone.io/question/746084)

### [<title>补办|圭尔夫大学本科毕业证/成绩单样本-b - DockOne.io</title>](http://dockone.io/question/746083)

### [<title>关于）呼和浩特个人电商执照怎么申请-爱上本地宝 - DockOne.io</title>](http://dockone.io/question/746082)

### [<title>关于）柳州个人电商执照怎么申请-爱上本地宝 - DockOne.io</title>](http://dockone.io/question/746081)

### [<title>补办|川特大学本科毕业证/成绩单样本-f - DockOne.io</title>](http://dockone.io/question/746080)

### [<title>补办|特伦特大学本科毕业证/成绩单样本-l - DockOne.io</title>](http://dockone.io/question/746079)

### [<title>关于）贵港个人电商执照怎么申请-爱上本地宝 - DockOne.io</title>](http://dockone.io/question/746078)