
## 2021-8-10

### [[2108.03297] Joint AP Probing and Scheduling: A Contextual Bandit Approach](http://arxiv.org/abs/2108.03297)


  We consider a set of APs with unknown data rates that cooperatively serve a
mobile client. The data rate of each link is i.i.d. sampled from a distribution
that is unknown a priori. In contrast to traditional link scheduling problems
under uncertainty, we assume that in each time step, the device can probe a
subset of links before deciding which one to use. We model this problem as a
contextual bandit problem with probing (CBwP) and present an efficient
algorithm. We further establish the regret of our algorithm for links with
Bernoulli data rates. Our CBwP model is a novel extension of the classic
contextual bandit model and can potentially be applied to a large class of
sequential decision-making problems that involve joint probing and play under
uncertainty.

    

### [[2108.03431] Next generation of IEEE 802.11 by minimizing the current problems](http://arxiv.org/abs/2108.03431)


  This main objective of this research is to analyze the common issues of the
current and past IEEE 802.11 versions (which is also called as Wi-Fi) that have
risen with the increase of internet users and their demands. While this study
is conducted with the help of several studies taken into analysis, it provides
the solutions that would be able to come out with next generation of IEEE
802.11 in order to minimize the limitations of the current technologies and
also to meet the demands of the customers.

    

### [[2108.03476] Evaluation of Age Control Protocol (ACP) and ACP+ on ESP32](http://arxiv.org/abs/2108.03476)


  Age Control Protocol (ACP) and its enhanced version, ACP+, are recently
proposed transport layer protocols to control Age of Information of data flows.
This study presents an experimental evaluation of ACP and ACP+ on the ESP32
microcontroller, a currently popular IoT device. We identify several issues
related to the implementation of these protocols on this platform and in
general on short-haul, low-delay connections. We propose solutions to overcome
these issues in the form of simple modifications to ACP+, and compare the
performance of the resulting modified ACP+ with that of the original protocols
on a small-delay local wireless IoT connection.

    

### [[2108.03514] Machine Learning Assisted Security Analysis of 5G-Network-Connected Systems](http://arxiv.org/abs/2108.03514)


  The core network architecture of telecommunication systems has undergone a
paradigm shift in the fifth-generation (5G)networks. 5G networks have
transitioned to software-defined infrastructures, thereby reducing their
dependence on hardware-based network functions. New technologies, like network
function virtualization and software-defined networking, have been incorporated
in the 5G core network (5GCN) architecture to enable this transition. This has
resulted in significant improvements in efficiency, performance, and robustness
of the networks. However, this has also made the core network more vulnerable,
as software systems are generally easier to compromise than hardware systems.
In this article, we present a comprehensive security analysis framework for the
5GCN. The novelty of this approach lies in the creation and analysis of attack
graphs of the software-defined and virtualized 5GCN through machine learning.
This analysis points to 119 novel possible exploits in the 5GCN. We demonstrate
that these possible exploits of 5GCN vulnerabilities generate five novel
attacks on the 5G Authentication and Key Agreement protocol. We combine the
attacks at the network, protocol, and the application layers to generate
complex attack vectors. In a case study, we use these attack vectors to find
four novel security loopholes in WhatsApp running on a 5G network.

    

### [[2108.03562] Master Graduation Thesis: A Lightweight and Distributed Container-based Framework](http://arxiv.org/abs/2108.03562)


  Edge/Fog computing is a novel computing paradigm that provides
resource-limited Internet of Things (IoT) devices with scalable computing and
storage resources. Compared to cloud computing, edge/fog servers have fewer
resources, but they can be accessed with higher bandwidth and less
communication latency. Thus, integrating edge/fog and cloud infrastructures can
support the execution of diverse latency-sensitive and computation-intensive
IoT applications. Although some frameworks attempt to provide such integration,
there are still several challenges to be addressed, such as dynamic scheduling
of different IoT applications, scalability mechanisms, multi-platform support,
and supporting different interaction models. To overcome these challenges, we
propose a lightweight and distributed container-based framework, called
FogBus2. It provides a mechanism for scheduling heterogeneous IoT applications
and implements several scheduling policies. Also, it proposes an optimized
genetic algorithm to obtain fast convergence to well-suited solutions. Besides,
it offers a scalability mechanism to ensure efficient responsiveness when
either the number of IoT devices increases or the resources become
overburdened. Also, the dynamic resource discovery mechanism of FogBus2 assists
new entities to quickly join the system. We have also developed two IoT
applications, called Conway's Game of Life and Video Optical Character
Recognition to demonstrate the effectiveness of FogBus2 for handling real-time
and non-real-time IoT applications. Experimental results show FogBus2's
scheduling policy improves the response time of IoT applications by 53\%
compared to other policies. Also, the scalability mechanism can reduce up to
48\% of the queuing waiting time compared to frameworks that do not support
scalability.

    

### [[2108.03710] Online Admission Control and Resource Allocation in Network Slicing under Demand Uncertainties](http://arxiv.org/abs/2108.03710)


  One of the most important aspects of moving forward to the next generation
networks like 5G/6G, is to enable network slicing in an efficient manner. The
most challenging issues are the uncertainties in consumption and communication
demand. Because the slices' arrive to the network in different times and their
lifespans vary, the solution should dynamically react to online slice requests.
The joint problem of online admission control and resource allocation
considering the energy consumption is formulated mathematically. It is based on
Integer Linear Programming (ILP), where, the $\Gamma$- Robustness concept is
exploited to overcome Virtual Links (VL) bandwidths' and Virtual Network
Functions (VNF) workloads' uncertainties. Then, an optimal algorithm that
adopts this mathematical model is proposed. To overcome the high computational
complexity of ILP which is NP-hard, a new heuristic algorithm is developed. The
assessments' results indicate that the efficiency of heuristic is vital in
increasing the accepted requests' count, decreasing power consumption and
providing adjustable tolerance vs. the VNFs workloads' and VLs traffics'
uncertainties, separately. Considering the acceptance ratio and power
consumption that constitute the two important components of the objective
function, heuristic has about 7% and 12% optimality gaps, respectively, while
being about 30X faster than that of optimal algorithm.

    

### [[2108.03745] Performance Evaluation of MU-MIMO Under the Impact of Open Loop Traffic Dynamics](http://arxiv.org/abs/2108.03745)


  Multi-user MIMO enhances throughput by simultaneously transmitting/receiving
parallel data streams to/from a group of users. However, the throughput
analysis does not ac-count for variable data traffic. The research objective of
this project is to analyze the effect of variable traffic as it will change the
system behavior. The two characteristic features of variable traffic viz.
packet size variation and traffic burstiness have been considered for the
analysis in this project. Their effects have been studied individually to
provide an insight into the problem. Through simulations, we show that in
certain scenarios the system performance of MU-MIMO deteriorates significantly
due to both packet size variations and traffic burstiness individually even
under ideal conditions with respect to channel variations, channel correlation,
mobility, etc. Furthermore, we show that under bursty traffic with higher
offered load the aggregate throughput first re-mains steady until a certain
peak to average rate ratio and then deteriorates linearly instead of
exponentially. Also, un-der high amount of traffic burstiness, the throughputs
are in-dependent of the aggregation rates. A thorough analysis is provided to
explain these two phenomena. This is the first research work that considers the
impact of variable traffic on the performance of MU-MIMO. Therefore, the
implications for MAC protocol design are significant.

    

### [[2108.03778] Velocity-adaptive Access Scheme for MEC-assisted Platooning Networks: Access Fairness Via Data Freshness](http://arxiv.org/abs/2108.03778)


  Platooning strategy is an important part of autonomous driving technology.
Due to the limited resource of autonomous vehicles in platoons, mobile edge
computing (MEC) is usually used to assist vehicles in platoons to obtain useful
information, increasing its safety. Specifically, vehicles usually adopt the
IEEE 802.11 distributed coordination function (DCF) mechanism to transmit large
amount of data to the base station (BS) through vehicle-to-infrastructure (V2I)
communications, where the useful information can be extracted by the edge
server connected to the BS and then sent back to the vehicles to make correct
decisions in time. However, vehicles may be moving on different lanes with
different velocities, which incurs the unfair access due to the characteristics
of platoons, i.e., vehicles on different lanes transmit different amount of
data to the BS when they pass through the coverage of the BS, which also
results in the different amount of useful information received by various
vehicles. Moreover, age of information (AoI) is an important performance metric
to measure the freshness of the data. Large average age of data implies not
receiving the useful information in time. It is necessary to design an access
scheme to jointly optimize the fairness and data freshness. In this paper, we
formulate a joint optimization problem in the MEC-assisted V2I networks and
present a multi-objective optimization scheme to solve the problem through
adjusting the minimum contention window under the IEEE 802.11 DCF mode
according to the velocities of vehicles. The effectiveness of the scheme has
been demonstrated by simulation.

    

### [[2108.03872] Adaptive Anomaly Detection for Internet of Things in Hierarchical Edge Computing: A Contextual-Bandit Approach](http://arxiv.org/abs/2108.03872)


  The advances in deep neural networks (DNN) have significantly enhanced
real-time detection of anomalous data in IoT applications. However, the
complexity-accuracy-delay dilemma persists: complex DNN models offer higher
accuracy, but typical IoT devices can barely afford the computation load, and
the remedy of offloading the load to the cloud incurs long delay. In this
paper, we address this challenge by proposing an adaptive anomaly detection
scheme with hierarchical edge computing (HEC). Specifically, we first construct
multiple anomaly detection DNN models with increasing complexity, and associate
each of them to a corresponding HEC layer. Then, we design an adaptive model
selection scheme that is formulated as a contextual-bandit problem and solved
by using a reinforcement learning policy network. We also incorporate a
parallelism policy training method to accelerate the training process by taking
advantage of distributed models. We build an HEC testbed using real IoT
devices, implement and evaluate our contextual-bandit approach with both
univariate and multivariate IoT datasets. In comparison with both baseline and
state-of-the-art schemes, our adaptive approach strikes the best accuracy-delay
tradeoff on the univariate dataset, and achieves the best accuracy and F1-score
on the multivariate dataset with only negligibly longer delay than the best
(but inflexible) scheme.

    

### [[1908.11313] Power and Beam Optimization for Uplink Millimeter-Wave Hotspot Communication Systems](http://arxiv.org/abs/1908.11313)


  We propose an effective interference management and beamforming mechanism for
uplink communication systems that yields fair allocation of rates. In
particular, we consider a hotspot area of a millimeter-wave (mmWave) access
network consisting of multiple user equipment (UE) in the uplink and multiple
access points (APs) with directional antennas and adjustable beam widths and
directions (beam configurations). This network suffers tremendously from
multi-beam multi-user interference, and, to improve the uplink transmission
performance, we propose a centralized scheme that optimizes the power, the beam
width, the beam direction of the APs, and the UE - AP assignments. This problem
involves both continuous and discrete variables, and it has the following
structure. If we fix all discrete variables, except for those related to the
UE-AP assignment, the resulting optimization problem can be solved optimally.
This property enables us to propose a heuristic based on simulated annealing
(SA) to address the intractable joint optimization problem with all discrete
variables. In more detail, for a fixed configuration of beams, we formulate a
weighted rate allocation problem where each user gets the same portion of its
maximum achievable rate that it would have under non-interfered conditions. We
solve this problem with an iterative fixed point algorithm that optimizes the
power of UEs and the UE - AP assignment in the uplink. This fixed point
algorithm is combined with SA to improve the beam configurations. Theoretical
and numerical results show that the proposed method improves both the UE rates
in the lower percentiles and the overall fairness in the network.

    

### [[2102.07572] Transfer Learning for Future Wireless Networks: A Comprehensive Survey](http://arxiv.org/abs/2102.07572)


  With outstanding features, Machine Learning (ML) has been the backbone of
numerous applications in wireless networks. However, the conventional ML
approaches have been facing many challenges in practical implementation, such
as the lack of labeled data, the constantly changing wireless environments, the
long training process, and the limited capacity of wireless devices. These
challenges, if not addressed, will impede the effectiveness and applicability
of ML in future wireless networks. To address these problems, Transfer Learning
(TL) has recently emerged to be a very promising solution. The core idea of TL
is to leverage and synthesize distilled knowledge from similar tasks as well as
from valuable experiences accumulated from the past to facilitate the learning
of new problems. Doing so, TL techniques can reduce the dependence on labeled
data, improve the learning speed, and enhance the ML methods' robustness to
different wireless environments. This article aims to provide a comprehensive
survey on applications of TL in wireless networks. Particularly, we first
provide an overview of TL including formal definitions, classification, and
various types of TL techniques. We then discuss diverse TL approaches proposed
to address emerging issues in wireless networks. The issues include spectrum
management, localization, signal recognition, security, human activity
recognition and caching, which are all important to next-generation networks
such as 5G and beyond. Finally, we highlight important challenges, open issues,
and future research directions of TL in future wireless networks.

    

### [[2108.03233] Operational Learning-based Boundary Estimation in Electromagnetic Medical Imaging](http://arxiv.org/abs/2108.03233)


  Incorporating boundaries of the imaging object as a priori information to
imaging algorithms can significantly improve the performance of electromagnetic
medical imaging systems. To avoid overly complicating the system by using
different sensors and the adverse effect of the subject's movement, a
learning-based method is proposed to estimate the boundary (external contour)
of the imaged object using the same electromagnetic imaging data. While imaging
techniques may discard the reflection coefficients for being dominant and
uninformative for imaging, these parameters are made use of for boundary
detection. The learned model is verified through independent clinical human
trials by using a head imaging system with a 16-element antenna array that
works across the band 0.7-1.6 GHz. The evaluation demonstrated that the model
achieves average dissimilarity of 0.012 in Hu-moment while detecting head
boundary. The model enables fast scan and image creation while eliminating the
need for additional devices for accurate boundary estimation.

    

### [[2108.03235] SMOTified-GAN for class imbalanced pattern classification problems](http://arxiv.org/abs/2108.03235)


  Class imbalance in a dataset is a major problem for classifiers that results
in poor prediction with a high true positive rate (TPR) but a low true negative
rate (TNR) for a majority positive training dataset. Generally, the
pre-processing technique of oversampling of minority class(es) are used to
overcome this deficiency. Our focus is on using the hybridization of Generative
Adversarial Network (GAN) and Synthetic Minority Over-Sampling Technique
(SMOTE) to address class imbalanced problems. We propose a novel two-phase
oversampling approach that has the synergy of SMOTE and GAN. The initial data
of minority class(es) generated by SMOTE is further enhanced by GAN that
produces better quality samples. We named it SMOTified-GAN as GAN works on
pre-sampled minority data produced by SMOTE rather than randomly generating the
samples itself. The experimental results prove the sample quality of minority
class(es) has been improved in a variety of tested benchmark datasets. Its
performance is improved by up to 9\% from the next best algorithm tested on
F1-score measurements. Its time complexity is also reasonable which is around
$O(N^2d^2T)$ for a sequential algorithm.

    

### [[2108.03236] Efficient Representation for Electric Vehicle Charging Station Operations using Reinforcement Learning](http://arxiv.org/abs/2108.03236)


  Effectively operating electrical vehicle charging station (EVCS) is crucial
for enabling the rapid transition of electrified transportation. To solve this
problem using reinforcement learning (RL), the dimension of state/action spaces
scales with the number of EVs and is thus very large and time-varying. This
dimensionality issue affects the efficiency and convergence properties of
generic RL algorithms. We develop aggregation schemes that are based on the
emergency of EV charging, namely the laxity value. A least-laxity first (LLF)
rule is adopted to consider only the total charging power of the EVCS which
ensures the feasibility of individual EV schedules. In addition, we propose an
equivalent state aggregation that can guarantee to attain the same optimal
policy. Based on the proposed representation, policy gradient method is used to
find the best parameters for the linear Gaussian policy . Numerical results
have validated the performance improvement of the proposed representation
approaches in attaining higher rewards and more effective policies as compared
to existing approximation based approach.

    

### [[2108.03272] IGibson 2.0: Object-Centric Simulation for Robot Learning of Everyday Household Tasks](http://arxiv.org/abs/2108.03272)


  Recent research in embodied AI has been boosted by the use of simulation
environments to develop and train robot learning approaches. However, the use
of simulation has skewed the attention to tasks that only require what robotics
simulators can simulate: motion and physical contact. We present iGibson 2.0,
an open-source simulation environment that supports the simulation of a more
diverse set of household tasks through three key innovations. First, iGibson
2.0 supports object states, including temperature, wetness level, cleanliness
level, and toggled and sliced states, necessary to cover a wider range of
tasks. Second, iGibson 2.0 implements a set of predicate logic functions that
map the simulator states to logic states like Cooked or Soaked. Additionally,
given a logic state, iGibson 2.0 can sample valid physical states that satisfy
it. This functionality can generate potentially infinite instances of tasks
with minimal effort from the users. The sampling mechanism allows our scenes to
be more densely populated with small objects in semantically meaningful
locations. Third, iGibson 2.0 includes a virtual reality (VR) interface to
immerse humans in its scenes to collect demonstrations. As a result, we can
collect demonstrations from humans on these new types of tasks, and use them
for imitation learning. We evaluate the new capabilities of iGibson 2.0 to
enable robot learning of novel tasks, in the hope of demonstrating the
potential of this new simulator to support new research in embodied AI. iGibson
2.0 and its new dataset will be publicly available at
this http URL.

    

### [[2108.03273] Concept Drift Detection with Variable Interaction Networks](http://arxiv.org/abs/2108.03273)


  The current development of today's production industry towards seamless
sensor-based monitoring is paving the way for concepts such as Predictive
Maintenance. By this means, the condition of plants and products in future
production lines will be continuously analyzed with the objective to predict
any kind of breakdown and trigger preventing actions proactively. Such
ambitious predictions are commonly performed with support of machine learning
algorithms. In this work, we utilize these algorithms to model complex systems,
such as production plants, by focusing on their variable interactions. The core
of this contribution is a sliding window based algorithm, designed to detect
changes of the identified interactions, which might indicate beginning
malfunctions in the context of a monitored production plant. Besides a detailed
description of the algorithm, we present results from experiments with a
synthetic dynamical system, simulating stable and drifting system behavior.

    

### [[2108.03274] Smooth Symbolic Regression: Transformation of Symbolic Regression into a Real-valued Optimization Problem](http://arxiv.org/abs/2108.03274)


  The typical methods for symbolic regression produce rather abrupt changes in
solution candidates. In this work, we have tried to transform symbolic
regression from an optimization problem, with a landscape that is so rugged
that typical analysis methods do not produce meaningful results, to one that
can be compared to typical and very smooth real-valued problems. While the
ruggedness might not interfere with the performance of optimization, it
restricts the possibilities of analysis. Here, we have explored different
aspects of a transformation and propose a simple procedure to create
real-valued optimization problems from symbolic regression problems.

    

### [[2108.03288] Ensemble Augmentation for Deep Neural Networks Using 1-D Time Series Vibration Data](http://arxiv.org/abs/2108.03288)


  Time-series data are one of the fundamental types of raw data representation
used in data-driven techniques. In machine condition monitoring, time-series
vibration data are overly used in data mining for deep neural networks.
Typically, vibration data is converted into images for classification using
Deep Neural Networks (DNNs), and scalograms are the most effective form of
image representation. However, the DNN classifiers require huge labeled
training samples to reach their optimum performance. So, many forms of data
augmentation techniques are applied to the classifiers to compensate for the
lack of training samples. However, the scalograms are graphical representations
where the existing augmentation techniques suffer because they either change
the graphical meaning or have too much noise in the samples that change the
physical meaning. In this study, a data augmentation technique named ensemble
augmentation is proposed to overcome this limitation. This augmentation method
uses the power of white noise added in ensembles to the original samples to
generate real-like samples. After averaging the signal with ensembles, a new
signal is obtained that contains the characteristics of the original signal.
The parameters for the ensemble augmentation are validated using a simulated
signal. The proposed method is evaluated using 10 class bearing vibration data
using three state-of-the-art Transfer Learning (TL) models, namely,
Inception-V3, MobileNet-V2, and ResNet50. Augmented samples are generated in
two increments: the first increment generates the same number of fake samples
as the training samples, and in the second increment, the number of samples is
increased gradually. The outputs from the proposed method are compared with no
augmentation, augmentations using deep convolution generative adversarial
network (DCGAN), and several geometric transformation-based augmentations...

    

### [[2108.03298] What Matters in Learning from Offline Human Demonstrations for Robot Manipulation](http://arxiv.org/abs/2108.03298)


  Imitating human demonstrations is a promising approach to endow robots with
various manipulation capabilities. While recent advances have been made in
imitation learning and batch (offline) reinforcement learning, a lack of
open-source human datasets and reproducible learning methods make assessing the
state of the field difficult. In this paper, we conduct an extensive study of
six offline learning algorithms for robot manipulation on five simulated and
three real-world multi-stage manipulation tasks of varying complexity, and with
datasets of varying quality. Our study analyzes the most critical challenges
when learning from offline human data for manipulation. Based on the study, we
derive a series of lessons including the sensitivity to different algorithmic
design choices, the dependence on the quality of the demonstrations, and the
variability based on the stopping criteria due to the different objectives in
training and evaluation. We also highlight opportunities for learning from
human datasets, such as the ability to learn proficient policies on
challenging, multi-stage tasks beyond the scope of current reinforcement
learning methods, and the ability to easily scale to natural, real-world
manipulation scenarios where only raw sensory signals are available. We have
open-sourced our datasets and all algorithm implementations to facilitate
future research and fair comparisons in learning from human demonstration data.
Codebase, datasets, trained models, and more available at
this https URL


### [[2108.03320] A Deep Neural Network Approach for Crop Selection and Yield Prediction in Bangladesh](http://arxiv.org/abs/2108.03320)


  Agriculture is the essential ingredients to mankind which is a major source
of livelihood. Agriculture work in Bangladesh is mostly done in old ways which
directly affects our economy. In addition, institutions of agriculture are
working with manual data which cannot provide a proper solution for crop
selection and yield prediction. This paper shows the best way of crop selection
and yield prediction in minimum cost and effort. Artificial Neural Network is
considered robust tools for modeling and prediction. This algorithm aims to get
better output and prediction, as well as, support vector machine, Logistic
Regression, and random forest algorithm is also considered in this study for
comparing the accuracy and error rate. Moreover, all of these algorithms used
here are just to see how well they performed for a dataset which is over 0.3
million. We have collected 46 parameters such as maximum and minimum
temperature, average rainfall, humidity, climate, weather, and types of land,
types of chemical fertilizer, types of soil, soil structure, soil composition,
soil moisture, soil consistency, soil reaction and soil texture for applying
into this prediction process. In this paper, we have suggested using the deep
neural network for agricultural crop selection and yield prediction.

    

### [[2108.03322] Distilling Transformers for Neural Cross-Domain Search](http://arxiv.org/abs/2108.03322)


  Pre-trained transformers have recently clinched top spots in the gamut of
natural language tasks and pioneered solutions to software engineering tasks.
Even information retrieval has not been immune to the charm of the transformer,
though their large size and cost is generally a barrier to deployment. While
there has been much work in streamlining, caching, and modifying transformer
architectures for production, here we explore a new direction: distilling a
large pre-trained translation model into a lightweight bi-encoder which can be
efficiently cached and queried. We argue from a probabilistic perspective that
sequence-to-sequence models are a conceptually ideal---albeit highly
impractical---retriever. We derive a new distillation objective, implementing
it as a data augmentation scheme. Using natural language source code search as
a case study for cross-domain search, we demonstrate the validity of this idea
by significantly improving upon the current leader of the CodeSearchNet
challenge, a recent natural language code search benchmark.

    

### [[2108.03336] Estimating Graph Dimension with Cross-validated Eigenvalues](http://arxiv.org/abs/2108.03336)


  In applied multivariate statistics, estimating the number of latent
dimensions or the number of clusters is a fundamental and recurring problem.
One common diagnostic is the scree plot, which shows the largest eigenvalues of
the data matrix; the user searches for a "gap" or "elbow" in the decreasing
eigenvalues; unfortunately, these patterns can hide beneath the bias of the
sample eigenvalues. This methodological problem is conceptually difficult
because, in many situations, there is only enough signal to detect a subset of
the $k$ population dimensions/eigenvectors. In this situation, one could argue
that the correct choice of $k$ is the number of detectable dimensions. We
alleviate these problems with cross-validated eigenvalues. Under a large class
of random graph models, without any parametric assumptions, we provide a
p-value for each sample eigenvector. It tests the null hypothesis that this
sample eigenvector is orthogonal to (i.e., uncorrelated with) the true latent
dimensions. This approach naturally adapts to problems where some dimensions
are not statistically detectable. In scenarios where all $k$ dimensions can be
estimated, we prove that our procedure consistently estimates $k$. In
simulations and a data example, the proposed estimator compares favorably to
alternative approaches in both computational and statistical performance.

    

### [[2108.03348] Edge-augmented Graph Transformers: Global Self-attention is Enough for Graphs](http://arxiv.org/abs/2108.03348)


  Transformer neural networks have achieved state-of-the-art results for
unstructured data such as text and images but their adoption for
graph-structured data has been limited. This is partly due to the difficulty in
incorporating complex structural information in the basic transformer
framework. We propose a simple yet powerful extension to the transformer -
residual edge channels. The resultant framework, which we call Edge-augmented
Graph Transformer (EGT), can directly accept, process and output structural
information as well as node information. This simple addition allows us to use
global self-attention, the key element of transformers, directly for graphs and
comes with the benefit of long-range interaction among nodes. Moreover, the
edge channels allow the structural information to evolve from layer to layer,
and prediction tasks on edges can be derived directly from these channels. In
addition to that, we introduce positional encodings based on Singular Value
Decomposition which can improve the performance of EGT. Our framework, which
relies on global node feature aggregation, achieves better performance compared
to Graph Convolutional Networks (GCN), which rely on local feature aggregation
within a neighborhood. We verify the performance of EGT in a supervised
learning setting on a wide range of experiments on benchmark datasets. Our
findings indicate that convolutional aggregation is not an essential inductive
bias for graphs and global self-attention can serve as a flexible and adaptive
alternative to graph convolution.

    

### [[2108.03353] Screen2Words: Automatic Mobile UI Summarization with Multimodal Learning](http://arxiv.org/abs/2108.03353)


  Mobile User Interface Summarization generates succinct language descriptions
of mobile screens for conveying important contents and functionalities of the
screen, which can be useful for many language-based application scenarios. We
present Screen2Words, a novel screen summarization approach that automatically
encapsulates essential information of a UI screen into a coherent language
phrase. Summarizing mobile screens requires a holistic understanding of the
multi-modal data of mobile UIs, including text, image, structures as well as UI
semantics, motivating our multi-modal learning approach. We collected and
analyzed a large-scale screen summarization dataset annotated by human workers.
Our dataset contains more than 112k language summarization across $\sim$22k
unique UI screens. We then experimented with a set of deep models with
different configurations. Our evaluation of these models with both automatic
accuracy metrics and human rating shows that our approach can generate
high-quality summaries for mobile screens. We demonstrate potential use cases
of Screen2Words and open-source our dataset and model to lay the foundations
for further bridging language and user interfaces.

    

### [[2108.03354] HetEmotionNet: Two-Stream Heterogeneous Graph Recurrent Neural Network for Multi-modal Emotion Recognition](http://arxiv.org/abs/2108.03354)


  The research on human emotion under multimedia stimulation based on
physiological signals is an emerging field, and important progress has been
achieved for emotion recognition based on multi-modal signals. However, it is
challenging to make full use of the complementarity among
spatial-spectral-temporal domain features for emotion recognition, as well as
model the heterogeneity and correlation among multi-modal signals. In this
paper, we propose a novel two-stream heterogeneous graph recurrent neural
network, named HetEmotionNet, fusing multi-modal physiological signals for
emotion recognition. Specifically, HetEmotionNet consists of the
spatial-temporal stream and the spatial-spectral stream, which can fuse
spatial-spectral-temporal domain features in a unified framework. Each stream
is composed of the graph transformer network for modeling the heterogeneity,
the graph convolutional network for modeling the correlation, and the gated
recurrent unit for capturing the temporal domain or spectral domain dependency.
Extensive experiments on two real-world datasets demonstrate that our proposed
model achieves better performance than state-of-the-art baselines.

    

### [[2108.03357] A Survey on Cross-domain Recommendation: Taxonomies, Methods, and Future Directions](http://arxiv.org/abs/2108.03357)


  Traditional recommendation systems are faced with two long-standing
obstacles, namely, data sparsity and cold-start problems, which promote the
emergence and development of Cross-Domain Recommendation (CDR). The core idea
of CDR is to leverage information collected from other domains to alleviate the
two problems in one domain. Over the last decade, many efforts have been
engaged for cross-domain recommendation. Recently, with the development of deep
learning and neural networks, a large number of methods have emerged. However,
there is a limited number of systematic surveys on CDR, especially regarding
the latest proposed methods as well as the recommendation scenarios and
recommendation tasks they address. In this survey paper, we first proposed a
two-level taxonomy of cross-domain recommendation which classifies different
recommendation scenarios and recommendation tasks. We then introduce and
summarize existing cross-domain recommendation approaches under different
recommendation scenarios in a structured manner. We also organize datasets
commonly used. We conclude this survey by providing several potential research
directions about this field.

    

### [[2108.03378] Learning Indoor Layouts from Simple Point-Clouds](http://arxiv.org/abs/2108.03378)


  Reconstructing a layout of indoor spaces has been a crucial part of growing
indoor location based services. One of the key challenges in the proliferation
of indoor location based services is the unavailability of indoor spatial maps
due to the complex nature of capturing an indoor space model (e.g., floor plan)
of an existing building. In this paper, we propose a system to automatically
generate floor plans that can recognize rooms from the point-clouds obtained
through smartphones like Google's Tango. In particular, we propose two
approaches - a Recurrent Neural Network based approach using Pointer Network
and a Convolutional Neural Network based approach using Mask-RCNN to identify
rooms (and thereby floor plans) from point-clouds. Experimental results on
different datasets demonstrate approximately 0.80-0.90 Intersection-over-Union
scores, which show that our models can effectively identify the rooms and
regenerate the shapes of the rooms in heterogeneous environment.

    

### [[2108.03388] Jointly Attacking Graph Neural Network and its Explanations](http://arxiv.org/abs/2108.03388)


  Graph Neural Networks (GNNs) have boosted the performance for many
graph-related tasks. Despite the great success, recent studies have shown that
GNNs are highly vulnerable to adversarial attacks, where adversaries can
mislead the GNNs' prediction by modifying graphs. On the other hand, the
explanation of GNNs (GNNExplainer) provides a better understanding of a trained
GNN model by generating a small subgraph and features that are most influential
for its prediction. In this paper, we first perform empirical studies to
validate that GNNExplainer can act as an inspection tool and have the potential
to detect the adversarial perturbations for graphs. This finding motivates us
to further initiate a new problem investigation: Whether a graph neural network
and its explanations can be jointly attacked by modifying graphs with malicious
desires? It is challenging to answer this question since the goals of
adversarial attacks and bypassing the GNNExplainer essentially contradict each
other. In this work, we give a confirmative answer to this question by
proposing a novel attack framework (GEAttack), which can attack both a GNN
model and its explanations by simultaneously exploiting their vulnerabilities.
Extensive experiments on two explainers (GNNExplainer and PGExplainer) under
various real-world datasets demonstrate the effectiveness of the proposed
method.

    

### [[2108.03400] Missing Data Estimation in Temporal Multilayer Position-aware Graph Neural Network (TMP-GNN)](http://arxiv.org/abs/2108.03400)


  GNNs have been proven to perform highly effective in various node-level,
edge-level, and graph-level prediction tasks in several domains. Existing
approaches mainly focus on static graphs. However, many graphs change over time
with their edge may disappear, or node or edge attribute may alter from one
time to the other. It is essential to consider such evolution in representation
learning of nodes in time varying graphs. In this paper, we propose a Temporal
Multilayered Position-aware Graph Neural Network (TMP-GNN), a node embedding
approach for dynamic graph that incorporates the interdependence of temporal
relations into embedding computation. We evaluate the performance of TMP-GNN on
two different representations of temporal multilayered graphs. The performance
is assessed against the most popular GNNs on node-level prediction tasks. Then,
we incorporate TMP-GNN into a deep learning framework to estimate missing data
and compare the performance with their corresponding competent GNNs from our
former experiment, and a baseline method. Experimental results on four
real-world datasets yield up to 58% of lower ROC AUC for pairwise node
classification task, and 96% of lower MAE in missing feature estimation,
particularly for graphs with a relatively high number of nodes and lower mean
degree of connectivity.

    

### [[2108.03411] Unsupervised learning of anomalous diffusion data](http://arxiv.org/abs/2108.03411)


  The characterization of diffusion processes is a keystone in our
understanding of a variety of physical phenomena. Many of these deviate from
Brownian motion, giving rise to anomalous diffusion. Various theoretical models
exists nowadays to describe such processes, but their application to
experimental setups is often challenging, due to the stochastic nature of the
phenomena and the difficulty to harness reliable data. The latter often
consists on short and noisy trajectories, which are hard to characterize with
usual statistical approaches. In recent years, we have witnessed an impressive
effort to bridge theory and experiments by means of supervised machine learning
techniques, with astonishing results. In this work, we explore the use of
unsupervised methods in anomalous diffusion data. We show that the main
diffusion characteristics can be learnt without the need of any labelling of
the data. We use such method to discriminate between anomalous diffusion models
and extract their physical parameters. Moreover, we explore the feasibility of
finding novel types of diffusion, in this case represented by compositions of
existing diffusion models. At last, we showcase the use of the method in
experimental data and demonstrate its advantages for cases where supervised
learning is not applicable.

    

### [[2108.03437] Secure Neuroimaging Analysis using Federated Learning with Homomorphic Encryption](http://arxiv.org/abs/2108.03437)


  Federated learning (FL) enables distributed computation of machine learning
models over various disparate, remote data sources, without requiring to
transfer any individual data to a centralized location. This results in an
improved generalizability of models and efficient scaling of computation as
more sources and larger datasets are added to the federation. Nevertheless,
recent membership attacks show that private or sensitive personal data can
sometimes be leaked or inferred when model parameters or summary statistics are
shared with a central site, requiring improved security solutions. In this
work, we propose a framework for secure FL using fully-homomorphic encryption
(FHE). Specifically, we use the CKKS construction, an approximate, floating
point compatible scheme that benefits from ciphertext packing and rescaling. In
our evaluation on large-scale brain MRI datasets, we use our proposed secure FL
framework to train a deep learning model to predict a person's age from
distributed MRI scans, a common benchmarking task, and demonstrate that there
is no degradation in the learning performance between the encrypted and
non-encrypted federated models.

    

### [[2108.03442] Clustering Large Data Sets with Incremental Estimation of Low-density Separating Hyperplanes](http://arxiv.org/abs/2108.03442)


  An efficient method for obtaining low-density hyperplane separators in the
unsupervised context is proposed. Low density separators can be used to obtain
a partition of a set of data based on their allocations to the different sides
of the separators. The proposed method is based on applying stochastic gradient
descent to the integrated density on the hyperplane with respect to a
convolution of the underlying distribution and a smoothing kernel. In the case
where the bandwidth of the smoothing kernel is decreased towards zero, the bias
of these updates with respect to the true underlying density tends to zero, and
convergence to a minimiser of the density on the hyperplane can be obtained. A
post-processing of the partition induced by a collection of low-density
hyperplanes yields an efficient and accurate clustering method which is capable
of automatically selecting an appropriate number of clusters. Experiments with
the proposed approach show that it is highly competitive in terms of both speed
and accuracy when compared with relevant benchmarks. Code to implement the
proposed approach is available in the form of an R package from
this https URL.

    

### [[2108.03444] A Machine Learning Tool to Determine State of Mind and Emotion](http://arxiv.org/abs/2108.03444)


  This paper investigates the possibility of creating a machine learning tool
that automatically determines the state of mind and emotion of an individual
through a questionnaire, without the aid of a human expert. The state of mind
and emotion is defined in this work as pertaining to preference, feelings, or
opinion that is not based on logic or reason. It is the case when a person
gives out an answer to start by saying, "I feel...". The tool is designed to
mimic the expertise of a psychologist and is built without any formal knowledge
of psychology. The idea is to build the expertise by purely computational
methods through thousands of questions collected from users. It is aimed
towards possibly diagnosing substance addiction, alcoholism, sexual attraction,
HIV status, degree of commitment, activity inclination, etc. First, the paper
presents the related literature and classifies them according to data gathering
methods. Another classification is created according to preference, emotion,
grouping, and rules to achieve a deeper interpretation and better understanding
of the state of mind and emotion. Second, the proposed tool is developed using
an online addiction questionnaire with 10 questions and 292 respondents. In
addition, an initial investigation on the dimension of addiction is presented
through the built machine learning model. Machine learning methods, namely,
artificial neural network (ANN) and support vector machine (SVM), are used to
determine a true or false or degree of state of a respondent.

    

### [[2108.03449] Self-learning sparse PCA for multimode process monitoring](http://arxiv.org/abs/2108.03449)


  This paper proposes a novel sparse principal component analysis algorithm
with self-learning ability for successive modes, where synaptic intelligence is
employed to measure the importance of variables and a regularization term is
added to preserve the learned knowledge of previous modes. Different from
traditional multimode monitoring methods, the monitoring model is updated based
on the current model and new data when a new mode arrives, thus delivering
prominent performance for sequential modes. Besides, the computation and
storage resources are saved in the long run, because it is not necessary to
retrain the model from scratch frequently and store data from previous modes.
More importantly, the model furnishes excellent interpretability owing to the
sparsity of parameters. Finally, a numerical case and a practical pulverizing
system are adopted to illustrate the effectiveness of the proposed algorithm.

    

### [[2108.03457] Stereo Waterdrop Removal with Row-wise Dilated Attention](http://arxiv.org/abs/2108.03457)


  Existing vision systems for autonomous driving or robots are sensitive to
waterdrops adhered to windows or camera lenses. Most recent waterdrop removal
approaches take a single image as input and often fail to recover the missing
content behind waterdrops faithfully. Thus, we propose a learning-based model
for waterdrop removal with stereo images. To better detect and remove
waterdrops from stereo images, we propose a novel row-wise dilated attention
module to enlarge attention's receptive field for effective information
propagation between the two stereo images. In addition, we propose an attention
consistency loss between the ground-truth disparity map and attention scores to
enhance the left-right consistency in stereo images. Because of related
datasets' unavailability, we collect a real-world dataset that contains stereo
images with and without waterdrops. Extensive experiments on our dataset
suggest that our model outperforms state-of-the-art methods both quantitatively
and qualitatively. Our source code and the stereo waterdrop dataset are
available at
\href{this https URL}{this https URL}

    

### [[2108.03465] A k-mer Based Approach for SARS-CoV-2 Variant Identification](http://arxiv.org/abs/2108.03465)


  With the rapid spread of the novel coronavirus (COVID-19) across the globe
and its continuous mutation, it is of pivotal importance to design a system to
identify different known (and unknown) variants of SARS-CoV-2. Identifying
particular variants helps to understand and model their spread patterns, design
effective mitigation strategies, and prevent future outbreaks. It also plays a
crucial role in studying the efficacy of known vaccines against each variant
and modeling the likelihood of breakthrough infections. It is well known that
the spike protein contains most of the information/variation pertaining to
coronavirus variants.
In this paper, we use spike sequences to classify different variants of the
coronavirus in humans. We show that preserving the order of the amino acids
helps the underlying classifiers to achieve better performance. We also show
that we can train our model to outperform the baseline algorithms using only a
small number of training samples ($1\%$ of the data). Finally, we show the
importance of the different amino acids which play a key role in identifying
variants and how they coincide with those reported by the USA's Centers for
Disease Control and Prevention (CDC).

    

### [[2108.03478] An empirical assessment of deep learning approaches to task-oriented dialog management](http://arxiv.org/abs/2108.03478)


  Deep learning is providing very positive results in areas related to
conversational interfaces, such as speech recognition, but its potential
benefit for dialog management has still not been fully studied. In this paper,
we perform an assessment of different configurations for deep-learned dialog
management with three dialog corpora from different application domains and
varying in size, dimensionality and possible system responses. Our results have
allowed us to identify several aspects that can have an impact on accuracy,
including the approaches used for feature extraction, input representation,
context consideration and the hyper-parameters of the deep neural networks
employed.

    

### [[2108.03489] Impact of Aliasing on Generalization in Deep Convolutional Networks](http://arxiv.org/abs/2108.03489)


  We investigate the impact of aliasing on generalization in Deep Convolutional
Networks and show that data augmentation schemes alone are unable to prevent it
due to structural limitations in widely used architectures. Drawing insights
from frequency analysis theory, we take a closer look at ResNet and
EfficientNet architectures and review the trade-off between aliasing and
information loss in each of their major components. We show how to mitigate
aliasing by inserting non-trainable low-pass filters at key locations,
particularly where networks lack the capacity to learn them. These simple
architectural changes lead to substantial improvements in generalization on
i.i.d. and even more on out-of-distribution conditions, such as image
classification under natural corruptions on ImageNet-C [11] and few-shot
learning on Meta-Dataset [26]. State-of-the art results are achieved on both
datasets without introducing additional trainable parameters and using the
default hyper-parameters of open source codebases.

    

### [[2108.03490] Clustering Algorithms to Analyze the Road Traffic Crashes](http://arxiv.org/abs/2108.03490)


  Selecting an appropriate clustering method as well as an optimal number of
clusters in road accident data is at times confusing and difficult. This paper
analyzes shortcomings of different existing techniques applied to cluster
accident-prone areas and recommends using Density-Based Spatial Clustering of
Applications with Noise (DBSCAN) and Ordering Points To Identify the Clustering
Structure (OPTICS) to overcome them. Comparative performance analysis based on
real-life data on the recorded cases of road accidents in North Carolina also
show more effectiveness and efficiency achieved by these algorithms.

    

### [[2108.03491] Approximate Last Iterate Convergence in Overparameterized GANs](http://arxiv.org/abs/2108.03491)


  In this work, we showed that the Implicit Update and Predictive Methods
dynamics introduced in prior work satisfy last iterate convergence to a
neighborhood around the optimum in overparameterized GANs, where the size of
the neighborhood shrinks with the width of the neural network. This is in
contrast to prior results, which only guaranteed average iterate convergence.

    

### [[2108.03498] Kinematics clustering enables head impact subtyping for better traumatic brain injury prediction](http://arxiv.org/abs/2108.03498)


  Traumatic brain injury can be caused by various types of head impacts.
However, due to different kinematic characteristics, many brain injury risk
estimation models are not generalizable across the variety of impacts that
humans may sustain. The current definitions of head impact subtypes are based
on impact sources (e.g., football, traffic accident), which may not reflect the
intrinsic kinematic similarities of impacts across the impact sources. To
investigate the potential new definitions of impact subtypes based on
kinematics, 3,161 head impacts from various sources including simulation,
college football, mixed martial arts, and car racing were collected. We applied
the K-means clustering to cluster the impacts on 16 standardized temporal
features from head rotation kinematics. Then, we developed subtype-specific
ridge regression models for cumulative strain damage (using the threshold of
15%), which significantly improved the estimation accuracy compared with the
baseline method which mixed impacts from different sources and developed one
model (R^2 from 0.7 to 0.9). To investigate the effect of kinematic features,
we presented the top three critical features (maximum resultant angular
acceleration, maximum angular acceleration along the z-axis, maximum linear
acceleration along the y-axis) based on regression accuracy and used logistic
regression to find the critical points for each feature that partitioned the
subtypes. This study enables researchers to define head impact subtypes in a
data-driven manner, which leads to more generalizable brain injury risk
estimation.

    

### [[2108.03506] Membership Inference Attacks on Lottery Ticket Networks](http://arxiv.org/abs/2108.03506)


  The vulnerability of the Lottery Ticket Hypothesis has not been studied from
the purview of Membership Inference Attacks. Through this work, we are the
first to empirically show that the lottery ticket networks are equally
vulnerable to membership inference attacks. A Membership Inference Attack (MIA)
is the process of determining whether a data sample belongs to a training set
of a trained model or not. Membership Inference Attacks could leak critical
information about the training data that can be used for targeted attacks.
Recent deep learning models often have very large memory footprints and a high
computational cost associated with training and drawing inferences. Lottery
Ticket Hypothesis is used to prune the networks to find smaller sub-networks
that at least match the performance of the original model in terms of test
accuracy in a similar number of iterations. We used CIFAR-10, CIFAR-100, and
ImageNet datasets to perform image classification tasks and observe that the
attack accuracies are similar. We also see that the attack accuracy varies
directly according to the number of classes in the dataset and the sparsity of
the network. We demonstrate that these attacks are transferable across models
with high accuracy.

    

### [[2108.03508] The Effect of Training Parameters and Mechanisms on Decentralized Federated Learning based on MNIST Dataset](http://arxiv.org/abs/2108.03508)


  Federated Learning is an algorithm suited for training models on
decentralized data, but the requirement of a central "server" node is a
bottleneck. In this document, we first introduce the notion of Decentralized
Federated Learning (DFL). We then perform various experiments on different
setups, such as changing model aggregation frequency, switching from
independent and identically distributed (IID) dataset partitioning to non-IID
partitioning with partial global sharing, using different optimization methods
across clients, and breaking models into segments with partial sharing. All
experiments are run on the MNIST handwritten digits dataset. We observe that
those altered training procedures are generally robust, albeit non-optimal. We
also observe failures in training when the variance between model weights is
too large. The open-source experiment code is accessible through
GitHub\footnote{Code was uploaded at
\url{this https URL}}.

    

### [[2108.03531] Learning to Transfer with von Neumann Conditional Divergence](http://arxiv.org/abs/2108.03531)


  The similarity of feature representations plays a pivotal role in the success
of domain adaptation and generalization. Feature similarity includes both the
invariance of marginal distributions and the closeness of conditional
distributions given the desired response $y$ (e.g., class labels).
Unfortunately, traditional methods always learn such features without fully
taking into consideration the information in $y$, which in turn may lead to a
mismatch of the conditional distributions or the mix-up of discriminative
structures underlying data distributions. In this work, we introduce the
recently proposed von Neumann conditional divergence to improve the
transferability across multiple domains. We show that this new divergence is
differentiable and eligible to easily quantify the functional dependence
between features and $y$. Given multiple source tasks, we integrate this
divergence to capture discriminative information in $y$ and design novel
learning objectives assuming those source tasks are observed either
simultaneously or sequentially. In both scenarios, we obtain favorable
performance against state-of-the-art methods in terms of smaller generalization
error on new tasks and less catastrophic forgetting on source tasks (in the
sequential setup).

    

### [[2108.03541] OSCAR-Net: Object-centric Scene Graph Attention for Image Attribution](http://arxiv.org/abs/2108.03541)


  Images tell powerful stories but cannot always be trusted. Matching images
back to trusted sources (attribution) enables users to make a more informed
judgment of the images they encounter online. We propose a robust image hashing
algorithm to perform such matching. Our hash is sensitive to manipulation of
subtle, salient visual details that can substantially change the story told by
an image. Yet the hash is invariant to benign transformations (changes in
quality, codecs, sizes, shapes, etc.) experienced by images during online
redistribution. Our key contribution is OSCAR-Net (Object-centric Scene Graph
Attention for Image Attribution Network); a robust image hashing model inspired
by recent successes of Transformers in the visual domain. OSCAR-Net constructs
a scene graph representation that attends to fine-grained changes of every
object's visual appearance and their spatial relationships. The network is
trained via contrastive learning on a dataset of original and manipulated
images yielding a state of the art image hash for content fingerprinting that
scales to millions of images.

    

### [[2108.03548] Recurrent Graph Neural Networks for Rumor Detection in Online Forums](http://arxiv.org/abs/2108.03548)


  The widespread adoption of online social networks in daily life has created a
pressing need for effectively classifying user-generated content. This work
presents techniques for classifying linked content spread on forum websites --
specifically, links to news articles or blogs -- using user interaction signals
alone. Importantly, online forums such as Reddit do not have a user-generated
social graph, which is assumed in social network behavioral-based
classification settings. Using Reddit as a case-study, we show how to obtain a
derived social graph, and use this graph, Reddit post sequences, and comment
trees as inputs to a Recurrent Graph Neural Network (R-GNN) encoder. We train
the R-GNN on news link categorization and rumor detection, showing superior
results to recent baselines. Our code is made publicly available at
this https URL.

    

### [[2108.03555] Contrastive Representation Learning for Rapid Intraoperative Diagnosis of Skull Base Tumors Imaged Using Stimulated Raman Histology](http://arxiv.org/abs/2108.03555)


  Background: Accurate diagnosis of skull base tumors is essential for
providing personalized surgical treatment strategies. Intraoperative diagnosis
can be challenging due to tumor diversity and lack of intraoperative pathology
resources.
Objective: To develop an independent and parallel intraoperative pathology
workflow that can provide rapid and accurate skull base tumor diagnoses using
label-free optical imaging and artificial intelligence (AI).
Method: We used a fiber laser-based, label-free, non-consumptive,
high-resolution microscopy method ($<$ 60 sec per 1 $\times$ 1 mm$^\text{2}$),
called stimulated Raman histology (SRH), to image a consecutive, multicenter
cohort of skull base tumor patients. SRH images were then used to train a
convolutional neural network (CNN) model using three representation learning
strategies: cross-entropy, self-supervised contrastive learning, and supervised
contrastive learning. Our trained CNN models were tested on a held-out,
multicenter SRH dataset.
Results: SRH was able to image the diagnostic features of both benign and
malignant skull base tumors. Of the three representation learning strategies,
supervised contrastive learning most effectively learned the distinctive and
diagnostic SRH image features for each of the skull base tumor types. In our
multicenter testing set, cross-entropy achieved an overall diagnostic accuracy
of 91.5%, self-supervised contrastive learning 83.9%, and supervised
contrastive learning 96.6%. Our trained model was able to identify tumor-normal
margins and detect regions of microscopic tumor infiltration in whole-slide SRH
images.
Conclusion: SRH with AI models trained using contrastive representation
learning can provide rapid and accurate intraoperative diagnosis of skull base
tumors.

    

### [[2108.03561] Combining machine learning and data assimilation to forecast dynamical systems from noisy partial observations](http://arxiv.org/abs/2108.03561)


  We present a supervised learning method to learn the propagator map of a
dynamical system from partial and noisy observations. In our computationally
cheap and easy-to-implement framework a neural network consisting of random
feature maps is trained sequentially by incoming observations within a data
assimilation procedure. By employing Takens' embedding theorem, the network is
trained on delay coordinates. We show that the combination of random feature
maps and data assimilation, called RAFDA, outperforms standard random feature
maps for which the dynamics is learned using batch data.

    

### [[2108.03570] Robust 1-bit Compressive Sensing with Partial Gaussian Circulant Matrices and Generative Priors](http://arxiv.org/abs/2108.03570)


  In 1-bit compressive sensing, each measurement is quantized to a single bit,
namely the sign of a linear function of an unknown vector, and the goal is to
accurately recover the vector. While it is most popular to assume a standard
Gaussian sensing matrix for 1-bit compressive sensing, using structured sensing
matrices such as partial Gaussian circulant matrices is of significant
practical importance due to their faster matrix operations. In this paper, we
provide recovery guarantees for a correlation-based optimization algorithm for
robust 1-bit compressive sensing with randomly signed partial Gaussian
circulant matrices and generative models. Under suitable assumptions, we match
guarantees that were previously only known to hold for i.i.d.~Gaussian matrices
that require significantly more computation. We make use of a practical
iterative algorithm, and perform numerical experiments on image datasets to
corroborate our theoretical results.

    

### [[2108.03576] BeatNet: CRNN and Particle Filtering for Online Joint Beat Downbeat and Meter Tracking](http://arxiv.org/abs/2108.03576)


  The online estimation of rhythmic information, such as beat positions,
downbeat positions, and meter, is critical for many real-time music
applications. Musical rhythm comprises complex hierarchical relationships
across time, rendering its analysis intrinsically challenging and at times
subjective. Furthermore, systems which attempt to estimate rhythmic information
in real-time must be causal and must produce estimates quickly and efficiently.
In this work, we introduce an online system for joint beat, downbeat, and meter
tracking, which utilizes causal convolutional and recurrent layers, followed by
a pair of sequential Monte Carlo particle filters applied during inference. The
proposed system does not need to be primed with a time signature in order to
perform downbeat tracking, and is instead able to estimate meter and adjust the
predictions over time. Additionally, we propose an information gate strategy to
significantly decrease the computational cost of particle filtering during the
inference step, making the system much faster than previous sampling-based
methods. Experiments on the GTZAN dataset, which is unseen during training,
show that the system outperforms various online beat and downbeat tracking
systems and achieves comparable performance to a baseline offline joint method.

    

### [[2108.03578] Language Model Evaluation in Open-ended Text Generation](http://arxiv.org/abs/2108.03578)


  Although current state-of-the-art language models have achieved impressive
results in numerous natural language processing tasks, still they could not
solve the problem of producing repetitive, dull and sometimes inconsistent text
in open-ended text generation. Studies often attribute this problem to the
maximum likelihood training objective, and propose alternative approaches by
using stochastic decoding methods or altering the training objective. However,
there is still a lack of consistent evaluation metrics to directly compare the
efficacy of these solutions. In this work, we study different evaluation
metrics that have been proposed to evaluate quality, diversity and consistency
of machine-generated text. From there, we propose a practical pipeline to
evaluate language models in open-ended generation task, and research on how to
improve the model's performance in all dimensions by leveraging different
auxiliary training objectives.

    

### [[2108.03579] Expressive Power and Loss Surfaces of Deep Learning Models](http://arxiv.org/abs/2108.03579)


  The goals of this paper are two-fold. The first goal is to serve as an
expository tutorial on the working of deep learning models which emphasizes
geometrical intuition about the reasons for success of deep learning. The
second goal is to complement the current results on the expressive power of
deep learning models and their loss surfaces with novel insights and results.
In particular, we describe how deep neural networks carve out manifolds
especially when the multiplication neurons are introduced. Multiplication is
used in dot products and the attention mechanism and it is employed in capsule
networks and self-attention based transformers. We also describe how random
polynomial, random matrix, spin glass and computational complexity perspectives
on the loss surfaces are interconnected.

    

### [[2108.03585] Ensemble neuroevolution based approach for multivariate time series anomaly detection](http://arxiv.org/abs/2108.03585)


  Multivariate time series anomaly detection is a very common problem in the
field of failure prevention. Fast prevention means lower repair costs and
losses. The amount of sensors in novel industry systems makes the anomaly
detection process quite difficult for humans. Algorithms which automates the
process of detecting anomalies are crucial in modern failure-prevention
systems. Therefore, many machine and deep learning models have been designed to
address this problem. Mostly, they are autoencoder-based architectures with
some generative adversarial elements. In this work, a framework is shown which
incorporates neuroevolution methods to boost the anomaly-detection scores of
new and already known models. The presented approach adapts evolution
strategies for evolving ensemble model, in which every single model works on a
subgroup of data sensors. The next goal of neuroevolution is to optimise
architecture and hyperparameters like window size, the number of layers, layer
depths, etc. The proposed framework shows that it is possible to boost most of
the anomaly detection deep learning models in a reasonable time and a fully
automated mode. The tests were run on SWAT and WADI datasets. To our knowledge,
this is the first approach in which an ensemble deep learning anomaly detection
model is built in a fully automatic way using a neuroevolution strategy.

    

### [[2108.03588] A Look at the Evaluation Setup of the M5 Forecasting Competition](http://arxiv.org/abs/2108.03588)


  Forecast evaluation plays a key role in how empirical evidence shapes the
development of the discipline. Domain experts are interested in error measures
relevant for their decision making needs. Such measures may produce unreliable
results. Although reliability properties of several metrics have already been
discussed, it has hardly been quantified in an objective way. We propose a
measure named Rank Stability, which evaluates how much the rankings of an
experiment differ in between similar datasets, when the models and errors are
constant. We use this to study the evaluation setup of the M5. We find that the
evaluation setup of the M5 is less reliable than other measures. The main
drivers of instability are hierarchical aggregation and scaling.
Price-weighting reduces the stability of all tested error measures. Scale
normalization of the M5 error measure results in less stability than other
scale-free errors. Hierarchical levels taken separately are less stable with
more aggregation, and their combination is even less stable than individual
levels. We also show positive tradeoffs of retaining aggregation importance
without affecting stability. Aggregation and stability can be linked to the
influence of much debated magic numbers. Many of our findings can be applied to
general hierarchical forecast benchmarking.

    

### [[2108.03591] FederatedNILM: A Distributed and Privacy-preserving Framework for Non-intrusive Load Monitoring based on Federated Deep Learning](http://arxiv.org/abs/2108.03591)


  Non-intrusive load monitoring (NILM), which usually utilizes machine learning
methods and is effective in disaggregating smart meter readings from the
household-level into appliance-level consumptions, can help to analyze
electricity consumption behaviours of users and enable practical smart energy
and smart grid applications. However, smart meters are privately owned and
distributed, which make real-world applications of NILM challenging. To this
end, this paper develops a distributed and privacy-preserving federated deep
learning framework for NILM (FederatedNILM), which combines federated learning
with a state-of-the-art deep learning architecture to conduct NILM for the
classification of typical states of household appliances. Through extensive
comparative experiments, the effectiveness of the proposed FederatedNILM
framework is demonstrated.

    

### [[2108.03594] MAF-GNN: Multi-adaptive Spatiotemporal-flow Graph Neural Network for Traffic Speed Forecasting](http://arxiv.org/abs/2108.03594)


  Traffic forecasting is a core element of intelligent traffic monitoring
system. Approaches based on graph neural networks have been widely used in this
task to effectively capture spatial and temporal dependencies of road networks.
However, these approaches can not effectively define the complicated network
topology. Besides, their cascade network structures have limitations in
transmitting distinct features in the time and space dimensions. In this paper,
we propose a Multi-adaptive Spatiotemporal-flow Graph Neural Network (MAF-GNN)
for traffic speed forecasting. MAF-GNN introduces an effective Multi-adaptive
Adjacency Matrices Mechanism to capture multiple latent spatial dependencies
between traffic nodes. Additionally, we propose Spatiotemporal-flow Modules
aiming to further enhance feature propagation in both time and space
dimensions. MAF-GNN achieves better performance than other models on two
real-world datasets of public traffic network, METR-LA and PeMS-Bay,
demonstrating the effectiveness of the proposed approach.

    

### [[2108.03601] Using Biological Variables and Social Determinants to Predict Malaria and Anemia among Children in Senegal](http://arxiv.org/abs/2108.03601)


  Integrating machine learning techniques in healthcare becomes very common
nowadays, and it contributes positively to improving clinical care and health
decisions planning. Anemia and malaria are two life-threatening diseases in
Africa that affect the red blood cells and reduce hemoglobin production. This
paper focuses on analyzing child health data in Senegal using four machine
learning algorithms in Python: KNN, Random Forests, SVM, and Naïve Bayes. Our
task aims to investigate large-scale data from The Demographic and Health
Survey (DHS) and to find out hidden information for anemia and malaria. We
present two classification models for the two blood disorders using biological
variables and social determinants. The findings of this research will
contribute to improving child healthcare in Senegal by eradicating anemia and
malaria, and decreasing the child mortality rate.

    

### [[2108.03613] An EM Framework for Online Incremental Learning of Semantic Segmentation](http://arxiv.org/abs/2108.03613)


  Incremental learning of semantic segmentation has emerged as a promising
strategy for visual scene interpretation in the open- world setting. However,
it remains challenging to acquire novel classes in an online fashion for the
segmentation task, mainly due to its continuously-evolving semantic label
space, partial pixelwise ground-truth annotations, and constrained data
availability. To ad- dress this, we propose an incremental learning strategy
that can fast adapt deep segmentation models without catastrophic forgetting,
using a streaming input data with pixel annotations on the novel classes only.
To this end, we develop a uni ed learning strategy based on the
Expectation-Maximization (EM) framework, which integrates an iterative
relabeling strategy that lls in the missing labels and a rehearsal-based
incremental learning step that balances the stability-plasticity of the model.
Moreover, our EM algorithm adopts an adaptive sampling method to select
informative train- ing data and a class-balancing training strategy in the
incremental model updates, both improving the e cacy of model learning. We
validate our approach on the PASCAL VOC 2012 and ADE20K datasets, and the
results demonstrate its superior performance over the existing incremental
methods.

    

### [[2108.03625] Unifying Heterogenous Electronic Health Records Systems via Text-Based Code Embedding](http://arxiv.org/abs/2108.03625)


  Substantial increase in the use of Electronic Health Records (EHRs) has
opened new frontiers for predictive healthcare. However, while EHR systems are
nearly ubiquitous, they lack a unified code system for representing medical
concepts. Heterogeneous formats of EHR present a substantial barrier for the
training and deployment of state-of-the-art deep learning models at scale. To
overcome this problem, we introduce Description-based Embedding, DescEmb, a
code-agnostic description-based representation learning framework for
predictive modeling on EHR. DescEmb takes advantage of the flexibility of
neural language understanding models while maintaining a neutral approach that
can be combined with prior frameworks for task-specific representation learning
or predictive modeling. We tested our model's capacity on various experiments
including prediction tasks, transfer learning and pooled learning. DescEmb
shows higher performance in overall experiments compared to code-based
approach, opening the door to a text-based approach in predictive healthcare
research that is not constrained by EHR structure nor special domain knowledge.

    

### [[2108.03632] Deep Neural Network for DrawiNg Networks, (DNN)^2](http://arxiv.org/abs/2108.03632)


  By leveraging recent progress of stochastic gradient descent methods, several
works have shown that graphs could be efficiently laid out through the
optimization of a tailored objective function. In the meantime, Deep Learning
(DL) techniques achieved great performances in many applications. We
demonstrate that it is possible to use DL techniques to learn a graph-to-layout
sequence of operations thanks to a graph-related objective function. In this
paper, we present a novel graph drawing framework called (DNN)^2: Deep Neural
Network for DrawiNg Networks. Our method uses Graph Convolution Networks to
learn a model. Learning is achieved by optimizing a graph topology related loss
function that evaluates (DNN)^2 generated layouts during training. Once
trained, the (DNN)^ model is able to quickly lay any input graph out. We
experiment (DNN)^2 and statistically compare it to optimization-based and
regular graph layout algorithms. The results show that (DNN)^2 performs well
and are encouraging as the Deep Learning approach to Graph Drawing is novel and
many leads for future works are identified.

    

### [[2108.03645] Online Evolutionary Batch Size Orchestration for Scheduling Deep Learning Workloads in GPU Clusters](http://arxiv.org/abs/2108.03645)


  Efficient GPU resource scheduling is essential to maximize resource
utilization and save training costs for the increasing amount of deep learning
workloads in shared GPU clusters. Existing GPU schedulers largely rely on
static policies to leverage the performance characteristics of deep learning
jobs. However, they can hardly reach optimal efficiency due to the lack of
elasticity. To address the problem, we propose ONES, an ONline Evolutionary
Scheduler for elastic batch size orchestration. ONES automatically manages the
elasticity of each job based on the training batch size, so as to maximize GPU
utilization and improve scheduling efficiency. It determines the batch size for
each job through an online evolutionary search that can continuously optimize
the scheduling decisions. We evaluate the effectiveness of ONES with 64 GPUs on
TACC's Longhorn supercomputers. The results show that ONES can outperform the
prior deep learning schedulers with a significantly shorter average job
completion time.

    

### [[2108.03649] Joint Depth and Normal Estimation from Real-world Time-of-flight Raw Data](http://arxiv.org/abs/2108.03649)


  We present a novel approach to joint depth and normal estimation for
time-of-flight (ToF) sensors. Our model learns to predict the high-quality
depth and normal maps jointly from ToF raw sensor data. To achieve this, we
meticulously constructed the first large-scale dataset (named ToF-100) with
paired raw ToF data and ground-truth high-resolution depth maps provided by an
industrial depth camera. In addition, we also design a simple but effective
framework for joint depth and normal estimation, applying a robust Chamfer loss
via jittering to improve the performance of our model. Our experiments
demonstrate that our proposed method can efficiently reconstruct
high-resolution depth and normal maps and significantly outperforms
state-of-the-art approaches. Our code and data will be available at
\url{this https URL}

    

### [[2108.03703] Audio Spectral Enhancement: Leveraging Autoencoders for Low Latency Reconstruction of Long, Lossy Audio Sequences](http://arxiv.org/abs/2108.03703)


  With active research in audio compression techniques yielding substantial
breakthroughs, spectral reconstruction of low-quality audio waves remains a
less indulged topic. In this paper, we propose a novel approach for
reconstructing higher frequencies from considerably longer sequences of
low-quality MP3 audio waves. Our technique involves inpainting audio
spectrograms with residually stacked autoencoder blocks by manipulating
individual amplitude and phase values in relation to perceptual differences.
Our architecture presents several bottlenecks while preserving the spectral
structure of the audio wave via skip-connections. We also compare several task
metrics and demonstrate our visual guide to loss selection. Moreover, we show
how to leverage differential quantization techniques to reduce the initial
model size by more than half while simultaneously reducing inference time,
which is crucial in real-world applications.

    

### [[2108.03706] Online Bootstrap Inference For Policy Evaluation in Reinforcement Learning](http://arxiv.org/abs/2108.03706)


  The recent emergence of reinforcement learning has created a demand for
robust statistical inference methods for the parameter estimates computed using
these algorithms. Existing methods for statistical inference in online learning
are restricted to settings involving independently sampled observations, while
existing statistical inference methods in reinforcement learning (RL) are
limited to the batch setting. The online bootstrap is a flexible and efficient
approach for statistical inference in linear stochastic approximation
algorithms, but its efficacy in settings involving Markov noise, such as RL,
has yet to be explored. In this paper, we study the use of the online bootstrap
method for statistical inference in RL. In particular, we focus on the temporal
difference (TD) learning and Gradient TD (GTD) learning algorithms, which are
themselves special instances of linear stochastic approximation under Markov
noise. The method is shown to be distributionally consistent for statistical
inference in policy evaluation, and numerical experiments are included to
demonstrate the effectiveness of this algorithm at statistical inference tasks
across a range of real RL environments.

    

### [[2108.03712] Generalizing Dynamic Mode Decomposition: Balancing Accuracy and Expressiveness in Koopman Approximations](http://arxiv.org/abs/2108.03712)


  This paper tackles the data-driven approximation of unknown dynamical systems
using Koopman-operator methods. Given a dictionary of functions, these methods
approximate the projection of the action of the operator on the
finite-dimensional subspace spanned by the dictionary. We propose the Tunable
Symmetric Subspace Decomposition algorithm to refine the dictionary, balancing
its expressiveness and accuracy. Expressiveness corresponds to the ability of
the dictionary to describe the evolution of as many observables as possible and
accuracy corresponds to the ability to correctly predict their evolution. Based
on the observation that Koopman-invariant subspaces give rise to exact
predictions, we reason that prediction accuracy is a function of the degree of
invariance of the subspace generated by the dictionary and provide a
data-driven measure to measure invariance proximity. The proposed algorithm
iteratively prunes the initial functional space to identify a refined
dictionary of functions that satisfies the desired level of accuracy while
retaining as much of the original expressiveness as possible. We provide a full
characterization of the algorithm properties and show that it generalizes both
Extended Dynamic Mode Decomposition and Symmetric Subspace Decomposition.
Simulations on planar systems show the effectiveness of the proposed methods in
producing Koopman approximations of tunable accuracy that capture relevant
information about the dynamical system.

    

### [[2108.03713] On the Difficulty of Generalizing Reinforcement Learning Framework for Combinatorial Optimization](http://arxiv.org/abs/2108.03713)


  Combinatorial optimization problems (COPs) on the graph with real-life
applications are canonical challenges in Computer Science. The difficulty of
finding quality labels for problem instances holds back leveraging supervised
learning across combinatorial problems. Reinforcement learning (RL) algorithms
have recently been adopted to solve this challenge automatically. The
underlying principle of this approach is to deploy a graph neural network (GNN)
for encoding both the local information of the nodes and the graph-structured
data in order to capture the current state of the environment. Then, it is
followed by the actor to learn the problem-specific heuristics on its own and
make an informed decision at each state for finally reaching a good solution.
Recent studies on this subject mainly focus on a family of combinatorial
problems on the graph, such as the travel salesman problem, where the proposed
model aims to find an ordering of vertices that optimizes a given objective
function. We use the security-aware phone clone allocation in the cloud as a
classical quadratic assignment problem (QAP) to investigate whether or not deep
RL-based model is generally applicable to solve other classes of such hard
problems. Extensive empirical evaluation shows that existing RL-based model may
not generalize to QAP.

    

### [[2108.03718] Meta-Reinforcement Learning in Broad and Non-Parametric Environments](http://arxiv.org/abs/2108.03718)


  Recent state-of-the-art artificial agents lack the ability to adapt rapidly
to new tasks, as they are trained exclusively for specific objectives and
require massive amounts of interaction to learn new skills. Meta-reinforcement
learning (meta-RL) addresses this challenge by leveraging knowledge learned
from training tasks to perform well in previously unseen tasks. However,
current meta-RL approaches limit themselves to narrow parametric task
distributions, ignoring qualitative differences between tasks that occur in the
real world. In this paper, we introduce TIGR, a Task-Inference-based meta-RL
algorithm using Gaussian mixture models (GMM) and gated Recurrent units,
designed for tasks in non-parametric environments. We employ a generative model
involving a GMM to capture the multi-modality of the tasks. We decouple the
policy training from the task-inference learning and efficiently train the
inference mechanism on the basis of an unsupervised reconstruction objective.
We provide a benchmark with qualitatively distinct tasks based on the
half-cheetah environment and demonstrate the superior performance of TIGR
compared to state-of-the-art meta-RL approaches in terms of sample efficiency
(3-10 times faster), asymptotic performance, and applicability in
non-parametric environments with zero-shot adaptation.

    

### [[2108.03762] EVGen: Adversarial Networks for Learning Electric Vehicle Charging Loads and Hidden Representations](http://arxiv.org/abs/2108.03762)


  The nexus between transportation, the power grid, and consumer behavior is
more pronounced than ever before as the race to decarbonize the transportation
sector intensifies. Electrification in the transportation sector has led to
technology shifts and rapid deployment of electric vehicles (EVs). The
potential increase in stochastic and spatially heterogeneous charging load
presents a unique challenge that is not well studied, and will have significant
impacts on grid operations, emissions, and system reliability if not managed
effectively. Realistic scenario generators can help operators prepare, and
machine learning can be leveraged to this end. In this work, we develop
generative adversarial networks (GANs) to learn distributions of electric
vehicle (EV) charging sessions and disentangled representations. We show that
this model structure successfully parameterizes unlabeled temporal and power
patterns without supervision and is able to generate synthetic data conditioned
on these parameters. We benchmark the generation capability of this model with
Gaussian Mixture Models (GMMs), and empirically show that our proposed model
framework is better at capturing charging distributions and temporal dynamics.

    

### [[2108.03782] Pathfinder: Parallel quasi-Newton variational inference](http://arxiv.org/abs/2108.03782)


  We introduce Pathfinder, a variational method for approximately sampling from
differentiable log densities. Starting from a random initialization, Pathfinder
locates normal approximations to the target density along a quasi-Newton
optimization path, with local covariance estimated using the inverse Hessian
estimates produced by the optimizer. Pathfinder returns draws from the
approximation with the lowest estimated Kullback-Leibler (KL) divergence to the
true posterior. We evaluate Pathfinder on a wide range of posterior
distributions, demonstrating that its approximate draws are better than those
from automatic differentiation variational inference (ADVI) and comparable to
those produced by short chains of dynamic Hamiltonian Monte Carlo (HMC), as
measured by 1-Wasserstein distance. Compared to ADVI and short dynamic HMC
runs, Pathfinder requires one to two orders of magnitude fewer log density and
gradient evaluations, with greater reductions for more challenging posteriors.
Importance resampling over multiple runs of Pathfinder improves the diversity
of approximate draws, reducing 1-Wasserstein distance further and providing a
measure of robustness to optimization failures on plateaus, saddle points, or
in minor modes. The Monte Carlo KL-divergence estimates are embarrassingly
parallelizable in the core Pathfinder algorithm, as are multiple runs in the
resampling version, further increasing Pathfinder's speed advantage with
multiple cores.

    

### [[2108.03786] Multi-Slice Net: A novel light weight framework for COVID-19 Diagnosis](http://arxiv.org/abs/2108.03786)


  This paper presents a novel lightweight COVID-19 diagnosis framework using CT
scans. Our system utilises a novel two-stage approach to generate robust and
efficient diagnoses across heterogeneous patient level inputs. We use a
powerful backbone network as a feature extractor to capture discriminative
slice-level features. These features are aggregated by a lightweight network to
obtain a patient level diagnosis. The aggregation network is carefully designed
to have a small number of trainable parameters while also possessing sufficient
capacity to generalise to diverse variations within different CT volumes and to
adapt to noise introduced during the data acquisition. We achieve a significant
performance increase over the baselines when benchmarked on the SPGC COVID-19
Radiomics Dataset, despite having only 2.5 million trainable parameters and
requiring only 0.623 seconds on average to process a single patient's CT volume
using an Nvidia-GeForce RTX 2080 GPU.

    

### [[2108.03803] Mis-spoke or mis-lead: Achieving Robustness in Multi-Agent Communicative Reinforcement Learning](http://arxiv.org/abs/2108.03803)


  Recent studies in multi-agent communicative reinforcement learning (MACRL)
demonstrate that multi-agent coordination can be significantly improved when
communication between agents is allowed. Meanwhile, advances in adversarial
machine learning (ML) have shown that ML and reinforcement learning (RL) models
are vulnerable to a variety of attacks that significantly degrade the
performance of learned behaviours. However, despite the obvious and growing
importance, the combination of adversarial ML and MACRL remains largely
uninvestigated. In this paper, we make the first step towards conducting
message attacks on MACRL methods. In our formulation, one agent in the
cooperating group is taken over by an adversary and can send malicious messages
to disrupt a deployed MACRL-based coordinated strategy during the deployment
phase. We further our study by developing a defence method via message
reconstruction. Finally, we address the resulting arms race, i.e., we consider
the ability of the malicious agent to adapt to the changing and improving
defensive communicative policies of the benign agents. Specifically, we model
the adversarial MACRL problem as a two-player zero-sum game and then utilize
Policy-Space Response Oracle to achieve communication robustness. Empirically,
we demonstrate that MACRL methods are vulnerable to message attacks while our
defence method the game-theoretic framework can effectively improve the
robustness of MACRL.

    

### [[2108.03812] Whittle Index for A Class of Restless Bandits with Imperfect Observations](http://arxiv.org/abs/2108.03812)


  We consider a class of restless bandit problems that finds a broad
application area in stochastic optimization, reinforcement learning and
operations research. In our model, there are $N$ independent $2$-state Markov
processes that may be observed and accessed for accruing rewards. The
observation is error-prone, i.e., both false alarm and miss detection may
happen. Furthermore, the user can only choose a subset of $M~(M<N)$ processes
to observe at each discrete time. If a process in state~$1$ is correctly
observed, then it will offer some reward. Due to the partial and imperfect
observation model, the system is formulated as a restless multi-armed bandit
problem with an information state space of uncountable cardinality. Restless
bandit problems with finite state spaces are PSPACE-HARD in general. In this
paper, we establish a low-complexity algorithm that achieves a strong
performance for this class of restless bandits. Under certain conditions, we
theoretically prove the existence (indexability) of Whittle index and its
equivalence to our algorithm. When those conditions do not hold, we show by
numerical experiments the near-optimal performance of our algorithm in general.

    

### [[2108.03834] Bob and Alice Go to a Bar: Reasoning About Future With Probabilistic Programs](http://arxiv.org/abs/2108.03834)


  Agent preferences should be specified stochastically rather than
deterministically. Planning as inference with stochastic preferences naturally
describes agent behaviors, does not require introducing rewards and exponential
weighing of behaviors, and allows to reason about agents using the solid
foundation of Bayesian statistics. Stochastic conditioning is the formalism
behind agents with stochastic preferences.

    

### [[2108.03837] Online Multiobjective Minimax Optimization and Applications](http://arxiv.org/abs/2108.03837)


  We introduce a simple but general online learning framework, in which at
every round, an adaptive adversary introduces a new game, consisting of an
action space for the learner, an action space for the adversary, and a vector
valued objective function that is convex-concave in every coordinate. The
learner and the adversary then play in this game. The learner's goal is to play
so as to minimize the maximum coordinate of the cumulative vector-valued loss.
The resulting one-shot game is not convex-concave, and so the minimax theorem
does not apply. Nevertheless, we give a simple algorithm that can compete with
the setting in which the adversary must announce their action first, with
optimally diminishing regret.
We demonstrate the power of our simple framework by using it to derive
optimal bounds and algorithms across a variety of domains. This includes no
regret learning: we can recover optimal algorithms and bounds for minimizing
external regret, internal regret, adaptive regret, multigroup regret,
subsequence regret, and a notion of regret in the sleeping experts setting.
Next, we use it to derive a variant of Blackwell's Approachability Theorem,
which we term "Fast Polytope Approachability". Finally, we are able to recover
recently derived algorithms and bounds for online adversarial multicalibration
and related notions (mean-conditioned moment multicalibration, and prediction
interval multivalidity).

    

### [[2108.03857] GAN Computers Generate Arts? A Survey on Visual Arts, Music, and Literary Text Generation using Generative Adversarial Network](http://arxiv.org/abs/2108.03857)


  "Art is the lie that enables us to realize the truth." - Pablo Picasso. For
centuries, humans have dedicated themselves to producing arts to convey their
imagination. The advancement in technology and deep learning in particular, has
caught the attention of many researchers trying to investigate whether art
generation is possible by computers and algorithms. Using generative
adversarial networks (GANs), applications such as synthesizing photorealistic
human faces and creating captions automatically from images were realized. This
survey takes a comprehensive look at the recent works using GANs for generating
visual arts, music, and literary text. A performance comparison and description
of the various GAN architecture are also presented. Finally, some of the key
challenges in art generation using GANs are highlighted along with
recommendations for future work.

    

### [[2108.03887] Collapsing the Decision Tree: the Concurrent Data Predictor](http://arxiv.org/abs/2108.03887)


  A family of concurrent data predictors is derived from the decision tree
classifier by removing the limitation of sequentially evaluating attributes. By
evaluating attributes concurrently, the decision tree collapses into a flat
structure. Experiments indicate improvements of the prediction accuracy.

    

### [[2108.03888] Efficient Hyperparameter Optimization for Differentially Private Deep Learning](http://arxiv.org/abs/2108.03888)


  Tuning the hyperparameters in the differentially private stochastic gradient
descent (DPSGD) is a fundamental challenge. Unlike the typical SGD, private
datasets cannot be used many times for hyperparameter search in DPSGD; e.g.,
via a grid search. Therefore, there is an essential need for algorithms that,
within a given search space, can find near-optimal hyperparameters for the best
achievable privacy-utility tradeoffs efficiently. We formulate this problem
into a general optimization framework for establishing a desirable
privacy-utility tradeoff, and systematically study three cost-effective
algorithms for being used in the proposed framework: evolutionary, Bayesian,
and reinforcement learning. Our experiments, for hyperparameter tuning in DPSGD
conducted on MNIST and CIFAR-10 datasets, show that these three algorithms
significantly outperform the widely used grid search baseline. As this paper
offers a first-of-a-kind framework for hyperparameter tuning in DPSGD, we
discuss existing challenges and open directions for future studies. As we
believe our work has implications to be utilized in the pipeline of private
deep learning, we open-source our code at
this https URL.

    

### [[2108.03891] Probabilistic Active Learning for Active Class Selection](http://arxiv.org/abs/2108.03891)


  In machine learning, active class selection (ACS) algorithms aim to actively
select a class and ask the oracle to provide an instance for that class to
optimize a classifier's performance while minimizing the number of requests. In
this paper, we propose a new algorithm (PAL-ACS) that transforms the ACS
problem into an active learning task by introducing pseudo instances. These are
used to estimate the usefulness of an upcoming instance for each class using
the performance gain model from probabilistic active learning. Our experimental
evaluation (on synthetic and real data) shows the advantages of our algorithm
compared to state-of-the-art algorithms. It effectively prefers the sampling of
difficult classes and thereby improves the classification performance.

    

### [[2108.03894] FIFA: Fast Inference Approximation for Action Segmentation](http://arxiv.org/abs/2108.03894)


  We introduce FIFA, a fast approximate inference method for action
segmentation and alignment. Unlike previous approaches, FIFA does not rely on
expensive dynamic programming for inference. Instead, it uses an approximate
differentiable energy function that can be minimized using gradient-descent.
FIFA is a general approach that can replace exact inference improving its speed
by more than 5 times while maintaining its performance. FIFA is an anytime
inference algorithm that provides a better speed vs. accuracy trade-off
compared to exact inference. We apply FIFA on top of state-of-the-art
approaches for weakly supervised action segmentation and alignment as well as
fully supervised action segmentation. FIFA achieves state-of-the-art results on
most metrics on two action segmentation datasets.

    

### [[2108.03913] Unified Regularity Measures for Sample-wise Learning and Generalization](http://arxiv.org/abs/2108.03913)


  Fundamental machine learning theory shows that different samples contribute
unequally both in learning and testing processes. Contemporary studies on DNN
imply that such sample di?erence is rooted on the distribution of intrinsic
pattern information, namely sample regularity. Motivated by the recent
discovery on network memorization and generalization, we proposed a pair of
sample regularity measures for both processes with a formulation-consistent
representation. Specifically, cumulative binary training/generalizing loss
(CBTL/CBGL), the cumulative number of correct classi?cations of the
training/testing sample within training stage, is proposed to quantize the
stability in memorization-generalization process; while
forgetting/mal-generalizing events, i.e., the mis-classification of previously
learned or generalized sample, are utilized to represent the uncertainty of
sample regularity with respect to optimization dynamics. Experiments validated
the effectiveness and robustness of the proposed approaches for mini-batch SGD
optimization. Further applications on training/testing sample selection show
the proposed measures sharing the uni?ed computing procedure could benefit for
both tasks.

    

### [[2108.03917] LatticeNet: Fast Spatio-Temporal Point Cloud Segmentation Using Permutohedral Lattices](http://arxiv.org/abs/2108.03917)


  Deep convolutional neural networks (CNNs) have shown outstanding performance
in the task of semantically segmenting images. Applying the same methods on 3D
data still poses challenges due to the heavy memory requirements and the lack
of structured data. Here, we propose LatticeNet, a novel approach for 3D
semantic segmentation, which takes raw point clouds as input. A PointNet
describes the local geometry which we embed into a sparse permutohedral
lattice. The lattice allows for fast convolutions while keeping a low memory
footprint. Further, we introduce DeformSlice, a novel learned data-dependent
interpolation for projecting lattice features back onto the point cloud. We
present results of 3D segmentation on multiple datasets where our method
achieves state-of-the-art performance. We also extend and evaluate our network
for instance and dynamic object segmentation.

    

### [[2108.03940] Some thoughts on catastrophic forgetting and how to learn an algorithm](http://arxiv.org/abs/2108.03940)


  The work of McCloskey and Cohen popularized the concept of catastrophic
interference. They used a neural network that tried to learn addition using two
groups of examples as two different tasks. In their case, learning the second
task rapidly deteriorated the acquired knowledge about the previous one. This
could be a symptom of a fundamental problem: addition is an algorithmic task
that should not be learned through pattern recognition. We propose to use a
neural network with a different architecture that can be trained to recover the
correct algorithm for the addition of binary numbers. We test it in the setting
proposed by McCloskey and Cohen and training on random examples one by one. The
neural network not only does not suffer from catastrophic forgetting but it
improves its predictive power on unseen pairs of numbers as training
progresses. This work emphasizes the importance that neural network
architecture has for the emergence of catastrophic forgetting and introduces a
neural network that is able to learn an algorithm.

    

### [[2108.03945] A Neural Approach for Detecting Morphological Analogies](http://arxiv.org/abs/2108.03945)


  Analogical proportions are statements of the form "A is to B as C is to D"
that are used for several reasoning and classification tasks in artificial
intelligence and natural language processing (NLP). For instance, there are
analogy based approaches to semantics as well as to morphology. In fact,
symbolic approaches were developed to solve or to detect analogies between
character strings, e.g., the axiomatic approach as well as that based on
Kolmogorov complexity. In this paper, we propose a deep learning approach to
detect morphological analogies, for instance, with reinflexion or conjugation.
We present empirical results that show that our framework is competitive with
the above-mentioned state of the art symbolic approaches. We also explore
empirically its transferability capacity across languages, which highlights
interesting similarities between them.

    

### [[2108.03947] On the Hyperparameters in Stochastic Gradient Descent with Momentum](http://arxiv.org/abs/2108.03947)


  Following the same routine as [SSJ20], we continue to present the theoretical
analysis for stochastic gradient descent with momentum (SGD with momentum) in
this paper. Differently, for SGD with momentum, we demonstrate it is the two
hyperparameters together, the learning rate and the momentum coefficient, that
play the significant role for the linear rate of convergence in non-convex
optimization. Our analysis is based on the use of a hyperparameters-dependent
stochastic differential equation (hp-dependent SDE) that serves as a continuous
surrogate for SGD with momentum. Similarly, we establish the linear convergence
for the continuous-time formulation of SGD with momentum and obtain an explicit
expression for the optimal linear rate by analyzing the spectrum of the
Kramers-Fokker-Planck operator. By comparison, we demonstrate how the optimal
linear rate of convergence and the final gap for SGD only about the learning
rate varies with the momentum coefficient increasing from zero to one when the
momentum is introduced. Then, we propose a mathematical interpretation why the
SGD with momentum converges faster and more robust about the learning rate than
the standard SGD in practice. Finally, we show the Nesterov momentum under the
existence of noise has no essential difference with the standard momentum.

    

### [[2108.03952] Safe Deep Reinforcement Learning for Multi-Agent Systems with Continuous Action Spaces](http://arxiv.org/abs/2108.03952)


  Multi-agent control problems constitute an interesting area of application
for deep reinforcement learning models with continuous action spaces. Such
real-world applications, however, typically come with critical safety
constraints that must not be violated. In order to ensure safety, we enhance
the well-known multi-agent deep deterministic policy gradient (MADDPG)
framework by adding a safety layer to the deep policy network. %which
automatically corrects invalid actions. In particular, we extend the idea of
linearizing the single-step transition dynamics, as was done for single-agent
systems in Safe DDPG (Dalal et al., 2018), to multi-agent settings. We
additionally propose to circumvent infeasibility problems in the action
correction step using soft constraints (Kerrigan & Maciejowski, 2000). Results
from the theory of exact penalty functions can be used to guarantee constraint
satisfaction of the soft constraints under mild assumptions. We empirically
find that the soft formulation achieves a dramatic decrease in constraint
violations, making safety available even during the learning procedure.

    

### [[2108.03953] A Framework for Joint Unsupervised Learning of Cluster-Aware Embedding for Heterogeneous Networks](http://arxiv.org/abs/2108.03953)


  Heterogeneous Information Network (HIN) embedding refers to the
low-dimensional projections of the HIN nodes that preserve the HIN structure
and semantics. HIN embedding has emerged as a promising research field for
network analysis as it enables downstream tasks such as clustering and node
classification. In this work, we propose \ours for joint learning of cluster
embeddings as well as cluster-aware HIN embedding. We assume that the connected
nodes are highly likely to fall in the same cluster, and adopt a variational
approach to preserve the information in the pairwise relations in a
cluster-aware manner. In addition, we deploy contrastive modules to
simultaneously utilize the information in multiple meta-paths, thereby
alleviating the meta-path selection problem - a challenge faced by many of the
famous HIN embedding approaches. The HIN embedding, thus learned, not only
improves the clustering performance but also preserves pairwise proximity as
well as the high-order HIN structure. We show the effectiveness of our approach
by comparing it with many competitive baselines on three real-world datasets on
clustering and downstream node classification.

    

### [[2108.03978] VeRLPy: Python Library for Verification of Digital Designs with Reinforcement Learning](http://arxiv.org/abs/2108.03978)


  Digital hardware is verified by comparing its behavior against a reference
model on a range of randomly generated input signals. The random generation of
the inputs hopes to achieve sufficient coverage of the different parts of the
design. However, such coverage is often difficult to achieve, amounting to
large verification efforts and delays. An alternative is to use Reinforcement
Learning (RL) to generate the inputs by learning to prioritize those inputs
which can more efficiently explore the design under test. In this work, we
present VeRLPy an open-source library to allow RL-driven verification with
limited additional engineering overhead. This contributes to two broad
movements within the EDA community of (a) moving to open-source toolchains and
(b) reducing barriers for development with Python support. We also demonstrate
the use of VeRLPy for a few designs and establish its value over randomly
generated input signals.

    

### [[2108.03979] Efficient Majority Voting in Digital Hardware](http://arxiv.org/abs/2108.03979)


  In recent years, machine learning methods became increasingly important for a
manifold number of applications. However, they often suffer from high
computational requirements impairing their efficient use in real-time systems,
even when employing dedicated hardware accelerators. Ensemble learning methods
are especially suitable for hardware acceleration since they can be constructed
from individual learners of low complexity and thus offer large parallelization
potential. For classification, the outputs of these learners are typically
combined by majority voting, which often represents the bottleneck of a
hardware accelerator for ensemble inference. In this work, we present a novel
architecture that allows obtaining a majority decision in a number of clock
cycles that is logarithmic in the number of inputs. We show, that for the
example application of handwritten digit recognition a random forest processing
engine employing this majority decision architecture implemented on an FPGA
allows the classification of more than seven million images per second.

    

### [[2108.03980] Decentralized Deep Learning for Mobile Edge Computing: A Survey on Communication Efficiency and Trustworthiness](http://arxiv.org/abs/2108.03980)


  A wider coverage and a better solution to latency reduction in 5G
necessitates its combination with mobile edge computing (MEC) technology.
Decentralized deep learning (DDL) as a promising solution to privacy-preserving
data processing for millions of edge smart devices, it leverages federated
learning within the networking of local models, without disclosing a client's
raw data. Especially, in industries such as finance and healthcare where
sensitive data of transactions and personal medical records is cautiously
maintained, DDL facilitates the collaboration among these institutes to improve
the performance of local models, while protecting data privacy of participating
clients. In this survey paper, we demonstrate technical fundamentals of DDL for
benefiting many walks of society through decentralized learning. Furthermore,
we offer a comprehensive overview of recent challenges of DDL and the most
relevant solutions from novel perspectives of communication efficiency and
trustworthiness.

    

### [[2108.03981] A Credibility-aware Swarm-Federated Deep Learning Framework in Internet of Vehicles](http://arxiv.org/abs/2108.03981)


  Federated Deep Learning (FDL) is helping to realize distributed machine
learning in the Internet of Vehicles (IoV). However, FDL's global model needs
multiple clients to upload learning model parameters, thus still existing
unavoidable communication overhead and data privacy risks. The recently
proposed Swarm Learning (SL) provides a decentralized machine-learning approach
uniting edge computing and blockchain-based coordination without the need for a
central coordinator. This paper proposes a Swarm-Federated Deep Learning
framework in the IoV system (IoV-SFDL) that integrates SL into the FDL
framework. The IoV-SFDL organizes vehicles to generate local SL models with
adjacent vehicles based on the blockchain empowered SL, then aggregates the
global FDL model among different SL groups with a proposed credibility weights
prediction algorithm. Extensive experimental results demonstrate that compared
with the baseline frameworks, the proposed IoV-SFDL framework achieves a 16.72%
reduction in edge-to-global communication overhead while improving about 5.02%
in model performance with the same training iterations.

    

### [[2108.03995] Predicting Mechanically Driven Full-Field Quantities of Interest with Deep Learning-Based Metamodels](http://arxiv.org/abs/2108.03995)


  Using simulation to predict the mechanical behavior of heterogeneous
materials has applications ranging from topology optimization to multi-scale
structural analysis. However, full-fidelity simulation techniques such as
Finite Element Analysis can be prohibitively computationally expensive when
they are used to explore the massive input parameter space of heterogeneous
materials. Therefore, there has been significant recent interest in machine
learning-based models that, once trained, can predict mechanical behavior at a
fraction of the computational cost. Over the past several years, research in
this area has been focused mainly on predicting single Quantities of Interest
(QoIs). However, there has recently been an increased interest in a more
challenging problem: predicting full-field QoI (e.g., displacement/strain
fields, damage fields) for mechanical problems. Due to the added complexity of
full-field information, network architectures that perform well on single QoI
problems may perform poorly in the full-field QoI problem setting. The work
presented in this paper is twofold. First, we made a significant extension to
the Mechanical MNIST dataset designed to enable the investigation of full field
QoI prediction. Specifically, we added Finite Element simulation results of
quasi-static brittle fracture in a heterogeneous material captured with the
phase-field method. Second, we established strong baseline performance for
predicting full-field QoI with MultiRes-WNet architecture. In addition to
presenting the results in this paper, we have released our model implementation
and the Mechanical MNIST Crack Path dataset under open-source licenses. We
anticipate that future researchers will directly use our model architecture on
related datasets and potentially design models that exceed the baseline
performance for predicting full-field QoI established in this paper.

    

### [[2108.04012] Uncertainty quantification for industrial design using dictionaries of reduced order models](http://arxiv.org/abs/2108.04012)


  We consider the dictionary-based ROM-net (Reduced Order Model) framework [T.
Daniel, F. Casenave, N. Akkari, D. Ryckelynck, Model order reduction assisted
by deep neural networks (ROM-net), Advanced modeling and Simulation in
Engineering Sciences 7 (16), 2020] and summarize the underlying methodologies
and their recent improvements. The main contribution of this work is the
application of the complete workflow to a real-life industrial model of an
elastoviscoplastic high-pressure turbine blade subjected to thermal,
centrifugal and pressure loadings, for the quantification of the uncertainty on
dual quantities (such as the accumulated plastic strain and the stress tensor),
generated by the uncertainty on the temperature loading field. The
dictionary-based ROM-net computes predictions of dual quantities of interest
for 1008 Monte Carlo draws of the temperature loading field in 2 hours and 48
minutes, which corresponds to a speedup greater than 600 with respect to a
reference parallel solver using domain decomposition, with a relative error in
the order of 2%. Another contribution of this work consists in the derivation
of a meta-model to reconstruct the dual quantities of interest over the
complete mesh from their values on the reduced integration points.

    

### [[2108.04031] DGEM: A New Dual-modal Graph Embedding Method in Recommendation System](http://arxiv.org/abs/2108.04031)


  In the current deep learning based recommendation system, the embedding
method is generally employed to complete the conversion from the
high-dimensional sparse feature vector to the low-dimensional dense feature
vector. However, as the dimension of the input vector of the embedding layer is
too large, the addition of the embedding layer significantly slows down the
convergence speed of the entire neural network, which is not acceptable in
real-world scenarios. In addition, as the interaction between users and items
increases and the relationship between items becomes more complicated, the
embedding method proposed for sequence data is no longer suitable for graphic
data in the current real environment. Therefore, in this paper, we propose the
Dual-modal Graph Embedding Method (DGEM) to solve these problems. DGEM includes
two modes, static and dynamic. We first construct the item graph to extract the
graph structure and use random walk of unequal probability to capture the
high-order proximity between the items. Then we generate the graph embedding
vector through the Skip-Gram model, and finally feed the downstream deep neural
network for the recommendation task. The experimental results show that DGEM
can mine the high-order proximity between items and enhance the expression
ability of the recommendation model. Meanwhile it also improves the
recommendation performance by utilizing the time dependent relationship between
items.

    

### [[2108.04033] Reproducible Performance Optimization of Complex Applications on the Edge-to-Cloud Continuum](http://arxiv.org/abs/2108.04033)


  In more and more application areas, we are witnessing the emergence of
complex workflows that combine computing, analytics and learning. They often
require a hybrid execution infrastructure with IoT devices interconnected to
cloud/HPC systems (aka Computing Continuum). Such workflows are subject to
complex constraints and requirements in terms of performance, resource usage,
energy consumption and financial costs. This makes it challenging to optimize
their configuration and deployment. We propose a methodology to support the
optimization of real-life applications on the Edge-to-Cloud Continuum. We
implement it as an extension of E2Clab, a previously proposed framework
supporting the complete experimental cycle across the Edge-to-Cloud Continuum.
Our approach relies on a rigorous analysis of possible configurations in a
controlled testbed environment to understand their behaviour and related
performance trade-offs. We illustrate our methodology by optimizing Pl@ntNet, a
world-wide plant identification application. Our methodology can be generalized
to other applications in the Edge-to-Cloud Continuum.

    

### [[2108.04035] Mixture of Linear Models Co-supervised by Deep Neural Networks](http://arxiv.org/abs/2108.04035)


  Deep neural network (DNN) models have achieved phenomenal success for
applications in many domains, ranging from academic research in science and
engineering to industry and business. The modeling power of DNN is believed to
have come from the complexity and over-parameterization of the model, which on
the other hand has been criticized for the lack of interpretation. Although
certainly not true for every application, in some applications, especially in
economics, social science, healthcare industry, and administrative decision
making, scientists or practitioners are resistant to use predictions made by a
black-box system for multiple reasons. One reason is that a major purpose of a
study can be to make discoveries based upon the prediction function, e.g., to
reveal the relationships between measurements. Another reason can be that the
training dataset is not large enough to make researchers feel completely sure
about a purely data-driven result. Being able to examine and interpret the
prediction function will enable researchers to connect the result with existing
knowledge or gain insights about new directions to explore. Although classic
statistical models are much more explainable, their accuracy often falls
considerably below DNN. In this paper, we propose an approach to fill the gap
between relatively simple explainable models and DNN such that we can more
flexibly tune the trade-off between interpretability and accuracy. Our main
idea is a mixture of discriminative models that is trained with the guidance
from a DNN. Although mixtures of discriminative models have been studied
before, our way of generating the mixture is quite different.

    

### [[2108.04036] Fed-BEV: A Federated Learning Framework for Modelling Energy Consumption of Battery Electric Vehicles](http://arxiv.org/abs/2108.04036)


  Recently, there has been an increasing interest in the roll-out of electric
vehicles (EVs) in the global automotive market. Compared to conventional
internal combustion engine vehicles (ICEVs), EVs can not only help users reduce
monetary costs in their daily commuting, but also can effectively help mitigate
the increasing level of traffic emissions produced in cities. Among many
others, battery electric vehicles (BEVs) exclusively use chemical energy stored
in their battery packs for propulsion. Hence, it becomes important to
understand how much energy can be consumed by such vehicles in various traffic
scenarios towards effective energy management. To address this challenge, we
propose a novel framework in this paper by leveraging the federated learning
approaches for modelling energy consumption for BEVs (Fed-BEV). More
specifically, a group of BEVs involved in the Fed-BEV framework can learn from
each other to jointly enhance their energy consumption model. We present the
design of the proposed system architecture and implementation details in a
co-simulation environment. Finally, comparative studies and simulation results
are discussed to illustrate the efficacy of our proposed framework for accurate
energy modelling of BEVs.

    

### [[2108.04051] A Streamwise GAN Vocoder for Wideband Speech Coding at Very Low Bit Rate](http://arxiv.org/abs/2108.04051)


  Recently, GAN vocoders have seen rapid progress in speech synthesis, starting
to outperform autoregressive models in perceptual quality with much higher
generation speed. However, autoregressive vocoders are still the common choice
for neural generation of speech signals coded at very low bit rates. In this
paper, we present a GAN vocoder which is able to generate wideband speech
waveforms from parameters coded at 1.6 kbit/s. The proposed model is a modified
version of the StyleMelGAN vocoder that can run in frame-by-frame manner,
making it suitable for streaming applications. The experimental results show
that the proposed model significantly outperforms prior autoregressive vocoders
like LPCNet for very low bit rate speech coding, with computational complexity
of about 5 GMACs, providing a new state of the art in this domain. Moreover,
this streamwise adversarial vocoder delivers quality competitive to advanced
speech codecs such as EVS at 5.9 kbit/s on clean speech, which motivates
further usage of feed-forward fully-convolutional models for low bit rate
speech coding.

    

### [[2108.04052] Training of deep residual networks with stochastic MG/OPT](http://arxiv.org/abs/2108.04052)


  We train deep residual networks with a stochastic variant of the nonlinear
multigrid method MG/OPT. To build the multilevel hierarchy, we use the
dynamical systems viewpoint specific to residual networks. We report
significant speed-ups and additional robustness for training MNIST on deep
residual networks. Our numerical experiments also indicate that multilevel
training can be used as a pruning technique, as many of the auxiliary networks
have accuracies comparable to the original network.

    

### [[2108.04055] The Role of Global Labels in Few-Shot Classification and How to Infer Them](http://arxiv.org/abs/2108.04055)


  Few-shot learning (FSL) is a central problem in meta-learning, where learners
must quickly adapt to new tasks given limited training data. Surprisingly,
recent works have outperformed meta-learning methods tailored to FSL by casting
it as standard supervised learning to jointly classify all classes shared
across tasks. However, this approach violates the standard FSL setting by
requiring global labels shared across tasks, which are often unavailable in
practice. In this paper, we show why solving FSL via standard classification is
theoretically advantageous. This motivates us to propose Meta Label Learning
(MeLa), a novel algorithm that infers global labels and obtains robust few-shot
models via standard classification. Empirically, we demonstrate that MeLa
outperforms meta-learning competitors and is comparable to the oracle setting
where ground truth labels are given. We provide extensive ablation studies to
highlight the key properties of the proposed strategy.

    

### [[2108.04058] An Interpretable Probabilistic Model for Short-Term Solar Power Forecasting Using Natural Gradient Boosting](http://arxiv.org/abs/2108.04058)


  The stochastic nature of photovoltaic (PV) power has led both academia and
industry to a large amount of research work aiming at the development of
accurate PV power forecasting models. However, most of those models are based
on machine learning algorithms and are considered as black boxes which do not
provide any insight or explanation about their predictions. Therefore, their
direct implementation in environments, where transparency is required, and the
trust associated with their predictions may be questioned. To this end, we
propose a two stage probabilistic forecasting framework able to generate highly
accurate, reliable, and sharp forecasts yet offering full transparency on both
the point forecasts and the prediction intervals (PIs). In the first stage, we
exploit natural gradient boosting (NGBoost) for yielding probabilistic
forecasts while in the second stage, we calculate the Shapley additive
explanation (SHAP) values in order to fully understand why a prediction was
made. To highlight the performance and the applicability of the proposed
framework, real data from two PV parks located in Southern Germany are
employed. Initially, the natural gradient boosting is thoroughly compared with
two state-of-the-art algorithms, namely Gaussian process and lower upper bound
estimation, in a wide range of forecasting metrics. Secondly, a detailed
analysis of the model's complex nonlinear relationships and interaction effects
between the various features is presented. The latter allows us to interpret
the model, identify some learned physical properties, explain individual
predictions, reduce the computational requirements for the training without
jeopardizing the model accuracy, detect possible bugs, and gain trust in the
model. Finally, we conclude that the model was able to develop nonlinear
relationships following human logic and intuition based on learned physical
properties.

    

### [[2108.04059] Memory-Aware Partitioning of Machine Learning Applications for Optimal Energy Use in Batteryless Systems](http://arxiv.org/abs/2108.04059)


  Sensing systems powered by energy harvesting have traditionally been designed
to tolerate long periods without energy. As the Internet of Things (IoT)
evolves towards a more transient and opportunistic execution paradigm, reducing
energy storage costs will be key for its economic and ecologic viability.
However, decreasing energy storage in harvesting systems introduces reliability
issues. Transducers only produce intermittent energy at low voltage and current
levels, making guaranteed task completion a challenge. Existing ad hoc methods
overcome this by buffering enough energy either for single tasks, incurring
large data-retention overheads, or for one full application cycle, requiring a
large energy buffer. We present Julienning: an automated method for optimizing
the total energy cost of batteryless applications. Using a custom specification
model, developers can describe transient applications as a set of atomically
executed kernels with explicit data dependencies. Our optimization flow can
partition data- and energy-intensive applications into multiple execution
cycles with bounded energy consumption. By leveraging interkernel data
dependencies, these energy-bounded execution cycles minimize the number of
system activations and nonvolatile data transfers, and thus the total energy
overhead. We validate our methodology with two batteryless cameras running
energy-intensive machine learning applications. Results demonstrate that
compared to ad hoc solutions, our method can reduce the required energy storage
by over 94% while only incurring a 0.12% energy overhead.

    

### [[2108.04062] Householder Activations for Provable Robustness against Adversarial Attacks](http://arxiv.org/abs/2108.04062)


  Training convolutional neural networks (CNNs) with a strict Lipschitz
constraint under the l_{2} norm is useful for provable adversarial robustness,
interpretable gradients and stable training. While 1-Lipschitz CNNs can be
designed by enforcing a 1-Lipschitz constraint on each layer, training such
networks requires each layer to have an orthogonal Jacobian matrix (for all
inputs) to prevent gradients from vanishing during backpropagation. A layer
with this property is said to be Gradient Norm Preserving (GNP). To construct
expressive GNP activation functions, we first prove that the Jacobian of any
GNP piecewise linear function is only allowed to change via Householder
transformations for the function to be continuous. Building on this result, we
introduce a class of nonlinear GNP activations with learnable Householder
transformations called Householder activations. A householder activation
parameterized by the vector $\mathbf{v}$ outputs $(\mathbf{I} -
2\mathbf{v}\mathbf{v}^{T})\mathbf{z}$ for its input $\mathbf{z}$ if
$\mathbf{v}^{T}\mathbf{z} \leq 0$; otherwise it outputs $\mathbf{z}$. Existing
GNP activations such as $\mathrm{MaxMin}$ can be viewed as special cases of
$\mathrm{HH}$ activations for certain settings of these transformations. Thus,
networks with $\mathrm{HH}$ activations have higher expressive power than those
with $\mathrm{MaxMin}$ activations. Although networks with $\mathrm{HH}$
activations have nontrivial provable robustness against adversarial attacks, we
further boost their robustness by (i) introducing a certificate regularization
and (ii) relaxing orthogonalization of the last layer of the network. Our
experiments on CIFAR-10 and CIFAR-100 show that our regularized networks with
$\mathrm{HH}$ activations lead to significant improvements in both the standard
and provable robust accuracy over the prior works (gain of 3.65\% and 4.46\% on
CIFAR-100 respectively).

    

### [[2108.04063] Co-learning: Learning from Noisy Labels with Self-supervision](http://arxiv.org/abs/2108.04063)


  Noisy labels, resulting from mistakes in manual labeling or webly data
collecting for supervised learning, can cause neural networks to overfit the
misleading information and degrade the generalization performance.
Self-supervised learning works in the absence of labels and thus eliminates the
negative impact of noisy labels. Motivated by co-training with both supervised
learning view and self-supervised learning view, we propose a simple yet
effective method called Co-learning for learning with noisy labels. Co-learning
performs supervised learning and self-supervised learning in a cooperative way.
The constraints of intrinsic similarity with the self-supervised module and the
structural similarity with the noisily-supervised module are imposed on a
shared common feature encoder to regularize the network to maximize the
agreement between the two constraints. Co-learning is compared with peer
methods on corrupted data from benchmark datasets fairly, and extensive results
are provided which demonstrate that Co-learning is superior to many
state-of-the-art approaches.

    

### [[2108.04074] Model-free inference of unseen attractors: Reconstructing phase space features from a single noisy trajectory using reservoir computing](http://arxiv.org/abs/2108.04074)


  Reservoir computers are powerful tools for chaotic time series prediction.
They can be trained to approximate phase space flows and can thus both predict
future values to a high accuracy, as well as reconstruct the general properties
of a chaotic attractor without requiring a model. In this work, we show that
the ability to learn the dynamics of a complex system can be extended to
systems with co-existing attractors, here a 4-dimensional extension of the
well-known Lorenz chaotic system. We demonstrate that a reservoir computer can
infer entirely unexplored parts of the phase space: a properly trained
reservoir computer can predict the existence of attractors that were never
approached during training and therefore are labelled as unseen. We provide
examples where attractor inference is achieved after training solely on a
single noisy trajectory.

    

### [[2108.04081] Leveraging Uncertainty for Improved Static Malware Detection Under Extreme False Positive Constraints](http://arxiv.org/abs/2108.04081)


  The detection of malware is a critical task for the protection of computing
environments. This task often requires extremely low false positive rates (FPR)
of 0.01% or even lower, for which modern machine learning has no readily
available tools. We introduce the first broad investigation of the use of
uncertainty for malware detection across multiple datasets, models, and feature
types. We show how ensembling and Bayesian treatments of machine learning
methods for static malware detection allow for improved identification of model
errors, uncovering of new malware families, and predictive performance under
extreme false positive constraints. In particular, we improve the true positive
rate (TPR) at an actual realized FPR of 1e-5 from an expected 0.69 for previous
methods to 0.80 on the best performing model class on the Sophos industry scale
dataset. We additionally demonstrate how previous works have used an evaluation
protocol that can lead to misleading results.

    

### [[2108.04085] Bayesian Deep Learning for Partial Differential Equation Parameter Discovery with Sparse and Noisy Data](http://arxiv.org/abs/2108.04085)


  Scientific machine learning has been successfully applied to inverse problems
and PDE discoveries in computational physics. One caveat of current methods
however is the need for large amounts of (clean) data in order to recover full
system responses or underlying physical models. Bayesian methods may be
particularly promising to overcome these challenges as they are naturally less
sensitive to sparse and noisy data. In this paper, we propose to use Bayesian
neural networks (BNN) in order to: 1) Recover the full system states from
measurement data (e.g. temperature, velocity field, etc.). We use Hamiltonian
Monte-Carlo to sample the posterior distribution of a deep and dense BNN, and
show that it is possible to accurately capture physics of varying complexity
without overfitting. 2) Recover the parameters in the underlying partial
differential equation (PDE) governing the physical system. Using the trained
BNN as a surrogate of the system response, we generate datasets of derivatives
potentially comprising the latent PDE of the observed system and perform a
Bayesian linear regression (BLR) between the successive derivatives in space
and time to recover the original PDE parameters. We take advantage of the
confidence intervals on the BNN outputs and introduce the spatial derivative
variance into the BLR likelihood to discard the influence of highly uncertain
surrogate data points, which allows for more accurate parameter discovery. We
demonstrate our approach on a handful of example applied to physics and
non-linear dynamics.

    

### [[2108.04087] Reinforcement Learning for Intelligent Healthcare Systems: A Comprehensive Survey](http://arxiv.org/abs/2108.04087)


  The rapid increase in the percentage of chronic disease patients along with
the recent pandemic pose immediate threats on healthcare expenditure and
elevate causes of death. This calls for transforming healthcare systems away
from one-on-one patient treatment into intelligent health systems, to improve
services, access and scalability, while reducing costs. Reinforcement Learning
(RL) has witnessed an intrinsic breakthrough in solving a variety of complex
problems for diverse applications and services. Thus, we conduct in this paper
a comprehensive survey of the recent models and techniques of RL that have been
developed/used for supporting Intelligent-healthcare (I-health) systems. This
paper can guide the readers to deeply understand the state-of-the-art regarding
the use of RL in the context of I-health. Specifically, we first present an
overview for the I-health systems challenges, architecture, and how RL can
benefit these systems. We then review the background and mathematical modeling
of different RL, Deep RL (DRL), and multi-agent RL models. After that, we
provide a deep literature review for the applications of RL in I-health
systems. In particular, three main areas have been tackled, i.e., edge
intelligence, smart core network, and dynamic treatment regimes. Finally, we
highlight emerging challenges and outline future research directions in driving
the future success of RL in I-health systems, which opens the door for
exploring some interesting and unsolved problems.

    

### [[1811.00414] Quantum principal component analysis only achieves an exponential speedup because of its state preparation assumptions](http://arxiv.org/abs/1811.00414)


  A central roadblock to analyzing quantum algorithms on quantum states is the
lack of a comparable input model for classical algorithms. Inspired by recent
work of the author [E. Tang, STOC'19], we introduce such a model, where we
assume we can efficiently perform $\ell^2$-norm samples of input data, a
natural analogue to quantum algorithms that assume efficient state preparation
of classical data. Though this model produces less practical algorithms than
the (stronger) standard model of classical computation, it captures versions of
many of the features and nuances of quantum linear algebra algorithms. With
this model, we describe classical analogues to Lloyd, Mohseni, and Rebentrost's
quantum algorithms for principal component analysis [Nat. Phys. 10, 631 (2014)]
and nearest-centroid clustering [arXiv:1307.0411]. Since they are only
polynomially slower, these algorithms suggest that the exponential speedups of
their quantum counterparts are simply an artifact of state preparation
assumptions.

    

### [[1903.08543] Optimizing thermodynamic trajectories using evolutionary and gradient-based reinforcement learning](http://arxiv.org/abs/1903.08543)


  Using a model heat engine, we show that neural network-based reinforcement
learning can identify thermodynamic trajectories of maximal efficiency. We
consider both gradient and gradient-free reinforcement learning. We use an
evolutionary learning algorithm to evolve a population of neural networks,
subject to a directive to maximize the efficiency of a trajectory composed of a
set of elementary thermodynamic processes; the resulting networks learn to
carry out the maximally-efficient Carnot, Stirling, or Otto cycles. When given
an additional irreversible process, this evolutionary scheme learns a
previously unknown thermodynamic cycle. Gradient-based reinforcement learning
is able to learn the Stirling cycle, whereas an evolutionary approach achieves
the optimal Carnot cycle. Our results show how the reinforcement learning
strategies developed for game playing can be applied to solve physical problems
conditioned upon path-extensive order parameters.

    

### [[1908.01602] Solving high-dimensional optimal stopping problems using deep learning](http://arxiv.org/abs/1908.01602)


  Nowadays many financial derivatives, such as American or Bermudan options,
are of early exercise type. Often the pricing of early exercise options gives
rise to high-dimensional optimal stopping problems, since the dimension
corresponds to the number of underlying assets. High-dimensional optimal
stopping problems are, however, notoriously difficult to solve due to the
well-known curse of dimensionality. In this work, we propose an algorithm for
solving such problems, which is based on deep learning and computes, in the
context of early exercise option pricing, both approximations of an optimal
exercise strategy and the price of the considered option. The proposed
algorithm can also be applied to optimal stopping problems that arise in other
areas where the underlying stochastic process can be efficiently simulated. We
present numerical results for a large number of example problems, which include
the pricing of many high-dimensional American and Bermudan options, such as
Bermudan max-call options in up to 5000 dimensions. Most of the obtained
results are compared to reference values computed by exploiting the specific
problem design or, where available, to reference values from the literature.
These numerical results suggest that the proposed algorithm is highly effective
in the case of many underlyings, in terms of both accuracy and speed.

    

### [[1909.11799] Manifold Oblique Random Forests: Towards Closing the Gap on Convolutional Deep Networks](http://arxiv.org/abs/1909.11799)


  Decision forests (Forests), in particular random forests and gradient
boosting trees, have demonstrated state-of-the-art accuracy compared to other
methods in many supervised learning scenarios. In particular, Forests dominate
other methods in tabular data, that is, when the feature space is unstructured,
so that the signal is invariant to a permutation of the feature indices.
However, in structured data lying on a manifold (such as images, text, and
speech) deep networks (Networks), specifically convolutional deep networks
(ConvNets), tend to outperform Forests. We conjecture that at least part of the
reason for this is that the input to Networks is not simply the feature
magnitudes, but also their indices. In contrast, naive Forest implementations
fail to explicitly consider feature indices. A recently proposed Forest
approach demonstrates that Forests, for each node, implicitly sample a random
matrix from some specific distribution. These Forests, like some classes of
Networks, learn by partitioning the feature space into convex polytopes
corresponding to linear functions. We build on that approach and show that one
can choose distributions in a manifold-aware fashion to incorporate feature
locality. We demonstrate the empirical performance on data whose features live
on three different manifolds: a torus, images, and time-series. Moreover, we
demonstrate its strength in multivariate simulated settings and also show
superiority in predicting surgical outcome in epilepsy patients and predicting
movement direction from raw stereotactic EEG data from non-motor brain regions.
In all simulations and real data, Manifold Oblique Random Forest (MORF)
algorithm outperforms approaches that ignore feature space structure and
challenges the performance of ConvNets. Moreover, MORF runs fast and maintains
interpretability and theoretical justification.

    

### [[1910.10202] Complex Transformer: A Framework for Modeling Complex-Valued Sequence](http://arxiv.org/abs/1910.10202)


  While deep learning has received a surge of interest in a variety of fields
in recent years, major deep learning models barely use complex numbers.
However, speech, signal and audio data are naturally complex-valued after
Fourier Transform, and studies have shown a potentially richer representation
of complex nets. In this paper, we propose a Complex Transformer, which
incorporates the transformer model as a backbone for sequence modeling; we also
develop attention and encoder-decoder network operating for complex input. The
model achieves state-of-the-art performance on the MusicNet dataset and an
In-phase Quadrature (IQ) signal dataset.

    

### [[1912.04138] A Weak Supervision Approach to Detecting Visual Anomalies for Automated Testing of Graphics Units](http://arxiv.org/abs/1912.04138)


  We present a deep learning system for testing graphics units by detecting
novel visual corruptions in videos. Unlike previous work in which manual
tagging was required to collect labeled training data, our weak supervision
method is fully automatic and needs no human labelling. This is achieved by
reproducing driver bugs that increase the probability of generating
corruptions, and by making use of ideas and methods from the Multiple Instance
Learning (MIL) setting. In our experiments, we significantly outperform
unsupervised methods such as GAN-based models and discover novel corruptions
undetected by baselines, while adhering to strict requirements on accuracy and
efficiency of our real-time system.

    

### [[2002.06487] Maxmin Q-learning: Controlling the Estimation Bias of Q-learning](http://arxiv.org/abs/2002.06487)


  Q-learning suffers from overestimation bias, because it approximates the
maximum action value using the maximum estimated action value. Algorithms have
been proposed to reduce overestimation bias, but we lack an understanding of
how bias interacts with performance, and the extent to which existing
algorithms mitigate bias. In this paper, we 1) highlight that the effect of
overestimation bias on learning efficiency is environment-dependent; 2) propose
a generalization of Q-learning, called \emph{Maxmin Q-learning}, which provides
a parameter to flexibly control bias; 3) show theoretically that there exists a
parameter choice for Maxmin Q-learning that leads to unbiased estimation with a
lower approximation variance than Q-learning; and 4) prove the convergence of
our algorithm in the tabular case, as well as convergence of several previous
Q-learning variants, using a novel Generalized Q-learning framework. We
empirically verify that our algorithm better controls estimation bias in toy
environments, and that it achieves superior performance on several benchmark
problems.

    

### [[2002.07873] A survey of statistical learning techniques as applied to inexpensive pediatric Obstructive Sleep Apnea data](http://arxiv.org/abs/2002.07873)


  Pediatric obstructive sleep apnea affects an estimated 1-5% of
elementary-school aged children and can lead to other detrimental health
problems. Swift diagnosis and treatment are critical to a child's growth and
development, but the variability of symptoms and the complexity of the
available data make this a challenge. We take a first step in streamlining the
process by focusing on inexpensive data from questionnaires and craniofacial
measurements. We apply correlation networks, the Mapper algorithm from
topological data analysis, and singular value decomposition in a process of
exploratory data analysis. We then apply a variety of supervised and
unsupervised learning techniques from statistics, machine learning, and
topology, ranging from support vector machines to Bayesian classifiers and
manifold learning. Finally, we analyze the results of each of these methods and
discuss the implications for a multi-data-sourced algorithm moving forward.

    

### [[2003.11769] Nonconvex sparse regularization for deep neural networks and its optimality](http://arxiv.org/abs/2003.11769)


  Recent theoretical studies proved that deep neural network (DNN) estimators
obtained by minimizing empirical risk with a certain sparsity constraint can
attain optimal convergence rates for regression and classification problems.
However, the sparsity constraint requires to know certain properties of the
true model, which are not available in practice. Moreover, computation is
difficult due to the discrete nature of the sparsity constraint. In this paper,
we propose a novel penalized estimation method for sparse DNNs, which resolves
the aforementioned problems existing in the sparsity constraint. We establish
an oracle inequality for the excess risk of the proposed sparse-penalized DNN
estimator and derive convergence rates for several learning tasks. In
particular, we prove that the sparse-penalized estimator can adaptively attain
minimax convergence rates for various nonparametric regression problems. For
computation, we develop an efficient gradient-based optimization algorithm that
guarantees the monotonic reduction of the objective function.

    

### [[2005.00959] On the Convergence Rate of Projected Gradient Descent for a Back-Projection based Objective](http://arxiv.org/abs/2005.00959)


  Ill-posed linear inverse problems appear in many scientific setups, and are
typically addressed by solving optimization problems, which are composed of
data fidelity and prior terms. Recently, several works have considered a
back-projection (BP) based fidelity term as an alternative to the common least
squares (LS), and demonstrated excellent results for popular inverse problems.
These works have also empirically shown that using the BP term, rather than the
LS term, requires fewer iterations of optimization algorithms. In this paper,
we examine the convergence rate of the projected gradient descent (PGD)
algorithm for the BP objective. Our analysis allows to identify an inherent
source for its faster convergence compared to using the LS objective, while
making only mild assumptions. We also analyze the more general proximal
gradient method under a relaxed contraction condition on the proximal mapping
of the prior. This analysis further highlights the advantage of BP when the
linear measurement operator is badly conditioned. Numerical experiments with
both $\ell_1$-norm and GAN-based priors corroborate our theoretical results.

    

### [[2006.03041] Sample Complexity of Asynchronous Q-Learning: Sharper Analysis and Variance Reduction](http://arxiv.org/abs/2006.03041)


  Asynchronous Q-learning aims to learn the optimal action-value function (or
Q-function) of a Markov decision process (MDP), based on a single trajectory of
Markovian samples induced by a behavior policy. Focusing on a
$\gamma$-discounted MDP with state space $\mathcal{S}$ and action space
$\mathcal{A}$, we demonstrate that the $\ell_{\infty}$-based sample complexity
of classical asynchronous Q-learning --- namely, the number of samples needed
to yield an entrywise $\varepsilon$-accurate estimate of the Q-function --- is
at most on the order of $\frac{1}{\mu_{\min}(1-\gamma)^5\varepsilon^2}+
\frac{t_{mix}}{\mu_{\min}(1-\gamma)}$ up to some logarithmic factor, provided
that a proper constant learning rate is adopted. Here, $t_{mix}$ and
$\mu_{\min}$ denote respectively the mixing time and the minimum state-action
occupancy probability of the sample trajectory. The first term of this bound
matches the sample complexity in the synchronous case with independent samples
drawn from the stationary distribution of the trajectory. The second term
reflects the cost taken for the empirical distribution of the Markovian
trajectory to reach a steady state, which is incurred at the very beginning and
becomes amortized as the algorithm runs. Encouragingly, the above bound
improves upon the state-of-the-art result \cite{qu2020finite} by a factor of at
least $|\mathcal{S}||\mathcal{A}|$ for all scenarios, and by a factor of at
least $t_{mix}|\mathcal{S}||\mathcal{A}|$ for any sufficiently small accuracy
level $\varepsilon$. Further, we demonstrate that the scaling on the effective
horizon $\frac{1}{1-\gamma}$ can be improved by means of variance reduction.

    

### [[2006.07986] Fairness Under Feature Exemptions: Counterfactual and Observational Measures](http://arxiv.org/abs/2006.07986)


  With the growing use of ML in highly consequential domains, quantifying
disparity with respect to protected attributes, e.g., gender, race, etc., is
important. While quantifying disparity is essential, sometimes the needs of an
occupation may require the use of certain features that are critical in a way
that any disparity that can be explained by them might need to be exempted.
E.g., in hiring a software engineer for a safety-critical application,
coding-skills may be weighed strongly, whereas name, zip code, or reference
letters may be used only to the extent that they do not add disparity. In this
work, we propose an information-theoretic decomposition of the total disparity
(a quantification inspired from counterfactual fairness) into two components: a
non-exempt component which quantifies the part that cannot be accounted for by
the critical features, and an exempt component that quantifies the remaining
disparity. This decomposition allows one to check if the disparity arose purely
due to the critical features (inspired from the business necessity defense of
disparate impact law) and also enables selective removal of the non-exempt
component if desired. We arrive at this decomposition through canonical
examples that lead to a set of desirable properties (axioms) that a measure of
non-exempt disparity should satisfy. Our proposed measure satisfies all of
them. Our quantification bridges ideas of causality, Simpson's paradox, and a
body of work from information theory called Partial Information Decomposition.
We also obtain an impossibility result showing that no observational measure
can satisfy all the desirable properties, leading us to relax our goals and
examine observational measures that satisfy only some of them. We perform case
studies to show how one can audit/train models while reducing non-exempt
disparity.

    

### [[2006.09134] AlphaGAN: Fully Differentiable Architecture Search for Generative Adversarial Networks](http://arxiv.org/abs/2006.09134)


  Generative Adversarial Networks (GANs) are formulated as minimax game
problems, whereby generators attempt to approach real data distributions by
virtue of adversarial learning against discriminators. The intrinsic problem
complexity poses the challenge to enhance the performance of generative
networks. In this work, we aim to boost model learning from the perspective of
network architectures, by incorporating recent progress on automated
architecture search into GANs. To this end, we propose a fully differentiable
search framework for generative adversarial networks, dubbed alphaGAN. The
searching process is formalized as solving a bi-level minimax optimization
problem, in which the outer-level objective aims for seeking a suitable network
architecture towards pure Nash Equilibrium conditioned on the generator and the
discriminator network parameters optimized with a traditional GAN loss in the
inner level. The entire optimization performs a first-order method by
alternately minimizing the two-level objective in a fully differentiable
manner, enabling architecture search to be completed in an enormous search
space. Extensive experiments on CIFAR-10 and STL-10 datasets show that our
algorithm can obtain high-performing architectures only with 3-GPU hours on a
single GPU in the search space comprised of approximate 2 ? 1011 possible
configurations. We also provide a comprehensive analysis on the behavior of the
searching process and the properties of searched architectures, which would
benefit further research on architectures for generative models. Pretrained
models and codes are available at this https URL.

    

### [[2007.15634] On the Nature and Types of Anomalies: A Review of Deviations in Data](http://arxiv.org/abs/2007.15634)


  Anomalies are occurrences in a dataset that are in some way unusual and do
not fit the general patterns. The concept of the anomaly is typically
ill-defined and perceived as vague and domain-dependent. Moreover, despite some
250 years of publications on the topic, no comprehensive and concrete overviews
of the different types of anomalies have hitherto been published. By means of
an extensive literature review this study therefore offers the first
theoretically principled and domain-independent typology of data anomalies and
presents a full overview of anomaly types and subtypes. To concretely define
the concept of the anomaly and its different manifestations, the typology
employs five dimensions: data type, cardinality of relationship, anomaly level,
data structure, and data distribution. These fundamental and data-centric
dimensions naturally yield 3 broad groups, 9 basic types, and 63 subtypes of
anomalies. The typology facilitates the evaluation of the functional
capabilities of anomaly detection algorithms, contributes to explainable data
science, and provides insights into relevant topics such as local versus global
anomalies.

    

### [[2008.02856] Iterative Pre-Conditioning for Expediting the Gradient-Descent Method: The Distributed Linear Least-Squares Problem](http://arxiv.org/abs/2008.02856)


  This paper considers the multi-agent linear least-squares problem in a
server-agent network. In this problem, the system comprises multiple agents,
each having a set of local data points, that are connected to a server. The
goal for the agents is to compute a linear mathematical model that optimally
fits the collective data points held by all the agents, without sharing their
individual local data points. This goal can be achieved, in principle, using
the server-agent variant of the traditional iterative gradient-descent method.
The gradient-descent method converges linearly to a solution, and its rate of
convergence is lower bounded by the conditioning of the agents' collective data
points. If the data points are ill-conditioned, the gradient-descent method may
require a large number of iterations to converge.
We propose an iterative pre-conditioning technique that mitigates the
deleterious effect of the conditioning of data points on the rate of
convergence of the gradient-descent method. We rigorously show that the
resulting pre-conditioned gradient-descent method, with the proposed iterative
pre-conditioning, achieves superlinear convergence when the least-squares
problem has a unique solution. In general, the convergence is linear with
improved rate of convergence in comparison to the traditional gradient-descent
method and the state-of-the-art accelerated gradient-descent methods. We
further illustrate the improved rate of convergence of our proposed algorithm
through experiments on different real-world least-squares problems in both
noise-free and noisy computation environment.

    

### [[2008.10581] Neural Bridge Sampling for Evaluating Safety-Critical Autonomous Systems](http://arxiv.org/abs/2008.10581)


  Learning-based methodologies increasingly find applications in
safety-critical domains like autonomous driving and medical robotics. Due to
the rare nature of dangerous events, real-world testing is prohibitively
expensive and unscalable. In this work, we employ a probabilistic approach to
safety evaluation in simulation, where we are concerned with computing the
probability of dangerous events. We develop a novel rare-event simulation
method that combines exploration, exploitation, and optimization techniques to
find failure modes and estimate their rate of occurrence. We provide rigorous
guarantees for the performance of our method in terms of both statistical and
computational efficiency. Finally, we demonstrate the efficacy of our approach
on a variety of scenarios, illustrating its usefulness as a tool for rapid
sensitivity analysis and model comparison that are essential to developing and
testing safety-critical autonomous systems.

    

### [[2008.12138] How to "Improve" Prediction Using Behavior Modification](http://arxiv.org/abs/2008.12138)


  Many internet platforms that collect behavioral big data use it to predict
user behavior for internal purposes and for their business customers (e.g.,
advertisers, insurers, security forces, governments, political consulting
firms) who utilize the predictions for personalization, targeting, and other
decision-making. Improving predictive accuracy is therefore extremely valuable.
Data science researchers design algorithms, models, and approaches to improve
prediction. Prediction is also improved with larger and richer data. Beyond
improving algorithms and data, platforms can stealthily achieve better
prediction accuracy by "pushing" users' behaviors towards their predicted
values, using behavior modification techniques, thereby demonstrating more
certain predictions. Such apparent "improved" prediction can unintentionally
result from employing reinforcement learning algorithms that combine prediction
and behavior modification. This strategy is absent from the machine learning
and statistics literature. Investigating its properties requires integrating
causal with predictive notation. To this end, we incorporate Pearl's causal
do(.) operator into the predictive vocabulary. We then decompose the expected
prediction error given behavior modification, and identify the components
impacting predictive power. Our derivation elucidates implications of such
behavior modification to data scientists, platforms, their customers, and the
humans whose behavior is manipulated. Behavior modification can make users'
behavior more predictable and even more homogeneous; yet this apparent
predictability might not generalize when customers use predictions in practice.
Outcomes pushed towards their predictions can be at odds with customers'
intentions, and harmful to manipulated users.

    

### [[2009.00524] Tensor Relational Algebra for Machine Learning System Design](http://arxiv.org/abs/2009.00524)


  We consider the question: what is the abstraction that should be implemented
by the computational engine of a machine learning system? Current machine
learning systems typically push whole tensors through a series of compute
kernels such as matrix multiplications or activation functions, where each
kernel runs on an AI accelerator (ASIC) such as a GPU. This implementation
abstraction provides little built-in support for ML systems to scale past a
single machine, or for handling large models with matrices or tensors that do
not easily fit into the RAM of an ASIC. In this paper, we present an
alternative implementation abstraction called the tensor relational algebra
(TRA). The TRA is a set-based algebra based on the relational algebra.
Expressions in the TRA operate over binary tensor relations, where keys are
multi-dimensional arrays and values are tensors. The TRA is easily executed
with high efficiency in a parallel or distributed environment, and amenable to
automatic optimization. Our empirical study shows that the optimized TRA-based
back-end can significantly outperform alternatives for running ML workflows in
distributed clusters.

    

### [[2009.10684] Let's Stop Incorrect Comparisons in End-to-end Relation Extraction!](http://arxiv.org/abs/2009.10684)


  Despite efforts to distinguish three different evaluation setups (Bekoulis et
al., 2018), numerous end-to-end Relation Extraction (RE) articles present
unreliable performance comparison to previous work. In this paper, we first
identify several patterns of invalid comparisons in published papers and
describe them to avoid their propagation. We then propose a small empirical
study to quantify the impact of the most common mistake and evaluate it leads
to overestimating the final RE performance by around 5% on ACE05. We also seize
this opportunity to study the unexplored ablations of two recent developments:
the use of language model pretraining (specifically BERT) and span-level NER.
This meta-analysis emphasizes the need for rigor in the report of both the
evaluation setting and the datasets statistics and we call for unifying the
evaluation setting in end-to-end RE.

    

### [[2010.03110] Causal Curiosity: RL Agents Discovering Self-supervised Experiments for Causal Representation Learning](http://arxiv.org/abs/2010.03110)


  Animals exhibit an innate ability to learn regularities of the world through
interaction. By performing experiments in their environment, they are able to
discern the causal factors of variation and infer how they affect the world's
dynamics. Inspired by this, we attempt to equip reinforcement learning agents
with the ability to perform experiments that facilitate a categorization of the
rolled-out trajectories, and to subsequently infer the causal factors of the
environment in a hierarchical manner. We introduce {\em causal curiosity}, a
novel intrinsic reward, and show that it allows our agents to learn optimal
sequences of actions and discover causal factors in the dynamics of the
environment. The learned behavior allows the agents to infer a binary quantized
representation for the ground-truth causal factors in every environment.
Additionally, we find that these experimental behaviors are semantically
meaningful (e.g., our agents learn to lift blocks to categorize them by
weight), and are learnt in a self-supervised manner with approximately 2.5
times less data than conventional supervised planners. We show that these
behaviors can be re-purposed and fine-tuned (e.g., from lifting to pushing or
other downstream tasks). Finally, we show that the knowledge of causal factor
representations aids zero-shot learning for more complex tasks. Visit
this https URL for website.

    

### [[2010.05690] COVID-19 Classification Using Staked Ensembles: A Comprehensive Analysis](http://arxiv.org/abs/2010.05690)


  The issue of COVID-19, increasing with a massive mortality rate. This led to
the WHO declaring it as a pandemic. In this situation, it is crucial to perform
efficient and fast diagnosis. The reverse transcript polymerase chain reaction
(RTPCR) test is conducted to detect the presence of SARS-CoV-2. This test is
time-consuming and instead chest CT (or Chest X-ray) can be used for a fast and
accurate diagnosis. Automated diagnosis is considered to be important as it
reduces human effort and provides accurate and low-cost tests. The
contributions of our research are three-fold. First, it is aimed to analyse the
behaviour and performance of variant vision models ranging from Inception to
NAS networks with the appropriate fine-tuning procedure. Second, the behaviour
of these models is visually analysed by plotting CAMs for individual networks
and determining classification performance with AUCROC curves. Thirdly, stacked
ensembles techniques are imparted to provide higher generalisation on combining
the fine-tuned models, in which six ensemble neural networks are designed by
combining the existing fine-tuned networks. Implying these stacked ensembles
provides a great generalization to the models. The ensemble model designed by
combining all the fine-tuned networks obtained a state-of-the-art accuracy
score of 99.17%. The precision and recall for the COVID-19 class are 99.99% and
89.79% respectively, which resembles the robustness of the stacked ensembles.

    

### [[2011.09128] MGIC: Multigrid-in-Channels Neural Network Architectures](http://arxiv.org/abs/2011.09128)


  We present a multigrid-in-channels (MGIC) approach that tackles the quadratic
growth of the number of parameters with respect to the number of channels in
standard convolutional neural networks (CNNs). Thereby our approach addresses
the redundancy in CNNs that is also exposed by the recent success of
lightweight CNNs. Lightweight CNNs can achieve comparable accuracy to standard
CNNs with fewer parameters; however, the number of weights still scales
quadratically with the CNN's width. Our MGIC architectures replace each CNN
block with an MGIC counterpart that utilizes a hierarchy of nested grouped
convolutions of small group size to address this.
Hence, our proposed architectures scale linearly with respect to the
network's width while retaining full coupling of the channels as in standard
CNNs.
Our extensive experiments on image classification, segmentation, and point
cloud classification show that applying this strategy to different
architectures like ResNet and MobileNetV3 reduces the number of parameters
while obtaining similar or better accuracy.

    

### [[2012.00489] Deep Gravity: enhancing mobility flows generation with deep neural networks and geographic information](http://arxiv.org/abs/2012.00489)


  The movements of individuals within and among cities influence critical
aspects of our society, such as well-being, the spreading of epidemics, and the
quality of the environment. When information about mobility flows is not
available for a particular region of interest, we must rely on mathematical
models to generate them. In this work, we propose the Deep Gravity model, an
effective method to generate flow probabilities that exploits many variables
(e.g., land use, road network, transport, food, health facilities) extracted
from voluntary geographic data, and uses deep neural networks to discover
non-linear relationships between those variables and mobility flows. Our
experiments, conducted on mobility flows in England, Italy, and New York State,
show that Deep Gravity has good geographic generalization capability, achieving
a significant increase in performance (especially in densely populated regions
of interest) with respect to the classic gravity model and models that do not
use deep neural networks or geographic data. We also show how flows generated
by Deep Gravity may be explained in terms of the geographic features using
explainable AI techniques.

    

### [[2012.02346] ChartPointFlow for Topology-Aware 3D Point Cloud Generation](http://arxiv.org/abs/2012.02346)


  A point cloud serves as a representation of the surface of a
three-dimensional (3D) shape. Deep generative models have been adapted to model
their variations typically using a map from a ball-like set of latent
variables. However, previous approaches did not pay much attention to the
topological structure of a point cloud, despite that a continuous map cannot
express the varying numbers of holes and intersections. Moreover, a point cloud
is often composed of multiple subparts, and it is also difficult to express. In
this study, we propose ChartPointFlow, a flow-based generative model with
multiple latent labels for 3D point clouds. Each label is assigned to points in
an unsupervised manner. Then, a map conditioned on a label is assigned to a
continuous subset of a point cloud, similar to a chart of a manifold. This
enables our proposed model to preserve the topological structure with clear
boundaries, whereas previous approaches tend to generate blurry point clouds
and fail to generate holes. The experimental results demonstrate that
ChartPointFlow achieves state-of-the-art performance in terms of generation and
reconstruction compared with other point cloud generators. Moreover,
ChartPointFlow divides an object into semantic subparts using charts, and it
demonstrates superior performance in case of unsupervised segmentation.

    

### [[2012.03019] Deep learning Local Reduced Density Matrices for Many-body Hamiltonian Estimation](http://arxiv.org/abs/2012.03019)


  Human experts cannot efficiently access the physical information of quantum
many-body states by simply "reading" the coefficients, but have to reply on the
previous knowledge such as order parameters and quantum measurements. In this
work, we demonstrate that convolutional neural network (CNN) can learn from the
coefficients of local reduced density matrices to estimate the physical
parameters of the many-body Hamiltonians, such as coupling strengths and
magnetic fields, provided the states as the ground states. We propose QubismNet
that consists of two main parts: the Qubism map that visualizes the ground
states (or the purified reduced density matrices) as images, and a CNN that
maps the images to the target physical parameters. By assuming certain
constraints on the training set for the sake of balance, QubismNet exhibits
impressive powers of learning and generalization on several quantum spin
models. While the training samples are restricted to the states from certain
ranges of the parameters, QubismNet can accurately estimate the parameters of
the states beyond such training regions. For instance, our results show that
QubismNet can estimate the magnetic fields near the critical point by learning
from the states away from the critical vicinity. Our work illuminates a
data-driven way to infer the Hamiltonians that give the designed ground states,
and therefore would benefit the existing and future generalizations of quantum
technologies such as Hamiltonian-based quantum simulations and state
tomography.

    

### [[2012.06400] Differential Evolution for Neural Architecture Search](http://arxiv.org/abs/2012.06400)


  Neural architecture search (NAS) methods rely on a search strategy for
deciding which architectures to evaluate next and a performance estimation
strategy for assessing their performance (e.g., using full evaluations,
multi-fidelity evaluations, or the one-shot model). In this paper, we focus on
the search strategy. We introduce the simple yet powerful evolutionary
algorithm of differential evolution to the NAS community. Using the simplest
performance evaluation strategy of full evaluations, we comprehensively compare
this search strategy to regularized evolution and Bayesian optimization and
demonstrate that it yields improved and more robust results for 13 tabular NAS
benchmarks based on NAS-Bench-101, NAS-Bench-1Shot1, NAS-Bench-201 and NAS-HPO
bench.

    

### [[2012.11228] Alternating linear scheme in a Bayesian framework for low-rank tensor approximation](http://arxiv.org/abs/2012.11228)


  Multiway data often naturally occurs in a tensorial format which can be
approximately represented by a low-rank tensor decomposition. This is useful
because complexity can be significantly reduced and the treatment of
large-scale data sets can be facilitated. In this paper, we find a low-rank
representation for a given tensor by solving a Bayesian inference problem. This
is achieved by dividing the overall inference problem into sub-problems where
we sequentially infer the posterior distribution of one tensor decomposition
component at a time. This leads to a probabilistic interpretation of the
well-known iterative algorithm alternating linear scheme (ALS). In this way,
the consideration of measurement noise is enabled, as well as the incorporation
of application-specific prior knowledge and the uncertainty quantification of
the low-rank tensor estimate. To compute the low-rank tensor estimate from the
posterior distributions of the tensor decomposition components, we present an
algorithm that performs the unscented transform in tensor train format.

    

### [[2102.12722] Combinatorial Bandits under Strategic Manipulations](http://arxiv.org/abs/2102.12722)


  Strategic behavior against sequential learning methods, such as "click
framing" in real recommendation systems, has been widely observed. Motivated by
such behavior we study the problem of combinatorial multi-armed bandits (CMAB)
under strategic manipulations of rewards, where each arm can modify the emitted
reward signals for its own interest. This characterization of the adversarial
behavior is a relaxation of previously well-studied settings such as
adversarial attacks and adversarial corruption. We propose a strategic variant
of the combinatorial UCB algorithm, which has a regret of at most $O(m\log T +
m B_{max})$ under strategic manipulations, where $T$ is the time horizon, $m$
is the number of arms, and $B_{max}$ is the maximum budget of an arm. We
provide lower bounds on the budget for arms to incur certain regret of the
bandit algorithm. Extensive experiments on online worker selection for
crowdsourcing systems, online influence maximization and online recommendations
with both synthetic and real datasets corroborate our theoretical findings on
robustness and regret bounds, in a variety of regimes of manipulation budgets.

    

### [[2103.02193] Adaptive Consistency Regularization for Semi-Supervised Transfer Learning](http://arxiv.org/abs/2103.02193)


  While recent studies on semi-supervised learning have shown remarkable
progress in leveraging both labeled and unlabeled data, most of them presume a
basic setting of the model is randomly initialized. In this work, we consider
semi-supervised learning and transfer learning jointly, leading to a more
practical and competitive paradigm that can utilize both powerful pre-trained
models from source domain as well as labeled/unlabeled data in the target
domain. To better exploit the value of both pre-trained weights and unlabeled
target examples, we introduce adaptive consistency regularization that consists
of two complementary components: Adaptive Knowledge Consistency (AKC) on the
examples between the source and target model, and Adaptive Representation
Consistency (ARC) on the target model between labeled and unlabeled examples.
Examples involved in the consistency regularization are adaptively selected
according to their potential contributions to the target task. We conduct
extensive experiments on popular benchmarks including CIFAR-10, CUB-200, and
MURA, by fine-tuning the ImageNet pre-trained ResNet-50 model. Results show
that our proposed adaptive consistency regularization outperforms
state-of-the-art semi-supervised learning techniques such as Pseudo Label, Mean
Teacher, and FixMatch. Moreover, our algorithm is orthogonal to existing
methods and thus able to gain additional improvements on top of MixMatch and
FixMatch. Our code is available at
this https URL.

    

### [[2103.03995] Convolution Neural Network Hyperparameter Optimization Using Simplified Swarm Optimization](http://arxiv.org/abs/2103.03995)


  Convolutional neural networks (CNNs) are widely used in image recognition.
Numerous CNN models, such as LeNet, AlexNet, VGG, ResNet, and GoogLeNet, have
been proposed by increasing the number of layers, to improve the performance of
CNNs. However, performance deteriorates beyond a certain number of layers.
Hence, hyperparameter optimisation is a more efficient way to improve CNNs. To
validate this concept, a new algorithm based on simplified swarm optimisation
is proposed to optimise the hyperparameters of the simplest CNN model, which is
LeNet. The results of experiments conducted on the MNIST, Fashion MNIST, and
Cifar10 datasets showed that the accuracy of the proposed algorithm is higher
than the original LeNet model and PSO-LeNet and that it has a high potential to
be extended to more complicated models, such as AlexNet.

    

### [[2103.05248] Practical Relative Order Attack in Deep Ranking](http://arxiv.org/abs/2103.05248)


  Recent studies unveil the vulnerabilities of deep ranking models, where an
imperceptible perturbation can trigger dramatic changes in the ranking result.
While previous attempts focus on manipulating absolute ranks of certain
candidates, the possibility of adjusting their relative order remains
under-explored. In this paper, we formulate a new adversarial attack against
deep ranking systems, i.e., the Order Attack, which covertly alters the
relative order among a selected set of candidates according to an
attacker-specified permutation, with limited interference to other unrelated
candidates. Specifically, it is formulated as a triplet-style loss imposing an
inequality chain reflecting the specified permutation. However, direct
optimization of such white-box objective is infeasible in a real-world attack
scenario due to various black-box limitations. To cope with them, we propose a
Short-range Ranking Correlation metric as a surrogate objective for black-box
Order Attack to approximate the white-box method. The Order Attack is evaluated
on the Fashion-MNIST and Stanford-Online-Products datasets under both white-box
and black-box threat models. The black-box attack is also successfully
implemented on a major e-commerce platform. Comprehensive experimental
evaluations demonstrate the effectiveness of the proposed methods, revealing a
new type of ranking model vulnerability.

    

### [[2104.01662] Learning Linear Policies for Robust Bipedal Locomotion on Terrains with Varying Slopes](http://arxiv.org/abs/2104.01662)


  In this paper, with a view toward deployment of light-weight control
frameworks for bipedal walking robots, we realize end-foot trajectories that
are shaped by a single linear feedback policy. We learn this policy via a
model-free and a gradient-free learning algorithm, Augmented Random Search
(ARS), in the two robot platforms Rabbit and Digit. Our contributions are
two-fold: a) By using torso and support plane orientation as inputs, we achieve
robust walking on slopes of up to 20 degrees in simulation. b) We demonstrate
additional behaviors like walking backwards, stepping-in-place, and recovery
from external pushes of up to 120 N. The end result is a robust and a fast
feedback control law for bipedal walking on terrains with varying slopes.
Towards the end, we also provide preliminary results of hardware transfer to
Digit.

    

### [[2104.06307] Detecting False Data Injection Attacks in Smart Grids with Modeling Errors: A Deep Transfer Learning Based Approach](http://arxiv.org/abs/2104.06307)


  Most traditional false data injection attack (FDIA) detection approaches rely
on a key assumption, i.e., the power system can be accurately modeled. However,
the transmission line parameters are dynamic and cannot be accurately known
during operation and thus the involved modeling errors should not be neglected.
In this paper, an illustrative case has revealed that modeling errors in
transmission lines significantly weaken the detection effectiveness of
conventional FDIA approaches. To tackle this issue, we propose an FDIA
detection mechanism from the perspective of transfer learning. Specifically,
the simulated power system is treated as a source domain, which provides
abundant simulated normal and attack data. The real world's running system
whose transmission line parameters are unknown is taken as a target domain
where sufficient real normal data are collected for tracking the latest system
states online. The designed transfer strategy that aims at making full use of
data in hand is divided into two optimization stages. In the first stage, a
deep neural network (DNN) is built by simultaneously optimizing several
well-designed objective terms with both simulated data and real data, and then
it is fine-tuned via real data in the second stage. Several case studies on the
IEEE 14-bus and 118-bus systems verify the effectiveness of the proposed
mechanism.

    

### [[2104.06534] Simultaneous Face Hallucination and Translation for Thermal to Visible Face Verification using Axial-GAN](http://arxiv.org/abs/2104.06534)


  Existing thermal-to-visible face verification approaches expect the thermal
and visible face images to be of similar resolution. This is unlikely in
real-world long-range surveillance systems, since humans are distant from the
cameras. To address this issue, we introduce the task of thermal-to-visible
face verification from low-resolution thermal images. Furthermore, we propose
Axial-Generative Adversarial Network (Axial-GAN) to synthesize high-resolution
visible images for matching. In the proposed approach we augment the GAN
framework with axial-attention layers which leverage the recent advances in
transformers for modelling long-range dependencies. We demonstrate the
effectiveness of the proposed method by evaluating on two different
thermal-visible face datasets. When compared to related state-of-the-art works,
our results show significant improvements in both image quality and face
verification performance, and are also much more efficient.

    

### [[2104.14072] Nonlinear Level Set Learning for Function Approximation on Sparse Data with Applications to Parametric Differential Equations](http://arxiv.org/abs/2104.14072)


  A dimension reduction method based on the "Nonlinear Level set Learning"
(NLL) approach is presented for the pointwise prediction of functions which
have been sparsely sampled. Leveraging geometric information provided by the
Implicit Function Theorem, the proposed algorithm effectively reduces the input
dimension to the theoretical lower bound with minor accuracy loss, providing a
one-dimensional representation of the function which can be used for regression
and sensitivity analysis. Experiments and applications are presented which
compare this modified NLL with the original NLL and the Active Subspaces (AS)
method. While accommodating sparse input data, the proposed algorithm is shown
to train quickly and provide a much more accurate and informative reduction
than either AS or the original NLL on two example functions with
high-dimensional domains, as well as two state-dependent quantities depending
on the solutions to parametric differential equations.

    

### [[2104.15007] Time Series Forecasting of New Cases and New Deaths Rate for COVID-19 using Deep Learning Methods](http://arxiv.org/abs/2104.15007)


  The first known case of Coronavirus disease 2019 (COVID-19) was identified in
December 2019. It has spread worldwide, leading to an ongoing pandemic, imposed
restrictions and costs to many countries. Predicting the number of new cases
and deaths during this period can be a useful step in predicting the costs and
facilities required in the future. The purpose of this study is to predict new
cases and deaths rate one, three and seven-day ahead during the next 100 days.
The motivation for predicting every n days (instead of just every day) is the
investigation of the possibility of computational cost reduction and still
achieving reasonable performance. Such a scenario may be encountered real-time
forecasting of time series. Six different deep learning methods are examined on
the data adopted from the WHO website. Three methods are LSTM, Convolutional
LSTM, and GRU. The bidirectional extension is then considered for each method
to forecast the rate of new cases and new deaths in Australia and Iran
countries.

    

### [[2105.00591] Single-Training Collaborative Object Detectors Adaptive to Bandwidth and Computation](http://arxiv.org/abs/2105.00591)


  In the past few years, mobile deep-learning deployment progressed by leaps
and bounds, but solutions still struggle to accommodate its severe and
fluctuating operational restrictions, which include bandwidth, latency,
computation, and energy. In this work, we help to bridge that gap, introducing
the first configurable solution for object detection that manages the triple
communication-computation-accuracy trade-off with a single set of weights. Our
solution shows state-of-the-art results on COCO-2017, adding only a minor
penalty on the base EfficientDet-D2 architecture. Our design is robust to the
choice of base architecture and compressor and should adapt well for future
architectures.

    

### [[2105.06977] Do Context-Aware Translation Models Pay the Right Attention?](http://arxiv.org/abs/2105.06977)


  Context-aware machine translation models are designed to leverage contextual
information, but often fail to do so. As a result, they inaccurately
disambiguate pronouns and polysemous words that require context for resolution.
In this paper, we ask several questions: What contexts do human translators use
to resolve ambiguous words? Are models paying large amounts of attention to the
same context? What if we explicitly train them to do so? To answer these
questions, we introduce SCAT (Supporting Context for Ambiguous Translations), a
new English-French dataset comprising supporting context words for 14K
translations that professional translators found useful for pronoun
disambiguation. Using SCAT, we perform an in-depth analysis of the context used
to disambiguate, examining positional and lexical characteristics of the
supporting words. Furthermore, we measure the degree of alignment between the
model's attention scores and the supporting context from SCAT, and apply a
guided attention strategy to encourage agreement between the two.

    

### [[2105.08649] DCAP: Deep Cross Attentional Product Network for User Response Prediction](http://arxiv.org/abs/2105.08649)


  User response prediction, which aims to predict the probability that a user
will provide a predefined positive response in a given context such as clicking
on an ad or purchasing an item, is crucial to many industrial applications such
as online advertising, recommender systems, and search ranking. However, due to
the high dimensionality and super sparsity of the data collected in these
tasks, handcrafting cross features is inevitably time expensive. Prior studies
in predicting user response leveraged the feature interactions by enhancing
feature vectors with products of features to model second-order or high-order
cross features, either explicitly or implicitly. Nevertheless, these existing
methods can be hindered by not learning sufficient cross features due to model
architecture limitations or modeling all high-order feature interactions with
equal weights. This work aims to fill this gap by proposing a novel
architecture Deep Cross Attentional Product Network (DCAP), which keeps cross
network's benefits in modeling high-order feature interactions explicitly at
the vector-wise level. Beyond that, it can differentiate the importance of
different cross features in each network layer inspired by the multi-head
attention mechanism and Product Neural Network (PNN), allowing practitioners to
perform a more in-depth analysis of user behaviors. Additionally, our proposed
model can be easily implemented and train in parallel. We conduct comprehensive
experiments on three real-world datasets. The results have robustly
demonstrated that our proposed model DCAP achieves superior prediction
performance compared with the state-of-the-art models.

    

### [[2105.08709] Learning and Certification under Instance-targeted Poisoning](http://arxiv.org/abs/2105.08709)


  In this paper, we study PAC learnability and certification of predictions
under instance-targeted poisoning attacks, where the adversary who knows the
test instance may change a fraction of the training set with the goal of
fooling the learner at the test instance. Our first contribution is to
formalize the problem in various settings and to explicitly model subtle
aspects such as the proper or improper nature of the learning, learner's
randomness, and whether (or not) adversary's attack can depend on it. Our main
result shows that when the budget of the adversary scales sublinearly with the
sample complexity, (improper) PAC learnability and certification are
achievable; in contrast, when the adversary's budget grows linearly with the
sample complexity, the adversary can potentially drive up the expected 0-1 loss
to one. We also study distribution-specific PAC learning in the same attack
model and show that proper learning with certification is possible for learning
half spaces under natural distributions. Finally, we empirically study the
robustness of K nearest neighbour, logistic regression, multi-layer perceptron,
and convolutional neural network on real data sets against targeted-poisoning
attacks. Our experimental results show that many models, especially
state-of-the-art neural networks, are indeed vulnerable to these strong
attacks. Interestingly, we observe that methods with high standard accuracy
might be more vulnerable to instance-targeted poisoning attacks.

    

### [[2105.09903] Multi-Perspective Anomaly Detection](http://arxiv.org/abs/2105.09903)


  Anomaly detection is a critical problem in the manufacturing industry. In
many applications, images of objects to be analyzed are captured from multiple
perspectives which can be exploited to improve the robustness of anomaly
detection. In this work, we build upon the deep support vector data description
algorithm and address multi-perspective anomaly detection using three different
fusion techniques, i.e., early fusion, late fusion, and late fusion with
multiple decoders. We employ different augmentation techniques with a denoising
process to deal with scarce one-class data, which further improves the
performance (ROC AUC $= 80\%$). Furthermore, we introduce the dices dataset,
which consists of over 2000 grayscale images of falling dices from multiple
perspectives, with 5\% of the images containing rare anomalies (e.g., drill
holes, sawing, or scratches). We evaluate our approach on the new dices dataset
using images from two different perspectives and also benchmark on the standard
MNIST dataset. Extensive experiments demonstrate that our proposed
{multi-perspective} approach exceeds the state-of-the-art {single-perspective
anomaly detection on both the MNIST and dices datasets}. To the best of our
knowledge, this is the first work that focuses on addressing multi-perspective
anomaly detection in images by jointly using different perspectives together
with one single objective function for anomaly detection.

    

### [[2105.10554] GNNIE: GNN Inference Engine with Load-balancing and Graph-Specific Caching](http://arxiv.org/abs/2105.10554)


  Graph neural networks (GNN) analysis engines are vital for real-world
problems that use large graph models. Challenges for a GNN hardware platform
include the ability to (a) host a variety of GNNs, (b) handle high sparsity in
input vertex feature vectors and the graph adjacency matrix and the
accompanying random memory access patterns, and (c) maintain load-balanced
computation in the face of uneven workloads, induced by high sparsity and
power-law vertex degree distributions. This paper proposes GNNIE, an
accelerator designed to run a broad range of GNNs. It tackles workload
imbalance by (i)~splitting vertex feature operands into blocks, (ii)~reordering
and redistributing computations, (iii)~using a novel flexible MAC architecture.
It adopts a graph-specific, degree-aware caching policy that is well suited to
real-world graph characteristics. The policy enhances on-chip data reuse and
avoids random memory access to DRAM.
GNNIE achieves average speedups of 21233x over a CPU and 699x over a GPU over
multiple datasets on graph attention networks (GATs), graph convolutional
networks (GCNs), GraphSAGE, GINConv, and DiffPool. Compared to prior
approaches, GNNIE achieves an average speedup of 35x over HyGCN (which cannot
implement GATs) for GCN, GraphSAGE, and GINConv, and, using 3.4x fewer
processing units, an average speedup of 2.1x over AWB-GCN (which runs only
GCNs).

    

### [[2105.12018] Towards a method to anticipate dark matter signals with deep learning at the LHC](http://arxiv.org/abs/2105.12018)


  We study several simplified dark matter (DM) models and their signatures at
the LHC using neural networks. We focus on the usual monojet plus missing
transverse energy channel, but to train the algorithms we organize the data in
2D histograms instead of event-by-event arrays. This results in a large
performance boost to distinguish between standard model (SM) only and SM plus
new physics signals. We use the kinematic monojet features as input data which
allow us to describe families of models with a single data sample. We found
that the neural network performance does not depend on the simulated number of
background events if they are presented as a function of $S/\sqrt{B}$, where
$S$ and $B$ are the number of signal and background events per histogram,
respectively. This provides flexibility to the method, since testing a
particular model in that case only requires knowing the new physics monojet
cross section. Furthermore, we also discuss the network performance under
incorrect assumptions about the true DM nature. Finally, we propose multimodel
classifiers to search and identify new signals in a more general way, for the
next LHC run.

    

### [[2105.15082] M6-T: Exploring Sparse Expert Models and Beyond](http://arxiv.org/abs/2105.15082)


  Mixture-of-Experts (MoE) models can achieve promising results with outrageous
large amount of parameters but constant computation cost, and thus it has
become a trend in model scaling. Still it is a mystery how MoE layers bring
quality gains by leveraging the parameters with sparse activation. In this
work, we investigate several key factors in sparse expert models. We observe
that load imbalance may not be a significant problem affecting model quality,
contrary to the perspectives of recent studies, while the number of sparsely
activated experts $k$ and expert capacity $C$ in top-$k$ routing can
significantly make a difference in this context. Furthermore, we take a step
forward to propose a simple method called expert prototyping that splits
experts into different prototypes and applies $k$ top-$1$ routing. This
strategy improves the model quality but maintains constant computational costs,
and our further exploration on extremely large-scale models reflects that it is
more effective in training larger models. We push the model scale to over $1$
trillion parameters and implement it on solely $480$ NVIDIA V100-32GB GPUs, in
comparison with the recent SOTAs on $2048$ TPU cores. The proposed giant model
achieves substantial speedup in convergence over the same-size baseline.

    

### [[2106.03126] Predicting Quantum Potentials by Deep Neural Network and Metropolis Sampling](http://arxiv.org/abs/2106.03126)


  The hybridizations of machine learning and quantum physics have caused
essential impacts to the methodology in both fields. Inspired by quantum
potential neural network, we here propose to solve the potential in the
Schrodinger equation provided the eigenstate, by combining Metropolis sampling
with deep neural network, which we dub as Metropolis potential neural network
(MPNN). A loss function is proposed to explicitly involve the energy in the
optimization for its accurate evaluation. Benchmarking on the harmonic
oscillator and hydrogen atom, MPNN shows excellent accuracy and stability on
predicting not just the potential to satisfy the Schrodinger equation, but also
the eigen-energy. Our proposal could be potentially applied to the ab-initio
simulations, and to inversely solving other partial differential equations in
physics and beyond.

    

### [[2106.05527] Score Matching Model for Unbounded Data Score](http://arxiv.org/abs/2106.05527)


  Recent advance in diffusion models incorporates the Stochastic Differential
Equation (SDE), which brings the state-of-the art performance on image
generation tasks. This paper improves such diffusion models by analyzing the
model at the zero diffusion time. In real datasets, the score function diverges
as the diffusion time ($t$) decreases to zero, and this observation leads an
argument that the score estimation fails at $t=0$ with any neural network
structure. Subsequently, we introduce Unbounded Diffusion Model (UDM) that
resolves the score diverging problem with an easily applicable modification to
any diffusion models. Additionally, we introduce a new SDE that overcomes the
theoretic and practical limitations of Variance Exploding SDE. On top of that,
the introduced Soft Truncation method improves the sample quality by mitigating
the loss scale issue that happens at $t=0$. We further provide a theoretic
result of the proposed method to uncover the behind mechanism of the diffusion
models.

    

### [[2106.13229] Model-Based Reinforcement Learning via Latent-Space Collocation](http://arxiv.org/abs/2106.13229)


  The ability to plan into the future while utilizing only raw high-dimensional
observations, such as images, can provide autonomous agents with broad
capabilities. Visual model-based reinforcement learning (RL) methods that plan
future actions directly have shown impressive results on tasks that require
only short-horizon reasoning, however, these methods struggle on temporally
extended tasks. We argue that it is easier to solve long-horizon tasks by
planning sequences of states rather than just actions, as the effects of
actions greatly compound over time and are harder to optimize. To achieve this,
we draw on the idea of collocation, which has shown good results on
long-horizon tasks in optimal control literature, and adapt it to the
image-based setting by utilizing learned latent state space models. The
resulting latent collocation method (LatCo) optimizes trajectories of latent
states, which improves over previously proposed shooting methods for visual
model-based RL on tasks with sparse rewards and long-term goals. Videos and
code at this https URL.

    

### [[2106.13475] Limitations of machine learning for building energy prediction](http://arxiv.org/abs/2106.13475)


  Machine learning for building energy prediction has exploded in popularity in
recent years, yet understanding its limitations and potential for improvement
are lacking. The ASHRAE Great Energy Predictor III (GEPIII) Kaggle competition
was the largest building energy meter machine learning competition ever held
with 4,370 participants who submitted 39,403 predictions. The test data set
included two years of hourly electricity, hot water, chilled water, and steam
readings from 2,380 meters in 1,448 buildings at 16 locations. This paper
analyzes the various sources and types of residual model error from an
aggregation of the competition's top 50 solutions. This analysis reveals the
limitations for machine learning using the standard model inputs of historical
meter, weather, and basic building metadata. The types of error are classified
according to the amount of time errors occur in each instance, abrupt versus
gradual behavior, the magnitude of error, and whether the error existed on
single buildings or several buildings at once from a single location. The
results show machine learning models have errors within a range of
acceptability on 79.1% of the test data. Lower magnitude model errors occur in
16.1% of the test data. These discrepancies can likely be addressed through
additional training data sources or innovations in machine learning. Higher
magnitude errors occur in 4.8% of the test data and are unlikely to be
accurately predicted regardless of innovation. There is a diversity of error
behavior depending on the energy meter type (electricity prediction models have
unacceptable error in under 10% of test data, while hot water is over 60%) and
building use type (public service less than 14%, while technology/science is
just over 46%).

    

### [[2106.13731] Ranger21: a synergistic deep learning optimizer](http://arxiv.org/abs/2106.13731)


  As optimizers are critical to the performances of neural networks, every year
a large number of papers innovating on the subject are published. However,
while most of these publications provide incremental improvements to existing
algorithms, they tend to be presented as new optimizers rather than composable
algorithms. Thus, many worthwhile improvements are rarely seen out of their
initial publication. Taking advantage of this untapped potential, we introduce
Ranger21, a new optimizer which combines AdamW with eight components, carefully
selected after reviewing and testing ideas from the literature. We found that
the resulting optimizer provides significantly improved validation accuracy and
training speed, smoother training curves, and is even able to train a ResNet50
on ImageNet2012 without Batch Normalization layers. A problem on which AdamW
stays systematically stuck in a bad initial state.

    

### [[2106.14463] RadGraph: Extracting Clinical Entities and Relations from Radiology Reports](http://arxiv.org/abs/2106.14463)


  Extracting structured clinical information from free-text radiology reports
can enable the use of radiology report information for a variety of critical
healthcare applications. In our work, we present RadGraph, a dataset of
entities and relations in full-text chest X-ray radiology reports based on a
novel information extraction schema we designed to structure radiology reports.
We release a development dataset, which contains board-certified radiologist
annotations for 500 radiology reports from the MIMIC-CXR dataset (14,579
entities and 10,889 relations), and a test dataset, which contains two
independent sets of board-certified radiologist annotations for 100 radiology
reports split equally across the MIMIC-CXR and CheXpert datasets. Using these
datasets, we train and test a deep learning model, RadGraph Benchmark, that
achieves a micro F1 of 0.82 and 0.73 on relation extraction on the MIMIC-CXR
and CheXpert test sets respectively. Additionally, we release an inference
dataset, which contains annotations automatically generated by RadGraph
Benchmark across 220,763 MIMIC-CXR reports (around 6 million entities and 4
million relations) and 500 CheXpert reports (13,783 entities and 9,908
relations) with mappings to associated chest radiographs. Our freely available
dataset can facilitate a wide range of research in medical natural language
processing, as well as computer vision and multi-modal learning when linked to
chest radiographs.

    

### [[2106.15481] Interactive Dimensionality Reduction for Comparative Analysis](http://arxiv.org/abs/2106.15481)


  Finding the similarities and differences between groups of datasets is a
fundamental analysis task. For high-dimensional data, dimensionality reduction
(DR) methods are often used to find the characteristics of each group. However,
existing DR methods provide limited capability and flexibility for such
comparative analysis as each method is designed only for a narrow analysis
target, such as identifying factors that most differentiate groups. This paper
presents an interactive DR framework where we integrate our new DR method,
called ULCA (unified linear comparative analysis), with an interactive visual
interface. ULCA unifies two DR schemes, discriminant analysis and contrastive
learning, to support various comparative analysis tasks. To provide flexibility
for comparative analysis, we develop an optimization algorithm that enables
analysts to interactively refine ULCA results. Additionally, the interactive
visualization interface facilitates interpretation and refinement of the ULCA
results. We evaluate ULCA and the optimization algorithm to show their
efficiency as well as present multiple case studies using real-world datasets
to demonstrate the usefulness of this framework.

    

### [[2108.00887] A Machine learning approach for rapid disaster response based on multi-modal data. The case of housing & shelter needs](http://arxiv.org/abs/2108.00887)


  Along with climate change, more frequent extreme events, such as flooding and
tropical cyclones, threaten the livelihoods and wellbeing of poor and
vulnerable populations. One of the most immediate needs of people affected by a
disaster is finding shelter. While the proliferation of data on disasters is
already helping to save lives, identifying damages in buildings, assessing
shelter needs, and finding appropriate places to establish emergency shelters
or settlements require a wide range of data to be combined rapidly. To address
this gap and make a headway in comprehensive assessments, this paper proposes a
machine learning workflow that aims to fuse and rapidly analyse multimodal
data. This workflow is built around open and online data to ensure scalability
and broad accessibility. Based on a database of 19 characteristics for more
than 200 disasters worldwide, a fusion approach at the decision level was used.
This technique allows the collected multimodal data to share a common semantic
space that facilitates the prediction of individual variables. Each fused
numerical vector was fed into an unsupervised clustering algorithm called
Self-Organizing-Maps (SOM). The trained SOM serves as a predictor for future
cases, allowing predicting consequences such as total deaths, total people
affected, and total damage, and provides specific recommendations for
assessments in the shelter and housing sector. To achieve such prediction, a
satellite image from before the disaster and the geographic and demographic
conditions are shown to the trained model, which achieved a prediction accuracy
of 62 %

    

### [[2104.03632] Can a CNN trained on the Ising model detect the phase transition of the $q$-state Potts model?](http://arxiv.org/abs/2104.03632)


  Employing a deep convolutional neural network (deep CNN) trained on spin
configurations of the 2D Ising model and the temperatures, we examine whether
the deep CNN can detect the phase transition of the 2D $q$-state Potts model.
To this end, we generate binarized images of spin configurations of the
$q$-state Potts model ($q\ge 3$) by replacing the spin variables
$\{0,1,\dots,\lfloor q/2\rfloor-1\}$ and $\{\lfloor q/2\rfloor,\dots,q-1\}$
with $\{0\}$ and $\{1\}$, respectively. Then, we input these images to the
trained CNN to output the predicted temperatures. The binarized images of the
$q$-state Potts model are entirely different from Ising spin configurations,
particularly at the transition temperature. Moreover, our CNN model is not
trained on the information about whether phases are ordered/disordered but is
naively trained by Ising spin configurations labeled with temperatures at which
they are generated. Nevertheless, the deep CNN can detect the transition point
with high accuracy, regardless of the type of transition. We also find that, in
the high-temperature region, the CNN outputs the temperature based on the
internal energy, whereas, in the low-temperature region, the output depends on
the magnetization and possibly the internal energy as well. However, in the
vicinity of the transition point, the CNN may use more general factors to
detect the transition point.

    

### [[2108.04029] Tensor Yard: One-Shot Algorithm of Hardware-Friendly Tensor-Train Decomposition for Convolutional Neural Networks](http://arxiv.org/abs/2108.04029)


  Nowadays Deep Learning became widely used in many economic, technical and
scientific areas of human interest. It is clear that efficiency of solutions
based on Deep Neural Networks should consider not only quality metric for the
target task, but also latency and constraints of target platform design should
be taken into account. In this paper we present novel hardware-friendly
Tensor-Train decomposition implementation for Convolutional Neural Networks
together with Tensor Yard - one-shot training algorithm which optimizes an
order of decomposition of network layers. These ideas allow to accelerate
ResNet models on Ascend 310 NPU devices without significant loss of accuracy.
For example we accelerate ResNet-101 by 14.6% with drop by 0.5 of top-1
ImageNet accuracy.

    

### [[2108.04042] Understanding Tool Synthesis Behavior and Safe Finite State Machine Design](http://arxiv.org/abs/2108.04042)


  High-reliability design requires understanding synthesis tool behavior and
best practices. Detection and protection against illegal states and transitions
is important for critical Finite State Machines (FSMs) within high reliability
applications. Single Event Upsets (SEUs) probability is increasing with
decreasing circuit dimensions and voltage [1]. SEU handling must be analyzed
post optimization to ensure designed protections are still functional. In this
work the default behavior of three synthesis tools interacting with high
reliability FSMs is discussed. Post-synthesis netlists of test FSMs are
analyzed for optimization induced changes that affect reliability during a SEU.
Best practices are proposed to curtail aggressive optimizers.

    

### [[2108.03284] Estimating Active Cases of COVID-19](http://arxiv.org/abs/2108.03284)


  Having accurate and timely data on confirmed active COVID-19 cases is
challenging, since it depends on testing capacity and the availability of an
appropriate infrastructure to perform tests and aggregate their results. In
this paper, we propose methods to estimate the number of active cases of
COVID-19 from the official data (of confirmed cases and fatalities) and from
survey data. We show that the latter is a viable option in countries with
reduced testing capacity or suboptimal infrastructures.

    

### [[2108.03326] From Domain-Specific Languages to Memory-Optimized Accelerators for Fluid Dynamics](http://arxiv.org/abs/2108.03326)


  Many applications are increasingly requiring numerical simulations for
solving complex problems. Most of these numerical algorithms are massively
parallel and often implemented on parallel high-performance computers. However,
classic CPU-based platforms suffers due to the demand for higher resolutions
and the exponential growth of data. FPGAs offer a powerful and flexible
alternative that can host accelerators to complement such platforms. Developing
such application-specific accelerators is still challenging because it is hard
to provide efficient code for hardware synthesis. In this paper, we study the
challenges of porting a numerical simulation kernel onto FPGA. We propose an
automated tool flow from a domain-specific language (DSL) to generate
accelerators for computational fluid dynamics on FPGA. Our DSL-based flow
simplifies the exploration of parameters and constraints such as on-chip memory
usage. We also propose a decoupled optimization of memory and logic resources,
which allows us to better use the limited FPGA resources. In our preliminary
evaluation, this enabled doubling the number of parallel kernels, increasing
the accelerator speedup versus ARM execution from 7 to 12 times.

    

### [[2108.03355] Asymmetry-aware Scalable Locking](http://arxiv.org/abs/2108.03355)


  The pursuit of power-efficiency is popularizing asymmetric multicore
processors (AMP) such as ARM big.LITTLE, Apple M1 and recent Intel Alder Lake
with big and little cores. However, we find that existing scalable locks fail
to scale on AMP and cause collapses in either throughput or latency, or both,
because their implicit assumption of symmetric cores no longer holds. To
address this issue, we propose the first asymmetry-aware scalable lock named
LibASL. LibASL provides a new lock ordering guided by applications' latency
requirements, which allows big cores to reorder with little cores for higher
throughput under the condition of preserving applications' latency
requirements. Using LibASL only requires linking the applications with it and,
if latency-critical, inserting few lines of code to annotate the coarse-grained
latency requirement. We evaluate LibASL in various benchmarks including five
popular databases on Apple M1. Evaluation results show that LibASL can improve
the throughput by up to 5 times while precisely preserving the tail latency
designated by applications.

    

### [[2108.03390] A High Throughput Parallel Hash Table on FPGA using XOR-based Memory](http://arxiv.org/abs/2108.03390)


  Hash table is a fundamental data structure for quick search and retrieval of
data. It is a key component in complex graph analytics and AI/ML applications.
State-of-the-art parallel hash table implementations either make some
simplifying assumptions such as supporting only a subset of hash table
operations or employ optimizations that lead to performance that is highly data
dependent and in the worst case can be similar to a sequential implementation.
In contrast, in this work we develop a dynamic hash table that supports all the
hash table queries - search, insert, delete, update, while allowing us to
support 'p' parallel queries (p>1) per clock cycle via p processing engines
(PEs) in the worst case i.e. the performance is data agnostic. We achieve this
by implementing novel XOR based multi-ported block memories on FPGAs.
Additionally, we develop a technique to optimize the memory requirement of the
hash table if the ratio of search to insert/update/delete queries is known
beforehand. We implement our design on state-of-the-art FPGA devices. Our
design is scalable to 16 PEs and supports throughput up to 5926 MOPS. It
matches the throughput of the state-of-the-art hash table design - FASTHash,
which only supports search and insert operations. Comparing with the best FPGA
design that supports the same set of operations, our hash table achieves up to
12.3x speedup.

    

### [[2108.03485] Building Analytics Pipelines for Querying Big Streams and Data Histories with H-STREAM](http://arxiv.org/abs/2108.03485)


  This paper introduces H-STREAM, a big stream/data processing pipelines
evaluation engine that proposes stream processing operators as micro-services
to support the analysis and visualisation of Big Data streams stemming from IoT
(Internet of Things) environments. H-STREAM micro-services combine stream
processing and data storage techniques tuned depending on the number of things
producing streams, the pace at which they produce them, and the physical
computing resources available for processing them online and delivering them to
consumers. H-STREAM delivers stream processing and visualisation micro-services
installed in a cloud environment. Micro-services can be composed for
implementing specific stream aggregation analysis pipelines as queries. The
paper presents an experimental validation using Microsoft Azure as a deployment
environment for testing the capacity of H-STREAM for dealing with velocity and
volume challenges in an (i) a neuroscience experiment and (in) a social
connectivity analysis scenario running on IoT farms.

    

### [[2108.03492] Clio: A Hardware-Software Co-Designed Disaggregated Memory System](http://arxiv.org/abs/2108.03492)


  Memory disaggregation has attracted great attention recently because of its
benefits in efficient memory utilization and ease of management. Research on
memory disaggregation so far has taken a software approach, running
disaggregated memory management software either at servers that act as
disaggregated memory nodes or at servers on the client side. This paper
proposes a hardware-based disaggregated memory device, Clio, that manages
disaggregated memory at the device side with novel software-hardware
co-designs. Clio includes a hardware-based virtual memory system, a customized
network stack, and a framework for computation offloading. Clio achieves low
median and tail latency, high throughput, excellent scalability, and low energy
cost.

    

### [[2108.03758] Tackling Consistency-related Design Challenges of Distributed Data-Intensive Systems - An Action Research Study](http://arxiv.org/abs/2108.03758)


  Background: Distributed data-intensive systems are increasingly designed to
be only eventually consistent. Persistent data is no longer processed with
serialized and transactional access, exposing applications to a range of
potential concurrency anomalies that need to be handled by the application
itself. Controlling concurrent data access in monolithic systems is already
challenging, but the problem is exacerbated in distributed systems. To make it
worse, only little systematic engineering guidance is provided by the software
architecture community regarding this issue. Aims: In this paper, we report on
our study of the effectiveness and applicability of the novel design guidelines
we are proposing in this regard. Method: We used action research and conducted
it in the context of the software architecture design process of a multi-site
platform development project. Results: Our hypotheses regarding effectiveness
and applicability have been accepted in the context of the study. The initial
design guidelines were refined throughout the study. Thus, we also contribute
concrete guidelines for architecting distributed data-intensive systems with
eventually consistent data. The guidelines are an advancement of Domain-Driven
Design and provide additional patterns for the tactical design part.
Conclusions: Based on our results, we recommend using the guidelines to
architect safe eventually consistent systems. Because of the relevance of
distributed data-intensive systems, we will drive this research forward and
evaluate it in further domains.

    

### [[2108.03982] Optimisation of an FPGA Credit Default Swap engine by embracing dataflow techniques](http://arxiv.org/abs/2108.03982)


  Quantitative finance is the use of mathematical models to analyse financial
markets and securities. Typically requiring significant amounts of computation,
an important question is the role that novel architectures can play in
accelerating these models in the future on HPC machines. In this paper we
explore the optimisation of an existing, open source, FPGA based Credit Default
Swap (CDS) engine using High Level Synthesis (HLS). Developed by Xilinx, and
part of their open source Vitis libraries, the implementation of this engine
currently favours flexibility and ease of integration over performance.
We explore redesigning the engine to fully embrace the dataflow approach,
ultimately resulting in an engine which is around eight times faster on an
Alveo U280 FPGA than the original Xilinx library version. We then compare five
of our engines on the U280 against a 24-core Xeon Platinum Cascade Lake CPU,
outperforming the CPU by around 1.55 times, with the FPGA consuming 4.7 times
less power and delivering around seven times the power efficiency of the CPU.

    

### [[2108.03991] A Parallel Boundary Element Method for the Electromagnetic Analysis of Large Structures With Lossy Conductors](http://arxiv.org/abs/2108.03991)


  In this paper, we propose an efficient parallelization strategy for boundary
element method (BEM) solvers that perform the electromagnetic analysis of
structures with lossy conductors. The proposed solver is accelerated with the
adaptive integral method, can model both homogeneous and multilayered
background media, and supports excitation via lumped ports or an incident
field. Unlike existing parallel BEM solvers, we use a formulation that
rigorously models the skin effect, which results in two coupled computational
workloads. The external-problem workload models electromagnetic coupling
between conductive objects, while the internal-problem workload describes field
distributions within them. We propose a parallelization strategy that
distributes these two workloads evenly over thousands of processing cores. The
external-problem workload is balanced in the same manner as existing parallel
solvers that employ approximate models for conductive objects. However, we
assert that the internal-problem workload should be balanced by algorithms from
scheduling theory. The parallel scalability of the proposed solver is tested on
three different structures found in both integrated circuits and metasurfaces.
The proposed parallelization strategy runs efficiently on distributed-memory
computers with thousands of CPU cores and outperforms competing strategies
derived from existing methods.

    

### [[2108.03994] Cloud to Ground Secured Computing: User Experiences on the Transition from Cloud-Based to Locally-Sited Hardware](http://arxiv.org/abs/2108.03994)


  The application of high-performance computing (HPC) processes, tools, and
technologies to Controlled Unclassified Information (CUI) creates both
opportunities and challenges. Building on our experiences developing,
deploying, and managing the Research Environment for Encumbered Data (REED)
hosted by AWS GovCloud, Research Computing at Purdue University has recently
deployed Weber, our locally-sited HPC solution for the storage and analysis of
CUI data. Weber presents our customer base with advances in data access,
portability, and usability at a low, stable cost while reducing administrative
overhead for our information technology support team.

    

### [[2108.03997] A new step for computing](http://arxiv.org/abs/2108.03997)


  The data center of tomorrow is a data center made up of heterogeneous
systems, which will run heterogeneous workloads. The systems will be located as
close as possible to the data. Heterogeneous systems will be equipped with
binary, biological inspired and quantum accelerators. These architectures will
be the foundations to address challenges. Like an orchestra conductor, the
hybrid cloud will make it possible to set these systems to music thanks to a
layer of security and intelligent automation.

    

### [[2108.04002] Preparing for Performance Analysis at Exascale](http://arxiv.org/abs/2108.04002)


  Performance tools for forthcoming heterogeneous exascale platforms must
address two principal challenges when analyzing execution measurements. First,
measurement of extreme-scale executions generates large volumes of performance
data. Second, performance metrics for heterogeneous applications are
significantly sparse across code regions. To address these challenges, we
developed a novel "streaming aggregation" approach to post-mortem analysis that
employs both shared and distributed memory parallelism to aggregate sparse
performance measurements from every rank, thread and GPU stream of a
large-scale application execution. Analysis results are stored in a pair of
sparse formats designed for efficient access to related data elements,
supporting responsive interactive presentation and scalable data analytics.
Empirical analysis shows that our implementation of this approach in HPCToolkit
effectively processes measurement data from thousands of threads using a
fraction of the compute resources employed by the application itself. Our
approach is able to perform analysis up to 9.4 times faster and store analysis
results 23 times smaller than HPCToolkit, providing a key building block for
scalable exascale performance tools.

    

### [[2108.04057] Peer-to-Peer Energy Sharing: A New Business Model Towards a Low-Carbon Future](http://arxiv.org/abs/2108.04057)


  The development of distributed generation technology is endowing consumers
the ability to produce energy and transforming them into "prosumers". This
transformation shall improve energy efficiency and pave the way to a low-carbon
future. However, it also exerts critical challenges on system operations, such
as the wasted backups for volatile renewable generation and the difficulty to
predict behavior of prosumers with conflicting interests and privacy concerns.
An emerging business model to tackle these challenges is peer-to-peer energy
sharing, whose concepts, structures, applications, models, and designs are
thoroughly reviewed in this paper, with an outlook of future research to better
realize its potentials.

    

### [[1901.08435] Mokka: BFT consensus](http://arxiv.org/abs/1901.08435)


  Mokka is a PC (CAP theorem) log-less BFT consensus algorithm for reaching the
consensus about a certain value in open networks. This algorithm has some
common approaches nested from RAFT, but its nature and design make Mokka a
better solution for DLT (distributed ledger).

    

### [[2101.03533] Con-Pi: A Distributed Container-based Edge and Fog Computing Framework](http://arxiv.org/abs/2101.03533)


  Edge and Fog computing paradigms overcome the limitations of cloud-centric
execution for different latency-sensitive Internet of Things (IoT) applications
by offering computing resources closer to the data sources. Small single-board
computers (SBCs) like Raspberry Pis (RPis) are widely used as computing nodes
in both paradigms. These devices are usually equipped with moderate speed
processors and provide support for peripheral interfacing and networking,
making them well-suited to deal with IoT-driven operations such as data
sensing, analysis, and actuation. However, these small Edge devices are
constrained in facilitating multi-tenancy and resource sharing. The management
of computing and peripheral resources through centralized entities further
degrades their performance and service quality significantly. To address these
issues, a fully distributed framework, named Con-Pi, is proposed in this work
to manage resources at the Edge or Fog environments. Con-Pi exploits the
concept of containerization and harnesses Docker containers to run IoT
applications as micro-services. %Moreover, Con-Pi operates in a distributed
manner across multiple RPis and enables them to share resources. The software
system of the proposed framework also provides a scope to integrate different
IoT applications, resource and energy management policies for Edge and Fog
computing. Its performance is compared with the state-of-the-art frameworks
through real-world experiments. The experimental results show that Con-Pi
outperforms others in enhancing response time and managing energy usage and
computing resources through its distributed offloading model. Further, we have
developed an automated pest bird deterrent system using Con-Pi to demonstrate
its suitability in developing practical solutions for various IoT-enabled use
cases, including smart agriculture.

    

### [[2105.09508] Fast Nonblocking Persistence for Concurrent Data Structures](http://arxiv.org/abs/2105.09508)


  We present a fully lock-free variant of the recent Montage system for
persistent data structures. Our variant, nbMontage, adds persistence to almost
any nonblocking concurrent structure without introducing significant overhead
or blocking of any kind. Like its predecessor, nbMontage is buffered durably
linearizable: it guarantees that the state recovered in the wake of a crash
will represent a consistent prefix of pre-crash execution. Unlike its
predecessor, nbMontage ensures wait-free progress of the persistence frontier,
thereby bounding the number of recent updates that may be lost on a crash, and
allowing a thread to force an update of the frontier (i.e., to perform a sync
operation) without the risk of blocking. As an extra benefit, the helping
mechanism employed by our wait-free sync significantly reduces its latency.
Performance results for nonblocking queues, skip lists, trees, and hash
tables rival custom data structures in the literature -- dramatically faster
than achieved with prior general-purpose systems, and generally within 50% of
equivalent non-persistent structures placed in DRAM.

    

### [[2108.03287] Semantic Segmentation and Object Detection Towards Instance Segmentation: Breast Tumor Identification](http://arxiv.org/abs/2108.03287)


  Breast cancer is one of the factors that cause the increase of mortality of
women. The most widely used method for diagnosing this geological disease i.e.
breast cancer is the ultrasound scan. Several key features such as the
smoothness and the texture of the tumor captured through ultrasound scans
encode the abnormality of the breast tumors (malignant from benign). However,
ultrasound scans are often noisy and include irrelevant parts of the breast
that may bias the segmentation of eventual tumors. In this paper, we are going
to extract the region of interest ( i.e, bounding boxes of the tumors) and
feed-forward them to one semantic segmentation encoder-decoder structure based
on its classification (i.e, malignant or benign). the whole process aims to
build an instance-based segmenter from a semantic segmenter and an object
detector.

    

### [[2108.03294] A Smart and Defensive Human-Machine Approach to Code Analysis](http://arxiv.org/abs/2108.03294)


  Static analysis remains one of the most popular approaches for detecting and
correcting poor or vulnerable program code. It involves the examination of code
listings, test results, or other documentation to identify errors, violations
of development standards, or other problems, with the ultimate goal of fixing
these errors so that systems and software are as secure as possible. There
exists a plethora of static analysis tools, which makes it challenging for
businesses and programmers to select a tool to analyze their program code. It
is imperative to find ways to improve code analysis so that it can be employed
by cyber defenders to mitigate security risks. In this research, we propose a
method that employs the use of virtual assistants to work with programmers to
ensure that software are as safe as possible in order to protect
safety-critical systems from data breaches and other attacks. The pro- posed
method employs a recommender system that uses various metrics to help
programmers select the most appropriate code analysis tool for their project
and guides them through the analysis process. The system further tracks the
user's behavior regarding the adoption of the recommended practices.

    

### [[2108.03305] Offensive Language and Hate Speech Detection with Deep Learning and Transfer Learning](http://arxiv.org/abs/2108.03305)


  Toxic online speech has become a crucial problem nowadays due to an
exponential increase in the use of internet by people from different cultures
and educational backgrounds. Differentiating if a text message belongs to hate
speech and offensive language is a key challenge in automatic detection of
toxic text content. In this paper, we propose an approach to automatically
classify tweets into three classes: Hate, offensive and Neither. Using public
tweet data set, we first perform experiments to build BI-LSTM models from empty
embedding and then we also try the same neural network architecture with
pre-trained Glove embedding. Next, we introduce a transfer learning approach
for hate speech detection using an existing pre-trained language model BERT
(Bidirectional Encoder Representations from Transformers), DistilBert
(Distilled version of BERT) and GPT-2 (Generative Pre-Training). We perform
hyper parameters tuning analysis of our best model (BI-LSTM) considering
different neural network architectures, learn-ratings and normalization methods
etc. After tuning the model and with the best combination of parameters, we
achieve over 92 percent accuracy upon evaluating it on test data. We also
create a class module which contains main functionality including text
classification, sentiment checking and text data augmentation. This model could
serve as an intermediate module between user and Twitter.

    

### [[2108.03319] Semantic Tracklets: An Object-Centric Representation for Visual Multi-Agent Reinforcement Learning](http://arxiv.org/abs/2108.03319)


  Solving complex real-world tasks, e.g., autonomous fleet control, often
involves a coordinated team of multiple agents which learn strategies from
visual inputs via reinforcement learning. Many existing multi-agent
reinforcement learning (MARL) algorithms however don't scale to environments
where agents operate on visual inputs. To address this issue, algorithmically,
recent works have focused on non-stationarity and exploration. In contrast, we
study whether scalability can also be achieved via a disentangled
representation. For this, we explicitly construct an object-centric
intermediate representation to characterize the states of an environment, which
we refer to as `semantic tracklets.' We evaluate `semantic tracklets' on the
visual multi-agent particle environment (VMPE) and on the challenging visual
multi-agent GFootball environment. `Semantic tracklets' consistently outperform
baselines on VMPE, and achieve a +2.4 higher score difference than baselines on
GFootball. Notably, this method is the first to successfully learn a strategy
for five players in the GFootball environment using only visual data.

    

### [[2108.03332] BEHAVIOR: Benchmark for Everyday Household Activities in Virtual, Interactive, and Ecological Environments](http://arxiv.org/abs/2108.03332)


  We introduce BEHAVIOR, a benchmark for embodied AI with 100 activities in
simulation, spanning a range of everyday household chores such as cleaning,
maintenance, and food preparation. These activities are designed to be
realistic, diverse, and complex, aiming to reproduce the challenges that agents
must face in the real world. Building such a benchmark poses three fundamental
difficulties for each activity: definition (it can differ by time, place, or
person), instantiation in a simulator, and evaluation. BEHAVIOR addresses these
with three innovations. First, we propose an object-centric, predicate
logic-based description language for expressing an activity's initial and goal
conditions, enabling generation of diverse instances for any activity. Second,
we identify the simulator-agnostic features required by an underlying
environment to support BEHAVIOR, and demonstrate its realization in one such
simulator. Third, we introduce a set of metrics to measure task progress and
efficiency, absolute and relative to human demonstrators. We include 500 human
demonstrations in virtual reality (VR) to serve as the human ground truth. Our
experiments demonstrate that even state of the art embodied AI solutions
struggle with the level of realism, diversity, and complexity imposed by the
activities in our benchmark. We make BEHAVIOR publicly available at
this http URL to facilitate and calibrate the development of new
embodied AI solutions.

    

### [[2108.03356] HelpViz: Automatic Generation of Contextual Visual MobileTutorials from Text-Based Instructions](http://arxiv.org/abs/2108.03356)


  We present HelpViz, a tool for generating contextual visual mobile tutorials
from text-based instructions that are abundant on the web. HelpViz transforms
text instructions to graphical tutorials in batch, by extracting a sequence of
actions from each text instruction through an instruction parsing model, and
executing the extracted actions on a simulation infrastructure that manages an
array of Android emulators. The automatic execution of each instruction
produces a set of graphical and structural assets, including images, videos,
and metadata such as clicked elements for each step. HelpViz then synthesizes a
tutorial by combining parsed text instructions with the generated assets, and
contextualizes the tutorial to user interaction by tracking the user's progress
and highlighting the next step.
Our experiments with HelpViz indicate that our pipeline improved tutorial
execution robustness and that participants preferred tutorials generated by
HelpViz over text-based instructions. HelpViz promises a cost-effective
approach for generating contextual visual tutorials for mobile interaction at
scale.

    

### [[2108.03360] DySR: A Dynamic Representation Learning and Aligning based Model for Service Bundle Recommendation](http://arxiv.org/abs/2108.03360)


  An increasing number and diversity of services are available, which result in
significant challenges to effective reuse service during requirement
satisfaction. There have been many service bundle recommendation studies and
achieved remarkable results. However, there is still plenty of room for
improvement in the performance of these methods. The fundamental problem with
these studies is that they ignore the evolution of services over time and the
representation gap between services and requirements. In this paper, we propose
a dynamic representation learning and aligning based model called DySR to
tackle these issues. DySR eliminates the representation gap between services
and requirements by learning a transformation function and obtains service
representations in an evolving social environment through dynamic graph
representation learning. Extensive experiments conducted on a real-world
dataset from ProgrammableWeb show that DySR outperforms existing
state-of-the-art methods in commonly used evaluation metrics, improving $F1@5$
from $36.1\%$ to $69.3\%$.

    

### [[2108.03369] A Logical Characterization of the Preferred Models of Logic Programs with Ordered Disjunction](http://arxiv.org/abs/2108.03369)


  Logic Programs with Ordered Disjunction (LPODs) extend classical logic
programs with the capability of expressing alternatives with decreasing degrees
of preference in the heads of program rules. Despite the fact that the
operational meaning of ordered disjunction is clear, there exists an important
open issue regarding its semantics. In particular, there does not exist a
purely model-theoretic approach for determining the most preferred models of an
LPOD. At present, the selection of the most preferred models is performed using
a technique that is not based exclusively on the models of the program and in
certain cases produces counterintuitive results. We provide a novel,
model-theoretic semantics for LPODs, which uses an additional truth value in
order to identify the most preferred models of a program. We demonstrate that
the proposed approach overcomes the shortcomings of the traditional semantics
of LPODs. Moreover, the new approach can be used to define the semantics of a
natural class of logic programs that can have both ordered and classical
disjunctions in the heads of clauses. This allows programs that can express not
only strict levels of preferences but also alternatives that are equally
preferred. This work is under consideration for acceptance in TPLP.

    

### [[2108.03374] What a million Indian farmers say?: A crowdsourcing-based method for pest surveillance](http://arxiv.org/abs/2108.03374)


  Many different technologies are used to detect pests in the crops, such as
manual sampling, sensors, and radar. However, these methods have scalability
issues as they fail to cover large areas, are uneconomical and complex. This
paper proposes a crowdsourced based method utilising the real-time farmer
queries gathered over telephones for pest surveillance. We developed
data-driven strategies by aggregating and analyzing historical data to find
patterns and get future insights into pest occurrence. We showed that it can be
an accurate and economical method for pest surveillance capable of enveloping a
large area with high spatio-temporal granularity. Forecasting the pest
population will help farmers in making informed decisions at the right time.
This will also help the government and policymakers to make the necessary
preparations as and when required and may also ensure food security.

    

### [[2108.03377] Generating Personalized Dialogue via Multi-Task Meta-Learning](http://arxiv.org/abs/2108.03377)


  Conventional approaches to personalized dialogue generation typically require
a large corpus, as well as predefined persona information. However, in a
real-world setting, neither a large corpus of training data nor persona
information are readily available. To address these practical limitations, we
propose a novel multi-task meta-learning approach which involves training a
model to adapt to new personas without relying on a large corpus, or on any
predefined persona information. Instead, the model is tasked with generating
personalized responses based on only the dialogue context. Unlike prior work,
our approach leverages on the provided persona information only during training
via the introduction of an auxiliary persona reconstruction task. In this
paper, we introduce 2 frameworks that adopt the proposed multi-task
meta-learning approach: the Multi-Task Meta-Learning (MTML) framework, and the
Alternating Multi-Task Meta-Learning (AMTML) framework. Experimental results
show that utilizing MTML and AMTML results in dialogue responses with greater
persona consistency.

    

### [[2108.03383] Artificial Intelligence-Driven Customized Manufacturing Factory: Key Technologies, Applications, and Challenges](http://arxiv.org/abs/2108.03383)


  The traditional production paradigm of large batch production does not offer
flexibility towards satisfying the requirements of individual customers. A new
generation of smart factories is expected to support new multi-variety and
small-batch customized production modes. For that, Artificial Intelligence (AI)
is enabling higher value-added manufacturing by accelerating the integration of
manufacturing and information communication technologies, including computing,
communication, and control. The characteristics of a customized smart factory
are to include self-perception, operations optimization, dynamic
reconfiguration, and intelligent decision-making. The AI technologies will
allow manufacturing systems to perceive the environment, adapt to the external
needs, and extract the process knowledge, including business models, such as
intelligent production, networked collaboration, and extended service models.
This paper focuses on the implementation of AI in customized manufacturing
(CM). The architecture of an AI-driven customized smart factory is presented.
Details of intelligent manufacturing devices, intelligent information
interaction, and construction of a flexible manufacturing line are showcased.
The state-of-the-art AI technologies of potential use in CM, i.e., machine
learning, multi-agent systems, Internet of Things, big data, and cloud-edge
computing are surveyed. The AI-enabled technologies in a customized smart
factory are validated with a case study of customized packaging. The
experimental results have demonstrated that the AI-assisted CM offers the
possibility of higher production flexibility and efficiency. Challenges and
solutions related to AI in CM are also discussed.

    

### [[2108.03414] Vision Transformers for femur fracture classification](http://arxiv.org/abs/2108.03414)


  Objectives: In recent years, the scientific community has focused on the
development of Computer-Aided Diagnosis (CAD) tools that could improve bone
fractures' classification. However, the results of the classification of
fractures in subtypes with the proposed datasets were far from optimal. This
paper proposes a very recent and outperforming deep learning technique, the
Vision Transformer (ViT), in order to improve the fracture classification, by
exploiting its self-attention mechanism.
Methods: 4207 manually annotated images were used and distributed, by
following the AO/OTA classification, in different fracture types, the largest
labeled dataset of proximal femur fractures used in literature. The ViT
architecture was used and compared with a classic Convolutional Neural Network
(CNN) and a multistage architecture composed by successive CNNs in cascade. To
demonstrate the reliability of this approach, 1) the attention maps were used
to visualize the most relevant areas of the images, 2) the performance of a
generic CNN and ViT was also compared through unsupervised learning techniques,
and 3) 11 specialists were asked to evaluate and classify 150 proximal femur
fractures' images with and without the help of the ViT.
Results: The ViT was able to correctly predict 83% of the test images.
Precision, recall and F1-score were 0.77 (CI 0.64-0.90), 0.76 (CI 0.62-0.91)
and 0.77 (CI 0.64-0.89), respectively. The average specialists' diagnostic
improvement was 29%.
Conclusions: This paper showed the potential of Transformers in bone fracture
classification. For the first time, good results were obtained in sub-fractures
with the largest and richest dataset ever.

    

### [[2108.03429] Enhancing MR Image Segmentation with Realistic Adversarial Data Augmentation](http://arxiv.org/abs/2108.03429)


  The success of neural networks on medical image segmentation tasks typically
relies on large labeled datasets for model training. However, acquiring and
manually labeling a large medical image set is resource-intensive, expensive,
and sometimes impractical due to data sharing and privacy issues. To address
this challenge, we propose an adversarial data augmentation approach to improve
the efficiency in utilizing training data and to enlarge the dataset via
simulated but realistic transformations. Specifically, we present a generic
task-driven learning framework, which jointly optimizes a data augmentation
model and a segmentation network during training, generating informative
examples to enhance network generalizability for the downstream task. The data
augmentation model utilizes a set of photometric and geometric image
transformations and chains them to simulate realistic complex imaging
variations that could exist in magnetic resonance (MR) imaging. The proposed
adversarial data augmentation does not rely on generative networks and can be
used as a plug-in module in general segmentation networks. It is
computationally efficient and applicable for both supervised and
semi-supervised learning. We analyze and evaluate the method on two MR image
segmentation tasks: cardiac segmentation and prostate segmentation. Results
show that the proposed approach can alleviate the need for labeled data while
improving model generalization ability, indicating its practical value in
medical imaging applications.

    

### [[2108.03434] NASOA: Towards Faster Task-oriented Online Fine-tuning with a Zoo of Models](http://arxiv.org/abs/2108.03434)


  Fine-tuning from pre-trained ImageNet models has been a simple, effective,
and popular approach for various computer vision tasks. The common practice of
fine-tuning is to adopt a default hyperparameter setting with a fixed
pre-trained model, while both of them are not optimized for specific tasks and
time constraints. Moreover, in cloud computing or GPU clusters where the tasks
arrive sequentially in a stream, faster online fine-tuning is a more desired
and realistic strategy for saving money, energy consumption, and CO2 emission.
In this paper, we propose a joint Neural Architecture Search and Online
Adaption framework named NASOA towards a faster task-oriented fine-tuning upon
the request of users. Specifically, NASOA first adopts an offline NAS to
identify a group of training-efficient networks to form a pretrained model zoo.
We propose a novel joint block and macro-level search space to enable a
flexible and efficient search. Then, by estimating fine-tuning performance via
an adaptive model by accumulating experience from the past tasks, an online
schedule generator is proposed to pick up the most suitable model and generate
a personalized training regime with respect to each desired task in a one-shot
fashion. The resulting model zoo is more training efficient than SOTA models,
e.g. 6x faster than RegNetY-16GF, and 1.7x faster than EfficientNetB3.
Experiments on multiple datasets also show that NASOA achieves much better
fine-tuning results, i.e. improving around 2.1% accuracy than the best
performance in RegNet series under various constraints and tasks; 40x faster
compared to the BOHB.

    

### [[2108.03439] Towards Discriminative Representation Learning for Unsupervised Person Re-identification](http://arxiv.org/abs/2108.03439)


  In this work, we address the problem of unsupervised domain adaptation for
person re-ID where annotations are available for the source domain but not for
target. Previous methods typically follow a two-stage optimization pipeline,
where the network is first pre-trained on source and then fine-tuned on target
with pseudo labels created by feature clustering. Such methods sustain two main
limitations. (1) The label noise may hinder the learning of discriminative
features for recognizing target classes. (2) The domain gap may hinder
knowledge transferring from source to target. We propose three types of
technical schemes to alleviate these issues. First, we propose a cluster-wise
contrastive learning algorithm (CCL) by iterative optimization of feature
learning and cluster refinery to learn noise-tolerant representations in the
unsupervised manner. Second, we adopt a progressive domain adaptation (PDA)
strategy to gradually mitigate the domain gap between source and target data.
Third, we propose Fourier augmentation (FA) for further maximizing the class
separability of re-ID models by imposing extra constraints in the Fourier
space. We observe that these proposed schemes are capable of facilitating the
learning of discriminative feature representations. Experiments demonstrate
that our method consistently achieves notable improvements over the
state-of-the-art unsupervised re-ID methods on multiple benchmarks, e.g.,
surpassing MMT largely by 8.1\%, 9.9\%, 11.4\% and 11.1\% mAP on the
Market-to-Duke, Duke-to-Market, Market-to-MSMT and Duke-to-MSMT tasks,
respectively.

    

### [[2108.03452] Rethinking of AlphaStar](http://arxiv.org/abs/2108.03452)


  We present a different view for AlphaStar (AS), the program achieving
Grand-Master level in the game StarCraft II. It is considered big progress for
AI research. However, in this paper, we present problems with the AS, some of
which are the defects of it, and some of which are important details that are
neglected in its article. These problems arise two questions. One is that what
can we get from the built of AS? The other is that does the battle between it
with humans fair? After the discussion, we present the future research
directions for these problems. Our study is based on a reproduction code of the
AS, and the codes are available online.

    

### [[2108.03456] A Unified Model for Zero-shot Music Source Separation, Transcription and Synthesis](http://arxiv.org/abs/2108.03456)


  We propose a unified model for three inter-related tasks: 1) to
\textit{separate} individual sound sources from a mixed music audio, 2) to
\textit{transcribe} each sound source to MIDI notes, and 3) to\textit{
synthesize} new pieces based on the timbre of separated sources. The model is
inspired by the fact that when humans listen to music, our minds can not only
separate the sounds of different instruments, but also at the same time
perceive high-level representations such as score and timbre. To mirror such
capability computationally, we designed a pitch-timbre disentanglement module
based on a popular encoder-decoder neural architecture for source separation.
The key inductive biases are vector-quantization for pitch representation and
pitch-transformation invariant for timbre representation. In addition, we
adopted a query-by-example method to achieve \textit{zero-shot} learning, i.e.,
the model is capable of doing source separation, transcription, and synthesis
for \textit{unseen} instruments. The current design focuses on audio mixtures
of two monophonic instruments. Experimental results show that our model
outperforms existing multi-task baselines, and the transcribed score serves as
a powerful auxiliary for separation tasks.

    

### [[2108.03474] Solution Enumeration by Optimality in Answer Set Programming](http://arxiv.org/abs/2108.03474)


  Given a combinatorial search problem, it may be highly useful to enumerate
its (all) solutions besides just finding one solution, or showing that none
exists. The same can be stated about optimal solutions if an objective function
is provided. This work goes beyond the bare enumeration of optimal solutions
and addresses the computational task of solution enumeration by optimality
(SEO). This task is studied in the context of Answer Set Programming (ASP)
where (optimal) solutions of a problem are captured with the answer sets of a
logic program encoding the problem. Existing answer-set solvers already support
the enumeration of all (optimal) answer sets. However, in this work, we
generalize the enumeration of optimal answer sets beyond strictly optimal ones,
giving rise to the idea of answer set enumeration in the order of optimality
(ASEO). This approach is applicable up to the best k answer sets or in an
unlimited setting, which amounts to a process of sorting answer sets based on
the objective function. As the main contribution of this work, we present the
first general algorithms for the aforementioned tasks of answer set
enumeration. Moreover, we illustrate the potential use cases of ASEO. First, we
study how efficiently access to the next-best solutions can be achieved in a
number of optimization problems that have been formalized and solved in ASP.
Second, we show that ASEO provides us with an effective sampling technique for
Bayesian networks.

    

### [[2108.03533] Improving Similar Language Translation With Transfer Learning](http://arxiv.org/abs/2108.03533)


  We investigate transfer learning based on pre-trained neural machine
translation models to translate between (low-resource) similar languages. This
work is part of our contribution to the WMT 2021 Similar Languages Translation
Shared Task where we submitted models for different language pairs, including
French-Bambara, Spanish-Catalan, and Spanish-Portuguese in both directions. Our
models for Catalan-Spanish ($82.79$ BLEU) and Portuguese-Spanish ($87.11$ BLEU)
rank top 1 in the official shared task evaluation, and we are the only team to
submit models for the French-Bambara pairs.

    

### [[2108.03538] Cough Detection Using Selected Informative Features from Audio Signals](http://arxiv.org/abs/2108.03538)


  Cough is a common symptom of respiratory and lung diseases. Cough detection
is important to prevent, assess and control epidemic, such as COVID-19. This
paper proposes a model to detect cough events from cough audio signals. The
models are trained by the dataset combined ESC-50 dataset with self-recorded
cough recordings. The test dataset contains inpatient cough recordings
collected from inpatients of the respiratory disease department in Ruijin
Hospital. We totally build 15 cough detection models based on different feature
numbers selected by Random Frog, Uninformative Variable Elimination (UVE), and
Variable influence on projection (VIP) algorithms respectively. The optimal
model is based on 20 features selected from Mel Frequency Cepstral Coefficients
(MFCC) features by UVE algorithm and classified with Support Vector Machine
(SVM) linear two-class classifier. The best cough detection model realizes the
accuracy, recall, precision and F1-score with 94.9%, 97.1%, 93.1% and 0.95
respectively. Its excellent performance with fewer dimensionality of the
feature vector shows the potential of being applied to mobile devices, such as
smartphones, thus making cough detection remote and non-contact.

    

### [[2108.03543] Spatio-Temporal Attention Mechanism and Knowledge Distillation for Lip Reading](http://arxiv.org/abs/2108.03543)


  Despite the advancement in the domain of audio and audio-visual speech
recognition, visual speech recognition systems are still quite under-explored
due to the visual ambiguity of some phonemes. In this work, we propose a new
lip-reading model that combines three contributions. First, the model front-end
adopts a spatio-temporal attention mechanism to help extract the informative
data from the input visual frames. Second, the model back-end utilizes a
sequence-level and frame-level Knowledge Distillation (KD) techniques that
allow leveraging audio data during the visual model training. Third, a data
preprocessing pipeline is adopted that includes facial landmarks
detection-based lip-alignment. On LRW lip-reading dataset benchmark, a
noticeable accuracy improvement is demonstrated; the spatio-temporal attention,
Knowledge Distillation, and lip-alignment contributions achieved 88.43%,
88.64%, and 88.37% respectively.

    

### [[2108.03554] Semantic-Based Explainable AI: Leveraging Semantic Scene Graphs and Pairwise Ranking to Explain Robot Failures](http://arxiv.org/abs/2108.03554)


  When interacting in unstructured human environments, occasional robot
failures are inevitable. When such failures occur, everyday people, rather than
trained technicians, will be the first to respond. Existing natural language
explanations hand-annotate contextual information from an environment to help
everyday people understand robot failures. However, this methodology lacks
generalizability and scalability. In our work, we introduce a more
generalizable semantic explanation framework. Our framework autonomously
captures the semantic information in a scene to produce semantically
descriptive explanations for everyday users. To generate failure-focused
explanations that are semantically grounded, we leverages both semantic scene
graphs to extract spatial relations and object attributes from an environment,
as well as pairwise ranking. Our results show that these semantically
descriptive explanations significantly improve everyday users' ability to both
identify failures and provide assistance for recovery than the existing
state-of-the-art context-based explanations.

    

### [[2108.03599] Identification of Play Styles in Universal Fighting Engine](http://arxiv.org/abs/2108.03599)


  AI-controlled characters in fighting games are expected to possess reasonably
high skills and behave in a believable, human-like manner, exhibiting a
diversity of play styles and strategies. Thus, the development of fighting game
AI requires the ability to evaluate these properties. For instance, it should
be possible to ensure that the characters created are believable and diverse.
In this paper, we show how an automated procedure can be used to compare play
styles of individual AI- and human-controlled characters, and to assess
human-likeness and diversity of game participants.

    

### [[2108.03603] Understanding the computational demands underlying visual reasoning](http://arxiv.org/abs/2108.03603)


  Visual understanding requires comprehending complex visual relations between
objects within a scene. Here, we seek to characterize the computational demands
for abstract visual reasoning. We do this by systematically assessing the
ability of modern deep convolutional neural networks (CNNs) to learn to solve
the Synthetic Visual Reasoning Test (SVRT) challenge, a collection of
twenty-three visual reasoning problems. Our analysis leads to a novel taxonomy
of visual reasoning tasks, which can be primarily explained by both the type of
relations (same-different vs. spatial-relation judgments) and the number of
relations used to compose the underlying rules. Prior cognitive neuroscience
work suggests that attention plays a key role in human's visual reasoning
ability. To test this, we extended the CNNs with spatial and feature-based
attention mechanisms. In a second series of experiments, we evaluated the
ability of these attention networks to learn to solve the SVRT challenge and
found the resulting architectures to be much more efficient at solving the
hardest of these visual reasoning tasks. Most importantly, the corresponding
improvements on individual tasks partially explained the taxonomy. Overall,
this work advances our understanding of visual reasoning and yields testable
Neuroscience predictions regarding the need for feature-based vs. spatial
attention in visual reasoning.

    

### [[2108.03694] Event-driven Vision and Control for UAVs on a Neuromorphic Chip](http://arxiv.org/abs/2108.03694)


  Event-based vision sensors achieve up to three orders of magnitude better
speed vs. power consumption trade off in high-speed control of UAVs compared to
conventional image sensors. Event-based cameras produce a sparse stream of
events that can be processed more efficiently and with a lower latency than
images, enabling ultra-fast vision-driven control. Here, we explore how an
event-based vision algorithm can be implemented as a spiking neuronal network
on a neuromorphic chip and used in a drone controller. We show how seamless
integration of event-based perception on chip leads to even faster control
rates and lower latency. In addition, we demonstrate how online adaptation of
the SNN controller can be realised using on-chip learning. Our spiking neuronal
network on chip is the first example of a neuromorphic vision-based controller
solving a high-speed UAV control task. The excellent scalability of processing
in neuromorphic hardware opens the possibility to solve more challenging visual
tasks in the future and integrate visual perception in fast control loops.

    

### [[2108.03704] OVIS: Open-Vocabulary Visual Instance Search via Visual-Semantic Aligned Representation Learning](http://arxiv.org/abs/2108.03704)


  We introduce the task of open-vocabulary visual instance search (OVIS). Given
an arbitrary textual search query, Open-vocabulary Visual Instance Search
(OVIS) aims to return a ranked list of visual instances, i.e., image patches,
that satisfies the search intent from an image database. The term "open
vocabulary" means that there are neither restrictions to the visual instance to
be searched nor restrictions to the word that can be used to compose the
textual search query. We propose to address such a search challenge via
visual-semantic aligned representation learning (ViSA). ViSA leverages massive
image-caption pairs as weak image-level (not instance-level) supervision to
learn a rich cross-modal semantic space where the representations of visual
instances (not images) and those of textual queries are aligned, thus allowing
us to measure the similarities between any visual instance and an arbitrary
textual query. To evaluate the performance of ViSA, we build two datasets named
OVIS40 and OVIS1600 and also introduce a pipeline for error analysis. Through
extensive experiments on the two datasets, we demonstrate ViSA's ability to
search for visual instances in images not available during training given a
wide range of textual queries including those composed of uncommon words.
Experimental results show that ViSA achieves an mAP@50 of 21.9% on OVIS40 under
the most challenging setting and achieves an mAP@6 of 14.9% on OVIS1600
dataset.

    

### [[2108.03730] Learning Proxemic Behavior Using Reinforcement Learning with Cognitive Agents](http://arxiv.org/abs/2108.03730)


  Proxemics is a branch of non-verbal communication concerned with studying the
spatial behavior of people and animals. This behavior is an essential part of
the communication process due to delimit the acceptable distance to interact
with another being. With increasing research on human-agent interaction, new
alternatives are needed that allow optimal communication, avoiding agents
feeling uncomfortable. Several works consider proxemic behavior with cognitive
agents, where human-robot interaction techniques and machine learning are
implemented. However, environments consider fixed personal space and that the
agent previously knows it. In this work, we aim to study how agents behave in
environments based on proxemic behavior, and propose a modified gridworld to
that aim. This environment considers an issuer with proxemic behavior that
provides a disagreement signal to the agent. Our results show that the learning
agent can identify the proxemic space when the issuer gives feedback about
agent performance.

    

### [[2108.03731] Leveraging Commonsense Knowledge on Classifying False News and Determining Checkworthiness of Claims](http://arxiv.org/abs/2108.03731)


  Widespread and rapid dissemination of false news has made fact-checking an
indispensable requirement. Given its time-consuming and labor-intensive nature,
the task calls for an automated support to meet the demand. In this paper, we
propose to leverage commonsense knowledge for the tasks of false news
classification and check-worthy claim detection. Arguing that commonsense
knowledge is a factor in human believability, we fine-tune the BERT language
model with a commonsense question answering task and the aforementioned tasks
in a multi-task learning environment. For predicting fine-grained false news
types, we compare the proposed fine-tuned model's performance with the false
news classification models on a public dataset as well as a newly collected
dataset. We compare the model's performance with the single-task BERT model and
a state-of-the-art check-worthy claim detection tool to evaluate the
check-worthy claim detection. Our experimental analysis demonstrates that
commonsense knowledge can improve performance in both tasks.

    

### [[2108.03760] Symptom based Hierarchical Classification of Diabetes and Thyroid disorders using Fuzzy Cognitive Maps](http://arxiv.org/abs/2108.03760)


  Fuzzy Cognitive Maps (FCMs) are soft computing technique that follows an
approach similar to human reasoning and human decision-making process, making
them a valuable modeling and simulation methodology. Medical Decision Systems
are complex systems consisting of many factors that may be complementary,
contradictory, and competitive; these factors influence each other and
determine the overall diagnosis with a different degree. Thus, FCMs are
suitable to model Medical Decision Support Systems. The proposed work therefore
uses FCMs arranged in hierarchical structure to classify between Diabetes,
Thyroid disorders and their subtypes. Subtypes include type 1 and type 2 for
diabetes and hyperthyroidism and hypothyroidism for thyroid.

    

### [[2108.03793] Toward Human-Level Artificial Intelligence](http://arxiv.org/abs/2108.03793)


  In this paper, we present our research on programming human-level artificial
intelligence (HLAI), including 1) a definition of HLAI, 2) an environment to
develop and test HLAI, and 3) a cognitive architecture for HLAI. The term AI is
used in a broad meaning, and HLAI is not clearly defined. I claim that the
essence of Human-Level Intelligence to be the capability to learn from others'
experiences via language. The key is that the event described by language has
the same effect as if the agent experiences it firsthand for the update of the
behavior policy. To develop and test models with such a capability, we are
developing a simulated environment called SEDRo. There is a 3D Home, and a
mother character takes care of the baby (the learning agent) and teaches
languages. The environment provides comparable experiences to that of a human
baby from birth to one year. Finally, I propose a cognitive architecture of
HLAI called Modulated Heterarchical Prediction Memory (mHPM). In mHPM, there
are three components: a universal module that learns to predict the next vector
given the sequence of vector signals, a heterarchical network of those modules,
and a reward-based modulation of learning. mHPM models the workings of the
neocortex but the innate auxiliary units such hippocampus, reward system,
instincts, and amygdala play critical roles, too.

    

### [[2108.03815] P-WAE: Generalized Patch-Wasserstein Autoencoder for Anomaly Screening](http://arxiv.org/abs/2108.03815)


  To mitigate the inspector's workload and improve the quality of the product,
computer vision-based anomaly detection (AD) techniques are gradually deployed
in real-world industrial scenarios. Recent anomaly analysis benchmarks progress
to generative models. The aim is to model the defect-free distribution so that
anomalies can be classified as out-of-distribution samples. Nevertheless, there
are two disturbing factors that need researchers and deployers to prioritize:
(i) the simplistic prior latent distribution inducing limited expressive
capability; (ii) the collapsed mutual-dependent features resulting in poor
generalization. In this paper, we propose a novel Patch-wise Wasserstein
AutoEncoder (P-WAE) architecture to alleviate those challenges. In particular,
a patch-wise variational inference model coupled with solving the jigsaw puzzle
is designed, which is a simple yet effective way to increase the expressiveness
and complexity of the latent manifold. This alleviates the blurry
reconstruction problem. In addition, the Hilbert-Schmidt Independence Criterion
(HSIC) bottleneck is introduced to constrain the over-regularization
representation. Comprehensive experiments, conducted on the MVTec AD dataset,
demonstrate the superior performance of our propo

    

### [[2108.03818] Time-Frequency Localization Using Deep Convolutional Maxout Neural Network in Persian Speech Recognition](http://arxiv.org/abs/2108.03818)


  In this paper, a CNN-based structure for time-frequency localization of audio
signal information in the ASR acoustic model is proposed for Persian speech
recognition. Research has shown that the receptive fields' time-frequency
flexibility in some mammals' auditory neurons system improves recognition
performance. Biosystems have inspired many artificial systems because of their
high efficiency and performance, so time-frequency localization has been used
extensively to improve system performance. In the last few years, much work has
been done to localize time-frequency information in ASR systems, which has used
the spatial immutability properties of methods such as TDNN, CNN and LSTM-RNN.
However, most of these models have large parameter volumes and are challenging
to train. In the structure we have designed, called Time-Frequency
Convolutional Maxout Neural Network (TFCMNN), two parallel blocks consisting of
1D-CMNN each have weight sharing in one dimension, are applied simultaneously
but independently to the feature vectors. Then their output is concatenated and
applied to a fully connected Maxout network for classification. To improve the
performance of this structure, we have used newly developed methods and models
such as the maxout, Dropout, and weight normalization. Two experimental sets
were designed and implemented on the Persian FARSDAT speech data set to
evaluate the performance of this model compared to conventional 1D-CMNN models.
According to the experimental results, the average recognition score of TFCMNN
models is about 1.6% higher than the average of conventional models. In
addition, the average training time of the TFCMNN models is about 17 hours
lower than the average training time of traditional models. As a result, as
mentioned in other references, time-frequency localization in ASR systems
increases system accuracy and speeds up the model training process.

    

### [[2108.03823] Towards to Robust and Generalized Medical Image Segmentation Framework](http://arxiv.org/abs/2108.03823)


  To mitigate the radiologist's workload, computer-aided diagnosis with the
capability to review and analyze medical images is gradually deployed. Deep
learning-based region of interest segmentation is among the most exciting use
cases. However, this paradigm is restricted in real-world clinical applications
due to poor robustness and generalization. The issue is more sinister with a
lack of training data. In this paper, we address the challenge from the
representation learning point of view. We investigate that the collapsed
representations, as one of the main reasons which caused poor robustness and
generalization, could be avoided through transfer learning. Therefore, we
propose a novel two-stage framework for robust generalized segmentation. In
particular, an unsupervised Tile-wise AutoEncoder (T-AE) pretraining
architecture is coined to learn meaningful representation for improving the
generalization and robustness of the downstream tasks. Furthermore, the learned
knowledge is transferred to the segmentation benchmark. Coupled with an image
reconstruction network, the representation keeps to be decoded, encouraging the
model to capture more semantic features. Experiments of lung segmentation on
multi chest X-ray datasets are conducted. Empirically, the related experimental
results demonstrate the superior generalization capability of the proposed
framework on unseen domains in terms of high performance and robustness to
corruption, especially under the scenario of the limited training data.

    

### [[2108.03861] Knowledge Graph Augmented Political Perspective Detection in News Media](http://arxiv.org/abs/2108.03861)


  Identifying political perspective in news media has become an important task
due to the rapid growth of political commentary and the increasingly polarized
ideologies. Previous approaches only focus on leveraging the semantic
information and leaves out the rich social and political context that helps
individuals understand political stances. In this paper, we propose a
perspective detection method that incorporates external knowledge of real-world
politics. Specifically, we construct a contemporary political knowledge graph
with 1,071 entities and 10,703 triples. We then build a heterogeneous
information network for each news document that jointly models article
semantics and external knowledge in knowledge graphs. Finally, we apply gated
relational graph convolutional networks and conduct political perspective
detection as graph-level classification. Extensive experiments show that our
method achieves the best performance and outperforms state-of-the-art methods
by 5.49\%. Numerous ablation studies further bear out the necessity of external
knowledge and the effectiveness of our graph-based approach.

    

### [[2108.03881] Encoding Heterogeneous Social and Political Context for Entity Stance Prediction](http://arxiv.org/abs/2108.03881)


  Political stance detection has become an important task due to the
increasingly polarized political ideologies. Most existing works focus on
identifying perspectives in news articles or social media posts, while social
entities, such as individuals and organizations, produce these texts and
actually take stances. In this paper, we propose the novel task of entity
stance prediction, which aims to predict entities' stances given their social
and political context. Specifically, we retrieve facts from Wikipedia about
social entities regarding contemporary U.S. politics. We then annotate social
entities' stances towards political ideologies with the help of domain experts.
After defining the task of entity stance prediction, we propose a graph-based
solution, which constructs a heterogeneous information network from collected
facts and adopts gated relational graph convolutional networks for
representation learning. Our model is then trained with a combination of
supervised, self-supervised and unsupervised loss functions, which are
motivated by multiple social and political phenomenons. We conduct extensive
experiments to compare our method with existing text and graph analysis
baselines. Our model achieves highest stance detection accuracy and yields
inspiring insights regarding social entity stances. We further conduct ablation
study and parameter analysis to study the mechanism and effectiveness of our
proposed approach.

    

### [[2108.03890] SPECT Angle Interpolation Based on Deep Learning Methodologies](http://arxiv.org/abs/2108.03890)


  A novel method for SPECT angle interpolation based on deep learning
methodologies is presented. Projection data from software phantoms were used to
train the proposed model. For evaluation of the efficacy of the method,
phantoms based on Shepp Logan, with various noise levels added were used, and
the resulting interpolated sinograms are reconstructed using Ordered Subset
Expectation Maximization (OSEM) and compared to the reconstructions of the
original sinograms. The proposed method can quadruple the projections, and
denoise the original sinogram, in the same process. As the results show, the
proposed model significantly improves the reconstruction accuracy. Finally, to
demonstrate the efficacy and capability of the proposed method results from
real-world DAT-SPECT sinograms are presented.

    

### [[2108.03897] Deep Convolutional Neural Network for Low Projection SPECT Imaging Reconstruction](http://arxiv.org/abs/2108.03897)


  In this paper, we present a novel method for tomographic image reconstruction
in SPECT imaging with a low number of projections. Deep convolutional neural
networks (CNN) are employed in the new reconstruction method. Projection data
from software phantoms were used to train the CNN network. For evaluation of
the efficacy of the proposed method, software phantoms and hardware phantoms
based on the FOV SPECT system were used. The resulting tomographic images are
compared to those produced by the "Maximum Likelihood Expectation Maximisation"
(MLEM).

    

### [[2108.03899] A Concise Function Representation for Faster Exact MPE and Constrained Optimisation in Graphical Models](http://arxiv.org/abs/2108.03899)


  We propose a novel concise function representation for graphical models, a
central theoretical framework that provides the basis for many reasoning tasks.
We then show how we exploit our concise representation based on deterministic
finite state automata within Bucket Elimination (BE), a general approach based
on the concept of variable elimination that accommodates many inference and
optimisation tasks such as most probable explanation and constrained
optimisation. We denote our version of BE as FABE. By using our concise
representation within FABE, we dramatically improve the performance of BE in
terms of runtime and memory requirements. Results on standard benchmarks
obtained using an established experimental methodology show that FABE often
outperforms the best available approach (RBFAOO), leading to significant
runtime improvements (up to 2 orders of magnitude in our tests).

    

### [[2108.03900] Multi-View TRGRU: Transformer based Spatiotemporal Model for Short-Term Metro Origin-Destination Matrix Prediction](http://arxiv.org/abs/2108.03900)


  Accurate prediction of short-term OD Matrix (i.e. the distribution of
passenger flows from various origins to destinations) is a crucial task in
metro systems. It is highly challenging due to the constantly changing nature
of many impacting factors and the real-time de- layed data collection problem.
Recently, some deep learning-based models have been proposed for OD Matrix
forecasting in ride- hailing and high way traffic scenarios. However, these
models can not sufficiently capture the complex spatiotemporal correlation
between stations in metro networks due to their different prior knowledge and
contextual settings. In this paper we propose a hy- brid framework Multi-view
TRGRU to address OD metro matrix prediction. In particular, it uses three
modules to model three flow change patterns: recent trend, daily trend, weekly
trend. In each module, a multi-view representation based on embedding for each
station is constructed and fed into a transformer based gated re- current
structure so as to capture the dynamic spatial dependency in OD flows of
different stations by a global self-attention mecha- nism. Extensive
experiments on three large-scale, real-world metro datasets demonstrate the
superiority of our Multi-view TRGRU over other competitors.

    

### [[2108.03903] Sinogram Denoise Based on Generative Adversarial Networks](http://arxiv.org/abs/2108.03903)


  A novel method for sinogram denoise based on Generative Adversarial Networks
(GANs) in the field of SPECT imaging is presented. Projection data from
software phantoms were used to train the proposed model. For evaluation of the
efficacy of the method Shepp Logan based phantom, with various noise levels
added where used. The resulting denoised sinograms are reconstructed using
Ordered Subset Expectation Maximization (OSEM) and compared to the
reconstructions of the original noised sinograms. As the results show, the
proposed method significantly denoise the sinograms and significantly improves
the reconstructions. Finally, to demonstrate the efficacy and capability of the
proposed method results from real-world DAT-SPECT sinograms are presented.

    

### [[2108.03906] "What makes my queries slow?": Subgroup Discovery for SQL Workload Analysis](http://arxiv.org/abs/2108.03906)


  Among daily tasks of database administrators (DBAs), the analysis of query
workloads to identify schema issues and improving performances is crucial.
Although DBAs can easily pinpoint queries repeatedly causing performance
issues, it remains challenging to automatically identify subsets of queries
that share some properties only (a pattern) and simultaneously foster some
target measures, such as execution time. Patterns are defined on combinations
of query clauses, environment variables, database alerts and metrics and help
answer questions like what makes SQL queries slow? What makes I/O
communications high? Automatically discovering these patterns in a huge search
space and providing them as hypotheses for helping to localize issues and
root-causes is important in the context of explainable AI. To tackle it, we
introduce an original approach rooted on Subgroup Discovery. We show how to
instantiate and develop this generic data-mining framework to identify
potential causes of SQL workloads issues. We believe that such data-mining
technique is not trivial to apply for DBAs. As such, we also provide a
visualization tool for interactive knowledge discovery. We analyse a one week
workload from hundreds of databases from our company, make both the dataset and
source code available, and experimentally show that insightful hypotheses can
be discovered.

    

### [[2108.03920] FA-GAN: Fused Attentive Generative Adversarial Networks for MRI Image Super-Resolution](http://arxiv.org/abs/2108.03920)


  High-resolution magnetic resonance images can provide fine-grained anatomical
information, but acquiring such data requires a long scanning time. In this
paper, a framework called the Fused Attentive Generative Adversarial
Networks(FA-GAN) is proposed to generate the super-resolution MR image from
low-resolution magnetic resonance images, which can reduce the scanning time
effectively but with high resolution MR images. In the framework of the FA-GAN,
the local fusion feature block, consisting of different three-pass networks by
using different convolution kernels, is proposed to extract image features at
different scales. And the global feature fusion module, including the channel
attention module, the self-attention module, and the fusion operation, is
designed to enhance the important features of the MR image. Moreover, the
spectral normalization process is introduced to make the discriminator network
stable. 40 sets of 3D magnetic resonance images (each set of images contains
256 slices) are used to train the network, and 10 sets of images are used to
test the proposed method. The experimental results show that the PSNR and SSIM
values of the super-resolution magnetic resonance image generated by the
proposed FA-GAN method are higher than the state-of-the-art reconstruction
methods.

    

### [[2108.03929] The State of AI Ethics Report (Volume 5)](http://arxiv.org/abs/2108.03929)


  This report from the Montreal AI Ethics Institute covers the most salient
progress in research and reporting over the second quarter of 2021 in the field
of AI ethics with a special emphasis on "Environment and AI", "Creativity and
AI", and "Geopolitics and AI." The report also features an exclusive piece
titled "Critical Race Quantum Computer" that applies ideas from quantum physics
to explain the complexities of human characteristics and how they can and
should shape our interactions with each other. The report also features special
contributions on the subject of pedagogy in AI ethics, sociology and AI ethics,
and organizational challenges to implementing AI ethics in practice. Given
MAIEI's mission to highlight scholars from around the world working on AI
ethics issues, the report also features two spotlights sharing the work of
scholars operating in Singapore and Mexico helping to shape policy measures as
they relate to the responsible use of technology. The report also has an
extensive section covering the gamut of issues when it comes to the societal
impacts of AI covering areas of bias, privacy, transparency, accountability,
fairness, interpretability, disinformation, policymaking, law, regulations, and
moral philosophy.

    

### [[2108.03938] On the Transferability of Neural Models of Morphological Analogies](http://arxiv.org/abs/2108.03938)


  Analogical proportions are statements expressed in the form "A is to B as C
is to D" and are used for several reasoning and classification tasks in
artificial intelligence and natural language processing (NLP). In this paper,
we focus on morphological tasks and we propose a deep learning approach to
detect morphological analogies. We present an empirical study to see how our
framework transfers across languages, and that highlights interesting
similarities and differences between these languages. In view of these results,
we also discuss the possibility of building a multilingual morphological model.

    

### [[2108.03941] Deep Learning Based Antenna-time Domain Channel Extrapolation for Hybrid mmWave Massive MIMO](http://arxiv.org/abs/2108.03941)


  In a time-varying massive multiple-input multipleoutput (MIMO) system, the
acquisition of the downlink channel state information at the base station (BS)
is a very challenging task due to the prohibitively high overheads associated
with downlink training and uplink feedback. In this paper, we consider the
hybrid precoding structure at BS and examine the antennatime domain channel
extrapolation. We design a latent ordinary differential equation (ODE)-based
network under the variational auto-encoder (VAE) framework to learn the mapping
function from the partial uplink channels to the full downlink ones at the BS
side. Specifically, the gated recurrent unit is adopted for the encoder and the
fully-connected neural network is used for the decoder. The end-to-end learning
is utilized to optimize the network parameters. Simulation results show that
the designed network can efficiently infer the full downlink channels from the
partial uplink ones, which can significantly reduce the channel training
overhead.

    

### [[2108.03988] Knowledge accumulating: The general pattern of learning](http://arxiv.org/abs/2108.03988)


  Artificial Intelligence has been developed for decades with the achievement
of great progress. Recently, deep learning shows its ability to solve many real
world problems, e.g. image classification and detection, natural language
processing, playing GO. Theoretically speaking, an artificial neural network
can fit any function and reinforcement learning can learn from any delayed
reward. But in solving real world tasks, we still need to spend a lot of effort
to adjust algorithms to fit task unique features. This paper proposes that the
reason of this phenomenon is the sparse feedback feature of the nature, and a
single algorithm, no matter how we improve it, can only solve dense feedback
tasks or specific sparse feedback tasks. This paper first analyses how sparse
feedback affects algorithm perfomance, and then proposes a pattern that
explains how to accumulate knowledge to solve sparse feedback problems.

    

### [[2108.03989] Spatial-Temporal Deep Intention Destination Networks for Online Travel Planning](http://arxiv.org/abs/2108.03989)


  Nowadays, artificial neural networks are widely used for users' online travel
planning. Personalized travel planning has many real applications and is
affected by various factors, such as transportation type, intention destination
estimation, budget limit and crowdness prediction. Among those factors, users'
intention destination prediction is an essential task in online travel
platforms. The reason is that, the user may be interested in the travel plan
only when the plan matches his real intention destination. Therefore, in this
paper, we focus on predicting users' intention destinations in online travel
platforms. In detail, we act as online travel platforms (such as Fliggy and
Airbnb) to recommend travel plans for users, and the plan consists of various
vacation items including hotel package, scenic packages and so on. Predicting
the actual intention destination in travel planning is challenging. Firstly,
users' intention destination is highly related to their travel status (e.g.,
planning for a trip or finishing a trip). Secondly, users' actions (e.g.
clicking, searching) over different product types (e.g. train tickets, visa
application) have different indications in destination prediction. Thirdly,
users may mostly visit the travel platforms just before public holidays, and
thus user behaviors in online travel platforms are more sparse, low-frequency
and long-period. Therefore, we propose a Deep Multi-Sequences fused neural
Networks (DMSN) to predict intention destinations from fused multi-behavior
sequences. Real datasets are used to evaluate the performance of our proposed
DMSN models. Experimental results indicate that the proposed DMSN models can
achieve high intention destination prediction accuracy.

    

### [[2108.04001] Development of Human Motion Prediction Strategy using Inception Residual Block](http://arxiv.org/abs/2108.04001)


  Human Motion Prediction is a crucial task in computer vision and robotics. It
has versatile application potentials such as in the area of human-robot
interactions, human action tracking for airport security systems, autonomous
car navigation, computer gaming to name a few. However, predicting human motion
based on past actions is an extremely challenging task due to the difficulties
in detecting spatial and temporal features correctly. To detect temporal
features in human poses, we propose an Inception Residual Block(IRB), due to
its inherent capability of processing multiple kernels to capture salient
features. Here, we propose to use multiple 1-D Convolution Neural Network (CNN)
with different kernel sizes and input sequence lengths and concatenate them to
get proper embedding. As kernels strides over different receptive fields, they
detect smaller and bigger salient features at multiple temporal scales. Our
main contribution is to propose a residual connection between input and the
output of the inception block to have a continuity between the previously
observed pose and the next predicted pose. With this proposed architecture, it
learns prior knowledge much better about human poses and we achieve much higher
prediction accuracy as detailed in the paper. Subsequently, we further propose
to feed the output of the inception residual block as an input to the Graph
Convolution Neural Network (GCN) due to its better spatial feature learning
capability. We perform a parametric analysis for better designing of our model
and subsequently, we evaluate our approach on the Human 3.6M dataset and
compare our short-term as well as long-term predictions with the state of the
art papers, where our model outperforms most of the pose results, the detailed
reasons of which have been elaborated in the paper.

    

### [[2108.04020] FOLASP: FO(.) as Input Language for Answer Ser Solvers](http://arxiv.org/abs/2108.04020)


  Over the past decades, Answer Set Programming (ASP) has emerged as an
important paradigm for declarative problem solving. Technological progress in
this area has been stimulated by the use of common standards, such as the
ASP-Core-2 language. While ASP has its roots in non-monotonic reasoning,
efforts have also been made to reconcile ASP with classical first-order logic
(FO). This has resulted in the development of FO(.), an expressive extension of
FO, which allows ASP-like problem solving in a purely classical setting. This
language may be more accessible to domain experts already familiar with FO, and
may be easier to combine with other formalisms that are based on classical
logic. It is supported by the IDP inference system, which has successfully
competed in a number of ASP competitions. Here, however, technological progress
has been hampered by the limited number of systems that are available for
FO(.). In this paper, we aim to address this gap by means of a translation tool
that transforms an FO(.) specification into ASP-Core-2, thereby allowing
ASP-Core-2 solvers to be used as solvers for FO(.) as well. We present
experimental results to show that the resulting combination of our translation
with an off-the-shelf ASP solver is competitive with the IDP system as a way of
solving problems formulated in FO(.).
Under consideration for acceptance in TPLP.

    

### [[2010.08719] Cascaded Refinement Network for Point Cloud Completion with Self-supervision](http://arxiv.org/abs/2010.08719)


  Point clouds are often sparse and incomplete, which imposes difficulties for
real-world applications. Existing shape completion methods tend to generate
rough shapes without fine-grained details. Considering this, we introduce a
two-branch network for shape completion. The first branch is a cascaded shape
completion sub-network to synthesize complete objects, where we propose to use
the partial input together with the coarse output to preserve the object
details during the dense point reconstruction. The second branch is an
auto-encoder to reconstruct the original partial input. The two branches share
a same feature extractor to learn an accurate global feature for shape
completion. Furthermore, we propose two strategies to enable the training of
our network when ground truth data are not available. This is to mitigate the
dependence of existing approaches on large amounts of ground truth training
data that are often difficult to obtain in real-world applications.
Additionally, our proposed strategies are also able to improve the
reconstruction quality for fully supervised learning. We verify our approach in
self-supervised, semi-supervised and fully supervised settings with superior
performances. Quantitative and qualitative results on different datasets
demonstrate that our method achieves more realistic outputs than
state-of-the-art approaches on the point cloud completion task.

    

### [[2101.00294] Rider: Reader-Guided Passage Reranking for Open-Domain Question Answering](http://arxiv.org/abs/2101.00294)


  Current open-domain question answering systems often follow a
Retriever-Reader architecture, where the retriever first retrieves relevant
passages and the reader then reads the retrieved passages to form an answer. In
this paper, we propose a simple and effective passage reranking method, named
Reader-guIDEd Reranker (RIDER), which does not involve training and reranks the
retrieved passages solely based on the top predictions of the reader before
reranking. We show that RIDER, despite its simplicity, achieves 10 to 20
absolute gains in top-1 retrieval accuracy and 1 to 4 Exact Match (EM) gains
without refining the retriever or reader. In addition, RIDER, without any
training, outperforms state-of-the-art transformer-based supervised rerankers.
Remarkably, RIDER achieves 48.3 EM on the Natural Questions dataset and 66.4 EM
on the TriviaQA dataset when only 1,024 tokens (7.8 passages on average) are
used as the reader input after passage reranking.

    

### [[2104.07750] Joint Attention for Multi-Agent Coordination and Social Learning](http://arxiv.org/abs/2104.07750)


  Joint attention - the ability to purposefully coordinate attention with
another agent, and mutually attend to the same thing -- is a critical component
of human social cognition. In this paper, we ask whether joint attention can be
useful as a mechanism for improving multi-agent coordination and social
learning. We first develop deep reinforcement learning (RL) agents with a
recurrent visual attention architecture. We then train agents to minimize the
difference between the attention weights that they apply to the environment at
each timestep, and the attention of other agents. Our results show that this
joint attention incentive improves agents' ability to solve difficult
coordination tasks, by reducing the exponential cost of exploring the joint
multi-agent action space. Joint attention leads to higher performance than a
competitive centralized critic baseline across multiple environments. Further,
we show that joint attention enhances agents' ability to learn from experts
present in their environment, even when completing hard exploration tasks that
do not require coordination. Taken together, these findings suggest that joint
attention may be a useful inductive bias for multi-agent learning.

    

### [[2105.14677] Characterization of Generalizability of Spike Timing Dependent Plasticity trained Spiking Neural Networks](http://arxiv.org/abs/2105.14677)


  A Spiking Neural Network (SNN) is trained with Spike Timing Dependent
Plasticity (STDP), which is a neuro-inspired unsupervised learning method for
various machine learning applications. This paper studies the generalizability
properties of the STDP learning processes using the Hausdorff dimension of the
trajectories of the learning algorithm. The paper analyzes the effects of STDP
learning models and associated hyper-parameters on the generalizability
properties of an SNN. The analysis is used to develop a Bayesian optimization
approach to optimize the hyper-parameters for an STDP model for improving the
generalizability properties of an SNN.

    

### [[2002.03744] A joint precoding framework for wideband reconfigurable intelligent surface-aided cell-free network](http://arxiv.org/abs/2002.03744)


  Thanks to the strong ability against the inter-cell interference, cell-free
network is considered as a promising technique to improve network capacity.
However, further capacity improvement requires to deploy more base stations
(BSs) with high cost and power consumption. To address this issue, inspired by
the recently developed reconfigurable intelligent surface (RIS) technique, we
propose the concept of RIS-aided cell-free network to improve the capacity with
low cost and power consumption. The key idea is to replace some of the required
BSs by low-cost and energy-efficient RISs. Then, in a wideband RIS-aided
cell-free network, we formulate the problem of joint precoding design at BSs
and RISs to maximize the network capacity. Due to the non-convexity and high
complexity of the formulated problem, we develop an alternating optimization
framework to solve this challenging problem. In particular, we decouple this
problem via fractional programming, and solve the subproblems alternatively.
Note that most of the scenarios considered in existing works are special cases
of the general scenario studied in this paper, and the proposed joint precoding
framework can serve as a general solution to maximize the capacity in most
existing RIS-aided scenarios. Finally, simulation results demonstrate that,
compared with the conventional cell-free network, the network capacity under
the proposed scheme can be improved significantly.

    

### [[2108.03402] Arduino Controlled Spy Robo Car using Wireless Camera With live Streaming](http://arxiv.org/abs/2108.03402)


  Nowadays, the technological and digital world is developing very fast.
Everything is getting smart, so we are talking about the technological world
the devices like home appliances and other things are getting control by mobile
applications, and this only happens by the device Arduino Uno / raspberry pi3
and many others. Still, in our research we have used Arduino Uno to create a
Wi-Fi controlled car with camera-top on it to monitor everything in its
surrounding, we have seen many similar projects which using Arduino to makes
things easy to use and its saving time and energy too. Automation is used for
operating an electronic device such as the remote control car, home lighting
system, and other useful things or reduced human invention. This report
proposes a design and implementation of a remote-controlled camera car by Wi-Fi
technology mobile devices. In this analysis work, radio code and hardware
technologies area unit used, like the wireless module of ESP8266 for
(transmitter and receiver), Arduino Uno as microcontroller, associate H-bridge
L293D IC for motor controller and 2 electrical DC motors are used to move the
car, & a Camera attached on the top of the vehicle.

    

### [[2108.03602] Planning for an Efficient Implementation of Hypothetical Bousi~Prolog](http://arxiv.org/abs/2108.03602)


  This paper explores the integration of hypothetical reasoning into an
efficient implementation of the fuzzy logic language Bousi~Prolog. To this end,
we first analyse what would be expected from a logic inference system, equipped
with what is called embedded implication, to model solving goals with respect
to assumptions. We start with a propositional system and incrementally build
more complex systems and implementations to satisfy the requirements imposed by
a system like Bousi~Prolog. Finally, we propose an inference system,
operational semantics, and the translation function to generate efficient
Prolog programs from Bousi~Prolog programs.

    

### [[2009.01686] Quingo: A Programming Framework for Heterogeneous Quantum-Classical Computing with NISQ Features](http://arxiv.org/abs/2009.01686)


  The increasing control complexity of Noisy Intermediate-Scale Quantum (NISQ)
systems underlines the necessity of integrating quantum hardware with quantum
software. While mapping heterogeneous quantum-classical computing (HQCC)
algorithms to NISQ hardware for execution, we observed a few dissatisfactions
in quantum programming languages (QPLs), including difficult mapping to
hardware, limited expressiveness, and counter-intuitive code. Also, noisy
qubits require repeatedly performed quantum experiments, which explicitly
operate low-level configurations, such as pulses and timing of operations. This
requirement is beyond the scope or capability of most existing QPLs.
We summarize three execution models to depict the quantum-classical
interaction of existing QPLs. Based on the refined HQCC model, we propose the
Quingo framework to integrate and manage quantum-classical software and
hardware to provide the programmability over HQCC applications and map them to
NISQ hardware. We propose a six-phase quantum program life-cycle model matching
the refined HQCC model, which is implemented by a runtime system. We also
propose the Quingo programming language, an external domain-specific language
highlighting timer-based timing control and opaque operation definition, which
can be used to describe quantum experiments. We believe the Quingo framework
could contribute to the clarification of key techniques in the design of future
HQCC systems.

    