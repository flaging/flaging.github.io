
## 2021-7-6

### [Daily Hacker News for 2021-07-05](https://www.daemonology.net/hn-daily/2021-07-05.html)

### [关于北京地铁，只通过 Apple Pay 就有很多方式过闸 - V2EX](https://www.v2ex.com/t/787525)

### [“在线办公”再添新贵，葡萄城表格技术助力网易灵犀 “一点通” - SegmentFault 思否](https://segmentfault.com/a/1190000040293653)

### [派早报：Wi-Fi 漏洞可彻底禁用 iPhone Wi-Fi 功能、三星或在八月举办发布会等 - 少数派](https://sspai.com/post/67567)

### [为什么有些开源项目选择在 gitee 开源而不是 github？ - V2EX](https://www.v2ex.com/t/787745)

### [Can you subdivide slurm allocation blocks in a dynamic way?` : HPC](https://www.reddit.com/r/HPC/comments/oeminp/can_you_subdivide_slurm_allocation_blocks_in_a/)

### [SolidJS硬气的说：我比React还react - SegmentFault 思否](https://segmentfault.com/a/1190000040275257)

### [Name of XGBoost parameters in Java - XGBoost](https://discuss.xgboost.ai/t/name-of-xgboost-parameters-in-java/2153/2)

### [Found input variables with inconsistent numbers of samples - XGBoost](https://discuss.xgboost.ai/t/found-input-variables-with-inconsistent-numbers-of-samples/1818/3)

### [Is training multiple models in parallel threads supported (Python)? - RFC - XGBoost](https://discuss.xgboost.ai/t/is-training-multiple-models-in-parallel-threads-supported-python/2348/1)

### [Qudro 5000 graphic card proplems - XGBoost](https://discuss.xgboost.ai/t/qudro-5000-graphic-card-proplems/2346/1)

### [Performance between spark-xgb and python-xgb - XGBoost](https://discuss.xgboost.ai/t/performance-between-spark-xgb-and-python-xgb/2344/1)

### [Random data weighting - XGBoost](https://discuss.xgboost.ai/t/random-data-weighting/2342/1)

### [Nested cross validation - XGBoost](https://discuss.xgboost.ai/t/nested-cross-validation/2340/1)

### [Invalid Parameter format for seed expect int but value='RandomState(MT19937)' - XGBoost](https://discuss.xgboost.ai/t/invalid-parameter-format-for-seed-expect-int-but-value-randomstate-mt19937/2338/1)

### [Min_child_weight range - XGBoost](https://discuss.xgboost.ai/t/min-child-weight-range/2335/1)

### [Sample_weight behaviour - XGBoost](https://discuss.xgboost.ai/t/sample-weight-behaviour/2334/1)

### [XGBRanker predictions are negative - XGBoost](https://discuss.xgboost.ai/t/xgbranker-predictions-are-negative/2329/2)

### [When modeling time series data how do we give more weight to more recent data? - XGBoost](https://discuss.xgboost.ai/t/when-modeling-time-series-data-how-do-we-give-more-weight-to-more-recent-data/1462/2)

### [Why is subsample >= 0.5 recommended? - XGBoost](https://discuss.xgboost.ai/t/why-is-subsample-0-5-recommended/2332/1)

### [XGBRanker predictions are negative - XGBoost](https://discuss.xgboost.ai/t/xgbranker-predictions-are-negative/2329/1)

### [XGBoost: How to add custom sampler (re: subsample)? - XGBoost](https://discuss.xgboost.ai/t/xgboost-how-to-add-custom-sampler-re-subsample/2326/4)

### [XGBoost: How to add custom sampler (re: subsample)? - XGBoost](https://discuss.xgboost.ai/t/xgboost-how-to-add-custom-sampler-re-subsample/2326/3)

### [XGBoost: How to add custom sampler (re: subsample)? - XGBoost](https://discuss.xgboost.ai/t/xgboost-how-to-add-custom-sampler-re-subsample/2326/2)

### [XGBoost: How to add custom sampler (re: subsample)? - XGBoost](https://discuss.xgboost.ai/t/xgboost-how-to-add-custom-sampler-re-subsample/2326/1)

### [How does XGBoost read file content inside the enclave? - XGBoost](https://discuss.xgboost.ai/t/how-does-xgboost-read-file-content-inside-the-enclave/2323/3)

### [Iteration_range = (0,0) - XGBoost](https://discuss.xgboost.ai/t/iteration-range-0-0/2321/2)

### [How does XGBoost read file content inside the enclave? - XGBoost](https://discuss.xgboost.ai/t/how-does-xgboost-read-file-content-inside-the-enclave/2323/2)

### [How does XGBoost read file content inside the enclave? - XGBoost](https://discuss.xgboost.ai/t/how-does-xgboost-read-file-content-inside-the-enclave/2323/1)

### [Iteration_range = (0,0) - XGBoost](https://discuss.xgboost.ai/t/iteration-range-0-0/2321/1)

### [Learning Rate and number of rounds - XGBoost](https://discuss.xgboost.ai/t/learning-rate-and-number-of-rounds/2320/1)

### [XGBRegressor, GammaRegression:label must be positive - XGBoost](https://discuss.xgboost.ai/t/xgbregressor-gammaregression-label-must-be-positive/2318/2)

### [XGBRegressor, GammaRegression:label must be positive - XGBoost](https://discuss.xgboost.ai/t/xgbregressor-gammaregression-label-must-be-positive/2318/1)

### [PSS/USS 和 RSS 其实是一回事，吗？ - Changkun's Blog](https://blog.changkun.de/posts/pss-uss-rss/)

### [缺页与预取带来的性能差异 - Changkun's Blog](https://blog.changkun.de/posts/page-fault-vs-prefetch/)

### [2020 年终总结 - Changkun's Blog](https://blog.changkun.de/posts/2020-summary/)

### [Migration with Zero Downtime - Changkun's Blog](https://blog.changkun.de/posts/zero-downtime-migration/)

### [2020 读书清单 - Changkun's Blog](https://blog.changkun.de/posts/2020-reading/)

### [The All in Go Stack - Changkun's Blog](https://blog.changkun.de/posts/all-in-go/)

### [Pointers Might Not Be Ideal for Parameters - Changkun's Blog](https://blog.changkun.de/posts/pointers-might-not-be-ideal-for-parameters/)

### [Eliminating A Source of Measurement Errors in Benchmarks - Changkun's Blog](https://blog.changkun.de/posts/eliminating-a-source-of-measurement-errors-in-benchmarks/)

### [Setup Wordpress in 10 Minutes - Changkun's Blog](https://blog.changkun.de/posts/setup-wordpress-in-10-minutes/)

### [我为什么不再写博客了？ - Changkun's Blog](https://blog.changkun.de/posts/why-i-stopped-blogging/)

### [2019 年终总结 - Changkun's Blog](https://blog.changkun.de/posts/2019-summary/)

### [2018-2019 读书清单 - Changkun's Blog](https://blog.changkun.de/posts/2018-2019-reading/)

### [Ten years of blogging - Changkun's Blog](https://blog.changkun.de/posts/ten-years-of-blogging/)

### [Rethinking the Reflections on Communications and Trusts - Changkun's Blog](https://blog.changkun.de/posts/rethinking-the-reflections-on-communications-and-trusts/)

### [2018 年终总结 - Changkun's Blog](https://blog.changkun.de/posts/2018-summary/)

### [Go source code study is open source - Changkun's Blog](https://blog.changkun.de/posts/go-source-code-study-is-open-source/)

### [Go source study: unsafe Pattern - Changkun's Blog](https://blog.changkun.de/posts/go-source-unsafe-pattern/)

### [Go source study: sync.Pool - Changkun's Blog](https://blog.changkun.de/posts/go-source-sync-pool/)

### [Go runtime programming - Changkun's Blog](https://blog.changkun.de/posts/go-runtime-programming/)

### [Guacamole Source Analysis - Changkun's Blog](https://blog.changkun.de/_todo/2018-08-14-guacamole-source-analysis/)

### [A Million WebSocket and Go - Changkun's Blog](https://blog.changkun.de/posts/a-million-websocket-and-go/)

### [Designing Asynchronous RESTful APIs - Changkun's Blog](https://blog.changkun.de/posts/designing-asynchronous-restful-apis/)

### [分布式杂谈01：CAP 理论的误解 - Changkun's Blog](https://blog.changkun.de/posts/a-common-misunderstanding-of-the-cap-theory/)

### [Strong man is strongest alone - Changkun's Blog](https://blog.changkun.de/_todo/2018-05-26-strong-man-is-strongest-alone/)

### [流形学习与对抗网络 - Changkun's Blog](https://blog.changkun.de/_todo/2018-05-23-%E6%B5%81%E5%BD%A2%E5%AD%A6%E4%B9%A0%E4%B8%8E%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/)

### [Issues of Human-Bot Interaction - Changkun's Blog](https://blog.changkun.de/posts/issues-of-human-bot-interaction/)

### [UMSLT05: The Consistency of Learning Process - Changkun's Blog](https://blog.changkun.de/_todo/2018-03-27-umslt05/)

### [论文笔记：Generalization in Machine Learning via Analytical Learning Theory - Changkun's Blog](https://blog.changkun.de/_todo/2018-03-26-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0generalization-in-machine-learning-via-analytical-learning-theory/)

### [压缩法与深度网络的泛化性 - Changkun's Blog](https://blog.changkun.de/posts/compression-and-dnn-generalization/)

### [Go Web in 1 Hour - Changkun's Blog](https://blog.changkun.de/_todo/2018-03-21-go-web-in-1-hour/)

### [Go in 1 Hour - Changkun's Blog](https://blog.changkun.de/posts/go-in-1-hour/)

### [UMSLT04: The Past and Present of SGD - Changkun's Blog](https://blog.changkun.de/posts/the-past-and-present-of-sgd/)

### [UMSLT03: A Gentle Start of Learning Theory - Changkun's Blog](https://blog.changkun.de/posts/a-gentle-start-of-learning-theory/)

### [UMSLT02: A Breif History of Neural Networks - Changkun's Blog](https://blog.changkun.de/posts/a-breif-history-of-neural-networks/)

### [UMSLT01: A Breif History of Regularization - Changkun's Blog](https://blog.changkun.de/posts/a-breif-history-of-regularization/)

### [不笑不足以为道 - Changkun's Blog](https://blog.changkun.de/posts/%E4%B8%8D%E7%AC%91%E4%B8%8D%E8%B6%B3%E4%BB%A5%E4%B8%BA%E9%81%93/)

### [统计学习理论综述 - Changkun's Blog](https://blog.changkun.de/_todo/2018-01-30-%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA%E7%BB%BC%E8%BF%B0/)

### [论文笔记：Generalization in Deep Learning - Changkun's Blog](https://blog.changkun.de/posts/generalization-in-deep-learning/)

### [2017 年终总结 - Changkun's Blog](https://blog.changkun.de/posts/2017-summary/)

### [2017 读书清单 - Changkun's Blog](https://blog.changkun.de/posts/2017-reading/)

### [深度学习的泛化理论简介 - Changkun's Blog](https://blog.changkun.de/posts/an-intro-to-generalization-theory/)

### [删除 GitHub 上已经提交的敏感信息 - Changkun's Blog](https://blog.changkun.de/posts/delete-pushed-info-from-github/)

### [CNN Literature Review - Changkun's Blog](https://blog.changkun.de/_todo/2017-10-24-cnn-literature-review/)

### [基于移动交互行为的用户情感推断 - Changkun's Blog](https://blog.changkun.de/_todo/2017-10-24-%E5%9F%BA%E4%BA%8E%E7%A7%BB%E5%8A%A8%E4%BA%A4%E4%BA%92%E8%A1%8C%E4%B8%BA%E7%9A%84%E7%94%A8%E6%88%B7%E6%83%85%E6%84%9F%E6%8E%A8%E6%96%AD/)

### [金融学七大理论之『有效市场假说』 - Changkun's Blog](https://blog.changkun.de/_todo/2017-09-23-%E9%87%91%E8%9E%8D%E5%AD%A6%E4%B8%83%E5%A4%A7%E7%90%86%E8%AE%BA%E4%B9%8B%E6%9C%89%E6%95%88%E5%B8%82%E5%9C%BA%E5%81%87%E8%AF%B4/)

### [硕士生涯的第一年就这样告一段落了 - Changkun's Blog](https://blog.changkun.de/posts/first-year-of-master-studies/)

### [Critique notes on HCI research - Changkun's Blog](https://blog.changkun.de/_todo/2017-08-25-critique-notes-on-hci-research/)

### [人肉计算(10): 系统参与激励 - Changkun's Blog](https://blog.changkun.de/posts/human-computation-10/)

### [人肉计算(9): 陷阱的解法 - Changkun's Blog](https://blog.changkun.de/posts/human-computation-9/)

### [别聊，一聊你就暴露 - Changkun's Blog](https://blog.changkun.de/posts/do-not-talk/)

### [人肉计算(8): 人肉计算与数据科学中的陷阱 - Changkun's Blog](https://blog.changkun.de/posts/human-computation-8/)

### [人肉计算(7): 社会行为分析 - Changkun's Blog](https://blog.changkun.de/posts/human-computation-7/)

### [Hexo + GitHub + Travis CI + VPS 自动部署 - Changkun's Blog](https://blog.changkun.de/posts/hexo-github-travis-ci-vps-ci-cd/)

### [人肉计算(6): 预测市场 - Changkun's Blog](https://blog.changkun.de/posts/human-computation-6/)

### [人肉计算(5): 信用风险评级模型 - Changkun's Blog](https://blog.changkun.de/posts/human-computation-5/)

### [读书与回报 - Changkun's Blog](https://blog.changkun.de/posts/return-of-studies/)

### [瞎扯: 对现代企业理论与当下IT企业的商业模式和信息产业链的规律性的思考 - Changkun's Blog](https://blog.changkun.de/posts/%E5%AF%B9%E7%8E%B0%E4%BB%A3%E4%BC%81%E4%B8%9A%E7%90%86%E8%AE%BA%E4%B8%8E%E5%BD%93%E4%B8%8Bit%E4%BC%81%E4%B8%9A%E7%9A%84%E5%95%86%E4%B8%9A%E6%A8%A1%E5%BC%8F%E5%92%8C%E4%BF%A1%E6%81%AF%E4%BA%A7%E4%B8%9A%E9%93%BE%E7%9A%84%E8%A7%84%E5%BE%8B%E6%80%A7%E7%9A%84%E6%80%9D%E8%80%83/)

### [人肉计算(4): 输入数据聚合与PageRank - Changkun's Blog](https://blog.changkun.de/posts/human-computation-4/)

### [又一次打整了一下博客 - Changkun's Blog](https://blog.changkun.de/posts/%E5%8F%88%E4%B8%80%E6%AC%A1%E6%89%93%E6%95%B4%E4%BA%86%E4%B8%80%E4%B8%8B%E5%8D%9A%E5%AE%A2/)

### [人肉计算(3): 输入数据聚合与链路预测 - Changkun's Blog](https://blog.changkun.de/posts/human-computation-3/)

### [人肉计算(2): 意图博弈 GWAPs - Changkun's Blog](https://blog.changkun.de/posts/human-computation-2/)

### [人肉计算(1): 众包与群众智慧 - Changkun's Blog](https://blog.changkun.de/posts/human-computation-1/)

### [对后辈同学在计算机专业上的答疑与解惑 - Changkun's Blog](https://blog.changkun.de/posts/%E5%AF%B9%E5%90%8E%E8%BE%88%E5%90%8C%E5%AD%A6%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%93%E4%B8%9A%E4%B8%8A%E7%9A%84%E7%AD%94%E7%96%91%E4%B8%8E%E8%A7%A3%E6%83%91/)

### [在德国的医疗及住院体验 - Changkun's Blog](https://blog.changkun.de/posts/%E5%9C%A8%E5%BE%B7%E5%9B%BD%E7%9A%84%E5%8C%BB%E7%96%97%E5%8F%8A%E4%BD%8F%E9%99%A2%E4%BD%93%E9%AA%8C/)

### [这可能不是一个技术博客了 - Changkun's Blog](https://blog.changkun.de/posts/%E8%BF%99%E5%8F%AF%E8%83%BD%E4%B8%8D%E6%98%AF%E4%B8%80%E4%B8%AA%E6%8A%80%E6%9C%AF%E5%8D%9A%E5%AE%A2%E4%BA%86/)

### [实验楼楼赛第3期-Python-题解 - Changkun's Blog](https://blog.changkun.de/posts/%E5%AE%9E%E9%AA%8C%E6%A5%BC%E6%A5%BC%E8%B5%9B%E7%AC%AC3%E6%9C%9F-python-%E9%A2%98%E8%A7%A3/)

### [迅速更换了 DISQUS - Changkun's Blog](https://blog.changkun.de/posts/migrate-to-disqus/)

### [Electron 深度实践总结 - Changkun's Blog](https://blog.changkun.de/posts/electron-summary/)

### [良好的编码体验的三个方面 - Changkun's Blog](https://blog.changkun.de/posts/%E8%89%AF%E5%A5%BD%E7%9A%84%E7%BC%96%E7%A0%81%E4%BD%93%E9%AA%8C%E7%9A%84%E4%B8%89%E4%B8%AA%E6%96%B9%E9%9D%A2/)

### [2016 年终总结 - Changkun's Blog](https://blog.changkun.de/posts/2016-summary/)

### [2016 读书清单 - Changkun's Blog](https://blog.changkun.de/posts/2016-reading/)

### [最近在着手写的文章 - Changkun's Blog](https://blog.changkun.de/posts/%E6%9C%80%E8%BF%91%E5%9C%A8%E7%9D%80%E6%89%8B%E5%86%99%E7%9A%84%E6%96%87%E7%AB%A0/)

### [最近两周经历的两次 Hackathon - Changkun's Blog](https://blog.changkun.de/_todo/%E6%9C%80%E8%BF%91%E7%BB%8F%E5%8E%86%E7%9A%84%E4%B8%A4%E6%AC%A1-hackathon/)

### [微信小程序文档极致总结 - Changkun's Blog](https://blog.changkun.de/posts/%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F%E6%96%87%E6%A1%A3%E6%9E%81%E8%87%B4%E6%80%BB%E7%BB%93/)

### [谈谈过去三个月在实验楼的实习经历 - Changkun's Blog](https://blog.changkun.de/posts/%E8%B0%88%E8%B0%88%E8%BF%87%E5%8E%BB%E4%B8%89%E4%B8%AA%E6%9C%88%E5%9C%A8%E5%AE%9E%E9%AA%8C%E6%A5%BC%E7%9A%84%E5%AE%9E%E4%B9%A0%E7%BB%8F%E5%8E%86/)

### [给博客做了一个桌面客户端… - Changkun's Blog](https://blog.changkun.de/posts/%E7%BB%99%E5%8D%9A%E5%AE%A2%E5%81%9A%E4%BA%86%E4%B8%80%E4%B8%AA%E6%A1%8C%E9%9D%A2%E5%AE%A2%E6%88%B7%E7%AB%AF/)

### [Guacamole 源码分析与 VNC 中 RFB 协议的坑 - Changkun's Blog](https://blog.changkun.de/posts/guacamole-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B8%8E-vnc-%E4%B8%AD-rfb-%E5%8D%8F%E8%AE%AE%E7%9A%84%E5%9D%91/)

### [《高速上手 C++11/14》正式发布 - Changkun's Blog](https://blog.changkun.de/posts/modern-cpp-v1-is-live/)

### [Docker 极速入门教程02 - 镜像与容器管理 - Changkun's Blog](https://blog.changkun.de/posts/docker-tutorial-2/)

### [Docker 极速入门教程01 - 基本概念和操作 - Changkun's Blog](https://blog.changkun.de/posts/docker-tutorial-1/)

### [阶段性沉默 - Changkun's Blog](https://blog.changkun.de/posts/%E9%98%B6%E6%AE%B5%E6%80%A7%E6%B2%89%E9%BB%98/)

### [ELK+Redis 最佳实践 - Changkun's Blog](https://blog.changkun.de/posts/elk-redis-best-practice/)

### [终于全面启用了 HTTPS - Changkun's Blog](https://blog.changkun.de/posts/all-in-https/)

### [苹果开源了LZFSE无损压缩 - Changkun's Blog](https://blog.changkun.de/posts/apple-released-lzfse/)

### [Specialization - Changkun's Blog](https://blog.changkun.de/specialization/)

### [Hash 碰撞的一种思路 - Changkun's Blog](https://blog.changkun.de/posts/hash-%E7%A2%B0%E6%92%9E%E7%9A%84%E4%B8%80%E7%A7%8D%E6%80%9D%E8%B7%AF/)

### [记一次完整的 Kaldi-TIMIT 示例运行 - Changkun's Blog](https://blog.changkun.de/posts/kaldi-timit-example2/)

### [Kaldi 上的 TIMIT 例子 - Changkun's Blog](https://blog.changkun.de/posts/kaldi-timit-example/)

### [Kaldi 安装与部署 - Changkun's Blog](https://blog.changkun.de/posts/install-kaldi/)

### [Bachelor Thesis - Changkun's Blog](https://blog.changkun.de/bachelorthesis/)

### [当我们进行学术演讲时，我们应该谈论什么 - Changkun's Blog](https://blog.changkun.de/_todo/%E5%BD%93%E6%88%91%E4%BB%AC%E5%9C%A8%E8%BF%9B%E8%A1%8C%E5%AD%A6%E6%9C%AF%E6%BC%94%E8%AE%B2%E6%97%B6%E6%88%91%E4%BB%AC%E5%BA%94%E8%AF%A5%E8%B0%88%E8%AE%BA%E4%BB%80%E4%B9%88/)

### [从科研写作谈起 - Changkun's Blog](https://blog.changkun.de/posts/%E4%BB%8E%E7%A7%91%E7%A0%94%E5%86%99%E4%BD%9C%E8%B0%88%E8%B5%B7/)

### [Swift API 设计指南 - Changkun's Blog](https://blog.changkun.de/posts/swift-api-design-guidelines/)

### [有趣的人类 - Changkun's Blog](https://blog.changkun.de/posts/%E6%9C%89%E8%B6%A3%E7%9A%84%E4%BA%BA%E7%B1%BB/)

### [所以其实论文并没有什么鬼用 - Changkun's Blog](https://blog.changkun.de/posts/%E6%89%80%E4%BB%A5%E5%85%B6%E5%AE%9E%E8%AE%BA%E6%96%87%E5%B9%B6%E6%B2%A1%E6%9C%89%E4%BB%80%E4%B9%88%E9%AC%BC%E7%94%A8/)

### [Githug 通关记录及指南 - Changkun's Blog](https://blog.changkun.de/posts/githug-record/)

### [小结一下这学期的收获 - Changkun's Blog](https://blog.changkun.de/posts/%E5%B0%8F%E7%BB%93%E4%B8%80%E4%B8%8B%E8%BF%99%E5%AD%A6%E6%9C%9F%E7%9A%84%E6%94%B6%E8%8E%B7/)

### [2015 读书清单 - Changkun's Blog](https://blog.changkun.de/posts/2015-reading/)

### [2015 年终总结 - Changkun's Blog](https://blog.changkun.de/posts/2015-summary/)

### [负能量爆表 - Changkun's Blog](https://blog.changkun.de/posts/%E8%B4%9F%E8%83%BD%E9%87%8F%E7%88%86%E8%A1%A8/)

### [转眼就快两个月了 - Changkun's Blog](https://blog.changkun.de/posts/%E8%BD%AC%E7%9C%BC%E5%B0%B1%E5%BF%AB%E4%B8%A4%E4%B8%AA%E6%9C%88%E4%BA%86/)

### [Tags - Changkun's Blog](https://blog.changkun.de/tags/)

### [博客迁移记录 - Changkun's Blog](https://blog.changkun.de/posts/%E5%8D%9A%E5%AE%A2%E8%BF%81%E7%A7%BB%E8%AE%B0%E5%BD%95/)

### [大三总结 - Changkun's Blog](https://blog.changkun.de/posts/%E5%A4%A7%E4%B8%89%E6%80%BB%E7%BB%93/)

### [这个世界，终究不会是我们的。 - Changkun's Blog](https://blog.changkun.de/posts/%E8%BF%99%E4%B8%AA%E4%B8%96%E7%95%8C%E7%BB%88%E7%A9%B6%E4%B8%8D%E4%BC%9A%E6%98%AF%E6%88%91%E4%BB%AC%E7%9A%84/)

### [Linux 内核分析 之六：Linux 内核创建进程的过程 - Changkun's Blog](https://blog.changkun.de/posts/linux-kernel-6/)

### [小说「泽缘」 - Changkun's Blog](https://blog.changkun.de/posts/%E5%B0%8F%E8%AF%B4%E6%B3%BD%E7%BC%98/)

### [Linux 内核分析 之五：system_call中断处理过程的简要分析 - Changkun's Blog](https://blog.changkun.de/posts/linux-kernel-5/)

### [大创项目的标题真是每年都在考验同学们的想象力啊 - Changkun's Blog](https://blog.changkun.de/posts/%E5%A4%A7%E5%88%9B%E9%A1%B9%E7%9B%AE%E7%9A%84%E6%A0%87%E9%A2%98%E7%9C%9F%E6%98%AF%E6%AF%8F%E5%B9%B4%E9%83%BD%E5%9C%A8%E8%80%83%E9%AA%8C%E5%90%8C%E5%AD%A6%E4%BB%AC%E7%9A%84%E6%83%B3%E8%B1%A1%E5%8A%9B%E5%95%8A/)

### [Linux 内核分析 之四：使用库函数API和嵌入汇编两种方式使用同一个系统调用 - Changkun's Blog](https://blog.changkun.de/posts/linux-kernel-4/)

### [Doxygen 生成中文 Latex 文档 - Changkun's Blog](https://blog.changkun.de/posts/doxygen-latex-doc/)

### [Wordpress 站点搬家 - Changkun's Blog](https://blog.changkun.de/posts/wordpress-%E7%AB%99%E7%82%B9%E6%90%AC%E5%AE%B6/)

### [Linux 内核分析 之三：Linux内核启动函数start_kernel()的简单分析 - Changkun's Blog](https://blog.changkun.de/posts/linux-kernel-3/)

### [Ubuntu14.04 安装 Oracle 11g R2 Express Edition - Changkun's Blog](https://blog.changkun.de/posts/ubuntu1404-install-oracle/)

### [Linux 内核分析 之二：基于时间片轮转的简单的系统内核构造 - Changkun's Blog](https://blog.changkun.de/posts/linux-kernel-2/)

### [Linux 内核分析 之一：How Computer Works 实验 - Changkun's Blog](https://blog.changkun.de/posts/linux-kernel-1/)

### [什么是泛函空间的大数定律？它是机器学习理论的里程碑吗？ - Changkun's Blog](https://blog.changkun.de/posts/%E4%BB%80%E4%B9%88%E6%98%AF%E6%B3%9B%E5%87%BD%E7%A9%BA%E9%97%B4%E7%9A%84%E5%A4%A7%E6%95%B0%E5%AE%9A%E5%BE%8B/)

### [2014 年终总结 - Changkun's Blog](https://blog.changkun.de/posts/2014-summary/)

### [2014 读书清单 - Changkun's Blog](https://blog.changkun.de/posts/2014-reading/)

### [WatchKit框架与WatchApp交互初窥 - Changkun's Blog](https://blog.changkun.de/posts/watchkit-and-watchapp/)

### [Python-MySQLdb 教程 - Changkun's Blog](https://blog.changkun.de/posts/python-mysqldb-tutorial/)

### [谈谈 CV - Changkun's Blog](https://blog.changkun.de/posts/talk-about-cv/)

### [简单的日志分析 - Changkun's Blog](https://blog.changkun.de/posts/simple-logging-analysis/)

### [图书馆真是越来越无趣了 - Changkun's Blog](https://blog.changkun.de/posts/%E5%9B%BE%E4%B9%A6%E9%A6%86%E7%9C%9F%E6%98%AF%E8%B6%8A%E6%9D%A5%E8%B6%8A%E6%97%A0%E8%B6%A3%E4%BA%86/)

### [The worst Presentation - Changkun's Blog](https://blog.changkun.de/posts/the-worst-presentation/)

### [关于 Riemann - Changkun's Blog](https://blog.changkun.de/posts/about-riemann/)

### [删除Mac MySQL - Changkun's Blog](https://blog.changkun.de/posts/delete-mac-mysql/)

### [大坑，难以置信的大坑！ - Changkun's Blog](https://blog.changkun.de/posts/%E9%9A%BE%E4%BB%A5%E7%BD%AE%E4%BF%A1%E7%9A%84%E5%A4%A7%E5%9D%91/)

### [从坐公交车想到的和看到的 - Changkun's Blog](https://blog.changkun.de/posts/%E4%BB%8E%E5%9D%90%E5%85%AC%E4%BA%A4%E8%BD%A6%E6%83%B3%E5%88%B0%E7%9A%84%E5%92%8C%E7%9C%8B%E5%88%B0%E7%9A%84/)

### [在 Mac 中安装 opencv-python - Changkun's Blog](https://blog.changkun.de/posts/install-opencv-python-on-mac/)

### [计算机科学和数学的那点儿破事儿 - Changkun's Blog](https://blog.changkun.de/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E5%92%8C%E6%95%B0%E5%AD%A6%E7%9A%84%E9%82%A3%E7%82%B9%E5%84%BF%E7%A0%B4%E4%BA%8B%E5%84%BF/)

### [拙谈人工智能的未来 - Changkun's Blog](https://blog.changkun.de/posts/%E6%8B%99%E8%B0%88%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E7%9A%84%E6%9C%AA%E6%9D%A5/)

### [Mac下新安装的MySQL无法登陆root用户解决方法 - Changkun's Blog](https://blog.changkun.de/posts/login-issue-after-install-mysql-on-mac/)

### [Matlab 2013a for Mac 帮助文档卡死解决方案 - Changkun's Blog](https://blog.changkun.de/posts/matlab-2013a-for-mac-hanging-issue/)

### [Put color on Mac terminal - Changkun's Blog](https://blog.changkun.de/posts/put-color-on-mac-terminal/)

### [论文都是逼出来的 - Changkun's Blog](https://blog.changkun.de/posts/%E8%AE%BA%E6%96%87%E9%83%BD%E6%98%AF%E9%80%BC%E5%87%BA%E6%9D%A5%E7%9A%84/)

### [Python QuickStart - Changkun's Blog](https://blog.changkun.de/posts/python-quickstart/)

### [谈谈对编程语言的一般性认识 - Changkun's Blog](https://blog.changkun.de/posts/%E8%B0%88%E8%B0%88%E5%AF%B9%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E7%9A%84%E4%B8%80%E8%88%AC%E6%80%A7%E8%AE%A4%E8%AF%86/)

### [写 App & 与打砖块：无尽版 - Changkun's Blog](https://blog.changkun.de/posts/%E6%89%93%E7%A0%96%E5%9D%97%E6%97%A0%E5%B0%BD%E7%89%88/)

### [MacTeX 卸载方法 - Changkun's Blog](https://blog.changkun.de/posts/delete-mactex/)

### [Lua一日游:(5) cocos2dx 与 Lua - Changkun's Blog](https://blog.changkun.de/posts/lua-5/)

### [Lua一日游:(4)面向对象——函数闭包形式 - Changkun's Blog](https://blog.changkun.de/posts/lua-4/)

### [Lua一日游:(3)面向对象——复制表形式 - Changkun's Blog](https://blog.changkun.de/posts/lua-3/)

### [Lua一日游:(2)Table和Array - Changkun's Blog](https://blog.changkun.de/posts/lua-2/)

### [Lua一日游:(1) Mac环境搭建与基本语法 - Changkun's Blog](https://blog.changkun.de/posts/lua-1/)

### [崇祯为什么不跑南京？ - Changkun's Blog](https://blog.changkun.de/posts/%E5%B4%87%E7%A5%AF%E4%B8%BA%E4%BB%80%E4%B9%88%E4%B8%8D%E8%B7%91%E5%8D%97%E4%BA%AC/)

### [看了这么久外剧，也就发现这部片子火这么厉害 - Changkun's Blog](https://blog.changkun.de/posts/%E7%9C%8B%E4%BA%86%E8%BF%99%E4%B9%88%E4%B9%85%E5%A4%96%E5%89%A7%E4%B9%9F%E5%B0%B1%E5%8F%91%E7%8E%B0%E8%BF%99%E9%83%A8%E7%89%87%E5%AD%90%E7%81%AB%E8%BF%99%E4%B9%88%E5%8E%89%E5%AE%B3/)

### [Paper读后感：人机交互的进展及面临的挑战 - Changkun's Blog](https://blog.changkun.de/posts/%E4%BA%BA%E6%9C%BA%E4%BA%A4%E4%BA%92%E7%9A%84%E8%BF%9B%E5%B1%95%E5%8F%8A%E9%9D%A2%E4%B8%B4%E7%9A%84%E6%8C%91%E6%88%98/)

### [今天编译项目真心觉得很憔悴啊 - Changkun's Blog](https://blog.changkun.de/posts/%E4%BB%8A%E5%A4%A9%E7%BC%96%E8%AF%91%E9%A1%B9%E7%9B%AE%E7%9C%9F%E5%BF%83%E8%A7%89%E5%BE%97%E5%BE%88%E6%86%94%E6%82%B4%E5%95%8A/)

### [最近读了点人工智能 - Changkun's Blog](https://blog.changkun.de/posts/%E6%9C%80%E8%BF%91%E8%AF%BB%E4%BA%86%E7%82%B9%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/)

### [2013 年终总结 - Changkun's Blog](https://blog.changkun.de/posts/2013-summary/)

### [重温《Harry Potter》 - Changkun's Blog](https://blog.changkun.de/posts/review-harry-potter/)

### [这学期大创项目总算是告一段落了 - Changkun's Blog](https://blog.changkun.de/posts/%E8%BF%99%E5%AD%A6%E6%9C%9F%E5%A4%A7%E5%88%9B%E9%A1%B9%E7%9B%AE%E6%80%BB%E7%AE%97%E6%98%AF%E5%91%8A%E4%B8%80%E6%AE%B5%E8%90%BD%E4%BA%86/)

### [Linux学习笔记 6 网络及配置相关 - Changkun's Blog](https://blog.changkun.de/posts/learn-linux-6/)

### [Linux学习笔记 5 UGO模型、Linux权限 - Changkun's Blog](https://blog.changkun.de/posts/learn-linux-5/)

### [Linux学习笔记 7 获取帮助 - Changkun's Blog](https://blog.changkun.de/posts/learn-linux-7/)

### [Linux学习笔记 4 Linux文件系统基本操作 - Changkun's Blog](https://blog.changkun.de/posts/learn-linux-4/)

### [又谈 Android - Changkun's Blog](https://blog.changkun.de/posts/talk-about-android/)

### [Linux学习笔记 3 Vim必背命令 - Changkun's Blog](https://blog.changkun.de/posts/learn-linux-3/)

### [Linux学习笔记 2 Linux常用必背命令 - Changkun's Blog](https://blog.changkun.de/posts/learn-linux-2/)

### [Linux学习笔记 1 Linux基本操作及其文件系统结构 - Changkun's Blog](https://blog.changkun.de/posts/learn-linux-1/)

### [《C专家编程》读书笔记 - Changkun's Blog](https://blog.changkun.de/posts/c-expert-programming-reading-notes/)

### [谈谈二分搜索 - Changkun's Blog](https://blog.changkun.de/posts/talk-about-binary-search/)

### [谈谈积分与函数 - Changkun's Blog](https://blog.changkun.de/posts/integral-and-function/)

### [「中国要是有一千个陈景润就不得了了」 - Changkun's Blog](https://blog.changkun.de/posts/%E4%B8%AD%E5%9B%BD%E8%A6%81%E6%98%AF%E6%9C%89%E4%B8%80%E5%8D%83%E4%B8%AA%E9%99%88%E6%99%AF%E6%B6%A6%E5%B0%B1%E4%B8%8D%E5%BE%97%E4%BA%86%E4%BA%86/)

### [数学建模协会入会笔试题 - Changkun's Blog](https://blog.changkun.de/posts/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E5%8D%8F%E4%BC%9A%E5%85%A5%E4%BC%9A%E7%AC%94%E8%AF%95%E9%A2%98/)

### [曾勇老师语录 - Changkun's Blog](https://blog.changkun.de/posts/%E6%9B%BE%E5%8B%87%E8%80%81%E5%B8%88%E8%AF%AD%E5%BD%95/)

### [Fourier变换应用小记 - Changkun's Blog](https://blog.changkun.de/posts/fourier-transformation-notes/)

### [模版链队实现 - Changkun's Blog](https://blog.changkun.de/posts/%E6%A8%A1%E7%89%88%E9%93%BE%E9%98%9F%E5%AE%9E%E7%8E%B0/)

### [模版链栈实现 - Changkun's Blog](https://blog.changkun.de/posts/%E6%A8%A1%E7%89%88%E9%93%BE%E6%A0%88%E5%AE%9E%E7%8E%B0/)

### [从圆锥体积谈起 - Changkun's Blog](https://blog.changkun.de/posts/%E4%BB%8E%E5%9C%86%E9%94%A5%E4%BD%93%E7%A7%AF%E8%B0%88%E8%B5%B7/)

### [《乔布斯》观后感 - Changkun's Blog](https://blog.changkun.de/posts/jobs-movie-review/)

### [反转单链表 - Changkun's Blog](https://blog.changkun.de/posts/%E5%8F%8D%E8%BD%AC%E5%8D%95%E9%93%BE%E8%A1%A8/)

### [通信信号与正交向量之间的关系 - Changkun's Blog](https://blog.changkun.de/posts/%E9%80%9A%E4%BF%A1%E4%BF%A1%E5%8F%B7%E4%B8%8E%E6%AD%A3%E4%BA%A4%E5%90%91%E9%87%8F%E4%B9%8B%E9%97%B4%E7%9A%84%E5%85%B3%E7%B3%BB/)

### [谈人与事 - Changkun's Blog](https://blog.changkun.de/posts/%E8%B0%88%E4%BA%BA%E4%B8%8E%E4%BA%8B/)

### [基于图像信号分析的碎纸片的拼接复原 - Changkun's Blog](https://blog.changkun.de/posts/%E5%9F%BA%E4%BA%8E%E5%9B%BE%E5%83%8F%E4%BF%A1%E5%8F%B7%E5%88%86%E6%9E%90%E7%9A%84%E7%A2%8E%E7%BA%B8%E7%89%87%E7%9A%84%E6%8B%BC%E6%8E%A5%E5%A4%8D%E5%8E%9F/)

### [从 Windows 到 Macintosh - Changkun's Blog](https://blog.changkun.de/posts/from-windows-to-macintosh/)

### [模版链表实现与模版声明定义分离编译错误 - Changkun's Blog](https://blog.changkun.de/posts/%E6%A8%A1%E7%89%88%E9%93%BE%E8%A1%A8%E5%AE%9E%E7%8E%B0%E4%B8%8E%E6%A8%A1%E7%89%88%E5%A3%B0%E6%98%8E%E5%AE%9A%E4%B9%89%E5%88%86%E7%A6%BB%E7%BC%96%E8%AF%91%E9%94%99%E8%AF%AF/)

### [MeanShift 均值漂移跟踪算法原理 - Changkun's Blog](https://blog.changkun.de/posts/meanshift-algorithm/)

### [下一次的数学突破？ - Changkun's Blog](https://blog.changkun.de/posts/%E4%B8%8B%E4%B8%80%E6%AC%A1%E7%9A%84%E6%95%B0%E5%AD%A6%E7%AA%81%E7%A0%B4/)

### [Kalman滤波器数学原理与应用 - Changkun's Blog](https://blog.changkun.de/posts/kalman-filter-principle/)

### [在堆上分配内存 - Changkun's Blog](https://blog.changkun.de/posts/%E5%9C%A8%E5%A0%86%E4%B8%8A%E5%88%86%E9%85%8D%E5%86%85%E5%AD%98/)

### [MFC系列（一）创建空白窗口 - Changkun's Blog](https://blog.changkun.de/posts/windows-mfc-1/)

### [在 Mac 中配置OpenCV - Changkun's Blog](https://blog.changkun.de/posts/install-opencv-on-mac/)

### [从两个例子谈起：条件收敛与一致收敛的深刻背景分析 - Changkun's Blog](https://blog.changkun.de/posts/%E6%9D%A1%E4%BB%B6%E6%94%B6%E6%95%9B%E4%B8%8E%E4%B8%80%E8%87%B4%E6%94%B6%E6%95%9B%E7%9A%84%E6%B7%B1%E5%88%BB%E8%83%8C%E6%99%AF%E5%88%86%E6%9E%90/)

### [大一总结 - Changkun's Blog](https://blog.changkun.de/posts/%E5%A4%A7%E4%B8%80%E6%80%BB%E7%BB%93/)

### [陈老爷子 - Changkun's Blog](https://blog.changkun.de/posts/%E9%99%88%E8%80%81%E7%88%B7%E5%AD%90/)

### [物理世界是离散的却为什么可以大量的使用微分和积分？ - Changkun's Blog](https://blog.changkun.de/posts/%E7%89%A9%E7%90%86%E4%B8%96%E7%95%8C%E6%98%AF%E7%A6%BB%E6%95%A3%E7%9A%84%E5%8D%B4%E4%B8%BA%E4%BB%80%E4%B9%88%E5%8F%AF%E4%BB%A5%E5%A4%A7%E9%87%8F%E7%9A%84%E4%BD%BF%E7%94%A8%E5%BE%AE%E5%88%86%E5%92%8C%E7%A7%AF%E5%88%86/)

### [Curriculum Vitae - Changkun's Blog](https://blog.changkun.de/cv/)

### [多线程之间的通信 GPU计算 单摄像头图像识别 初步思考 - Changkun's Blog](https://blog.changkun.de/posts/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B9%8B%E9%97%B4%E7%9A%84%E9%80%9A%E4%BF%A1-gpu%E8%AE%A1%E7%AE%97-%E5%8D%95%E6%91%84%E5%83%8F%E5%A4%B4%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB-%E5%88%9D%E6%AD%A5%E6%80%9D%E8%80%83/)

### [我这是要转 Ubuntu 的节奏吗 - Changkun's Blog](https://blog.changkun.de/posts/%E6%88%91%E8%BF%99%E6%98%AF%E8%A6%81%E8%BD%AC-ubuntu-%E7%9A%84%E8%8A%82%E5%A5%8F%E5%90%97/)

### [结构体排序引发的一连串问题 - Changkun's Blog](https://blog.changkun.de/posts/%E7%BB%93%E6%9E%84%E4%BD%93%E6%8E%92%E5%BA%8F%E5%BC%95%E5%8F%91%E7%9A%84%E4%B8%80%E8%BF%9E%E4%B8%B2%E9%97%AE%E9%A2%98/)

### [Ubuntu 相关 - Changkun's Blog](https://blog.changkun.de/posts/ubuntu-%E7%9B%B8%E5%85%B3/)

### [逃生时间初算 - Changkun's Blog](https://blog.changkun.de/posts/%E9%80%83%E7%94%9F%E6%97%B6%E9%97%B4%E5%88%9D%E7%AE%97/)

### [全空间和空空间？ - Changkun's Blog](https://blog.changkun.de/posts/%E5%85%A8%E7%A9%BA%E9%97%B4%E5%92%8C%E7%A9%BA%E7%A9%BA%E9%97%B4/)

### [从装系统中学到的 - Changkun's Blog](https://blog.changkun.de/posts/%E4%BB%8E%E8%A3%85%E7%B3%BB%E7%BB%9F%E4%B8%AD%E5%AD%A6%E5%88%B0%E7%9A%84/)

### [string 标准库实现日志 - Changkun's Blog](https://blog.changkun.de/posts/string-%E6%A0%87%E5%87%86%E5%BA%93%E5%AE%9E%E7%8E%B0%E6%97%A5%E5%BF%97/)

### [通过安装 Wordpress 插件所学到的 - Changkun's Blog](https://blog.changkun.de/posts/%E9%80%9A%E8%BF%87%E5%AE%89%E8%A3%85-wordpress-%E6%8F%92%E4%BB%B6%E6%89%80%E5%AD%A6%E5%88%B0%E7%9A%84/)

### [C Details 之 基本细节 - Changkun's Blog](https://blog.changkun.de/posts/c-details-%E4%B9%8B-%E5%9F%BA%E6%9C%AC%E7%BB%86%E8%8A%82/)

### [又读流形 - Changkun's Blog](https://blog.changkun.de/posts/%E5%8F%88%E8%AF%BB%E6%B5%81%E5%BD%A2/)

### [Bio - Changkun's Blog](https://blog.changkun.de/about/)

### [ZFC 集合论中各公理的意义及作用 - Changkun's Blog](https://blog.changkun.de/posts/zfc-%E9%9B%86%E5%90%88%E8%AE%BA%E4%B8%AD%E5%90%84%E5%85%AC%E7%90%86%E7%9A%84%E6%84%8F%E4%B9%89%E5%8F%8A%E4%BD%9C%E7%94%A8/)

### [谈线性空间定义中加法交换律独立性 - Changkun's Blog](https://blog.changkun.de/posts/%E8%B0%88%E7%BA%BF%E6%80%A7%E7%A9%BA%E9%97%B4%E5%AE%9A%E4%B9%89%E4%B8%AD%E5%8A%A0%E6%B3%95%E4%BA%A4%E6%8D%A2%E5%BE%8B%E7%8B%AC%E7%AB%8B%E6%80%A7/)

### [为什么要有特征值和特征向量 - Changkun's Blog](https://blog.changkun.de/posts/%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E6%9C%89%E7%89%B9%E5%BE%81%E5%80%BC%E5%92%8C%E7%89%B9%E5%BE%81%E5%90%91%E9%87%8F/)

### [2012 年终总结 - Changkun's Blog](https://blog.changkun.de/posts/2012-summary/)

### [一个人的锦都 - Changkun's Blog](https://blog.changkun.de/posts/%E4%B8%80%E4%B8%AA%E4%BA%BA%E7%9A%84%E9%94%A6%E9%83%BD/)

### [谈谈Google和Android - Changkun's Blog](https://blog.changkun.de/posts/%E8%B0%88%E8%B0%88google%E5%92%8Candroid/)

### [又是一月流阙时 - Changkun's Blog](https://blog.changkun.de/posts/%E5%8F%88%E6%98%AF%E4%B8%80%E6%9C%88%E6%B5%81%E9%98%99%E6%97%B6/)

### [文艺还是苦逼 确实不是一个问题 - Changkun's Blog](https://blog.changkun.de/posts/%E6%96%87%E8%89%BA%E8%BF%98%E6%98%AF%E8%8B%A6%E9%80%BC%E7%A1%AE%E5%AE%9E%E4%B8%8D%E6%98%AF%E4%B8%80%E4%B8%AA%E9%97%AE%E9%A2%98/)

### [谈谈良序原理 - Changkun's Blog](https://blog.changkun.de/posts/%E8%B0%88%E8%B0%88%E8%89%AF%E5%BA%8F%E5%8E%9F%E7%90%86/)

### [X-2222 计划 - Changkun's Blog](https://blog.changkun.de/posts/x-2222-%E8%AE%A1%E5%88%92/)

### [我坚信 - Changkun's Blog](https://blog.changkun.de/posts/%E6%88%91%E5%9D%9A%E4%BF%A1/)

### [黎明前的黑暗 - Changkun's Blog](https://blog.changkun.de/posts/%E9%BB%8E%E6%98%8E%E5%89%8D%E7%9A%84%E9%BB%91%E6%9A%97/)

### [高考后的计划，第一版 - Changkun's Blog](https://blog.changkun.de/posts/%E9%AB%98%E8%80%83%E5%90%8E%E7%9A%84%E8%AE%A1%E5%88%92%E7%AC%AC%E4%B8%80%E7%89%88/)

### [数学系就业方向小谈 - Changkun's Blog](https://blog.changkun.de/posts/%E6%95%B0%E5%AD%A6%E7%B3%BB%E5%B0%B1%E4%B8%9A%E6%96%B9%E5%90%91%E5%B0%8F%E8%B0%88/)

### [用数学眼光看世界之「焦点访谈的播出时间」 - Changkun's Blog](https://blog.changkun.de/posts/%E7%94%A8%E6%95%B0%E5%AD%A6%E7%9C%BC%E5%85%89%E7%9C%8B%E4%B8%96%E7%95%8C%E4%B9%8B%E7%84%A6%E7%82%B9%E8%AE%BF%E8%B0%88%E7%9A%84%E6%92%AD%E5%87%BA%E6%97%B6%E9%97%B4/)

### [故城1 - Changkun's Blog](https://blog.changkun.de/posts/%E6%95%85%E5%9F%8E1/)

### [记事簿5 - Changkun's Blog](https://blog.changkun.de/posts/%E8%AE%B0%E4%BA%8B%E7%B0%BF5/)

### [记事簿4 - Changkun's Blog](https://blog.changkun.de/posts/%E8%AE%B0%E4%BA%8B%E7%B0%BF4/)

### [记事簿3 - Changkun's Blog](https://blog.changkun.de/posts/%E8%AE%B0%E4%BA%8B%E7%B0%BF3/)

### [记事簿2 - Changkun's Blog](https://blog.changkun.de/posts/%E8%AE%B0%E4%BA%8B%E7%B0%BF2/)

### [记事簿1 - Changkun's Blog](https://blog.changkun.de/posts/%E8%AE%B0%E4%BA%8B%E7%B0%BF1/)

### [某不等式 - Changkun's Blog](https://blog.changkun.de/posts/%E6%9F%90%E4%B8%8D%E7%AD%89%E5%BC%8F/)

### [核辐射没那么可怕 - Changkun's Blog](https://blog.changkun.de/posts/%E6%A0%B8%E8%BE%90%E5%B0%84%E6%B2%A1%E9%82%A3%E4%B9%88%E5%8F%AF%E6%80%95/)

### [赌博（装备锻造）必定破产 - Changkun's Blog](https://blog.changkun.de/posts/%E8%B5%8C%E5%8D%9A%E5%BF%85%E5%AE%9A%E7%A0%B4%E4%BA%A7/)

### [把伤痕当酒窝 - Changkun's Blog](https://blog.changkun.de/posts/%E6%8A%8A%E4%BC%A4%E7%97%95%E5%BD%93%E9%85%92%E7%AA%9D/)

### [游戏装备升级问题 - Changkun's Blog](https://blog.changkun.de/posts/%E6%B8%B8%E6%88%8F%E8%A3%85%E5%A4%87%E5%8D%87%E7%BA%A7%E9%97%AE%E9%A2%98/)

### [为什么做不完高考数学 - Changkun's Blog](https://blog.changkun.de/posts/%E4%B8%BA%E4%BB%80%E4%B9%88%E5%81%9A%E4%B8%8D%E5%AE%8C%E9%AB%98%E8%80%83%E6%95%B0%E5%AD%A6/)

### [素数无穷多的拓扑学证明 - Changkun's Blog](https://blog.changkun.de/posts/%E7%B4%A0%E6%95%B0%E6%97%A0%E7%A9%B7%E5%A4%9A%E7%9A%84%E6%8B%93%E6%89%91%E5%AD%A6%E8%AF%81%E6%98%8E/)

### [人品守恒定律 - Changkun's Blog](https://blog.changkun.de/posts/%E4%BA%BA%E5%93%81%E5%AE%88%E6%81%92%E5%AE%9A%E5%BE%8B/)

### [无奈 - Changkun's Blog](https://blog.changkun.de/posts/%E6%97%A0%E5%A5%88/)

### [错失的机遇 - Changkun's Blog](https://blog.changkun.de/posts/%E9%94%99%E5%A4%B1%E7%9A%84%E6%9C%BA%E9%81%87/)

### [小 Q 自传 - Changkun's Blog](https://blog.changkun.de/posts/%E5%B0%8F-q-%E8%87%AA%E4%BC%A0/)

### [见鬼 - Changkun's Blog](https://blog.changkun.de/posts/%E8%A7%81%E9%AC%BC/)

### [龙崎君 - Changkun's Blog](https://blog.changkun.de/posts/%E9%BE%99%E5%B4%8E%E5%90%9B/)

### [禁言 - Changkun's Blog](https://blog.changkun.de/posts/%E7%A6%81%E8%A8%80/)

### [春天来了 一切都还是温暖的 - Changkun's Blog](https://blog.changkun.de/posts/%E6%98%A5%E5%A4%A9%E6%9D%A5%E4%BA%86%E4%B8%80%E5%88%87%E9%83%BD%E8%BF%98%E6%98%AF%E6%B8%A9%E6%9A%96%E7%9A%84/)

### [狗屁二三事 - Changkun's Blog](https://blog.changkun.de/posts/%E7%8B%97%E5%B1%81%E4%BA%8C%E4%B8%89%E4%BA%8B/)

### [黑白世 - Changkun's Blog](https://blog.changkun.de/posts/%E9%BB%91%E7%99%BD%E4%B8%96/)

### [再谈例证法 - Changkun's Blog](https://blog.changkun.de/posts/%E5%86%8D%E8%B0%88%E4%BE%8B%E8%AF%81%E6%B3%95/)

### [例证法 - Changkun's Blog](https://blog.changkun.de/posts/%E4%BE%8B%E8%AF%81%E6%B3%95/)

### [又见夏洛克 - Changkun's Blog](https://blog.changkun.de/posts/%E5%8F%88%E8%A7%81%E5%A4%8F%E6%B4%9B%E5%85%8B/)

### [无言 - Changkun's Blog](https://blog.changkun.de/posts/%E6%97%A0%E8%A8%80/)

### [牢不可破的誓言 - Changkun's Blog](https://blog.changkun.de/posts/%E7%89%A2%E4%B8%8D%E5%8F%AF%E7%A0%B4%E7%9A%84%E8%AA%93%E8%A8%80/)

### [丢失的情人节 - Changkun's Blog](https://blog.changkun.de/posts/%E4%B8%A2%E5%A4%B1%E7%9A%84%E6%83%85%E4%BA%BA%E8%8A%82/)

### [追逐 - Changkun's Blog](https://blog.changkun.de/posts/%E8%BF%BD%E9%80%90/)

### [所谓语言 - Changkun's Blog](https://blog.changkun.de/posts/%E6%89%80%E8%B0%93%E8%AF%AD%E8%A8%80/)

### [风雪二月 - Changkun's Blog](https://blog.changkun.de/posts/%E9%A3%8E%E9%9B%AA%E4%BA%8C%E6%9C%88/)

### [深夜 - Changkun's Blog](https://blog.changkun.de/posts/%E6%B7%B1%E5%A4%9C/)

### [烟火过境 - Changkun's Blog](https://blog.changkun.de/posts/%E7%83%9F%E7%81%AB%E8%BF%87%E5%A2%83/)

### [无题 - Changkun's Blog](https://blog.changkun.de/posts/%E6%97%A0%E9%A2%98/)

### [所谓爱情 - Changkun's Blog](https://blog.changkun.de/posts/%E6%89%80%E8%B0%93%E7%88%B1%E6%83%85/)

### [On my way。 - Changkun's Blog](https://blog.changkun.de/posts/on-my-way/)

### [电影「2012」影评 - Changkun's Blog](https://blog.changkun.de/posts/%E7%94%B5%E5%BD%B12012%E5%BD%B1%E8%AF%84/)

### [永远不要问我为什么！ - Changkun's Blog](https://blog.changkun.de/posts/%E6%B0%B8%E8%BF%9C%E4%B8%8D%E8%A6%81%E9%97%AE%E6%88%91%E4%B8%BA%E4%BB%80%E4%B9%88/)

### [得与失 - Changkun's Blog](https://blog.changkun.de/posts/%E5%BE%97%E4%B8%8E%E5%A4%B1/)

### [一阵风 一场梦 - Changkun's Blog](https://blog.changkun.de/posts/%E4%B8%80%E9%98%B5%E9%A3%8E%E4%B8%80%E5%9C%BA%E6%A2%A6/)

### [我将记得你 - Changkun's Blog](https://blog.changkun.de/posts/%E6%88%91%E5%B0%86%E8%AE%B0%E5%BE%97%E4%BD%A0/)

### [人事已非 - Changkun's Blog](https://blog.changkun.de/posts/%E4%BA%BA%E4%BA%8B%E5%B7%B2%E9%9D%9E/)

### [小谈瑕积分 - Changkun's Blog](https://blog.changkun.de/posts/%E5%B0%8F%E8%B0%88%E7%91%95%E7%A7%AF%E5%88%86/)

### [再谈孤独的果实 - Changkun's Blog](https://blog.changkun.de/posts/%E5%86%8D%E8%B0%88%E5%AD%A4%E7%8B%AC%E7%9A%84%E6%9E%9C%E5%AE%9E/)

### [人生 - Changkun's Blog](https://blog.changkun.de/posts/%E4%BA%BA%E7%94%9F/)

### [任性了 太久之后想法 - Changkun's Blog](https://blog.changkun.de/posts/%E4%BB%BB%E6%80%A7%E4%BA%86%E5%A4%AA%E4%B9%85%E4%B9%8B%E5%90%8E%E6%83%B3%E6%B3%95/)

### [繁星客栈 - Changkun's Blog](https://blog.changkun.de/posts/%E7%B9%81%E6%98%9F%E5%AE%A2%E6%A0%88/)

### [那片海 - Changkun's Blog](https://blog.changkun.de/posts/%E9%82%A3%E7%89%87%E6%B5%B7/)

### [隔帘听 - Changkun's Blog](https://blog.changkun.de/posts/%E9%9A%94%E5%B8%98%E5%90%AC/)

### [路灯 - Changkun's Blog](https://blog.changkun.de/posts/%E8%B7%AF%E7%81%AF/)

### [祝纯数学永无用处 & 数学家不后悔 - Changkun's Blog](https://blog.changkun.de/posts/%E6%95%B0%E5%AD%A6%E5%AE%B6%E4%B8%8D%E5%90%8E%E6%82%94/)

### [烟波江南 - Changkun's Blog](https://blog.changkun.de/posts/%E7%83%9F%E6%B3%A2%E6%B1%9F%E5%8D%97/)

### [一路走来我 - Changkun's Blog](https://blog.changkun.de/posts/%E4%B8%80%E8%B7%AF%E8%B5%B0%E6%9D%A5%E6%88%91/)

### [娱乐至死 - Changkun's Blog](https://blog.changkun.de/posts/%E5%A8%B1%E4%B9%90%E8%87%B3%E6%AD%BB/)

### [我家的鹦鹉 & 纪念 5.12 - Changkun's Blog](https://blog.changkun.de/posts/%E6%88%91%E5%AE%B6%E7%9A%84%E9%B9%A6%E9%B9%89/)

### [不想说再见 - Changkun's Blog](https://blog.changkun.de/posts/%E4%B8%8D%E6%83%B3%E8%AF%B4%E5%86%8D%E8%A7%81/)

### [夏季。流年。昨 - Changkun's Blog](https://blog.changkun.de/posts/%E5%A4%8F%E5%AD%A3%E6%B5%81%E5%B9%B4%E6%98%A8/)

### [删除记忆 - Changkun's Blog](https://blog.changkun.de/posts/%E5%88%A0%E9%99%A4%E8%AE%B0%E5%BF%86/)

### [真与伪 善与恶 - Changkun's Blog](https://blog.changkun.de/posts/%E7%9C%9F%E4%B8%8E%E4%BC%AA-%E5%96%84%E4%B8%8E%E6%81%B6/)

### [人的一生，时起时落，但是，确实美好的。 - Changkun's Blog](https://blog.changkun.de/posts/%E4%BA%BA%E7%9A%84%E4%B8%80%E7%94%9F%E6%97%B6%E8%B5%B7%E6%97%B6%E8%90%BD%E4%BD%86%E6%98%AF%E7%A1%AE%E5%AE%9E%E7%BE%8E%E5%A5%BD%E7%9A%84/)

### [那些花儿，盛开了，散落了 - Changkun's Blog](https://blog.changkun.de/posts/%E9%82%A3%E4%BA%9B%E8%8A%B1%E5%84%BF%E7%9B%9B%E5%BC%80%E4%BA%86%E6%95%A3%E8%90%BD%E4%BA%86/)

### [独唱情歌 - Changkun's Blog](https://blog.changkun.de/posts/%E7%8B%AC%E5%94%B1%E6%83%85%E6%AD%8C/)

### [无愁 - Changkun's Blog](https://blog.changkun.de/posts/%E6%97%A0%E6%84%81/)

### [暖春 - Changkun's Blog](https://blog.changkun.de/posts/%E6%9A%96%E6%98%A5/)

### [思念 - Changkun's Blog](https://blog.changkun.de/posts/%E6%80%9D%E5%BF%B5/)

### [冷月 - Changkun's Blog](https://blog.changkun.de/posts/%E5%86%B7%E6%9C%88/)

### [春醒 - Changkun's Blog](https://blog.changkun.de/posts/%E6%98%A5%E9%86%92/)

### [人生轨迹II - Changkun's Blog](https://blog.changkun.de/posts/%E4%BA%BA%E7%94%9F%E8%BD%A8%E8%BF%B9ii/)

### [人生轨迹 - Changkun's Blog](https://blog.changkun.de/posts/%E4%BA%BA%E7%94%9F%E8%BD%A8%E8%BF%B9/)

### [释怀 - Changkun's Blog](https://blog.changkun.de/posts/%E9%87%8A%E6%80%80/)

### [- Changkun's Blog](https://blog.changkun.de/_todo/ddos-%E6%94%BB%E5%87%BB%E9%98%B2%E6%8A%A4%E5%88%9D%E6%AD%A5/)

### [- Changkun's Blog](https://blog.changkun.de/_todo/docker-%E6%9E%81%E9%80%9F%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B3-%E6%95%B0%E6%8D%AE%E4%B8%8E%E7%BD%91%E7%BB%9C%E7%AE%A1%E7%90%86%E5%8F%8Adocker%E5%AE%89%E5%85%A8/)

### [- Changkun's Blog](https://blog.changkun.de/_todo/html-css-%E4%B8%80%E6%97%A5%E6%A6%82%E8%A7%88/)

### [- Changkun's Blog](https://blog.changkun.de/_todo/r%E8%AF%AD%E8%A8%80%E5%88%9D%E4%BD%93%E9%AA%8C/)

### [- Changkun's Blog](https://blog.changkun.de/_todo/shallow-learning-and-deep-learning/)

### [- Changkun's Blog](https://blog.changkun.de/_todo/ycbcr%E9%A2%9C%E8%89%B2%E7%A9%BA%E9%97%B4%E4%B8%AD%E7%9A%84%E8%82%A4%E8%89%B2%E6%A4%AD%E5%9C%86%E6%A8%A1%E5%9E%8B/)

### [- Changkun's Blog](https://blog.changkun.de/_todo/%E4%BA%BA%E6%9C%BA%E4%BA%A4%E4%BA%92%E7%9A%84%E8%BF%87%E5%8E%BB%E7%8E%B0%E5%9C%A8%E4%B8%8E%E6%9C%AA%E6%9D%A5%E4%B8%80%E5%8F%AF%E7%94%A8%E6%80%A7%E6%98%93%E7%94%A8%E6%80%A7%E7%94%A8%E6%88%B7%E7%94%A8%E6%88%B7%E4%BD%93%E9%AA%8C/)

### [- Changkun's Blog](https://blog.changkun.de/_todo/%E5%9F%BA%E4%BA%8E-electron-%E7%BC%96%E5%86%99%E8%B7%A8%E5%B9%B3%E5%8F%B0%E5%AE%A2%E6%88%B7%E7%AB%AF%E7%9A%84%E8%8B%A5%E5%B9%B2%E5%AE%9E%E8%B7%B5/)

### [- Changkun's Blog](https://blog.changkun.de/_todo/%E5%BE%B7%E5%9B%BD%E4%B8%8E%E4%B8%AD%E5%9B%BD%E7%9A%84%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%99%E8%82%B2%E6%9C%89%E4%BD%95%E4%B8%8D%E5%90%8C/)

### [- Changkun's Blog](https://blog.changkun.de/_todo/%E9%85%8D%E7%BD%AEmysql%E4%B8%BB%E4%BB%8E%E7%83%AD%E5%A4%87%E5%AE%9E%E7%8E%B0%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB/)

### [- Changkun's Blog](https://blog.changkun.de/_todo/%E9%9D%A2%E5%90%91%E5%8D%8F%E8%AE%AE%E7%BC%96%E7%A8%8B%E5%B0%8F%E8%B0%88/)

### [- Changkun's Blog](https://blog.changkun.de/augmentedtouch/)

### [- Changkun's Blog](https://blog.changkun.de/clients/)

### [- Changkun's Blog](https://blog.changkun.de/demo/)

### [- Changkun's Blog](https://blog.changkun.de/math-modeling/)

### [0.99... = 1? 尝试证明 - Changkun's Blog](https://blog.changkun.de/_todo/0-99-1-%E5%B0%9D%E8%AF%95%E8%AF%81%E6%98%8E/)

### [Hamilton 找不到「三维复数」的原因 - Changkun's Blog](https://blog.changkun.de/_todo/hamilton-%E6%89%BE%E4%B8%8D%E5%88%B0%E4%B8%89%E7%BB%B4%E5%A4%8D%E6%95%B0%E7%9A%84%E5%8E%9F%E5%9B%A0/)

### [Ideas - Changkun's Blog](https://blog.changkun.de/ideas/)

### [N 次方程，有N个根？ - Changkun's Blog](https://blog.changkun.de/_todo/n-%E6%AC%A1%E6%96%B9%E7%A8%8B%E6%9C%89n%E4%B8%AA%E6%A0%B9/)

### [sinx 与超越数 - Changkun's Blog](https://blog.changkun.de/_todo/sinx-%E4%B8%8E%E8%B6%85%E8%B6%8A%E6%95%B0/)

### [The Ultimate Brownie Pan - Changkun's Blog](https://blog.changkun.de/_todo/the-ultimate-brownie-pan/)

### [三斜求积 海伦公式 - Changkun's Blog](https://blog.changkun.de/_todo/%E4%B8%89%E6%96%9C%E6%B1%82%E7%A7%AF-%E6%B5%B7%E4%BC%A6%E5%85%AC%E5%BC%8F/)

### [关于教学质量的评价问题的分析与探讨 - Changkun's Blog](https://blog.changkun.de/_todo/%E5%85%B3%E4%BA%8E%E6%95%99%E5%AD%A6%E8%B4%A8%E9%87%8F%E7%9A%84%E8%AF%84%E4%BB%B7%E9%97%AE%E9%A2%98%E7%9A%84%E5%88%86%E6%9E%90%E4%B8%8E%E6%8E%A2%E8%AE%A8/)

### [图形拓扑论 趣话：奇趣矩阵 - Changkun's Blog](https://blog.changkun.de/_todo/%E5%9B%BE%E5%BD%A2%E6%8B%93%E6%89%91%E8%AE%BA-%E8%B6%A3%E8%AF%9D%E5%A5%87%E8%B6%A3%E7%9F%A9%E9%98%B5/)

### [基于PID与神经元算法的制动器试验台的控制方法分析 - Changkun's Blog](https://blog.changkun.de/_todo/%E5%9F%BA%E4%BA%8Epid%E4%B8%8E%E7%A5%9E%E7%BB%8F%E5%85%83%E7%AE%97%E6%B3%95%E7%9A%84%E5%88%B6%E5%8A%A8%E5%99%A8%E8%AF%95%E9%AA%8C%E5%8F%B0%E7%9A%84%E6%8E%A7%E5%88%B6%E6%96%B9%E6%B3%95%E5%88%86%E6%9E%90/)

### [基于非线性拟合与积分方法对储油罐的变位识别与罐容表标定相关问题的研究 - Changkun's Blog](https://blog.changkun.de/_todo/%E5%9F%BA%E4%BA%8E%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%8B%9F%E5%90%88%E4%B8%8E%E7%A7%AF%E5%88%86%E6%96%B9%E6%B3%95%E5%AF%B9%E5%82%A8%E6%B2%B9%E7%BD%90%E7%9A%84%E5%8F%98%E4%BD%8D%E8%AF%86%E5%88%AB%E4%B8%8E%E7%BD%90%E5%AE%B9%E8%A1%A8%E6%A0%87%E5%AE%9A%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98%E7%9A%84%E7%A0%94%E7%A9%B6/)

### [基于非线性规划方法的露天矿生产车辆的安排算法 - Changkun's Blog](https://blog.changkun.de/_todo/%E5%9F%BA%E4%BA%8E%E9%9D%9E%E7%BA%BF%E6%80%A7%E8%A7%84%E5%88%92%E6%96%B9%E6%B3%95%E7%9A%84%E9%9C%B2%E5%A4%A9%E7%9F%BF%E7%94%9F%E4%BA%A7%E8%BD%A6%E8%BE%86%E7%9A%84%E5%AE%89%E6%8E%92%E7%AE%97%E6%B3%95/)

### [基础不等式 - Changkun's Blog](https://blog.changkun.de/_todo/%E5%9F%BA%E7%A1%80%E4%B8%8D%E7%AD%89%E5%BC%8F/)

### [对最优漂流调度方案的研究（Study on the Optimal Scheduling Scheme for Drifting） - Changkun's Blog](https://blog.changkun.de/_todo/%E5%AF%B9%E6%9C%80%E4%BC%98%E6%BC%82%E6%B5%81%E8%B0%83%E5%BA%A6%E6%96%B9%E6%A1%88%E7%9A%84%E7%A0%94%E7%A9%B6study-on-the-optimal-scheduling-scheme-for-drifting/)

### [广义结合律 - Changkun's Blog](https://blog.changkun.de/_todo/%E5%B9%BF%E4%B9%89%E7%BB%93%E5%90%88%E5%BE%8B/)

### [数论定理证明 - Changkun's Blog](https://blog.changkun.de/_todo/%E6%95%B0%E8%AE%BA%E5%AE%9A%E7%90%86%E8%AF%81%E6%98%8E/)

### [极大初等图像变换浅谈 - Changkun's Blog](https://blog.changkun.de/_todo/%E6%9E%81%E5%A4%A7%E5%88%9D%E7%AD%89%E5%9B%BE%E5%83%8F%E5%8F%98%E6%8D%A2%E6%B5%85%E8%B0%88/)

### [概率论值古典概率模型 - Changkun's Blog](https://blog.changkun.de/_todo/%E6%A6%82%E7%8E%87%E8%AE%BA%E5%80%BC%E5%8F%A4%E5%85%B8%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B/)

### [深圳人口增长及其对医疗需求影响的预测 - Changkun's Blog](https://blog.changkun.de/_todo/%E6%B7%B1%E5%9C%B3%E4%BA%BA%E5%8F%A3%E5%A2%9E%E9%95%BF%E5%8F%8A%E5%85%B6%E5%AF%B9%E5%8C%BB%E7%96%97%E9%9C%80%E6%B1%82%E5%BD%B1%E5%93%8D%E7%9A%84%E9%A2%84%E6%B5%8B/)

### [论十九世纪单值函数论的发展对流形思想演变的影响以及微分流形的未来 - Changkun's Blog](https://blog.changkun.de/_todo/%E8%AE%BA%E5%8D%81%E4%B9%9D%E4%B8%96%E7%BA%AA%E5%8D%95%E5%80%BC%E5%87%BD%E6%95%B0%E8%AE%BA%E7%9A%84%E5%8F%91%E5%B1%95%E5%AF%B9%E6%B5%81%E5%BD%A2%E6%80%9D%E6%83%B3%E6%BC%94%E5%8F%98%E7%9A%84%E5%BD%B1%E5%93%8D%E4%BB%A5%E5%8F%8A%E5%BE%AE%E5%88%86%E6%B5%81%E5%BD%A2%E7%9A%84%E6%9C%AA%E6%9D%A5/)

### [论预测模型 - Changkun's Blog](https://blog.changkun.de/_todo/%E8%AE%BA%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B/)

### [试用 Fermat 数 再证素数无穷 - Changkun's Blog](https://blog.changkun.de/_todo/%E8%AF%95%E7%94%A8-fermat-%E6%95%B0-%E5%86%8D%E8%AF%81%E7%B4%A0%E6%95%B0%E6%97%A0%E7%A9%B7/)

### [话说矩阵 定义新变换 赢得矩阵 - Changkun's Blog](https://blog.changkun.de/_todo/%E8%AF%9D%E8%AF%B4%E7%9F%A9%E9%98%B5-%E5%AE%9A%E4%B9%89%E6%96%B0%E5%8F%98%E6%8D%A2-%E8%B5%A2%E5%BE%97%E7%9F%A9%E9%98%B5/)

### [这些年。 - Changkun's Blog](https://blog.changkun.de/_todo/%E8%BF%99%E4%BA%9B%E5%B9%B4/)

### [饮酒与驾车关系的微分方程模型分析与评价 - Changkun's Blog](https://blog.changkun.de/_todo/%E9%A5%AE%E9%85%92%E4%B8%8E%E9%A9%BE%E8%BD%A6%E5%85%B3%E7%B3%BB%E7%9A%84%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%E6%A8%A1%E5%9E%8B%E5%88%86%E6%9E%90%E4%B8%8E%E8%AF%84%E4%BB%B7/)

### [DockOne翻译列表 - DockOne.io](http://dockone.io/question/4)

### [几天前创建的容器能够使用docker exec 进入，运行几天之后docker exec 无法进入 - DockOne.io](http://dockone.io/question/1541)

### [使用eureka的微服务如何平滑的使用k8s的负载均衡？ - DockOne.io](http://dockone.io/question/4076)

### [Faas平台开发工程师招聘 - DockOne.io](http://dockone.io/question/4075)

### [joywork DevOps工程师招聘 - DockOne.io](http://dockone.io/question/4074)

### [docker容器内root用户的一些问题 - DockOne.io](http://dockone.io/question/4073)

### [实现微服务的高可用 - DockOne.io](http://dockone.io/question/4072)

### [时速云获B+轮战略融资，加速容器云PaaS规模化增长 - DockOne.io](http://dockone.io/question/4071)

### [Cloud Native Patterns 云原生模式 - DockOne.io](http://dockone.io/question/4070)

### [本地docker-compose创建的加入了同一个虚拟网络的容器，在宿主机里怎么访问？ - DockOne.io](http://dockone.io/question/4069)

### [docker单点故障问题 - DockOne.io](http://dockone.io/question/4068)

### [Docker Weekly 不维护了吗？ - DockOne.io](http://dockone.io/question/4067)

### [Docker安全性讨论 - DockOne.io](http://dockone.io/question/4065)

### [docker中文文档挂了? - DockOne.io](http://dockone.io/question/4064)

### [pod在mount到宿主机的时候如何路径上带上podname - DockOne.io](http://dockone.io/question/4063)

### [【深圳】环球易购招聘容器云leader（基于Kubernetes的Paas平台） - DockOne.io](http://dockone.io/question/4062)

### [k8s搭建zookeeper 失败， - DockOne.io](http://dockone.io/question/4061)

### [rancher: Failed to obtain metrics. The metrics service may not be available. - DockOne.io](http://dockone.io/question/4058)

### [devicemapper存储，容器mount点的问题 - DockOne.io](http://dockone.io/question/340)

### [k8s 中在自定义的 namespace 中创建 ingress 后不能访问 - DockOne.io](http://dockone.io/question/4057)

### [Upcoming Sokol header API changes (Feb 2021)](https://floooh.github.io/2021/02/07/sokol-api-overhaul.html)

### [Automatic Language Bindings](https://floooh.github.io/2020/08/23/sokol-bindgen.html)

### [Sokol headers: spring 2020 update](https://floooh.github.io/2020/04/26/sokol-spring-2020-update.html)

### [sokol_gfx.h Backend Tour: Metal](https://floooh.github.io/2020/02/20/sokol-gfx-backend-tour-metal.html)

### [sokol_gfx.h Backend Tour: D3D11](https://floooh.github.io/2020/02/18/sokol-gfx-backend-tour-d3d11.html)

### [sokol_gfx.h Backend Tour: OpenGL](https://floooh.github.io/2020/02/17/sokol-gfx-backend-tour-gl.html)

### [A new cycle-stepped 6502 CPU emulator](https://floooh.github.io/2019/12/13/cycle-stepped-6502.html)

### [Modern C for C++ Peeps](https://floooh.github.io/2019/09/27/modern-c-for-cpp-peeps.html)

### [A small sokol_gfx.h API update](https://floooh.github.io/2019/01/12/sokol-apply-pipeline.html)

### [Emulators as embedded file viewers](https://floooh.github.io/2019/01/05/wasm-embedding.html)

### [Reasons why bugs might feel "impossible"](https://jvns.ca/blog/2021/06/08/reasons-why-bugs-might-feel-impossible/)

### [You can now buy print version of my zines!](https://jvns.ca/blog/2021/06/02/you-can-now-buy-print-version-of-my-zines-/)

### [Blog about what you've struggled with](https://jvns.ca/blog/2021/05/24/blog-about-what-you-ve-struggled-with/)

### [How to look at the stack with gdb](https://jvns.ca/blog/2021/05/17/how-to-look-at-the-stack-in-gdb/)

### [The OSI model doesn't map well to TCP/IP](https://jvns.ca/blog/2021/05/11/what-s-the-osi-model-/)

### [I put all of my comics online!](https://jvns.ca/blog/2021/05/02/publishing-comics/)

### [Notes on building debugging puzzles](https://jvns.ca/blog/2021/04/16/notes-on-debugging-puzzles/)

### [What problems do people solve with strace?](https://jvns.ca/blog/2021/04/03/what-problems-do-people-solve-with-strace/)

### [A tool to spy on your DNS queries: dnspeep](https://jvns.ca/blog/2021/03/31/dnspeep-tool/)

### [Get better at programming by learning how things work](https://jvns.ca/blog/learn-how-things-work/)

### [Things your manager might not know](https://jvns.ca/blog/things-your-manager-might-not-know/)

### [A little tool to make DNS queries](https://jvns.ca/blog/2021/02/24/a-little-tool-to-make-dns-queries/)

### [Firecracker: start a VM in less than a second](https://jvns.ca/blog/2021/01/23/firecracker--start-a-vm-in-less-than-a-second/)

### [Server-sent events: a simple way to stream events from a server](https://jvns.ca/blog/2021/01/12/day-36--server-sent-events-are-cool--and-a-fun-bug/)

### [Daily blog posts about my time at RC](https://jvns.ca/blog/2021/01/08/some-extra-daily-blog-posts/)

### [Docker Compose: a nice way to set up a dev environment](https://jvns.ca/blog/2021/01/04/docker-compose-is-nice/)

### [2020: Year in review](https://jvns.ca/blog/2020/12/31/2020--year-in-review/)

### [How I write useful programming comics](https://jvns.ca/blog/2020/12/05/how-i-write-useful-programming-comics/)

### [An attempt at implementing char-rnn with PyTorch](https://jvns.ca/blog/2020/11/30/implement-char-rnn-in-pytorch/)

### [New zine: Hell Yes! CSS!](https://jvns.ca/blog/2020/11/22/new-zine--hell-yes--css-/)

### [What is Data Parallel C++ | CodeGuru.com](http://www.codeguru.com/cpp/cpp/what-is-data-parallel-c.html)

### [Pure Virtual C++ Event Summary | CodeGuru.com](http://www.codeguru.com/cpp/pure-virtual-c-event-summary.html)

### [Visual Studio vs. Visual Studio Code](http://www.codeguru.com/cpp/v-s/visual-studio-vs.-visual-studio-code.html)

### [CLinkedList Doubly Linked List Class](http://www.codeguru.com/cpp/cpp/algorithms/lists/article.php/c5121/CLinkedList-Doubly-Linked-List-Class.htm)

### [[2107.01381] Recent Advancements In Distributed System Communications](http://arxiv.org/abs/2107.01381)

### [[2107.01398] TrafPy: Benchmarking Data Centre Network Systems](http://arxiv.org/abs/2107.01398)

### [[2107.01427] Multi-Objective Congestion Control](http://arxiv.org/abs/2107.01427)

### [[2107.01914] Ranking Online Social Users by their Influence](http://arxiv.org/abs/2107.01914)

### [[1910.06895] CRISLoc: Reconstructable CSI Fingerprintingfor Indoor Smartphone Localization](http://arxiv.org/abs/1910.06895)

### [[2104.04572] Smart and Secure CAV Networks Empowered by AI-Enabled Blockchain: Next Frontier for Intelligent Safe-Driving Assessment](http://arxiv.org/abs/2104.04572)

### [[2104.10533] On the Path to 6G: Low Orbit is the New High](http://arxiv.org/abs/2104.10533)

### [[2107.01003] PI$^2$ Parameters](http://arxiv.org/abs/2107.01003)

### [[2104.03044] A First Look into the Structural Properties and Resilience of Blockchain Overlays](http://arxiv.org/abs/2104.03044)

### [[2107.01214] Truncated Marginal Neural Ratio Estimation](http://arxiv.org/abs/2107.01214)

### [[2107.01238] Solving Machine Learning Problems](http://arxiv.org/abs/2107.01238)

### [[2107.01253] Designing Machine Learning Pipeline Toolkit for AutoML Surrogate Modeling Optimization](http://arxiv.org/abs/2107.01253)

### [[2107.01264] Beyond Value-Function Gaps: Improved Instance-Dependent Regret Bounds for Episodic Reinforcement Learning](http://arxiv.org/abs/2107.01264)

### [[2107.01269] Dual Causal/Non-Causal Self-Attention for Streaming End-to-End Speech Recognition](http://arxiv.org/abs/2107.01269)

### [[2107.01272] Physics-Guided Deep Learning for Dynamical Systems: A survey](http://arxiv.org/abs/2107.01272)

### [[2107.01273] Visual Time Series Forecasting: An Image-driven Approach](http://arxiv.org/abs/2107.01273)

### [[2107.01275] Relaxed Attention: A Simple Method to Boost Performance of End-to-End Automatic Speech Recognition](http://arxiv.org/abs/2107.01275)

### [[2107.01277] Non-Comparative Fairness for Human-Auditing and Its Relation to Traditional Fairness Notions](http://arxiv.org/abs/2107.01277)

### [[2107.01281] Prescient teleoperation of humanoid robots](http://arxiv.org/abs/2107.01281)

### [[2107.01285] Optimizing ROC Curves with a Sort-Based Surrogate Loss Function for Binary Classification and Changepoint Detection](http://arxiv.org/abs/2107.01285)

### [[2107.01296] Subspace Clustering Based Analysis of Neural Networks](http://arxiv.org/abs/2107.01296)

### [[2107.01301] Implicit Greedy Rank Learning in Autoencoders via Overparameterized Linear Networks](http://arxiv.org/abs/2107.01301)

### [[2107.01303] Data-driven mapping between functional connectomes using optimal transport](http://arxiv.org/abs/2107.01303)

### [[2107.01310] Clustering of Time Series Data with Prior Geographical Information](http://arxiv.org/abs/2107.01310)

### [[2107.01319] Learning Hierarchical Graph Neural Networks for Image Clustering](http://arxiv.org/abs/2107.01319)

### [[2107.01323] Minimum Wasserstein Distance Estimator under Finite Location-scale Mixtures](http://arxiv.org/abs/2107.01323)

### [[2107.01325] Fair Decision Rules for Binary Classification](http://arxiv.org/abs/2107.01325)

### [[2107.01326] SHORING: Design Provable Conditional High-Order Interaction Network via Symbolic Testing](http://arxiv.org/abs/2107.01326)

### [[2107.01330] SPI-GAN: Towards Single-Pixel Imaging through Generative Adversarial Network](http://arxiv.org/abs/2107.01330)

### [[2107.01333] A Uniformly Consistent Estimator of non-Gaussian Causal Effects Under the k-Triangle-Faithfulness Assumption](http://arxiv.org/abs/2107.01333)

### [[2107.01335] Average-Case Communication Complexity of Statistical Problems](http://arxiv.org/abs/2107.01335)

### [[2107.01337] CT Image Harmonization for Enhancing Radiomics Studies](http://arxiv.org/abs/2107.01337)

### [[2107.01338] Sibling Regression for Generalized Linear Models](http://arxiv.org/abs/2107.01338)

### [[2107.01343] Short-term probabilistic photovoltaic power forecast based on deep convolutional long short-term memory network and kernel density estimation](http://arxiv.org/abs/2107.01343)

### [[2107.01345] Cluster Representatives Selection in Non-Metric Spaces for Nearest Prototype Classification](http://arxiv.org/abs/2107.01345)

### [[2107.01347] Traffic Signal Control with Communicative Deep Reinforcement Learning Agents: a Case Study](http://arxiv.org/abs/2107.01347)

### [[2107.01348] Examining average and discounted reward optimality criteria in reinforcement learning](http://arxiv.org/abs/2107.01348)

### [[2107.01349] Split-and-Bridge: Adaptable Class Incremental Learning within a Single Neural Network](http://arxiv.org/abs/2107.01349)

### [[2107.01353] Spatiotemporal convolutional network for time-series prediction and causal inference](http://arxiv.org/abs/2107.01353)

### [[2107.01354] Pool of Experts: Realtime Querying Specialized Knowledge in Massive Neural Networks](http://arxiv.org/abs/2107.01354)

### [[2107.01358] CInC Flow: Characterizable Invertible 3x3 Convolution](http://arxiv.org/abs/2107.01358)

### [[2107.01360] Supervised Off-Policy Ranking](http://arxiv.org/abs/2107.01360)

### [[2107.01366] Can Transformers Jump Around Right in Natural Language? Assessing Performance Transfer from SCAN](http://arxiv.org/abs/2107.01366)

### [[2107.01372] Learning Debiased Representation via Disentangled Feature Augmentation](http://arxiv.org/abs/2107.01372)

### [[2107.01390] Memory and attention in deep learning](http://arxiv.org/abs/2107.01390)

### [[2107.01392] WisdomNet: Prognosis of COVID-19 with Slender Prospect of False Negative Cases and Vaticinating the Probability of Maturation to ARDS using Posteroanterior Chest X-Rays](http://arxiv.org/abs/2107.01392)

### [[2107.01400] Exact Backpropagation in Binary Weighted Networks with Group Weight Transformations](http://arxiv.org/abs/2107.01400)

### [[2107.01407] Where is the Grass Greener? Revisiting Generalized Policy Iteration for Offline Reinforcement Learning](http://arxiv.org/abs/2107.01407)

### [[2107.01408] Scale Mixtures of Neural Network Gaussian Processes](http://arxiv.org/abs/2107.01408)

### [[2107.01410] Maximum Entropy Weighted Independent Set Pooling for Graph Neural Networks](http://arxiv.org/abs/2107.01410)

### [[2107.01412] Isotonic Data Augmentation for Knowledge Distillation](http://arxiv.org/abs/2107.01412)

### [[2107.01460] Mava: a research framework for distributed multi-agent reinforcement learning](http://arxiv.org/abs/2107.01460)

### [[2107.01461] A Lottery Ticket Hypothesis Framework for Low-Complexity Device-Robust Neural Acoustic Scene Classification](http://arxiv.org/abs/2107.01461)

### [[2107.01466] A convolutional neural network for prestack fracture detection](http://arxiv.org/abs/2107.01466)

### [[2107.01473] Slope and generalization properties of neural networks](http://arxiv.org/abs/2107.01473)

### [[2107.01475] Privacy-Preserving Representation Learning on Graphs: A Mutual Information Perspective](http://arxiv.org/abs/2107.01475)

### [[2107.01477] Byzantine-robust Federated Learning through Spatial-temporal Analysis of Local Model Updates](http://arxiv.org/abs/2107.01477)

### [[2107.01495] On Positional and Structural Node Features for Graph Neural Networks on Non-attributed Graphs](http://arxiv.org/abs/2107.01495)

### [[2107.01499] BAGUA: Scaling up Distributed Learning with System Relaxations](http://arxiv.org/abs/2107.01499)

### [[2107.01502] Pulmonary Vessel Segmentation based on Orthogonal Fused U-Net++ of Chest CT Images](http://arxiv.org/abs/2107.01502)

### [[2107.01509] Bayesian decision-making under misspecified priors with applications to meta-learning](http://arxiv.org/abs/2107.01509)

### [[2107.01516] Improved Representation Learning for Session-based Recommendation](http://arxiv.org/abs/2107.01516)

### [[2107.01525] AdaL: Adaptive Gradient Transformation Contributes to Convergences and Generalizations](http://arxiv.org/abs/2107.01525)

### [[2107.01528] Incorporating Reachability Knowledge into a Multi-Spatial Graph Convolution Based Seq2Seq Model for Traffic Forecasting](http://arxiv.org/abs/2107.01528)

### [[2107.01529] Learning Complex Users' Preferences for Recommender Systems](http://arxiv.org/abs/2107.01529)

### [[2107.01557] Leveraging Evidential Deep Learning Uncertainties with Graph-based Clustering to Detect Anomalies](http://arxiv.org/abs/2107.01557)

### [[2107.01559] Smoothed Differential Privacy](http://arxiv.org/abs/2107.01559)

### [[2107.01561] Certifiably Robust Interpretation via Renyi Differential Privacy](http://arxiv.org/abs/2107.01561)

### [[2107.01562] Random Neural Networks in the Infinite Width Limit as Gaussian Processes](http://arxiv.org/abs/2107.01562)

### [[2107.01569] Cross-Modal Transformer-Based Neural Correction Models for Automatic Speech Recognition](http://arxiv.org/abs/2107.01569)

### [[2107.01590] Deep Gaussian Process Emulation using Stochastic Imputation](http://arxiv.org/abs/2107.01590)

### [[2107.01595] Learning in nonatomic games, Part I: Finite action spaces and population games](http://arxiv.org/abs/2107.01595)

### [[2107.01598] Domain Adaptation for Sentiment Analysis Using Increased Intraclass Separation](http://arxiv.org/abs/2107.01598)

### [[2107.01606] A Comparison of the Delta Method and the Bootstrap in Deep Learning Classification](http://arxiv.org/abs/2107.01606)

### [[2107.01614] Survey: Leakage and Privacy at Inference Time](http://arxiv.org/abs/2107.01614)

### [[2107.01615] A Typology of Data Anomalies](http://arxiv.org/abs/2107.01615)

### [[2107.01620] Auxiliary-Classifier GAN for Malware Analysis](http://arxiv.org/abs/2107.01620)

### [[2107.01622] Multiple-criteria Based Active Learning with Fixed-size Determinantal Point Processes](http://arxiv.org/abs/2107.01622)

### [[2107.01627] Machine Learning for Malware Evolution Detection](http://arxiv.org/abs/2107.01627)

### [[2107.01629] The Role of "Live" in Livestreaming Markets: Evidence Using Orthogonal Random Forest](http://arxiv.org/abs/2107.01629)

### [[2107.01641] A Theoretical Analysis of Fine-tuning with Linear Teachers](http://arxiv.org/abs/2107.01641)

### [[2107.01650] Learning ODEs via Diffeomorphisms for Fast and Robust Integration](http://arxiv.org/abs/2107.01650)

### [[2107.01655] Attribute-aware Explainable Complementary Clothing Recommendation](http://arxiv.org/abs/2107.01655)

### [[2107.01657] Class Introspection: A Novel Technique for Detecting Unlabeled Subclasses by Leveraging Classifier Explainability Methods](http://arxiv.org/abs/2107.01657)

### [[2107.01658] Learning Bayesian Networks through Birkhoff Polytope: A Relaxation Method](http://arxiv.org/abs/2107.01658)

### [[2107.01677] Low-Dimensional State and Action Representation Learning with MDP Homomorphism Metrics](http://arxiv.org/abs/2107.01677)

### [[2107.01689] Robust Restless Bandits: Tackling Interval Uncertainty with Deep Reinforcement Learning](http://arxiv.org/abs/2107.01689)

### [[2107.01702] Data-Driven Learning of Feedforward Neural Networks with Different Activation Functions](http://arxiv.org/abs/2107.01702)

### [[2107.01705] Randomized Neural Networks for Forecasting Time Series with Multiple Seasonality](http://arxiv.org/abs/2107.01705)

### [[2107.01707] Towards Scheduling Federated Deep Learning using Meta-Gradients for Inter-Hospital Learning](http://arxiv.org/abs/2107.01707)

### [[2107.01711] Autoencoder based Randomized Learning of Feedforward Neural Networks for Regression](http://arxiv.org/abs/2107.01711)

### [[2107.01726] Adaptive calibration for binary classification](http://arxiv.org/abs/2107.01726)

### [[2107.01734] Latent structure blockmodels for Bayesian spectral graph clustering](http://arxiv.org/abs/2107.01734)

### [[2107.01739] KAISA: An Adaptive Second-order Optimizer Framework for Deep Neural Networks](http://arxiv.org/abs/2107.01739)

### [[2107.01752] Polymorphic dynamic programming by algebraic shortcut fusion](http://arxiv.org/abs/2107.01752)

### [[2107.01757] The Least Restriction for Offline Reinforcement Learning](http://arxiv.org/abs/2107.01757)

### [[2107.01760] Single Model for Influenza Forecasting of Multiple Countries by Multi-task Learning](http://arxiv.org/abs/2107.01760)

### [[2107.01777] Statistical Theory for Imbalanced Binary Classification](http://arxiv.org/abs/2107.01777)

### [[2107.01782] A contextual analysis of multi-layer perceptron models in classifying hand-written digits and letters: limited resources](http://arxiv.org/abs/2107.01782)

### [[2107.01784] Learning a Model for Inferring a Spatial Road Lane Network Graph using Self-Supervision](http://arxiv.org/abs/2107.01784)

### [[2107.01799] An Information-Theoretic Approach for Automatically Determining the Number of States when Aggregating Markov Chains](http://arxiv.org/abs/2107.01799)

### [[2107.01804] Randomized Dimensionality Reduction for Facility Location and Single-Linkage Clustering](http://arxiv.org/abs/2107.01804)

### [[2107.01806] A Framework for Evaluating the Cybersecurity Risk of Real World, Machine Learning Production Systems](http://arxiv.org/abs/2107.01806)

### [[2107.01807] Q-SpiNN: A Framework for Quantizing Spiking Neural Networks](http://arxiv.org/abs/2107.01807)

### [[2107.01808] Why is Pruning at Initialization Immune to Reinitializing and Shuffling?](http://arxiv.org/abs/2107.01808)

### [[2107.01809] Boosting Transferability of Targeted Adversarial Examples via Hierarchical Generative Networks](http://arxiv.org/abs/2107.01809)

### [[2107.01820] An Explainable AI System for the Diagnosis of High Dimensional Biomedical Data](http://arxiv.org/abs/2107.01820)

### [[2107.01825] Sample Efficient Reinforcement Learning via Model-Ensemble Exploration and Exploitation](http://arxiv.org/abs/2107.01825)

### [[2107.01830] ARM-Net: Adaptive Relation Modeling Network for Structured Data](http://arxiv.org/abs/2107.01830)

### [[2107.01832] Provable Convergence of Nesterov Accelerated Method for Over-Parameterized Neural Networks](http://arxiv.org/abs/2107.01832)

### [[2107.01835] Fast Rate Learning in Stochastic First Price Bidding](http://arxiv.org/abs/2107.01835)

### [[2107.01848] Differentially Private Sliced Wasserstein Distance](http://arxiv.org/abs/2107.01848)

### [[2107.01850] Matching a Desired Causal State via Shift Interventions](http://arxiv.org/abs/2107.01850)

### [[2107.01854] Poisoning Attack against Estimating from Pairwise Comparisons](http://arxiv.org/abs/2107.01854)

### [[2107.01856] Winning at Any Cost -- Infringing the Cartel Prohibition With Reinforcement Learning](http://arxiv.org/abs/2107.01856)

### [[2107.01858] Automating Generative Deep Learning for Artistic Purposes: Challenges and Opportunities](http://arxiv.org/abs/2107.01858)

### [[2107.01863] On the Efficiency of Various Deep Transfer Learning Models in Glitch Waveform Detection in Gravitational-Wave Data](http://arxiv.org/abs/2107.01863)

### [[2107.01873] Detecting Concept Drift With Neural Network Model Uncertainty](http://arxiv.org/abs/2107.01873)

### [[2107.01875] DeepRapper: Neural Rap Generation with Rhyme and Rhythm Modeling](http://arxiv.org/abs/2107.01875)

### [[2107.01876] Causally Invariant Predictor with Shift-Robustness](http://arxiv.org/abs/2107.01876)

### [[2107.01881] Robust Online Convex Optimization in the Presence of Outliers](http://arxiv.org/abs/2107.01881)

### [[2107.01892] NOTE: Solution for KDD-CUP 2021 WikiKG90M-LSC](http://arxiv.org/abs/2107.01892)

### [[2107.01894] Automated Recovery of Issue-Commit Links Leveraging Both Textual and Non-textual Data](http://arxiv.org/abs/2107.01894)

### [[2107.01895] Optimizing the Numbers of Queries and Replies in Federated Learning with Differential Privacy](http://arxiv.org/abs/2107.01895)

### [[2107.01900] On The Distribution of Penultimate Activations of Classification Networks](http://arxiv.org/abs/2107.01900)

### [[2107.01904] Ensemble and Auxiliary Tasks for Data-Efficient Deep Reinforcement Learning](http://arxiv.org/abs/2107.01904)

### [[2107.01906] The Last-Iterate Convergence Rate of Optimistic Mirror Descent in Stochastic Variational Inequalities](http://arxiv.org/abs/2107.01906)

### [[2107.01927] Android Malware Category and Family Detection and Identification using Machine Learning](http://arxiv.org/abs/2107.01927)

### [[2107.01936] Adversarial Robustness of Probabilistic Network Embedding for Link Prediction](http://arxiv.org/abs/2107.01936)

### [[2107.01943] When and How to Fool Explainable Models (and Humans) with Adversarial Examples](http://arxiv.org/abs/2107.01943)

### [[2107.01952] Partition and Code: learning how to compress graphs](http://arxiv.org/abs/2107.01952)

### [[2107.01955] Detecting Faults during Automatic Screwdriving: A Dataset and Use Case of Anomaly Detection for Automatic Screwdriving](http://arxiv.org/abs/2107.01955)

### [[2107.01959] Universal Approximation of Functions on Sets](http://arxiv.org/abs/2107.01959)

### [[2107.01969] The MineRL BASALT Competition on Learning from Human Feedback](http://arxiv.org/abs/2107.01969)

### [[2107.01979] Machine Learning for Fraud Detection in E-Commerce: A Research Agenda](http://arxiv.org/abs/2107.01979)

### [[2107.01983] Imputation-Free Learning from Incomplete Observations](http://arxiv.org/abs/2107.01983)

### [[2107.01988] UCSL : A Machine Learning Expectation-Maximization framework for Unsupervised Clustering driven by Supervised Learning](http://arxiv.org/abs/2107.01988)

### [[2107.01994] Template-Based Graph Clustering](http://arxiv.org/abs/2107.01994)

### [[2107.01996] Explainability via Interactivity? Supporting Nonexperts' Sensemaking of Pretrained CNN by Interacting with Their Daily Surroundings](http://arxiv.org/abs/2107.01996)

### [[1602.03822] A Critical Connectivity Radius for Randomly-Generated, High Dimensional Data Points](http://arxiv.org/abs/1602.03822)

### [[1807.04209] Differentially Private False Discovery Rate Control](http://arxiv.org/abs/1807.04209)

### [[1902.09434] S-TRIGGER: Continual State Representation Learning via Self-Triggered Generative Replay](http://arxiv.org/abs/1902.09434)

### [[1905.01489] WoodScape: A multi-task, multi-camera fisheye dataset for autonomous driving](http://arxiv.org/abs/1905.01489)

### [[1905.10488] GAN2GAN: Generative Noise Learning for Blind Denoising with Single Noisy Images](http://arxiv.org/abs/1905.10488)

### [[1906.04675] Taxonomy of Saliency Metrics for Channel Pruning](http://arxiv.org/abs/1906.04675)

### [[1910.02325] Bayesian Learning-Based Adaptive Control for Safety Critical Systems](http://arxiv.org/abs/1910.02325)

### [[1911.02903] How Implicit Regularization of ReLU Neural Networks Characterizes the Learned Function -- Part I: the 1-D Case of Two Layers with Random First Layer](http://arxiv.org/abs/1911.02903)

### [[1911.04872] Two Ridge Solutions for the Incremental Broad Learning System on Added Nodes](http://arxiv.org/abs/1911.04872)

### [[2001.04463] Unsupervised Audiovisual Synthesis via Exemplar Autoencoders](http://arxiv.org/abs/2001.04463)

### [[2001.07248] SGLB: Stochastic Gradient Langevin Boosting](http://arxiv.org/abs/2001.07248)

### [[2001.08603] Learning Distributional Programs for Relational Autocompletion](http://arxiv.org/abs/2001.08603)

### [[2002.00291] Oracle Lower Bounds for Stochastic Gradient Sampling Algorithms](http://arxiv.org/abs/2002.00291)

### [[2002.09650] Learning Cost Functions for Optimal Transport](http://arxiv.org/abs/2002.09650)

### [[2005.08081] Layer-Wise Cross-View Decoding for Sequence-to-Sequence Learning](http://arxiv.org/abs/2005.08081)

### [[2005.09310] Distilling Knowledge from Ensembles of Acoustic Models for Joint CTC-Attention End-to-End Speech Recognition](http://arxiv.org/abs/2005.09310)

### [[2005.10190] Feature Purification: How Adversarial Training Performs Robust Deep Learning](http://arxiv.org/abs/2005.10190)

### [[2005.10696] Novel Policy Seeking with Constrained Optimization](http://arxiv.org/abs/2005.10696)

### [[2006.00492] BiERU: Bidirectional Emotional Recurrent Unit for Conversational Sentiment Analysis](http://arxiv.org/abs/2006.00492)

### [[2006.05900] All Local Minima are Global for Two-Layer ReLU Neural Networks: The Hidden Convex Optimization Landscape](http://arxiv.org/abs/2006.05900)

### [[2006.06600] Zeroth-Order Supervised Policy Improvement](http://arxiv.org/abs/2006.06600)

### [[2006.07616] SDCOR: Scalable Density-based Clustering for Local Outlier Detection in Massive-Scale Datasets](http://arxiv.org/abs/2006.07616)

### [[2006.07796] Structure by Architecture: Disentangled Representations without Regularization](http://arxiv.org/abs/2006.07796)

### [[2006.09252] Improving Graph Neural Network Expressivity via Subgraph Isomorphism Counting](http://arxiv.org/abs/2006.09252)

### [[2006.10621] On the Predictability of Pruning Across Scales](http://arxiv.org/abs/2006.10621)

### [[2006.11918] MaxVA: Fast Adaptation of Step Sizes by Maximizing Observed Variance of Gradients](http://arxiv.org/abs/2006.11918)

### [[2006.12655] Perceptual Adversarial Robustness: Defense Against Unseen Threat Models](http://arxiv.org/abs/2006.12655)

### [[2006.15714] Active Finite Reward Automaton Inference and Reinforcement Learning Using Queries and Counterexamples](http://arxiv.org/abs/2006.15714)

### [[2006.16161] A Two-step Surface-based 3D Deep Learning Pipeline for Segmentation of Intracranial Aneurysms](http://arxiv.org/abs/2006.16161)

### [[2006.16785] Lipschitzness Is All You Need To Tame Off-policy Generative Adversarial Imitation Learning](http://arxiv.org/abs/2006.16785)

### [[2007.01099] Reinforcement Learning and its Connections with Neuroscience and Psychology](http://arxiv.org/abs/2007.01099)

### [[2007.08243] Lottery Tickets in Linear Models: An Analysis of Iterative Magnitude Pruning](http://arxiv.org/abs/2007.08243)

### [[2007.08864] Sparse Linear Networks with a Fixed Butterfly Structure: Theory and Practice](http://arxiv.org/abs/2007.08864)

### [[2007.14209] Langevin Monte Carlo: random coordinate descent and variance reduction](http://arxiv.org/abs/2007.14209)

### [[2008.01393] Neural Granular Sound Synthesis](http://arxiv.org/abs/2008.01393)

### [[2008.10208] Multi-view Graph Learning by Joint Modeling of Consistency and Inconsistency](http://arxiv.org/abs/2008.10208)

### [[2008.13578] Against Membership Inference Attack: Pruning is All You Need](http://arxiv.org/abs/2008.13578)

### [[2009.02755] Anomaly Detection With Partitioning Overfitting Autoencoder Ensembles](http://arxiv.org/abs/2009.02755)

### [[2009.03228] Information Theoretic Meta Learning with Gaussian Processes](http://arxiv.org/abs/2009.03228)

### [[2009.03671] A Self-Supervised Gait Encoding Approach with Locality-Awareness for 3D Skeleton Based Person Re-Identification](http://arxiv.org/abs/2009.03671)

### [[2010.00378] GraphXCOVID: Explainable Deep Graph Diffusion Pseudo-Labelling for Identifying COVID-19 on Chest X-rays](http://arxiv.org/abs/2010.00378)

### [[2010.01592] Unknown Presentation Attack Detection against Rational Attackers](http://arxiv.org/abs/2010.01592)

### [[2010.02358] VisualWordGrid: Information Extraction From Scanned Documents Using A Multimodal Approach](http://arxiv.org/abs/2010.02358)

### [[2010.04389] A Survey of Knowledge-Enhanced Text Generation](http://arxiv.org/abs/2010.04389)

### [[2010.08707] Constrained Motion Planning Networks X](http://arxiv.org/abs/2010.08707)

### [[2010.14672] How Does the Task Landscape Affect MAML Performance?](http://arxiv.org/abs/2010.14672)

### [[2011.07006] Federated Multi-Mini-Batch: An Efficient Training Approach to Federated Learning in Non-IID Environments](http://arxiv.org/abs/2011.07006)

### [[2012.02130] A similarity-based Bayesian mixture-of-experts model](http://arxiv.org/abs/2012.02130)

### [[2012.03292] FedSiam: Towards Adaptive Federated Semi-Supervised Learning](http://arxiv.org/abs/2012.03292)

### [[2012.06188] Recent Theoretical Advances in Non-Convex Optimization](http://arxiv.org/abs/2012.06188)

### [[2012.06244] The Implicit Bias for Adaptive Optimization Algorithms on Homogeneous Neural Networks](http://arxiv.org/abs/2012.06244)

### [[2012.06279] Autoencoding Slow Representations for Semi-supervised Data Efficient Regression](http://arxiv.org/abs/2012.06279)

### [[2012.07176] Extended Few-Shot Learning: Exploiting Existing Resources for Novel Tasks](http://arxiv.org/abs/2012.07176)

### [[2012.09816] Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning](http://arxiv.org/abs/2012.09816)

### [[2012.10033] Exploring Fluent Query Reformulations with Text-to-Text Transformers and Reinforcement Learning](http://arxiv.org/abs/2012.10033)

### [[2012.12561] GANDA: A deep generative adversarial network predicts the spatial distribution of nanoparticles in tumor pixelly](http://arxiv.org/abs/2012.12561)

### [[2012.13962] A Tutorial on Sparse Gaussian Processes and Variational Inference](http://arxiv.org/abs/2012.13962)

### [[2101.02931] Block-Term Tensor Decomposition Model Selection and Computation: The Bayesian Way](http://arxiv.org/abs/2101.02931)

### [[2102.02410] A Local Convergence Theory for Mildly Over-Parameterized Two-Layer Neural Network](http://arxiv.org/abs/2102.02410)

### [[2102.06462] Two Sides of the Same Coin: Heterophily and Oversmoothing in Graph Convolutional Neural Networks](http://arxiv.org/abs/2102.06462)

### [[2102.07845] MARINA: Faster Non-Convex Distributed Learning with Compression](http://arxiv.org/abs/2102.07845)

### [[2102.08201] Improper Reinforcement Learning with Gradient-based Policy Optimization](http://arxiv.org/abs/2102.08201)

### [[2102.08474] Adversarially Robust Kernel Smoothing](http://arxiv.org/abs/2102.08474)

### [[2102.12017] Annotating Motion Primitives for Simplifying Action Search in Reinforcement Learning](http://arxiv.org/abs/2102.12017)

### [[2102.12330] Re-Evaluating GermEval17 Using German Pre-Trained Language Models](http://arxiv.org/abs/2102.12330)

### [[2103.04379] Repurposing GANs for One-shot Semantic Part Segmentation](http://arxiv.org/abs/2103.04379)

### [[2103.06326] S4RL: Surprisingly Simple Self-Supervision for Offline Reinforcement Learning](http://arxiv.org/abs/2103.06326)

### [[2103.13355] Bag of Tricks for Node Classification with Graph Neural Networks](http://arxiv.org/abs/2103.13355)

### [[2103.15890] Learning Domain Invariant Representations for Generalizable Person Re-Identification](http://arxiv.org/abs/2103.15890)

### [[2103.15990] An Overview of Human Activity Recognition Using Wearable Sensors: Healthcare and Artificial Intelligence](http://arxiv.org/abs/2103.15990)

### [[2103.16525] Endo-Depth-and-Motion: Reconstruction and Tracking in Endoscopic Videos using Depth Networks and Photometric Constraints](http://arxiv.org/abs/2103.16525)

### [[2104.07279] COVID-19 detection using deep convolutional neural networks and binary-differential-algorithm-based feature selection on X-ray images](http://arxiv.org/abs/2104.07279)

### [[2104.10314] Efficient Sparse Coding using Hierarchical Riemannian Pursuit](http://arxiv.org/abs/2104.10314)

### [[2104.11557] Knodle: Modular Weakly Supervised Learning with PyTorch](http://arxiv.org/abs/2104.11557)

### [[2104.11824] Optimal Dynamic Regret in Exp-Concave Online Learning](http://arxiv.org/abs/2104.11824)

### [[2104.12437] Towards Rigorous Interpretations: a Formalisation of Feature Attribution](http://arxiv.org/abs/2104.12437)

### [[2105.00173] Emotion Recognition of the Singing Voice: Toward a Real-Time Analysis Tool for Singers](http://arxiv.org/abs/2105.00173)

### [[2105.03075] A Survey of Data Augmentation Approaches for NLP](http://arxiv.org/abs/2105.03075)

### [[2105.04030] A Bit More Bayesian: Domain-Invariant Learning with Uncertainty](http://arxiv.org/abs/2105.04030)

### [[2105.04963] Exploring a Handwriting Programming Language for Educational Robots](http://arxiv.org/abs/2105.04963)

### [[2105.07603] EasyFL: A Low-code Federated Learning Platform For Dummies](http://arxiv.org/abs/2105.07603)

### [[2105.07830] Learning to Relate Depth and Semantics for Unsupervised Domain Adaptation](http://arxiv.org/abs/2105.07830)

### [[2105.09856] High-Fidelity and Low-Latency Universal Neural Vocoder based on Multiband WaveRNN with Data-Driven Linear Prediction for Discrete Waveform Modeling](http://arxiv.org/abs/2105.09856)

### [[2105.09858] Low-Latency Real-Time Non-Parallel Voice Conversion based on Cyclic Variational Autoencoder and Multiband WaveRNN with Data-Driven Linear Prediction](http://arxiv.org/abs/2105.09858)

### [[2105.12807] XOmiVAE: an interpretable deep learning model for cancer classification using high-dimensional omics data](http://arxiv.org/abs/2105.12807)

### [[2105.13502] Unsupervised Domain Adaptation of Object Detectors: A Survey](http://arxiv.org/abs/2105.13502)

### [[2105.13783] Quantile Encoder: Tackling High Cardinality Categorical Features in Regression Problems](http://arxiv.org/abs/2105.13783)

### [[2105.15134] Toward Understanding the Feature Learning Process of Self-supervised Contrastive Learning](http://arxiv.org/abs/2105.15134)

### [[2106.02994] Learning Topology from Synthetic Data for Unsupervised Depth Completion](http://arxiv.org/abs/2106.02994)

### [[2106.04679] Self-Adaptive Swarm System (SASS)](http://arxiv.org/abs/2106.04679)

### [[2106.07250] Unified Interpretation of Softmax Cross-Entropy and Negative Sampling: With Case Study for Knowledge Graph Embedding](http://arxiv.org/abs/2106.07250)

### [[2106.07474] Discovering Interpretable Machine Learning Models in Parallel Coordinates](http://arxiv.org/abs/2106.07474)

### [[2106.07568] Full interpretable machine learning in 2D with inline coordinates](http://arxiv.org/abs/2106.07568)

### [[2106.08775] Momentum-inspired Low-Rank Coordinate Descent for Diagonally Constrained SDPs](http://arxiv.org/abs/2106.08775)

### [[2106.09989] BinarizedAttack: Structural Poisoning Attacks to Graph-based Anomaly Detection](http://arxiv.org/abs/2106.09989)

### [[2106.10333] Non-parametric Differentially Private Confidence Intervals for the Median](http://arxiv.org/abs/2106.10333)

### [[2106.10417] Variance-Dependent Best Arm Identification](http://arxiv.org/abs/2106.10417)

### [[2106.10558] Rayleigh-Gauss-Newton optimization with enhanced sampling for variational Monte Carlo](http://arxiv.org/abs/2106.10558)

### [[2106.11160] Effects of boundary conditions in fully convolutional networks for learning spatio-temporal dynamics](http://arxiv.org/abs/2106.11160)

### [[2106.11655] On Constrained Optimization in Differentiable Neural Architecture Search](http://arxiv.org/abs/2106.11655)

### [[2106.11929] Physics-Informed Deep Reversible Regression Model for Temperature Field Reconstruction of Heat-Source Systems](http://arxiv.org/abs/2106.11929)

### [[2106.12194] Uncertainty-Aware Model-Based Reinforcement Learning with Application to Autonomous Driving](http://arxiv.org/abs/2106.12194)

### [[2106.13122] Exploring Corruption Robustness: Inductive Biases in Vision Transformers and MLP-Mixers](http://arxiv.org/abs/2106.13122)

### [[2106.13884] Multimodal Few-Shot Learning with Frozen Language Models](http://arxiv.org/abs/2106.13884)

### [[2106.14344] Non-Exhaustive Learning Using Gaussian Mixture Generative Adversarial Networks](http://arxiv.org/abs/2106.14344)

### [[2106.14993] Modularity in Reinforcement Learning via Algorithmic Independence in Credit Assignment](http://arxiv.org/abs/2106.14993)

### [[2106.15338] Probabilistic Attention for Interactive Segmentation](http://arxiv.org/abs/2106.15338)

### [[2106.16036] A Generative Model for Raw Audio Using Transformer Architectures](http://arxiv.org/abs/2106.16036)

### [[2107.00946] Online Metro Origin-Destination Prediction via Heterogeneous Information Aggregation](http://arxiv.org/abs/2107.00946)

### [[2107.01725] Real-time Detection and Adaptive Mitigation of Power-based Side-Channel Leakage in SoC](http://arxiv.org/abs/2107.01725)

### [[2107.01857] Versatile and concurrent FPGA-based architecture for practical quantum communication systems](http://arxiv.org/abs/2107.01857)

### [[2106.04205] Micro BTB: A High Performance and Lightweight Last-Level Branch Target Buffer for Servers](http://arxiv.org/abs/2106.04205)

### [[2106.06433] FPGA-Based Near-Memory Acceleration of Modern Data-Intensive Applications](http://arxiv.org/abs/2106.06433)

### [[2107.01405] A Fuzzy Scheduling Strategy for Deadline-Based Workflow Applications in Uncertain Edge-Cloud Environments](http://arxiv.org/abs/2107.01405)

### [[2107.01542] The Semantics of Package Management via Event Structures](http://arxiv.org/abs/2107.01542)

### [[2107.01600] ETHTID: Deployable Threshold Information Disclosure on Ethereum](http://arxiv.org/abs/2107.01600)

### [[2107.01735] Cloud Versus Local Processing in Distributed Networks](http://arxiv.org/abs/2107.01735)

### [[2005.10435] Optimal Distributed Subsampling for Maximum Quasi-Likelihood Estimators with Massive Data](http://arxiv.org/abs/2005.10435)

### [[2009.10593] The Ultimate DataFlow for Ultimate SuperComputers-on-a-Chip, for Scientific Computing, Geo Physics, Complex Mathematics, and Information Processing](http://arxiv.org/abs/2009.10593)

### [[2010.07541] Byzantine-Resilient Federated Learning with Heterogeneous Data Distribution](http://arxiv.org/abs/2010.07541)

### [[2105.06571] Toward Real-time Analysis of Experimental Science Workloads on Geographically Distributed Supercomputers](http://arxiv.org/abs/2105.06571)

### [[2105.10798] On the Complexity and Parallel Implementation of Hensel's Lemma and Weierstrass Preparation](http://arxiv.org/abs/2105.10798)

### [[2105.13487] Multidimensional Byzantine Agreement in a Synchronous Setting](http://arxiv.org/abs/2105.13487)

### [[2106.04979] Benchmarking the Nvidia GPU Lineage: From Early K80 to Modern A100 with Asynchronous Memory Transfers](http://arxiv.org/abs/2106.04979)

### [[2106.05485] VaLiPro: Linear Programming Validator for Cluster Computing Systems](http://arxiv.org/abs/2106.05485)

### [[2107.01248] Data Uncertainty Guided Noise-aware Preprocessing Of Fingerprints](http://arxiv.org/abs/2107.01248)

### [[2107.01361] Sensor-invariant Fingerprint ROI Segmentation Using Recurrent Adversarial Learning](http://arxiv.org/abs/2107.01361)

### [[2107.01396] Demiguise Attack: Crafting Invisible Semantic Adversarial Perturbations with Perceptual Similarity](http://arxiv.org/abs/2107.01396)

### [[2107.01428] Solving Infinite-Domain CSPs Using the Patchwork Property](http://arxiv.org/abs/2107.01428)

### [[2107.01429] QKSA: Quantum Knowledge Seeking Agent](http://arxiv.org/abs/2107.01429)

### [[2107.01462] Development of a Conversation State Recognition System](http://arxiv.org/abs/2107.01462)

### [[2107.01496] A Data-Driven Method for Recognizing Automated Negotiation Strategies](http://arxiv.org/abs/2107.01496)

### [[2107.01621] The Composability of Intermediate Values in Composable Inductive Programming](http://arxiv.org/abs/2107.01621)

### [[2107.01654] Efficient Explanations for Knowledge Compilation Languages](http://arxiv.org/abs/2107.01654)

### [[2107.01656] IITP at WAT 2021: System description for English-Hindi Multimodal Translation Task](http://arxiv.org/abs/2107.01656)

### [[2107.01667] Low Dimensional State Representation Learning with Robotics Priors in Continuous Action Spaces](http://arxiv.org/abs/2107.01667)

### [[2107.01715] Improve Agents without Retraining: Parallel Tree Search with Off-Policy Correction](http://arxiv.org/abs/2107.01715)

### [[2107.01759] Learning Delaunay Triangulation using Self-attention and Domain Knowledge](http://arxiv.org/abs/2107.01759)

### [[2107.01776] Continual Contrastive Self-supervised Learning for Image Classification](http://arxiv.org/abs/2107.01776)

### [[2107.01829] A System for Traded Control Teleoperation of Manipulation Tasks using Intent Prediction from Hand Gestures](http://arxiv.org/abs/2107.01829)

### [[2107.01836] GraspME -- Grasp Manifold Estimator](http://arxiv.org/abs/2107.01836)

### [[2107.01867] Control of rough terrain vehicles using deep reinforcement learning](http://arxiv.org/abs/2107.01867)

### [[2107.01877] Faster-LTN: a neuro-symbolic, end-to-end object detection architecture](http://arxiv.org/abs/2107.01877)

### [[2107.01903] SM-SGE: A Self-Supervised Multi-Scale Skeleton Graph Encoding Framework for Person Re-Identification](http://arxiv.org/abs/2107.01903)

### [[2107.01905] Creating Unbiased Public Benchmark Datasets with Data Leakage Prevention for Predictive Process Monitoring](http://arxiv.org/abs/2107.01905)

### [[2107.01915] Logic Locking at the Frontiers of Machine Learning: A Survey on Developments and Opportunities](http://arxiv.org/abs/2107.01915)

### [[1805.03696] Bayeslands: A Bayesian inference approach for parameter uncertainty quantification in Badlands](http://arxiv.org/abs/1805.03696)

### [[1806.02127] Addendum to "HTN Acting: A Formalism and an Algorithm"](http://arxiv.org/abs/1806.02127)

### [[2004.01274] Does Comma Selection Help To Cope With Local Optima](http://arxiv.org/abs/2004.01274)

### [[2007.03727] TripMD: Driving patterns investigation via Motif Analysis](http://arxiv.org/abs/2007.03727)

### [[2009.14715] Learning Rewards from Linguistic Feedback](http://arxiv.org/abs/2009.14715)

### [[2101.00376] RiddleSense: Reasoning about Riddle Questions Featuring Linguistic Creativity and Commonsense Knowledge](http://arxiv.org/abs/2101.00376)

### [[2102.00621] Polyphone Disambiguition in Mandarin Chinese with Semi-Supervised Learning](http://arxiv.org/abs/2102.00621)

### [[2102.12010] PixSet : An Opportunity for 3D Computer Vision to Go Beyond Point Clouds With a Full-Waveform LiDAR Dataset](http://arxiv.org/abs/2102.12010)

### [[2103.00112] Transformer in Transformer](http://arxiv.org/abs/2103.00112)

### [[2103.11811] MasakhaNER: Named Entity Recognition for African Languages](http://arxiv.org/abs/2103.11811)

### [[2105.11763] Efficiently Explaining CSPs with Unsatisfiable Subset Optimization](http://arxiv.org/abs/2105.11763)

### [[2105.12846] General Game Heuristic Prediction Based on Ludeme Descriptions](http://arxiv.org/abs/2105.12846)

### [[2106.03706] A Comprehensive Assessment of Dialog Evaluation Metrics](http://arxiv.org/abs/2106.03706)

### [[2106.06944] SASICM A Multi-Task Benchmark For Subtext Recognition](http://arxiv.org/abs/2106.06944)

### [[2106.07932] Medical Code Prediction from Discharge Summary: Document to Sequence BERT using Sequence Attention](http://arxiv.org/abs/2106.07932)

### [[2106.08253] Code Generation Based on Deep Learning: a Brief Review](http://arxiv.org/abs/2106.08253)

### [[2106.09455] Conference proceedings KI4Industry AI for SMEs -- the online congress for practical entry into AI for SMEs](http://arxiv.org/abs/2106.09455)

### [[2106.13367] SeaNet -- Towards A Knowledge Graph Based Autonomic Management of Software Defined Networks](http://arxiv.org/abs/2106.13367)

### [[2106.15877] Experience-Driven PCG via Reinforcement Learning: A Super Mario Bros Study](http://arxiv.org/abs/2106.15877)

### [[2107.01183] Ethics Sheets for AI Tasks](http://arxiv.org/abs/2107.01183)

### [[2107.01194] How Incomplete is Contrastive Learning? An Inter-intra Variant Dual Representation Method for Self-supervised Video Recognition](http://arxiv.org/abs/2107.01194)

### [[2107.01295] Dependent Type Systems as Macros](http://arxiv.org/abs/2107.01295)

### [[2107.01815] A Formal Semantics of the GraalVM Intermediate Representation](http://arxiv.org/abs/2107.01815)

### [[2107.01883] A Theory of Higher-Order Subtyping with Type Intervals (Extended Version)](http://arxiv.org/abs/2107.01883)

### [[2102.00378] Model-Based Testing of Networked Applications](http://arxiv.org/abs/2102.00378)

### [[2103.03198] Catala: A Programming Language for the Law](http://arxiv.org/abs/2103.03198)

### [[2106.14938] Seeking Stability by being Lazy and Shallow](http://arxiv.org/abs/2106.14938)

### [<title>Name of XGBoost parameters in Java - XGBoost</title>](https://discuss.xgboost.ai/t/name-of-xgboost-parameters-in-java/2153/2)

### [<title>Found input variables with inconsistent numbers of samples - XGBoost</title>](https://discuss.xgboost.ai/t/found-input-variables-with-inconsistent-numbers-of-samples/1818/3)

### [<title>Is training multiple models in parallel threads supported (Python)? - RFC - XGBoost</title>](https://discuss.xgboost.ai/t/is-training-multiple-models-in-parallel-threads-supported-python/2348/1)

### [<title>Qudro 5000 graphic card proplems - XGBoost</title>](https://discuss.xgboost.ai/t/qudro-5000-graphic-card-proplems/2346/1)

### [<title>Performance between spark-xgb and python-xgb - XGBoost</title>](https://discuss.xgboost.ai/t/performance-between-spark-xgb-and-python-xgb/2344/1)

### [<title>Random data weighting - XGBoost</title>](https://discuss.xgboost.ai/t/random-data-weighting/2342/1)

### [<title>Nested cross validation - XGBoost</title>](https://discuss.xgboost.ai/t/nested-cross-validation/2340/1)

### [<title>Invalid Parameter format for seed expect int but value='RandomState(MT19937)' - XGBoost</title>](https://discuss.xgboost.ai/t/invalid-parameter-format-for-seed-expect-int-but-value-randomstate-mt19937/2338/1)

### [<title>Min_child_weight range - XGBoost</title>](https://discuss.xgboost.ai/t/min-child-weight-range/2335/1)

### [<title>Sample_weight behaviour - XGBoost</title>](https://discuss.xgboost.ai/t/sample-weight-behaviour/2334/1)

### [<title>XGBRanker predictions are negative - XGBoost</title>](https://discuss.xgboost.ai/t/xgbranker-predictions-are-negative/2329/2)

### [<title>When modeling time series data how do we give more weight to more recent data? - XGBoost</title>](https://discuss.xgboost.ai/t/when-modeling-time-series-data-how-do-we-give-more-weight-to-more-recent-data/1462/2)

### [<title>Why is subsample &gt;= 0.5 recommended? - XGBoost</title>](https://discuss.xgboost.ai/t/why-is-subsample-0-5-recommended/2332/1)

### [<title>XGBRanker predictions are negative - XGBoost</title>](https://discuss.xgboost.ai/t/xgbranker-predictions-are-negative/2329/1)

### [<title>XGBoost: How to add custom sampler (re: subsample)? - XGBoost</title>](https://discuss.xgboost.ai/t/xgboost-how-to-add-custom-sampler-re-subsample/2326/4)

### [<title>XGBoost: How to add custom sampler (re: subsample)? - XGBoost</title>](https://discuss.xgboost.ai/t/xgboost-how-to-add-custom-sampler-re-subsample/2326/3)

### [<title>XGBoost: How to add custom sampler (re: subsample)? - XGBoost</title>](https://discuss.xgboost.ai/t/xgboost-how-to-add-custom-sampler-re-subsample/2326/2)

### [<title>XGBoost: How to add custom sampler (re: subsample)? - XGBoost</title>](https://discuss.xgboost.ai/t/xgboost-how-to-add-custom-sampler-re-subsample/2326/1)

### [<title>How does XGBoost read file content inside the enclave? - XGBoost</title>](https://discuss.xgboost.ai/t/how-does-xgboost-read-file-content-inside-the-enclave/2323/3)

### [<title>Iteration_range = (0,0) - XGBoost</title>](https://discuss.xgboost.ai/t/iteration-range-0-0/2321/2)

### [<title>How does XGBoost read file content inside the enclave? - XGBoost</title>](https://discuss.xgboost.ai/t/how-does-xgboost-read-file-content-inside-the-enclave/2323/2)

### [<title>How does XGBoost read file content inside the enclave? - XGBoost</title>](https://discuss.xgboost.ai/t/how-does-xgboost-read-file-content-inside-the-enclave/2323/1)

### [<title>Iteration_range = (0,0) - XGBoost</title>](https://discuss.xgboost.ai/t/iteration-range-0-0/2321/1)

### [<title>Learning Rate and number of rounds - XGBoost</title>](https://discuss.xgboost.ai/t/learning-rate-and-number-of-rounds/2320/1)

### [<title>XGBRegressor, GammaRegression:label must be positive - XGBoost</title>](https://discuss.xgboost.ai/t/xgbregressor-gammaregression-label-must-be-positive/2318/2)

### [<title>XGBRegressor, GammaRegression:label must be positive - XGBoost</title>](https://discuss.xgboost.ai/t/xgbregressor-gammaregression-label-must-be-positive/2318/1)

### [<title>PSS/USS 和 RSS 其实是一回事，吗？ - Changkun's Blog</title>](https://blog.changkun.de/posts/pss-uss-rss/)

### [<title>缺页与预取带来的性能差异 - Changkun's Blog</title>](https://blog.changkun.de/posts/page-fault-vs-prefetch/)

### [<title>2020 年终总结 - Changkun's Blog</title>](https://blog.changkun.de/posts/2020-summary/)

### [<title>Migration with Zero Downtime - Changkun's Blog</title>](https://blog.changkun.de/posts/zero-downtime-migration/)

### [<title>2020 读书清单 - Changkun's Blog</title>](https://blog.changkun.de/posts/2020-reading/)

### [<title>The All in Go Stack - Changkun's Blog</title>](https://blog.changkun.de/posts/all-in-go/)

### [<title>Pointers Might Not Be Ideal for Parameters - Changkun's Blog</title>](https://blog.changkun.de/posts/pointers-might-not-be-ideal-for-parameters/)

### [<title>Eliminating A Source of Measurement Errors in Benchmarks - Changkun's Blog</title>](https://blog.changkun.de/posts/eliminating-a-source-of-measurement-errors-in-benchmarks/)

### [<title>Setup Wordpress in 10 Minutes - Changkun's Blog</title>](https://blog.changkun.de/posts/setup-wordpress-in-10-minutes/)

### [<title>我为什么不再写博客了？ - Changkun's Blog</title>](https://blog.changkun.de/posts/why-i-stopped-blogging/)

### [<title>2019 年终总结 - Changkun's Blog</title>](https://blog.changkun.de/posts/2019-summary/)

### [<title>2018-2019 读书清单 - Changkun's Blog</title>](https://blog.changkun.de/posts/2018-2019-reading/)

### [<title>Ten years of blogging - Changkun's Blog</title>](https://blog.changkun.de/posts/ten-years-of-blogging/)

### [<title>Rethinking the Reflections on Communications and Trusts - Changkun's Blog</title>](https://blog.changkun.de/posts/rethinking-the-reflections-on-communications-and-trusts/)

### [<title>2018 年终总结 - Changkun's Blog</title>](https://blog.changkun.de/posts/2018-summary/)

### [<title>Go source code study is open source - Changkun's Blog</title>](https://blog.changkun.de/posts/go-source-code-study-is-open-source/)

### [<title>Go source study: unsafe Pattern - Changkun's Blog</title>](https://blog.changkun.de/posts/go-source-unsafe-pattern/)

### [<title>Go source study: sync.Pool - Changkun's Blog</title>](https://blog.changkun.de/posts/go-source-sync-pool/)

### [<title>Go runtime programming - Changkun's Blog</title>](https://blog.changkun.de/posts/go-runtime-programming/)

### [<title>Guacamole Source Analysis - Changkun's Blog</title>](https://blog.changkun.de/_todo/2018-08-14-guacamole-source-analysis/)

### [<title>A Million WebSocket and Go - Changkun's Blog</title>](https://blog.changkun.de/posts/a-million-websocket-and-go/)

### [<title>Designing Asynchronous RESTful APIs - Changkun's Blog</title>](https://blog.changkun.de/posts/designing-asynchronous-restful-apis/)

### [<title>分布式杂谈01：CAP 理论的误解 - Changkun's Blog</title>](https://blog.changkun.de/posts/a-common-misunderstanding-of-the-cap-theory/)

### [<title>Strong man is strongest alone - Changkun's Blog</title>](https://blog.changkun.de/_todo/2018-05-26-strong-man-is-strongest-alone/)

### [<title>流形学习与对抗网络 - Changkun's Blog</title>](https://blog.changkun.de/_todo/2018-05-23-%E6%B5%81%E5%BD%A2%E5%AD%A6%E4%B9%A0%E4%B8%8E%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/)

### [<title>Issues of Human-Bot Interaction - Changkun's Blog</title>](https://blog.changkun.de/posts/issues-of-human-bot-interaction/)

### [<title>UMSLT05: The Consistency of Learning Process - Changkun's Blog</title>](https://blog.changkun.de/_todo/2018-03-27-umslt05/)

### [<title>论文笔记：Generalization in Machine Learning via Analytical Learning Theory - Changkun's Blog</title>](https://blog.changkun.de/_todo/2018-03-26-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0generalization-in-machine-learning-via-analytical-learning-theory/)

### [<title>压缩法与深度网络的泛化性 - Changkun's Blog</title>](https://blog.changkun.de/posts/compression-and-dnn-generalization/)

### [<title>Go Web in 1 Hour - Changkun's Blog</title>](https://blog.changkun.de/_todo/2018-03-21-go-web-in-1-hour/)

### [<title>Go in 1 Hour - Changkun's Blog</title>](https://blog.changkun.de/posts/go-in-1-hour/)

### [<title>UMSLT04: The Past and Present of SGD - Changkun's Blog</title>](https://blog.changkun.de/posts/the-past-and-present-of-sgd/)

### [<title>UMSLT03: A Gentle Start of Learning Theory - Changkun's Blog</title>](https://blog.changkun.de/posts/a-gentle-start-of-learning-theory/)

### [<title>UMSLT02: A Breif History of Neural Networks - Changkun's Blog</title>](https://blog.changkun.de/posts/a-breif-history-of-neural-networks/)

### [<title>UMSLT01: A Breif History of Regularization - Changkun's Blog</title>](https://blog.changkun.de/posts/a-breif-history-of-regularization/)

### [<title>不笑不足以为道 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E4%B8%8D%E7%AC%91%E4%B8%8D%E8%B6%B3%E4%BB%A5%E4%B8%BA%E9%81%93/)

### [<title>统计学习理论综述 - Changkun's Blog</title>](https://blog.changkun.de/_todo/2018-01-30-%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA%E7%BB%BC%E8%BF%B0/)

### [<title>论文笔记：Generalization in Deep Learning - Changkun's Blog</title>](https://blog.changkun.de/posts/generalization-in-deep-learning/)

### [<title>2017 年终总结 - Changkun's Blog</title>](https://blog.changkun.de/posts/2017-summary/)

### [<title>2017 读书清单 - Changkun's Blog</title>](https://blog.changkun.de/posts/2017-reading/)

### [<title>深度学习的泛化理论简介 - Changkun's Blog</title>](https://blog.changkun.de/posts/an-intro-to-generalization-theory/)

### [<title>删除 GitHub 上已经提交的敏感信息 - Changkun's Blog</title>](https://blog.changkun.de/posts/delete-pushed-info-from-github/)

### [<title>CNN Literature Review - Changkun's Blog</title>](https://blog.changkun.de/_todo/2017-10-24-cnn-literature-review/)

### [<title>基于移动交互行为的用户情感推断 - Changkun's Blog</title>](https://blog.changkun.de/_todo/2017-10-24-%E5%9F%BA%E4%BA%8E%E7%A7%BB%E5%8A%A8%E4%BA%A4%E4%BA%92%E8%A1%8C%E4%B8%BA%E7%9A%84%E7%94%A8%E6%88%B7%E6%83%85%E6%84%9F%E6%8E%A8%E6%96%AD/)

### [<title>金融学七大理论之『有效市场假说』 - Changkun's Blog</title>](https://blog.changkun.de/_todo/2017-09-23-%E9%87%91%E8%9E%8D%E5%AD%A6%E4%B8%83%E5%A4%A7%E7%90%86%E8%AE%BA%E4%B9%8B%E6%9C%89%E6%95%88%E5%B8%82%E5%9C%BA%E5%81%87%E8%AF%B4/)

### [<title>硕士生涯的第一年就这样告一段落了 - Changkun's Blog</title>](https://blog.changkun.de/posts/first-year-of-master-studies/)

### [<title>Critique notes on HCI research - Changkun's Blog</title>](https://blog.changkun.de/_todo/2017-08-25-critique-notes-on-hci-research/)

### [<title>人肉计算(10): 系统参与激励 - Changkun's Blog</title>](https://blog.changkun.de/posts/human-computation-10/)

### [<title>人肉计算(9): 陷阱的解法 - Changkun's Blog</title>](https://blog.changkun.de/posts/human-computation-9/)

### [<title>别聊，一聊你就暴露 - Changkun's Blog</title>](https://blog.changkun.de/posts/do-not-talk/)

### [<title>人肉计算(8): 人肉计算与数据科学中的陷阱 - Changkun's Blog</title>](https://blog.changkun.de/posts/human-computation-8/)

### [<title>人肉计算(7): 社会行为分析 - Changkun's Blog</title>](https://blog.changkun.de/posts/human-computation-7/)

### [<title>Hexo + GitHub + Travis CI + VPS 自动部署 - Changkun's Blog</title>](https://blog.changkun.de/posts/hexo-github-travis-ci-vps-ci-cd/)

### [<title>人肉计算(6): 预测市场 - Changkun's Blog</title>](https://blog.changkun.de/posts/human-computation-6/)

### [<title>人肉计算(5): 信用风险评级模型 - Changkun's Blog</title>](https://blog.changkun.de/posts/human-computation-5/)

### [<title>读书与回报 - Changkun's Blog</title>](https://blog.changkun.de/posts/return-of-studies/)

### [<title>瞎扯: 对现代企业理论与当下IT企业的商业模式和信息产业链的规律性的思考 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E5%AF%B9%E7%8E%B0%E4%BB%A3%E4%BC%81%E4%B8%9A%E7%90%86%E8%AE%BA%E4%B8%8E%E5%BD%93%E4%B8%8Bit%E4%BC%81%E4%B8%9A%E7%9A%84%E5%95%86%E4%B8%9A%E6%A8%A1%E5%BC%8F%E5%92%8C%E4%BF%A1%E6%81%AF%E4%BA%A7%E4%B8%9A%E9%93%BE%E7%9A%84%E8%A7%84%E5%BE%8B%E6%80%A7%E7%9A%84%E6%80%9D%E8%80%83/)

### [<title>人肉计算(4): 输入数据聚合与PageRank - Changkun's Blog</title>](https://blog.changkun.de/posts/human-computation-4/)

### [<title>又一次打整了一下博客 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E5%8F%88%E4%B8%80%E6%AC%A1%E6%89%93%E6%95%B4%E4%BA%86%E4%B8%80%E4%B8%8B%E5%8D%9A%E5%AE%A2/)

### [<title>人肉计算(3): 输入数据聚合与链路预测 - Changkun's Blog</title>](https://blog.changkun.de/posts/human-computation-3/)

### [<title>人肉计算(2): 意图博弈 GWAPs - Changkun's Blog</title>](https://blog.changkun.de/posts/human-computation-2/)

### [<title>人肉计算(1): 众包与群众智慧 - Changkun's Blog</title>](https://blog.changkun.de/posts/human-computation-1/)

### [<title>对后辈同学在计算机专业上的答疑与解惑 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E5%AF%B9%E5%90%8E%E8%BE%88%E5%90%8C%E5%AD%A6%E5%9C%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%93%E4%B8%9A%E4%B8%8A%E7%9A%84%E7%AD%94%E7%96%91%E4%B8%8E%E8%A7%A3%E6%83%91/)

### [<title>在德国的医疗及住院体验 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E5%9C%A8%E5%BE%B7%E5%9B%BD%E7%9A%84%E5%8C%BB%E7%96%97%E5%8F%8A%E4%BD%8F%E9%99%A2%E4%BD%93%E9%AA%8C/)

### [<title>这可能不是一个技术博客了 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E8%BF%99%E5%8F%AF%E8%83%BD%E4%B8%8D%E6%98%AF%E4%B8%80%E4%B8%AA%E6%8A%80%E6%9C%AF%E5%8D%9A%E5%AE%A2%E4%BA%86/)

### [<title>实验楼楼赛第3期-Python-题解 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E5%AE%9E%E9%AA%8C%E6%A5%BC%E6%A5%BC%E8%B5%9B%E7%AC%AC3%E6%9C%9F-python-%E9%A2%98%E8%A7%A3/)

### [<title>迅速更换了 DISQUS - Changkun's Blog</title>](https://blog.changkun.de/posts/migrate-to-disqus/)

### [<title>Electron 深度实践总结 - Changkun's Blog</title>](https://blog.changkun.de/posts/electron-summary/)

### [<title>良好的编码体验的三个方面 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E8%89%AF%E5%A5%BD%E7%9A%84%E7%BC%96%E7%A0%81%E4%BD%93%E9%AA%8C%E7%9A%84%E4%B8%89%E4%B8%AA%E6%96%B9%E9%9D%A2/)

### [<title>2016 年终总结 - Changkun's Blog</title>](https://blog.changkun.de/posts/2016-summary/)

### [<title>2016 读书清单 - Changkun's Blog</title>](https://blog.changkun.de/posts/2016-reading/)

### [<title>最近在着手写的文章 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E6%9C%80%E8%BF%91%E5%9C%A8%E7%9D%80%E6%89%8B%E5%86%99%E7%9A%84%E6%96%87%E7%AB%A0/)

### [<title>最近两周经历的两次 Hackathon - Changkun's Blog</title>](https://blog.changkun.de/_todo/%E6%9C%80%E8%BF%91%E7%BB%8F%E5%8E%86%E7%9A%84%E4%B8%A4%E6%AC%A1-hackathon/)

### [<title>微信小程序文档极致总结 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F%E6%96%87%E6%A1%A3%E6%9E%81%E8%87%B4%E6%80%BB%E7%BB%93/)

### [<title>谈谈过去三个月在实验楼的实习经历 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E8%B0%88%E8%B0%88%E8%BF%87%E5%8E%BB%E4%B8%89%E4%B8%AA%E6%9C%88%E5%9C%A8%E5%AE%9E%E9%AA%8C%E6%A5%BC%E7%9A%84%E5%AE%9E%E4%B9%A0%E7%BB%8F%E5%8E%86/)

### [<title>给博客做了一个桌面客户端… - Changkun's Blog</title>](https://blog.changkun.de/posts/%E7%BB%99%E5%8D%9A%E5%AE%A2%E5%81%9A%E4%BA%86%E4%B8%80%E4%B8%AA%E6%A1%8C%E9%9D%A2%E5%AE%A2%E6%88%B7%E7%AB%AF/)

### [<title>Guacamole 源码分析与 VNC 中 RFB 协议的坑 - Changkun's Blog</title>](https://blog.changkun.de/posts/guacamole-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B8%8E-vnc-%E4%B8%AD-rfb-%E5%8D%8F%E8%AE%AE%E7%9A%84%E5%9D%91/)

### [<title>《高速上手 C++11/14》正式发布 - Changkun's Blog</title>](https://blog.changkun.de/posts/modern-cpp-v1-is-live/)

### [<title>Docker 极速入门教程02 - 镜像与容器管理 - Changkun's Blog</title>](https://blog.changkun.de/posts/docker-tutorial-2/)

### [<title>Docker 极速入门教程01 - 基本概念和操作 - Changkun's Blog</title>](https://blog.changkun.de/posts/docker-tutorial-1/)

### [<title>阶段性沉默 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E9%98%B6%E6%AE%B5%E6%80%A7%E6%B2%89%E9%BB%98/)

### [<title>ELK+Redis 最佳实践 - Changkun's Blog</title>](https://blog.changkun.de/posts/elk-redis-best-practice/)

### [<title>终于全面启用了 HTTPS - Changkun's Blog</title>](https://blog.changkun.de/posts/all-in-https/)

### [<title>苹果开源了LZFSE无损压缩 - Changkun's Blog</title>](https://blog.changkun.de/posts/apple-released-lzfse/)

### [<title>Specialization - Changkun's Blog</title>](https://blog.changkun.de/specialization/)

### [<title>Hash 碰撞的一种思路 - Changkun's Blog</title>](https://blog.changkun.de/posts/hash-%E7%A2%B0%E6%92%9E%E7%9A%84%E4%B8%80%E7%A7%8D%E6%80%9D%E8%B7%AF/)

### [<title>记一次完整的 Kaldi-TIMIT 示例运行 - Changkun's Blog</title>](https://blog.changkun.de/posts/kaldi-timit-example2/)

### [<title>Kaldi 上的 TIMIT 例子 - Changkun's Blog</title>](https://blog.changkun.de/posts/kaldi-timit-example/)

### [<title>Kaldi 安装与部署 - Changkun's Blog</title>](https://blog.changkun.de/posts/install-kaldi/)

### [<title>Bachelor Thesis - Changkun's Blog</title>](https://blog.changkun.de/bachelorthesis/)

### [<title>当我们进行学术演讲时，我们应该谈论什么 - Changkun's Blog</title>](https://blog.changkun.de/_todo/%E5%BD%93%E6%88%91%E4%BB%AC%E5%9C%A8%E8%BF%9B%E8%A1%8C%E5%AD%A6%E6%9C%AF%E6%BC%94%E8%AE%B2%E6%97%B6%E6%88%91%E4%BB%AC%E5%BA%94%E8%AF%A5%E8%B0%88%E8%AE%BA%E4%BB%80%E4%B9%88/)

### [<title>从科研写作谈起 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E4%BB%8E%E7%A7%91%E7%A0%94%E5%86%99%E4%BD%9C%E8%B0%88%E8%B5%B7/)

### [<title>Swift API 设计指南 - Changkun's Blog</title>](https://blog.changkun.de/posts/swift-api-design-guidelines/)

### [<title>有趣的人类 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E6%9C%89%E8%B6%A3%E7%9A%84%E4%BA%BA%E7%B1%BB/)

### [<title>所以其实论文并没有什么鬼用 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E6%89%80%E4%BB%A5%E5%85%B6%E5%AE%9E%E8%AE%BA%E6%96%87%E5%B9%B6%E6%B2%A1%E6%9C%89%E4%BB%80%E4%B9%88%E9%AC%BC%E7%94%A8/)

### [<title>Githug 通关记录及指南 - Changkun's Blog</title>](https://blog.changkun.de/posts/githug-record/)

### [<title>小结一下这学期的收获 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E5%B0%8F%E7%BB%93%E4%B8%80%E4%B8%8B%E8%BF%99%E5%AD%A6%E6%9C%9F%E7%9A%84%E6%94%B6%E8%8E%B7/)

### [<title>2015 读书清单 - Changkun's Blog</title>](https://blog.changkun.de/posts/2015-reading/)

### [<title>2015 年终总结 - Changkun's Blog</title>](https://blog.changkun.de/posts/2015-summary/)

### [<title>负能量爆表 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E8%B4%9F%E8%83%BD%E9%87%8F%E7%88%86%E8%A1%A8/)

### [<title>转眼就快两个月了 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E8%BD%AC%E7%9C%BC%E5%B0%B1%E5%BF%AB%E4%B8%A4%E4%B8%AA%E6%9C%88%E4%BA%86/)

### [<title>Tags - Changkun's Blog</title>](https://blog.changkun.de/tags/)

### [<title>博客迁移记录 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E5%8D%9A%E5%AE%A2%E8%BF%81%E7%A7%BB%E8%AE%B0%E5%BD%95/)

### [<title>大三总结 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E5%A4%A7%E4%B8%89%E6%80%BB%E7%BB%93/)

### [<title>这个世界，终究不会是我们的。 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E8%BF%99%E4%B8%AA%E4%B8%96%E7%95%8C%E7%BB%88%E7%A9%B6%E4%B8%8D%E4%BC%9A%E6%98%AF%E6%88%91%E4%BB%AC%E7%9A%84/)

### [<title>Linux 内核分析 之六：Linux 内核创建进程的过程 - Changkun's Blog</title>](https://blog.changkun.de/posts/linux-kernel-6/)

### [<title>小说「泽缘」 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E5%B0%8F%E8%AF%B4%E6%B3%BD%E7%BC%98/)

### [<title>Linux 内核分析 之五：system_call中断处理过程的简要分析 - Changkun's Blog</title>](https://blog.changkun.de/posts/linux-kernel-5/)

### [<title>大创项目的标题真是每年都在考验同学们的想象力啊 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E5%A4%A7%E5%88%9B%E9%A1%B9%E7%9B%AE%E7%9A%84%E6%A0%87%E9%A2%98%E7%9C%9F%E6%98%AF%E6%AF%8F%E5%B9%B4%E9%83%BD%E5%9C%A8%E8%80%83%E9%AA%8C%E5%90%8C%E5%AD%A6%E4%BB%AC%E7%9A%84%E6%83%B3%E8%B1%A1%E5%8A%9B%E5%95%8A/)

### [<title>Linux 内核分析 之四：使用库函数API和嵌入汇编两种方式使用同一个系统调用 - Changkun's Blog</title>](https://blog.changkun.de/posts/linux-kernel-4/)

### [<title>Doxygen 生成中文 Latex 文档 - Changkun's Blog</title>](https://blog.changkun.de/posts/doxygen-latex-doc/)

### [<title>Wordpress 站点搬家 - Changkun's Blog</title>](https://blog.changkun.de/posts/wordpress-%E7%AB%99%E7%82%B9%E6%90%AC%E5%AE%B6/)

### [<title>Linux 内核分析 之三：Linux内核启动函数start_kernel()的简单分析 - Changkun's Blog</title>](https://blog.changkun.de/posts/linux-kernel-3/)

### [<title>Ubuntu14.04 安装 Oracle 11g R2 Express Edition - Changkun's Blog</title>](https://blog.changkun.de/posts/ubuntu1404-install-oracle/)

### [<title>Linux 内核分析 之二：基于时间片轮转的简单的系统内核构造 - Changkun's Blog</title>](https://blog.changkun.de/posts/linux-kernel-2/)

### [<title>Linux 内核分析 之一：How Computer Works 实验 - Changkun's Blog</title>](https://blog.changkun.de/posts/linux-kernel-1/)

### [<title>什么是泛函空间的大数定律？它是机器学习理论的里程碑吗？ - Changkun's Blog</title>](https://blog.changkun.de/posts/%E4%BB%80%E4%B9%88%E6%98%AF%E6%B3%9B%E5%87%BD%E7%A9%BA%E9%97%B4%E7%9A%84%E5%A4%A7%E6%95%B0%E5%AE%9A%E5%BE%8B/)

### [<title>2014 年终总结 - Changkun's Blog</title>](https://blog.changkun.de/posts/2014-summary/)

### [<title>2014 读书清单 - Changkun's Blog</title>](https://blog.changkun.de/posts/2014-reading/)

### [<title>WatchKit框架与WatchApp交互初窥 - Changkun's Blog</title>](https://blog.changkun.de/posts/watchkit-and-watchapp/)

### [<title>Python-MySQLdb 教程 - Changkun's Blog</title>](https://blog.changkun.de/posts/python-mysqldb-tutorial/)

### [<title>谈谈 CV - Changkun's Blog</title>](https://blog.changkun.de/posts/talk-about-cv/)

### [<title>简单的日志分析 - Changkun's Blog</title>](https://blog.changkun.de/posts/simple-logging-analysis/)

### [<title>图书馆真是越来越无趣了 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E5%9B%BE%E4%B9%A6%E9%A6%86%E7%9C%9F%E6%98%AF%E8%B6%8A%E6%9D%A5%E8%B6%8A%E6%97%A0%E8%B6%A3%E4%BA%86/)

### [<title>The worst Presentation - Changkun's Blog</title>](https://blog.changkun.de/posts/the-worst-presentation/)

### [<title>关于 Riemann - Changkun's Blog</title>](https://blog.changkun.de/posts/about-riemann/)

### [<title>删除Mac MySQL - Changkun's Blog</title>](https://blog.changkun.de/posts/delete-mac-mysql/)

### [<title>大坑，难以置信的大坑！ - Changkun's Blog</title>](https://blog.changkun.de/posts/%E9%9A%BE%E4%BB%A5%E7%BD%AE%E4%BF%A1%E7%9A%84%E5%A4%A7%E5%9D%91/)

### [<title>从坐公交车想到的和看到的 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E4%BB%8E%E5%9D%90%E5%85%AC%E4%BA%A4%E8%BD%A6%E6%83%B3%E5%88%B0%E7%9A%84%E5%92%8C%E7%9C%8B%E5%88%B0%E7%9A%84/)

### [<title>在 Mac 中安装 opencv-python - Changkun's Blog</title>](https://blog.changkun.de/posts/install-opencv-python-on-mac/)

### [<title>计算机科学和数学的那点儿破事儿 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E5%92%8C%E6%95%B0%E5%AD%A6%E7%9A%84%E9%82%A3%E7%82%B9%E5%84%BF%E7%A0%B4%E4%BA%8B%E5%84%BF/)

### [<title>拙谈人工智能的未来 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E6%8B%99%E8%B0%88%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E7%9A%84%E6%9C%AA%E6%9D%A5/)

### [<title>Mac下新安装的MySQL无法登陆root用户解决方法 - Changkun's Blog</title>](https://blog.changkun.de/posts/login-issue-after-install-mysql-on-mac/)

### [<title>Matlab 2013a for Mac 帮助文档卡死解决方案 - Changkun's Blog</title>](https://blog.changkun.de/posts/matlab-2013a-for-mac-hanging-issue/)

### [<title>Put color on Mac terminal - Changkun's Blog</title>](https://blog.changkun.de/posts/put-color-on-mac-terminal/)

### [<title>论文都是逼出来的 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E8%AE%BA%E6%96%87%E9%83%BD%E6%98%AF%E9%80%BC%E5%87%BA%E6%9D%A5%E7%9A%84/)

### [<title>Python QuickStart - Changkun's Blog</title>](https://blog.changkun.de/posts/python-quickstart/)

### [<title>谈谈对编程语言的一般性认识 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E8%B0%88%E8%B0%88%E5%AF%B9%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E7%9A%84%E4%B8%80%E8%88%AC%E6%80%A7%E8%AE%A4%E8%AF%86/)

### [<title>写 App &amp; 与打砖块：无尽版 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E6%89%93%E7%A0%96%E5%9D%97%E6%97%A0%E5%B0%BD%E7%89%88/)

### [<title>MacTeX 卸载方法 - Changkun's Blog</title>](https://blog.changkun.de/posts/delete-mactex/)

### [<title>Lua一日游:(5) cocos2dx 与 Lua - Changkun's Blog</title>](https://blog.changkun.de/posts/lua-5/)

### [<title>Lua一日游:(4)面向对象——函数闭包形式 - Changkun's Blog</title>](https://blog.changkun.de/posts/lua-4/)

### [<title>Lua一日游:(3)面向对象——复制表形式 - Changkun's Blog</title>](https://blog.changkun.de/posts/lua-3/)

### [<title>Lua一日游:(2)Table和Array - Changkun's Blog</title>](https://blog.changkun.de/posts/lua-2/)

### [<title>Lua一日游:(1) Mac环境搭建与基本语法 - Changkun's Blog</title>](https://blog.changkun.de/posts/lua-1/)

### [<title>崇祯为什么不跑南京？ - Changkun's Blog</title>](https://blog.changkun.de/posts/%E5%B4%87%E7%A5%AF%E4%B8%BA%E4%BB%80%E4%B9%88%E4%B8%8D%E8%B7%91%E5%8D%97%E4%BA%AC/)

### [<title>看了这么久外剧，也就发现这部片子火这么厉害 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E7%9C%8B%E4%BA%86%E8%BF%99%E4%B9%88%E4%B9%85%E5%A4%96%E5%89%A7%E4%B9%9F%E5%B0%B1%E5%8F%91%E7%8E%B0%E8%BF%99%E9%83%A8%E7%89%87%E5%AD%90%E7%81%AB%E8%BF%99%E4%B9%88%E5%8E%89%E5%AE%B3/)

### [<title>Paper读后感：人机交互的进展及面临的挑战 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E4%BA%BA%E6%9C%BA%E4%BA%A4%E4%BA%92%E7%9A%84%E8%BF%9B%E5%B1%95%E5%8F%8A%E9%9D%A2%E4%B8%B4%E7%9A%84%E6%8C%91%E6%88%98/)

### [<title>今天编译项目真心觉得很憔悴啊 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E4%BB%8A%E5%A4%A9%E7%BC%96%E8%AF%91%E9%A1%B9%E7%9B%AE%E7%9C%9F%E5%BF%83%E8%A7%89%E5%BE%97%E5%BE%88%E6%86%94%E6%82%B4%E5%95%8A/)

### [<title>最近读了点人工智能 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E6%9C%80%E8%BF%91%E8%AF%BB%E4%BA%86%E7%82%B9%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/)

### [<title>2013 年终总结 - Changkun's Blog</title>](https://blog.changkun.de/posts/2013-summary/)

### [<title>重温《Harry Potter》 - Changkun's Blog</title>](https://blog.changkun.de/posts/review-harry-potter/)

### [<title>这学期大创项目总算是告一段落了 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E8%BF%99%E5%AD%A6%E6%9C%9F%E5%A4%A7%E5%88%9B%E9%A1%B9%E7%9B%AE%E6%80%BB%E7%AE%97%E6%98%AF%E5%91%8A%E4%B8%80%E6%AE%B5%E8%90%BD%E4%BA%86/)

### [<title>Linux学习笔记 6 网络及配置相关 - Changkun's Blog</title>](https://blog.changkun.de/posts/learn-linux-6/)

### [<title>Linux学习笔记 5 UGO模型、Linux权限 - Changkun's Blog</title>](https://blog.changkun.de/posts/learn-linux-5/)

### [<title>Linux学习笔记 7 获取帮助 - Changkun's Blog</title>](https://blog.changkun.de/posts/learn-linux-7/)

### [<title>Linux学习笔记 4 Linux文件系统基本操作 - Changkun's Blog</title>](https://blog.changkun.de/posts/learn-linux-4/)

### [<title>又谈 Android - Changkun's Blog</title>](https://blog.changkun.de/posts/talk-about-android/)

### [<title>Linux学习笔记 3 Vim必背命令 - Changkun's Blog</title>](https://blog.changkun.de/posts/learn-linux-3/)

### [<title>Linux学习笔记 2 Linux常用必背命令 - Changkun's Blog</title>](https://blog.changkun.de/posts/learn-linux-2/)

### [<title>Linux学习笔记 1 Linux基本操作及其文件系统结构 - Changkun's Blog</title>](https://blog.changkun.de/posts/learn-linux-1/)

### [<title>《C专家编程》读书笔记 - Changkun's Blog</title>](https://blog.changkun.de/posts/c-expert-programming-reading-notes/)

### [<title>谈谈二分搜索 - Changkun's Blog</title>](https://blog.changkun.de/posts/talk-about-binary-search/)

### [<title>谈谈积分与函数 - Changkun's Blog</title>](https://blog.changkun.de/posts/integral-and-function/)

### [<title>「中国要是有一千个陈景润就不得了了」 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E4%B8%AD%E5%9B%BD%E8%A6%81%E6%98%AF%E6%9C%89%E4%B8%80%E5%8D%83%E4%B8%AA%E9%99%88%E6%99%AF%E6%B6%A6%E5%B0%B1%E4%B8%8D%E5%BE%97%E4%BA%86%E4%BA%86/)

### [<title>数学建模协会入会笔试题 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E5%8D%8F%E4%BC%9A%E5%85%A5%E4%BC%9A%E7%AC%94%E8%AF%95%E9%A2%98/)

### [<title>曾勇老师语录 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E6%9B%BE%E5%8B%87%E8%80%81%E5%B8%88%E8%AF%AD%E5%BD%95/)

### [<title>Fourier变换应用小记 - Changkun's Blog</title>](https://blog.changkun.de/posts/fourier-transformation-notes/)

### [<title>模版链队实现 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E6%A8%A1%E7%89%88%E9%93%BE%E9%98%9F%E5%AE%9E%E7%8E%B0/)

### [<title>模版链栈实现 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E6%A8%A1%E7%89%88%E9%93%BE%E6%A0%88%E5%AE%9E%E7%8E%B0/)

### [<title>从圆锥体积谈起 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E4%BB%8E%E5%9C%86%E9%94%A5%E4%BD%93%E7%A7%AF%E8%B0%88%E8%B5%B7/)

### [<title>《乔布斯》观后感 - Changkun's Blog</title>](https://blog.changkun.de/posts/jobs-movie-review/)

### [<title>反转单链表 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E5%8F%8D%E8%BD%AC%E5%8D%95%E9%93%BE%E8%A1%A8/)

### [<title>通信信号与正交向量之间的关系 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E9%80%9A%E4%BF%A1%E4%BF%A1%E5%8F%B7%E4%B8%8E%E6%AD%A3%E4%BA%A4%E5%90%91%E9%87%8F%E4%B9%8B%E9%97%B4%E7%9A%84%E5%85%B3%E7%B3%BB/)

### [<title>谈人与事 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E8%B0%88%E4%BA%BA%E4%B8%8E%E4%BA%8B/)

### [<title>基于图像信号分析的碎纸片的拼接复原 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E5%9F%BA%E4%BA%8E%E5%9B%BE%E5%83%8F%E4%BF%A1%E5%8F%B7%E5%88%86%E6%9E%90%E7%9A%84%E7%A2%8E%E7%BA%B8%E7%89%87%E7%9A%84%E6%8B%BC%E6%8E%A5%E5%A4%8D%E5%8E%9F/)

### [<title>从 Windows 到 Macintosh - Changkun's Blog</title>](https://blog.changkun.de/posts/from-windows-to-macintosh/)

### [<title>模版链表实现与模版声明定义分离编译错误 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E6%A8%A1%E7%89%88%E9%93%BE%E8%A1%A8%E5%AE%9E%E7%8E%B0%E4%B8%8E%E6%A8%A1%E7%89%88%E5%A3%B0%E6%98%8E%E5%AE%9A%E4%B9%89%E5%88%86%E7%A6%BB%E7%BC%96%E8%AF%91%E9%94%99%E8%AF%AF/)

### [<title>MeanShift 均值漂移跟踪算法原理 - Changkun's Blog</title>](https://blog.changkun.de/posts/meanshift-algorithm/)

### [<title>下一次的数学突破？ - Changkun's Blog</title>](https://blog.changkun.de/posts/%E4%B8%8B%E4%B8%80%E6%AC%A1%E7%9A%84%E6%95%B0%E5%AD%A6%E7%AA%81%E7%A0%B4/)

### [<title>Kalman滤波器数学原理与应用 - Changkun's Blog</title>](https://blog.changkun.de/posts/kalman-filter-principle/)

### [<title>在堆上分配内存 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E5%9C%A8%E5%A0%86%E4%B8%8A%E5%88%86%E9%85%8D%E5%86%85%E5%AD%98/)

### [<title>MFC系列（一）创建空白窗口 - Changkun's Blog</title>](https://blog.changkun.de/posts/windows-mfc-1/)

### [<title>在 Mac 中配置OpenCV - Changkun's Blog</title>](https://blog.changkun.de/posts/install-opencv-on-mac/)

### [<title>从两个例子谈起：条件收敛与一致收敛的深刻背景分析 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E6%9D%A1%E4%BB%B6%E6%94%B6%E6%95%9B%E4%B8%8E%E4%B8%80%E8%87%B4%E6%94%B6%E6%95%9B%E7%9A%84%E6%B7%B1%E5%88%BB%E8%83%8C%E6%99%AF%E5%88%86%E6%9E%90/)

### [<title>大一总结 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E5%A4%A7%E4%B8%80%E6%80%BB%E7%BB%93/)

### [<title>陈老爷子 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E9%99%88%E8%80%81%E7%88%B7%E5%AD%90/)

### [<title>物理世界是离散的却为什么可以大量的使用微分和积分？ - Changkun's Blog</title>](https://blog.changkun.de/posts/%E7%89%A9%E7%90%86%E4%B8%96%E7%95%8C%E6%98%AF%E7%A6%BB%E6%95%A3%E7%9A%84%E5%8D%B4%E4%B8%BA%E4%BB%80%E4%B9%88%E5%8F%AF%E4%BB%A5%E5%A4%A7%E9%87%8F%E7%9A%84%E4%BD%BF%E7%94%A8%E5%BE%AE%E5%88%86%E5%92%8C%E7%A7%AF%E5%88%86/)

### [<title>Curriculum Vitae - Changkun's Blog</title>](https://blog.changkun.de/cv/)

### [<title>多线程之间的通信 GPU计算 单摄像头图像识别 初步思考 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B9%8B%E9%97%B4%E7%9A%84%E9%80%9A%E4%BF%A1-gpu%E8%AE%A1%E7%AE%97-%E5%8D%95%E6%91%84%E5%83%8F%E5%A4%B4%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB-%E5%88%9D%E6%AD%A5%E6%80%9D%E8%80%83/)

### [<title>我这是要转 Ubuntu 的节奏吗 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E6%88%91%E8%BF%99%E6%98%AF%E8%A6%81%E8%BD%AC-ubuntu-%E7%9A%84%E8%8A%82%E5%A5%8F%E5%90%97/)

### [<title>结构体排序引发的一连串问题 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E7%BB%93%E6%9E%84%E4%BD%93%E6%8E%92%E5%BA%8F%E5%BC%95%E5%8F%91%E7%9A%84%E4%B8%80%E8%BF%9E%E4%B8%B2%E9%97%AE%E9%A2%98/)

### [<title>Ubuntu 相关 - Changkun's Blog</title>](https://blog.changkun.de/posts/ubuntu-%E7%9B%B8%E5%85%B3/)

### [<title>逃生时间初算 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E9%80%83%E7%94%9F%E6%97%B6%E9%97%B4%E5%88%9D%E7%AE%97/)

### [<title>全空间和空空间？ - Changkun's Blog</title>](https://blog.changkun.de/posts/%E5%85%A8%E7%A9%BA%E9%97%B4%E5%92%8C%E7%A9%BA%E7%A9%BA%E9%97%B4/)

### [<title>从装系统中学到的 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E4%BB%8E%E8%A3%85%E7%B3%BB%E7%BB%9F%E4%B8%AD%E5%AD%A6%E5%88%B0%E7%9A%84/)

### [<title>string 标准库实现日志 - Changkun's Blog</title>](https://blog.changkun.de/posts/string-%E6%A0%87%E5%87%86%E5%BA%93%E5%AE%9E%E7%8E%B0%E6%97%A5%E5%BF%97/)

### [<title>通过安装 Wordpress 插件所学到的 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E9%80%9A%E8%BF%87%E5%AE%89%E8%A3%85-wordpress-%E6%8F%92%E4%BB%B6%E6%89%80%E5%AD%A6%E5%88%B0%E7%9A%84/)

### [<title>C Details 之 基本细节 - Changkun's Blog</title>](https://blog.changkun.de/posts/c-details-%E4%B9%8B-%E5%9F%BA%E6%9C%AC%E7%BB%86%E8%8A%82/)

### [<title>又读流形 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E5%8F%88%E8%AF%BB%E6%B5%81%E5%BD%A2/)

### [<title>Bio - Changkun's Blog</title>](https://blog.changkun.de/about/)

### [<title>ZFC 集合论中各公理的意义及作用 - Changkun's Blog</title>](https://blog.changkun.de/posts/zfc-%E9%9B%86%E5%90%88%E8%AE%BA%E4%B8%AD%E5%90%84%E5%85%AC%E7%90%86%E7%9A%84%E6%84%8F%E4%B9%89%E5%8F%8A%E4%BD%9C%E7%94%A8/)

### [<title>谈线性空间定义中加法交换律独立性 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E8%B0%88%E7%BA%BF%E6%80%A7%E7%A9%BA%E9%97%B4%E5%AE%9A%E4%B9%89%E4%B8%AD%E5%8A%A0%E6%B3%95%E4%BA%A4%E6%8D%A2%E5%BE%8B%E7%8B%AC%E7%AB%8B%E6%80%A7/)

### [<title>为什么要有特征值和特征向量 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E6%9C%89%E7%89%B9%E5%BE%81%E5%80%BC%E5%92%8C%E7%89%B9%E5%BE%81%E5%90%91%E9%87%8F/)

### [<title>2012 年终总结 - Changkun's Blog</title>](https://blog.changkun.de/posts/2012-summary/)

### [<title>一个人的锦都 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E4%B8%80%E4%B8%AA%E4%BA%BA%E7%9A%84%E9%94%A6%E9%83%BD/)

### [<title>谈谈Google和Android - Changkun's Blog</title>](https://blog.changkun.de/posts/%E8%B0%88%E8%B0%88google%E5%92%8Candroid/)

### [<title>又是一月流阙时 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E5%8F%88%E6%98%AF%E4%B8%80%E6%9C%88%E6%B5%81%E9%98%99%E6%97%B6/)

### [<title>文艺还是苦逼 确实不是一个问题 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E6%96%87%E8%89%BA%E8%BF%98%E6%98%AF%E8%8B%A6%E9%80%BC%E7%A1%AE%E5%AE%9E%E4%B8%8D%E6%98%AF%E4%B8%80%E4%B8%AA%E9%97%AE%E9%A2%98/)

### [<title>谈谈良序原理 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E8%B0%88%E8%B0%88%E8%89%AF%E5%BA%8F%E5%8E%9F%E7%90%86/)

### [<title>X-2222 计划 - Changkun's Blog</title>](https://blog.changkun.de/posts/x-2222-%E8%AE%A1%E5%88%92/)

### [<title>我坚信 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E6%88%91%E5%9D%9A%E4%BF%A1/)

### [<title>黎明前的黑暗 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E9%BB%8E%E6%98%8E%E5%89%8D%E7%9A%84%E9%BB%91%E6%9A%97/)

### [<title>高考后的计划，第一版 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E9%AB%98%E8%80%83%E5%90%8E%E7%9A%84%E8%AE%A1%E5%88%92%E7%AC%AC%E4%B8%80%E7%89%88/)

### [<title>数学系就业方向小谈 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E6%95%B0%E5%AD%A6%E7%B3%BB%E5%B0%B1%E4%B8%9A%E6%96%B9%E5%90%91%E5%B0%8F%E8%B0%88/)

### [<title>用数学眼光看世界之「焦点访谈的播出时间」 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E7%94%A8%E6%95%B0%E5%AD%A6%E7%9C%BC%E5%85%89%E7%9C%8B%E4%B8%96%E7%95%8C%E4%B9%8B%E7%84%A6%E7%82%B9%E8%AE%BF%E8%B0%88%E7%9A%84%E6%92%AD%E5%87%BA%E6%97%B6%E9%97%B4/)

### [<title>故城1 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E6%95%85%E5%9F%8E1/)

### [<title>记事簿5 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E8%AE%B0%E4%BA%8B%E7%B0%BF5/)

### [<title>记事簿4 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E8%AE%B0%E4%BA%8B%E7%B0%BF4/)

### [<title>记事簿3 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E8%AE%B0%E4%BA%8B%E7%B0%BF3/)

### [<title>记事簿2 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E8%AE%B0%E4%BA%8B%E7%B0%BF2/)

### [<title>记事簿1 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E8%AE%B0%E4%BA%8B%E7%B0%BF1/)

### [<title>某不等式 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E6%9F%90%E4%B8%8D%E7%AD%89%E5%BC%8F/)

### [<title>核辐射没那么可怕 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E6%A0%B8%E8%BE%90%E5%B0%84%E6%B2%A1%E9%82%A3%E4%B9%88%E5%8F%AF%E6%80%95/)

### [<title>赌博（装备锻造）必定破产 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E8%B5%8C%E5%8D%9A%E5%BF%85%E5%AE%9A%E7%A0%B4%E4%BA%A7/)

### [<title>把伤痕当酒窝 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E6%8A%8A%E4%BC%A4%E7%97%95%E5%BD%93%E9%85%92%E7%AA%9D/)

### [<title>游戏装备升级问题 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E6%B8%B8%E6%88%8F%E8%A3%85%E5%A4%87%E5%8D%87%E7%BA%A7%E9%97%AE%E9%A2%98/)

### [<title>为什么做不完高考数学 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E4%B8%BA%E4%BB%80%E4%B9%88%E5%81%9A%E4%B8%8D%E5%AE%8C%E9%AB%98%E8%80%83%E6%95%B0%E5%AD%A6/)

### [<title>素数无穷多的拓扑学证明 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E7%B4%A0%E6%95%B0%E6%97%A0%E7%A9%B7%E5%A4%9A%E7%9A%84%E6%8B%93%E6%89%91%E5%AD%A6%E8%AF%81%E6%98%8E/)

### [<title>人品守恒定律 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E4%BA%BA%E5%93%81%E5%AE%88%E6%81%92%E5%AE%9A%E5%BE%8B/)

### [<title>无奈 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E6%97%A0%E5%A5%88/)

### [<title>错失的机遇 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E9%94%99%E5%A4%B1%E7%9A%84%E6%9C%BA%E9%81%87/)

### [<title>小 Q 自传 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E5%B0%8F-q-%E8%87%AA%E4%BC%A0/)

### [<title>见鬼 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E8%A7%81%E9%AC%BC/)

### [<title>龙崎君 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E9%BE%99%E5%B4%8E%E5%90%9B/)

### [<title>禁言 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E7%A6%81%E8%A8%80/)

### [<title>春天来了 一切都还是温暖的 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E6%98%A5%E5%A4%A9%E6%9D%A5%E4%BA%86%E4%B8%80%E5%88%87%E9%83%BD%E8%BF%98%E6%98%AF%E6%B8%A9%E6%9A%96%E7%9A%84/)

### [<title>狗屁二三事 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E7%8B%97%E5%B1%81%E4%BA%8C%E4%B8%89%E4%BA%8B/)

### [<title>黑白世 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E9%BB%91%E7%99%BD%E4%B8%96/)

### [<title>再谈例证法 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E5%86%8D%E8%B0%88%E4%BE%8B%E8%AF%81%E6%B3%95/)

### [<title>例证法 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E4%BE%8B%E8%AF%81%E6%B3%95/)

### [<title>又见夏洛克 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E5%8F%88%E8%A7%81%E5%A4%8F%E6%B4%9B%E5%85%8B/)

### [<title>无言 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E6%97%A0%E8%A8%80/)

### [<title>牢不可破的誓言 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E7%89%A2%E4%B8%8D%E5%8F%AF%E7%A0%B4%E7%9A%84%E8%AA%93%E8%A8%80/)

### [<title>丢失的情人节 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E4%B8%A2%E5%A4%B1%E7%9A%84%E6%83%85%E4%BA%BA%E8%8A%82/)

### [<title>追逐 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E8%BF%BD%E9%80%90/)

### [<title>所谓语言 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E6%89%80%E8%B0%93%E8%AF%AD%E8%A8%80/)

### [<title>风雪二月 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E9%A3%8E%E9%9B%AA%E4%BA%8C%E6%9C%88/)

### [<title>深夜 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E6%B7%B1%E5%A4%9C/)

### [<title>烟火过境 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E7%83%9F%E7%81%AB%E8%BF%87%E5%A2%83/)

### [<title>无题 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E6%97%A0%E9%A2%98/)

### [<title>所谓爱情 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E6%89%80%E8%B0%93%E7%88%B1%E6%83%85/)

### [<title>On my way。 - Changkun's Blog</title>](https://blog.changkun.de/posts/on-my-way/)

### [<title>电影「2012」影评 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E7%94%B5%E5%BD%B12012%E5%BD%B1%E8%AF%84/)

### [<title>永远不要问我为什么！ - Changkun's Blog</title>](https://blog.changkun.de/posts/%E6%B0%B8%E8%BF%9C%E4%B8%8D%E8%A6%81%E9%97%AE%E6%88%91%E4%B8%BA%E4%BB%80%E4%B9%88/)

### [<title>得与失 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E5%BE%97%E4%B8%8E%E5%A4%B1/)

### [<title>一阵风 一场梦 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E4%B8%80%E9%98%B5%E9%A3%8E%E4%B8%80%E5%9C%BA%E6%A2%A6/)

### [<title>我将记得你 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E6%88%91%E5%B0%86%E8%AE%B0%E5%BE%97%E4%BD%A0/)

### [<title>人事已非 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E4%BA%BA%E4%BA%8B%E5%B7%B2%E9%9D%9E/)

### [<title>小谈瑕积分 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E5%B0%8F%E8%B0%88%E7%91%95%E7%A7%AF%E5%88%86/)

### [<title>再谈孤独的果实 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E5%86%8D%E8%B0%88%E5%AD%A4%E7%8B%AC%E7%9A%84%E6%9E%9C%E5%AE%9E/)

### [<title>人生 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E4%BA%BA%E7%94%9F/)

### [<title>任性了 太久之后想法 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E4%BB%BB%E6%80%A7%E4%BA%86%E5%A4%AA%E4%B9%85%E4%B9%8B%E5%90%8E%E6%83%B3%E6%B3%95/)

### [<title>繁星客栈 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E7%B9%81%E6%98%9F%E5%AE%A2%E6%A0%88/)

### [<title>那片海 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E9%82%A3%E7%89%87%E6%B5%B7/)

### [<title>隔帘听 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E9%9A%94%E5%B8%98%E5%90%AC/)

### [<title>路灯 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E8%B7%AF%E7%81%AF/)

### [<title>祝纯数学永无用处 &amp; 数学家不后悔 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E6%95%B0%E5%AD%A6%E5%AE%B6%E4%B8%8D%E5%90%8E%E6%82%94/)

### [<title>烟波江南 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E7%83%9F%E6%B3%A2%E6%B1%9F%E5%8D%97/)

### [<title>一路走来我 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E4%B8%80%E8%B7%AF%E8%B5%B0%E6%9D%A5%E6%88%91/)

### [<title>娱乐至死 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E5%A8%B1%E4%B9%90%E8%87%B3%E6%AD%BB/)

### [<title>我家的鹦鹉 &amp; 纪念 5.12 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E6%88%91%E5%AE%B6%E7%9A%84%E9%B9%A6%E9%B9%89/)

### [<title>不想说再见 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E4%B8%8D%E6%83%B3%E8%AF%B4%E5%86%8D%E8%A7%81/)

### [<title>夏季。流年。昨 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E5%A4%8F%E5%AD%A3%E6%B5%81%E5%B9%B4%E6%98%A8/)

### [<title>删除记忆 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E5%88%A0%E9%99%A4%E8%AE%B0%E5%BF%86/)

### [<title>真与伪 善与恶 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E7%9C%9F%E4%B8%8E%E4%BC%AA-%E5%96%84%E4%B8%8E%E6%81%B6/)

### [<title>人的一生，时起时落，但是，确实美好的。 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E4%BA%BA%E7%9A%84%E4%B8%80%E7%94%9F%E6%97%B6%E8%B5%B7%E6%97%B6%E8%90%BD%E4%BD%86%E6%98%AF%E7%A1%AE%E5%AE%9E%E7%BE%8E%E5%A5%BD%E7%9A%84/)

### [<title>那些花儿，盛开了，散落了 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E9%82%A3%E4%BA%9B%E8%8A%B1%E5%84%BF%E7%9B%9B%E5%BC%80%E4%BA%86%E6%95%A3%E8%90%BD%E4%BA%86/)

### [<title>独唱情歌 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E7%8B%AC%E5%94%B1%E6%83%85%E6%AD%8C/)

### [<title>无愁 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E6%97%A0%E6%84%81/)

### [<title>暖春 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E6%9A%96%E6%98%A5/)

### [<title>思念 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E6%80%9D%E5%BF%B5/)

### [<title>冷月 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E5%86%B7%E6%9C%88/)

### [<title>春醒 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E6%98%A5%E9%86%92/)

### [<title>人生轨迹II - Changkun's Blog</title>](https://blog.changkun.de/posts/%E4%BA%BA%E7%94%9F%E8%BD%A8%E8%BF%B9ii/)

### [<title>人生轨迹 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E4%BA%BA%E7%94%9F%E8%BD%A8%E8%BF%B9/)

### [<title>释怀 - Changkun's Blog</title>](https://blog.changkun.de/posts/%E9%87%8A%E6%80%80/)

### [<title>- Changkun's Blog</title>](https://blog.changkun.de/_todo/ddos-%E6%94%BB%E5%87%BB%E9%98%B2%E6%8A%A4%E5%88%9D%E6%AD%A5/)

### [<title>- Changkun's Blog</title>](https://blog.changkun.de/_todo/docker-%E6%9E%81%E9%80%9F%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B3-%E6%95%B0%E6%8D%AE%E4%B8%8E%E7%BD%91%E7%BB%9C%E7%AE%A1%E7%90%86%E5%8F%8Adocker%E5%AE%89%E5%85%A8/)

### [<title>- Changkun's Blog</title>](https://blog.changkun.de/_todo/html-css-%E4%B8%80%E6%97%A5%E6%A6%82%E8%A7%88/)

### [<title>- Changkun's Blog</title>](https://blog.changkun.de/_todo/r%E8%AF%AD%E8%A8%80%E5%88%9D%E4%BD%93%E9%AA%8C/)

### [<title>- Changkun's Blog</title>](https://blog.changkun.de/_todo/shallow-learning-and-deep-learning/)

### [<title>- Changkun's Blog</title>](https://blog.changkun.de/_todo/ycbcr%E9%A2%9C%E8%89%B2%E7%A9%BA%E9%97%B4%E4%B8%AD%E7%9A%84%E8%82%A4%E8%89%B2%E6%A4%AD%E5%9C%86%E6%A8%A1%E5%9E%8B/)

### [<title>- Changkun's Blog</title>](https://blog.changkun.de/_todo/%E4%BA%BA%E6%9C%BA%E4%BA%A4%E4%BA%92%E7%9A%84%E8%BF%87%E5%8E%BB%E7%8E%B0%E5%9C%A8%E4%B8%8E%E6%9C%AA%E6%9D%A5%E4%B8%80%E5%8F%AF%E7%94%A8%E6%80%A7%E6%98%93%E7%94%A8%E6%80%A7%E7%94%A8%E6%88%B7%E7%94%A8%E6%88%B7%E4%BD%93%E9%AA%8C/)

### [<title>- Changkun's Blog</title>](https://blog.changkun.de/_todo/%E5%9F%BA%E4%BA%8E-electron-%E7%BC%96%E5%86%99%E8%B7%A8%E5%B9%B3%E5%8F%B0%E5%AE%A2%E6%88%B7%E7%AB%AF%E7%9A%84%E8%8B%A5%E5%B9%B2%E5%AE%9E%E8%B7%B5/)

### [<title>- Changkun's Blog</title>](https://blog.changkun.de/_todo/%E5%BE%B7%E5%9B%BD%E4%B8%8E%E4%B8%AD%E5%9B%BD%E7%9A%84%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%99%E8%82%B2%E6%9C%89%E4%BD%95%E4%B8%8D%E5%90%8C/)

### [<title>- Changkun's Blog</title>](https://blog.changkun.de/_todo/%E9%85%8D%E7%BD%AEmysql%E4%B8%BB%E4%BB%8E%E7%83%AD%E5%A4%87%E5%AE%9E%E7%8E%B0%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB/)

### [<title>- Changkun's Blog</title>](https://blog.changkun.de/_todo/%E9%9D%A2%E5%90%91%E5%8D%8F%E8%AE%AE%E7%BC%96%E7%A8%8B%E5%B0%8F%E8%B0%88/)

### [<title>- Changkun's Blog</title>](https://blog.changkun.de/augmentedtouch/)

### [<title>- Changkun's Blog</title>](https://blog.changkun.de/clients/)

### [<title>- Changkun's Blog</title>](https://blog.changkun.de/demo/)

### [<title>- Changkun's Blog</title>](https://blog.changkun.de/math-modeling/)

### [<title>0.99... = 1? 尝试证明 - Changkun's Blog</title>](https://blog.changkun.de/_todo/0-99-1-%E5%B0%9D%E8%AF%95%E8%AF%81%E6%98%8E/)

### [<title>Hamilton 找不到「三维复数」的原因 - Changkun's Blog</title>](https://blog.changkun.de/_todo/hamilton-%E6%89%BE%E4%B8%8D%E5%88%B0%E4%B8%89%E7%BB%B4%E5%A4%8D%E6%95%B0%E7%9A%84%E5%8E%9F%E5%9B%A0/)

### [<title>Ideas - Changkun's Blog</title>](https://blog.changkun.de/ideas/)

### [<title>N 次方程，有N个根？ - Changkun's Blog</title>](https://blog.changkun.de/_todo/n-%E6%AC%A1%E6%96%B9%E7%A8%8B%E6%9C%89n%E4%B8%AA%E6%A0%B9/)

### [<title>sinx 与超越数 - Changkun's Blog</title>](https://blog.changkun.de/_todo/sinx-%E4%B8%8E%E8%B6%85%E8%B6%8A%E6%95%B0/)

### [<title>The Ultimate Brownie Pan - Changkun's Blog</title>](https://blog.changkun.de/_todo/the-ultimate-brownie-pan/)

### [<title>三斜求积 海伦公式 - Changkun's Blog</title>](https://blog.changkun.de/_todo/%E4%B8%89%E6%96%9C%E6%B1%82%E7%A7%AF-%E6%B5%B7%E4%BC%A6%E5%85%AC%E5%BC%8F/)

### [<title>关于教学质量的评价问题的分析与探讨 - Changkun's Blog</title>](https://blog.changkun.de/_todo/%E5%85%B3%E4%BA%8E%E6%95%99%E5%AD%A6%E8%B4%A8%E9%87%8F%E7%9A%84%E8%AF%84%E4%BB%B7%E9%97%AE%E9%A2%98%E7%9A%84%E5%88%86%E6%9E%90%E4%B8%8E%E6%8E%A2%E8%AE%A8/)

### [<title>图形拓扑论 趣话：奇趣矩阵 - Changkun's Blog</title>](https://blog.changkun.de/_todo/%E5%9B%BE%E5%BD%A2%E6%8B%93%E6%89%91%E8%AE%BA-%E8%B6%A3%E8%AF%9D%E5%A5%87%E8%B6%A3%E7%9F%A9%E9%98%B5/)

### [<title>基于PID与神经元算法的制动器试验台的控制方法分析 - Changkun's Blog</title>](https://blog.changkun.de/_todo/%E5%9F%BA%E4%BA%8Epid%E4%B8%8E%E7%A5%9E%E7%BB%8F%E5%85%83%E7%AE%97%E6%B3%95%E7%9A%84%E5%88%B6%E5%8A%A8%E5%99%A8%E8%AF%95%E9%AA%8C%E5%8F%B0%E7%9A%84%E6%8E%A7%E5%88%B6%E6%96%B9%E6%B3%95%E5%88%86%E6%9E%90/)

### [<title>基于非线性拟合与积分方法对储油罐的变位识别与罐容表标定相关问题的研究 - Changkun's Blog</title>](https://blog.changkun.de/_todo/%E5%9F%BA%E4%BA%8E%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%8B%9F%E5%90%88%E4%B8%8E%E7%A7%AF%E5%88%86%E6%96%B9%E6%B3%95%E5%AF%B9%E5%82%A8%E6%B2%B9%E7%BD%90%E7%9A%84%E5%8F%98%E4%BD%8D%E8%AF%86%E5%88%AB%E4%B8%8E%E7%BD%90%E5%AE%B9%E8%A1%A8%E6%A0%87%E5%AE%9A%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98%E7%9A%84%E7%A0%94%E7%A9%B6/)

### [<title>基于非线性规划方法的露天矿生产车辆的安排算法 - Changkun's Blog</title>](https://blog.changkun.de/_todo/%E5%9F%BA%E4%BA%8E%E9%9D%9E%E7%BA%BF%E6%80%A7%E8%A7%84%E5%88%92%E6%96%B9%E6%B3%95%E7%9A%84%E9%9C%B2%E5%A4%A9%E7%9F%BF%E7%94%9F%E4%BA%A7%E8%BD%A6%E8%BE%86%E7%9A%84%E5%AE%89%E6%8E%92%E7%AE%97%E6%B3%95/)

### [<title>基础不等式 - Changkun's Blog</title>](https://blog.changkun.de/_todo/%E5%9F%BA%E7%A1%80%E4%B8%8D%E7%AD%89%E5%BC%8F/)

### [<title>对最优漂流调度方案的研究（Study on the Optimal Scheduling Scheme for Drifting） - Changkun's Blog</title>](https://blog.changkun.de/_todo/%E5%AF%B9%E6%9C%80%E4%BC%98%E6%BC%82%E6%B5%81%E8%B0%83%E5%BA%A6%E6%96%B9%E6%A1%88%E7%9A%84%E7%A0%94%E7%A9%B6study-on-the-optimal-scheduling-scheme-for-drifting/)

### [<title>广义结合律 - Changkun's Blog</title>](https://blog.changkun.de/_todo/%E5%B9%BF%E4%B9%89%E7%BB%93%E5%90%88%E5%BE%8B/)

### [<title>数论定理证明 - Changkun's Blog</title>](https://blog.changkun.de/_todo/%E6%95%B0%E8%AE%BA%E5%AE%9A%E7%90%86%E8%AF%81%E6%98%8E/)

### [<title>极大初等图像变换浅谈 - Changkun's Blog</title>](https://blog.changkun.de/_todo/%E6%9E%81%E5%A4%A7%E5%88%9D%E7%AD%89%E5%9B%BE%E5%83%8F%E5%8F%98%E6%8D%A2%E6%B5%85%E8%B0%88/)

### [<title>概率论值古典概率模型 - Changkun's Blog</title>](https://blog.changkun.de/_todo/%E6%A6%82%E7%8E%87%E8%AE%BA%E5%80%BC%E5%8F%A4%E5%85%B8%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B/)

### [<title>深圳人口增长及其对医疗需求影响的预测 - Changkun's Blog</title>](https://blog.changkun.de/_todo/%E6%B7%B1%E5%9C%B3%E4%BA%BA%E5%8F%A3%E5%A2%9E%E9%95%BF%E5%8F%8A%E5%85%B6%E5%AF%B9%E5%8C%BB%E7%96%97%E9%9C%80%E6%B1%82%E5%BD%B1%E5%93%8D%E7%9A%84%E9%A2%84%E6%B5%8B/)

### [<title>论十九世纪单值函数论的发展对流形思想演变的影响以及微分流形的未来 - Changkun's Blog</title>](https://blog.changkun.de/_todo/%E8%AE%BA%E5%8D%81%E4%B9%9D%E4%B8%96%E7%BA%AA%E5%8D%95%E5%80%BC%E5%87%BD%E6%95%B0%E8%AE%BA%E7%9A%84%E5%8F%91%E5%B1%95%E5%AF%B9%E6%B5%81%E5%BD%A2%E6%80%9D%E6%83%B3%E6%BC%94%E5%8F%98%E7%9A%84%E5%BD%B1%E5%93%8D%E4%BB%A5%E5%8F%8A%E5%BE%AE%E5%88%86%E6%B5%81%E5%BD%A2%E7%9A%84%E6%9C%AA%E6%9D%A5/)

### [<title>论预测模型 - Changkun's Blog</title>](https://blog.changkun.de/_todo/%E8%AE%BA%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B/)

### [<title>试用 Fermat 数 再证素数无穷 - Changkun's Blog</title>](https://blog.changkun.de/_todo/%E8%AF%95%E7%94%A8-fermat-%E6%95%B0-%E5%86%8D%E8%AF%81%E7%B4%A0%E6%95%B0%E6%97%A0%E7%A9%B7/)

### [<title>话说矩阵 定义新变换 赢得矩阵 - Changkun's Blog</title>](https://blog.changkun.de/_todo/%E8%AF%9D%E8%AF%B4%E7%9F%A9%E9%98%B5-%E5%AE%9A%E4%B9%89%E6%96%B0%E5%8F%98%E6%8D%A2-%E8%B5%A2%E5%BE%97%E7%9F%A9%E9%98%B5/)

### [<title>这些年。 - Changkun's Blog</title>](https://blog.changkun.de/_todo/%E8%BF%99%E4%BA%9B%E5%B9%B4/)

### [<title>饮酒与驾车关系的微分方程模型分析与评价 - Changkun's Blog</title>](https://blog.changkun.de/_todo/%E9%A5%AE%E9%85%92%E4%B8%8E%E9%A9%BE%E8%BD%A6%E5%85%B3%E7%B3%BB%E7%9A%84%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%E6%A8%A1%E5%9E%8B%E5%88%86%E6%9E%90%E4%B8%8E%E8%AF%84%E4%BB%B7/)

### [<title>DockOne翻译列表 - DockOne.io</title>](http://dockone.io/question/4)

### [<title>几天前创建的容器能够使用docker exec 进入，运行几天之后docker exec 无法进入 - DockOne.io</title>](http://dockone.io/question/1541)

### [<title>使用eureka的微服务如何平滑的使用k8s的负载均衡？ - DockOne.io</title>](http://dockone.io/question/4076)

### [<title>Faas平台开发工程师招聘 - DockOne.io</title>](http://dockone.io/question/4075)

### [<title>joywork DevOps工程师招聘 - DockOne.io</title>](http://dockone.io/question/4074)

### [<title>docker容器内root用户的一些问题 - DockOne.io</title>](http://dockone.io/question/4073)

### [<title>实现微服务的高可用 - DockOne.io</title>](http://dockone.io/question/4072)

### [<title>时速云获B+轮战略融资，加速容器云PaaS规模化增长 - DockOne.io</title>](http://dockone.io/question/4071)

### [<title>Cloud Native Patterns 云原生模式 - DockOne.io</title>](http://dockone.io/question/4070)

### [<title>本地docker-compose创建的加入了同一个虚拟网络的容器，在宿主机里怎么访问？ - DockOne.io</title>](http://dockone.io/question/4069)

### [<title>docker单点故障问题 - DockOne.io</title>](http://dockone.io/question/4068)

### [<title>Docker Weekly 不维护了吗？ - DockOne.io</title>](http://dockone.io/question/4067)

### [<title>Docker安全性讨论 - DockOne.io</title>](http://dockone.io/question/4065)

### [<title>docker中文文档挂了? - DockOne.io</title>](http://dockone.io/question/4064)

### [<title>pod在mount到宿主机的时候如何路径上带上podname - DockOne.io</title>](http://dockone.io/question/4063)

### [<title>【深圳】环球易购招聘容器云leader（基于Kubernetes的Paas平台） - DockOne.io</title>](http://dockone.io/question/4062)

### [<title>k8s搭建zookeeper 失败， - DockOne.io</title>](http://dockone.io/question/4061)

### [<title>rancher: Failed to obtain metrics. The metrics service may not be available. - DockOne.io</title>](http://dockone.io/question/4058)

### [<title>devicemapper存储，容器mount点的问题 - DockOne.io</title>](http://dockone.io/question/340)

### [<title>k8s 中在自定义的 namespace 中创建 ingress 后不能访问 - DockOne.io</title>](http://dockone.io/question/4057)

### [<title>Upcoming Sokol header API changes (Feb 2021)</title>](https://floooh.github.io/2021/02/07/sokol-api-overhaul.html)

### [<title>Automatic Language Bindings</title>](https://floooh.github.io/2020/08/23/sokol-bindgen.html)

### [<title>Sokol headers: spring 2020 update</title>](https://floooh.github.io/2020/04/26/sokol-spring-2020-update.html)

### [<title>sokol_gfx.h Backend Tour: Metal</title>](https://floooh.github.io/2020/02/20/sokol-gfx-backend-tour-metal.html)

### [<title>sokol_gfx.h Backend Tour: D3D11</title>](https://floooh.github.io/2020/02/18/sokol-gfx-backend-tour-d3d11.html)

### [<title>sokol_gfx.h Backend Tour: OpenGL</title>](https://floooh.github.io/2020/02/17/sokol-gfx-backend-tour-gl.html)

### [<title>A new cycle-stepped 6502 CPU emulator</title>](https://floooh.github.io/2019/12/13/cycle-stepped-6502.html)

### [<title>Modern C for C++ Peeps</title>](https://floooh.github.io/2019/09/27/modern-c-for-cpp-peeps.html)

### [<title>A small sokol_gfx.h API update</title>](https://floooh.github.io/2019/01/12/sokol-apply-pipeline.html)

### [<title>Emulators as embedded file viewers</title>](https://floooh.github.io/2019/01/05/wasm-embedding.html)

### [<title>Reasons why bugs might feel "impossible"</title>](https://jvns.ca/blog/2021/06/08/reasons-why-bugs-might-feel-impossible/)

### [<title>You can now buy print version of my zines!</title>](https://jvns.ca/blog/2021/06/02/you-can-now-buy-print-version-of-my-zines-/)

### [<title>Blog about what you've struggled with</title>](https://jvns.ca/blog/2021/05/24/blog-about-what-you-ve-struggled-with/)

### [<title>How to look at the stack with gdb</title>](https://jvns.ca/blog/2021/05/17/how-to-look-at-the-stack-in-gdb/)

### [<title>The OSI model doesn't map well to TCP/IP</title>](https://jvns.ca/blog/2021/05/11/what-s-the-osi-model-/)

### [<title>I put all of my comics online!</title>](https://jvns.ca/blog/2021/05/02/publishing-comics/)

### [<title>Notes on building debugging puzzles</title>](https://jvns.ca/blog/2021/04/16/notes-on-debugging-puzzles/)

### [<title>What problems do people solve with strace?</title>](https://jvns.ca/blog/2021/04/03/what-problems-do-people-solve-with-strace/)

### [<title>A tool to spy on your DNS queries: dnspeep</title>](https://jvns.ca/blog/2021/03/31/dnspeep-tool/)

### [<title>Get better at programming by learning how things work</title>](https://jvns.ca/blog/learn-how-things-work/)

### [<title>Things your manager might not know</title>](https://jvns.ca/blog/things-your-manager-might-not-know/)

### [<title>A little tool to make DNS queries</title>](https://jvns.ca/blog/2021/02/24/a-little-tool-to-make-dns-queries/)

### [<title>Firecracker: start a VM in less than a second</title>](https://jvns.ca/blog/2021/01/23/firecracker--start-a-vm-in-less-than-a-second/)

### [<title>Server-sent events: a simple way to stream events from a server</title>](https://jvns.ca/blog/2021/01/12/day-36--server-sent-events-are-cool--and-a-fun-bug/)

### [<title>Daily blog posts about my time at RC</title>](https://jvns.ca/blog/2021/01/08/some-extra-daily-blog-posts/)

### [<title>Docker Compose: a nice way to set up a dev environment</title>](https://jvns.ca/blog/2021/01/04/docker-compose-is-nice/)

### [<title>2020: Year in review</title>](https://jvns.ca/blog/2020/12/31/2020--year-in-review/)

### [<title>How I write useful programming comics</title>](https://jvns.ca/blog/2020/12/05/how-i-write-useful-programming-comics/)

### [<title>An attempt at implementing char-rnn with PyTorch</title>](https://jvns.ca/blog/2020/11/30/implement-char-rnn-in-pytorch/)

### [<title>New zine: Hell Yes! CSS!</title>](https://jvns.ca/blog/2020/11/22/new-zine--hell-yes--css-/)

### [<title>What is Data Parallel C++ | CodeGuru.com</title>](http://www.codeguru.com/cpp/cpp/what-is-data-parallel-c.html)

### [<title>Pure Virtual C++ Event Summary | CodeGuru.com</title>](http://www.codeguru.com/cpp/pure-virtual-c-event-summary.html)

### [<title>Visual Studio vs. Visual Studio Code</title>](http://www.codeguru.com/cpp/v-s/visual-studio-vs.-visual-studio-code.html)

### [<title>CLinkedList Doubly Linked List Class</title>](http://www.codeguru.com/cpp/cpp/algorithms/lists/article.php/c5121/CLinkedList-Doubly-Linked-List-Class.htm)

### [[2107.01381] Recent Advancements In Distributed System Communications](http://arxiv.org/abs/2107.01381)


  Overheads in Operating System kernel network stacks and sockets have been
hindering OSes from managing networking operations efficiently for years.
Moreover, when building Remote Procedure Calls over TCP, certain TCP features
do not match the needs of RPCs, imposing additional overheads. These issues
degrade the performance of distributed systems, which rely on fast
communications between machines to be able to serve a large number of client
requests with low latency and high throughput. The purpose of this literature
survey is to look into recent proposals in research literature that aim to
overcome these issues. The survey investigates research literature published
between 2010-2020, in order to include important advancements during the most
recent decade at the time of writing. The proposals found in papers have been
categorized into hardware-based and software-based approaches. The former
require specialized hardware to offer high communications performance. The
latter are implemented in software and don't rely on specialized hardware or
require only certain hardware features. Furthermore, the proposals where also
classified according to whether they implement kernel bypass, to avoid using
the Operating System kernel network stack, or not. The hardware-based
approaches examined here are RDMA, programmable Network Interface Controllers
(NIC) and System-on-a-Chip (SoC), while the software-based approaches include
optimized socket implementations and RPC frameworks, as well as user space
networking.

    

### [[2107.01398] TrafPy: Benchmarking Data Centre Network Systems](http://arxiv.org/abs/2107.01398)


  Benchmarking is commonly used in research fields such as computer
architecture design and machine learning as a powerful paradigm for rigorously
assessing, comparing, and developing novel technologies. However, the data
centre networking community lacks a standard open-access benchmark. This is
curtailing the community's understanding of existing systems and hindering the
ability with which novel technologies can be developed, compared, and tested.
We present TrafPy; an open-access framework for generating both realistic and
custom data centre network traffic traces. TrafPy is compatible with any
simulation, emulation, or experimentation environment, and can be used for
standardised benchmarking and for investigating the properties and limitations
of network systems such as schedulers, switches, routers, and resource
managers. To demonstrate the efficacy of TrafPy, we use it to conduct a
thorough investigation into the sensitivity of 4 canonical scheduling
algorithms (shortest remaining processing time, fair share, first fit, and
random) to varying traffic trace characteristics. We show how the fundamental
scheduler performance insights revealed by these tests translate to 4 realistic
data centre network types; University, Private Enterprise, Commercial Cloud,
and Social Media Cloud. We then draw conclusions as to which types of
scheduling policies are most suited to which types of network load conditions
and traffic characteristics, leading to the possibility of application-informed
decision making at the design stage and new dynamically adaptable scheduling
policies. TrafPy is open-sourced via GitHub and all data associated with this
manuscript via RDR.

    

### [[2107.01427] Multi-Objective Congestion Control](http://arxiv.org/abs/2107.01427)


  Decades of research on Internet congestion control (CC) has produced a
plethora of algorithms that optimize for different performance objectives.
Applications face the challenge of choosing the most suitable algorithm based
on their needs, and it takes tremendous efforts and expertise to customize CC
algorithms when new demands emerge. In this paper, we explore a basic question:
can we design a single CC algorithm to satisfy different objectives? We propose
MOCC, the first multi-objective congestion control algorithm that attempts to
address this challenge. The core of MOCC is a novel multi-objective
reinforcement learning framework for CC that can automatically learn the
correlations between different application requirements and the corresponding
optimal control policies. Under this framework, MOCC further applies transfer
learning to transfer the knowledge from past experience to new applications,
quickly adapting itself to a new objective even if it is unforeseen. We provide
both user-space and kernel-space implementation of MOCC. Real-world experiments
and extensive simulations show that MOCC well supports multi-objective,
competing or outperforming the best existing CC algorithms on individual
objectives, and quickly adapting to new applications (e.g., 14.2x faster than
prior work) without compromising old ones.

    

### [[2107.01914] Ranking Online Social Users by their Influence](http://arxiv.org/abs/2107.01914)


  We introduce an original mathematical model to analyse the diffusion of posts
within a generic online social platform. The main novelty is that each user is
not simply considered as a node on the social graph, but is further equipped
with his/her own Wall and Newsfeed, and has his/her own individual self-posting
and re-posting activity. As a main result using our developed model, we derive
in closed form the probabilities that posts originating from a given user are
found on the Wall and Newsfeed of any other. These are the solution of a linear
system of equations, which can be resolved iteratively. In fact, our model is
very flexible with respect to the modelling assumptions. Using the
probabilities derived from the solution, we define a new measure of per-user
influence over the entire network, the $\Psi$-score, which combines the user
position on the graph with user (re-)posting activity. In the homogeneous case
where all users have the same activity rates, it is shown that a variant of the
$\Psi$-score is equal to PageRank. Furthermore, we compare the new model and
its $\Psi$-score against the empirical influence measured from very large data
traces (Twitter, Weibo). The results illustrate that these new tools can
accurately rank influencers with asymmetric (re-)posting activity for such real
world applications.

    

### [[1910.06895] CRISLoc: Reconstructable CSI Fingerprintingfor Indoor Smartphone Localization](http://arxiv.org/abs/1910.06895)


  Channel state information (CSI) based fingerprinting for WIFI indoor
localization has attracted lots of attention very recently.The frequency
diverse and temporally stable CSI better represents the location dependent
channel characteristics than the coarsereceived signal strength (RSS). However,
the acquisition of CSI requires the cooperation of access points (APs) and
involves only dataframes, which imposes restrictions on real-world deployment.
In this paper, we present CRISLoc, the first CSI fingerprinting
basedlocalization prototype system using ubiquitous smartphones. CRISLoc
operates in a completely passive mode, overhearing thepackets on-the-fly for
his own CSI acquisition. The smartphone CSI is sanitized via calibrating the
distortion enforced by WiFi amplifiercircuits. CRISLoc tackles the challenge of
altered APs with a joint clustering and outlier detection method to find them.
A novel transferlearning approach is proposed to reconstruct the
high-dimensional CSI fingerprint database on the basis of the outdated
fingerprintsand a few fresh measurements, and an enhanced KNN approach is
proposed to pinpoint the location of a smartphone. Our studyreveals important
properties about the stability and sensitivity of smartphone CSI that has not
been reported previously. Experimentalresults show that CRISLoc can achieve a
mean error of around 0.29m in a6m times 8mresearch laboratory. The mean error
increases by 5.4 cm and 8.6 cm upon the movement of one and two APs, which
validates the robustness of CRISLoc against environment changes.

    

### [[2104.04572] Smart and Secure CAV Networks Empowered by AI-Enabled Blockchain: Next Frontier for Intelligent Safe-Driving Assessment](http://arxiv.org/abs/2104.04572)


  Securing safe-driving for connected and autonomous vehicles (CAVs) continues
to be a widespread concern despite various sophisticated functions delivered by
artificial intelligence for in-vehicle devices. Besides, diverse malicious
network attacks become ubiquitous along with the worldwide implementation of
the Internet of Vehicles, which exposes a range of reliability and privacy
threats for managing data in CAV networks. Combined with the fact that the
capability of existing CAVs in handling intensive computation tasks is limited,
this implies a need for designing an efficient assessment system to guarantee
autonomous driving safety without compromising data security. Motivated by
this, in this article, we propose a novel framework, namely Blockchain-enabled
intElligent Safe-driving assessmenT (BEST), that offers a smart and reliable
approach for conducting safe driving supervision while protecting vehicular
information. Specifically, a promising solution that exploits a long short-term
memory model is introduced to assess the safety level of the moving CAVs. Then,
we investigate how a distributed blockchain obtains adequate trustworthiness
and robustness for CAV data by adopting a byzantine fault tolerance-based
delegated proof-of-stake consensus mechanism. Simulation results demonstrate
that our presented BEST gains better data credibility with a higher prediction
accuracy for vehicular safety assessment when compared with existing schemes.
Finally, we discuss several open challenges that need to be addressed in future
CAV networks.

    

### [[2104.10533] On the Path to 6G: Low Orbit is the New High](http://arxiv.org/abs/2104.10533)


  We provide an overview of the key aspects of low Earth orbit (LEO) satellite
communication networks towards 6G. Offering space-based Internet services with
mega-constellations of LEO satellites is a promising solution to connecting the
unserved and underserved, thereby complementing the coverage of terrestrial
networks and contributing to bridging the digital divide. Integrating LEO
satellite access in cellular systems will be one of the connectivity's new
frontiers on the path to 6G. There are however challenges from operational and
technical obstacles to regulatory hurdles facing the development of LEO based
communication networks. In this article, we review the evolution of LEO
satellite constellations and capabilities, analyze the technical challenges and
solutions of LEO satellite access networks, discuss the standardization aspects
from 5G evolution to 6G, and investigate the use cases and business
considerations towards 6G.

    

### [[2107.01003] PI$^2$ Parameters](http://arxiv.org/abs/2107.01003)


  This report gives the reasoning for the parameter settings of the reference
Linux implementation of the PI$^2$ AQM, focusing initially on the target queue
delay.

    

### [[2104.03044] A First Look into the Structural Properties and Resilience of Blockchain Overlays](http://arxiv.org/abs/2104.03044)


  Blockchain (BC) systems are highly distributed peer-to-peer networks that
offer an alternative to centralized services and promise robustness to
coordinated attacks. However, the resilience and overall security of a BC
system rests heavily on the structural properties of its underlying
peer-to-peer overlay. Despite their success, BC overlay networks' critical
design aspects, connectivity properties and network-layer inter-dependencies
are still poorly understood. In this work, we set out to fill this gap and
study the most important overlay network structural properties and robustness
to targeted attacks of seven distinct BC networks. In particular, we probe and
crawl these BC networks every two hours to gather information about all their
available peers, over a duration of 28 days. We analyze 335 network snapshots
per BC network, for a total of 2345 snapshots. We construct, at frequent
intervals, connectivity graphs for each BC network, consisting of all potential
connections between peers. We analyze the structural graph properties of these
networks and compare them across the seven BC networks. We also study how these
properties associate with the resilience of each network to partitioning
attacks, i.e., when peers are selected, attacked and taken offline, using
different selection strategies driven by the aforementioned structural
properties. In fact, we show that by targeting fewer than 10 highly-connected
peers, major BCs such as Bitcoin can be partitioned into disjoint, i.e.,
disconnected, components. Finally, we uncover a hidden interconnection between
different BC networks, where certain peers participate in more than one BC
network, which has serious implications for the robustness of the overall BC
network ecosystem.

    

### [[2107.01214] Truncated Marginal Neural Ratio Estimation](http://arxiv.org/abs/2107.01214)


  Parametric stochastic simulators are ubiquitous in science, often featuring
high-dimensional input parameters and/or an intractable likelihood. Performing
Bayesian parameter inference in this context can be challenging. We present a
neural simulator-based inference algorithm which simultaneously offers
simulation efficiency and fast empirical posterior testability, which is unique
among modern algorithms. Our approach is simulation efficient by simultaneously
estimating low-dimensional marginal posteriors instead of the joint posterior
and by proposing simulations targeted to an observation of interest via a prior
suitably truncated by an indicator function. Furthermore, by estimating a
locally amortized posterior our algorithm enables efficient empirical tests of
the robustness of the inference results. Such tests are important for
sanity-checking inference in real-world applications, which do not feature a
known ground truth. We perform experiments on a marginalized version of the
simulation-based inference benchmark and two complex and narrow posteriors,
highlighting the simulator efficiency of our algorithm as well as the quality
of the estimated marginal posteriors. Implementation on GitHub.

    

### [[2107.01238] Solving Machine Learning Problems](http://arxiv.org/abs/2107.01238)


  Can a machine learn Machine Learning? This work trains a machine learning
model to solve machine learning problems from a University undergraduate level
course. We generate a new training set of questions and answers consisting of
course exercises, homework, and quiz questions from MIT's 6.036 Introduction to
Machine Learning course and train a machine learning model to answer these
questions. Our system demonstrates an overall accuracy of 96% for open-response
questions and 97% for multiple-choice questions, compared with MIT students'
average of 93%, achieving grade A performance in the course, all in real-time.
Questions cover all 12 topics taught in the course, excluding coding questions
or questions with images. Topics include: (i) basic machine learning
principles; (ii) perceptrons; (iii) feature extraction and selection; (iv)
logistic regression; (v) regression; (vi) neural networks; (vii) advanced
neural networks; (viii) convolutional neural networks; (ix) recurrent neural
networks; (x) state machines and MDPs; (xi) reinforcement learning; and (xii)
decision trees. Our system uses Transformer models within an encoder-decoder
architecture with graph and tree representations. An important aspect of our
approach is a data-augmentation scheme for generating new example problems. We
also train a machine learning model to generate problem hints. Thus, our system
automatically generates new questions across topics, answers both open-response
questions and multiple-choice questions, classifies problems, and generates
problem hints, pushing the envelope of AI for STEM education.

    

### [[2107.01253] Designing Machine Learning Pipeline Toolkit for AutoML Surrogate Modeling Optimization](http://arxiv.org/abs/2107.01253)


  The pipeline optimization problem in machine learning requires simultaneous
optimization of pipeline structures and parameter adaptation of their elements.
Having an elegant way to express these structures can help lessen the
complexity in the management and analysis of their performances together with
the different choices of optimization strategies. With these issues in mind, we
created the AMLP toolkit which facilitates the creation and evaluation of
complex machine learning pipeline structures using simple expressions. We use
AMLP to find optimal pipeline signatures, datamine them, and use these
datamined features to speed-up learning and prediction. We formulated a
two-stage pipeline optimization with surrogate modeling in AMLP which
outperforms other AutoML approaches with a 4-hour time budget in less than 5
minutes of AMLP computation time.

    

### [[2107.01264] Beyond Value-Function Gaps: Improved Instance-Dependent Regret Bounds for Episodic Reinforcement Learning](http://arxiv.org/abs/2107.01264)


  We provide improved gap-dependent regret bounds for reinforcement learning in
finite episodic Markov decision processes. Compared to prior work, our bounds
depend on alternative definitions of gaps. These definitions are based on the
insight that, in order to achieve a favorable regret, an algorithm does not
need to learn how to behave optimally in states that are not reached by an
optimal policy. We prove tighter upper regret bounds for optimistic algorithms
and accompany them with new information-theoretic lower bounds for a large
class of MDPs. Our results show that optimistic algorithms can not achieve the
information-theoretic lower bounds even in deterministic MDPs unless there is a
unique optimal policy.

    

### [[2107.01269] Dual Causal/Non-Causal Self-Attention for Streaming End-to-End Speech Recognition](http://arxiv.org/abs/2107.01269)


  Attention-based end-to-end automatic speech recognition (ASR) systems have
recently demonstrated state-of-the-art results for numerous tasks. However, the
application of self-attention and attention-based encoder-decoder models
remains challenging for streaming ASR, where each word must be recognized
shortly after it was spoken. In this work, we present the dual
causal/non-causal self-attention (DCN) architecture, which in contrast to
restricted self-attention prevents the overall context to grow beyond the
look-ahead of a single layer when used in a deep architecture. DCN is compared
to chunk-based and restricted self-attention using streaming transformer and
conformer architectures, showing improved ASR performance over restricted
self-attention and competitive ASR results compared to chunk-based
self-attention, while providing the advantage of frame-synchronous processing.
Combined with triggered attention, the proposed streaming end-to-end ASR
systems obtained state-of-the-art results on the LibriSpeech, HKUST, and
Switchboard ASR tasks.

    

### [[2107.01272] Physics-Guided Deep Learning for Dynamical Systems: A survey](http://arxiv.org/abs/2107.01272)


  Modeling complex physical dynamics is a fundamental task in science and
engineering. Traditional physics-based models are interpretable but rely on
rigid assumptions. And the direct numerical approximation is usually
computationally intensive, requiring significant computational resources and
expertise. While deep learning (DL) provides novel alternatives for efficiently
recognizing complex patterns and emulating nonlinear dynamics, it does not
necessarily obey the governing laws of physical systems, nor do they generalize
well across different systems. Thus, the study of physics-guided DL emerged and
has gained great progress. It aims to take the best from both physics-based
modeling and state-of-the-art DL models to better solve scientific problems. In
this paper, we provide a structured overview of existing methodologies of
integrating prior physical knowledge or physics-based modeling into DL and
discuss the emerging opportunities.

    

### [[2107.01273] Visual Time Series Forecasting: An Image-driven Approach](http://arxiv.org/abs/2107.01273)


  In this work, we address time-series forecasting as a computer vision task.
We capture input data as an image and train a model to produce the subsequent
image. This approach results in predicting distributions as opposed to
pointwise values. To assess the robustness and quality of our approach, we
examine various datasets and multiple evaluation metrics. Our experiments show
that our forecasting tool is effective for cyclic data but somewhat less for
irregular data such as stock prices. Importantly, when using image-based
evaluation metrics, we find our method to outperform various baselines,
including ARIMA, and a numerical variation of our deep learning approach.

    

### [[2107.01275] Relaxed Attention: A Simple Method to Boost Performance of End-to-End Automatic Speech Recognition](http://arxiv.org/abs/2107.01275)


  Recently, attention-based encoder-decoder (AED) models have shown high
performance for end-to-end automatic speech recognition (ASR) across several
tasks. Addressing overconfidence in such models, in this paper we introduce the
concept of relaxed attention, which is a simple gradual injection of a uniform
distribution to the encoder-decoder attention weights during training that is
easily implemented with two lines of code. We investigate the effect of relaxed
attention across different AED model architectures and two prominent ASR tasks,
Wall Street Journal (WSJ) and Librispeech. We found that transformers trained
with relaxed attention outperform the standard baseline models consistently
during decoding with external language models. On WSJ, we set a new benchmark
for transformer-based end-to-end speech recognition with a word error rate of
3.65%, outperforming state of the art (4.20%) by 13.1% relative, while
introducing only a single hyperparameter. Upon acceptance, models will be
published on github.

    

### [[2107.01277] Non-Comparative Fairness for Human-Auditing and Its Relation to Traditional Fairness Notions](http://arxiv.org/abs/2107.01277)


  Bias evaluation in machine-learning based services (MLS) based on traditional
algorithmic fairness notions that rely on comparative principles is practically
difficult, making it necessary to rely on human auditor feedback. However, in
spite of taking rigorous training on various comparative fairness notions,
human auditors are known to disagree on various aspects of fairness notions in
practice, making it difficult to collect reliable feedback. This paper offers a
paradigm shift to the domain of algorithmic fairness via proposing a new
fairness notion based on the principle of non-comparative justice. In contrary
to traditional fairness notions where the outcomes of two individuals/groups
are compared, our proposed notion compares the MLS' outcome with a desired
outcome for each input. This desired outcome naturally describes a human
auditor's expectation, and can be easily used to evaluate MLS on crowd-auditing
platforms. We show that any MLS can be deemed fair from the perspective of
comparative fairness (be it in terms of individual fairness, statistical
parity, equal opportunity or calibration) if it is non-comparatively fair with
respect to a fair auditor. We also show that the converse holds true in the
context of individual fairness. Given that such an evaluation relies on the
trustworthiness of the auditor, we also present an approach to identify fair
and reliable auditors by estimating their biases with respect to a given set of
sensitive attributes, as well as quantify the uncertainty in the estimation of
biases within a given MLS. Furthermore, all of the above results are also
validated on COMPAS, German credit and Adult Census Income datasets.

    

### [[2107.01281] Prescient teleoperation of humanoid robots](http://arxiv.org/abs/2107.01281)


  Humanoid robots could be versatile and intuitive human avatars that operate
remotely in inaccessible places: the robot could reproduce in the remote
location the movements of an operator equipped with a wearable motion capture
device while sending visual feedback to the operator. While substantial
progress has been made on transferring ("retargeting") human motions to
humanoid robots, a major problem preventing the deployment of such systems in
real applications is the presence of communication delays between the human
input and the feedback from the robot: even a few hundred milliseconds of delay
can irreversibly disturb the operator, let alone a few seconds. To overcome
these delays, we introduce a system in which a humanoid robot executes commands
before it actually receives them, so that the visual feedback appears to be
synchronized to the operator, whereas the robot executed the commands in the
past. To do so, the robot continuously predicts future commands by querying a
machine learning model that is trained on past trajectories and conditioned on
the last received commands. In our experiments, an operator was able to
successfully control a humanoid robot (32 degrees of freedom) with stochastic
delays up to 2 seconds in several whole-body manipulation tasks, including
reaching different targets, picking up, and placing a box at distinct
locations.

    

### [[2107.01285] Optimizing ROC Curves with a Sort-Based Surrogate Loss Function for Binary Classification and Changepoint Detection](http://arxiv.org/abs/2107.01285)


  Receiver Operating Characteristic (ROC) curves are plots of true positive
rate versus false positive rate which are useful for evaluating binary
classification models, but difficult to use for learning since the Area Under
the Curve (AUC) is non-convex. ROC curves can also be used in other problems
that have false positive and true positive rates such as changepoint detection.
We show that in this more general context, the ROC curve can have loops, points
with highly sub-optimal error rates, and AUC greater than one. This observation
motivates a new optimization objective: rather than maximizing the AUC, we
would like a monotonic ROC curve with AUC=1 that avoids points with large
values for Min(FP,FN). We propose a convex relaxation of this objective that
results in a new surrogate loss function called the AUM, short for Area Under
Min(FP, FN). Whereas previous loss functions are based on summing over all
labeled examples or pairs, the AUM requires a sort and a sum over the sequence
of points on the ROC curve. We show that AUM directional derivatives can be
efficiently computed and used in a gradient descent learning algorithm. In our
empirical study of supervised binary classification and changepoint detection
problems, we show that our new AUM minimization learning algorithm results in
improved AUC and comparable speed relative to previous baselines.

    

### [[2107.01296] Subspace Clustering Based Analysis of Neural Networks](http://arxiv.org/abs/2107.01296)


  Tools to analyze the latent space of deep neural networks provide a step
towards better understanding them. In this work, we motivate sparse subspace
clustering (SSC) with an aim to learn affinity graphs from the latent structure
of a given neural network layer trained over a set of inputs. We then use tools
from Community Detection to quantify structures present in the input. These
experiments reveal that as we go deeper in a network, inputs tend to have an
increasing affinity to other inputs of the same class. Subsequently, we utilise
matrix similarity measures to perform layer-wise comparisons between affinity
graphs. In doing so we first demonstrate that when comparing a given layer
currently under training to its final state, the shallower the layer of the
network, the quicker it is to converge than the deeper layers. When performing
a pairwise analysis of the entire network architecture, we observe that, as the
network increases in size, it reorganises from a state where each layer is
moderately similar to its neighbours, to a state where layers within a block
have high similarity than to layers in other blocks. Finally, we analyze the
learned affinity graphs of the final convolutional layer of the network and
demonstrate how an input's local neighbourhood affects its classification by
the network.

    

### [[2107.01301] Implicit Greedy Rank Learning in Autoencoders via Overparameterized Linear Networks](http://arxiv.org/abs/2107.01301)


  Deep linear networks trained with gradient descent yield low rank solutions,
as is typically studied in matrix factorization. In this paper, we take a step
further and analyze implicit rank regularization in autoencoders. We show
greedy learning of low-rank latent codes induced by a linear sub-network at the
autoencoder bottleneck. We further propose orthogonal initialization and
principled learning rate adjustment to mitigate sensitivity of training
dynamics to spectral prior and linear depth. With linear autoencoders on
synthetic data, our method converges stably to ground-truth latent code rank.
With nonlinear autoencoders, our method converges to latent ranks optimal for
downstream classification and image sampling.

    

### [[2107.01303] Data-driven mapping between functional connectomes using optimal transport](http://arxiv.org/abs/2107.01303)


  Functional connectomes derived from functional magnetic resonance imaging
have long been used to understand the functional organization of the brain.
Nevertheless, a connectome is intrinsically linked to the atlas used to create
it. In other words, a connectome generated from one atlas is different in scale
and resolution compared to a connectome generated from another atlas. Being
able to map connectomes and derived results between different atlases without
additional pre-processing is a crucial step in improving interpretation and
generalization between studies that use different atlases. Here, we use optimal
transport, a powerful mathematical technique, to find an optimum mapping
between two atlases. This mapping is then used to transform time series from
one atlas to another in order to reconstruct a connectome. We validate our
approach by comparing transformed connectomes against their "gold-standard"
counterparts (i.e., connectomes generated directly from an atlas) and
demonstrate the utility of transformed connectomes by applying these
connectomes to predictive models based on a different atlas. We show that these
transformed connectomes are significantly similar to their "gold-standard"
counterparts and maintain individual differences in brain-behavior
associations, demonstrating both the validity of our approach and its utility
in downstream analyses. Overall, our approach is a promising avenue to increase
the generalization of connectome-based results across different atlases.

    

### [[2107.01310] Clustering of Time Series Data with Prior Geographical Information](http://arxiv.org/abs/2107.01310)


  Time Series data are broadly studied in various domains of transportation
systems. Traffic data area challenging example of spatio-temporal data, as it
is multi-variate time series with high correlations in spatial and temporal
neighborhoods. Spatio-temporal clustering of traffic flow data find similar
patterns in both spatial and temporal domain, where it provides better
capability for analyzing a transportation network, and improving related
machine learning models, such as traffic flow prediction and anomaly detection.
In this paper, we propose a spatio-temporal clustering model, where it clusters
time series data based on spatial and temporal contexts. We propose a variation
of a Deep Embedded Clustering(DEC) model for finding spatio-temporal clusters.
The proposed model Spatial-DEC (S-DEC) use prior geographical information in
building latent feature representations. We also define evaluation metrics for
spatio-temporal clusters. Not only do the obtained clusters have better
temporal similarity when evaluated using DTW distance, but also the clusters
better represents spatial connectivity and dis-connectivity. We use traffic
flow data obtained by PeMS in our analysis. The results show that the proposed
Spatial-DEC can find more desired spatio-temporal clusters.

    

### [[2107.01319] Learning Hierarchical Graph Neural Networks for Image Clustering](http://arxiv.org/abs/2107.01319)


  We propose a hierarchical graph neural network (GNN) model that learns how to
cluster a set of images into an unknown number of identities using a training
set of images annotated with labels belonging to a disjoint set of identities.
Our hierarchical GNN uses a novel approach to merge connected components
predicted at each level of the hierarchy to form a new graph at the next level.
Unlike fully unsupervised hierarchical clustering, the choice of grouping and
complexity criteria stems naturally from supervision in the training set. The
resulting method, Hi-LANDER, achieves an average of 54% improvement in F-score
and 8% increase in Normalized Mutual Information (NMI) relative to current
GNN-based clustering algorithms. Additionally, state-of-the-art GNN-based
methods rely on separate models to predict linkage probabilities and node
densities as intermediate steps of the clustering process. In contrast, our
unified framework achieves a seven-fold decrease in computational cost. We
release our training and inference code at
this https URL.

    

### [[2107.01323] Minimum Wasserstein Distance Estimator under Finite Location-scale Mixtures](http://arxiv.org/abs/2107.01323)


  When a population exhibits heterogeneity, we often model it via a finite
mixture: decompose it into several different but homogeneous subpopulations.
Contemporary practice favors learning the mixtures by maximizing the likelihood
for statistical efficiency and the convenient EM-algorithm for numerical
computation. Yet the maximum likelihood estimate (MLE) is not well defined for
the most widely used finite normal mixture in particular and for finite
location-scale mixture in general. We hence investigate feasible alternatives
to MLE such as minimum distance estimators. Recently, the Wasserstein distance
has drawn increased attention in the machine learning community. It has
intuitive geometric interpretation and is successfully employed in many new
applications. Do we gain anything by learning finite location-scale mixtures
via a minimum Wasserstein distance estimator (MWDE)? This paper investigates
this possibility in several respects. We find that the MWDE is consistent and
derive a numerical solution under finite location-scale mixtures. We study its
robustness against outliers and mild model mis-specifications. Our moderate
scaled simulation study shows the MWDE suffers some efficiency loss against a
penalized version of MLE in general without noticeable gain in robustness. We
reaffirm the general superiority of the likelihood based learning strategies
even for the non-regular finite location-scale mixtures.

    

### [[2107.01325] Fair Decision Rules for Binary Classification](http://arxiv.org/abs/2107.01325)


  In recent years, machine learning has begun automating decision making in
fields as varied as college admissions, credit lending, and criminal
sentencing. The socially sensitive nature of some of these applications
together with increasing regulatory constraints has necessitated the need for
algorithms that are both fair and interpretable. In this paper we consider the
problem of building Boolean rule sets in disjunctive normal form (DNF), an
interpretable model for binary classification, subject to fairness constraints.
We formulate the problem as an integer program that maximizes classification
accuracy with explicit constraints on two different measures of classification
parity: equality of opportunity and equalized odds. Column generation
framework, with a novel formulation, is used to efficiently search over
exponentially many possible rules. When combined with faster heuristics, our
method can deal with large data-sets. Compared to other fair and interpretable
classifiers, our method is able to find rule sets that meet stricter notions of
fairness with a modest trade-off in accuracy.

    

### [[2107.01326] SHORING: Design Provable Conditional High-Order Interaction Network via Symbolic Testing](http://arxiv.org/abs/2107.01326)


  Deep learning provides a promising way to extract effective representations
from raw data in an end-to-end fashion and has proven its effectiveness in
various domains such as computer vision, natural language processing, etc.
However, in domains such as content/product recommendation and risk management,
where sequence of event data is the most used raw data form and experts derived
features are more commonly used, deep learning models struggle to dominate the
game. In this paper, we propose a symbolic testing framework that helps to
answer the question of what kinds of expert-derived features could be learned
by a neural network. Inspired by this testing framework, we introduce an
efficient architecture named SHORING, which contains two components:
\textit{event network} and \textit{sequence network}. The \textit{event}
network learns arbitrarily yet efficiently high-order \textit{event-level}
embeddings via a provable reparameterization trick, the \textit{sequence}
network aggregates from sequence of \textit{event-level} embeddings. We argue
that SHORING is capable of learning certain standard symbolic expressions which
the standard multi-head self-attention network fails to learn, and conduct
comprehensive experiments and ablation studies on four synthetic datasets and
three real-world datasets. The results show that SHORING empirically
outperforms the state-of-the-art methods.

    

### [[2107.01330] SPI-GAN: Towards Single-Pixel Imaging through Generative Adversarial Network](http://arxiv.org/abs/2107.01330)


  Single-pixel imaging is a novel imaging scheme that has gained popularity due
to its huge computational gain and potential for a low-cost alternative to
imaging beyond the visible spectrum. The traditional reconstruction methods
struggle to produce a clear recovery when one limits the number of illumination
patterns from a spatial light modulator. As a remedy, several
deep-learning-based solutions have been proposed which lack good generalization
ability due to the architectural setup and loss functions. In this paper, we
propose a generative adversarial network-based reconstruction framework for
single-pixel imaging, referred to as SPI-GAN. Our method can reconstruct images
with 17.92 dB PSNR and 0.487 SSIM, even if the sampling ratio drops to 5%. This
facilitates much faster reconstruction making our method suitable for
single-pixel video. Furthermore, our ResNet-like architecture for the generator
leads to useful representation learning that allows us to reconstruct
completely unseen objects. The experimental results demonstrate that SPI-GAN
achieves significant performance gain, e.g. near 3dB PSNR gain, over the
current state-of-the-art method.

    

### [[2107.01333] A Uniformly Consistent Estimator of non-Gaussian Causal Effects Under the k-Triangle-Faithfulness Assumption](http://arxiv.org/abs/2107.01333)


  Kalisch and Bühlmann (2007) showed that for linear Gaussian models, under
the Causal Markov Assumption, the Strong Causal Faithfulness Assumption, and
the assumption of causal sufficiency, the PC algorithm is a uniformly
consistent estimator of the Markov Equivalence Class of the true causal DAG for
linear Gaussian models; it follows from this that for the identifiable causal
effects in the Markov Equivalence Class, there are uniformly consistent
estimators of causal effects as well. The $k$-Triangle-Faithfulness Assumption
is a strictly weaker assumption that avoids some implausible implications of
the Strong Causal Faithfulness Assumption and also allows for uniformly
consistent estimates of Markov Equivalence Classes (in a weakened sense), and
of identifiable causal effects. However, both of these assumptions are
restricted to linear Gaussian models. We propose the Generalized $k$-Triangle
Faithfulness, which can be applied to any smooth distribution. In addition,
under the Generalized $k$-Triangle Faithfulness Assumption, we describe the
Edge Estimation Algorithm that provides uniformly consistent estimates of
causal effects in some cases (and otherwise outputs "can't tell"), and the
\textit{Very Conservative }$SGS$ Algorithm that (in a slightly weaker sense) is
a uniformly consistent estimator of the Markov equivalence class of the true
DAG.

    

### [[2107.01335] Average-Case Communication Complexity of Statistical Problems](http://arxiv.org/abs/2107.01335)


  We study statistical problems, such as planted clique, its variants, and
sparse principal component analysis in the context of average-case
communication complexity. Our motivation is to understand the
statistical-computational trade-offs in streaming, sketching, and query-based
models. Communication complexity is the main tool for proving lower bounds in
these models, yet many prior results do not hold in an average-case setting. We
provide a general reduction method that preserves the input distribution for
problems involving a random graph or matrix with planted structure. Then, we
derive two-party and multi-party communication lower bounds for detecting or
finding planted cliques, bipartite cliques, and related problems. As a
consequence, we obtain new bounds on the query complexity in the edge-probe,
vector-matrix-vector, matrix-vector, linear sketching, and
$\mathbb{F}_2$-sketching models. Many of these results are nearly tight, and we
use our techniques to provide simple proofs of some known lower bounds for the
edge-probe model.

    

### [[2107.01337] CT Image Harmonization for Enhancing Radiomics Studies](http://arxiv.org/abs/2107.01337)


  While remarkable advances have been made in Computed Tomography (CT),
capturing CT images with non-standardized protocols causes low reproducibility
regarding radiomic features, forming a barrier on CT image analysis in a large
scale. RadiomicGAN is developed to effectively mitigate the discrepancy caused
by using non-standard reconstruction kernels. RadiomicGAN consists of hybrid
neural blocks including both pre-trained and trainable layers adopted to learn
radiomic feature distributions efficiently. A novel training approach, called
Dynamic Window-based Training, has been developed to smoothly transform the
pre-trained model to the medical imaging domain. Model performance evaluated
using 1401 radiomic features show that RadiomicGAN clearly outperforms the
state-of-art image standardization models.

    

### [[2107.01338] Sibling Regression for Generalized Linear Models](http://arxiv.org/abs/2107.01338)


  Field observations form the basis of many scientific studies, especially in
ecological and social sciences. Despite efforts to conduct such surveys in a
standardized way, observations can be prone to systematic measurement errors.
The removal of systematic variability introduced by the observation process, if
possible, can greatly increase the value of this data. Existing non-parametric
techniques for correcting such errors assume linear additive noise models. This
leads to biased estimates when applied to generalized linear models (GLM). We
present an approach based on residual functions to address this limitation. We
then demonstrate its effectiveness on synthetic data and show it reduces
systematic detection variability in moth surveys.

    

### [[2107.01343] Short-term probabilistic photovoltaic power forecast based on deep convolutional long short-term memory network and kernel density estimation](http://arxiv.org/abs/2107.01343)


  Solar energy is a clean and renewable energy. Photovoltaic (PV) power is an
important way to utilize solar energy. Accurate PV power forecast is crucial to
the large-scale application of PV power and the stability of electricity grid.
This paper proposes a novel method for short-term photovoltaic power forecast
using deep convolutional long short-term memory (ConvLSTM) network and kernel
density estimation (KDE). In the proposed method, ConvLSTM is used to forecast
the future photovoltaic power and KDE is used for estimating the joint
probabilistic density function and giving the probabilistic confidence
interval. Experiments in an actual photovoltaic power station verify the
effectiveness of the proposed method. Comparison experiments with convolutional
neural network (CNN) and long short-term memory network (LSTM)shows that
ConvLSTM can combine the advantages of both CNN and LSTM and significantly
outperform CNN and LSTM in terms of forecast accuracy. Through further
comparison with other five conventional methods including multilayer perceptron
(MLP), support vector regression (SVR), extreme learning machine (ELM),
classification and regression tree (CART) and gradient boosting decision tree
(GBDT), ConvLSTM can significantly improve the forecast accuracy by more than
20% for most of the five methods and the superiorities of ConvLSTM are further
verified.

    

### [[2107.01345] Cluster Representatives Selection in Non-Metric Spaces for Nearest Prototype Classification](http://arxiv.org/abs/2107.01345)


  The nearest prototype classification is a less computationally intensive
replacement for the $k$-NN method, especially when large datasets are
considered. In metric spaces, centroids are often used as prototypes to
represent whole clusters. The selection of cluster prototypes in non-metric
spaces is more challenging as the idea of computing centroids is not directly
applicable.
In this paper, we present CRS, a novel method for selecting a small yet
representative subset of objects as a cluster prototype. Memory and
computationally efficient selection of representatives is enabled by leveraging
the similarity graph representation of each cluster created by the NN-Descent
algorithm. CRS can be used in an arbitrary metric or non-metric space because
of the graph-based approach, which requires only a pairwise similarity measure.
As we demonstrate in the experimental evaluation, our method outperforms the
state of the art techniques on multiple datasets from different domains.

    

### [[2107.01347] Traffic Signal Control with Communicative Deep Reinforcement Learning Agents: a Case Study](http://arxiv.org/abs/2107.01347)


  In this work we theoretically and experimentally analyze Multi-Agent
Advantage Actor-Critic (MA2C) and Independent Advantage Actor-Critic (IA2C),
two recently proposed multi-agent reinforcement learning methods that can be
applied to control traffic signals in urban areas. The two methods differ in
their use of a reward calculated locally or globally and in the management of
agents' communication. We analyze the methods theoretically with the framework
provided by non-Markov decision processes, which provides useful insights in
the analysis of the algorithms. Moreover, we analyze the efficacy and the
robustness of the methods experimentally by testing them in two traffic areas
in the Bologna (Italy) area, simulated by SUMO, a software tool. The
experimental results indicate that MA2C achieves the best performance in the
majority of cases, outperforms the alternative method considered, and displays
sufficient stability during the learning process.

    

### [[2107.01348] Examining average and discounted reward optimality criteria in reinforcement learning](http://arxiv.org/abs/2107.01348)


  In reinforcement learning (RL), the goal is to obtain an optimal policy, for
which the optimality criterion is fundamentally important. Two major optimality
criteria are average and discounted rewards, where the later is typically
considered as an approximation to the former. While the discounted reward is
more popular, it is problematic to apply in environments that have no natural
notion of discounting. This motivates us to revisit a) the progression of
optimality criteria in dynamic programming, b) justification for and
complication of an artificial discount factor, and c) benefits of directly
maximizing the average reward. Our contributions include a thorough examination
of the relationship between average and discounted rewards, as well as a
discussion of their pros and cons in RL. We emphasize that average-reward RL
methods possess the ingredient and mechanism for developing the general
discounting-free optimality criterion (Veinott, 1969) in RL.

    

### [[2107.01349] Split-and-Bridge: Adaptable Class Incremental Learning within a Single Neural Network](http://arxiv.org/abs/2107.01349)


  Continual learning has been a major problem in the deep learning community,
where the main challenge is how to effectively learn a series of newly arriving
tasks without forgetting the knowledge of previous tasks. Initiated by Learning
without Forgetting (LwF), many of the existing works report that knowledge
distillation is effective to preserve the previous knowledge, and hence they
commonly use a soft label for the old task, namely a knowledge distillation
(KD) loss, together with a class label for the new task, namely a cross entropy
(CE) loss, to form a composite loss for a single neural network. However, this
approach suffers from learning the knowledge by a CE loss as a KD loss often
more strongly influences the objective function when they are in a competitive
situation within a single network. This could be a critical problem
particularly in a class incremental scenario, where the knowledge across tasks
as well as within the new task, both of which can only be acquired by a CE
loss, is essentially learned due to the existence of a unified classifier. In
this paper, we propose a novel continual learning method, called
Split-and-Bridge, which can successfully address the above problem by partially
splitting a neural network into two partitions for training the new task
separated from the old task and re-connecting them for learning the knowledge
across tasks. In our thorough experimental analysis, our Split-and-Bridge
method outperforms the state-of-the-art competitors in KD-based continual
learning.

    

### [[2107.01353] Spatiotemporal convolutional network for time-series prediction and causal inference](http://arxiv.org/abs/2107.01353)


  Making predictions in a robust way is not easy for nonlinear systems. In this
work, a neural network computing framework, i.e., a spatiotemporal
convolutional network (STCN), was developed to efficiently and accurately
render a multistep-ahead prediction of a time series by employing a
spatial-temporal information (STI) transformation. The STCN combines the
advantages of both the temporal convolutional network (TCN) and the STI
equation, which maps the high-dimensional/spatial data to the future temporal
values of a target variable, thus naturally providing the prediction of the
target variable. From the observed variables, the STCN also infers the causal
factors of the target variable in the sense of Granger causality, which are in
turn selected as effective spatial information to improve the prediction
robustness. The STCN was successfully applied to both benchmark systems and
real-world datasets, all of which show superior and robust performance in
multistep-ahead prediction, even when the data were perturbed by noise. From
both theoretical and computational viewpoints, the STCN has great potential in
practical applications in artificial intelligence (AI) or machine learning
fields as a model-free method based only on the observed data, and also opens a
new way to explore the observed high-dimensional data in a dynamical manner for
machine learning.

    

### [[2107.01354] Pool of Experts: Realtime Querying Specialized Knowledge in Massive Neural Networks](http://arxiv.org/abs/2107.01354)


  In spite of the great success of deep learning technologies, training and
delivery of a practically serviceable model is still a highly time-consuming
process. Furthermore, a resulting model is usually too generic and heavyweight,
and hence essentially goes through another expensive model compression phase to
fit in a resource-limited device like embedded systems. Inspired by the fact
that a machine learning task specifically requested by mobile users is often
much simpler than it is supported by a massive generic model, this paper
proposes a framework, called Pool of Experts (PoE), that instantly builds a
lightweight and task-specific model without any training process. For a
realtime model querying service, PoE first extracts a pool of primitive
components, called experts, from a well-trained and sufficiently generic
network by exploiting a novel conditional knowledge distillation method, and
then performs our train-free knowledge consolidation to quickly combine
necessary experts into a lightweight network for a target task. Thanks to this
train-free property, in our thorough empirical study, PoE can build a fairly
accurate yet compact model in a realtime manner, whereas it takes a few minutes
per query for the other training methods to achieve a similar level of the
accuracy.

    

### [[2107.01358] CInC Flow: Characterizable Invertible 3x3 Convolution](http://arxiv.org/abs/2107.01358)


  Normalizing flows are an essential alternative to GANs for generative
modelling, which can be optimized directly on the maximum likelihood of the
dataset. They also allow computation of the exact latent vector corresponding
to an image since they are composed of invertible transformations. However, the
requirement of invertibility of the transformation prevents standard and
expressive neural network models such as CNNs from being directly used.
Emergent convolutions were proposed to construct an invertible 3$\times$3 CNN
layer using a pair of masked CNN layers, making them inefficient. We study
conditions such that 3$\times$3 CNNs are invertible, allowing them to construct
expressive normalizing flows. We derive necessary and sufficient conditions on
a padded CNN for it to be invertible. Our conditions for invertibility are
simple, can easily be maintained during the training process. Since we require
only a single CNN layer for every effective invertible CNN layer, our approach
is more efficient than emerging convolutions. We also proposed a coupling
method, Quad-coupling. We benchmark our approach and show similar performance
results to emergent convolutions while improving the model's efficiency.

    

### [[2107.01360] Supervised Off-Policy Ranking](http://arxiv.org/abs/2107.01360)


  Off-policy evaluation (OPE) leverages data generated by other policies to
evaluate a target policy. Previous OPE methods mainly focus on precisely
estimating the true performance of a policy. We observe that in many
applications, (1) the end goal of OPE is to compare two or multiple candidate
policies and choose a good one, which is actually a much simpler task than
evaluating their true performance; and (2) there are usually multiple policies
that have been deployed in real-world systems and thus whose true performance
is known through serving real users. Inspired by the two observations, in this
work, we define a new problem, supervised off-policy ranking (SOPR), which aims
to rank a set of new/target policies based on supervised learning by leveraging
off-policy data and policies with known performance. We further propose a
method for supervised off-policy ranking that learns a policy scoring model by
correctly ranking training policies with known performance rather than
estimating their precise performance. Our method leverages logged states and
policies to learn a Transformer based model that maps offline interaction data
including logged states and the actions taken by a target policy on these
states to a score. Experiments on different games, datasets, training policy
sets, and test policy sets show that our method outperforms strong baseline OPE
methods in terms of both rank correlation and performance gap between the truly
best and the best of the ranked top three policies. Furthermore, our method is
more stable than baseline methods.

    

### [[2107.01366] Can Transformers Jump Around Right in Natural Language? Assessing Performance Transfer from SCAN](http://arxiv.org/abs/2107.01366)


  Despite their practical success, modern seq2seq architectures are unable to
generalize systematically on several SCAN tasks. Hence, it is not clear if
SCAN-style compositional generalization is useful in realistic NLP tasks. In
this work, we study the benefit that such compositionality brings about to
several machine translation tasks. We present several focused modifications of
Transformer that greatly improve generalization capabilities on SCAN and select
one that remains on par with a vanilla Transformer on a standard machine
translation (MT) task. Next, we study its performance in low-resource settings
and on a newly introduced distribution-shifted English-French translation task.
Overall, we find that improvements of a SCAN-capable model do not directly
transfer to the resource-rich MT setup. In contrast, in the low-resource setup,
general modifications lead to an improvement of up to 13.1% BLEU score w.r.t. a
vanilla Transformer. Similarly, an improvement of 14% in an accuracy-based
metric is achieved in the introduced compositional English-French translation
task. This provides experimental evidence that the compositional generalization
assessed in SCAN is particularly useful in resource-starved and domain-shifted
scenarios.

    

### [[2107.01372] Learning Debiased Representation via Disentangled Feature Augmentation](http://arxiv.org/abs/2107.01372)


  Image classification models tend to make decisions based on peripheral
attributes of data items that have strong correlation with a target variable
(i.e., dataset bias). These biased models suffer from the poor generalization
capability when evaluated on unbiased datasets. Existing approaches for
debiasing often identify and emphasize those samples with no such correlation
(i.e., bias-conflicting) without defining the bias type in advance. However,
such bias-conflicting samples are significantly scarce in biased datasets,
limiting the debiasing capability of these approaches. This paper first
presents an empirical analysis revealing that training with "diverse"
bias-conflicting samples beyond a given training set is crucial for debiasing
as well as the generalization capability. Based on this observation, we propose
a novel feature-level data augmentation technique in order to synthesize
diverse bias-conflicting samples. To this end, our method learns the
disentangled representation of (1) the intrinsic attributes (i.e., those
inherently defining a certain class) and (2) bias attributes (i.e., peripheral
attributes causing the bias), from a large number of bias-aligned samples, the
bias attributes of which have strong correlation with the target variable.
Using the disentangled representation, we synthesize bias-conflicting samples
that contain the diverse intrinsic attributes of bias-aligned samples by
swapping their latent features. By utilizing these diversified bias-conflicting
features during the training, our approach achieves superior classification
accuracy and debiasing results against the existing baselines on both synthetic
as well as real-world datasets.

    

### [[2107.01390] Memory and attention in deep learning](http://arxiv.org/abs/2107.01390)


  Intelligence necessitates memory. Without memory, humans fail to perform
various nontrivial tasks such as reading novels, playing games or solving
maths. As the ultimate goal of machine learning is to derive intelligent
systems that learn and act automatically just like human, memory construction
for machine is inevitable. Artificial neural networks model neurons and
synapses in the brain by interconnecting computational units via weights, which
is a typical class of machine learning algorithms that resembles memory
structure. Their descendants with more complicated modeling techniques (a.k.a
deep learning) have been successfully applied to many practical problems and
demonstrated the importance of memory in the learning process of machinery
systems. Recent progresses on modeling memory in deep learning have revolved
around external memory constructions, which are highly inspired by
computational Turing models and biological neuronal systems. Attention
mechanisms are derived to support acquisition and retention operations on the
external memory. Despite the lack of theoretical foundations, these approaches
have shown promises to help machinery systems reach a higher level of
intelligence. The aim of this thesis is to advance the understanding on memory
and attention in deep learning. Its contributions include: (i) presenting a
collection of taxonomies for memory, (ii) constructing new memory-augmented
neural networks (MANNs) that support multiple control and memory units, (iii)
introducing variability via memory in sequential generative models, (iv)
searching for optimal writing operations to maximise the memorisation capacity
in slot-based memory networks, and (v) simulating the Universal Turing Machine
via Neural Stored-program Memory-a new kind of external memory for neural
networks.

    

### [[2107.01392] WisdomNet: Prognosis of COVID-19 with Slender Prospect of False Negative Cases and Vaticinating the Probability of Maturation to ARDS using Posteroanterior Chest X-Rays](http://arxiv.org/abs/2107.01392)


  Coronavirus is a large virus family consisting of diverse viruses, some of
which disseminate among mammals and others cause sickness among humans.
COVID-19 is highly contagious and is rapidly spreading, rendering its early
diagnosis of preeminent status. Researchers, medical specialists and
organizations all over the globe have been working tirelessly to combat this
virus and help in its containment. In this paper, a novel neural network called
WisdomNet has been proposed, for the diagnosis of COVID-19 using chest X-rays.
The WisdomNet uses the concept of Wisdom of Crowds as its founding idea. It is
a two-layered convolutional Neural Network (CNN), which takes chest x-ray
images as input. Both layers of the proposed neural network consist of a number
of neural networks each. The dataset used for this study consists of chest
x-ray images of COVID-19 positive patients, compiled and shared by Dr. Cohen on
GitHub, and the chest x-ray images of healthy lungs and lungs affected by viral
and bacterial pneumonia were obtained from Kaggle. The network not only
pinpoints the presence of COVID-19, but also gives the probability of the
disease maturing into Acute Respiratory Distress Syndrome (ARDS). Thus,
predicting the progression of the disease in the COVID-19 positive patients.
The network also slender the occurrences of false negative cases by employing a
high threshold value, thus aids in curbing the spread of the disease and gives
an accuracy of 100% for successfully predicting COVID-19 among the chest x-rays
of patients affected with COVID-19, bacterial and viral pneumonia.

    

### [[2107.01400] Exact Backpropagation in Binary Weighted Networks with Group Weight Transformations](http://arxiv.org/abs/2107.01400)


  Quantization based model compression serves as high performing and fast
approach for inference that yields highly compressed models compared to their
full-precision floating point counterparts. The most extreme quantization is a
1-bit representation of parameters such that they have only two possible
values, typically -1(0) or +1. Models that constrain the weights to binary
values enable efficient implementation of the ubiquitous dot product by
additions only without requiring floating point multiplications which is
beneficial for resources constrained inference. The main contribution of this
work is the introduction of a method to smooth the combinatorial problem of
determining a binary vector of weights to minimize the expected loss for a
given objective by means of empirical risk minimization with backpropagation.
This is achieved by approximating a multivariate binary state over the weights
utilizing a deterministic and differentiable transformation of real-valued
continuous parameters. The proposed method adds little overhead in training,
can be readily applied without any substantial modifications to the original
architecture, does not introduce additional saturating non-linearities or
auxiliary losses, and does not prohibit applying other methods for binarizing
the activations. It is demonstrated that contrary to common assertions made in
the literature, binary weighted networks can train well with the same standard
optimization techniques and similar hyperparameters settings as their
full-precision counterparts, namely momentum SGD with large learning rates and
$L_2$ regularization. The source code is publicly available at
this https URL


### [[2107.01407] Where is the Grass Greener? Revisiting Generalized Policy Iteration for Offline Reinforcement Learning](http://arxiv.org/abs/2107.01407)


  The performance of state-of-the-art baselines in the offline RL regime varies
widely over the spectrum of dataset qualities, ranging from "far-from-optimal"
random data to "close-to-optimal" expert demonstrations. We re-implement these
under a fair, unified, and highly factorized framework, and show that when a
given baseline outperforms its competing counterparts on one end of the
spectrum, it never does on the other end. This consistent trend prevents us
from naming a victor that outperforms the rest across the board. We attribute
the asymmetry in performance between the two ends of the quality spectrum to
the amount of inductive bias injected into the agent to entice it to posit that
the behavior underlying the offline dataset is optimal for the task. The more
bias is injected, the higher the agent performs, provided the dataset is
close-to-optimal. Otherwise, its effect is brutally detrimental. Adopting an
advantage-weighted regression template as base, we conduct an investigation
which corroborates that injections of such optimality inductive bias, when not
done parsimoniously, makes the agent subpar in the datasets it was dominant as
soon as the offline policy is sub-optimal. In an effort to design methods that
perform well across the whole spectrum, we revisit the generalized policy
iteration scheme for the offline regime, and study the impact of nine distinct
newly-introduced proposal distributions over actions, involved in proposed
generalization of the policy evaluation and policy improvement update rules. We
show that certain orchestrations strike the right balance and can improve the
performance on one end of the spectrum without harming it on the other end.

    

### [[2107.01408] Scale Mixtures of Neural Network Gaussian Processes](http://arxiv.org/abs/2107.01408)


  Recent works have revealed that infinitely-wide feed-forward or recurrent
neural networks of any architecture correspond to Gaussian processes referred
to as $\mathrm{NNGP}$. While these works have extended the class of neural
networks converging to Gaussian processes significantly, however, there has
been little focus on broadening the class of stochastic processes that such
neural networks converge to. In this work, inspired by the scale mixture of
Gaussian random variables, we propose the scale mixture of $\mathrm{NNGP}$ for
which we introduce a prior distribution on the scale of the last-layer
parameters. We show that simply introducing a scale prior on the last-layer
parameters can turn infinitely-wide neural networks of any architecture into a
richer class of stochastic processes. Especially, with certain scale priors, we
obtain heavy-tailed stochastic processes, and we recover Student's $t$
processes in the case of inverse gamma priors. We further analyze the
distributions of the neural networks initialized with our prior setting and
trained with gradient descents and obtain similar results as for
$\mathrm{NNGP}$. We present a practical posterior-inference algorithm for the
scale mixture of $\mathrm{NNGP}$ and empirically demonstrate its usefulness on
regression and classification tasks.

    

### [[2107.01410] Maximum Entropy Weighted Independent Set Pooling for Graph Neural Networks](http://arxiv.org/abs/2107.01410)


  In this paper, we propose a novel pooling layer for graph neural networks
based on maximizing the mutual information between the pooled graph and the
input graph. Since the maximum mutual information is difficult to compute, we
employ the Shannon capacity of a graph as an inductive bias to our pooling
method. More precisely, we show that the input graph to the pooling layer can
be viewed as a representation of a noisy communication channel. For such a
channel, sending the symbols belonging to an independent set of the graph
yields a reliable and error-free transmission of information. We show that
reaching the maximum mutual information is equivalent to finding a maximum
weight independent set of the graph where the weights convey entropy contents.
Through this communication theoretic standpoint, we provide a distinct
perspective for posing the problem of graph pooling as maximizing the
information transmission rate across a noisy communication channel, implemented
by a graph neural network. We evaluate our method, referred to as Maximum
Entropy Weighted Independent Set Pooling (MEWISPool), on graph classification
tasks and the combinatorial optimization problem of the maximum independent
set. Empirical results demonstrate that our method achieves the
state-of-the-art and competitive results on graph classification tasks and the
maximum independent set problem in several benchmark datasets.

    

### [[2107.01412] Isotonic Data Augmentation for Knowledge Distillation](http://arxiv.org/abs/2107.01412)


  Knowledge distillation uses both real hard labels and soft labels predicted
by teacher models as supervision. Intuitively, we expect the soft labels and
hard labels to be concordant w.r.t. their orders of probabilities. However, we
found {\it critical order violations} between hard labels and soft labels in
augmented samples. For example, for an augmented sample $x=0.7*panda+0.3*cat$,
we expect the order of meaningful soft labels to be
$P_\text{soft}(panda|x)>P_\text{soft}(cat|x)>P_\text{soft}(other|x)$. But real
soft labels usually violate the order, e.g.
$P_\text{soft}(tiger|x)>P_\text{soft}(panda|x)>P_\text{soft}(cat|x)$. We
attribute this to the unsatisfactory generalization ability of the teacher,
which leads to the prediction error of augmented samples. Empirically, we found
the violations are common and injure the knowledge this http URL this paper, we
introduce order restrictions to data augmentation for knowledge distillation,
which is denoted as isotonic data augmentation (IDA). We use isotonic
regression (IR) -- a classic technique from statistics -- to eliminate the
order violations. We show that IDA can be modeled as a tree-structured IR
problem. We thereby adapt the classical IRT-BIN algorithm for optimal solutions
with $O(c \log c)$ time complexity, where $c$ is the number of labels. In order
to further reduce the time complexity, we also \cwy{propose} a GPU-friendly
approximation with linear time complexity. We have verified on variant datasets
and data augmentation techniques that our proposed IDA algorithms effectively
increases the accuracy of knowledge distillation by eliminating the rank
violations.

    

### [[2107.01460] Mava: a research framework for distributed multi-agent reinforcement learning](http://arxiv.org/abs/2107.01460)


  Breakthrough advances in reinforcement learning (RL) research have led to a
surge in the development and application of RL. To support the field and its
rapid growth, several frameworks have emerged that aim to help the community
more easily build effective and scalable agents. However, very few of these
frameworks exclusively support multi-agent RL (MARL), an increasingly active
field in itself, concerned with decentralised decision-making problems. In this
work, we attempt to fill this gap by presenting Mava: a research framework
specifically designed for building scalable MARL systems. Mava provides useful
components, abstractions, utilities and tools for MARL and allows for simple
scaling for multi-process system training and execution, while providing a high
level of flexibility and composability. Mava is built on top of DeepMind's Acme
\citep{hoffman2020acme}, and therefore integrates with, and greatly benefits
from, a wide range of already existing single-agent RL components made
available in Acme. Several MARL baseline systems have already been implemented
in Mava. These implementations serve as examples showcasing Mava's reusable
features, such as interchangeable system architectures, communication and
mixing modules. Furthermore, these implementations allow existing MARL
algorithms to be easily reproduced and extended. We provide experimental
results for these implementations on a wide range of multi-agent environments
and highlight the benefits of distributed system training.

    

### [[2107.01461] A Lottery Ticket Hypothesis Framework for Low-Complexity Device-Robust Neural Acoustic Scene Classification](http://arxiv.org/abs/2107.01461)


  We propose a novel neural model compression strategy combining data
augmentation, knowledge transfer, pruning, and quantization for device-robust
acoustic scene classification (ASC). Specifically, we tackle the ASC task in a
low-resource environment leveraging a recently proposed advanced neural network
pruning mechanism, namely Lottery Ticket Hypothesis (LTH), to find a
sub-network neural model associated with a small amount non-zero model
parameters. The effectiveness of LTH for low-complexity acoustic modeling is
assessed by investigating various data augmentation and compression schemes,
and we report an efficient joint framework for low-complexity multi-device ASC,
called Acoustic Lottery. Acoustic Lottery could compress an ASC model over
$1/10^{4}$ and attain a superior performance (validation accuracy of 74.01% and
Log loss of 0.76) compared to its not compressed seed model. All results
reported in this work are based on a joint effort of four groups, namely
GT-USTC-UKE-Tencent, aiming to address the "Low-Complexity Acoustic Scene
Classification (ASC) with Multiple Devices" in the DCASE 2021 Challenge Task
1a.

    

### [[2107.01466] A convolutional neural network for prestack fracture detection](http://arxiv.org/abs/2107.01466)


  Fractures are widely developed in hydrocarbon reservoirs and constitute the
accumulation spaces and transport channels of oil and gas. Fracture detection
is a fundamental task for reservoir characterization. From prestack seismic
gathers, anisotropic analysis and inversion were commonly applied to
characterize the dominant orientations and relative intensities of fractures.
However, the existing methods were mostly based on the vertical aligned facture
hypothesis, it is impossible for them to recognize fracture dip. Furthermore,
it is difficult or impractical for existing methods to attain the real fracture
densities. Based on data-driven deep learning, this paper designed a
convolutional neural network to perform prestack fracture detection.
Capitalizing on the connections between seismic responses and fracture
parameters, a suitable azimuth dataset was firstly generated through fracture
effective medium modeling and anisotropic plane wave analyzing. Then a
multi-input and multi-output convolutional neural network was constructed to
simultaneously detect fracture density, dip and strike azimuth. The application
on a practical survey validated the effectiveness of the proposed CNN model.

    

### [[2107.01473] Slope and generalization properties of neural networks](http://arxiv.org/abs/2107.01473)


  Neural networks are very successful tools in for example advanced
classification. From a statistical point of view, fitting a neural network may
be seen as a kind of regression, where we seek a function from the input space
to a space of classification probabilities that follows the "general" shape of
the data, but avoids overfitting by avoiding memorization of individual data
points. In statistics, this can be done by controlling the geometric complexity
of the regression function. We propose to do something similar when fitting
neural networks by controlling the slope of the network.
After defining the slope and discussing some of its theoretical properties,
we go on to show empirically in examples, using ReLU networks, that the
distribution of the slope of a well-trained neural network classifier is
generally independent of the width of the layers in a fully connected network,
and that the mean of the distribution only has a weak dependence on the model
architecture in general. The slope is of similar size throughout the relevant
volume, and varies smoothly. It also behaves as predicted in rescaling
examples. We discuss possible applications of the slope concept, such as using
it as a part of the loss function or stopping criterion during network
training, or ranking data sets in terms of their complexity.

    

### [[2107.01475] Privacy-Preserving Representation Learning on Graphs: A Mutual Information Perspective](http://arxiv.org/abs/2107.01475)


  Learning with graphs has attracted significant attention recently. Existing
representation learning methods on graphs have achieved state-of-the-art
performance on various graph-related tasks such as node classification, link
prediction, etc. However, we observe that these methods could leak serious
private information. For instance, one can accurately infer the links (or node
identity) in a graph from a node classifier (or link predictor) trained on the
learnt node representations by existing methods. To address the issue, we
propose a privacy-preserving representation learning framework on graphs from
the \emph{mutual information} perspective. Specifically, our framework includes
a primary learning task and a privacy protection task, and we consider node
classification and link prediction as the two tasks of interest. Our goal is to
learn node representations such that they can be used to achieve high
performance for the primary learning task, while obtaining performance for the
privacy protection task close to random guessing. We formally formulate our
goal via mutual information objectives. However, it is intractable to compute
mutual information in practice. Then, we derive tractable variational bounds
for the mutual information terms, where each bound can be parameterized via a
neural network. Next, we train these parameterized neural networks to
approximate the true mutual information and learn privacy-preserving node
representations. We finally evaluate our framework on various graph datasets.

    

### [[2107.01477] Byzantine-robust Federated Learning through Spatial-temporal Analysis of Local Model Updates](http://arxiv.org/abs/2107.01477)


  Federated Learning (FL) enables multiple distributed clients (e.g., mobile
devices) to collaboratively train a centralized model while keeping the
training data locally on the client. Compared to traditional centralized
machine learning, FL offers many favorable features such as offloading
operations which would usually be performed by a central server and reducing
risks of serious privacy leakage. However, Byzantine clients that send
incorrect or disruptive updates due to system failures or adversarial attacks
may disturb the joint learning process, consequently degrading the performance
of the resulting model. In this paper, we propose to mitigate these failures
and attacks from a spatial-temporal perspective. Specifically, we use a
clustering-based method to detect and exclude incorrect updates by leveraging
their geometric properties in the parameter space. Moreover, to further handle
malicious clients with time-varying behaviors, we propose to adaptively adjust
the learning rate according to momentum-based update speculation. Extensive
experiments on 4 public datasets demonstrate that our algorithm achieves
enhanced robustness comparing to existing methods under both cross-silo and
cross-device FL settings with faulty/malicious clients.

    

### [[2107.01495] On Positional and Structural Node Features for Graph Neural Networks on Non-attributed Graphs](http://arxiv.org/abs/2107.01495)


  Graph neural networks (GNNs) have been widely used in various graph-related
problems such as node classification and graph classification, where the
superior performance is mainly established when natural node features are
available. However, it is not well understood how GNNs work without natural
node features, especially regarding the various ways to construct artificial
ones. In this paper, we point out the two types of artificial node
features,i.e., positional and structural node features, and provide insights on
why each of them is more appropriate for certain tasks,i.e., positional node
classification, structural node classification, and graph classification.
Extensive experimental results on 10 benchmark datasets validate our insights,
thus leading to a practical guideline on the choices between different
artificial node features for GNNs on non-attributed graphs. The code is
available at this https URL.

    

### [[2107.01499] BAGUA: Scaling up Distributed Learning with System Relaxations](http://arxiv.org/abs/2107.01499)


  Recently years have witnessed a growing list of systems for distributed
data-parallel training. Existing systems largely fit into two paradigms, i.e.,
parameter server and MPI-style collective operations. On the algorithmic side,
researchers have proposed a wide range of techniques to lower the communication
via system relaxations: quantization, decentralization, and communication
delay. However, most, if not all, existing systems only rely on standard
synchronous and asynchronous stochastic gradient (SG) based optimization,
therefore, cannot take advantage of all possible optimizations that the machine
learning community has been developing recently. Given this emerging gap
between the current landscapes of systems and theory, we build BAGUA, a
communication framework whose design goal is to provide a system abstraction
that is both flexible and modular to support state-of-the-art system relaxation
techniques of distributed training. Powered by the new system design, BAGUA has
a great ability to implement and extend various state-of-the-art distributed
learning algorithms. In a production cluster with up to 16 machines (128 GPUs),
BAGUA can outperform PyTorch-DDP, Horovod and BytePS in the end-to-end training
time by a significant margin (up to 1.95 times) across a diverse range of
tasks. Moreover, we conduct a rigorous tradeoff exploration showing that
different algorithms and system relaxations achieve the best performance over
different network conditions.

    

### [[2107.01502] Pulmonary Vessel Segmentation based on Orthogonal Fused U-Net++ of Chest CT Images](http://arxiv.org/abs/2107.01502)


  Pulmonary vessel segmentation is important for clinical diagnosis of
pulmonary diseases, while is also challenging due to the complicated structure.
In this work, we present an effective framework and refinement process of
pulmonary vessel segmentation from chest computed tomographic (CT) images. The
key to our approach is a 2.5D segmentation network applied from three
orthogonal axes, which presents a robust and fully automated pulmonary vessel
segmentation result with lower network complexity and memory usage compared to
3D networks. The slice radius is introduced to convolve the adjacent
information of the center slice and the multi-planar fusion optimizes the
presentation of intra- and inter- slice features. Besides, the tree-like
structure of the pulmonary vessel is extracted in the post-processing process,
which is used for segmentation refining and pruning. In the evaluation
experiments, three fusion methods are tested and the most promising one is
compared with the state-of-the-art 2D and 3D structures on 300 cases of lung
images randomly selected from LIDC dataset. Our method outperforms other
network structures by a large margin and achieves by far the highest average
DICE score of 0.9272 and precision of 0.9310, as per our knowledge from the
pulmonary vessel segmentation models available in the literature.

    

### [[2107.01509] Bayesian decision-making under misspecified priors with applications to meta-learning](http://arxiv.org/abs/2107.01509)


  Thompson sampling and other Bayesian sequential decision-making algorithms
are among the most popular approaches to tackle explore/exploit trade-offs in
(contextual) bandits. The choice of prior in these algorithms offers
flexibility to encode domain knowledge but can also lead to poor performance
when misspecified. In this paper, we demonstrate that performance degrades
gracefully with misspecification. We prove that the expected reward accrued by
Thompson sampling (TS) with a misspecified prior differs by at most
$\tilde{\mathcal{O}}(H^2 \epsilon)$ from TS with a well specified prior, where
$\epsilon$ is the total-variation distance between priors and $H$ is the
learning horizon. Our bound does not require the prior to have any parametric
form. For priors with bounded support, our bound is independent of the
cardinality or structure of the action space, and we show that it is tight up
to universal constants in the worst case.
Building on our sensitivity analysis, we establish generic PAC guarantees for
algorithms in the recently studied Bayesian meta-learning setting and derive
corollaries for various families of priors. Our results generalize along two
axes: (1) they apply to a broader family of Bayesian decision-making
algorithms, including a Monte-Carlo implementation of the knowledge gradient
algorithm (KG), and (2) they apply to Bayesian POMDPs, the most general
Bayesian decision-making setting, encompassing contextual bandits as a special
case. Through numerical simulations, we illustrate how prior misspecification
and the deployment of one-step look-ahead (as in KG) can impact the convergence
of meta-learning in multi-armed and contextual bandits with structured and
correlated priors.

    

### [[2107.01516] Improved Representation Learning for Session-based Recommendation](http://arxiv.org/abs/2107.01516)


  Session-based recommendation systems suggest relevant items to users by
modeling user behavior and preferences using short-term anonymous sessions.
Existing methods leverage Graph Neural Networks (GNNs) that propagate and
aggregate information from neighboring nodes i.e., local message passing. Such
graph-based architectures have representational limits, as a single sub-graph
is susceptible to overfit the sequential dependencies instead of accounting for
complex transitions between items in different sessions. We propose using a
Transformer in combination with a target attentive GNN, which allows richer
Representation Learning. Our experimental results and ablation show that our
proposed method outperforms the existing methods on real-world benchmark
datasets.

    

### [[2107.01525] AdaL: Adaptive Gradient Transformation Contributes to Convergences and Generalizations](http://arxiv.org/abs/2107.01525)


  Adaptive optimization methods have been widely used in deep learning. They
scale the learning rates adaptively according to the past gradient, which has
been shown to be effective to accelerate the convergence. However, they suffer
from poor generalization performance compared with SGD. Recent studies point
that smoothing exponential gradient noise leads to generalization degeneration
phenomenon. Inspired by this, we propose AdaL, with a transformation on the
original gradient. AdaL accelerates the convergence by amplifying the gradient
in the early stage, as well as dampens the oscillation and stabilizes the
optimization by shrinking the gradient later. Such modification alleviates the
smoothness of gradient noise, which produces better generalization performance.
We have theoretically proved the convergence of AdaL and demonstrated its
effectiveness on several benchmarks.

    

### [[2107.01528] Incorporating Reachability Knowledge into a Multi-Spatial Graph Convolution Based Seq2Seq Model for Traffic Forecasting](http://arxiv.org/abs/2107.01528)


  Accurate traffic state prediction is the foundation of transportation control
and guidance. It is very challenging due to the complex spatiotemporal
dependencies in traffic data. Existing works cannot perform well for multi-step
traffic prediction that involves long future time period. The spatiotemporal
information dilution becomes serve when the time gap between input step and
predicted step is large, especially when traffic data is not sufficient or
noisy. To address this issue, we propose a multi-spatial graph convolution
based Seq2Seq model. Our main novelties are three aspects: (1) We enrich the
spatiotemporal information of model inputs by fusing multi-view features (time,
location and traffic states) (2) We build multiple kinds of spatial
correlations based on both prior knowledge and data-driven knowledge to improve
model performance especially in insufficient or noisy data cases. (3) A
spatiotemporal attention mechanism based on reachability knowledge is novelly
designed to produce high-level features fed into decoder of Seq2Seq directly to
ease information dilution. Our model is evaluated on two real world traffic
datasets and achieves better performance than other competitors.

    

### [[2107.01529] Learning Complex Users' Preferences for Recommender Systems](http://arxiv.org/abs/2107.01529)


  Recommender systems (RSs) have emerged as very useful tools to help customers
with their decision-making process, find items of their interest, and alleviate
the information overload problem. There are two different lines of approaches
in RSs: (1) general recommenders with the main goal of discovering long-term
users' preferences, and (2) sequential recommenders with the main focus of
capturing short-term users' preferences in a session of user-item interaction
(here, a session refers to a record of purchasing multiple items in one
shopping event). While considering short-term users' preferences may satisfy
their current needs and interests, long-term users' preferences provide users
with the items that they may interact with, eventually. In this thesis, we
first focus on improving the performance of general RSs. Most of the existing
general RSs tend to exploit the users' rating patterns on common items to
detect similar users. The data sparsity problem (i.e. the lack of available
information) is one of the major challenges for the current general RSs, and
they may fail to have any recommendations when there are no common items of
interest among users. We call this problem data sparsity with no feedback on
common items (DSW-n-FCI). To overcome this problem, we propose a
personality-based RS in which similar users are identified based on the
similarity of their personality traits.

    

### [[2107.01557] Leveraging Evidential Deep Learning Uncertainties with Graph-based Clustering to Detect Anomalies](http://arxiv.org/abs/2107.01557)


  Understanding and representing traffic patterns are key to detecting
anomalies in the maritime domain. To this end, we propose a novel graph-based
traffic representation and association scheme to cluster trajectories of
vessels using automatic identification system (AIS) data. We utilize the
(un)clustered data to train a recurrent neural network (RNN)-based evidential
regression model, which can predict a vessel's trajectory at future timesteps
with its corresponding prediction uncertainty. This paper proposes the usage of
a deep learning (DL)-based uncertainty estimation in detecting maritime
anomalies, such as unusual vessel maneuvering. Furthermore, we utilize the
evidential deep learning classifiers to detect unusual turns of vessels and the
loss of AIS signal using predicted class probabilities with associated
uncertainties. Our experimental results suggest that using graph-based
clustered data improves the ability of the DL models to learn the
temporal-spatial correlation of data and associated uncertainties. Using
different AIS datasets and experiments, we demonstrate that the estimated
prediction uncertainty yields fundamental information for the detection of
traffic anomalies in the maritime and, possibly in other domains.

    

### [[2107.01559] Smoothed Differential Privacy](http://arxiv.org/abs/2107.01559)


  Differential privacy (DP) is a widely-accepted and widely-applied notion of
privacy based on worst-case analysis. Often, DP classifies most mechanisms
without external noise as non-private [Dwork et al., 2014], and external
noises, such as Gaussian noise or Laplacian noise [Dwork et al., 2006], are
introduced to improve privacy. In many real-world applications, however, adding
external noise is undesirable and sometimes prohibited. For example,
presidential elections often require a deterministic rule to be used [Liu et
al., 2020], and small noises can lead to dramatic decreases in the prediction
accuracy of deep neural networks, especially the underrepresented classes
[Bagdasaryan et al., 2019].
In this paper, we propose a natural extension and relaxation of DP following
the worst average-case idea behind the celebrated smoothed analysis [Spielman
and Teng, 2004]. Our notion, the smoothed DP, can effectively measure the
privacy leakage of mechanisms without external noises under realistic settings.
We prove several strong properties of the smoothed DP, including
composability, robustness to post-processing and etc. We proved that any
discrete mechanism with sampling procedures is more private than what DP
predicts. In comparison, many continuous mechanisms with sampling procedures
are still non-private under smoothed DP. Experimentally, we first verified that
the discrete sampling mechanisms are private in real-world elections. Then, we
apply the smoothed DP notion on quantized gradient descent, which indicates
some neural networks can be private without adding any extra noises. We believe
that these results contribute to the theoretical foundation of realistic
privacy measures beyond worst-case analysis.

    

### [[2107.01561] Certifiably Robust Interpretation via Renyi Differential Privacy](http://arxiv.org/abs/2107.01561)


  Motivated by the recent discovery that the interpretation maps of CNNs could
easily be manipulated by adversarial attacks against network interpretability,
we study the problem of interpretation robustness from a new perspective of
\Renyi differential privacy (RDP). The advantages of our Renyi-Robust-Smooth
(RDP-based interpretation method) are three-folds. First, it can offer provable
and certifiable top-$k$ robustness. That is, the top-$k$ important attributions
of the interpretation map are provably robust under any input perturbation with
bounded $\ell_d$-norm (for any $d\geq 1$, including $d = \infty$). Second, our
proposed method offers $\sim10\%$ better experimental robustness than existing
approaches in terms of the top-$k$ attributions. Remarkably, the accuracy of
Renyi-Robust-Smooth also outperforms existing approaches. Third, our method can
provide a smooth tradeoff between robustness and computational efficiency.
Experimentally, its top-$k$ attributions are {\em twice} more robust than
existing approaches when the computational resources are highly constrained.

    

### [[2107.01562] Random Neural Networks in the Infinite Width Limit as Gaussian Processes](http://arxiv.org/abs/2107.01562)


  This article gives a new proof that fully connected neural networks with
random weights and biases converge to Gaussian processes in the regime where
the input dimension, output dimension, and depth are kept fixed, while the
hidden layer widths tend to infinity. Unlike prior work, convergence is shown
assuming only moment conditions for the distribution of weights and for quite
general non-linearities.

    

### [[2107.01569] Cross-Modal Transformer-Based Neural Correction Models for Automatic Speech Recognition](http://arxiv.org/abs/2107.01569)


  We propose a cross-modal transformer-based neural correction models that
refines the output of an automatic speech recognition (ASR) system so as to
exclude ASR errors. Generally, neural correction models are composed of
encoder-decoder networks, which can directly model sequence-to-sequence mapping
problems. The most successful method is to use both input speech and its ASR
output text as the input contexts for the encoder-decoder networks. However,
the conventional method cannot take into account the relationships between
these two different modal inputs because the input contexts are separately
encoded for each modal. To effectively leverage the correlated information
between the two different modal inputs, our proposed models encode two
different contexts jointly on the basis of cross-modal self-attention using a
transformer. We expect that cross-modal self-attention can effectively capture
the relationships between two different modals for refining ASR hypotheses. We
also introduce a shallow fusion technique to efficiently integrate the
first-pass ASR model and our proposed neural correction model. Experiments on
Japanese natural language ASR tasks demonstrated that our proposed models
achieve better ASR performance than conventional neural correction models.

    

### [[2107.01590] Deep Gaussian Process Emulation using Stochastic Imputation](http://arxiv.org/abs/2107.01590)


  We propose a novel deep Gaussian process (DGP) inference method for computer
model emulation using stochastic imputation. By stochastically imputing the
latent layers, the approach transforms the DGP into the linked GP, a
state-of-the-art surrogate model formed by linking a system of feed-forward
coupled GPs. This transformation renders a simple while efficient DGP training
procedure that only involves optimizations of conventional stationary GPs. In
addition, the analytically tractable mean and variance of the linked GP allows
one to implement predictions from DGP emulators in a fast and accurate manner.
We demonstrate the method in a series of synthetic examples and real-world
applications, and show that it is a competitive candidate for efficient DGP
surrogate modeling in comparison to the variational inference and the
fully-Bayesian approach. A $\texttt{Python}$ package $\texttt{dgpsi}$
implementing the method is also produced and available at
this https URL.

    

### [[2107.01595] Learning in nonatomic games, Part I: Finite action spaces and population games](http://arxiv.org/abs/2107.01595)


  We examine the long-run behavior of a wide range of dynamics for learning in
nonatomic games, in both discrete and continuous time. The class of dynamics
under consideration includes fictitious play and its regularized variants, the
best-reply dynamics (again, possibly regularized), as well as the dynamics of
dual averaging / "follow the regularized leader" (which themselves include as
special cases the replicator dynamics and Friedman's projection dynamics). Our
analysis concerns both the actual trajectory of play and its time-average, and
we cover potential and monotone games, as well as games with an evolutionarily
stable state (global or otherwise). We focus exclusively on games with finite
action spaces; nonatomic games with continuous action spaces are treated in
detail in Part II of this paper.

    

### [[2107.01598] Domain Adaptation for Sentiment Analysis Using Increased Intraclass Separation](http://arxiv.org/abs/2107.01598)


  Sentiment analysis is a costly yet necessary task for enterprises to study
the opinions of their customers to improve their products and to determine
optimal marketing strategies. Due to the existence of a wide range of domains
across different products and services, cross-domain sentiment analysis methods
have received significant attention. These methods mitigate the domain gap
between different applications by training cross-domain generalizable
classifiers which help to relax the need for data annotation for each domain.
Most existing methods focus on learning domain-agnostic representations that
are invariant with respect to both the source and the target domains. As a
result, a classifier that is trained using the source domain annotated data
would generalize well in a related target domain. We introduce a new domain
adaptation method which induces large margins between different classes in an
embedding space. This embedding space is trained to be domain-agnostic by
matching the data distributions across the domains. Large intraclass margins in
the source domain help to reduce the effect of "domain shift" on the classifier
performance in the target domain. Theoretical and empirical analysis are
provided to demonstrate that the proposed method is effective.

    

### [[2107.01606] A Comparison of the Delta Method and the Bootstrap in Deep Learning Classification](http://arxiv.org/abs/2107.01606)


  We validate the recently introduced deep learning classification adapted
Delta method by a comparison with the classical Bootstrap. We show that there
is a strong linear relationship between the quantified predictive epistemic
uncertainty levels obtained from the two methods when applied on two
LeNet-based neural network classifiers using the MNIST and CIFAR-10 datasets.
Furthermore, we demonstrate that the Delta method offers a five times
computation time reduction compared to the Bootstrap.

    

### [[2107.01614] Survey: Leakage and Privacy at Inference Time](http://arxiv.org/abs/2107.01614)


  Leakage of data from publicly available Machine Learning (ML) models is an
area of growing significance as commercial and government applications of ML
can draw on multiple sources of data, potentially including users' and clients'
sensitive data. We provide a comprehensive survey of contemporary advances on
several fronts, covering involuntary data leakage which is natural to ML
models, potential malevolent leakage which is caused by privacy attacks, and
currently available defence mechanisms. We focus on inference-time leakage, as
the most likely scenario for publicly available models. We first discuss what
leakage is in the context of different data, tasks, and model architectures. We
then propose a taxonomy across involuntary and malevolent leakage, available
defences, followed by the currently available assessment metrics and
applications. We conclude with outstanding challenges and open questions,
outlining some promising directions for future research.

    

### [[2107.01615] A Typology of Data Anomalies](http://arxiv.org/abs/2107.01615)


  Anomalies are cases that are in some way unusual and do not appear to fit the
general patterns present in the dataset. Several conceptualizations exist to
distinguish between different types of anomalies. However, these are either too
specific to be generally applicable or so abstract that they neither provide
concrete insight into the nature of anomaly types nor facilitate the functional
evaluation of anomaly detection algorithms. With the recent criticism on 'black
box' algorithms and analytics it has become clear that this is an undesirable
situation. This paper therefore introduces a general typology of anomalies that
offers a clear and tangible definition of the different types of anomalies in
datasets. The typology also facilitates the evaluation of the functional
capabilities of anomaly detection algorithms and as a framework assists in
analyzing the conceptual levels of data, patterns and anomalies. Finally, it
serves as an analytical tool for studying anomaly types from other typologies.

    

### [[2107.01620] Auxiliary-Classifier GAN for Malware Analysis](http://arxiv.org/abs/2107.01620)


  Generative adversarial networks (GAN) are a class of powerful machine
learning techniques, where both a generative and discriminative model are
trained simultaneously. GANs have been used, for example, to successfully
generate "deep fake" images. A recent trend in malware research consists of
treating executables as images and employing image-based analysis techniques.
In this research, we generate fake malware images using auxiliary classifier
GANs (AC-GAN), and we consider the effectiveness of various techniques for
classifying the resulting images. Our results indicate that the resulting
multiclass classification problem is challenging, yet we can obtain strong
results when restricting the problem to distinguishing between real and fake
samples. While the AC-GAN generated images often appear to be very similar to
real malware images, we conclude that from a deep learning perspective, the
AC-GAN generated samples do not rise to the level of deep fake malware images.

    

### [[2107.01622] Multiple-criteria Based Active Learning with Fixed-size Determinantal Point Processes](http://arxiv.org/abs/2107.01622)


  Active learning aims to achieve greater accuracy with less training data by
selecting the most useful data samples from which it learns. Single-criterion
based methods (i.e., informativeness and representativeness based methods) are
simple and efficient; however, they lack adaptability to different real-world
scenarios. In this paper, we introduce a multiple-criteria based active
learning algorithm, which incorporates three complementary criteria, i.e.,
informativeness, representativeness and diversity, to make appropriate
selections in the active learning rounds under different data types. We
consider the selection process as a Determinantal Point Process, which good
balance among these criteria. We refine the query selection strategy by both
selecting the hardest unlabeled data sample and biasing towards the classifiers
that are more suitable for the current data distribution. In addition, we also
consider the dependencies and relationships between these data points in data
selection by means of centroidbased clustering approaches. Through evaluations
on synthetic and real-world datasets, we show that our method performs
significantly better and is more stable than other multiple-criteria based AL
algorithms.

    

### [[2107.01627] Machine Learning for Malware Evolution Detection](http://arxiv.org/abs/2107.01627)


  Malware evolves over time and antivirus must adapt to such evolution. Hence,
it is critical to detect those points in time where malware has evolved so that
appropriate countermeasures can be undertaken. In this research, we perform a
variety of experiments on a significant number of malware families to determine
when malware evolution is likely to have occurred. All of the evolution
detection techniques that we consider are based on machine learning and can be
fully automated -- in particular, no reverse engineering or other
labor-intensive manual analysis is required. Specifically, we consider analysis
based on hidden Markov models (HMM) and the word embedding techniques HMM2Vec
and Word2Vec.

    

### [[2107.01629] The Role of "Live" in Livestreaming Markets: Evidence Using Orthogonal Random Forest](http://arxiv.org/abs/2107.01629)


  The common belief about the growing medium of livestreaming is that its value
lies in its "live" component. In this paper, we leverage data from a large
livestreaming platform to examine this belief. We are able to do this as this
platform also allows viewers to purchase the recorded version of the
livestream. We summarize the value of livestreaming content by estimating how
demand responds to price before, on the day of, and after the livestream. We do
this by proposing a generalized Orthogonal Random Forest framework. This
framework allows us to estimate heterogeneous treatment effects in the presence
of high-dimensional confounders whose relationships with the treatment policy
(i.e., price) are complex but partially known. We find significant dynamics in
the price elasticity of demand over the temporal distance to the scheduled
livestreaming day and after. Specifically, demand gradually becomes less price
sensitive over time to the livestreaming day and is inelastic on the
livestreaming day. Over the post-livestream period, demand is still sensitive
to price, but much less than the pre-livestream period. This indicates that the
vlaue of livestreaming persists beyond the live component. Finally, we provide
suggestive evidence for the likely mechanisms driving our results. These are
quality uncertainty reduction for the patterns pre- and post-livestream and the
potential of real-time interaction with the creator on the day of the
livestream.

    

### [[2107.01641] A Theoretical Analysis of Fine-tuning with Linear Teachers](http://arxiv.org/abs/2107.01641)


  Fine-tuning is a common practice in deep learning, achieving excellent
generalization results on downstream tasks using relatively little training
data. Although widely used in practice, it is lacking strong theoretical
understanding. We analyze the sample complexity of this scheme for regression
with linear teachers in several architectures. Intuitively, the success of
fine-tuning depends on the similarity between the source tasks and the target
task, however measuring it is non trivial. We show that a relevant measure
considers the relation between the source task, the target task and the
covariance structure of the target data. In the setting of linear regression,
we show that under realistic settings a substantial sample complexity reduction
is plausible when the above measure is low. For deep linear regression, we
present a novel result regarding the inductive bias of gradient-based training
when the network is initialized with pretrained weights. Using this result we
show that the similarity measure for this setting is also affected by the depth
of the network. We further present results on shallow ReLU models, and analyze
the dependence of sample complexity there on source and target tasks. We
empirically demonstrate our results for both synthetic and realistic data.

    

### [[2107.01650] Learning ODEs via Diffeomorphisms for Fast and Robust Integration](http://arxiv.org/abs/2107.01650)


  Advances in differentiable numerical integrators have enabled the use of
gradient descent techniques to learn ordinary differential equations (ODEs). In
the context of machine learning, differentiable solvers are central for Neural
ODEs (NODEs), a class of deep learning models with continuous depth, rather
than discrete layers. However, these integrators can be unsatisfactorily slow
and inaccurate when learning systems of ODEs from long sequences, or when
solutions of the system vary at widely different timescales in each dimension.
In this paper we propose an alternative approach to learning ODEs from data: we
represent the underlying ODE as a vector field that is related to another base
vector field by a differentiable bijection, modelled by an invertible neural
network. By restricting the base ODE to be amenable to integration, we can
drastically speed up and improve the robustness of integration. We demonstrate
the efficacy of our method in training and evaluating continuous neural
networks models, as well as in learning benchmark ODE systems. We observe
improvements of up to two orders of magnitude when integrating learned ODEs
with GPUs computation.

    

### [[2107.01655] Attribute-aware Explainable Complementary Clothing Recommendation](http://arxiv.org/abs/2107.01655)


  Modelling mix-and-match relationships among fashion items has become
increasingly demanding yet challenging for modern E-commerce recommender
systems. When performing clothes matching, most existing approaches leverage
the latent visual features extracted from fashion item images for compatibility
modelling, which lacks explainability of generated matching results and can
hardly convince users of the recommendations. Though recent methods start to
incorporate pre-defined attribute information (e.g., colour, style, length,
etc.) for learning item representations and improving the model
interpretability, their utilisation of attribute information is still mainly
reserved for enhancing the learned item representations and generating
explanations via post-processing. As a result, this creates a severe bottleneck
when we are trying to advance the recommendation accuracy and generating
fine-grained explanations since the explicit attributes have only loose
connections to the actual recommendation process. This work aims to tackle the
explainability challenge in fashion recommendation tasks by proposing a novel
Attribute-aware Fashion Recommender (AFRec). Specifically, AFRec recommender
assesses the outfit compatibility by explicitly leveraging the extracted
attribute-level representations from each item's visual feature. The attributes
serve as the bridge between two fashion items, where we quantify the affinity
of a pair of items through the learned compatibility between their attributes.
Extensive experiments have demonstrated that, by making full use of the
explicit attributes in the recommendation process, AFRec is able to achieve
state-of-the-art recommendation accuracy and generate intuitive explanations at
the same time.

    

### [[2107.01657] Class Introspection: A Novel Technique for Detecting Unlabeled Subclasses by Leveraging Classifier Explainability Methods](http://arxiv.org/abs/2107.01657)


  Detecting latent structure within a dataset is a crucial step in performing
analysis of a dataset. However, existing state-of-the-art techniques for
subclass discovery are limited: either they are limited to detecting very small
numbers of outliers or they lack the statistical power to deal with complex
data such as image or audio. This paper proposes a solution to this subclass
discovery problem: by leveraging instance explanation methods, an existing
classifier can be extended to detect latent classes via differences in the
classifier's internal decisions about each instance. This works not only with
simple classification techniques but also with deep neural networks, allowing
for a powerful and flexible approach to detecting latent structure within
datasets. Effectively, this represents a projection of the dataset into the
classifier's "explanation space," and preliminary results show that this
technique outperforms the baseline for the detection of latent classes even
with limited processing. This paper also contains a pipeline for analyzing
classifiers automatically, and a web application for interactively exploring
the results from this technique.

    

### [[2107.01658] Learning Bayesian Networks through Birkhoff Polytope: A Relaxation Method](http://arxiv.org/abs/2107.01658)


  We establish a novel framework for learning a directed acyclic graph (DAG)
when data are generated from a Gaussian, linear structural equation model. It
consists of two parts: (1) introduce a permutation matrix as a new parameter
within a regularized Gaussian log-likelihood to represent variable ordering;
and (2) given the ordering, estimate the DAG structure through sparse Cholesky
factor of the inverse covariance matrix. For permutation matrix estimation, we
propose a relaxation technique that avoids the NP-hard combinatorial problem of
order estimation. Given an ordering, a sparse Cholesky factor is estimated
using a cyclic coordinatewise descent algorithm which decouples row-wise. Our
framework recovers DAGs without the need for an expensive verification of the
acyclicity constraint or enumeration of possible parent sets. We establish
numerical convergence of the algorithm, and consistency of the Cholesky factor
estimator when the order of variables is known. Through several simulated and
macro-economic datasets, we study the scope and performance of the proposed
methodology.

    

### [[2107.01677] Low-Dimensional State and Action Representation Learning with MDP Homomorphism Metrics](http://arxiv.org/abs/2107.01677)


  Deep Reinforcement Learning has shown its ability in solving complicated
problems directly from high-dimensional observations. However, in end-to-end
settings, Reinforcement Learning algorithms are not sample-efficient and
requires long training times and quantities of data. In this work, we proposed
a framework for sample-efficient Reinforcement Learning that take advantage of
state and action representations to transform a high-dimensional problem into a
low-dimensional one. Moreover, we seek to find the optimal policy mapping
latent states to latent actions. Because now the policy is learned on abstract
representations, we enforce, using auxiliary loss functions, the lifting of
such policy to the original problem domain. Results show that the novel
framework can efficiently learn low-dimensional and interpretable state and
action representations and the optimal latent policy.

    

### [[2107.01689] Robust Restless Bandits: Tackling Interval Uncertainty with Deep Reinforcement Learning](http://arxiv.org/abs/2107.01689)


  We introduce Robust Restless Bandits, a challenging generalization of
restless multi-arm bandits (RMAB). RMABs have been widely studied for
intervention planning with limited resources. However, most works make the
unrealistic assumption that the transition dynamics are known perfectly,
restricting the applicability of existing methods to real-world scenarios. To
make RMABs more useful in settings with uncertain dynamics: (i) We introduce
the Robust RMAB problem and develop solutions for a minimax regret objective
when transitions are given by interval uncertainties; (ii) We develop a double
oracle algorithm for solving Robust RMABs and demonstrate its effectiveness on
three experimental domains; (iii) To enable our double oracle approach, we
introduce RMABPPO, a novel deep reinforcement learning algorithm for solving
RMABs. RMABPPO hinges on learning an auxiliary "$\lambda$-network" that allows
each arm's learning to decouple, greatly reducing sample complexity required
for training; (iv) Under minimax regret, the adversary in the double oracle
approach is notoriously difficult to implement due to non-stationarity. To
address this, we formulate the adversary oracle as a multi-agent reinforcement
learning problem and solve it with a multi-agent extension of RMABPPO, which
may be of independent interest as the first known algorithm for this setting.
Code is available at this https URL.

    

### [[2107.01702] Data-Driven Learning of Feedforward Neural Networks with Different Activation Functions](http://arxiv.org/abs/2107.01702)


  This work contributes to the development of a new data-driven method (D-DM)
of feedforward neural networks (FNNs) learning. This method was proposed
recently as a way of improving randomized learning of FNNs by adjusting the
network parameters to the target function fluctuations. The method employs
logistic sigmoid activation functions for hidden nodes. In this study, we
introduce other activation functions, such as bipolar sigmoid, sine function,
saturating linear functions, reLU, and softplus. We derive formulas for their
parameters, i.e. weights and biases. In the simulation study, we evaluate the
performance of FNN data-driven learning with different activation functions.
The results indicate that the sigmoid activation functions perform much better
than others in the approximation of complex, fluctuated target functions.

    

### [[2107.01705] Randomized Neural Networks for Forecasting Time Series with Multiple Seasonality](http://arxiv.org/abs/2107.01705)


  This work contributes to the development of neural forecasting models with
novel randomization-based learning methods. These methods improve the fitting
abilities of the neural model, in comparison to the standard method, by
generating network parameters in accordance with the data and target function
features. A pattern-based representation of time series makes the proposed
approach useful for forecasting time series with multiple seasonality. In the
simulation study, we evaluate the performance of the proposed models and find
that they can compete in terms of forecasting accuracy with fully-trained
networks. Extremely fast and easy training, simple architecture, ease of
implementation, high accuracy as well as dealing with nonstationarity and
multiple seasonality in time series make the proposed model very attractive for
a wide range of complex time series forecasting problems.

    

### [[2107.01707] Towards Scheduling Federated Deep Learning using Meta-Gradients for Inter-Hospital Learning](http://arxiv.org/abs/2107.01707)


  Given the abundance and ease of access of personal data today, individual
privacy has become of paramount importance, particularly in the healthcare
domain. In this work, we aim to utilise patient data extracted from multiple
hospital data centres to train a machine learning model without sacrificing
patient privacy. We develop a scheduling algorithm in conjunction with a
student-teacher algorithm that is deployed in a federated manner. This allows a
central model to learn from batches of data at each federal node. The teacher
acts between data centres to update the main task (student) algorithm using the
data that is stored in the various data centres. We show that the scheduler,
trained using meta-gradients, can effectively organise training and as a result
train a machine learning model on a diverse dataset without needing explicit
access to the patient data. We achieve state-of-the-art performance and show
how our method overcomes some of the problems faced in the federated learning
such as node poisoning. We further show how the scheduler can be used as a
mechanism for transfer learning, allowing different teachers to work together
in training a student for state-of-the-art performance.

    

### [[2107.01711] Autoencoder based Randomized Learning of Feedforward Neural Networks for Regression](http://arxiv.org/abs/2107.01711)


  Feedforward neural networks are widely used as universal predictive models to
fit data distribution. Common gradient-based learning, however, suffers from
many drawbacks making the training process ineffective and time-consuming.
Alternative randomized learning does not use gradients but selects hidden node
parameters randomly. This makes the training process extremely fast. However,
the problem in randomized learning is how to determine the random parameters. A
recently proposed method uses autoencoders for unsupervised parameter learning.
This method showed superior performance on classification tasks. In this work,
we apply this method to regression problems, and, finding that it has some
drawbacks, we show how to improve it. We propose a learning method of
autoencoders that controls the produced random weights. We also propose how to
determine the biases of hidden nodes. We empirically compare autoencoder based
learning with other randomized learning methods proposed recently for
regression and find that despite the proposed improvement of the autoencoder
based learning, it does not outperform its competitors in fitting accuracy.
Moreover, the method is much more complex than its competitors.

    

### [[2107.01726] Adaptive calibration for binary classification](http://arxiv.org/abs/2107.01726)


  This note proposes a way of making probability forecasting rules less
sensitive to changes in data distribution, concentrating on the simple case of
binary classification. This is important in applications of machine learning,
where the quality of a trained predictor may drop significantly in the process
of its exploitation. Our techniques are based on recent work on conformal test
martingales and older work on prediction with expert advice, namely tracking
the best expert.

    

### [[2107.01734] Latent structure blockmodels for Bayesian spectral graph clustering](http://arxiv.org/abs/2107.01734)


  Spectral embedding of network adjacency matrices often produces node
representations living approximately around low-dimensional submanifold
structures. In particular, hidden substructure is expected to arise when the
graph is generated from a latent position model. Furthermore, the presence of
communities within the network might generate community-specific submanifold
structures in the embedding, but this is not explicitly accounted for in most
statistical models for networks. In this article, a class of models called
latent structure block models (LSBM) is proposed to address such scenarios,
allowing for graph clustering when community-specific one dimensional manifold
structure is present. LSBMs focus on a specific class of latent space model,
the random dot product graph (RDPG), and assign a latent submanifold to the
latent positions of each community. A Bayesian model for the embeddings arising
from LSBMs is discussed, and shown to have a good performance on simulated and
real world network data. The model is able to correctly recover the underlying
communities living in a one-dimensional manifold, even when the parametric form
of the underlying curves is unknown, achieving remarkable results on a variety
of real data.

    

### [[2107.01739] KAISA: An Adaptive Second-order Optimizer Framework for Deep Neural Networks](http://arxiv.org/abs/2107.01739)


  Kronecker-factored Approximate Curvature (K-FAC) has recently been shown to
converge faster in deep neural network (DNN) training than stochastic gradient
descent (SGD); however, K-FAC's larger memory footprint hinders its
applicability to large models. We present KAISA, a K-FAC-enabled, Adaptable,
Improved, and ScAlable second-order optimizer framework that adapts the memory
footprint, communication, and computation given specific models and hardware to
achieve maximized performance and enhanced scalability. We quantify the
tradeoffs between memory and communication cost and evaluate KAISA on large
models, including ResNet-50, Mask R-CNN, U-Net, and BERT, on up to 128 NVIDIA
A100 GPUs. Compared to the original optimizers, KAISA converges 18.1-36.3%
faster across applications with the same global batch size. Under a fixed
memory budget, KAISA converges 32.5% and 41.6% faster in ResNet-50 and
BERT-Large, respectively. KAISA can balance memory and communication to achieve
scaling efficiency equal to or better than the baseline optimizers.

    

### [[2107.01752] Polymorphic dynamic programming by algebraic shortcut fusion](http://arxiv.org/abs/2107.01752)


  Dynamic programming (DP) is a broadly applicable algorithmic design paradigm
for the efficient, exact solution of otherwise intractable, combinatorial
problems. However, the design of such algorithms is often presented informally
in an ad-hoc manner, and as a result is often difficult to apply correctly. In
this paper, we present a rigorous algebraic formalism for systematically
deriving novel DP algorithms, either from existing DP algorithms or from simple
functional recurrences. These derivations lead to algorithms which are provably
correct and polymorphic over any semiring, which means that they can be applied
to the full scope of combinatorial problems expressible in terms of semirings.
This includes, for example: optimization, optimal probability and Viterbi
decoding, probabilistic marginalization, logical inference, fuzzy sets,
differentiable softmax, and relational and provenance queries. The approach,
building on many ideas from the existing literature on constructive
algorithmics, exploits generic properties of (semiring) polymorphic functions,
tupling and formal sums (lifting), and algebraic simplifications arising from
constraint algebras. We demonstrate the effectiveness of this formalism for
some example applications arising in signal processing, bioinformatics and
reliability engineering.

    

### [[2107.01757] The Least Restriction for Offline Reinforcement Learning](http://arxiv.org/abs/2107.01757)


  Many practical applications of reinforcement learning (RL) constrain the
agent to learn from a fixed offline dataset of logged interactions, which has
already been gathered, without offering further possibility for data
collection. However, commonly used off-policy RL algorithms, such as the Deep Q
Network and the Deep Deterministic Policy Gradient, are incapable of learning
without data correlated to the distribution under the current policy, making
them ineffective for this offline setting. As the first step towards useful
offline RL algorithms, we analysis the reason of instability in standard
off-policy RL algorithms. It is due to the bootstrapping error. The key to
avoiding this error, is ensuring that the agent's action space does not go out
of the fixed offline dataset. Based on our consideration, a creative offline RL
framework, the Least Restriction (LR), is proposed in this paper. The LR
regards selecting an action as taking a sample from the probability
distribution. It merely set a little limit for action selection, which not only
avoid the action being out of the offline dataset but also remove all the
unreasonable restrictions in earlier approaches (e.g. Batch-Constrained Deep
Q-Learning). In the further, we will demonstrate that the LR, is able to learn
robustly from different offline datasets, including random and suboptimal
demonstrations, on a range of practical control tasks.

    

### [[2107.01760] Single Model for Influenza Forecasting of Multiple Countries by Multi-task Learning](http://arxiv.org/abs/2107.01760)


  The accurate forecasting of infectious epidemic diseases such as influenza is
a crucial task undertaken by medical institutions. Although numerous flu
forecasting methods and models based mainly on historical flu activity data and
online user-generated contents have been proposed in previous studies, no flu
forecasting model targeting multiple countries using two types of data exists
at present. Our paper leverages multi-task learning to tackle the challenge of
building one flu forecasting model targeting multiple countries; each country
as each task. Also, to develop the flu prediction model with higher
performance, we solved two issues; finding suitable search queries, which are
part of the user-generated contents, and how to leverage search queries
efficiently in the model creation. For the first issue, we propose the transfer
approaches from English to other languages. For the second issue, we propose a
novel flu forecasting model that takes advantage of search queries using an
attention mechanism and extend the model to a multi-task model for multiple
countries' flu forecasts. Experiments on forecasting flu epidemics in five
countries demonstrate that our model significantly improved the performance by
leveraging the search queries and multi-task learning compared to the
baselines.

    

### [[2107.01777] Statistical Theory for Imbalanced Binary Classification](http://arxiv.org/abs/2107.01777)


  Within the vast body of statistical theory developed for binary
classification, few meaningful results exist for imbalanced classification, in
which data are dominated by samples from one of the two classes. Existing
theory faces at least two main challenges. First, meaningful results must
consider more complex performance measures than classification accuracy. To
address this, we characterize a novel generalization of the Bayes-optimal
classifier to any performance metric computed from the confusion matrix, and we
use this to show how relative performance guarantees can be obtained in terms
of the error of estimating the class probability function under uniform
($\mathcal{L}_\infty$) loss. Second, as we show, optimal classification
performance depends on certain properties of class imbalance that have not
previously been formalized. Specifically, we propose a novel sub-type of class
imbalance, which we call Uniform Class Imbalance. We analyze how Uniform Class
Imbalance influences optimal classifier performance and show that it
necessitates different classifier behavior than other types of class imbalance.
We further illustrate these two contributions in the case of $k$-nearest
neighbor classification, for which we develop novel guarantees. Together, these
results provide some of the first meaningful finite-sample statistical theory
for imbalanced binary classification.

    

### [[2107.01782] A contextual analysis of multi-layer perceptron models in classifying hand-written digits and letters: limited resources](http://arxiv.org/abs/2107.01782)


  Classifying hand-written digits and letters has taken a big leap with the
introduction of ConvNets. However, on very constrained hardware the time
necessary to train such models would be high. Our main contribution is twofold.
First, we extensively test an end-to-end vanilla neural network (MLP) approach
in pure numpy without any pre-processing or feature extraction done beforehand.
Second, we show that basic data mining operations can significantly improve the
performance of the models in terms of computational time, without sacrificing
much accuracy. We illustrate our claims on a simpler variant of the Extended
MNIST dataset, called Balanced EMNIST dataset. Our experiments show that,
without any data mining, we get increased generalization performance when using
more hidden layers and regularization techniques, the best model achieving
84.83% accuracy on a test dataset. Using dimensionality reduction done by PCA
we were able to increase that figure to 85.08% with only 10% of the original
feature space, reducing the memory size needed by 64%. Finally, adding methods
to remove possibly harmful training samples like deviation from the mean helped
us to still achieve over 84% test accuracy but with only 32.8% of the original
memory size for the training set. This compares favorably to the majority of
literature results obtained through similar architectures. Although this
approach gets outshined by state-of-the-art models, it does scale to some
(AlexNet, VGGNet) trained on 50% of the same dataset.

    

### [[2107.01784] Learning a Model for Inferring a Spatial Road Lane Network Graph using Self-Supervision](http://arxiv.org/abs/2107.01784)


  Interconnected road lanes are a central concept for navigating urban roads.
Currently, most autonomous vehicles rely on preconstructed lane maps as
designing an algorithmic model is difficult. However, the generation and
maintenance of such maps is costly and hinders large-scale adoption of
autonomous vehicle technology. This paper presents the first self-supervised
learning method to train a model to infer a spatially grounded lane-level road
network graph based on a dense segmented representation of the road scene
generated from onboard sensors. A formal road lane network model is presented
and proves that any structured road scene can be represented by a directed
acyclic graph of at most depth three while retaining the notion of intersection
regions, and that this is the most compressed representation. The formal model
is implemented by a hybrid neural and search-based model, utilizing a novel
barrier function loss formulation for robust learning from partial labels.
Experiments are conducted for all common road intersection layouts. Results
show that the model can generalize to new road layouts, unlike previous
approaches, demonstrating its potential for real-world application as a
practical learning-based lane-level map generator.

    

### [[2107.01799] An Information-Theoretic Approach for Automatically Determining the Number of States when Aggregating Markov Chains](http://arxiv.org/abs/2107.01799)


  A fundamental problem when aggregating Markov chains is the specification of
the number of state groups. Too few state groups may fail to sufficiently
capture the pertinent dynamics of the original, high-order Markov chain. Too
many state groups may lead to a non-parsimonious, reduced-order Markov chain
whose complexity rivals that of the original. In this paper, we show that an
augmented value-of-information-based approach to aggregating Markov chains
facilitates the determination of the number of state groups. The optimal
state-group count coincides with the case where the complexity of the
reduced-order chain is balanced against the mutual dependence between the
original- and reduced-order chain dynamics.

    

### [[2107.01804] Randomized Dimensionality Reduction for Facility Location and Single-Linkage Clustering](http://arxiv.org/abs/2107.01804)


  Random dimensionality reduction is a versatile tool for speeding up
algorithms for high-dimensional problems. We study its application to two
clustering problems: the facility location problem, and the single-linkage
hierarchical clustering problem, which is equivalent to computing the minimum
spanning tree. We show that if we project the input pointset $X$ onto a random
$d = O(d_X)$-dimensional subspace (where $d_X$ is the doubling dimension of
$X$), then the optimum facility location cost in the projected space
approximates the original cost up to a constant factor. We show an analogous
statement for minimum spanning tree, but with the dimension $d$ having an extra
$\log \log n$ term and the approximation factor being arbitrarily close to $1$.
Furthermore, we extend these results to approximating solutions instead of just
their costs. Lastly, we provide experimental results to validate the quality of
solutions and the speedup due to the dimensionality reduction. Unlike several
previous papers studying this approach in the context of $k$-means and
$k$-medians, our dimension bound does not depend on the number of clusters but
only on the intrinsic dimensionality of $X$.

    

### [[2107.01806] A Framework for Evaluating the Cybersecurity Risk of Real World, Machine Learning Production Systems](http://arxiv.org/abs/2107.01806)


  Although cyberattacks on machine learning (ML) production systems can be
destructive, many industry practitioners are ill equipped, lacking tactical and
strategic tools that would allow them to analyze, detect, protect against, and
respond to cyberattacks targeting their ML-based systems. In this paper, we
take a significant step toward securing ML production systems by integrating
these systems and their vulnerabilities into cybersecurity risk assessment
frameworks. Specifically, we performed a comprehensive threat analysis of ML
production systems and developed an extension to the MulVAL attack graph
generation and analysis framework to incorporate cyberattacks on ML production
systems. Using the proposed extension, security practitioners can apply attack
graph analysis methods in environments that include ML components, thus
providing security experts with a practical tool for evaluating the impact and
quantifying the risk of a cyberattack targeting an ML production system.

    

### [[2107.01807] Q-SpiNN: A Framework for Quantizing Spiking Neural Networks](http://arxiv.org/abs/2107.01807)


  A prominent technique for reducing the memory footprint of Spiking Neural
Networks (SNNs) without decreasing the accuracy significantly is quantization.
However, the state-of-the-art only focus on employing the weight quantization
directly from a specific quantization scheme, i.e., either the post-training
quantization (PTQ) or the in-training quantization (ITQ), and do not consider
(1) quantizing other SNN parameters (e.g., neuron membrane potential), (2)
exploring different combinations of quantization approaches (i.e., quantization
schemes, precision levels, and rounding schemes), and (3) selecting the SNN
model with a good memory-accuracy trade-off at the end. Therefore, the memory
saving offered by these state-of-the-art to meet the targeted accuracy is
limited, thereby hindering processing SNNs on the resource-constrained systems
(e.g., the IoT-Edge devices). Towards this, we propose Q-SpiNN, a novel
quantization framework for memory-efficient SNNs. The key mechanisms of the
Q-SpiNN are: (1) employing quantization for different SNN parameters based on
their significance to the accuracy, (2) exploring different combinations of
quantization schemes, precision levels, and rounding schemes to find efficient
SNN model candidates, and (3) developing an algorithm that quantifies the
benefit of the memory-accuracy trade-off obtained by the candidates, and
selects the Pareto-optimal one. The experimental results show that, for the
unsupervised network, the Q-SpiNN reduces the memory footprint by ca. 4x, while
maintaining the accuracy within 1% from the baseline on the MNIST dataset. For
the supervised network, the Q-SpiNN reduces the memory by ca. 2x, while keeping
the accuracy within 2% from the baseline on the DVS-Gesture dataset.

    

### [[2107.01808] Why is Pruning at Initialization Immune to Reinitializing and Shuffling?](http://arxiv.org/abs/2107.01808)


  Recent studies assessing the efficacy of pruning neural networks methods
uncovered a surprising finding: when conducting ablation studies on existing
pruning-at-initialization methods, namely SNIP, GraSP, SynFlow, and magnitude
pruning, performances of these methods remain unchanged and sometimes even
improve when randomly shuffling the mask positions within each layer (Layerwise
Shuffling) or sampling new initial weight values (Reinit), while keeping
pruning masks the same. We attempt to understand the reason behind such network
immunity towards weight/mask modifications, by studying layer-wise statistics
before and after randomization operations. We found that under each of the
pruning-at-initialization methods, the distribution of unpruned weights changed
minimally with randomization operations.

    

### [[2107.01809] Boosting Transferability of Targeted Adversarial Examples via Hierarchical Generative Networks](http://arxiv.org/abs/2107.01809)


  Transfer-based adversarial attacks can effectively evaluate model robustness
in the black-box setting. Though several methods have demonstrated impressive
transferability of untargeted adversarial examples, targeted adversarial
transferability is still challenging. The existing methods either have low
targeted transferability or sacrifice computational efficiency. In this paper,
we develop a simple yet practical framework to efficiently craft targeted
transfer-based adversarial examples. Specifically, we propose a conditional
generative attacking model, which can generate the adversarial examples
targeted at different classes by simply altering the class embedding and share
a single backbone. Extensive experiments demonstrate that our method improves
the success rates of targeted black-box attacks by a significant margin over
the existing methods -- it reaches an average success rate of 29.6\% against
six diverse models based only on one substitute white-box model in the standard
testing of NeurIPS 2017 competition, which outperforms the state-of-the-art
gradient-based attack methods (with an average success rate of $<$2\%) by a
large margin. Moreover, the proposed method is also more efficient beyond an
order of magnitude than gradient-based methods.

    

### [[2107.01820] An Explainable AI System for the Diagnosis of High Dimensional Biomedical Data](http://arxiv.org/abs/2107.01820)


  Typical state of the art flow cytometry data samples consists of measures of
more than 100.000 cells in 10 or more features. AI systems are able to diagnose
such data with almost the same accuracy as human experts. However, there is one
central challenge in such systems: their decisions have far-reaching
consequences for the health and life of people, and therefore, the decisions of
AI systems need to be understandable and justifiable by humans. In this work,
we present a novel explainable AI method, called ALPODS, which is able to
classify (diagnose) cases based on clusters, i.e., subpopulations, in the
high-dimensional data. ALPODS is able to explain its decisions in a form that
is understandable for human experts. For the identified subpopulations, fuzzy
reasoning rules expressed in the typical language of domain experts are
generated. A visualization method based on these rules allows human experts to
understand the reasoning used by the AI system. A comparison to a selection of
state of the art explainable AI systems shows that ALPODS operates efficiently
on known benchmark data and also on everyday routine case data.

    

### [[2107.01825] Sample Efficient Reinforcement Learning via Model-Ensemble Exploration and Exploitation](http://arxiv.org/abs/2107.01825)


  Model-based deep reinforcement learning has achieved success in various
domains that require high sample efficiencies, such as Go and robotics.
However, there are some remaining issues, such as planning efficient
explorations to learn more accurate dynamic models, evaluating the uncertainty
of the learned models, and more rational utilization of models. To mitigate
these issues, we present MEEE, a model-ensemble method that consists of
optimistic exploration and weighted exploitation. During exploration, unlike
prior methods directly selecting the optimal action that maximizes the expected
accumulative return, our agent first generates a set of action candidates and
then seeks out the optimal action that takes both expected return and future
observation novelty into account. During exploitation, different discounted
weights are assigned to imagined transition tuples according to their model
uncertainty respectively, which will prevent model predictive error propagation
in agent training. Experiments on several challenging continuous control
benchmark tasks demonstrated that our approach outperforms other model-free and
model-based state-of-the-art methods, especially in sample complexity.

    

### [[2107.01830] ARM-Net: Adaptive Relation Modeling Network for Structured Data](http://arxiv.org/abs/2107.01830)


  Relational databases are the de facto standard for storing and querying
structured data, and extracting insights from structured data requires advanced
analytics. Deep neural networks (DNNs) have achieved super-human prediction
performance in particular data types, e.g., images. However, existing DNNs may
not produce meaningful results when applied to structured data. The reason is
that there are correlations and dependencies across combinations of attribute
values in a table, and these do not follow simple additive patterns that can be
easily mimicked by a DNN. The number of possible such cross features is
combinatorial, making them computationally prohibitive to model. Furthermore,
the deployment of learning models in real-world applications has also
highlighted the need for interpretability, especially for high-stakes
applications, which remains another issue of concern to DNNs.
In this paper, we present ARM-Net, an adaptive relation modeling network
tailored for structured data, and a lightweight framework ARMOR based on
ARM-Net for relational data analytics. The key idea is to model feature
interactions with cross features selectively and dynamically, by first
transforming the input features into exponential space, and then determining
the interaction order and interaction weights adaptively for each cross
feature. We propose a novel sparse attention mechanism to dynamically generate
the interaction weights given the input tuple, so that we can explicitly model
cross features of arbitrary orders with noisy features filtered selectively.
Then during model inference, ARM-Net can specify the cross features being used
for each prediction for higher accuracy and better interpretability. Our
extensive experiments on real-world datasets demonstrate that ARM-Net
consistently outperforms existing models and provides more interpretable
predictions for data-driven decision making.

    

### [[2107.01832] Provable Convergence of Nesterov Accelerated Method for Over-Parameterized Neural Networks](http://arxiv.org/abs/2107.01832)


  Despite the empirical success of deep learning, it still lacks theoretical
understandings to explain why randomly initialized neural network trained by
first-order optimization methods is able to achieve zero training loss, even
though its landscape is non-convex and non-smooth. Recently, there are some
works to demystifies this phenomenon under over-parameterized regime. In this
work, we make further progress on this area by considering a commonly used
momentum optimization algorithm: Nesterov accelerated method (NAG). We analyze
the convergence of NAG for two-layer fully connected neural network with ReLU
activation. Specifically, we prove that the error of NAG converges to zero at a
linear convergence rate $1-\Theta(1/\sqrt{\kappa})$, where $\kappa > 1$ is
determined by the initialization and the architecture of neural network.
Comparing to the rate $1-\Theta(1/\kappa)$ of gradient descent, NAG achieves an
acceleration. Besides, it also validates NAG and Heavy-ball method can achieve
a similar convergence rate.

    

### [[2107.01835] Fast Rate Learning in Stochastic First Price Bidding](http://arxiv.org/abs/2107.01835)


  First-price auctions have largely replaced traditional bidding approaches
based on Vickrey auctions in programmatic advertising. As far as learning is
concerned, first-price auctions are more challenging because the optimal
bidding strategy does not only depend on the value of the item but also
requires some knowledge of the other bids. They have already given rise to
several works in sequential learning, many of which consider models for which
the value of the buyer or the opponents' maximal bid is chosen in an
adversarial manner. Even in the simplest settings, this gives rise to
algorithms whose regret grows as $\sqrt{T}$ with respect to the time horizon
$T$. Focusing on the case where the buyer plays against a stationary stochastic
environment, we show how to achieve significantly lower regret: when the
opponents' maximal bid distribution is known we provide an algorithm whose
regret can be as low as $\log^2(T)$; in the case where the distribution must be
learnt sequentially, a generalization of this algorithm can achieve $T^{1/3+
\epsilon}$ regret, for any $\epsilon>0$. To obtain these results, we introduce
two novel ideas that can be of interest in their own right. First, by
transposing results obtained in the posted price setting, we provide conditions
under which the first-price biding utility is locally quadratic around its
optimum. Second, we leverage the observation that, on small sub-intervals, the
concentration of the variations of the empirical distribution function may be
controlled more accurately than by using the classical
Dvoretzky-Kiefer-Wolfowitz inequality. Numerical simulations confirm that our
algorithms converge much faster than alternatives proposed in the literature
for various bid distributions, including for bids collected on an actual
programmatic advertising platform.

    

### [[2107.01848] Differentially Private Sliced Wasserstein Distance](http://arxiv.org/abs/2107.01848)


  Developing machine learning methods that are privacy preserving is today a
central topic of research, with huge practical impacts. Among the numerous ways
to address privacy-preserving learning, we here take the perspective of
computing the divergences between distributions under the Differential Privacy
(DP) framework -- being able to compute divergences between distributions is
pivotal for many machine learning problems, such as learning generative models
or domain adaptation problems. Instead of resorting to the popular
gradient-based sanitization method for DP, we tackle the problem at its roots
by focusing on the Sliced Wasserstein Distance and seamlessly making it
differentially private. Our main contribution is as follows: we analyze the
property of adding a Gaussian perturbation to the intrinsic randomized
mechanism of the Sliced Wasserstein Distance, and we establish the
sensitivityof the resulting differentially private mechanism. One of our
important findings is that this DP mechanism transforms the Sliced Wasserstein
distance into another distance, that we call the Smoothed Sliced Wasserstein
Distance. This new differentially private distribution distance can be plugged
into generative models and domain adaptation algorithms in a transparent way,
and we empirically show that it yields highly competitive performance compared
with gradient-based DP approaches from the literature, with almost no loss in
accuracy for the domain adaptation problems that we consider.

    

### [[2107.01850] Matching a Desired Causal State via Shift Interventions](http://arxiv.org/abs/2107.01850)


  Transforming a causal system from a given initial state to a desired target
state is an important task permeating multiple fields including control theory,
biology, and materials science. In causal models, such transformations can be
achieved by performing a set of interventions. In this paper, we consider the
problem of identifying a shift intervention that matches the desired mean of a
system through active learning. We define the Markov equivalence class that is
identifiable from shift interventions and propose two active learning
strategies that are guaranteed to exactly match a desired mean. We then derive
a worst-case lower bound for the number of interventions required and show that
these strategies are optimal for certain classes of graphs. In particular, we
show that our strategies may require exponentially fewer interventions than the
previously considered approaches, which optimize for structure learning in the
underlying causal graph. In line with our theoretical results, we also
demonstrate experimentally that our proposed active learning strategies require
fewer interventions compared to several baselines.

    

### [[2107.01854] Poisoning Attack against Estimating from Pairwise Comparisons](http://arxiv.org/abs/2107.01854)


  As pairwise ranking becomes broadly employed for elections, sports
competitions, recommendations, and so on, attackers have strong motivation and
incentives to manipulate the ranking list. They could inject malicious
comparisons into the training data to fool the victim. Such a technique is
called poisoning attack in regression and classification tasks. In this paper,
to the best of our knowledge, we initiate the first systematic investigation of
data poisoning attacks on pairwise ranking algorithms, which can be formalized
as the dynamic and static games between the ranker and the attacker and can be
modeled as certain kinds of integer programming problems. To break the
computational hurdle of the underlying integer programming problems, we
reformulate them into the distributionally robust optimization (DRO) problems,
which are computationally tractable. Based on such DRO formulations, we propose
two efficient poisoning attack algorithms and establish the associated
theoretical guarantees. The effectiveness of the suggested poisoning attack
strategies is demonstrated by a series of toy simulations and several real data
experiments. These experimental results show that the proposed methods can
significantly reduce the performance of the ranker in the sense that the
correlation between the true ranking list and the aggregated results can be
decreased dramatically.

    

### [[2107.01856] Winning at Any Cost -- Infringing the Cartel Prohibition With Reinforcement Learning](http://arxiv.org/abs/2107.01856)


  Pricing decisions are increasingly made by AI. Thanks to their ability to
train with live market data while making decisions on the fly, deep
reinforcement learning algorithms are especially effective in taking such
pricing decisions. In e-commerce scenarios, multiple reinforcement learning
agents can set prices based on their competitor's prices. Therefore, research
states that agents might end up in a state of collusion in the long run. To
further analyze this issue, we build a scenario that is based on a modified
version of a prisoner's dilemma where three agents play the game of rock paper
scissors. Our results indicate that the action selection can be dissected into
specific stages, establishing the possibility to develop collusion prevention
systems that are able to recognize situations which might lead to a collusion
between competitors. We furthermore provide evidence for a situation where
agents are capable of performing a tacit cooperation strategy without being
explicitly trained to do so.

    

### [[2107.01858] Automating Generative Deep Learning for Artistic Purposes: Challenges and Opportunities](http://arxiv.org/abs/2107.01858)


  We present a framework for automating generative deep learning with a
specific focus on artistic applications. The framework provides opportunities
to hand over creative responsibilities to a generative system as targets for
automation. For the definition of targets, we adopt core concepts from
automated machine learning and an analysis of generative deep learning
pipelines, both in standard and artistic settings. To motivate the framework,
we argue that automation aligns well with the goal of increasing the creative
responsibility of a generative system, a central theme in computational
creativity research. We understand automation as the challenge of granting a
generative system more creative autonomy, by framing the interaction between
the user and the system as a co-creative process. The development of the
framework is informed by our analysis of the relationship between automation
and creative autonomy. An illustrative example shows how the framework can give
inspiration and guidance in the process of handing over creative
responsibility.

    

### [[2107.01863] On the Efficiency of Various Deep Transfer Learning Models in Glitch Waveform Detection in Gravitational-Wave Data](http://arxiv.org/abs/2107.01863)


  LIGO is considered the most sensitive and complicated gravitational
experiment ever built. Its main objective is to detect the gravitational wave
from the strongest events in the universe by observing if the length of its
4-kilometer arms change by a distance 10,000 times smaller than the diameter of
a proton. Due to its sensitivity, LIGO is prone to the disturbance of external
noises which affects the data being collected to detect the gravitational wave.
These noises are commonly called by the LIGO community as glitches. The
objective of this study is to evaluate the effeciency of various deep trasnfer
learning models namely VGG19, ResNet50V2, VGG16 and ResNet101 to detect glitch
waveform in gravitational wave data. The accuracy achieved by the said models
are 98.98%, 98.35%, 97.56% and 94.73% respectively. Even though the models
achieved fairly high accuracy, it is observed that all of the model suffered
from the lack of data for certain classes which is the main concern found in
the experiment

    

### [[2107.01873] Detecting Concept Drift With Neural Network Model Uncertainty](http://arxiv.org/abs/2107.01873)


  Deployed machine learning models are confronted with the problem of changing
data over time, a phenomenon also called concept drift. While existing
approaches of concept drift detection already show convincing results, they
require true labels as a prerequisite for successful drift detection.
Especially in many real-world application scenarios-like the ones covered in
this work-true labels are scarce, and their acquisition is expensive.
Therefore, we introduce a new algorithm for drift detection, Uncertainty Drift
Detection (UDD), which is able to detect drifts without access to true labels.
Our approach is based on the uncertainty estimates provided by a deep neural
network in combination with Monte Carlo Dropout. Structural changes over time
are detected by applying the ADWIN technique on the uncertainty estimates, and
detected drifts trigger a retraining of the prediction model. In contrast to
input data-based drift detection, our approach considers the effects of the
current input data on the properties of the prediction model rather than
detecting change on the input data only (which can lead to unnecessary
retrainings). We show that UDD outperforms other state-of-the-art strategies on
two synthetic as well as ten real-world data sets for both regression and
classification tasks.

    

### [[2107.01875] DeepRapper: Neural Rap Generation with Rhyme and Rhythm Modeling](http://arxiv.org/abs/2107.01875)


  Rap generation, which aims to produce lyrics and corresponding singing beats,
needs to model both rhymes and rhythms. Previous works for rap generation
focused on rhyming lyrics but ignored rhythmic beats, which are important for
rap performance. In this paper, we develop DeepRapper, a Transformer-based rap
generation system that can model both rhymes and rhythms. Since there is no
available rap dataset with rhythmic beats, we develop a data mining pipeline to
collect a large-scale rap dataset, which includes a large number of rap songs
with aligned lyrics and rhythmic beats. Second, we design a Transformer-based
autoregressive language model which carefully models rhymes and rhythms.
Specifically, we generate lyrics in the reverse order with rhyme representation
and constraint for rhyme enhancement and insert a beat symbol into lyrics for
rhythm/beat modeling. To our knowledge, DeepRapper is the first system to
generate rap with both rhymes and rhythms. Both objective and subjective
evaluations demonstrate that DeepRapper generates creative and high-quality
raps with rhymes and rhythms. Code will be released on GitHub.

    

### [[2107.01876] Causally Invariant Predictor with Shift-Robustness](http://arxiv.org/abs/2107.01876)


  This paper proposes an invariant causal predictor that is robust to
distribution shift across domains and maximally reserves the transferable
invariant information. Based on a disentangled causal factorization, we
formulate the distribution shift as soft interventions in the system, which
covers a wide range of cases for distribution shift as we do not make prior
specifications on the causal structure or the intervened variables. Instead of
imposing regularizations to constrain the invariance of the predictor, we
propose to predict by the intervened conditional expectation based on the
do-operator and then prove that it is invariant across domains. More
importantly, we prove that the proposed predictor is the robust predictor that
minimizes the worst-case quadratic loss among the distributions of all domains.
For empirical learning, we propose an intuitive and flexible estimating method
based on data regeneration and present a local causal discovery procedure to
guide the regeneration step. The key idea is to regenerate data such that the
regenerated distribution is compatible with the intervened graph, which allows
us to incorporate standard supervised learning methods with the regenerated
data. Experimental results on both synthetic and real data demonstrate the
efficacy of our predictor in improving the predictive accuracy and robustness
across domains.

    

### [[2107.01881] Robust Online Convex Optimization in the Presence of Outliers](http://arxiv.org/abs/2107.01881)


  We consider online convex optimization when a number k of data points are
outliers that may be corrupted. We model this by introducing the notion of
robust regret, which measures the regret only on rounds that are not outliers.
The aim for the learner is to achieve small robust regret, without knowing
where the outliers are. If the outliers are chosen adversarially, we show that
a simple filtering strategy on extreme gradients incurs O(k) additive overhead
compared to the usual regret bounds, and that this is unimprovable, which means
that k needs to be sublinear in the number of rounds. We further ask which
additional assumptions would allow for a linear number of outliers. It turns
out that the usual benign cases of independently, identically distributed
(i.i.d.) observations or strongly convex losses are not sufficient. However,
combining i.i.d. observations with the assumption that outliers are those
observations that are in an extreme quantile of the distribution, does lead to
sublinear robust regret, even though the expected number of outliers is linear.

    

### [[2107.01892] NOTE: Solution for KDD-CUP 2021 WikiKG90M-LSC](http://arxiv.org/abs/2107.01892)


  WikiKG90M in KDD Cup 2021 is a large encyclopedic knowledge graph, which
could benefit various downstream applications such as question answering and
recommender systems. Participants are invited to complete the knowledge graph
by predicting missing triplets. Recent representation learning methods have
achieved great success on standard datasets like FB15k-237. Thus, we train the
advanced algorithms in different domains to learn the triplets, including OTE,
QuatE, RotatE and TransE. Significantly, we modified OTE into NOTE (short for
Norm-OTE) for better performance. Besides, we use both the DeepWalk and the
post-smoothing technique to capture the graph structure for supplementation. In
addition to the representations, we also use various statistical probabilities
among the head entities, the relations and the tail entities for the final
prediction. Experimental results show that the ensemble of state-of-the-art
representation learning methods could draw on each others strengths. And we
develop feature engineering from validation candidates for further
improvements. Please note that we apply the same strategy on the test set for
final inference. And these features may not be practical in the real world when
considering ranking against all the entities.

    

### [[2107.01894] Automated Recovery of Issue-Commit Links Leveraging Both Textual and Non-textual Data](http://arxiv.org/abs/2107.01894)


  An issue documents discussions around required changes in issue-tracking
systems, while a commit contains the change itself in the version control
systems. Recovering links between issues and commits can facilitate many
software evolution tasks such as bug localization, and software documentation.
A previous study on over half a million issues from GitHub reports only about
42.2% of issues are manually linked by developers to their pertinent commits.
Automating the linking of commit-issue pairs can contribute to the improvement
of the said tasks. By far, current state-of-the-art approaches for automated
commit-issue linking suffer from low precision, leading to unreliable results,
sometimes to the point that imposes human supervision on the predicted links.
The low performance gets even more severe when there is a lack of textual
information in either commits or issues. Current approaches are also proven
computationally expensive.
We propose Hybrid-Linker to overcome such limitations by exploiting two
information channels; (1) a non-textual-based component that operates on
non-textual, automatically recorded information of the commit-issue pairs to
predict a link, and (2) a textual-based one which does the same using textual
information of the commit-issue pairs. Then, combining the results from the two
classifiers, Hybrid-Linker makes the final prediction. Thus, every time one
component falls short in predicting a link, the other component fills the gap
and improves the results. We evaluate Hybrid-Linker against competing
approaches, namely FRLink and DeepLink on a dataset of 12 projects.
Hybrid-Linker achieves 90.1%, 87.8%, and 88.9% based on recall, precision, and
F-measure, respectively. It also outperforms FRLink and DeepLink by 31.3%, and
41.3%, regarding the F-measure. Moreover, Hybrid-Linker exhibits extensive
improvements in terms of performance as well.

    

### [[2107.01895] Optimizing the Numbers of Queries and Replies in Federated Learning with Differential Privacy](http://arxiv.org/abs/2107.01895)


  Federated learning (FL) empowers distributed clients to collaboratively train
a shared machine learning model through exchanging parameter information.
Despite the fact that FL can protect clients' raw data, malicious users can
still crack original data with disclosed parameters. To amend this flaw,
differential privacy (DP) is incorporated into FL clients to disturb original
parameters, which however can significantly impair the accuracy of the trained
model. In this work, we study a crucial question which has been vastly
overlooked by existing works: what are the optimal numbers of queries and
replies in FL with DP so that the final model accuracy is maximized. In FL, the
parameter server (PS) needs to query participating clients for multiple global
iterations to complete training. Each client responds a query from the PS by
conducting a local iteration. Our work investigates how many times the PS
should query clients and how many times each client should reply the PS. We
investigate two most extensively used DP mechanisms (i.e., the Laplace
mechanism and Gaussian mechanisms). Through conducting convergence rate
analysis, we can determine the optimal numbers of queries and replies in FL
with DP so that the final model accuracy can be maximized. Finally, extensive
experiments are conducted with publicly available datasets: MNIST and FEMNIST,
to verify our analysis and the results demonstrate that properly setting the
numbers of queries and replies can significantly improve the final model
accuracy in FL with DP.

    

### [[2107.01900] On The Distribution of Penultimate Activations of Classification Networks](http://arxiv.org/abs/2107.01900)


  This paper studies probability distributions ofpenultimate activations of
classification networks.We show that, when a classification network istrained
with the cross-entropy loss, its final classi-fication layer forms
aGenerative-Discriminativepairwith a generative classifier based on a
specificdistribution of penultimate activations. More im-portantly, the
distribution is parameterized by theweights of the final fully-connected layer,
and canbe considered as a generative model that synthe-sizes the penultimate
activations without feedinginput data. We empirically demonstrate that
thisgenerative model enables stable knowledge dis-tillation in the presence of
domain shift, and cantransfer knowledge from a classifier to
variationalautoencoders and generative adversarial networksfor
class-conditional image generation.

    

### [[2107.01904] Ensemble and Auxiliary Tasks for Data-Efficient Deep Reinforcement Learning](http://arxiv.org/abs/2107.01904)


  Ensemble and auxiliary tasks are both well known to improve the performance
of machine learning models when data is limited. However, the interaction
between these two methods is not well studied, particularly in the context of
deep reinforcement learning. In this paper, we study the effects of ensemble
and auxiliary tasks when combined with the deep Q-learning algorithm. We
perform a case study on ATARI games under limited data constraint. Moreover, we
derive a refined bias-variance-covariance decomposition to analyze the
different ways of learning ensembles and using auxiliary tasks, and use the
analysis to help provide some understanding of the case study. Our code is open
source and available at this https URL.

    

### [[2107.01906] The Last-Iterate Convergence Rate of Optimistic Mirror Descent in Stochastic Variational Inequalities](http://arxiv.org/abs/2107.01906)


  In this paper, we analyze the local convergence rate of optimistic mirror
descent methods in stochastic variational inequalities, a class of optimization
problems with important applications to learning theory and machine learning.
Our analysis reveals an intricate relation between the algorithm's rate of
convergence and the local geometry induced by the method's underlying Bregman
function. We quantify this relation by means of the Legendre exponent, a notion
that we introduce to measure the growth rate of the Bregman divergence relative
to the ambient norm near a solution. We show that this exponent determines both
the optimal step-size policy of the algorithm and the optimal rates attained,
explaining in this way the differences observed for some popular Bregman
functions (Euclidean projection, negative entropy, fractional power, etc.).

    

### [[2107.01927] Android Malware Category and Family Detection and Identification using Machine Learning](http://arxiv.org/abs/2107.01927)


  Android malware is one of the most dangerous threats on the internet, and
it's been on the rise for several years. Despite significant efforts in
detecting and classifying android malware from innocuous android applications,
there is still a long way to go. As a result, there is a need to provide a
basic understanding of the behavior displayed by the most common Android
malware categories and families. Each Android malware family and category has a
distinct objective. As a result, it has impacted every corporate area,
including healthcare, banking, transportation, government, and e-commerce. In
this paper, we presented two machine-learning approaches for Dynamic Analysis
of Android Malware: one for detecting and identifying Android Malware
Categories and the other for detecting and identifying Android Malware
Families, which was accomplished by analyzing a massive malware dataset with 14
prominent malware categories and 180 prominent malware families of
CCCS-CIC-AndMal2020 dataset on Dynamic Layers. Our approach achieves in Android
Malware Category detection more than 96 % accurate and achieves in Android
Malware Family detection more than 99% accurate. Our approach provides a method
for high-accuracy Dynamic Analysis of Android Malware while also shortening the
time required to analyze smartphone malware.

    

### [[2107.01936] Adversarial Robustness of Probabilistic Network Embedding for Link Prediction](http://arxiv.org/abs/2107.01936)


  In today's networked society, many real-world problems can be formalized as
predicting links in networks, such as Facebook friendship suggestions,
e-commerce recommendations, and the prediction of scientific collaborations in
citation networks. Increasingly often, link prediction problem is tackled by
means of network embedding methods, owing to their state-of-the-art
performance. However, these methods lack transparency when compared to simpler
baselines, and as a result their robustness against adversarial attacks is a
possible point of concern: could one or a few small adversarial modifications
to the network have a large impact on the link prediction performance when
using a network embedding model? Prior research has already investigated
adversarial robustness for network embedding models, focused on classification
at the node and graph level. Robustness with respect to the link prediction
downstream task, on the other hand, has been explored much less.
This paper contributes to filling this gap, by studying adversarial
robustness of Conditional Network Embedding (CNE), a state-of-the-art
probabilistic network embedding model, for link prediction. More specifically,
given CNE and a network, we measure the sensitivity of the link predictions of
the model to small adversarial perturbations of the network, namely changes of
the link status of a node pair. Thus, our approach allows one to identify the
links and non-links in the network that are most vulnerable to such
perturbations, for further investigation by an analyst. We analyze the
characteristics of the most and least sensitive perturbations, and empirically
confirm that our approach not only succeeds in identifying the most vulnerable
links and non-links, but also that it does so in a time-efficient manner thanks
to an effective approximation.

    

### [[2107.01943] When and How to Fool Explainable Models (and Humans) with Adversarial Examples](http://arxiv.org/abs/2107.01943)


  Reliable deployment of machine learning models such as neural networks
continues to be challenging due to several limitations. Some of the main
shortcomings are the lack of interpretability and the lack of robustness
against adversarial examples or out-of-distribution inputs. In this paper, we
explore the possibilities and limits of adversarial attacks for explainable
machine learning models. First, we extend the notion of adversarial examples to
fit in explainable machine learning scenarios, in which the inputs, the output
classifications and the explanations of the model's decisions are assessed by
humans. Next, we propose a comprehensive framework to study whether (and how)
adversarial examples can be generated for explainable models under human
assessment, introducing novel attack paradigms. In particular, our framework
considers a wide range of relevant (yet often ignored) factors such as the type
of problem, the user expertise or the objective of the explanations in order to
identify the attack strategies that should be adopted in each scenario to
successfully deceive the model (and the human). These contributions intend to
serve as a basis for a more rigorous and realistic study of adversarial
examples in the field of explainable machine learning.

    

### [[2107.01952] Partition and Code: learning how to compress graphs](http://arxiv.org/abs/2107.01952)


  Can we use machine learning to compress graph data? The absence of ordering
in graphs poses a significant challenge to conventional compression algorithms,
limiting their attainable gains as well as their ability to discover relevant
patterns. On the other hand, most graph compression approaches rely on
domain-dependent handcrafted representations and cannot adapt to different
underlying graph distributions. This work aims to establish the necessary
principles a lossless graph compression method should follow to approach the
entropy storage lower bound. Instead of making rigid assumptions about the
graph distribution, we formulate the compressor as a probabilistic model that
can be learned from data and generalise to unseen instances. Our "Partition and
Code" framework entails three steps: first, a partitioning algorithm decomposes
the graph into elementary structures, then these are mapped to the elements of
a small dictionary on which we learn a probability distribution, and finally,
an entropy encoder translates the representation into bits. All three steps are
parametric and can be trained with gradient descent. We theoretically compare
the compression quality of several graph encodings and prove, under mild
conditions, a total ordering of their expected description lengths. Moreover,
we show that, under the same conditions, PnC achieves compression gains w.r.t.
the baselines that grow either linearly or quadratically with the number of
vertices. Our algorithms are quantitatively evaluated on diverse real-world
networks obtaining significant performance improvements with respect to
different families of non-parametric and parametric graph compressors.

    

### [[2107.01955] Detecting Faults during Automatic Screwdriving: A Dataset and Use Case of Anomaly Detection for Automatic Screwdriving](http://arxiv.org/abs/2107.01955)


  Detecting faults in manufacturing applications can be difficult, especially
if each fault model is to be engineered by hand. Data-driven approaches, using
Machine Learning (ML) for detecting faults have recently gained increasing
interest, where a ML model can be trained on a set of data from a manufacturing
process. In this paper, we present a use case of using ML models for detecting
faults during automated screwdriving operations, and introduce a new dataset
containing fully monitored and registered data from a Universal Robot and
OnRobot screwdriver during both normal and anomalous operations. We illustrate,
with the use of two time-series ML models, how to detect faults in an automated
screwdriving application.

    

### [[2107.01959] Universal Approximation of Functions on Sets](http://arxiv.org/abs/2107.01959)


  Modelling functions of sets, or equivalently, permutation-invariant
functions, is a long-standing challenge in machine learning. Deep Sets is a
popular method which is known to be a universal approximator for continuous set
functions. We provide a theoretical analysis of Deep Sets which shows that this
universal approximation property is only guaranteed if the model's latent space
is sufficiently high-dimensional. If the latent space is even one dimension
lower than necessary, there exist piecewise-affine functions for which Deep
Sets performs no better than a naïve constant baseline, as judged by
worst-case error. Deep Sets may be viewed as the most efficient incarnation of
the Janossy pooling paradigm. We identify this paradigm as encompassing most
currently popular set-learning methods. Based on this connection, we discuss
the implications of our results for set learning more broadly, and identify
some open questions on the universality of Janossy pooling in general.

    

### [[2107.01969] The MineRL BASALT Competition on Learning from Human Feedback](http://arxiv.org/abs/2107.01969)


  The last decade has seen a significant increase of interest in deep learning
research, with many public successes that have demonstrated its potential. As
such, these systems are now being incorporated into commercial products. With
this comes an additional challenge: how can we build AI systems that solve
tasks where there is not a crisp, well-defined specification? While multiple
solutions have been proposed, in this competition we focus on one in
particular: learning from human feedback. Rather than training AI systems using
a predefined reward function or using a labeled dataset with a predefined set
of categories, we instead train the AI system using a learning signal derived
from some form of human feedback, which can evolve over time as the
understanding of the task changes, or as the capabilities of the AI system
improve.
The MineRL BASALT competition aims to spur forward research on this important
class of techniques. We design a suite of four tasks in Minecraft for which we
expect it will be hard to write down hardcoded reward functions. These tasks
are defined by a paragraph of natural language: for example, "create a
waterfall and take a scenic picture of it", with additional clarifying details.
Participants must train a separate agent for each task, using any method they
want. Agents are then evaluated by humans who have read the task description.
To help participants get started, we provide a dataset of human demonstrations
on each of the four tasks, as well as an imitation learning baseline that
leverages these demonstrations.
Our hope is that this competition will improve our ability to build AI
systems that do what their designers intend them to do, even when the intent
cannot be easily formalized. Besides allowing AI to solve more tasks, this can
also enable more effective regulation of AI systems, as well as making progress
on the value alignment problem.

    

### [[2107.01979] Machine Learning for Fraud Detection in E-Commerce: A Research Agenda](http://arxiv.org/abs/2107.01979)


  Fraud detection and prevention play an important part in ensuring the
sustained operation of any e-commerce business. Machine learning (ML) often
plays an important role in these anti-fraud operations, but the organizational
context in which these ML models operate cannot be ignored. In this paper, we
take an organization-centric view on the topic of fraud detection by
formulating an operational model of the anti-fraud departments in e-commerce
organizations. We derive 6 research topics and 12 practical challenges for
fraud detection from this operational model. We summarize the state of the
literature for each research topic, discuss potential solutions to the
practical challenges, and identify 22 open research challenges.

    

### [[2107.01983] Imputation-Free Learning from Incomplete Observations](http://arxiv.org/abs/2107.01983)


  Although recent works have developed methods that can generate estimations
(or imputations) of the missing entries in a dataset to facilitate downstream
analysis, most depend on assumptions that may not align with real-world
applications and could suffer from poor performance in subsequent tasks. This
is particularly true if the data have large missingness rates or a small
population. More importantly, the imputation error could be propagated into the
prediction step that follows, causing the gradients used to train the
prediction models to be biased. Consequently, in this work, we introduce the
importance guided stochastic gradient descent (IGSGD) method to train
multilayer perceptrons (MLPs) and long short-term memories (LSTMs) to directly
perform inference from inputs containing missing values without imputation.
Specifically, we employ reinforcement learning (RL) to adjust the gradients
used to train the models via back-propagation. This not only reduces bias but
allows the model to exploit the underlying information behind missingness
patterns. We test the proposed approach on real-world time-series (i.e.,
MIMIC-III), tabular data obtained from an eye clinic, and a standard dataset
(i.e., MNIST), where our imputation-free predictions outperform the traditional
two-step imputation-based predictions using state-of-the-art imputation
methods.

    

### [[2107.01988] UCSL : A Machine Learning Expectation-Maximization framework for Unsupervised Clustering driven by Supervised Learning](http://arxiv.org/abs/2107.01988)


  Subtype Discovery consists in finding interpretable and consistent sub-parts
of a dataset, which are also relevant to a certain supervised task. From a
mathematical point of view, this can be defined as a clustering task driven by
supervised learning in order to uncover subgroups in line with the supervised
prediction. In this paper, we propose a general Expectation-Maximization
ensemble framework entitled UCSL (Unsupervised Clustering driven by Supervised
Learning). Our method is generic, it can integrate any clustering method and
can be driven by both binary classification and regression. We propose to
construct a non-linear model by merging multiple linear estimators, one per
cluster. Each hyperplane is estimated so that it correctly discriminates - or
predict - only one cluster. We use SVC or Logistic Regression for
classification and SVR for regression. Furthermore, to perform cluster analysis
within a more suitable space, we also propose a dimension-reduction algorithm
that projects the data onto an orthonormal space relevant to the supervised
task. We analyze the robustness and generalization capability of our algorithm
using synthetic and experimental datasets. In particular, we validate its
ability to identify suitable consistent sub-types by conducting a
psychiatric-diseases cluster analysis with known ground-truth labels. The gain
of the proposed method over previous state-of-the-art techniques is about +1.9
points in terms of balanced accuracy. Finally, we make codes and examples
available in a scikit-learn-compatible Python package at
this https URL


### [[2107.01994] Template-Based Graph Clustering](http://arxiv.org/abs/2107.01994)


  We propose a novel graph clustering method guided by additional information
on the underlying structure of the clusters (or communities). The problem is
formulated as the matching of a graph to a template with smaller dimension,
hence matching $n$ vertices of the observed graph (to be clustered) to the $k$
vertices of a template graph, using its edges as support information, and
relaxed on the set of orthonormal matrices in order to find a $k$ dimensional
embedding. With relevant priors that encode the density of the clusters and
their relationships, our method outperforms classical methods, especially for
challenging cases.

    

### [[2107.01996] Explainability via Interactivity? Supporting Nonexperts' Sensemaking of Pretrained CNN by Interacting with Their Daily Surroundings](http://arxiv.org/abs/2107.01996)


  Current research on Explainable AI (XAI) heavily targets on expert users
(data scientists or AI developers). However, increasing importance has been
argued for making AI more understandable to nonexperts, who are expected to
leverage AI techniques, but have limited knowledge about AI. We present a
mobile application to support nonexperts to interactively make sense of
Convolutional Neural Networks (CNN); it allows users to play with a pretrained
CNN by taking pictures of their surrounding objects. We use an up-to-date XAI
technique (Class Activation Map) to intuitively visualize the model's decision
(the most important image regions that lead to a certain result). Deployed in a
university course, this playful learning tool was found to support design
students to gain vivid understandings about the capabilities and limitations of
pretrained CNNs in real-world environments. Concrete examples of students'
playful explorations are reported to characterize their sensemaking processes
reflecting different depths of thought.

    

### [[1602.03822] A Critical Connectivity Radius for Randomly-Generated, High Dimensional Data Points](http://arxiv.org/abs/1602.03822)


  We use random geometric graphs to describe clusters of higher dimensional
data points which are bijectively mapped to a (possibly) lower dimensional
space where an equivalent random cluster model is used to calculate the
expected number of modes to be found when separating the data of a multi-modal
data set into distinct clusters. Furthermore, as a function of the expected
number of modes and the number of data points in the sample, an upper bound on
a given distance measure is found such that data points have the greatest
correlation if their mutual distances from a common center is less than or
equal to the calculated bound. Anomalies are exposed, which lie outside of the
union of all regularized clusters of data points. Finally, similarly to finding
a hyperplane which can be shifted along its normal to expose the maximal
distance between binary classes, it is shown that the union of regularized
clusters can be used to define a hyperplane which can be shifted by a certain
amount to separate the data into binary classes.

    

### [[1807.04209] Differentially Private False Discovery Rate Control](http://arxiv.org/abs/1807.04209)


  Differential privacy provides a rigorous framework for privacy-preserving
data analysis. This paper proposes the first differentially private procedure
for controlling the false discovery rate (FDR) in multiple hypothesis testing.
Inspired by the Benjamini-Hochberg procedure (BHq), our approach is to first
repeatedly add noise to the logarithms of the $p$-values to ensure differential
privacy and to select an approximately smallest $p$-value serving as a
promising candidate at each iteration; the selected $p$-values are further
supplied to the BHq and our private procedure releases only the rejected ones.
Moreover, we develop a new technique that is based on a backward submartingale
for proving FDR control of a broad class of multiple testing procedures,
including our private procedure, and both the BHq step-up and step-down
procedures. As a novel aspect, the proof works for arbitrary dependence between
the true null and false null test statistics, while FDR control is maintained
up to a small multiplicative factor.

    

### [[1902.09434] S-TRIGGER: Continual State Representation Learning via Self-Triggered Generative Replay](http://arxiv.org/abs/1902.09434)


  We consider the problem of building a state representation model for control,
in a continual learning setting. As the environment changes, the aim is to
efficiently compress the sensory state's information without losing past
knowledge, and then use Reinforcement Learning on the resulting features for
efficient policy learning. To this end, we propose S-TRIGGER, a general method
for Continual State Representation Learning applicable to Variational
Auto-Encoders and its many variants. The method is based on Generative Replay,
i.e. the use of generated samples to maintain past knowledge. It comes along
with a statistically sound method for environment change detection, which
self-triggers the Generative Replay. Our experiments on VAEs show that
S-TRIGGER learns state representations that allows fast and high-performing
Reinforcement Learning, while avoiding catastrophic forgetting. The resulting
system is capable of autonomously learning new information without using past
data and with a bounded system size. Code for our experiments is attached in
Appendix.

    

### [[1905.01489] WoodScape: A multi-task, multi-camera fisheye dataset for autonomous driving](http://arxiv.org/abs/1905.01489)


  Fisheye cameras are commonly employed for obtaining a large field of view in
surveillance, augmented reality and in particular automotive applications. In
spite of their prevalence, there are few public datasets for detailed
evaluation of computer vision algorithms on fisheye images. We release the
first extensive fisheye automotive dataset, WoodScape, named after Robert Wood
who invented the fisheye camera in 1906. WoodScape comprises of four surround
view cameras and nine tasks including segmentation, depth estimation, 3D
bounding box detection and soiling detection. Semantic annotation of 40 classes
at the instance level is provided for over 10,000 images and annotation for
other tasks are provided for over 100,000 images. With WoodScape, we would like
to encourage the community to adapt computer vision models for fisheye camera
instead of using naive rectification.

    

### [[1905.10488] GAN2GAN: Generative Noise Learning for Blind Denoising with Single Noisy Images](http://arxiv.org/abs/1905.10488)


  We tackle a challenging blind image denoising problem, in which only single
distinct noisy images are available for training a denoiser, and no information
about noise is known, except for it being zero-mean, additive, and independent
of the clean image. In such a setting, which often occurs in practice, it is
not possible to train a denoiser with the standard discriminative training or
with the recently developed Noise2Noise (N2N) training; the former requires the
underlying clean image for the given noisy image, and the latter requires two
independently realized noisy image pair for a clean image. To that end, we
propose GAN2GAN (Generated-Artificial-Noise to Generated-Artificial-Noise)
method that first learns a generative model that can 1) simulate the noise in
the given noisy images and 2) generate a rough, noisy estimates of the clean
images, then 3) iteratively trains a denoiser with subsequently synthesized
noisy image pairs (as in N2N), obtained from the generative model. In results,
we show the denoiser trained with our GAN2GAN achieves an impressive denoising
performance on both synthetic and real-world datasets for the blind denoising
setting; it almost approaches the performance of the standard
discriminatively-trained or N2N-trained models that have more information than
ours, and it significantly outperforms the recent baseline for the same
setting, \textit{e.g.}, Noise2Void, and a more conventional yet strong one,
BM3D. The official code of our method is available at
this https URL.

    

### [[1906.04675] Taxonomy of Saliency Metrics for Channel Pruning](http://arxiv.org/abs/1906.04675)


  Pruning unimportant parameters can allow deep neural networks (DNNs) to
reduce their heavy computation and memory requirements. A saliency metric
estimates which parameters can be safely pruned with little impact on the
classification performance of the DNN. Many saliency metrics have been
proposed, each within the context of a wider pruning algorithm. The result is
that it is difficult to separate the effectiveness of the saliency metric from
the wider pruning algorithm that surrounds it. Similar-looking saliency metrics
can yield very different results because of apparently minor design choices. We
propose a taxonomy of saliency metrics based on four mostly-orthogonal
principal components. We show that a broad range of metrics from the pruning
literature can be grouped according to these components. Our taxonomy not only
serves as a guide to prior work, but allows us to construct new saliency
metrics by exploring novel combinations of our taxonomic components. We perform
an in-depth experimental investigation of more than 300 saliency metrics. Our
results provide decisive answers to open research questions, and demonstrate
the importance of reduction and scaling when pruning groups of weights. We find
that some of our constructed metrics can outperform the best existing
state-of-the-art metrics for convolutional neural network channel pruning.

    

### [[1910.02325] Bayesian Learning-Based Adaptive Control for Safety Critical Systems](http://arxiv.org/abs/1910.02325)


  Deep learning has enjoyed much recent success, and applying state-of-the-art
model learning methods to controls is an exciting prospect. However, there is a
strong reluctance to use these methods on safety-critical systems, which have
constraints on safety, stability, and real-time performance. We propose a
framework which satisfies these constraints while allowing the use of deep
neural networks for learning model uncertainties. Central to our method is the
use of Bayesian model learning, which provides an avenue for maintaining
appropriate degrees of caution in the face of the unknown. In the proposed
approach, we develop an adaptive control framework leveraging the theory of
stochastic CLFs (Control Lyapunov Functions) and stochastic CBFs (Control
Barrier Functions) along with tractable Bayesian model learning via Gaussian
Processes or Bayesian neural networks. Under reasonable assumptions, we
guarantee stability and safety while adapting to unknown dynamics with
probability 1. We demonstrate this architecture for high-speed terrestrial
mobility targeting potential applications in safety-critical high-speed Mars
rover missions.

    

### [[1911.02903] How Implicit Regularization of ReLU Neural Networks Characterizes the Learned Function -- Part I: the 1-D Case of Two Layers with Random First Layer](http://arxiv.org/abs/1911.02903)


  Today, various forms of neural networks are trained to perform approximation
tasks in many fields. However, the estimates obtained are not fully understood
on function space. Empirical results suggest that typical training algorithms
favor regularized solutions. These observations motivate us to analyze
properties of the neural networks found by gradient descent initialized close
to zero, that is frequently employed to perform the training task. As a
starting point, we consider one dimensional (shallow) ReLU neural networks in
which weights are chosen randomly and only the terminal layer is trained.
First, we rigorously show that for such networks ridge regularized regression
corresponds in function space to regularizing the estimate's second derivative
for fairly general loss functionals. For least squares regression, we show that
the trained network converges to the smooth spline interpolation of the
training data as the number of hidden nodes tends to infinity. Moreover, we
derive a correspondence between the early stopped gradient descent and the
smoothing spline regression. Our analysis might give valuable insight on the
properties of the solutions obtained using gradient descent methods in general
settings.

    

### [[1911.04872] Two Ridge Solutions for the Incremental Broad Learning System on Added Nodes](http://arxiv.org/abs/1911.04872)


  The original Broad Learning System (BLS) on new added nodes and its existing
efficient implementation both assume the ridge parameter is near 0 in the ridge
inverse to approximate the generalized inverse, and compute the generalized
inverse solution for the output weights. In this paper, we propose two ridge
solutions for the output weights in the BLS on added nodes, where the ridge
parameter can be any positive real number. One of the proposed ridge solutions
computes the output weights from the inverse Cholesky factor, which is updated
by extending the existing inverse Cholesky factorization. The other proposed
ridge solution computes the output weights from the ridge inverse, and updates
the ridge inverse by extending the Greville method that can only computes the
generalized inverse of a partitioned matrix. The proposed BLS algorithm based
on the ridge inverse requires the same complexity as the original BLS
algorithm, while the proposed BLS algorithm based on the inverse Cholesky
factor requires less complexity and training time than the original BLS and the
existing efficient BLS. Both the proposed ridge solutions for BLS achieve the
same testing accuracy as the standard ridge solution in the numerical
experiments. The difference between the testing accuracy of the proposed ridge
solutions and that of the existing generalized inverse solutions is negligible
when the ridge parameter is very small, and becomes too big to be ignored when
the ridge parameter is not very small. When the ridge parameter is not near 0,
usually the proposed two ridge solutions for BLS achieve better testing
accuracy than the existing generalized inverse solutions for BLS, and then the
former are more preferred than the latter.

    

### [[2001.04463] Unsupervised Audiovisual Synthesis via Exemplar Autoencoders](http://arxiv.org/abs/2001.04463)


  We present an unsupervised approach that converts the input speech of any
individual into audiovisual streams of potentially-infinitely many output
speakers. Our approach builds on simple autoencoders that project out-of-sample
data onto the distribution of the training set. We use Exemplar Autoencoders to
learn the voice, stylistic prosody, and visual appearance of a specific target
exemplar speech. In contrast to existing methods, the proposed approach can be
easily extended to an arbitrarily large number of speakers and styles using
only 3 minutes of target audio-video data, without requiring {\em any} training
data for the input speaker. To do so, we learn audiovisual bottleneck
representations that capture the structured linguistic content of speech. We
outperform prior approaches on both audio and video synthesis, and provide
extensive qualitative analysis on our project page --
this https URL.

    

### [[2001.07248] SGLB: Stochastic Gradient Langevin Boosting](http://arxiv.org/abs/2001.07248)


  This paper introduces Stochastic Gradient Langevin Boosting (SGLB) - a
powerful and efficient machine learning framework that may deal with a wide
range of loss functions and has provable generalization guarantees. The method
is based on a special form of the Langevin diffusion equation specifically
designed for gradient boosting. This allows us to theoretically guarantee the
global convergence even for multimodal loss functions, while standard gradient
boosting algorithms can guarantee only local optimum. We also empirically show
that SGLB outperforms classic gradient boosting when applied to classification
tasks with 0-1 loss function, which is known to be multimodal.

    

### [[2001.08603] Learning Distributional Programs for Relational Autocompletion](http://arxiv.org/abs/2001.08603)


  Relational autocompletion is the problem of automatically filling out some
missing values in multi-relational data. We tackle this problem within the
probabilistic logic programming framework of Distributional Clauses (DC), which
supports both discrete and continuous probability distributions. Within this
framework, we introduce DiceML { an approach to learn both the structure and
the parameters of DC programs from relational data (with possibly missing
data). To realize this, DiceML integrates statistical modeling and
distributional clauses with rule learning. The distinguishing features of
DiceML are that it 1) tackles autocompletion in relational data, 2) learns
distributional clauses extended with statistical models, 3) deals with both
discrete and continuous distributions, 4) can exploit background knowledge, and
5) uses an expectation-maximization based algorithm to cope with missing data.
The empirical results show the promise of the approach, even when there is
missing data.

    

### [[2002.00291] Oracle Lower Bounds for Stochastic Gradient Sampling Algorithms](http://arxiv.org/abs/2002.00291)


  We consider the problem of sampling from a strongly log-concave density in
$\mathbb{R}^d$, and prove an information theoretic lower bound on the number of
stochastic gradient queries of the log density needed. Several popular sampling
algorithms (including many Markov chain Monte Carlo methods) operate by using
stochastic gradients of the log density to generate a sample; our results
establish an information theoretic limit for all these algorithms.
We show that for every algorithm, there exists a well-conditioned strongly
log-concave target density for which the distribution of points generated by
the algorithm would be at least $\varepsilon$ away from the target in total
variation distance if the number of gradient queries is less than
$\Omega(\sigma^2 d/\varepsilon^2)$, where $\sigma^2 d$ is the variance of the
stochastic gradient. Our lower bound follows by combining the ideas of Le Cam
deficiency routinely used in the comparison of statistical experiments along
with standard information theoretic tools used in lower bounding Bayes risk
functions. To the best of our knowledge our results provide the first
nontrivial dimension-dependent lower bound for this problem.

    

### [[2002.09650] Learning Cost Functions for Optimal Transport](http://arxiv.org/abs/2002.09650)


  Inverse optimal transport (OT) refers to the problem of learning the cost
function for OT from observed transport plan or its samples. In this paper, we
derive an unconstrained convex optimization formulation of the inverse OT
problem, which can be further augmented by any customizable regularization. We
provide a comprehensive characterization of the properties of inverse OT,
including uniqueness of solutions. We also develop two numerical algorithms,
one is a fast matrix scaling method based on the Sinkhorn-Knopp algorithm for
discrete OT, and the other one is a learning based algorithm that parameterizes
the cost function as a deep neural network for continuous OT. The novel
framework proposed in the work avoids repeatedly solving a forward OT in each
iteration which has been a thorny computational bottleneck for the bi-level
optimization in existing inverse OT approaches. Numerical results demonstrate
promising efficiency and accuracy advantages of the proposed algorithms over
existing state-of-the-art methods.

    

### [[2005.08081] Layer-Wise Cross-View Decoding for Sequence-to-Sequence Learning](http://arxiv.org/abs/2005.08081)


  In sequence-to-sequence learning, the decoder relies on the attention
mechanism to efficiently extract information from the encoder. While it is
common practice to draw information from only the last encoder layer, recent
work has proposed to use representations from different encoder layers for
diversified levels of information. Nonetheless, the decoder still obtains only
a single view of the source sequences, which might lead to insufficient
training of the encoder layer stack due to the hierarchy bypassing problem. In
this work, we propose layer-wise cross-view decoding, where for each decoder
layer, together with the representations from the last encoder layer, which
serve as a global view, those from other encoder layers are supplemented for a
stereoscopic view of the source sequences. Systematic experiments show that we
successfully address the hierarchy bypassing problem and substantially improve
the performance of sequence-to-sequence learning with deep representations on
diverse tasks.

    

### [[2005.09310] Distilling Knowledge from Ensembles of Acoustic Models for Joint CTC-Attention End-to-End Speech Recognition](http://arxiv.org/abs/2005.09310)


  Knowledge distillation has been widely used to compress existing deep
learning models while preserving the performance on a wide range of
applications. In the specific context of Automatic Speech Recognition (ASR),
distillation from ensembles of acoustic models has recently shown promising
results in increasing recognition performance. In this paper, we propose an
extension of multi-teacher distillation methods to joint CTC-attention
end-to-end ASR systems. We also introduce three novel distillation strategies.
The core intuition behind them is to integrate the error rate metric to the
teacher selection rather than solely focusing on the observed losses. In this
way, we directly distill and optimize the student toward the relevant metric
for speech recognition. We evaluate these strategies under a selection of
training procedures on different datasets (TIMIT, Librispeech, Common Voice)
and various languages (English, French, Italian). In particular,
state-of-the-art error rates are reported on the Common Voice French, Italian
and TIMIT datasets.

    

### [[2005.10190] Feature Purification: How Adversarial Training Performs Robust Deep Learning](http://arxiv.org/abs/2005.10190)


  Despite the empirical success of using Adversarial Training to defend deep
learning models against adversarial perturbations, so far, it still remains
rather unclear what the principles are behind the existence of adversarial
perturbations, and what adversarial training does to the neural network to
remove them.
In this paper, we present a principle that we call Feature Purification,
where we show one of the causes of the existence of adversarial examples is the
accumulation of certain small dense mixtures in the hidden weights during the
training process of a neural network; and more importantly, one of the goals of
adversarial training is to remove such mixtures to purify hidden weights. We
present both experiments on the CIFAR-10 dataset to illustrate this principle,
and a theoretical result proving that for certain natural classification tasks,
training a two-layer neural network with ReLU activation using randomly
initialized gradient descent indeed satisfies this principle.
Technically, we give, to the best of our knowledge, the first result proving
that the following two can hold simultaneously for training a neural network
with ReLU activation. (1) Training over the original data is indeed non-robust
to small adversarial perturbations of some radius. (2) Adversarial training,
even with an empirical perturbation algorithm such as FGM, can in fact be
provably robust against ANY perturbations of the same radius. Finally, we also
prove a complexity lower bound, showing that low complexity models such as
linear classifiers, low-degree polynomials, or even the neural tangent kernel
for this network, CANNOT defend against perturbations of this same radius, no
matter what algorithms are used to train them.

    

### [[2005.10696] Novel Policy Seeking with Constrained Optimization](http://arxiv.org/abs/2005.10696)


  In problem-solving, we humans can come up with multiple novel solutions to
the same problem. However, reinforcement learning algorithms can only produce a
set of monotonous policies that maximize the cumulative reward but lack
diversity and novelty. In this work, we address the problem of generating novel
policies in reinforcement learning tasks. Instead of following the
multi-objective framework used in existing methods, we propose to rethink the
problem under a novel perspective of constrained optimization. We first
introduce a new metric to evaluate the difference between policies and then
design two practical novel policy generation methods following the new
perspective. The two proposed methods, namely the Constrained Task Novel
Bisector (CTNB) and the Interior Policy Differentiation (IPD), are derived from
the feasible direction method and the interior point method commonly known in
the constrained optimization literature. Experimental comparisons on the MuJoCo
control suite show our methods can achieve substantial improvement over
previous novelty-seeking methods in terms of both the novelty of policies and
their performances in the primal task.

    

### [[2006.00492] BiERU: Bidirectional Emotional Recurrent Unit for Conversational Sentiment Analysis](http://arxiv.org/abs/2006.00492)


  Sentiment analysis in conversations has gained increasing attention in recent
years for the growing amount of applications it can serve, e.g., sentiment
analysis, recommender systems, and human-robot interaction. The main difference
between conversational sentiment analysis and single sentence sentiment
analysis is the existence of context information which may influence the
sentiment of an utterance in a dialogue. How to effectively encode contextual
information in dialogues, however, remains a challenge. Existing approaches
employ complicated deep learning structures to distinguish different parties in
a conversation and then model the context information. In this paper, we
propose a fast, compact and parameter-efficient party-ignorant framework named
bidirectional emotional recurrent unit for conversational sentiment analysis.
In our system, a generalized neural tensor block followed by a two-channel
classifier is designed to perform context compositionality and sentiment
classification, respectively. Extensive experiments on three standard datasets
demonstrate that our model outperforms the state of the art in most cases.

    

### [[2006.05900] All Local Minima are Global for Two-Layer ReLU Neural Networks: The Hidden Convex Optimization Landscape](http://arxiv.org/abs/2006.05900)


  We prove that finding all globally optimal two-layer ReLU neural networks can
be performed by solving a convex optimization program with cone constraints.
Our analysis is novel, characterizes all optimal solutions, and does not
leverage duality-based analysis which was recently used to lift neural network
training into convex spaces. Given the set of solutions of our convex
optimization program, we show how to construct exactly the entire set of
optimal neural networks. We provide a detailed characterization of this optimal
set and its invariant transformations. As additional consequences of our convex
perspective, (i) we establish that Clarke stationary points found by stochastic
gradient descent correspond to the global optimum of a subsampled convex
problem (ii) we provide a polynomial-time algorithm for checking if a neural
network is a global minimum of the training loss (iii) we provide an explicit
construction of a continuous path between any neural network and the global
minimum of its sublevel set and (iv) characterize the minimal size of the
hidden layer so that the neural network optimization landscape has no spurious
valleys. Overall, we provide a rich framework for studying the landscape of
neural network training loss through convexity.

    

### [[2006.06600] Zeroth-Order Supervised Policy Improvement](http://arxiv.org/abs/2006.06600)


  Policy gradient (PG) algorithms have been widely used in reinforcement
learning (RL). However, PG algorithms rely on exploiting the value function
being learned with the first-order update locally, which results in limited
sample efficiency. In this work, we propose an alternative method called
Zeroth-Order Supervised Policy Improvement (ZOSPI). ZOSPI exploits the
estimated value function $Q$ globally while preserving the local exploitation
of the PG methods based on zeroth-order policy optimization. This learning
paradigm follows Q-learning but overcomes the difficulty of efficiently
operating argmax in continuous action space. It finds max-valued action within
a small number of samples. The policy learning of ZOSPI has two steps: First,
it samples actions and evaluates those actions with a learned value estimator,
and then it learns to perform the action with the highest value through
supervised learning. We further demonstrate such a supervised learning
framework can learn multi-modal policies. Experiments show that ZOSPI achieves
competitive results on the continuous control benchmarks with a remarkable
sample efficiency.

    

### [[2006.07616] SDCOR: Scalable Density-based Clustering for Local Outlier Detection in Massive-Scale Datasets](http://arxiv.org/abs/2006.07616)


  This paper presents a batch-wise density-based clustering approach for local
outlier detection in massive-scale datasets. Unlike the well-known traditional
algorithms, which assume that all the data is memory-resident, our proposed
method is scalable and processes the input data chunk-by-chunk within the
confines of a limited memory buffer. A temporary clustering model is built at
the first phase; then, it is gradually updated by analyzing consecutive memory
loads of points. Subsequently, at the end of scalable clustering, the
approximate structure of the original clusters is obtained. Finally, by another
scan of the entire dataset and using a suitable criterion, an outlying score is
assigned to each object called SDCOR (Scalable Density-based Clustering
Outlierness Ratio). Evaluations on real-life and synthetic datasets demonstrate
that the proposed method has a low linear time complexity and is more effective
and efficient compared to best-known conventional density-based methods, which
need to load all data into the memory; and also, to some fast distance-based
methods, which can perform on data resident in the disk.

    

### [[2006.07796] Structure by Architecture: Disentangled Representations without Regularization](http://arxiv.org/abs/2006.07796)


  We study the problem of self-supervised structured representation learning
using autoencoders for generative modeling. Unlike most methods which rely on
matching an arbitrary, relatively unstructured, prior distribution for
sampling, we propose a sampling technique that relies solely on the
independence of latent variables, thereby avoiding the trade-off between
reconstruction quality and generative performance inherent to VAEs. We design a
novel autoencoder architecture capable of learning a structured representation
without the need for aggressive regularization. Our structural decoders learn a
hierarchy of latent variables, akin to structural causal models, thereby
ordering the information without any additional regularization. We demonstrate
how these models learn a representation that improves results in a variety of
downstream tasks including generation, disentanglement, and extrapolation using
several challenging and natural image datasets.

    

### [[2006.09252] Improving Graph Neural Network Expressivity via Subgraph Isomorphism Counting](http://arxiv.org/abs/2006.09252)


  While Graph Neural Networks (GNNs) have achieved remarkable results in a
variety of applications, recent studies exposed important shortcomings in their
ability to capture the structure of the underlying graph. It has been shown
that the expressive power of standard GNNs is bounded by the Weisfeiler-Leman
(WL) graph isomorphism test, from which they inherit proven limitations such as
the inability to detect and count graph substructures. On the other hand, there
is significant empirical evidence, e.g. in network science and bioinformatics,
that substructures are often intimately related to downstream tasks. To this
end, we propose "Graph Substructure Networks" (GSN), a topologically-aware
message passing scheme based on substructure encoding. We theoretically analyse
the expressive power of our architecture, showing that it is strictly more
expressive than the WL test, and provide sufficient conditions for
universality. Importantly, we do not attempt to adhere to the WL hierarchy;
this allows us to retain multiple attractive properties of standard GNNs such
as locality and linear network complexity, while being able to disambiguate
even hard instances of graph isomorphism. We perform an extensive experimental
evaluation on graph classification and regression tasks and obtain
state-of-the-art results in diverse real-world settings including molecular
graphs and social networks. The code is publicly available at
this https URL.

    

### [[2006.10621] On the Predictability of Pruning Across Scales](http://arxiv.org/abs/2006.10621)


  We show that the error of iteratively magnitude-pruned networks empirically
follows a scaling law with interpretable coefficients that depend on the
architecture and task. We functionally approximate the error of the pruned
networks, showing it is predictable in terms of an invariant tying width,
depth, and pruning level, such that networks of vastly different pruned
densities are interchangeable. We demonstrate the accuracy of this
approximation over orders of magnitude in depth, width, dataset size, and
density. We show that the functional form holds (generalizes) for large scale
data (e.g., ImageNet) and architectures (e.g., ResNets). As neural networks
become ever larger and costlier to train, our findings suggest a framework for
reasoning conceptually and analytically about a standard method for
unstructured pruning.

    

### [[2006.11918] MaxVA: Fast Adaptation of Step Sizes by Maximizing Observed Variance of Gradients](http://arxiv.org/abs/2006.11918)


  Adaptive gradient methods such as RMSProp and Adam use exponential moving
estimate of the squared gradient to compute adaptive step sizes, achieving
better convergence than SGD in face of noisy objectives. However, Adam can have
undesirable convergence behaviors due to unstable or extreme adaptive learning
rates. Methods such as AMSGrad and AdaBound have been proposed to stabilize the
adaptive learning rates of Adam in the later stage of training, but they do not
outperform Adam in some practical tasks such as training Transformers
\cite{transformer}. In this paper, we propose an adaptive learning rate
principle, in which the running mean of squared gradient in Adam is replaced by
a weighted mean, with weights chosen to maximize the estimated variance of each
coordinate. This results in a faster adaptation to the local gradient variance,
which leads to more desirable empirical convergence behaviors than Adam. We
prove the proposed algorithm converges under mild assumptions for nonconvex
stochastic optimization problems, and demonstrate the improved efficacy of our
adaptive averaging approach on machine translation, natural language
understanding and large-batch pretraining of BERT. The code is available at
this https URL.

    

### [[2006.12655] Perceptual Adversarial Robustness: Defense Against Unseen Threat Models](http://arxiv.org/abs/2006.12655)


  A key challenge in adversarial robustness is the lack of a precise
mathematical characterization of human perception, used in the very definition
of adversarial attacks that are imperceptible to human eyes. Most current
attacks and defenses try to avoid this issue by considering restrictive
adversarial threat models such as those bounded by $L_2$ or $L_\infty$
distance, spatial perturbations, etc. However, models that are robust against
any of these restrictive threat models are still fragile against other threat
models. To resolve this issue, we propose adversarial training against the set
of all imperceptible adversarial examples, approximated using deep neural
networks. We call this threat model the neural perceptual threat model (NPTM);
it includes adversarial examples with a bounded neural perceptual distance (a
neural network-based approximation of the true perceptual distance) to natural
images. Through an extensive perceptual study, we show that the neural
perceptual distance correlates well with human judgements of perceptibility of
adversarial examples, validating our threat model.
Under the NPTM, we develop novel perceptual adversarial attacks and defenses.
Because the NPTM is very broad, we find that Perceptual Adversarial Training
(PAT) against a perceptual attack gives robustness against many other types of
adversarial attacks. We test PAT on CIFAR-10 and ImageNet-100 against five
diverse adversarial attacks. We find that PAT achieves state-of-the-art
robustness against the union of these five attacks, more than doubling the
accuracy over the next best model, without training against any of them. That
is, PAT generalizes well to unforeseen perturbation types. This is vital in
sensitive applications where a particular threat model cannot be assumed, and
to the best of our knowledge, PAT is the first adversarial training defense
with this property.

    

### [[2006.15714] Active Finite Reward Automaton Inference and Reinforcement Learning Using Queries and Counterexamples](http://arxiv.org/abs/2006.15714)


  Despite the fact that deep reinforcement learning (RL) has surpassed
human-level performances in various tasks, it still has several fundamental
challenges. First, most RL methods require intensive data from the exploration
of the environment to achieve satisfactory performance. Second, the use of
neural networks in RL renders it hard to interpret the internals of the system
in a way that humans can understand. To address these two challenges, we
propose a framework that enables an RL agent to reason over its exploration
process and distill high-level knowledge for effectively guiding its future
explorations. Specifically, we propose a novel RL algorithm that learns
high-level knowledge in the form of a finite reward automaton by using the L*
learning algorithm. We prove that in episodic RL, a finite reward automaton can
express any non-Markovian bounded reward functions with finitely many reward
values and approximate any non-Markovian bounded reward function (with
infinitely many reward values) with arbitrary precision. We also provide a
lower bound for the episode length such that the proposed RL approach almost
surely converges to an optimal policy in the limit. We test this approach on
two RL environments with non-Markovian reward functions, choosing a variety of
tasks with increasing complexity for each environment. We compare our algorithm
with the state-of-the-art RL algorithms for non-Markovian reward functions,
such as Joint Inference of Reward machines and Policies for RL (JIRP), Learning
Reward Machine (LRM), and Proximal Policy Optimization (PPO2). Our results show
that our algorithm converges to an optimal policy faster than other baseline
methods.

    

### [[2006.16161] A Two-step Surface-based 3D Deep Learning Pipeline for Segmentation of Intracranial Aneurysms](http://arxiv.org/abs/2006.16161)


  The exact shape of intracranial aneurysms is critical in medical diagnosis
and surgical planning. While voxel-based deep learning frameworks have been
proposed for this segmentation task, their performance remains limited. In this
study, we offer a two-step surface-based deep learning pipeline that achieves
significantly higher performance. Our proposed model takes a surface model of
entire principal brain arteries containing aneurysms as input and returns
aneurysms surfaces as output. A user first generates a surface model by
manually specifying multiple thresholds for time-of-flight magnetic resonance
angiography images. The system then samples small surface fragments from the
entire brain arteries and classifies the surface fragments according to whether
aneurysms are present using a point-based deep learning network (PointNet++).
Finally, the system applies surface segmentation (SO-Net) to surface fragments
containing aneurysms. We conduct a direct comparison of segmentation
performance by counting voxels between the proposed surface-based framework and
the existing voxel-based method, in which our framework achieves a much higher
dice similarity coefficient score (72%) than the prior approach (46%).

    

### [[2006.16785] Lipschitzness Is All You Need To Tame Off-policy Generative Adversarial Imitation Learning](http://arxiv.org/abs/2006.16785)


  Despite the recent success of reinforcement learning in various domains,
these approaches remain, for the most part, deterringly sensitive to
hyper-parameters and are often riddled with essential engineering feats
allowing their success. We consider the case of off-policy generative
adversarial imitation learning, and perform an in-depth review, qualitative and
quantitative, of the method. We show that forcing the learned reward function
to be local Lipschitz-continuous is a sine qua non condition for the method to
perform well. We then study the effects of this necessary condition and provide
several theoretical results involving the local Lipschitzness of the
state-value function. We complement these guarantees with empirical evidence
attesting to the strong positive effect that the consistent satisfaction of the
Lipschitzness constraint on the reward has on imitation performance. Finally,
we tackle a generic pessimistic reward preconditioning add-on spawning a large
class of reward shaping methods, which makes the base method it is plugged into
provably more robust, as shown in several additional theoretical guarantees. We
then discuss these through a fine-grained lens and share our insights.
Crucially, the guarantees derived and reported in this work are valid for any
reward satisfying the Lipschitzness condition, nothing is specific to
imitation. As such, these may be of independent interest.

    

### [[2007.01099] Reinforcement Learning and its Connections with Neuroscience and Psychology](http://arxiv.org/abs/2007.01099)


  Reinforcement learning methods have recently been very successful at
performing complex sequential tasks like playing Atari games, Go and Poker.
These algorithms have outperformed humans in several tasks by learning from
scratch, using only scalar rewards obtained through interaction with their
environment. While there certainly has been considerable independent innovation
to produce such results, many core ideas in reinforcement learning are inspired
by phenomena in animal learning, psychology and neuroscience. In this paper, we
comprehensively review a large number of findings in both neuroscience and
psychology that evidence reinforcement learning as a promising candidate for
modeling learning and decision making in the brain. In doing so, we construct a
mapping between various classes of modern RL algorithms and specific findings
in both neurophysiological and behavioral literature. We then discuss the
implications of this observed relationship between RL, neuroscience and
psychology and its role in advancing research in both AI and brain science.

    

### [[2007.08243] Lottery Tickets in Linear Models: An Analysis of Iterative Magnitude Pruning](http://arxiv.org/abs/2007.08243)


  We analyse the pruning procedure behind the lottery ticket hypothesis
arXiv:1803.03635v5, iterative magnitude pruning (IMP), when applied to linear
models trained by gradient flow. We begin by presenting sufficient conditions
on the statistical structure of the features under which IMP prunes those
features that have smallest projection onto the data. Following this, we
explore IMP as a method for sparse estimation.

    

### [[2007.08864] Sparse Linear Networks with a Fixed Butterfly Structure: Theory and Practice](http://arxiv.org/abs/2007.08864)


  A butterfly network consists of logarithmically many layers, each with a
linear number of non-zero weights (pre-specified). The fast
Johnson-Lindenstrauss transform (FJLT) can be represented as a butterfly
network followed by a projection onto a random subset of the coordinates.
Moreover, a random matrix based on FJLT with high probability approximates the
action of any matrix on a vector. Motivated by these facts, we propose to
replace a dense linear layer in any neural network by an architecture based on
the butterfly network. The proposed architecture significantly improves upon
the quadratic number of weights required in a standard dense layer to nearly
linear with little compromise in expressibility of the resulting operator. In a
collection of wide variety of experiments, including supervised prediction on
both the NLP and vision data, we show that this not only produces results that
match and at times outperform existing well-known architectures, but it also
offers faster training and prediction in deployment. To understand the
optimization problems posed by neural networks with a butterfly network, we
also study the optimization landscape of the encoder-decoder network, where the
encoder is replaced by a butterfly network followed by a dense linear layer in
smaller dimension. Theoretical result presented in the paper explains why the
training speed and outcome are not compromised by our proposed approach.

    

### [[2007.14209] Langevin Monte Carlo: random coordinate descent and variance reduction](http://arxiv.org/abs/2007.14209)


  Sampling from a log-concave distribution function on $\mathbb{R}^d$ (with
$d\gg 1$) is a popular problem that has wide applications. In this paper we
study the application of random coordinate descent method (RCD) on the Langevin
Monte Carlo (LMC) sampling method, and we find two sides of the theory:
1. The direct application of RCD on LMC does reduce the number of finite
differencing approximations per iteration, but it induces a large variance
error term. More iterations are then needed, and ultimately the method gains no
computational advantage;
2. When variance reduction techniques (such as SAGA and SVRG) are
incorporated in RCD-LMC, the variance error term is reduced. The new methods,
compared to the vanilla LMC, reduce the total computational cost by $d$ folds,
and achieve the optimal cost rate.
We perform our investigations in both overdamped and underdamped settings.

    

### [[2008.01393] Neural Granular Sound Synthesis](http://arxiv.org/abs/2008.01393)


  Granular sound synthesis is a popular audio generation technique based on
rearranging sequences of small waveform windows. In order to control the
synthesis, all grains in a given corpus are analyzed through a set of acoustic
descriptors. This provides a representation reflecting some form of local
similarities across the grains. However, the quality of this grain space is
bound by that of the descriptors. Its traversal is not continuously invertible
to signal and does not render any structured temporality.
We demonstrate that generative neural networks can implement granular
synthesis while alleviating most of its shortcomings. We efficiently replace
its audio descriptor basis by a probabilistic latent space learned with a
Variational Auto-Encoder. In this setting the learned grain space is
invertible, meaning that we can continuously synthesize sound when traversing
its dimensions. It also implies that original grains are not stored for
synthesis. Another major advantage of our approach is to learn structured paths
inside this latent space by training a higher-level temporal embedding over
arranged grain sequences.
The model can be applied to many types of libraries, including pitched notes
or unpitched drums and environmental noises. We report experiments on the
common granular synthesis processes as well as novel ones such as conditional
sampling and morphing.

    

### [[2008.10208] Multi-view Graph Learning by Joint Modeling of Consistency and Inconsistency](http://arxiv.org/abs/2008.10208)


  Graph learning has emerged as a promising technique for multi-view clustering
with its ability to learn a unified and robust graph from multiple views.
However, existing graph learning methods mostly focus on the multi-view
consistency issue, yet often neglect the inconsistency across multiple views,
which makes them vulnerable to possibly low-quality or noisy datasets. To
overcome this limitation, we propose a new multi-view graph learning framework,
which for the first time simultaneously and explicitly models multi-view
consistency and multi-view inconsistency in a unified objective function,
through which the consistent and inconsistent parts of each single-view graph
as well as the unified graph that fuses the consistent parts can be iteratively
learned. Though optimizing the objective function is NP-hard, we design a
highly efficient optimization algorithm which is able to obtain an approximate
solution with linear time complexity in the number of edges in the unified
graph. Furthermore, our multi-view graph learning approach can be applied to
both similarity graphs and dissimilarity graphs, which lead to two graph
fusion-based variants in our framework. Experiments on twelve multi-view
datasets have demonstrated the robustness and efficiency of the proposed
approach.

    

### [[2008.13578] Against Membership Inference Attack: Pruning is All You Need](http://arxiv.org/abs/2008.13578)


  The large model size, high computational operations, and vulnerability
against membership inference attack (MIA) have impeded deep learning or deep
neural networks (DNNs) popularity, especially on mobile devices. To address the
challenge, we envision that the weight pruning technique will help DNNs against
MIA while reducing model storage and computational operation. In this work, we
propose a pruning algorithm, and we show that the proposed algorithm can find a
subnetwork that can prevent privacy leakage from MIA and achieves competitive
accuracy with the original DNNs. We also verify our theoretical insights with
experiments. Our experimental results illustrate that the attack accuracy using
model compression is up to 13.6% and 10% lower than that of the baseline and
Min-Max game, accordingly.

    

### [[2009.02755] Anomaly Detection With Partitioning Overfitting Autoencoder Ensembles](http://arxiv.org/abs/2009.02755)


  In this paper, we propose POTATOES (Partitioning OverfiTting AuTOencoder
EnSemble), a new method for unsupervised outlier detection (UOD). More
precisely, given any autoencoder for UOD, this technique can be used to improve
its accuracy while at the same time removing the burden of tuning its
regularization. The idea is to not regularize at all, but to rather randomly
partition the data into sufficiently many equally sized parts, overfit each
part with its own autoencoder, and to use the maximum over all autoencoder
reconstruction errors as the anomaly score. We apply our model to various
realistic datasets and show that if the set of inliers is dense enough, our
method indeed improves the UOD performance of a given autoencoder
significantly. For reproducibility, the code is made available on github so the
reader can recreate the results in this paper as well as apply the method to
other autoencoders and datasets.

    

### [[2009.03228] Information Theoretic Meta Learning with Gaussian Processes](http://arxiv.org/abs/2009.03228)


  We formulate meta learning using information theoretic concepts; namely,
mutual information and the information bottleneck. The idea is to learn a
stochastic representation or encoding of the task description, given by a
training set, that is highly informative about predicting the validation set.
By making use of variational approximations to the mutual information, we
derive a general and tractable framework for meta learning. This framework
unifies existing gradient-based algorithms and also allows us to derive new
algorithms. In particular, we develop a memory-based algorithm that uses
Gaussian processes to obtain non-parametric encoding representations. We
demonstrate our method on a few-shot regression problem and on four few-shot
classification problems, obtaining competitive accuracy when compared to
existing baselines.

    

### [[2009.03671] A Self-Supervised Gait Encoding Approach with Locality-Awareness for 3D Skeleton Based Person Re-Identification](http://arxiv.org/abs/2009.03671)


  Person re-identification (Re-ID) via gait features within 3D skeleton
sequences is a newly-emerging topic with several advantages. Existing solutions
either rely on hand-crafted descriptors or supervised gait representation
learning. This paper proposes a self-supervised gait encoding approach that can
leverage unlabeled skeleton data to learn gait representations for person
Re-ID. Specifically, we first create self-supervision by learning to
reconstruct unlabeled skeleton sequences reversely, which involves richer
high-level semantics to obtain better gait representations. Other pretext tasks
are also explored to further improve self-supervised learning. Second, inspired
by the fact that motion's continuity endows adjacent skeletons in one skeleton
sequence and temporally consecutive skeleton sequences with higher correlations
(referred as locality in 3D skeleton data), we propose a locality-aware
attention mechanism and a locality-aware contrastive learning scheme, which aim
to preserve locality-awareness on intra-sequence level and inter-sequence level
respectively during self-supervised learning. Last, with context vectors
learned by our locality-aware attention mechanism and contrastive learning
scheme, a novel feature named Constrastive Attention-based Gait Encodings
(CAGEs) is designed to represent gait effectively. Empirical evaluations show
that our approach significantly outperforms skeleton-based counterparts by
15-40% Rank-1 accuracy, and it even achieves superior performance to numerous
multi-modal methods with extra RGB or depth information. Our codes are
available at this https URL.

    

### [[2010.00378] GraphXCOVID: Explainable Deep Graph Diffusion Pseudo-Labelling for Identifying COVID-19 on Chest X-rays](http://arxiv.org/abs/2010.00378)


  Can one learn to diagnose COVID-19 under extreme minimal supervision? Since
the outbreak of the novel COVID-19 there has been a rush for developing
Artificial Intelligence techniques for expert-level disease identification on
Chest X-ray data. In particular, the use of deep supervised learning has become
the go-to paradigm. However, the performance of such models is heavily
dependent on the availability of a large and representative labelled dataset.
The creation of which is a heavily expensive and time consuming task, and
especially imposes a great challenge for a novel disease. Semi-supervised
learning has shown the ability to match the incredible performance of
supervised models whilst requiring a small fraction of the labelled examples.
This makes the semi-supervised paradigm an attractive option for identifying
COVID-19. In this work, we introduce a graph based deep semi-supervised
framework for classifying COVID-19 from chest X-rays. Our framework introduces
an optimisation model for graph diffusion that reinforces the natural relation
among the tiny labelled set and the vast unlabelled data. We then connect the
diffusion prediction output as pseudo-labels that are used in an iterative
scheme in a deep net. We demonstrate, through our experiments, that our model
is able to outperform the current leading supervised model with a tiny fraction
of the labelled examples. Finally, we provide attention maps to accommodate the
radiologist's mental model, better fitting their perceptual and cognitive
abilities. These visualisation aims to assist the radiologist in judging
whether the diagnostic is correct or not, and in consequence to accelerate the
decision.

    

### [[2010.01592] Unknown Presentation Attack Detection against Rational Attackers](http://arxiv.org/abs/2010.01592)


  Despite the impressive progress in the field of presentation attack detection
and multimedia forensics over the last decade, these systems are still
vulnerable to attacks in real-life settings. Some of the challenges for
existing solutions are the detection of unknown attacks, the ability to perform
in adversarial settings, few-shot learning, and explainability. In this study,
these limitations are approached by reliance on a game-theoretic view for
modeling the interactions between the attacker and the detector. Consequently,
a new optimization criterion is proposed and a set of requirements are defined
for improving the performance of these systems in real-life settings.
Furthermore, a novel detection technique is proposed using generator-based
feature sets that are not biased towards any specific attack species. To
further optimize the performance on known attacks, a new loss function coined
categorical margin maximization loss (C-marmax) is proposed which gradually
improves the performance against the most powerful attack. The proposed
approach provides a more balanced performance across known and unknown attacks
and achieves state-of-the-art performance in known and unknown attack detection
cases against rational attackers. Lastly, the few-shot learning potential of
the proposed approach is studied as well as its ability to provide pixel-level
explainability.

    

### [[2010.02358] VisualWordGrid: Information Extraction From Scanned Documents Using A Multimodal Approach](http://arxiv.org/abs/2010.02358)


  We introduce a novel approach for scanned document representation to perform
field extraction. It allows the simultaneous encoding of the textual, visual
and layout information in a 3-axis tensor used as an input to a segmentation
model. We improve the recent Chargrid and Wordgrid \cite{chargrid} models in
several ways, first by taking into account the visual modality, then by
boosting its robustness in regards to small datasets while keeping the
inference time low. Our approach is tested on public and private document-image
datasets, showing higher performances compared to the recent state-of-the-art
methods.

    

### [[2010.04389] A Survey of Knowledge-Enhanced Text Generation](http://arxiv.org/abs/2010.04389)


  The goal of text generation is to make machines express in human language. It
is one of the most important yet challenging tasks in natural language
processing (NLP). Since 2014, various neural encoder-decoder models pioneered
by Seq2Seq have been proposed to achieve the goal by learning to map input text
to output text. However, the input text alone often provides limited knowledge
to generate the desired output, so the performance of text generation is still
far from satisfaction in many real-world scenarios. To address this issue,
researchers have considered incorporating various forms of knowledge beyond the
input text into the generation models. This research direction is known as
knowledge-enhanced text generation. In this survey, we present a comprehensive
review of the research on knowledge enhanced text generation over the past five
years. The main content includes two parts: (i) general methods and
architectures for integrating knowledge into text generation; (ii) specific
techniques and applications according to different forms of knowledge data.
This survey can have broad audiences, researchers and practitioners, in
academia and industry.

    

### [[2010.08707] Constrained Motion Planning Networks X](http://arxiv.org/abs/2010.08707)


  Constrained motion planning is a challenging field of research, aiming for
computationally efficient methods that can find a collision-free path on the
constraint manifolds between a given start and goal configuration. These
planning problems come up surprisingly frequently, such as in robot
manipulation for performing daily life assistive tasks. However, few solutions
to constrained motion planning are available, and those that exist struggle
with high computational time complexity in finding a path solution on the
manifolds. To address this challenge, we present Constrained Motion Planning
Networks X (CoMPNetX). It is a neural planning approach, comprising a
conditional deep neural generator and discriminator with neural gradients-based
fast projection operator. We also introduce neural task and scene
representations conditioned on which the CoMPNetX generates implicit manifold
configurations to turbo-charge any underlying classical planner such as
Sampling-based Motion Planning methods for quickly solving complex constrained
planning tasks. We show that our method finds path solutions with high success
rates and lower computation times than state-of-the-art traditional
path-finding tools on various challenging scenarios.

    

### [[2010.14672] How Does the Task Landscape Affect MAML Performance?](http://arxiv.org/abs/2010.14672)


  Model-Agnostic Meta-Learning (MAML) has become increasingly popular for
training models that can quickly adapt to new tasks via one or few stochastic
gradient descent steps. However, the MAML objective is significantly more
difficult to optimize compared to standard Empirical Risk Minimization (ERM),
and little is understood about how much MAML improves over ERM in terms of the
fast adaptability of their solutions in various scenarios. We analytically
address this issue in a linear regression setting consisting of a mixture of
easy and hard tasks, where hardness is related to the condition number of the
task's loss function. Specifically, we prove that in order for MAML to achieve
substantial gain over ERM, (i) there must be some discrepancy in hardness among
the tasks, and (ii) the optimal solutions of the hard tasks must be closely
packed with the center far from the center of the easy tasks optimal solutions.
We also give numerical and analytical results suggesting that these insights
also apply to two-layer neural networks. Finally, we provide few-shot image
classification experiments that support our insights for when MAML should be
used and emphasize the importance of training MAML on hard tasks in practice.

    

### [[2011.07006] Federated Multi-Mini-Batch: An Efficient Training Approach to Federated Learning in Non-IID Environments](http://arxiv.org/abs/2011.07006)


  Federated learning has faced performance and network communication
challenges, especially in the environments where the data is not independent
and identically distributed (IID) across the clients. To address the former
challenge, we introduce the federated-centralized concordance property and show
that the federated single-mini-batch training approach can achieve comparable
performance as the corresponding centralized training in the Non-IID
environments. To deal with the latter, we present the federated
multi-mini-batch approach and illustrate that it can establish a trade-off
between the performance and communication efficiency and outperforms federated
averaging in the Non-IID settings.

    

### [[2012.02130] A similarity-based Bayesian mixture-of-experts model](http://arxiv.org/abs/2012.02130)


  We present a new nonparametric mixture-of-experts model for multivariate
regression problems, inspired by the probabilistic $k$-nearest neighbors
algorithm. Using a conditionally specified model, predictions for out-of-sample
inputs are based on similarities to each observed data point, yielding
predictive distributions represented by Gaussian mixtures. Posterior inference
is performed on the parameters of the mixture components as well as the
distance metric using a mean-field variational Bayes algorithm accompanied with
a stochastic gradient-based optimization procedure. The proposed method is
especially advantageous in settings where inputs are of relatively high
dimension in comparison to the data size, where input--output relationships are
complex, and where predictive distributions may be skewed or multimodal.
Computational studies on two synthetic datasets and one dataset comprising dose
statistics of radiation therapy treatment plans show that our
mixture-of-experts method performs similarly or better than a conditional
Dirichlet process mixture model both in terms of validation metrics and visual
inspection.

    

### [[2012.03292] FedSiam: Towards Adaptive Federated Semi-Supervised Learning](http://arxiv.org/abs/2012.03292)


  Federated learning (FL) has emerged as an effective technique to co-training
machine learning models without actually sharing data and leaking privacy.
However, most existing FL methods focus on the supervised setting and ignore
the utilization of unlabeled data. Although there are a few existing studies
trying to incorporate unlabeled data into FL, they all fail to maintain
performance guarantees or generalization ability in various real-world
settings. In this paper, we focus on designing a general framework FedSiam to
tackle different scenarios of federated semi-supervised learning, including
four settings in the labels-at-client scenario and two setting in the
labels-at-server scenario. FedSiam is built upon a siamese network into FL with
a momentum update to handle the non-IID challenges introduced by unlabeled
data. We further propose a new metric to measure the divergence of local model
layers within the siamese network. Based on the divergence, FedSiam can
automatically select layer-level parameters to be uploaded to the server in an
adaptive manner. Experimental results on three datasets under two scenarios
with different data distribution settings demonstrate that the proposed FedSiam
framework outperforms state-of-the-art baselines.

    

### [[2012.06188] Recent Theoretical Advances in Non-Convex Optimization](http://arxiv.org/abs/2012.06188)


  Motivated by recent increased interest in optimization algorithms for
non-convex optimization in application to training deep neural networks and
other optimization problems in data analysis, we give an overview of recent
theoretical results on global performance guarantees of optimization algorithms
for non-convex optimization. We start with classical arguments showing that
general non-convex problems could not be solved efficiently in a reasonable
time. Then we give a list of problems that can be solved efficiently to find
the global minimizer by exploiting the structure of the problem as much as it
is possible. Another way to deal with non-convexity is to relax the goal from
finding the global minimum to finding a stationary point or a local minimum.
For this setting, we first present known results for the convergence rates of
deterministic first-order methods, which are then followed by a general
theoretical analysis of optimal stochastic and randomized gradient schemes, and
an overview of the stochastic first-order methods. After that, we discuss quite
general classes of non-convex problems, such as minimization of
$\alpha$-weakly-quasi-convex functions and functions that satisfy
Polyak--Lojasiewicz condition, which still allow obtaining theoretical
convergence guarantees of first-order methods. Then we consider higher-order
and zeroth-order/derivative-free methods and their convergence rates for
non-convex optimization problems.

    

### [[2012.06244] The Implicit Bias for Adaptive Optimization Algorithms on Homogeneous Neural Networks](http://arxiv.org/abs/2012.06244)


  Despite their overwhelming capacity to overfit, deep neural networks trained
by specific optimization algorithms tend to generalize well to unseen data.
Recently, researchers explained it by investigating the implicit regularization
effect of optimization algorithms. A remarkable progress is the work (Lyu&Li,
2019), which proves gradient descent (GD) maximizes the margin of homogeneous
deep neural networks. Except GD, adaptive algorithms such as AdaGrad, RMSProp
and Adam are popular owing to their rapid training process. However,
theoretical guarantee for the generalization of adaptive optimization
algorithms is still lacking. In this paper, we study the implicit
regularization of adaptive optimization algorithms when they are optimizing the
logistic loss on homogeneous deep neural networks. We prove that adaptive
algorithms that adopt exponential moving average strategy in conditioner (such
as Adam and RMSProp) can maximize the margin of the neural network, while
AdaGrad that directly sums historical squared gradients in conditioner can not.
It indicates superiority on generalization of exponential moving average
strategy in the design of the conditioner. Technically, we provide a unified
framework to analyze convergent direction of adaptive optimization algorithms
by constructing novel adaptive gradient flow and surrogate margin. Our
experiments can well support the theoretical findings on convergent direction
of adaptive optimization algorithms.

    

### [[2012.06279] Autoencoding Slow Representations for Semi-supervised Data Efficient Regression](http://arxiv.org/abs/2012.06279)


  The slowness principle is a concept inspired by the visual cortex of the
brain. It postulates that the underlying generative factors of a quickly
varying sensory signal change on a slower time scale. Unsupervised learning of
intermediate representations utilizing abundant unlabeled sensory data can be
leveraged to perform data-efficient supervised downstream regression. In this
paper, we propose a general formulation of slowness for unsupervised
representation learning adding a slowness regularization term to the estimate
lower bound of the beta-VAE to encourage temporal similarity in observation and
latent space. Within this framework we compare existing slowness regularization
terms such as the L1 and L2 loss used in existing end-to-end methods, the
SlowVAE and propose a new term based on Brownian motion. We empirically
evaluate these slowness regularization terms with respect to their downstream
task performance and data efficiency. We find that slow representations lead to
equal or better downstream task performance and data efficiency in different
experiment domains when compared to representations without slowness
regularization. Finally, we discuss how the Frechet Inception Distance (FID),
traditionally used to determine the generative capabilities of GANs, can serve
as a measure to predict the performance of pre-trained Autoencoder model in a
supervised downstream task and accelerate hyperparameter search.

    

### [[2012.07176] Extended Few-Shot Learning: Exploiting Existing Resources for Novel Tasks](http://arxiv.org/abs/2012.07176)


  In many practical few-shot learning problems, even though labeled examples
are scarce, there are abundant auxiliary datasets that potentially contain
useful information. We propose the problem of extended few-shot learning to
study these scenarios. We then introduce a framework to address the challenges
of efficiently selecting and effectively using auxiliary data in few-shot image
classification. Given a large auxiliary dataset and a notion of semantic
similarity among classes, we automatically select pseudo shots, which are
labeled examples from other classes related to the target task. We show that
naive approaches, such as (1) modeling these additional examples the same as
the target task examples or (2) using them to learn features via transfer
learning, only increase accuracy by a modest amount. Instead, we propose a
masking module that adjusts the features of auxiliary data to be more similar
to those of the target classes. We show that this masking module performs
better than naively modeling the support examples and transfer learning by 4.68
and 6.03 percentage points, respectively.

    

### [[2012.09816] Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning](http://arxiv.org/abs/2012.09816)


  We formally study how ensemble of deep learning models can improve test
accuracy, and how the superior performance of ensemble can be distilled into a
single model using knowledge distillation. We consider the challenging case
where the ensemble is simply an average of the outputs of a few independently
trained neural networks with the SAME architecture, trained using the SAME
algorithm on the SAME data set, and they only differ by the random seeds used
in the initialization.
We empirically show that ensemble/knowledge distillation in deep learning
works very differently from traditional learning theory, especially differently
from ensemble of random feature mappings or the neural-tangent-kernel feature
mappings, and is potentially out of the scope of existing theorems. Thus, to
properly understand ensemble and knowledge distillation in deep learning, we
develop a theory showing that when data has a structure we refer to as
"multi-view", then ensemble of independently trained neural networks can
provably improve test accuracy, and such superior test accuracy can also be
provably distilled into a single model by training a single model to match the
output of the ensemble instead of the true label. Our result sheds light on how
ensemble works in deep learning in a way that is completely different from
traditional theorems, and how the "dark knowledge" is hidden in the outputs of
the ensemble -- that can be used in knowledge distillation -- comparing to the
true data labels. In the end, we prove that self-distillation can also be
viewed as implicitly combining ensemble and knowledge distillation to improve
test accuracy.

    

### [[2012.10033] Exploring Fluent Query Reformulations with Text-to-Text Transformers and Reinforcement Learning](http://arxiv.org/abs/2012.10033)


  Query reformulation aims to alter noisy or ambiguous text sequences into
coherent ones closer to natural language questions. This is to prevent errors
from propagating in a client-facing pipeline and promote better communication
with users. Besides, it is crucial to maintain performance in downstream
environments like question answering when rephrased queries are given as input.
We show that under the previous framework (AQA), attempts to alter RL
algorithms do not bring significant benefits to either reward acquisition or
sequence fluency. Instead, we leverage a query-reformulating text-to-text
transformer (QRT5) and apply policy-based RL algorithms to further nudge this
reformulator and obtain better answers downstream by generating
reward-acquiring query trajectories. QRT5 shows better sample efficiency in RL
to achieve the same level of QA performance as the previous approach. It can
generate reformulations with more readability based on query well-formedness
evaluations and can generalize to out-of-sample data. Our framework is
demonstrated to be flexible, allowing reward signals to be sourced from
different downstream environments such as intent classification.

    

### [[2012.12561] GANDA: A deep generative adversarial network predicts the spatial distribution of nanoparticles in tumor pixelly](http://arxiv.org/abs/2012.12561)


  Intratumoral nanoparticles (NPs) distribution is critical for the success of
nanomedicine in imaging and treatment, but computational models to describe the
NPs distribution remain unavailable due to the complex tumor-nano interactions.
Here, we develop a Generative Adversarial Network for Distribution Analysis
(GANDA) to describe and conditionally generates the intratumoral quantum dots
(QDs) distribution after i.v. injection. This deep generative model is trained
automatically by 27 775 patches of tumor vessels and cell nuclei decomposed
from whole-slide images of 4T1 breast cancer sections. The GANDA model can
conditionally generate images of intratumoral QDs distribution under the
constraint of given tumor vessels and cell nuclei channels with the same
spatial resolution (pixels-to-pixels), minimal loss (mean squared error, MSE =
1.871) and excellent reliability (intraclass correlation, ICC = 0.94).
Quantitative analysis of QDs extravasation distance (ICC = 0.95) and subarea
distribution (ICC = 0.99) is allowed on the generated images without knowing
the real QDs distribution. We believe this deep generative model may provide
opportunities to investigate how influencing factors affect NPs distribution in
individual tumors and guide nanomedicine optimization for molecular imaging and
personalized treatment.

    

### [[2012.13962] A Tutorial on Sparse Gaussian Processes and Variational Inference](http://arxiv.org/abs/2012.13962)


  Gaussian processes (GPs) provide a framework for Bayesian inference that can
offer principled uncertainty estimates for a large range of problems. For
example, if we consider regression problems with Gaussian likelihoods, a GP
model enjoys a posterior in closed form. However, identifying the posterior GP
scales cubically with the number of training examples and requires to store all
examples in memory. In order to overcome these obstacles, sparse GPs have been
proposed that approximate the true posterior GP with pseudo-training examples.
Importantly, the number of pseudo-training examples is user-defined and enables
control over computational and memory complexity. In the general case, sparse
GPs do not enjoy closed-form solutions and one has to resort to approximate
inference. In this context, a convenient choice for approximate inference is
variational inference (VI), where the problem of Bayesian inference is cast as
an optimization problem -- namely, to maximize a lower bound of the log
marginal likelihood. This paves the way for a powerful and versatile framework,
where pseudo-training examples are treated as optimization arguments of the
approximate posterior that are jointly identified together with hyperparameters
of the generative model (i.e. prior and likelihood). The framework can
naturally handle a wide scope of supervised learning problems, ranging from
regression with heteroscedastic and non-Gaussian likelihoods to classification
problems with discrete labels, but also multilabel problems. The purpose of
this tutorial is to provide access to the basic matter for readers without
prior knowledge in both GPs and VI. A proper exposition to the subject enables
also access to more recent advances (like importance-weighted VI as well as
interdomain, multioutput and deep GPs) that can serve as an inspiration for new
research ideas.

    

### [[2101.02931] Block-Term Tensor Decomposition Model Selection and Computation: The Bayesian Way](http://arxiv.org/abs/2101.02931)


  The so-called block-term decomposition (BTD) tensor model, especially in its
rank-$(L_r,L_r,1)$ version, has been recently receiving increasing attention
due to its enhanced ability of representing systems and signals that are
composed of \emph{blocks} of rank higher than one, a scenario encountered in
numerous and diverse applications. Uniqueness conditions and fitting methods
have thus been thoroughly studied. Nevertheless, the challenging problem of
estimating the BTD model structure, namely the number of block terms, $R$, and
their individual ranks, $L_r$, has only recently started to attract significant
attention, mainly through regularization-based approaches which entail the need
to tune the regularization parameter(s). In this work, we build on ideas of
sparse Bayesian learning (SBL) and put forward a fully automated Bayesian
approach. Through a suitably crafted multi-level \emph{hierarchical}
probabilistic model, which gives rise to heavy-tailed prior distributions for
the BTD factors, structured sparsity is \emph{jointly} imposed. Ranks are then
estimated from the numbers of blocks ($R$) and columns ($L_r$) of
non-negligible energy. Approximate posterior inference is implemented, within
the variational inference framework. The resulting iterative algorithm
completely avoids hyperparameter tuning, which is a significant defect of
regularization-based methods. Alternative probabilistic models are also
explored and the connections with their regularization-based counterparts are
brought to light with the aid of the associated maximum a-posteriori (MAP)
estimators. We report simulation results with both synthetic and real-word
data, which demonstrate the merits of the proposed method in terms of both rank
estimation and model fitting as compared to state-of-the-art relevant methods.

    

### [[2102.02410] A Local Convergence Theory for Mildly Over-Parameterized Two-Layer Neural Network](http://arxiv.org/abs/2102.02410)


  While over-parameterization is widely believed to be crucial for the success
of optimization for the neural networks, most existing theories on
over-parameterization do not fully explain the reason -- they either work in
the Neural Tangent Kernel regime where neurons don't move much, or require an
enormous number of neurons. In practice, when the data is generated using a
teacher neural network, even mildly over-parameterized neural networks can
achieve 0 loss and recover the directions of teacher neurons. In this paper we
develop a local convergence theory for mildly over-parameterized two-layer
neural net. We show that as long as the loss is already lower than a threshold
(polynomial in relevant parameters), all student neurons in an
over-parameterized two-layer neural network will converge to one of teacher
neurons, and the loss will go to 0. Our result holds for any number of student
neurons as long as it is at least as large as the number of teacher neurons,
and our convergence rate is independent of the number of student neurons. A key
component of our analysis is the new characterization of local optimization
landscape -- we show the gradient satisfies a special case of Lojasiewicz
property which is different from local strong convexity or PL conditions used
in previous work.

    

### [[2102.06462] Two Sides of the Same Coin: Heterophily and Oversmoothing in Graph Convolutional Neural Networks](http://arxiv.org/abs/2102.06462)


  Most graph convolutional neural networks (GCNs) perform poorly in graphs
where neighbors typically have different features/classes (heterophily) and
when stacking multiple layers (oversmoothing). These two seemingly unrelated
problems have been studied independently, but there is recent empirical
evidence that solving one problem may benefit the other. In this work, going
beyond empirical observations, we aim to: (1) propose a new perspective to
analyze the heterophily and oversmoothing problems under a unified theoretical
framework, (2) identify the common causes of the two problems based on the
proposed framework, and (3) propose simple yet effective strategies that
address the common causes. Focusing on the node classification task, we use
linear separability of node representations as an indicator to reflect the
performance of GCNs and we propose to study the linear separability by
analyzing the statistical change of the node representations in the graph
convolution. We find that the relative degree of a node (compared to its
neighbors) and the heterophily level of a node's neighborhood are the root
causes that influence the separability of node representations. Our analysis
suggests that: (1) Nodes with high heterophily always produce less separable
representations after graph convolution; (2) Even with low heterophily, degree
disparity between nodes can influence the network dynamics and result in a
pseudo-heterophily situation, which helps to explain oversmoothing. Based on
our insights, we propose simple modifications to the GCN architecture -- i.e.,
degree corrections and signed messages -- which alleviate the root causes of
these issues, and also show this empirically on 9 real networks. Compared to
other approaches, which tend to work well in one regime but fail in others, our
modified GCN model consistently performs well across all settings.

    

### [[2102.07845] MARINA: Faster Non-Convex Distributed Learning with Compression](http://arxiv.org/abs/2102.07845)


  We develop and analyze MARINA: a new communication efficient method for
non-convex distributed learning over heterogeneous datasets. MARINA employs a
novel communication compression strategy based on the compression of gradient
differences that is reminiscent of but different from the strategy employed in
the DIANA method of Mishchenko et al. (2019). Unlike virtually all competing
distributed first-order methods, including DIANA, ours is based on a carefully
designed biased gradient estimator, which is the key to its superior
theoretical and practical performance. The communication complexity bounds we
prove for MARINA are evidently better than those of all previous first-order
methods. Further, we develop and analyze two variants of MARINA: VR-MARINA and
PP-MARINA. The first method is designed for the case when the local loss
functions owned by clients are either of a finite sum or of an expectation
form, and the second method allows for a partial participation of clients -- a
feature important in federated learning. All our methods are superior to
previous state-of-the-art methods in terms of oracle/communication complexity.
Finally, we provide a convergence analysis of all methods for problems
satisfying the Polyak-Lojasiewicz condition.

    

### [[2102.08201] Improper Reinforcement Learning with Gradient-based Policy Optimization](http://arxiv.org/abs/2102.08201)


  We consider an improper reinforcement learning setting where a learner is
given $M$ base controllers for an unknown Markov decision process, and wishes
to combine them optimally to produce a potentially new controller that can
outperform each of the base ones. This can be useful in tuning across
controllers, learnt possibly in mismatched or simulated environments, to obtain
a good controller for a given target environment with relatively few trials.
\par We propose a gradient-based approach that operates over a class of
improper mixtures of the controllers. We derive convergence rate guarantees for
the approach assuming access to a gradient oracle. The value function of the
mixture and its gradient may not be available in closed-form; however, we show
that we can employ rollouts and simultaneous perturbation stochastic
approximation (SPSA) for explicit gradient descent optimization. Numerical
results on (i) the standard control theoretic benchmark of stabilizing an
inverted pendulum and (ii) a constrained queueing task show that our improper
policy optimization algorithm can stabilize the system even when the base
policies at its disposal are unstable\footnote{Under review. Please do not
distribute.}.

    

### [[2102.08474] Adversarially Robust Kernel Smoothing](http://arxiv.org/abs/2102.08474)


  We propose the adversarially robust kernel smoothing (ARKS) algorithm,
combining kernel smoothing, robust optimization, and adversarial training for
robust learning. Our methods are motivated by the convex analysis perspective
of distributionally robust optimization based on probability metrics, such as
the Wasserstein distance and the maximum mean discrepancy. We adapt the
integral operator using supremal convolution in convex analysis to form a novel
function majorant used for enforcing robustness. Our method is simple in form
and applies to general loss functions and machine learning models. Furthermore,
we report experiments with general machine learning models, such as deep neural
networks, to demonstrate that ARKS performs competitively with the
state-of-the-art methods based on the Wasserstein distance.

    

### [[2102.12017] Annotating Motion Primitives for Simplifying Action Search in Reinforcement Learning](http://arxiv.org/abs/2102.12017)


  Reinforcement learning in large-scale environments is challenging due to the
many possible actions that can be taken in specific situations. We have
previously developed a means of constraining, and hence speeding up, the search
process through the use of motion primitives; motion primitives are sequences
of pre-specified actions taken across a state series. As a byproduct of this
work, we have found that if the motion primitives' motions and actions are
labeled, then the search can be sped up further. Since motion primitives may
initially lack such details, we propose a theoretically viewpoint-insensitive
and speed-insensitive means of automatically annotating the underlying motions
and actions. We do this through a differential-geometric, spatio-temporal
kinematics descriptor, which analyzes how the poses of entities in two motion
sequences change over time. We use this descriptor in conjunction with a
weighted-nearest-neighbor classifier to label the primitives using a limited
set of training examples. In our experiments, we achieve high motion and action
annotation rates for human-action-derived primitives with as few as one
training sample. We also demonstrate that reinforcement learning using
accurately labeled trajectories leads to high-performing policies more quickly
than standard reinforcement learning techniques. This is partly because motion
primitives encode prior domain knowledge and preempt the need to re-discover
that knowledge during training. It is also because agents can leverage the
labels to systematically ignore action classes that do not facilitate task
objectives, thereby reducing the action space.

    

### [[2102.12330] Re-Evaluating GermEval17 Using German Pre-Trained Language Models](http://arxiv.org/abs/2102.12330)


  The lack of a commonly used benchmark data set (collection) such as
(Super-)GLUE (Wang et al., 2018, 2019) for the evaluation of non-English
pre-trained language models is a severe shortcoming of current English-centric
NLP-research. It concentrates a large part of the research on English,
neglecting the uncertainty when transferring conclusions found for the English
language to other languages. We evaluate the performance of the German and
multilingual BERT-based models currently available via the huggingface
transformers library on the four tasks of the GermEval17 workshop. We compare
them to pre-BERT architectures (Wojatzki et al., 2017; Schmitt et al., 2018;
Attia et al., 2018) as well as to an ELMo-based architecture (Biesialska et
al., 2020) and a BERT-based approach (Guhr et al., 2020). The observed
improvements are put in relation to those for similar tasks and similar models
(pre-BERT vs. BERT-based) for the English language in order to draw tentative
conclusions about whether the observed improvements are transferable to German
or potentially other related languages.

    

### [[2103.04379] Repurposing GANs for One-shot Semantic Part Segmentation](http://arxiv.org/abs/2103.04379)


  While GANs have shown success in realistic image generation, the idea of
using GANs for other tasks unrelated to synthesis is underexplored. Do GANs
learn meaningful structural parts of objects during their attempt to reproduce
those objects? In this work, we test this hypothesis and propose a simple and
effective approach based on GANs for semantic part segmentation that requires
as few as one label example along with an unlabeled dataset. Our key idea is to
leverage a trained GAN to extract pixel-wise representation from the input
image and use it as feature vectors for a segmentation network. Our experiments
demonstrate that GANs representation is "readily discriminative" and produces
surprisingly good results that are comparable to those from supervised
baselines trained with significantly more labels. We believe this novel
repurposing of GANs underlies a new class of unsupervised representation
learning that is applicable to many other tasks. More results are available at
this https URL.

    

### [[2103.06326] S4RL: Surprisingly Simple Self-Supervision for Offline Reinforcement Learning](http://arxiv.org/abs/2103.06326)


  Offline reinforcement learning proposes to learn policies from large
collected datasets without interacting with the physical environment. These
algorithms have made it possible to learn useful skills from data that can then
be deployed in the environment in real-world settings where interactions may be
costly or dangerous, such as autonomous driving or factories. However, current
algorithms overfit to the dataset they are trained on and exhibit poor
out-of-distribution generalization to the environment when deployed. In this
paper, we study the effectiveness of performing data augmentations on the state
space, and study 7 different augmentation schemes and how they behave with
existing offline RL algorithms. We then combine the best data performing
augmentation scheme with a state-of-the-art Q-learning technique, and improve
the function approximation of the Q-networks by smoothening out the learned
state-action space. We experimentally show that using this Surprisingly Simple
Self-Supervision technique in RL (S4RL), we significantly improve over the
current state-of-the-art algorithms on offline robot learning environments such
as MetaWorld [1] and RoboSuite [2,3], and benchmark datasets such as D4RL [4].

    

### [[2103.13355] Bag of Tricks for Node Classification with Graph Neural Networks](http://arxiv.org/abs/2103.13355)


  Over the past few years, graph neural networks (GNN) and label
propagation-based methods have made significant progress in addressing node
classification tasks on graphs. However, in addition to their reliance on
elaborate architectures and algorithms, there are several key technical details
that are frequently overlooked, and yet nonetheless can play a vital role in
achieving satisfactory performance. In this paper, we first summarize a series
of existing tricks-of-the-trade, and then propose several new ones related to
label usage, loss function formulation, and model design that can significantly
improve various GNN architectures. We empirically evaluate their impact on
final node classification accuracy by conducting ablation studies and
demonstrate consistently-improved performance, often to an extent that
outweighs the gains from more dramatic changes in the underlying GNN
architecture. Notably, many of the top-ranked models on the Open Graph
Benchmark (OGB) leaderboard and KDDCUP 2021 Large-Scale Challenge MAG240M-LSC
benefit from these techniques.

    

### [[2103.15890] Learning Domain Invariant Representations for Generalizable Person Re-Identification](http://arxiv.org/abs/2103.15890)


  Generalizable person Re-Identification (ReID) has attracted growing attention
in recent computer vision community. In this work, we construct a structural
causal model among identity labels, identity-specific factors (clothes/shoes
color etc), and domain-specific factors (background, viewpoints etc). According
to the causal analysis, we propose a novel Domain Invariant Representation
Learning for generalizable person Re-Identification (DIR-ReID) framework.
Specifically, we first propose to disentangle the identity-specific and
domain-specific feature spaces, based on which we propose an effective
algorithmic implementation for backdoor adjustment, essentially serving as a
causal intervention towards the SCM. Extensive experiments have been conducted,
showing that DIR-ReID outperforms state-of-the-art methods on large-scale
domain generalization ReID benchmarks.

    

### [[2103.15990] An Overview of Human Activity Recognition Using Wearable Sensors: Healthcare and Artificial Intelligence](http://arxiv.org/abs/2103.15990)


  With the rapid development of the internet of things (IoT) and artificial
intelligence (AI) technologies, human activity recognition (HAR) has been
applied in a variety of domains such as security and surveillance, human-robot
interaction, and entertainment. Even though a number of surveys and review
papers have been published, there is a lack of HAR overview papers focusing on
healthcare applications that use wearable sensors. Therefore, we fill in the
gap by presenting this overview paper. In particular, we present our projects
to illustrate the system design of HAR applications for healthcare. Our
projects include early mobility identification of human activities for
intensive care unit (ICU) patients and gait analysis of Duchenne muscular
dystrophy (DMD) patients. We cover essential components of designing HAR
systems including sensor factors (e.g., type, number, and placement location),
AI model selection (e.g., classical machine learning models versus deep
learning models), and feature engineering. In addition, we highlight the
challenges of such healthcare-oriented HAR systems and propose several research
opportunities for both the medical and the computer science community.

    

### [[2103.16525] Endo-Depth-and-Motion: Reconstruction and Tracking in Endoscopic Videos using Depth Networks and Photometric Constraints](http://arxiv.org/abs/2103.16525)


  Estimating a scene reconstruction and the camera motion from in-body videos
is challenging due to several factors, e.g. the deformation of in-body cavities
or the lack of texture. In this paper we present Endo-Depth-and-Motion, a
pipeline that estimates the 6-degrees-of-freedom camera pose and dense 3D scene
models from monocular endoscopic videos. Our approach leverages recent advances
in self-supervised depth networks to generate pseudo-RGBD frames, then tracks
the camera pose using photometric residuals and fuses the registered depth maps
in a volumetric representation. We present an extensive experimental evaluation
in the public dataset Hamlyn, showing high-quality results and comparisons
against relevant baselines. We also release all models and code for future
comparisons.

    

### [[2104.07279] COVID-19 detection using deep convolutional neural networks and binary-differential-algorithm-based feature selection on X-ray images](http://arxiv.org/abs/2104.07279)


  The new Coronavirus is spreading rapidly, and it has taken the lives of many
people so far. The virus has destructive effects on the human lung, and early
detection is very important. Deep Convolution neural networks are such powerful
tools in classifying images. Therefore, in this paper, a hybrid approach based
on a deep network is presented. Feature vectors were extracted by applying a
deep convolution neural network on the images, and useful features were
selected by the binary differential meta-heuristic algorithm. These optimized
features were given to the SVM classifier. A database consisting of three
categories of images such as COVID-19, pneumonia, and healthy included in 1092
X-ray samples was considered. The proposed method achieved an accuracy of
99.43%, a sensitivity of 99.16%, and a specificity of 99.57%. Our results
demonstrate that the suggested approach is better than recent studies on
COVID-19 detection with X-ray images.

    

### [[2104.10314] Efficient Sparse Coding using Hierarchical Riemannian Pursuit](http://arxiv.org/abs/2104.10314)


  Sparse coding is a class of unsupervised methods for learning a sparse
representation of the input data in the form of a linear combination of a
dictionary and a sparse code. This learning framework has led to
state-of-the-art results in various image and video processing tasks. However,
classical methods learn the dictionary and the sparse code based on alternative
optimizations, usually without theoretical guarantees for either optimality or
convergence due to non-convexity of the problem. Recent works on sparse coding
with a complete dictionary provide strong theoretical guarantees thanks to the
development of the non-convex optimization. However, initial non-convex
approaches learn the dictionary in the sparse coding problem sequentially in an
atom-by-atom manner, which leads to a long execution time. More recent works
seek to directly learn the entire dictionary at once, which substantially
reduces the execution time. However, the associated recovery performance is
degraded with a finite number of data samples. In this paper, we propose an
efficient sparse coding scheme with a two-stage optimization. The proposed
scheme leverages the global and local Riemannian geometry of the two-stage
optimization problem and facilitates fast implementation for superb dictionary
recovery performance by a finite number of samples without atom-by-atom
calculation. We further prove that, with high probability, the proposed scheme
can exactly recover any atom in the target dictionary with a finite number of
samples if it is adopted to recover one atom of the dictionary. An application
on wireless sensor data compression is also proposed. Experiments on both
synthetic and real-world data verify the efficiency and effectiveness of the
proposed scheme.

    

### [[2104.11557] Knodle: Modular Weakly Supervised Learning with PyTorch](http://arxiv.org/abs/2104.11557)


  Strategies for improving the training and prediction quality of weakly
supervised machine learning models vary in how much they are tailored to a
specific task or integrated with a specific model architecture. In this work,
we introduce Knodle, a software framework that treats weak data annotations,
deep learning models, and methods for improving weakly supervised training as
separate, modular components. This modularization gives the training process
access to fine-grained information such as data set characteristics, matches of
heuristic rules, or elements of the deep learning model ultimately used for
prediction. Hence, our framework can encompass a wide range of training methods
for improving weak supervision, ranging from methods that only look at
correlations of rules and output classes (independently of the machine learning
model trained with the resulting labels), to those that harness the interplay
of neural networks and weakly labeled data. We illustrate the benchmarking
potential of the framework with a performance comparison of several reference
implementations on a selection of datasets that are already available in
Knodle.
The framework is published as an open-source Python package knodle and
available at this https URL.

    

### [[2104.11824] Optimal Dynamic Regret in Exp-Concave Online Learning](http://arxiv.org/abs/2104.11824)


  We consider the problem of the Zinkevich (2003)-style dynamic regret
minimization in online learning with exp-concave losses. We show that whenever
improper learning is allowed, a Strongly Adaptive online learner achieves the
dynamic regret of $\tilde O^*(n^{1/3}C_n^{2/3} \vee 1)$ where $C_n$ is the
total variation (a.k.a. path length) of the an arbitrary sequence of
comparators that may not be known to the learner ahead of time. Achieving this
rate was highly nontrivial even for squared losses in 1D where the best known
upper bound was $O(\sqrt{nC_n} \vee \log n)$ (Yuan and Lamperski, 2019). Our
new proof techniques make elegant use of the intricate structures of the primal
and dual variables imposed by the KKT conditions and could be of independent
interest. Finally, we apply our results to the classical statistical problem of
locally adaptive non-parametric regression (Mammen, 1991; Donoho and Johnstone,
1998) and obtain a stronger and more flexible algorithm that do not require any
statistical assumptions or any hyperparameter tuning.

    

### [[2104.12437] Towards Rigorous Interpretations: a Formalisation of Feature Attribution](http://arxiv.org/abs/2104.12437)


  Feature attribution is often loosely presented as the process of selecting a
subset of relevant features as a rationale of a prediction. Task-dependent by
nature, precise definitions of "relevance" encountered in the literature are
however not always consistent. This lack of clarity stems from the fact that we
usually do not have access to any notion of ground-truth attribution and from a
more general debate on what good interpretations are. In this paper we propose
to formalise feature selection/attribution based on the concept of relaxed
functional dependence. In particular, we extend our notions to the
instance-wise setting and derive necessary properties for candidate selection
solutions, while leaving room for task-dependence. By computing ground-truth
attributions on synthetic datasets, we evaluate many state-of-the-art
attribution methods and show that, even when optimised, some fail to verify the
proposed properties and provide wrong solutions.

    

### [[2105.00173] Emotion Recognition of the Singing Voice: Toward a Real-Time Analysis Tool for Singers](http://arxiv.org/abs/2105.00173)


  Current computational-emotion research has focused on applying acoustic
properties to analyze how emotions are perceived mathematically or used in
natural language processing machine learning models. While recent interest has
focused on analyzing emotions from the spoken voice, little experimentation has
been performed to discover how emotions are recognized in the singing voice --
both in noiseless and noisy data (i.e., data that is either inaccurate,
difficult to interpret, has corrupted/distorted/nonsense information like
actual noise sounds in this case, or has a low ratio of usable/unusable
information). Not only does this ignore the challenges of training machine
learning models on more subjective data and testing them with much noisier
data, but there is also a clear disconnect in progress between advancing the
development of convolutional neural networks and the goal of emotionally
cognizant artificial intelligence. By training a new model to include this type
of information with a rich comprehension of psycho-acoustic properties, not
only can models be trained to recognize information within extremely noisy
data, but advancement can be made toward more complex biofeedback applications
-- including creating a model which could recognize emotions given any human
information (language, breath, voice, body, posture) and be used in any
performance medium (music, speech, acting) or psychological assistance for
patients with disorders such as BPD, alexithymia, autism, among others. This
paper seeks to reflect and expand upon the findings of related research and
present a stepping-stone toward this end goal.

    

### [[2105.03075] A Survey of Data Augmentation Approaches for NLP](http://arxiv.org/abs/2105.03075)


  Data augmentation has recently seen increased interest in NLP due to more
work in low-resource domains, new tasks, and the popularity of large-scale
neural networks that require large amounts of training data. Despite this
recent upsurge, this area is still relatively underexplored, perhaps due to the
challenges posed by the discrete nature of language data. In this paper, we
present a comprehensive and unifying survey of data augmentation for NLP by
summarizing the literature in a structured manner. We first introduce and
motivate data augmentation for NLP, and then discuss major methodologically
representative approaches. Next, we highlight techniques that are used for
popular NLP applications and tasks. We conclude by outlining current challenges
and directions for future research. Overall, our paper aims to clarify the
landscape of existing literature in data augmentation for NLP and motivate
additional work in this area. We also present a GitHub repository with a paper
list that will be continuously updated at
this https URL


### [[2105.04030] A Bit More Bayesian: Domain-Invariant Learning with Uncertainty](http://arxiv.org/abs/2105.04030)


  Domain generalization is challenging due to the domain shift and the
uncertainty caused by the inaccessibility of target domain data. In this paper,
we address both challenges with a probabilistic framework based on variational
Bayesian inference, by incorporating uncertainty into neural network weights.
We couple domain invariance in a probabilistic formula with the variational
Bayesian inference. This enables us to explore domain-invariant learning in a
principled way. Specifically, we derive domain-invariant representations and
classifiers, which are jointly established in a two-layer Bayesian neural
network. We empirically demonstrate the effectiveness of our proposal on four
widely used cross-domain visual recognition benchmarks. Ablation studies
validate the synergistic benefits of our Bayesian treatment when jointly
learning domain-invariant representations and classifiers for domain
generalization. Further, our method consistently delivers state-of-the-art mean
accuracy on all benchmarks.

    

### [[2105.04963] Exploring a Handwriting Programming Language for Educational Robots](http://arxiv.org/abs/2105.04963)


  Recently, introducing computer science and educational robots in compulsory
education has received increasing attention. However, the use of screens in
classrooms is often met with resistance, especially in primary school. To
address this issue, this study presents the development of a handwriting-based
programming language for educational robots. Aiming to align better with
existing classroom practices, it allows students to program a robot by drawing
symbols with ordinary pens and paper. Regular smartphones are leveraged to
process the hand-drawn instructions using computer vision and machine learning
algorithms, and send the commands to the robot for execution. To align with the
local computer science curriculum, an appropriate playground and scaffolded
learning tasks were designed. The system was evaluated in a preliminary test
with eight teachers, developers and educational researchers. While the
participants pointed out that some technical aspects could be improved, they
also acknowledged the potential of the approach to make computer science
education in primary school more accessible.

    

### [[2105.07603] EasyFL: A Low-code Federated Learning Platform For Dummies](http://arxiv.org/abs/2105.07603)


  Academia and industry have developed several platforms to support the popular
privacy-preserving distributed learning method -- Federated Learning (FL).
However, these platforms are complex to use and require a deep understanding of
FL, which imposes high barriers to entry for beginners, limits the productivity
of researchers, and compromises deployment efficiency. In this paper, we
propose the first low-code FL platform, EasyFL, to enable users with various
levels of expertise to experiment and prototype FL applications with little
coding. We achieve this goal while ensuring great flexibility and extensibility
for customization by unifying simple API design, modular design, and granular
training flow abstraction. With only a few lines of code, EasyFL empowers them
with many out-of-the-box functionalities to accelerate experimentation and
deployment. These practical functionalities are heterogeneity simulation,
comprehensive tracking, distributed training optimization, and seamless
deployment. They are proposed based on challenges identified in the proposed FL
life cycle. Compared with other platforms, EasyFL not only requires just three
lines of code (at least 10x lesser) to build a vanilla FL application but also
incurs lower training overhead. Besides, our evaluations demonstrate that
EasyFL expedites distributed training by 1.5x. It also improves the efficiency
of deployment. We believe that EasyFL will increase the productivity of
researchers and democratize FL to wider audiences.

    

### [[2105.07830] Learning to Relate Depth and Semantics for Unsupervised Domain Adaptation](http://arxiv.org/abs/2105.07830)


  We present an approach for encoding visual task relationships to improve
model performance in an Unsupervised Domain Adaptation (UDA) setting. Semantic
segmentation and monocular depth estimation are shown to be complementary
tasks; in a multi-task learning setting, a proper encoding of their
relationships can further improve performance on both tasks. Motivated by this
observation, we propose a novel Cross-Task Relation Layer (CTRL), which encodes
task dependencies between the semantic and depth predictions. To capture the
cross-task relationships, we propose a neural network architecture that
contains task-specific and cross-task refinement heads. Furthermore, we propose
an Iterative Self-Learning (ISL) training scheme, which exploits semantic
pseudo-labels to provide extra supervision on the target domain. We
experimentally observe improvements in both tasks' performance because the
complementary information present in these tasks is better captured.
Specifically, we show that: (1) our approach improves performance on all tasks
when they are complementary and mutually dependent; (2) the CTRL helps to
improve both semantic segmentation and depth estimation tasks performance in
the challenging UDA setting; (3) the proposed ISL training scheme further
improves the semantic segmentation performance. The implementation is available
at this https URL.

    

### [[2105.09856] High-Fidelity and Low-Latency Universal Neural Vocoder based on Multiband WaveRNN with Data-Driven Linear Prediction for Discrete Waveform Modeling](http://arxiv.org/abs/2105.09856)


  This paper presents a novel high-fidelity and low-latency universal neural
vocoder framework based on multiband WaveRNN with data-driven linear prediction
for discrete waveform modeling (MWDLP). MWDLP employs a coarse-fine bit WaveRNN
architecture for 10-bit mu-law waveform modeling. A sparse gated recurrent unit
with a relatively large size of hidden units is utilized, while the multiband
modeling is deployed to achieve real-time low-latency usage. A novel technique
for data-driven linear prediction (LP) with discrete waveform modeling is
proposed, where the LP coefficients are estimated in a data-driven manner.
Moreover, a novel loss function using short-time Fourier transform (STFT) for
discrete waveform modeling with Gumbel approximation is also proposed. The
experimental results demonstrate that the proposed MWDLP framework generates
high-fidelity synthetic speech for seen and unseen speakers and/or language on
300 speakers training data including clean and noisy/reverberant conditions,
where the number of training utterances is limited to 60 per speaker, while
allowing for real-time low-latency processing using a single core of $\sim\!$
2.1--2.7 GHz CPU with $\sim\!$ 0.57--0.64 real-time factor including
input/output and feature extraction.

    

### [[2105.09858] Low-Latency Real-Time Non-Parallel Voice Conversion based on Cyclic Variational Autoencoder and Multiband WaveRNN with Data-Driven Linear Prediction](http://arxiv.org/abs/2105.09858)


  This paper presents a low-latency real-time (LLRT) non-parallel voice
conversion (VC) framework based on cyclic variational autoencoder (CycleVAE)
and multiband WaveRNN with data-driven linear prediction (MWDLP). CycleVAE is a
robust non-parallel multispeaker spectral model, which utilizes a
speaker-independent latent space and a speaker-dependent code to generate
reconstructed/converted spectral features given the spectral features of an
input speaker. On the other hand, MWDLP is an efficient and a high-quality
neural vocoder that can handle multispeaker data and generate speech waveform
for LLRT applications with CPU. To accommodate LLRT constraint with CPU, we
propose a novel CycleVAE framework that utilizes mel-spectrogram as spectral
features and is built with a sparse network architecture. Further, to improve
the modeling performance, we also propose a novel fine-tuning procedure that
refines the frame-rate CycleVAE network by utilizing the waveform loss from the
MWDLP network. The experimental results demonstrate that the proposed framework
achieves high-performance VC, while allowing for LLRT usage with a single-core
of $2.1$--$2.7$ GHz CPU on a real-time factor of $0.87$--$0.95$, including
input/output, feature extraction, on a frame shift of $10$ ms, a window length
of $27.5$ ms, and $2$ lookup frames.

    

### [[2105.12807] XOmiVAE: an interpretable deep learning model for cancer classification using high-dimensional omics data](http://arxiv.org/abs/2105.12807)


  The lack of explainability is one of the most prominent disadvantages of deep
learning applications in omics. This "black box" problem can undermine the
credibility and limit the practical implementation of biomedical deep learning
models. Here we present XOmiVAE, a variational autoencoder (VAE) based
interpretable deep learning model for cancer classification using
high-dimensional omics data. XOmiVAE is capable of revealing the contribution
of each gene and latent dimension for each classification prediction, and the
correlation between each gene and each latent dimension. It is also
demonstrated that XOmiVAE can explain not only the supervised classification
but the unsupervised clustering results from the deep learning network. To the
best of our knowledge, XOmiVAE is one of the first activation level-based
interpretable deep learning models explaining novel clusters generated by VAE.
The explainable results generated by XOmiVAE were validated by both the
performance of downstream tasks and the biomedical knowledge. In our
experiments, XOmiVAE explanations of deep learning based cancer classification
and clustering aligned with current domain knowledge including biological
annotation and academic literature, which shows great potential for novel
biomedical knowledge discovery from deep learning models.

    

### [[2105.13502] Unsupervised Domain Adaptation of Object Detectors: A Survey](http://arxiv.org/abs/2105.13502)


  Recent advances in deep learning have led to the development of accurate and
efficient models for various computer vision applications such as
classification, segmentation, and detection. However, learning highly accurate
models relies on the availability of large-scale annotated datasets. Due to
this, model performance drops drastically when evaluated on label-scarce
datasets having visually distinct images, termed as domain adaptation problem.
There is a plethora of works to adapt classification and segmentation models to
label-scarce target datasets through unsupervised domain adaptation.
Considering that detection is a fundamental task in computer vision, many
recent works have focused on developing novel domain adaptive detection
techniques. Here, we describe in detail the domain adaptation problem for
detection and present an extensive survey of the various methods. Furthermore,
we highlight strategies proposed and the associated shortcomings. Subsequently,
we identify multiple aspects of the problem that are most promising for future
research. We believe that this survey shall be valuable to the pattern
recognition experts working in the fields of computer vision, biometrics,
medical imaging, and autonomous navigation by introducing them to the problem,
and familiarizing them with the current status of the progress while providing
promising directions for future research.

    

### [[2105.13783] Quantile Encoder: Tackling High Cardinality Categorical Features in Regression Problems](http://arxiv.org/abs/2105.13783)


  Regression problems have been widely studied in machinelearning literature
resulting in a plethora of regression models and performance measures. However,
there are few techniques specially dedicated to solve the problem of how to
incorporate categorical features to regression problems. Usually, categorical
feature encoders are general enough to cover both classification and regression
problems. This lack of specificity results in underperforming regression
models. In this paper,we provide an in-depth analysis of how to tackle high
cardinality categor-ical features with the quantile. Our proposal outperforms
state-of-the-encoders, including the traditional statistical mean target
encoder, when considering the Mean Absolute Error, especially in the presence
of long-tailed or skewed distributions. Besides, to deal with possible
overfitting when there are categories with small support, our encoder benefits
from additive smoothing. Finally, we describe how to expand the encoded values
by creating a set of features with different quantiles. This expanded encoder
provides a more informative output about the categorical feature in question,
further boosting the performance of the regression model.

    

### [[2105.15134] Toward Understanding the Feature Learning Process of Self-supervised Contrastive Learning](http://arxiv.org/abs/2105.15134)


  How can neural networks trained by contrastive learning extract features from
the unlabeled data? Why does contrastive learning usually need much stronger
data augmentations than supervised learning to ensure good representations?
These questions involve both the optimization and statistical aspects of deep
learning, but can hardly be answered by analyzing supervised learning, where
the target functions are the highest pursuit. Indeed, in self-supervised
learning, it is inevitable to relate to the optimization/generalization of
neural networks to how they can encode the latent structures in the data, which
we refer to as the feature learning process.
In this work, we formally study how contrastive learning learns the feature
representations for neural networks by analyzing its feature learning process.
We consider the case where our data are comprised of two types of features: the
more semantically aligned sparse features which we want to learn from, and the
other dense features we want to avoid. Theoretically, we prove that contrastive
learning using $\mathbf{ReLU}$ networks provably learns the desired sparse
features if proper augmentations are adopted. We present an underlying
principle called $\textbf{feature decoupling}$ to explain the effects of
augmentations, where we theoretically characterize how augmentations can reduce
the correlations of dense features between positive samples while keeping the
correlations of sparse features intact, thereby forcing the neural networks to
learn from the self-supervision of sparse features. Empirically, we verified
that the feature decoupling principle matches the underlying mechanism of
contrastive learning in practice.

    

### [[2106.02994] Learning Topology from Synthetic Data for Unsupervised Depth Completion](http://arxiv.org/abs/2106.02994)


  We present a method for inferring dense depth maps from images and sparse
depth measurements by leveraging synthetic data to learn the association of
sparse point clouds with dense natural shapes, and using the image as evidence
to validate the predicted depth map. Our learned prior for natural shapes uses
only sparse depth as input, not images, so the method is not affected by the
covariate shift when attempting to transfer learned models from synthetic data
to real ones. This allows us to use abundant synthetic data with ground truth
to learn the most difficult component of the reconstruction process, which is
topology estimation, and use the image to refine the prediction based on
photometric evidence. Our approach uses fewer parameters than previous methods,
yet, achieves the state of the art on both indoor and outdoor benchmark
datasets. Code available at:
this https URL.

    

### [[2106.04679] Self-Adaptive Swarm System (SASS)](http://arxiv.org/abs/2106.04679)


  Distributed artificial intelligence (DAI) studies artificial intelligence
entities working together to reason, plan, solve problems, organize behaviors
and strategies, make collective decisions and learn. This Ph.D. research
proposes a principled Multi-Agent Systems (MAS) cooperation framework,
Self-Adaptive Swarm System (SASS), to bridge the fourth level automation gap
between perception, communication, planning, execution, decision-making, and
learning.

    

### [[2106.07250] Unified Interpretation of Softmax Cross-Entropy and Negative Sampling: With Case Study for Knowledge Graph Embedding](http://arxiv.org/abs/2106.07250)


  In knowledge graph embedding, the theoretical relationship between the
softmax cross-entropy and negative sampling loss functions has not been
investigated. This makes it difficult to fairly compare the results of the two
different loss functions. We attempted to solve this problem by using the
Bregman divergence to provide a unified interpretation of the softmax
cross-entropy and negative sampling loss functions. Under this interpretation,
we can derive theoretical findings for fair comparison. Experimental results on
the FB15k-237 and WN18RR datasets show that the theoretical findings are valid
in practical settings.

    

### [[2106.07474] Discovering Interpretable Machine Learning Models in Parallel Coordinates](http://arxiv.org/abs/2106.07474)


  This paper contributes to interpretable machine learning via visual knowledge
discovery in parallel coordinates. The concepts of hypercubes and hyper-blocks
are used as easily understandable by end-users in the visual form in parallel
coordinates. The Hyper algorithm for classification with mixed and pure
hyper-blocks (HBs) is proposed to discover hyper-blocks interactively and
automatically in individual, multiple, overlapping, and non-overlapping
setting. The combination of hyper-blocks with linguistic description of visual
patterns is presented too. It is shown that Hyper models generalize decision
trees. The Hyper algorithm was tested on the benchmark data from UCI ML
repository. It allowed discovering pure and mixed HBs with all data and then
with 10-fold cross validation. The links between hyper-blocks, dimension
reduction and visualization are established. Major benefits of hyper-block
technology and the Hyper algorithm are in their ability to discover and observe
hyper-blocks by end-users including side by side visualizations making patterns
visible for all classes. Another advantage of sets of HBs relative to the
decision trees is the ability to avoid both data overgeneralization and
overfitting.

    

### [[2106.07568] Full interpretable machine learning in 2D with inline coordinates](http://arxiv.org/abs/2106.07568)


  This paper proposed a new methodology for machine learning in 2-dimensional
space (2-D ML) in inline coordinates. It is a full machine learning approach
that does not require to deal with n-dimensional data in n-dimensional space.
It allows discovering n-D patterns in 2-D space without loss of n-D information
using graph representations of n-D data in 2-D. Specifically, it can be done
with the inline based coordinates in different modifications, including static
and dynamic ones. The classification and regression algorithms based on these
inline coordinates were introduced. A successful case study based on a
benchmark data demonstrated the feasibility of the approach. This approach
helps to consolidate further a whole new area of full 2-D machine learning as a
promising ML methodology. It has advantages of abilities to involve actively
the end-users into the discovering of models and their justification. Another
advantage is providing interpretable ML models.

    

### [[2106.08775] Momentum-inspired Low-Rank Coordinate Descent for Diagonally Constrained SDPs](http://arxiv.org/abs/2106.08775)


  We present a novel, practical, and provable approach for solving diagonally
constrained semi-definite programming (SDP) problems at scale using accelerated
non-convex programming. Our algorithm non-trivially combines acceleration
motions from convex optimization with coordinate power iteration and matrix
factorization techniques. The algorithm is extremely simple to implement, and
adds only a single extra hyperparameter -- momentum. We prove that our method
admits local linear convergence in the neighborhood of the optimum and always
converges to a first-order critical point. Experimentally, we showcase the
merits of our method on three major application domains: MaxCut, MaxSAT, and
MIMO signal detection. In all cases, our methodology provides significant
speedups over non-convex and convex SDP solvers -- 5X faster than
state-of-the-art non-convex solvers, and 9 to 10^3 X faster than convex SDP
solvers -- with comparable or improved solution quality.

    

### [[2106.09989] BinarizedAttack: Structural Poisoning Attacks to Graph-based Anomaly Detection](http://arxiv.org/abs/2106.09989)


  Graph-based Anomaly Detection (GAD) is becoming prevalent due to the powerful
representation abilities of graphs as well as recent advances in graph mining
techniques. These GAD tools, however, expose a new attacking surface,
ironically due to their unique advantage of being able to exploit the relations
among data. That is, attackers now can manipulate those relations (i.e., the
structure of the graph) to allow some target nodes to evade detection. In this
paper, we exploit this vulnerability by designing a new type of targeted
structural poisoning attacks to a representative regression-based GAD system
termed OddBall. Specially, we formulate the attack against OddBall as a
bi-level optimization problem, where the key technical challenge is to
efficiently solve the problem in a discrete domain. We propose a novel attack
method termed BinarizedAttack based on gradient descent. Comparing to prior
arts, BinarizedAttack can better use the gradient information, making it
particularly suitable for solving combinatorial optimization problems.
Furthermore, we investigate the attack transferability of BinarizedAttack by
employing it to attack other representation-learning-based GAD systems. Our
comprehensive experiments demonstrate that BinarizedAttack is very effective in
enabling target nodes to evade graph-based anomaly detection tools with limited
attackers' budget, and in the black-box transfer attack setting,
BinarizedAttack is also tested effective and in particular, can significantly
change the node embeddings learned by the GAD systems. Our research thus opens
the door to studying a new type of attack against security analytic tools that
rely on graph data.

    

### [[2106.10333] Non-parametric Differentially Private Confidence Intervals for the Median](http://arxiv.org/abs/2106.10333)


  Differential privacy is a restriction on data processing algorithms that
provides strong confidentiality guarantees for individual records in the data.
However, research on proper statistical inference, that is, research on
properly quantifying the uncertainty of the (noisy) sample estimate regarding
the true value in the population, is currently still limited. This paper
proposes and evaluates several strategies to compute valid differentially
private confidence intervals for the median. Instead of computing a
differentially private point estimate and deriving its uncertainty, we directly
estimate the interval bounds and discuss why this approach is superior if
ensuring privacy is important. We also illustrate that addressing both sources
of uncertainty--the error from sampling and the error from protecting the
output--simultaneously should be preferred over simpler approaches that
incorporate the uncertainty in a sequential fashion. We evaluate the
performance of the different algorithms under various parameter settings in
extensive simulation studies and demonstrate how the findings could be applied
in practical settings using data from the 1940 Decennial Census.

    

### [[2106.10417] Variance-Dependent Best Arm Identification](http://arxiv.org/abs/2106.10417)


  We study the problem of identifying the best arm in a stochastic multi-armed
bandit game. Given a set of $n$ arms indexed from $1$ to $n$, each arm $i$ is
associated with an unknown reward distribution supported on $[0,1]$ with mean
$\theta_i$ and variance $\sigma_i^2$. Assume $\theta_1 > \theta_2 \geq \cdots
\geq\theta_n$. We propose an adaptive algorithm which explores the gaps and
variances of the rewards of the arms and makes future decisions based on the
gathered information using a novel approach called \textit{grouped median
elimination}. The proposed algorithm guarantees to output the best arm with
probability $(1-\delta)$ and uses at most $O \left(\sum_{i = 1}^n
\left(\frac{\sigma_i^2}{\Delta_i^2} + \frac{1}{\Delta_i}\right)(\ln \delta^{-1}
+ \ln \ln \Delta_i^{-1})\right)$ samples, where $\Delta_i$ ($i \geq 2$) denotes
the reward gap between arm $i$ and the best arm and we define $\Delta_1 =
\Delta_2$. This achieves a significant advantage over the variance-independent
algorithms in some favorable scenarios and is the first result that removes the
extra $\ln n$ factor on the best arm compared with the state-of-the-art. We
further show that $\Omega \left( \sum_{i = 1}^n \left(
\frac{\sigma_i^2}{\Delta_i^2} + \frac{1}{\Delta_i} \right) \ln \delta^{-1}
\right)$ samples are necessary for an algorithm to achieve the same goal,
thereby illustrating that our algorithm is optimal up to doubly logarithmic
terms.

    

### [[2106.10558] Rayleigh-Gauss-Newton optimization with enhanced sampling for variational Monte Carlo](http://arxiv.org/abs/2106.10558)


  Variational Monte Carlo (VMC) is an approach for computing ground-state
wavefunctions that has recently become more powerful due to the introduction of
neural network-based wavefunction parametrizations. However, efficiently
training neural wavefunctions to converge to an energy minimum remains a
difficult problem. In this work, we analyze optimization and sampling methods
used in VMC and introduce alterations to improve their performance. First,
based on theoretical convergence analysis in a noiseless setting, we motivate a
new optimizer that we call the Rayleigh-Gauss-Newton method, which can improve
upon gradient descent and natural gradient descent to achieve superlinear
convergence with little added computational cost. Second, in order to realize
this favorable comparison in the presence of stochastic noise, we analyze the
effect of sampling error on VMC parameter updates and experimentally
demonstrate that it can be reduced by the parallel tempering method. In
particular, we demonstrate that RGN can be made robust to energy spikes that
occur when new regions of configuration space become available to the sampler
over the course of optimization. Finally, putting theory into practice, we
apply our enhanced optimization and sampling methods to the transverse-field
Ising and XXZ models on large lattices, yielding ground-state energy estimates
with remarkably high accuracy after just 200-500 parameter updates.

    

### [[2106.11160] Effects of boundary conditions in fully convolutional networks for learning spatio-temporal dynamics](http://arxiv.org/abs/2106.11160)


  Accurate modeling of boundary conditions is crucial in computational physics.
The ever increasing use of neural networks as surrogates for physics-related
problems calls for an improved understanding of boundary condition treatment,
and its influence on the network accuracy. In this paper, several strategies to
impose boundary conditions (namely padding, improved spatial context, and
explicit encoding of physical boundaries) are investigated in the context of
fully convolutional networks applied to recurrent tasks. These strategies are
evaluated on two spatio-temporal evolving problems modeled by partial
differential equations: the 2D propagation of acoustic waves (hyperbolic PDE)
and the heat equation (parabolic PDE). Results reveal a high sensitivity of
both accuracy and stability on the boundary implementation in such recurrent
tasks. It is then demonstrated that the choice of the optimal padding strategy
is directly linked to the data semantics. Furthermore, the inclusion of
additional input spatial context or explicit physics-based rules allows a
better handling of boundaries in particular for large number of recurrences,
resulting in more robust and stable neural networks, while facilitating the
design and versatility of such networks.

    

### [[2106.11655] On Constrained Optimization in Differentiable Neural Architecture Search](http://arxiv.org/abs/2106.11655)


  Differentiable Architecture Search (DARTS) is a recently proposed neural
architecture search (NAS) method based on a differentiable relaxation. Due to
its success, numerous variants analyzing and improving parts of the DARTS
framework have recently been proposed. By considering the problem as a
constrained bilevel optimization, we propose and analyze three improvements to
architectural weight competition, update scheduling, and regularization towards
discretization. First, we introduce a new approach to the activation of
architecture weights, which prevents confounding competition within an edge and
allows for fair comparison across edges to aid in discretization. Next, we
propose a dynamic schedule based on per-minibatch network information to make
architecture updates more informed. Finally, we consider two regularizations,
based on proximity to discretization and the Alternating Directions Method of
Multipliers (ADMM) algorithm, to promote early discretization. Our results show
that this new activation scheme reduces final architecture size and the
regularizations improve reliability in search results while maintaining
comparable performance to state-of-the-art in NAS, especially when used with
our new dynamic informed schedule.

    

### [[2106.11929] Physics-Informed Deep Reversible Regression Model for Temperature Field Reconstruction of Heat-Source Systems](http://arxiv.org/abs/2106.11929)


  Temperature monitoring during the life time of heat source components in
engineering systems becomes essential to guarantee the normal work and the
working life of these components. However, prior methods, which mainly use the
interpolate estimation to reconstruct the temperature field from limited
monitoring points, require large amounts of temperature tensors for an accurate
estimation. This may decrease the availability and reliability of the system
and sharply increase the monitoring cost. To solve this problem, this work
develops a novel physics-informed deep reversible regression models for
temperature field reconstruction of heat-source systems (TFR-HSS), which can
better reconstruct the temperature field with limited monitoring points
unsupervisedly. First, we define the TFR-HSS task mathematically, and
numerically model the task, and hence transform the task as an image-to-image
regression problem. Then this work develops the deep reversible regression
model which can better learn the physical information, especially over the
boundary. Finally, considering the physical characteristics of heat conduction
as well as the boundary conditions, this work proposes the physics-informed
reconstruction loss including four training losses and jointly learns the deep
surrogate model with these losses unsupervisedly. Experimental studies have
conducted over typical two-dimensional heat-source systems to demonstrate the
effectiveness of the proposed method.

    

### [[2106.12194] Uncertainty-Aware Model-Based Reinforcement Learning with Application to Autonomous Driving](http://arxiv.org/abs/2106.12194)


  To further improve the learning efficiency and performance of reinforcement
learning (RL), in this paper we propose a novel uncertainty-aware model-based
RL (UA-MBRL) framework, and then implement and validate it in autonomous
driving under various task scenarios. First, an action-conditioned ensemble
model with the ability of uncertainty assessment is established as the virtual
environment model. Then, a novel uncertainty-aware model-based RL framework is
developed based on the adaptive truncation approach, providing virtual
interactions between the agent and environment model, and improving RL's
training efficiency and performance. The developed algorithms are then
implemented in end-to-end autonomous vehicle control tasks, validated and
compared with state-of-the-art methods under various driving scenarios. The
validation results suggest that the proposed UA-MBRL method surpasses the
existing model-based and model-free RL approaches, in terms of learning
efficiency and achieved performance. The results also demonstrate the good
ability of the proposed method with respect to the adaptiveness and robustness,
under various autonomous driving scenarios.

    

### [[2106.13122] Exploring Corruption Robustness: Inductive Biases in Vision Transformers and MLP-Mixers](http://arxiv.org/abs/2106.13122)


  Recently, vision transformers and MLP-based models have been developed in
order to address some of the prevalent weaknesses in convolutional neural
networks. Due to the novelty of transformers being used in this domain along
with the self-attention mechanism, it remains unclear to what degree these
architectures are robust to corruptions. Despite some works proposing that data
augmentation remains essential for a model to be robust against corruptions, we
propose to explore the impact that the architecture has on corruption
robustness. We find that vision transformer architectures are inherently more
robust to corruptions than the ResNet-50 and MLP-Mixers. We also find that
vision transformers with 5 times fewer parameters than a ResNet-50 have more
shape bias. Our code is available to reproduce.

    

### [[2106.13884] Multimodal Few-Shot Learning with Frozen Language Models](http://arxiv.org/abs/2106.13884)


  When trained at sufficient scale, auto-regressive language models exhibit the
notable ability to learn a new language task after being prompted with just a
few examples. Here, we present a simple, yet effective, approach for
transferring this few-shot learning ability to a multimodal setting (vision and
language). Using aligned image and caption data, we train a vision encoder to
represent each image as a sequence of continuous embeddings, such that a
pre-trained, frozen language model prompted with this prefix generates the
appropriate caption. The resulting system is a multimodal few-shot learner,
with the surprising ability to learn a variety of new tasks when conditioned on
examples, represented as a sequence of multiple interleaved image and text
embeddings. We demonstrate that it can rapidly learn words for new objects and
novel visual categories, do visual question-answering with only a handful of
examples, and make use of outside knowledge, by measuring a single model on a
variety of established and new benchmarks.

    

### [[2106.14344] Non-Exhaustive Learning Using Gaussian Mixture Generative Adversarial Networks](http://arxiv.org/abs/2106.14344)


  Supervised learning, while deployed in real-life scenarios, often encounters
instances of unknown classes. Conventional algorithms for training a supervised
learning model do not provide an option to detect such instances, so they
miss-classify such instances with 100% probability. Open Set Recognition (OSR)
and Non-Exhaustive Learning (NEL) are potential solutions to overcome this
problem. Most existing methods of OSR first classify members of existing
classes and then identify instances of new classes. However, many of the
existing methods of OSR only makes a binary decision, i.e., they only identify
the existence of the unknown class. Hence, such methods cannot distinguish test
instances belonging to incremental unseen classes. On the other hand, the
majority of NEL methods often make a parametric assumption over the data
distribution, which either fail to return good results, due to the reason that
real-life complex datasets may not follow a well-known data distribution. In
this paper, we propose a new online non-exhaustive learning model, namely,
Non-Exhaustive Gaussian Mixture Generative Adversarial Networks (NE-GM-GAN) to
address these issues. Our proposed model synthesizes Gaussian mixture based
latent representation over a deep generative model, such as GAN, for
incremental detection of instances of emerging classes in the test data.
Extensive experimental results on several benchmark datasets show that
NE-GM-GAN significantly outperforms the state-of-the-art methods in detecting
instances of novel classes in streaming data.

    

### [[2106.14993] Modularity in Reinforcement Learning via Algorithmic Independence in Credit Assignment](http://arxiv.org/abs/2106.14993)


  Many transfer problems require re-using previously optimal decisions for
solving new tasks, which suggests the need for learning algorithms that can
modify the mechanisms for choosing certain actions independently of those for
choosing others. However, there is currently no formalism nor theory for how to
achieve this kind of modular credit assignment. To answer this question, we
define modular credit assignment as a constraint on minimizing the algorithmic
mutual information among feedback signals for different decisions. We introduce
what we call the modularity criterion for testing whether a learning algorithm
satisfies this constraint by performing causal analysis on the algorithm
itself. We generalize the recently proposed societal decision-making framework
as a more granular formalism than the Markov decision process to prove that for
decision sequences that do not contain cycles, certain single-step temporal
difference action-value methods meet this criterion while all policy-gradient
methods do not. Empirical evidence suggests that such action-value methods are
more sample efficient than policy-gradient methods on transfer problems that
require only sparse changes to a sequence of previously optimal decisions.

    

### [[2106.15338] Probabilistic Attention for Interactive Segmentation](http://arxiv.org/abs/2106.15338)


  We provide a probabilistic interpretation of attention and show that the
standard dot-product attention in transformers is a special case of Maximum A
Posteriori (MAP) inference. The proposed approach suggests the use of
Expectation Maximization algorithms for online adaptation of key and value
model parameters. This approach is useful for cases in which external agents,
e.g., annotators, provide inference-time information about the correct values
of some tokens, e.g, the semantic category of some pixels, and we need for this
new information to propagate to other tokens in a principled manner. We
illustrate the approach on an interactive semantic segmentation task in which
annotators and models collaborate online to improve annotation efficiency.
Using standard benchmarks, we observe that key adaptation boosts model
performance ($\sim10\%$ mIoU) in the low feedback regime and value propagation
improves model responsiveness in the high feedback regime. A PyTorch layer
implementation of our probabilistic attention model will be made publicly
available here: this https URL.

    

### [[2106.16036] A Generative Model for Raw Audio Using Transformer Architectures](http://arxiv.org/abs/2106.16036)


  This paper proposes a novel way of doing audio synthesis at the waveform
level using Transformer architectures. We propose a deep neural network for
generating waveforms, similar to wavenet \cite{oord2016wavenet}. This is fully
probabilistic, auto-regressive, and causal, i.e. each sample generated depends
only on the previously observed samples. Our approach outperforms a widely used
wavenet architecture by up to 9\% on a similar dataset for predicting the next
step. Using the attention mechanism, we enable the architecture to learn which
audio samples are important for the prediction of the future sample. We show
how causal transformer generative models can be used for raw waveform
synthesis. We also show that this performance can be improved by another 2\% by
conditioning samples over a wider context. The flexibility of the current model
to synthesize audio from latent representations suggests a large number of
potential applications. The novel approach of using generative transformer
architectures for raw audio synthesis is, however, still far away from
generating any meaningful music, without using latent codes/meta-data to aid
the generation process.

    

### [[2107.00946] Online Metro Origin-Destination Prediction via Heterogeneous Information Aggregation](http://arxiv.org/abs/2107.00946)


  Metro origin-destination prediction is a crucial yet challenging task for
intelligent transportation management, which aims to accurately forecast two
specific types of cross-station ridership, i.e., Origin-Destination (OD) one
and Destination-Origin (DO) one. However, complete OD matrices of previous time
intervals can not be obtained immediately in online metro systems, and
conventional methods only used limited information to forecast the future OD
and DO ridership separately. In this work, we proposed a novel neural network
module termed Heterogeneous Information Aggregation Machine (HIAM), which fully
exploits heterogeneous information of historical data (e.g., incomplete OD
matrices, unfinished order vectors, and DO matrices) to jointly learn the
evolutionary patterns of OD and DO ridership. Specifically, an OD modeling
branch estimates the potential destinations of unfinished orders explicitly to
complement the information of incomplete OD matrices, while a DO modeling
branch takes DO matrices as input to capture the spatial-temporal distribution
of DO ridership. Moreover, a Dual Information Transformer is introduced to
propagate the mutual information among OD features and DO features for modeling
the OD-DO causality and correlation. Based on the proposed HIAM, we develop a
unified Seq2Seq network to forecast the future OD and DO ridership
simultaneously. Extensive experiments conducted on two large-scale benchmarks
demonstrate the effectiveness of our method for online metro origin-destination
prediction.

    

### [[2107.01725] Real-time Detection and Adaptive Mitigation of Power-based Side-Channel Leakage in SoC](http://arxiv.org/abs/2107.01725)


  Power-based side-channel is a serious security threat to the System on Chip
(SoC). The secret information is leaked from the power profile of the system
while a cryptographic algorithm is running. The mitigation requires efforts
from both the software level and hardware level. Currently, there is no
comprehensive solution that can guarantee the whole complex system is free of
leakage and can generically protect all cryptographic algorithms. In this
paper, we propose a real-time leakage detection and mitigation system which
enables the system to monitor the side-channel leakage effects of the hardware.
Our proposed system has extensions that provide a real-time monitor of power
consumption, detection of side-channel leakage, and real-time adaptive
mitigation of detected side-channel leakage. Our proposed system is generic and
can protect any algorithm running on it.

    

### [[2107.01857] Versatile and concurrent FPGA-based architecture for practical quantum communication systems](http://arxiv.org/abs/2107.01857)


  This work presents a hardware and software architecture which can be used in
those systems that implement practical Quantum Key Distribution (QKD) and
Quantum Random Number Generation (QRNG) schemes. This architecture fully
exploits the capability of a System-on-a-Chip (SoC) which comprehends both a
Field Programmable Gate Array (FPGA) and a dual core CPU unit. By assigning the
time-related tasks to the FPGA and the management to the CPU, we built a
flexible system with optimized resource sharing on a commercial off-the-shelf
(COTS) evaluation board which includes a SoC. Furthermore, by changing the
dataflow direction, the versatile system architecture can be exploited as a QKD
transmitter, QKD receiver and QRNG control-acquiring unit. Finally, we
exploited the dual core functionality and realized a concurrent stream device
to implement a practical QKD transmitter where one core continuously receives
fresh data at a sustained rate from an external QRNG source while the other
operates with the FPGA to drive the qubits transmission to the QKD receiver.
The system was successfully tested on a long-term run proving its stability and
security. This demonstration paves the way towards a more secure QKD
implementation, with fully unconditional security as the QKD states are
entirely generated by a true random process and not by deterministic expansion
algorithms. Eventually, this enables the realization of a standalone quantum
transmitter, including both the random numbers and the qubits generation.

    

### [[2106.04205] Micro BTB: A High Performance and Lightweight Last-Level Branch Target Buffer for Servers](http://arxiv.org/abs/2106.04205)


  High-performance branch target buffers (BTBs) and the L1I cache are key to
high-performance front-end. Modern branch predictors are highly accurate, but
with an increase in code footprint in modern-day server workloads, BTB and L1I
misses are still frequent. Recent industry trend shows usage of large BTBs
(100s of KB per core) that provide performance closer to the ideal BTB along
with a decoupled front-end that provides efficient fetch-directed L1I
instruction prefetching. On the other hand, techniques proposed by academia,
like BTB prefetching and using retire order stream for learning, fail to
provide significant performance with modern-day processor cores that are deeper
and wider.
We solve the problem fundamentally by increasing the storage density of the
last-level BTB. We observe that not all branch instructions require a full
branch target address. Instead, we can store the branch target as a branch
offset, relative to the branch instruction. Using branch offset enables the BTB
to store multiple branches per entry. We reduce the BTB storage in half, but we
observe that it increases skewness in the BTB. We propose a skewed indexed and
compressed last-level BTB design called MicroBTB (MBTB) that stores multiple
branches per BTB entry. We evaluate MBTB on 100 industry-provided server
workloads. A 4K-entry MBTB provides 17.61% performance improvement compared to
an 8K-entry baseline BTB design with a storage savings of 47.5KB per core.

    

### [[2106.06433] FPGA-Based Near-Memory Acceleration of Modern Data-Intensive Applications](http://arxiv.org/abs/2106.06433)


  Modern data-intensive applications demand high computation capabilities with
strict power constraints. Unfortunately, such applications suffer from a
significant waste of both execution cycles and energy in current computing
systems due to the costly data movement between the computation units and the
memory units. Genome analysis and weather prediction are two examples of such
applications. Recent FPGAs couple a reconfigurable fabric with high-bandwidth
memory (HBM) to enable more efficient data movement and improve overall
performance and energy efficiency. This trend is an example of a paradigm shift
to near-memory computing. We leverage such an FPGA with high-bandwidth memory
(HBM) for improving the pre-alignment filtering step of genome analysis and
representative kernels from a weather prediction model. Our evaluation
demonstrates large speedups and energy savings over a high-end IBM POWER9
system and a conventional FPGA board with DDR4 memory. We conclude that
FPGA-based near-memory computing has the potential to alleviate the data
movement bottleneck for modern data-intensive applications.

    

### [[2107.01405] A Fuzzy Scheduling Strategy for Deadline-Based Workflow Applications in Uncertain Edge-Cloud Environments](http://arxiv.org/abs/2107.01405)


  Workflow scheduling is critical to performing many practical workflow
applications. Scheduling based on edge-cloud computing can help addressing the
high complexity of workflow applications, while decreasing the data
transmission delay. However, due to the nature of heterogeneous resources in
edge-cloud environments and the complicated data dependencies between the tasks
in such a workflow, significant challenges for workflow scheduling remain,
including the selection of an optimal tasks-servers solution from the possible
numerous combinations. Existing studies are mainly done subject to rigorous
conditions without fluctuations, ignoring the fact that workflow scheduling is
typically present in uncertain environments. In this study, we focus on
reducing the execution cost of workflow applications mainly caused by task
computation and data transmission, while satisfying the workflow deadline in
uncertain edge-cloud environments. The Triangular Fuzzy Numbers (TFNs) are
adopted to represent task processing time and data transferring time. A
cost-driven fuzzy scheduling strategy based on an Adaptive Discrete Particle
Swarm Optimization (ADPSO) algorithm is proposed, which is employed the
operators of Genetic Algorithm (GA). This strategy introduces the randomly
two-point crossover operator, neighborhood mutation operator, and adaptive
multipoint mutation operator of GA to effectively avoid converging on local
optima. The experimental results show that our strategy can effectively reduce
the workflow execution cost in uncertain edge-cloud environments, compared with
other benchmark solutions.

    

### [[2107.01542] The Semantics of Package Management via Event Structures](http://arxiv.org/abs/2107.01542)


  We propose an approach to the semantics of package management which relates
it to general event structures, well-known mathematical objects used in the
semantics of concurrent, nondeterministic systems. In this approach, the data
of a package repository is treated as a declarative specification of a
nondeterministic, concurrent program. We introduce a process calculus
corresponding to this data, and investigate its operational and categorical
semantics. Our hope is this lays the basis for further formal study of package
management in which the weight of existing tools can be brought to bear.

    

### [[2107.01600] ETHTID: Deployable Threshold Information Disclosure on Ethereum](http://arxiv.org/abs/2107.01600)


  We address the Threshold Information Disclosure (TID) problem on Ethereum: An
arbitrary number of users commit to the scheduled disclosure of their
individual messages recorded on the Ethereum blockchain if and only if all such
messages are disclosed. Before a disclosure, only the original sender of each
message should know its contents. To accomplish this, we task a small council
with executing a distributed generation and threshold sharing of an asymmetric
key pair. The public key can be used to encrypt messages which only become
readable once the threshold-shared decryption key is reconstructed at a
predefined point in time and recorded on-chain. With blockchains like Ethereum,
it is possible to coordinate such procedures and attach economic stakes to the
actions of participating individuals. In this paper, we present ETHTID, an
Ethereum smart contract application to coordinate Threshold Information
Disclosure. We base our implementation on ETHDKG [1], a smart contract
application for distributed key generation and threshold sharing, and adapt it
to fit our differing use case as well as add functionality to oversee a
scheduled reconstruction of the decryption key. For our main cost saving
optimisation, we show that the security of the underlying cryptographic scheme
is maintained. We evaluate how the execution costs depend on the size of the
council and the threshold and show that the presented protocol is deployable on
Ethereum with a council of more than 200 members with gas savings of 20-40%
compared to ETHDKG.

    

### [[2107.01735] Cloud Versus Local Processing in Distributed Networks](http://arxiv.org/abs/2107.01735)


  A method for evaluating the relative performance of local, cloud and combined
processing of divisible (i.e. partitionable) data loads is presented. It is
shown how to do this in the context of Amdahl's law. A single level (star)
network operating under each of three fundamental scheduling policies is used
as an example. Applications include mobile computing, cloud computing and
signature searching.

    

### [[2005.10435] Optimal Distributed Subsampling for Maximum Quasi-Likelihood Estimators with Massive Data](http://arxiv.org/abs/2005.10435)


  Nonuniform subsampling methods are effective to reduce computational burden
and maintain estimation efficiency for massive data. Existing methods mostly
focus on subsampling with replacement due to its high computational efficiency.
If the data volume is so large that nonuniform subsampling probabilities cannot
be calculated all at once, then subsampling with replacement is infeasible to
implement. This paper solves this problem using Poisson subsampling. We first
derive optimal Poisson subsampling probabilities in the context of
quasi-likelihood estimation under the A- and L-optimality criteria. For a
practically implementable algorithm with approximated optimal subsampling
probabilities, we establish the consistency and asymptotic normality of the
resultant estimators. To deal with the situation that the full data are stored
in different blocks or at multiple locations, we develop a distributed
subsampling framework, in which statistics are computed simultaneously on
smaller partitions of the full data. Asymptotic properties of the resultant
aggregated estimator are investigated. We illustrate and evaluate the proposed
strategies through numerical experiments on simulated and real data sets.

    

### [[2009.10593] The Ultimate DataFlow for Ultimate SuperComputers-on-a-Chip, for Scientific Computing, Geo Physics, Complex Mathematics, and Information Processing](http://arxiv.org/abs/2009.10593)


  This article starts from the assumption that near future 100BTransistor
SuperComputers-on-a-Chip will include N big multi-core processors, 1000N small
many-core processors, a TPU-like fixed-structure systolic array accelerator for
the most frequently used Machine Learning algorithms needed in bandwidth-bound
applications and a flexible-structure reprogrammable accelerator for less
frequently used Machine Learning algorithms needed in latency-critical
applications.

    

### [[2010.07541] Byzantine-Resilient Federated Learning with Heterogeneous Data Distribution](http://arxiv.org/abs/2010.07541)


  For mitigating Byzantine behaviors in federated learning (FL), most
state-of-the-art approaches, such as Bulyan, tend to leverage the similarity of
updates from the benign clients. However, in many practical FL scenarios, data
is non-IID across clients, thus the updates received from even the benign
clients are quite dissimilar. Hence, using similarity based methods result in
wasted opportunities to train a model from interesting non-IID data, and also
slower model convergence. We propose DiverseFL to overcome this challenge in
heterogeneous data distribution settings. Rather than comparing each client's
update with other client updates to detect Byzantine clients, DiverseFL
compares each client's update with a guiding update of that client. Any client
whose update diverges from its associated guiding update is then tagged as a
Byzantine node. The FL server in DiverseFL computes the guiding update in every
round for each client over a small sample of the client's local data that is
received only once before start of the training. However, sharing even a small
sample of client's data with the FL server can compromise client's data privacy
needs. To tackle this challenge, DiverseFL creates a Trusted Execution
Environment (TEE)-based enclave to receive each client's sample and to compute
its guiding updates. TEE provides a hardware assisted verification and
attestation to each client that its data is not leaked outside of TEE. Through
experiments involving neural networks, benchmark datasets and popular Byzantine
attacks, we demonstrate that DiverseFL not only performs Byzantine mitigation
quite effectively, it also almost matches the performance of OracleSGD, where
the server only aggregates the updates from the benign clients.

    

### [[2105.06571] Toward Real-time Analysis of Experimental Science Workloads on Geographically Distributed Supercomputers](http://arxiv.org/abs/2105.06571)


  Massive upgrades to science infrastructure are driving data velocities
upwards while stimulating adoption of increasingly data-intensive analytics.
While next-generation exascale supercomputers promise strong support for
I/O-intensive workflows, HPC remains largely untapped by live experiments,
because data transfers and disparate batch-queueing policies are prohibitive
when faced with scarce instrument time. To bridge this divide, we introduce
Balsam: a distributed orchestration platform enabling workflows at the edge to
securely and efficiently trigger analytics tasks across a user-managed
federation of HPC execution sites. We describe the architecture of the Balsam
service, which provides a workflow management API, and distributed sites that
provision resources and schedule scalable, fault-tolerant execution. We
demonstrate Balsam in efficiently scaling real-time analytics from two DOE
light sources simultaneously onto three supercomputers (Theta, Summit, and
Cori), while maintaining low overheads for on-demand computing, and providing a
Python library for seamless integration with existing ecosystems of data
analysis tools.

    

### [[2105.10798] On the Complexity and Parallel Implementation of Hensel's Lemma and Weierstrass Preparation](http://arxiv.org/abs/2105.10798)


  Hensel's lemma, combined with repeated applications of Weierstrass
preparation theorem, allows for the factorization of polynomials with
multivariate power series coefficients. We present a complexity analysis for
this method and leverage those results to guide the load-balancing of a
parallel implementation to concurrently update all factors. In particular, the
factorization creates a pipeline where the terms of degree k of the first
factor are computed simultaneously with the terms of degree k-1 of the second
factor, etc. An implementation challenge is the inherent irregularity of
computational work between factors, as our complexity analysis reveals.
Additional resource utilization and load-balancing is achieved through the
parallelization of Weierstrass preparation. Experimental results show the
efficacy of this mixed parallel scheme, achieving up to 9x parallel speedup on
12 cores.

    

### [[2105.13487] Multidimensional Byzantine Agreement in a Synchronous Setting](http://arxiv.org/abs/2105.13487)


  In this paper we will present the Multidimensional Byzantine Agreement (MBA)
Protocol, a leaderless Byzantine agreement protocol defined for complete and
synchronous networks that allows a network of nodes to reach consensus on a
vector of relevant information regarding a set of observed events.
The consensus process is carried out in parallel on each component, and the
output is a vector whose components are either values with wide agreement in
the network (even if no individual node agrees on every value) or a special
value $\bot$ that signals irreconcilable disagreement. The MBA Protocol is
probabilistic and its execution halts with probability 1, and the number of
steps necessary to halt follows a Bernoulli-like distribution.
The design combines a Multidimensional Graded Consensus and a
Multidimensional Binary Byzantine Agreement, the generalization to the
multidimensional case of two protocols by Micali and Feldman.
We prove the correctness and security of the protocol assuming a synchronous
network where less than a third of the nodes are malicious.

    

### [[2106.04979] Benchmarking the Nvidia GPU Lineage: From Early K80 to Modern A100 with Asynchronous Memory Transfers](http://arxiv.org/abs/2106.04979)


  For many, Graphics Processing Units (GPUs) provides a source of reliable
computing power. Recently, Nvidia introduced its 9th generation HPC-grade GPUs,
the Ampere 100, claiming significant performance improvements over previous
generations, particularly for AI-workloads, as well as introducing new
architectural features such as asynchronous data movement. But how well does
the A100 perform on non-AI benchmarks, and can we expect the A100 to deliver
the application improvements we have grown used to with previous GPU
generations? In this paper, we benchmark the A100 GPU and compare it to four
previous generations of GPUs, with particular focus on empirically quantifying
our derived performance expectations, and -- should those expectations be
undelivered -- investigate whether the introduced data-movement features can
offset any eventual loss in performance? We find that the A100 delivers less
performance increase than previous generations for the well-known Rodinia
benchmark suite; we show that some of these performance anomalies can be
remedied through clever use of the new data-movement features, which we
microbenchmark and demonstrate where (and more importantly, how) they should be
used.

    

### [[2106.05485] VaLiPro: Linear Programming Validator for Cluster Computing Systems](http://arxiv.org/abs/2106.05485)


  The article presents and evaluates a scalable algorithm for validating
solutions of linear programming problems on cluster computing systems. The main
idea of the method is to generate a regular set of points (validation set) on a
small-radius hypersphere centered at the point of the solution under
validation. The objective function is calculated for each point of the
validation set that belongs to the feasible region. If all these values are
less than or equal to the value of the objective function at the point under
validation, then this point is the correct solution. The parallel
implementation of the VaLiPro algorithm is performed in C++ through the
parallel BSF-skeleton, which encapsulates all aspects related to the MPI-based
parallelization of the program. We provide the results of large-scale
computational experiments on a cluster computing system to study the
scalability of the VaLiPro algorithm.

    

### [[2107.01248] Data Uncertainty Guided Noise-aware Preprocessing Of Fingerprints](http://arxiv.org/abs/2107.01248)


  The effectiveness of fingerprint-based authentication systems on good quality
fingerprints is established long back. However, the performance of standard
fingerprint matching systems on noisy and poor quality fingerprints is far from
satisfactory. Towards this, we propose a data uncertainty-based framework which
enables the state-of-the-art fingerprint preprocessing models to quantify noise
present in the input image and identify fingerprint regions with background
noise and poor ridge clarity. Quantification of noise helps the model two
folds: firstly, it makes the objective function adaptive to the noise in a
particular input fingerprint and consequently, helps to achieve robust
performance on noisy and distorted fingerprint regions. Secondly, it provides a
noise variance map which indicates noisy pixels in the input fingerprint image.
The predicted noise variance map enables the end-users to understand erroneous
predictions due to noise present in the input image. Extensive experimental
evaluation on 13 publicly available fingerprint databases, across different
architectural choices and two fingerprint processing tasks demonstrate
effectiveness of the proposed framework.

    

### [[2107.01361] Sensor-invariant Fingerprint ROI Segmentation Using Recurrent Adversarial Learning](http://arxiv.org/abs/2107.01361)


  A fingerprint region of interest (roi) segmentation algorithm is designed to
separate the foreground fingerprint from the background noise. All the learning
based state-of-the-art fingerprint roi segmentation algorithms proposed in the
literature are benchmarked on scenarios when both training and testing
databases consist of fingerprint images acquired from the same sensors.
However, when testing is conducted on a different sensor, the segmentation
performance obtained is often unsatisfactory. As a result, every time a new
fingerprint sensor is used for testing, the fingerprint roi segmentation model
needs to be re-trained with the fingerprint image acquired from the new sensor
and its corresponding manually marked ROI. Manually marking fingerprint ROI is
expensive because firstly, it is time consuming and more importantly, requires
domain expertise. In order to save the human effort in generating annotations
required by state-of-the-art, we propose a fingerprint roi segmentation model
which aligns the features of fingerprint images derived from the unseen sensor
such that they are similar to the ones obtained from the fingerprints whose
ground truth roi masks are available for training. Specifically, we propose a
recurrent adversarial learning based feature alignment network that helps the
fingerprint roi segmentation model to learn sensor-invariant features.
Consequently, sensor-invariant features learnt by the proposed roi segmentation
model help it to achieve improved segmentation performance on fingerprints
acquired from the new sensor. Experiments on publicly available FVC databases
demonstrate the efficacy of the proposed work.

    

### [[2107.01396] Demiguise Attack: Crafting Invisible Semantic Adversarial Perturbations with Perceptual Similarity](http://arxiv.org/abs/2107.01396)


  Deep neural networks (DNNs) have been found to be vulnerable to adversarial
examples. Adversarial examples are malicious images with visually imperceptible
perturbations. While these carefully crafted perturbations restricted with
tight $\Lp$ norm bounds are small, they are still easily perceivable by humans.
These perturbations also have limited success rates when attacking black-box
models or models with defenses like noise reduction filters. To solve these
problems, we propose Demiguise Attack, crafting ``unrestricted'' perturbations
with Perceptual Similarity. Specifically, we can create powerful and
photorealistic adversarial examples by manipulating semantic information based
on Perceptual Similarity. Adversarial examples we generate are friendly to the
human visual system (HVS), although the perturbations are of large magnitudes.
We extend widely-used attacks with our approach, enhancing adversarial
effectiveness impressively while contributing to imperceptibility. Extensive
experiments show that the proposed method not only outperforms various
state-of-the-art attacks in terms of fooling rate, transferability, and
robustness against defenses but can also improve attacks effectively. In
addition, we also notice that our implementation can simulate illumination and
contrast changes that occur in real-world scenarios, which will contribute to
exposing the blind spots of DNNs.

    

### [[2107.01428] Solving Infinite-Domain CSPs Using the Patchwork Property](http://arxiv.org/abs/2107.01428)


  The constraint satisfaction problem (CSP) has important applications in
computer science and AI. In particular, infinite-domain CSPs have been
intensively used in subareas of AI such as spatio-temporal reasoning. Since
constraint satisfaction is a computationally hard problem, much work has been
devoted to identifying restricted problems that are efficiently solvable. One
way of doing this is to restrict the interactions of variables and constraints,
and a highly successful approach is to bound the treewidth of the underlying
primal graph. Bodirsky & Dalmau [J. Comput. System. Sci. 79(1), 2013] and Huang
et al. [Artif. Intell. 195, 2013] proved that CSP$(\Gamma)$ can be solved in
$n^{f(w)}$ time (where $n$ is the size of the instance, $w$ is the treewidth of
the primal graph and $f$ is a computable function) for certain classes of
constraint languages $\Gamma$. We improve this bound to $f(w) \cdot n^{O(1)}$,
where the function $f$ only depends on the language $\Gamma$, for CSPs whose
basic relations have the patchwork property. Hence, such problems are
fixed-parameter tractable and our algorithm is asymptotically faster than the
previous ones. Additionally, our approach is not restricted to binary
constraints, so it is applicable to a strictly larger class of problems than
that of Huang et al. However, there exist natural problems that are covered by
Bodirsky & Dalmau's algorithm but not by ours, and we begin investigating ways
of generalising our results to larger families of languages. We also analyse
our algorithm with respect to its running time and show that it is optimal
(under the Exponential Time Hypothesis) for certain languages such as Allen's
Interval Algebra.

    

### [[2107.01429] QKSA: Quantum Knowledge Seeking Agent](http://arxiv.org/abs/2107.01429)


  In this article we present the motivation and the core thesis towards the
implementation of a Quantum Knowledge Seeking Agent (QKSA). QKSA is a general
reinforcement learning agent that can be used to model classical and quantum
dynamics. It merges ideas from universal artificial general intelligence,
constructor theory and genetic programming to build a robust and general
framework for testing the capabilities of the agent in a variety of
environments. It takes the artificial life (or, animat) path to artificial
general intelligence where a population of intelligent agents are instantiated
to explore valid ways of modelling the perceptions. The multiplicity and
survivability of the agents are defined by the fitness, with respect to the
explainability and predictability, of a resource-bounded computational model of
the environment. This general learning approach is then employed to model the
physics of an environment based on subjective observer states of the agents. A
specific case of quantum process tomography as a general modelling principle is
presented. The various background ideas and a baseline formalism are discussed
in this article which sets the groundwork for the implementations of the QKSA
that are currently in active development.

    

### [[2107.01462] Development of a Conversation State Recognition System](http://arxiv.org/abs/2107.01462)


  With the evolution of the concept of Speaker diarization using LSTM, it is
relatively easier to understand the speaker identities for specific segments of
input audio stream data than manually tagging the data. With such a concept, it
is highly desirable to consider the possibility of using the identified speaker
identities to aid in recognizing the Speaker States in a conversation. In this
study, the Markov Chains are used to identify and update the Speaker States for
the next conversations between the same set of speakers, to enable
identification of their states in the most natural and long conversations. The
model is based on several audio samples from natural conversations of three or
greater than three speakers in two datasets with overall total error
percentages for recognized states being lesser than or equal to 12%. The
findings imply that the proposed extension to the Speaker diarization is
effective to predict the states for a conversation.

    

### [[2107.01496] A Data-Driven Method for Recognizing Automated Negotiation Strategies](http://arxiv.org/abs/2107.01496)


  Understanding an opponent agent helps in negotiating with it. Existing works
on understanding opponents focus on preference modeling (or estimating the
opponent's utility function). An important but largely unexplored direction is
recognizing an opponent's negotiation strategy, which captures the opponent's
tactics, e.g., to be tough at the beginning but to concede toward the deadline.
Recognizing complex, state-of-the-art, negotiation strategies is extremely
challenging, and simple heuristics may not be adequate for this purpose. We
propose a novel data-driven approach for recognizing an opponent's s
negotiation strategy. Our approach includes a data generation method for an
agent to generate domain-independent sequences by negotiating with a variety of
opponents across domains, a feature engineering method for representing
negotiation data as time series with time-step features and overall features,
and a hybrid (recurrent neural network-based) deep learning method for
recognizing an opponent's strategy from the time series of bids. We perform
extensive experiments, spanning four problem scenarios, to demonstrate the
effectiveness of our approach.

    

### [[2107.01621] The Composability of Intermediate Values in Composable Inductive Programming](http://arxiv.org/abs/2107.01621)


  It is believed that mechanisms including intermediate values enable
composable inductive programming (CIP) to be used to produce software of any
size. We present the results of a study that investigated the relationships
between program size, the number of intermediate values and the number of test
cases used to specify programs using CIP. In the study 96,000 programs of
various sizes were randomly generated, decomposed into fragments and
transformed into test cases. The test cases were then used to regenerate new
versions of the original programs using Zoea. The results show linear
relationships between the number of intermediate values and regenerated program
size, and between the number of test cases and regenerated program size within
the size range studied. In addition, as program size increases there is
increasing scope for trading off the number of test cases against the number of
intermediate values and vice versa.

    

### [[2107.01654] Efficient Explanations for Knowledge Compilation Languages](http://arxiv.org/abs/2107.01654)


  Knowledge compilation (KC) languages find a growing number of practical uses,
including in Constraint Programming (CP) and in Machine Learning (ML). In most
applications, one natural question is how to explain the decisions made by
models represented by a KC language. This paper shows that for many of the best
known KC languages, well-known classes of explanations can be computed in
polynomial time. These classes include deterministic decomposable negation
normal form (d-DNNF), and so any KC language that is strictly less succinct
than d-DNNF. Furthermore, the paper also investigates the conditions under
which polynomial time computation of explanations can be extended to KC
languages more succinct than d-DNNF.

    

### [[2107.01656] IITP at WAT 2021: System description for English-Hindi Multimodal Translation Task](http://arxiv.org/abs/2107.01656)


  Neural Machine Translation (NMT) is a predominant machine translation
technology nowadays because of its end-to-end trainable flexibility. However,
NMT still struggles to translate properly in low-resource settings specifically
on distant language pairs. One way to overcome this is to use the information
from other modalities if available. The idea is that despite differences in
languages, both the source and target language speakers see the same thing and
the visual representation of both the source and target is the same, which can
positively assist the system. Multimodal information can help the NMT system to
improve the translation by removing ambiguity on some phrases or words. We
participate in the 8th Workshop on Asian Translation (WAT - 2021) for
English-Hindi multimodal translation task and achieve 42.47 and 37.50 BLEU
points for Evaluation and Challenge subset, respectively.

    

### [[2107.01667] Low Dimensional State Representation Learning with Robotics Priors in Continuous Action Spaces](http://arxiv.org/abs/2107.01667)


  Autonomous robots require high degrees of cognitive and motoric intelligence
to come into our everyday life. In non-structured environments and in the
presence of uncertainties, such degrees of intelligence are not easy to obtain.
Reinforcement learning algorithms have proven to be capable of solving
complicated robotics tasks in an end-to-end fashion without any need for
hand-crafted features or policies. Especially in the context of robotics, in
which the cost of real-world data is usually extremely high, reinforcement
learning solutions achieving high sample efficiency are needed. In this paper,
we propose a framework combining the learning of a low-dimensional state
representation, from high-dimensional observations coming from the robot's raw
sensory readings, with the learning of the optimal policy, given the learned
state representation. We evaluate our framework in the context of mobile robot
navigation in the case of continuous state and action spaces. Moreover, we
study the problem of transferring what learned in the simulated virtual
environment to the real robot without further retraining using real-world data
in the presence of visual and depth distractors, such as lighting changes and
moving obstacles.

    

### [[2107.01715] Improve Agents without Retraining: Parallel Tree Search with Off-Policy Correction](http://arxiv.org/abs/2107.01715)


  Tree Search (TS) is crucial to some of the most influential successes in
reinforcement learning. Here, we tackle two major challenges with TS that limit
its usability: \textit{distribution shift} and \textit{scalability}. We first
discover and analyze a counter-intuitive phenomenon: action selection through
TS and a pre-trained value function often leads to lower performance compared
to the original pre-trained agent, even when having access to the exact state
and reward in future steps. We show this is due to a distribution shift to
areas where value estimates are highly inaccurate and analyze this effect using
Extreme Value theory. To overcome this problem, we introduce a novel off-policy
correction term that accounts for the mismatch between the pre-trained value
and its corresponding TS policy by penalizing under-sampled trajectories. We
prove that our correction eliminates the above mismatch and bound the
probability of sub-optimal action selection. Our correction significantly
improves pre-trained Rainbow agents without any further training, often more
than doubling their scores on Atari games. Next, we address the scalability
issue given by the computational complexity of exhaustive TS that scales
exponentially with the tree depth. We introduce Batch-BFS: a GPU breadth-first
search that advances all nodes in each depth of the tree simultaneously.
Batch-BFS reduces runtime by two orders of magnitude and, beyond inference,
enables also training with TS of depths that were not feasible before. We train
DQN agents from scratch using TS and show improvement in several Atari games
compared to both the original DQN and the more advanced Rainbow.

    

### [[2107.01759] Learning Delaunay Triangulation using Self-attention and Domain Knowledge](http://arxiv.org/abs/2107.01759)


  Delaunay triangulation is a well-known geometric combinatorial optimization
problem with various applications. Many algorithms can generate Delaunay
triangulation given an input point set, but most are nontrivial algorithms
requiring an understanding of geometry or the performance of additional
geometric operations, such as the edge flip. Deep learning has been used to
solve various combinatorial optimization problems; however, generating Delaunay
triangulation based on deep learning remains a difficult problem, and very few
research has been conducted due to its complexity. In this paper, we propose a
novel deep-learning-based approach for learning Delaunay triangulation using a
new attention mechanism based on self-attention and domain knowledge. The
proposed model is designed such that the model efficiently learns
point-to-point relationships using self-attention in the encoder. In the
decoder, a new attention score function using domain knowledge is proposed to
provide a high penalty when the geometric requirement is not satisfied. The
strength of the proposed attention score function lies in its ability to extend
its application to solving other combinatorial optimization problems involving
geometry. When the proposed neural net model is well trained, it is simple and
efficient because it automatically predicts the Delaunay triangulation for an
input point set without requiring any additional geometric operations. We
conduct experiments to demonstrate the effectiveness of the proposed model and
conclude that it exhibits better performance compared with other
deep-learning-based approaches.

    

### [[2107.01776] Continual Contrastive Self-supervised Learning for Image Classification](http://arxiv.org/abs/2107.01776)


  For artificial learning systems, continual learning over time from a stream
of data is essential. The burgeoning studies on supervised continual learning
have achieved great progress, while the study of catastrophic forgetting in
unsupervised learning is still blank. Among unsupervised learning methods,
self-supervise learning method shows tremendous potential on visual
representation without any labeled data at scale. To improve the visual
representation of self-supervised learning, larger and more varied data is
needed. In the real world, unlabeled data is generated at all times. This
circumstance provides a huge advantage for the learning of the self-supervised
method. However, in the current paradigm, packing previous data and current
data together and training it again is a waste of time and resources. Thus, a
continual self-supervised learning method is badly needed. In this paper, we
make the first attempt to implement the continual contrastive self-supervised
learning by proposing a rehearsal method, which keeps a few exemplars from the
previous data. Instead of directly combining saved exemplars with the current
data set for training, we leverage self-supervised knowledge distillation to
transfer contrastive information among previous data to the current network by
mimicking similarity score distribution inferred by the old network over a set
of saved exemplars. Moreover, we build an extra sample queue to assist the
network to distinguish between previous and current data and prevent mutual
interference while learning their own feature representation. Experimental
results show that our method performs well on CIFAR100 and ImageNet-Sub.
Compared with self-supervised baselines, which learning tasks one by one
without taking any technique, we improve the image classification top-1
accuracy by 1.60% on CIFAR100 and 2.86% on ImageNet-Sub under 10 incremental
steps setting.

    

### [[2107.01829] A System for Traded Control Teleoperation of Manipulation Tasks using Intent Prediction from Hand Gestures](http://arxiv.org/abs/2107.01829)


  This paper presents a teleoperation system that includes robot perception and
intent prediction from hand gestures. The perception module identifies the
objects present in the robot workspace and the intent prediction module which
object the user likely wants to grasp. This architecture allows the approach to
rely on traded control instead of direct control: we use hand gestures to
specify the goal objects for a sequential manipulation task, the robot then
autonomously generates a grasping or a retrieving motion using trajectory
optimization. The perception module relies on the model-based tracker to
precisely track the 6D pose of the objects and makes use of a state of the art
learning-based object detection and segmentation method, to initialize the
tracker by automatically detecting objects in the scene. Goal objects are
identified from user hand gestures using a trained a multi-layer perceptron
classifier. After presenting all the components of the system and their
empirical evaluation, we present experimental results comparing our pipeline to
a direct traded control approach (i.e., one that does not use prediction) which
shows that using intent prediction allows to bring down the overall task
execution time.

    

### [[2107.01836] GraspME -- Grasp Manifold Estimator](http://arxiv.org/abs/2107.01836)


  In this paper, we introduce a Grasp Manifold Estimator (GraspME) to detect
grasp affordances for objects directly in 2D camera images. To perform
manipulation tasks autonomously it is crucial for robots to have such
graspability models of the surrounding objects. Grasp manifolds have the
advantage of providing continuously infinitely many grasps, which is not the
case when using other grasp representations such as predefined grasp points.
For instance, this property can be leveraged in motion optimization to define
goal sets as implicit surface constraints in the robot configuration space. In
this work, we restrict ourselves to the case of estimating possible
end-effector positions directly from 2D camera images. To this extend, we
define grasp manifolds via a set of key points and locate them in images using
a Mask R-CNN backbone. Using learned features allows generalizing to different
view angles, with potentially noisy images, and objects that were not part of
the training set. We rely on simulation data only and perform experiments on
simple and complex objects, including unseen ones. Our framework achieves an
inference speed of 11.5 fps on a GPU, an average precision for keypoint
estimation of 94.5% and a mean pixel distance of only 1.29. This shows that we
can estimate the objects very well via bounding boxes and segmentation masks as
well as approximate the correct grasp manifold's keypoint coordinates.

    

### [[2107.01867] Control of rough terrain vehicles using deep reinforcement learning](http://arxiv.org/abs/2107.01867)


  We explore the potential to control terrain vehicles using deep reinforcement
in scenarios where human operators and traditional control methods are
inadequate. This letter presents a controller that perceives, plans, and
successfully controls a 16-tonne forestry vehicle with two frame articulation
joints, six wheels, and their actively articulated suspensions to traverse
rough terrain. The carefully shaped reward signal promotes safe, environmental,
and efficient driving, which leads to the emergence of unprecedented driving
skills. We test learned skills in a virtual environment, including terrains
reconstructed from high-density laser scans of forest sites. The controller
displays the ability to handle obstructing obstacles, slopes up to 27$^\circ$,
and a variety of natural terrains, all with limited wheel slip, smooth, and
upright traversal with intelligent use of the active suspensions. The results
confirm that deep reinforcement learning has the potential to enhance control
of vehicles with complex dynamics and high-dimensional observation data
compared to human operators or traditional control methods, especially in rough
terrain.

    

### [[2107.01877] Faster-LTN: a neuro-symbolic, end-to-end object detection architecture](http://arxiv.org/abs/2107.01877)


  The detection of semantic relationships between objects represented in an
image is one of the fundamental challenges in image interpretation.
Neural-Symbolic techniques, such as Logic Tensor Networks (LTNs), allow the
combination of semantic knowledge representation and reasoning with the ability
to efficiently learn from examples typical of neural networks. We here propose
Faster-LTN, an object detector composed of a convolutional backbone and an LTN.
To the best of our knowledge, this is the first attempt to combine both
frameworks in an end-to-end training setting. This architecture is trained by
optimizing a grounded theory which combines labelled examples with prior
knowledge, in the form of logical axioms. Experimental comparisons show
competitive performance with respect to the traditional Faster R-CNN
architecture.

    

### [[2107.01903] SM-SGE: A Self-Supervised Multi-Scale Skeleton Graph Encoding Framework for Person Re-Identification](http://arxiv.org/abs/2107.01903)


  Person re-identification via 3D skeletons is an emerging topic with great
potential in security-critical applications. Existing methods typically learn
body and motion features from the body-joint trajectory, whereas they lack a
systematic way to model body structure and underlying relations of body
components beyond the scale of body joints. In this paper, we for the first
time propose a Self-supervised Multi-scale Skeleton Graph Encoding (SM-SGE)
framework that comprehensively models human body, component relations, and
skeleton dynamics from unlabeled skeleton graphs of various scales to learn an
effective skeleton representation for person Re-ID. Specifically, we first
devise multi-scale skeleton graphs with coarse-to-fine human body partitions,
which enables us to model body structure and skeleton dynamics at multiple
levels. Second, to mine inherent correlations between body components in
skeletal motion, we propose a multi-scale graph relation network to learn
structural relations between adjacent body-component nodes and collaborative
relations among nodes of different scales, so as to capture more discriminative
skeleton graph features. Last, we propose a novel multi-scale skeleton
reconstruction mechanism to enable our framework to encode skeleton dynamics
and high-level semantics from unlabeled skeleton graphs, which encourages
learning a discriminative skeleton representation for person Re-ID. Extensive
experiments show that SM-SGE outperforms most state-of-the-art skeleton-based
methods. We further demonstrate its effectiveness on 3D skeleton data estimated
from large-scale RGB videos. Our codes are open at
this https URL.

    

### [[2107.01905] Creating Unbiased Public Benchmark Datasets with Data Leakage Prevention for Predictive Process Monitoring](http://arxiv.org/abs/2107.01905)


  Advances in AI, and especially machine learning, are increasingly drawing
research interest and efforts towards predictive process monitoring, the
subfield of process mining (PM) that concerns predicting next events, process
outcomes and remaining execution times. Unfortunately, researchers use a
variety of datasets and ways to split them into training and test sets. The
documentation of these preprocessing steps is not always complete.
Consequently, research results are hard or even impossible to reproduce and to
compare between papers. At times, the use of non-public domain knowledge
further hampers the fair competition of ideas. Often the training and test sets
are not completely separated, a data leakage problem particular to predictive
process monitoring. Moreover, test sets usually suffer from bias in terms of
both the mix of case durations and the number of running cases. These obstacles
pose a challenge to the field's progress. The contribution of this paper is to
identify and demonstrate the importance of these obstacles and to propose
preprocessing steps to arrive at unbiased benchmark datasets in a principled
way, thus creating representative test sets without data leakage with the aim
of levelling the playing field, promoting open science and contributing to more
rapid progress in predictive process monitoring.

    

### [[2107.01915] Logic Locking at the Frontiers of Machine Learning: A Survey on Developments and Opportunities](http://arxiv.org/abs/2107.01915)


  In the past decade, a lot of progress has been made in the design and
evaluation of logic locking; a premier technique to safeguard the integrity of
integrated circuits throughout the electronics supply chain. However, the
widespread proliferation of machine learning has recently introduced a new
pathway to evaluating logic locking schemes. This paper summarizes the recent
developments in logic locking attacks and countermeasures at the frontiers of
contemporary machine learning models. Based on the presented work, the key
takeaways, opportunities, and challenges are highlighted to offer
recommendations for the design of next-generation logic locking.

    

### [[1805.03696] Bayeslands: A Bayesian inference approach for parameter uncertainty quantification in Badlands](http://arxiv.org/abs/1805.03696)


  Bayesian inference provides a rigorous methodology for estimation and
uncertainty quantification of parameters in geophysical forward models.
Badlands (basin and landscape dynamics model) is a landscape evolution model
that simulates topography development at various space and time scales.
Badlands consists of a number of geophysical parameters that needs estimation
with appropriate uncertainty quantification; given the observed present-day
ground truth such as surface topography and the stratigraphy of sediment
deposition through time. The inference of unknown parameters is challenging due
to the scarcity of data, sensitivity of the parameter setting and complexity of
the Badlands model. In this paper, we take a Bayesian approach to provide
inference using Markov chain Monte Carlo sampling (MCMC). We present
\textit{Bayeslands}; a Bayesian framework for Badlands that fuses information
obtained from complex forward models with observational data and prior
knowledge. As a proof-of-concept, we consider a synthetic and real-world
topography with two parameters for Bayeslands inference, namely precipitation
and erodibility. The results of the experiments show that Bayeslands yields a
promising distribution of the parameters. Moreover, we demonstrate the
challenge in sampling irregular and multi-modal posterior distributions using a
likelihood surface that has a range of sub-optimal modes.

    

### [[1806.02127] Addendum to "HTN Acting: A Formalism and an Algorithm"](http://arxiv.org/abs/1806.02127)


  Hierarchical Task Network (HTN) planning is a practical and efficient
approach to planning when the 'standard operating procedures' for a domain are
available. Like Belief-Desire-Intention (BDI) agent reasoning, HTN planning
performs hierarchical and context-based refinement of goals into subgoals and
basic actions. However, while HTN planners 'lookahead' over the consequences of
choosing one refinement over another, BDI agents interleave refinement with
acting. There has been renewed interest in making HTN planners behave more like
BDI agent systems, e.g. to have a unified representation for acting and
planning. However, past work on the subject has remained informal or
implementation-focused. This paper is a formal account of 'HTN acting', which
supports interleaved deliberation, acting, and failure recovery. We use the
syntax of the most general HTN planning formalism and build on its core
semantics, and we provide an algorithm which combines our new formalism with
the processing of exogenous events. We also study the properties of HTN acting
and its relation to HTN planning.

    

### [[2004.01274] Does Comma Selection Help To Cope With Local Optima](http://arxiv.org/abs/2004.01274)


  One hope when using non-elitism in evolutionary computation is that the
ability to abandon the current-best solution aids leaving local optima. To
improve our understanding of this mechanism, we perform a rigorous runtime
analysis of a basic non-elitist evolutionary algorithm (EA), the
$(\mu,\lambda)$ EA, on the most basic benchmark function with a local optimum,
the jump function. We prove that for all reasonable values of the parameters
and the problem, the expected runtime of the $(\mu,\lambda)$~EA is, apart from
lower order terms, at least as large as the expected runtime of its elitist
counterpart, the $(\mu+\lambda)$~EA (for which we conduct the first runtime
analysis on jump functions to allow this comparison). Consequently, the ability
of the $(\mu,\lambda)$~EA to leave local optima to inferior solutions does not
lead to a runtime advantage.
We complement this lower bound with an upper bound that, for broad ranges of
the parameters, is identical to our lower bound apart from lower order terms.
This is the first runtime result for a non-elitist algorithm on a multi-modal
problem that is tight apart from lower order terms.

    

### [[2007.03727] TripMD: Driving patterns investigation via Motif Analysis](http://arxiv.org/abs/2007.03727)


  Processing driving data and investigating driving behavior has been receiving
an increasing interest in the last decades, with applications ranging from car
insurance pricing to policy making. A common strategy to analyze driving
behavior is to study the maneuvers being performance by the driver. In this
paper, we propose TripMD, a system that extracts the most relevant driving
patterns from sensor recordings (such as acceleration) and provides a
visualization that allows for an easy investigation. Additionally, we test our
system using the UAH-DriveSet dataset, a publicly available naturalistic
driving dataset. We show that (1) our system can extract a rich number of
driving patterns from a single driver that are meaningful to understand driving
behaviors and (2) our system can be used to identify the driving behavior of an
unknown driver from a set of drivers whose behavior we know.

    

### [[2009.14715] Learning Rewards from Linguistic Feedback](http://arxiv.org/abs/2009.14715)


  We explore unconstrained natural language feedback as a learning signal for
artificial agents. Humans use rich and varied language to teach, yet most prior
work on interactive learning from language assumes a particular form of input
(e.g., commands). We propose a general framework which does not make this
assumption, using aspect-based sentiment analysis to decompose feedback into
sentiment about the features of a Markov decision process. We then perform an
analogue of inverse reinforcement learning, regressing the sentiment on the
features to infer the teacher's latent reward function. To evaluate our
approach, we first collect a corpus of teaching behavior in a cooperative task
where both teacher and learner are human. We implement three artificial
learners: sentiment-based "literal" and "pragmatic" models, and an inference
network trained end-to-end to predict latent rewards. We then repeat our
initial experiment and pair them with human teachers. All three successfully
learn from interactive human feedback. The sentiment models outperform the
inference network, with the "pragmatic" model approaching human performance.
Our work thus provides insight into the information structure of naturalistic
linguistic feedback as well as methods to leverage it for reinforcement
learning.

    

### [[2101.00376] RiddleSense: Reasoning about Riddle Questions Featuring Linguistic Creativity and Commonsense Knowledge](http://arxiv.org/abs/2101.00376)


  Question: I have five fingers but I am not alive. What am I? Answer: a glove.
Answering such a riddle-style question is a challenging cognitive process, in
that it requires complex commonsense reasoning abilities, an understanding of
figurative language, and counterfactual reasoning skills, which are all
important abilities for advanced natural language understanding (NLU). However,
there are currently no dedicated datasets aiming to test these abilities.
Herein, we present RiddleSense, a new multiple-choice question answering task,
which comes with the first large dataset (5.7k examples) for answering
riddle-style commonsense questions. We systematically evaluate a wide range of
models over the challenge, and point out that there is a large gap between the
best-supervised model and human performance -- suggesting intriguing future
research in the direction of higher-order commonsense reasoning and linguistic
creativity towards building advanced NLU systems.

    

### [[2102.00621] Polyphone Disambiguition in Mandarin Chinese with Semi-Supervised Learning](http://arxiv.org/abs/2102.00621)


  The majority of Chinese characters are monophonic, while a special group of
characters, called polyphonic characters, have multiple pronunciations. As a
prerequisite of performing speech-related generative tasks, the correct
pronunciation must be identified among several candidates. This process is
called Polyphone Disambiguation. Although the problem has been well explored
with both knowledge-based and learning-based approaches, it remains challenging
due to the lack of publicly available labeled datasets and the irregular nature
of polyphone in Mandarin Chinese. In this paper, we propose a novel
semi-supervised learning (SSL) framework for Mandarin Chinese polyphone
disambiguation that can potentially leverage unlimited unlabeled text data. We
explore the effect of various proxy labeling strategies including
entropy-thresholding and lexicon-based labeling. Qualitative and quantitative
experiments demonstrate that our method achieves state-of-the-art performance.
In addition, we publish a novel dataset specifically for the polyphone
disambiguation task to promote further researches.

    

### [[2102.12010] PixSet : An Opportunity for 3D Computer Vision to Go Beyond Point Clouds With a Full-Waveform LiDAR Dataset](http://arxiv.org/abs/2102.12010)


  Leddar PixSet is a new publicly available dataset (this http URL)
for autonomous driving research and development. One key novelty of this
dataset is the presence of full-waveform data from the Leddar Pixell sensor, a
solid-state flash LiDAR. Full-waveform data has been shown to improve the
performance of perception algorithms in airborne applications but is yet to be
demonstrated for terrestrial applications such as autonomous driving. The
PixSet dataset contains approximately 29k frames from 97 sequences recorded in
high-density urban areas, using a set of various sensors (cameras, LiDARs,
radar, IMU, etc.) Each frame has been manually annotated with 3D bounding
boxes.

    

### [[2103.00112] Transformer in Transformer](http://arxiv.org/abs/2103.00112)


  Transformer is a new kind of neural architecture which encodes the input data
as powerful features via the attention mechanism. Basically, the visual
transformers first divide the input images into several local patches and then
calculate both representations and their relationship. Since natural images are
of high complexity with abundant detail and color information, the granularity
of the patch dividing is not fine enough for excavating features of objects in
different scales and locations. In this paper, we point out that the attention
inside these local patches are also essential for building visual transformers
with high performance and we explore a new architecture, namely, Transformer iN
Transformer (TNT). Specifically, we regard the local patches (e.g.,
16$\times$16) as "visual sentences" and present to further divide them into
smaller patches (e.g., 4$\times$4) as "visual words". The attention of each
word will be calculated with other words in the given visual sentence with
negligible computational costs. Features of both words and sentences will be
aggregated to enhance the representation ability. Experiments on several
benchmarks demonstrate the effectiveness of the proposed TNT architecture,
e.g., we achieve an $81.5%$ top-1 accuracy on the ImageNet, which is about
$1.7%$ higher than that of the state-of-the-art visual transformer with similar
computational cost. The PyTorch code is available at
this https URL, and the
MindSpore code is at
this https URL.

    

### [[2103.11811] MasakhaNER: Named Entity Recognition for African Languages](http://arxiv.org/abs/2103.11811)


  We take a step towards addressing the under-representation of the African
continent in NLP research by creating the first large publicly available
high-quality dataset for named entity recognition (NER) in ten African
languages, bringing together a variety of stakeholders. We detail
characteristics of the languages to help researchers understand the challenges
that these languages pose for NER. We analyze our datasets and conduct an
extensive empirical evaluation of state-of-the-art methods across both
supervised and transfer learning settings. We release the data, code, and
models in order to inspire future research on African NLP.

    

### [[2105.11763] Efficiently Explaining CSPs with Unsatisfiable Subset Optimization](http://arxiv.org/abs/2105.11763)


  We build on a recently proposed method for explaining solutions of constraint
satisfaction problems. An explanation here is a sequence of simple inference
steps, where the simplicity of an inference step is measured by the number and
types of constraints and facts used, and where the sequence explains all
logical consequences of the problem. We build on these formal foundations and
tackle two emerging questions, namely how to generate explanations that are
provably optimal (with respect to the given cost metric) and how to generate
them efficiently. To answer these questions, we develop 1) an implicit hitting
set algorithm for finding optimal unsatisfiable subsets; 2) a method to reduce
multiple calls for (optimal) unsatisfiable subsets to a single call that takes
constraints on the subset into account, and 3) a method for re-using relevant
information over multiple calls to these algorithms. The method is also
applicable to other problems that require finding cost-optimal unsatiable
subsets. We specifically show that this approach can be used to effectively
find sequences of optimal explanation steps for constraint satisfaction
problems like logic grid puzzles.

    

### [[2105.12846] General Game Heuristic Prediction Based on Ludeme Descriptions](http://arxiv.org/abs/2105.12846)


  This paper investigates the performance of different general-game-playing
heuristics for games in the Ludii general game system. Based on these results,
we train several regression learning models to predict the performance of these
heuristics based on each game's description file. We also provide a condensed
analysis of the games available in Ludii, and the different ludemes that define
them.

    

### [[2106.03706] A Comprehensive Assessment of Dialog Evaluation Metrics](http://arxiv.org/abs/2106.03706)


  Automatic evaluation metrics are a crucial component of dialog systems
research. Standard language evaluation metrics are known to be ineffective for
evaluating dialog. As such, recent research has proposed a number of novel,
dialog-specific metrics that correlate better with human judgements. Due to the
fast pace of research, many of these metrics have been assessed on different
datasets and there has as yet been no time for a systematic comparison between
them. To this end, this paper provides a comprehensive assessment of recently
proposed dialog evaluation metrics on a number of datasets. In this paper, 23
different automatic evaluation metrics are evaluated on 10 different datasets.
Furthermore, the metrics are assessed in different settings, to better qualify
their respective strengths and weaknesses. Metrics are assessed (1) on both the
turn level and the dialog level, (2) for different dialog lengths, (3) for
different dialog qualities (e.g., coherence, engaging), (4) for different types
of response generation models (i.e., generative, retrieval, simple models and
state-of-the-art models), (5) taking into account the similarity of different
metrics and (6) exploring combinations of different metrics. This comprehensive
assessment offers several takeaways pertaining to dialog evaluation metrics in
general. It also suggests how to best assess evaluation metrics and indicates
promising directions for future work.

    

### [[2106.06944] SASICM A Multi-Task Benchmark For Subtext Recognition](http://arxiv.org/abs/2106.06944)


  Subtext is a kind of deep semantics which can be acquired after one or more
rounds of expression transformation. As a popular way of expressing one's
intentions, it is well worth studying. In this paper, we try to make computers
understand whether there is a subtext by means of machine learning. We build a
Chinese dataset whose source data comes from the popular social media (e.g.
Weibo, Netease Music, Zhihu, and Bilibili). In addition, we also build a
baseline model called SASICM to deal with subtext recognition. The F1 score of
SASICMg, whose pretrained model is GloVe, is as high as 64.37%, which is 3.97%
higher than that of BERT based model, 12.7% higher than that of traditional
methods on average, including support vector machine, logistic regression
classifier, maximum entropy classifier, naive bayes classifier and decision
tree and 2.39% higher than that of the state-of-the-art, including MARIN and
BTM. The F1 score of SASICMBERT, whose pretrained model is BERT, is 65.12%,
which is 0.75% higher than that of SASICMg. The accuracy rates of SASICMg and
SASICMBERT are 71.16% and 70.76%, respectively, which can compete with those of
other methods which are mentioned before.

    

### [[2106.07932] Medical Code Prediction from Discharge Summary: Document to Sequence BERT using Sequence Attention](http://arxiv.org/abs/2106.07932)


  Clinical notes are unstructured text generated by clinicians during patient
encounters. Clinical notes are usually accompanied by a set of metadata codes
from the International Classification of Diseases(ICD). ICD code is an
important code used in various operations, including insurance, reimbursement,
medical diagnosis, etc. Therefore, it is important to classify ICD codes
quickly and accurately. However, annotating these codes is costly and
time-consuming. So we propose a model based on bidirectional encoder
representations from transformers (BERT) using the sequence attention method
for automatic ICD code assignment. We evaluate our approach on the medical
information mart for intensive care III (MIMIC-III) benchmark dataset. Our
model achieved performance of macro-averaged F1: 0.62898 and micro-averaged F1:
0.68555 and is performing better than a performance of the state-of-the-art
model using the MIMIC-III dataset. The contribution of this study proposes a
method of using BERT that can be applied to documents and a sequence attention
method that can capture important sequence in-formation appearing in documents.

    

### [[2106.08253] Code Generation Based on Deep Learning: a Brief Review](http://arxiv.org/abs/2106.08253)


  Automatic software development has been a research hot spot in the field of
software engineering (SE) in the past decade. In particular, deep learning (DL)
has been applied and achieved a lot of progress in various SE tasks. Among all
applications, automatic code generation by machines as a general concept,
including code completion and code synthesis, is a common expectation in the
field of SE, which may greatly reduce the development burden of the software
developers and improves the efficiency and quality of the software development
process to a certain extent. Code completion is an important part of modern
integrated development environments (IDEs). Code completion technology
effectively helps programmers complete code class names, method names, and
key-words, etc., which improves the efficiency of program development and
reduces spelling errors in the coding process. Such tools use static analysis
on the code and provide candidates for completion arranged in alphabetical
order. Code synthesis is implemented from two aspects, one based on
input-output samples and the other based on functionality description. In this
study, we introduce existing techniques of these two aspects and the
corresponding DL techniques, and present some possible future research
directions.

    

### [[2106.09455] Conference proceedings KI4Industry AI for SMEs -- the online congress for practical entry into AI for SMEs](http://arxiv.org/abs/2106.09455)


  The Institute of Materials and Processes, IMP, of the University of Applied
Sciences in Karlsruhe, Germany in cooperation with VDI Verein Deutscher
Ingenieure e.V, AEN Automotive Engineering Network and their cooperation
partners present their competences of AI-based solution approaches in the
production engineering field. The online congress KI 4 Industry on November 12
and 13, 2020, showed what opportunities the use of artificial intelligence
offers for medium-sized manufacturing companies, SMEs, and where potential
fields of application lie. The main purpose of KI 4 Industry is to increase the
transfer of knowledge, research and technology from universities to small and
medium-sized enterprises, to demystify the term AI and to encourage companies
to use AI-based solutions in their own value chain or in their products.

    

### [[2106.13367] SeaNet -- Towards A Knowledge Graph Based Autonomic Management of Software Defined Networks](http://arxiv.org/abs/2106.13367)


  Automatic network management driven by Artificial Intelligent technologies
has been heatedly discussed over decades. However, current reports mainly focus
on theoretic proposals and architecture designs, works on practical
implementations on real-life networks are yet to appear. This paper proposes
our effort toward the implementation of knowledge graph driven approach for
autonomic network management in software defined networks (SDNs), termed as
SeaNet. Driven by the ToCo ontology, SeaNet is reprogrammed based on Mininet (a
SDN emulator). It consists three core components, a knowledge graph generator,
a SPARQL engine, and a network management API. The knowledge graph generator
represents the knowledge in the telecommunication network management tasks into
formally represented ontology driven model. Expert experience and network
management rules can be formalized into knowledge graph and by automatically
inferenced by SPARQL engine, Network management API is able to packet
technology-specific details and expose technology-independent interfaces to
users. The Experiments are carried out to evaluate proposed work by comparing
with a commercial SDN controller Ryu implemented by the same language Python.
The evaluation results show that SeaNet is considerably faster in most
circumstances than Ryu and the SeaNet code is significantly more compact.
Benefit from RDF reasoning, SeaNet is able to achieve O(1) time complexity on
different scales of the knowledge graph while the traditional database can
achieve O(nlogn) at its best. With the developed network management API, SeaNet
enables researchers to develop semantic-intelligent applications on their own
SDNs.

    

### [[2106.15877] Experience-Driven PCG via Reinforcement Learning: A Super Mario Bros Study](http://arxiv.org/abs/2106.15877)


  We introduce a procedural content generation (PCG) framework at the
intersections of experience-driven PCG and PCG via reinforcement learning,
named ED(PCG)RL, EDRL in short. EDRL is able to teach RL designers to generate
endless playable levels in an online manner while respecting particular
experiences for the player as designed in the form of reward functions. The
framework is tested initially in the Super Mario Bros game. In particular, the
RL designers of Super Mario Bros generate and concatenate level segments while
considering the diversity among the segments. The correctness of the generation
is ensured by a neural net-assisted evolutionary level repairer and the
playability of the whole level is determined through AI-based testing. Our
agents in this EDRL implementation learn to maximise a quantification of
Koster's principle of fun by moderating the degree of diversity across level
segments. Moreover, we test their ability to design fun levels that are diverse
over time and playable. Our proposed framework is capable of generating
endless, playable Super Mario Bros levels with varying degrees of fun,
deviation from earlier segments, and playability. EDRL can be generalised to
any game that is built as a segment-based sequential process and features a
built-in compressed representation of its game content.

    

### [[2107.01183] Ethics Sheets for AI Tasks](http://arxiv.org/abs/2107.01183)


  Several high-profile events, such as the use of biased recidivism systems and
mass testing of emotion recognition systems on vulnerable sub-populations, have
highlighted how technology will often lead to more adverse outcomes for those
that are already marginalized. In this paper, I will make a case for thinking
about ethical considerations not just at the level of individual models and
datasets, but also at the level of AI tasks. I will present a new form of such
an effort, Ethics Sheets for AI Tasks, dedicated to fleshing out the
assumptions and ethical considerations hidden in how a task is commonly framed
and in the choices we make regarding the data, method, and evaluation. Finally,
I will provide an example ethics sheet for automatic emotion recognition.
Together with Data Sheets for datasets and Model Cards for AI systems, Ethics
Sheets aid in the development and deployment of responsible AI systems.

    

### [[2107.01194] How Incomplete is Contrastive Learning? An Inter-intra Variant Dual Representation Method for Self-supervised Video Recognition](http://arxiv.org/abs/2107.01194)


  Contrastive learning applied to self-supervised representation learning has
seen a resurgence in deep models. In this paper, we find that existing
contrastive learning based solutions for self-supervised video recognition
focus on inter-variance encoding but ignore the intra-variance existing in
clips within the same video. We thus propose to learn dual representations for
each clip which (\romannumeral 1) encode intra-variance through a shuffle-rank
pretext task; (\romannumeral 2) encode inter-variance through a temporal
coherent contrastive loss. Experiment results show that our method plays an
essential role in balancing inter and intra variances and brings consistent
performance gains on multiple backbones and contrastive learning frameworks.
Integrated with SimCLR and pretrained on Kinetics-400, our method achieves
$\textbf{82.0\%}$ and $\textbf{51.2\%}$ downstream classification accuracy on
UCF101 and HMDB51 test sets respectively and $\textbf{46.1\%}$ video retrieval
accuracy on UCF101, outperforming both pretext-task based and contrastive
learning based counterparts.

    

### [[2107.01295] Dependent Type Systems as Macros](http://arxiv.org/abs/2107.01295)


  We present Turnstile+, a high-level, macros-based metaDSL for building
dependently typed languages. With it, programmers may rapidly prototype and
iterate on the design of new dependently typed features and extensions. Or they
may create entirely new DSLs whose dependent type "power" is tailored to a
specific domain. Our framework's support of language-oriented programming also
makes it suitable for experimenting with systems of interacting components,
e.g., a proof assistant and its companion DSLs. This paper explains the
implementation details of Turnstile+, as well as how it may be used to create a
wide-variety of dependently typed languages, from a lightweight one with
indexed types, to a full spectrum proof assistant, complete with a tactic
system and extensions for features like sized types and SMT interaction.

    

### [[2107.01815] A Formal Semantics of the GraalVM Intermediate Representation](http://arxiv.org/abs/2107.01815)


  The optimization phase of a compiler is responsible for transforming an
intermediate representation (IR) of a program into a more efficient form.
Modern optimizers, such as that used in the GraalVM compiler, use an IR
consisting of a sophisticated graph data structure that combines data flow and
control flow into the one structure. As part of a wider project on the
verification of optimization passes of GraalVM, this paper describes a
semantics for its IR within Isabelle/HOL. The semantics consists of a big-step
operational semantics for data nodes (which are represented in a graph-based
static single assignment (SSA) form) and a small-step operational semantics for
handling control flow including heap-based reads and writes, exceptions, and
method calls. We have proved a suite of canonicalization optimizations and
conditional elimination optimizations with respect to the semantics.

    

### [[2107.01883] A Theory of Higher-Order Subtyping with Type Intervals (Extended Version)](http://arxiv.org/abs/2107.01883)


  The calculus of Dependent Object Types (DOT) has enabled a more principled
and robust implementation of Scala, but its support for type-level computation
has proven insufficient. As a remedy, we propose $F^\omega_{..}$, a rigorous
theoretical foundation for Scala's higher-kinded types. $F^\omega_{..}$ extends
$F^\omega_{<:}$ with interval kinds, which afford a unified treatment of
important type- and kind-level abstraction mechanisms found in Scala, such as
bounded quantification, bounded operator abstractions, translucent type
definitions and first-class subtyping constraints. The result is a flexible and
general theory of higher-order subtyping. We prove type and kind safety of
$F^\omega_{..}$, as well as weak normalization of types and undecidability of
subtyping. All our proofs are mechanized in Agda using a fully syntactic
approach based on hereditary substitution.

    

### [[2102.00378] Model-Based Testing of Networked Applications](http://arxiv.org/abs/2102.00378)


  We present a principled automatic testing framework for application-layer
protocols. The key innovation is a domain-specific embedded language for
writing nondeterministic models of the behavior of networked servers. These
models are defined within the Coq interactive theorem prover, supporting a
smooth transition from testing to formal verification.
Given a server model, we show how to automatically derive a tester that
probes the server for unexpected behaviors. We address the uncertainties caused
by both the server's internal choices and the network delaying messages
nondeterministically. The derived tester accepts server implementations whose
possible behaviors are a subset of those allowed by the nondeterministic model.
We demonstrate the effectiveness of this framework by using it to specify and
test a fragment of the HTTP/1.1 protocol, showing that the automatically
derived tester can capture RFC violations in buggy server implementations,
including the latest versions of Apache and Nginx.

    

### [[2103.03198] Catala: A Programming Language for the Law](http://arxiv.org/abs/2103.03198)


  Law at large underpins modern society, codifying and governing many aspects
of citizens' daily lives. Oftentimes, law is subject to interpretation, debate
and challenges throughout various courts and jurisdictions. But in some other
areas, law leaves little room for interpretation, and essentially aims to
rigorously describe a computation, a decision procedure or, simply said, an
algorithm. Unfortunately, prose remains a woefully inadequate tool for the job.
The lack of formalism leaves room for ambiguities; the structure of legal
statutes, with many paragraphs and sub-sections spread across multiple pages,
makes it hard to compute the intended outcome of the algorithm underlying a
given text; and, as with any other piece of poorly-specified critical software,
the use of informal language leaves corner cases unaddressed. We introduce
Catala, a new programming language that we specifically designed to allow a
straightforward and systematic translation of statutory law into an executable
implementation. Catala aims to bring together lawyers and programmers through a
shared medium, which together they can understand, edit and evolve, bridging a
gap that often results in dramatically incorrect implementations of the law. We
have implemented a compiler for Catala, and have proven the correctness of its
core compilation steps using the F* proof assistant. We evaluate Catala on
several legal texts that are algorithms in disguise, notably section 121 of the
US federal income tax and the byzantine French family benefits; in doing so, we
uncover a bug in the official implementation. We observe as a consequence of
the formalization process that using Catala enables rich interactions between
lawyers and programmers, leading to a greater understanding of the original
legislative intent, while producing a correct-by-construction executable
specification reusable by the greater software ecosystem.

    

### [[2106.14938] Seeking Stability by being Lazy and Shallow](http://arxiv.org/abs/2106.14938)


  Designing a language feature often requires a choice between several,
similarly expressive possibilities. Given that user studies are generally
impractical, we propose using stability as a way of making such decisions.
Stability is a measure of whether the meaning of a program alters under small,
seemingly innocuous changes in the code. Directly motivated by a need to pin
down a feature in GHC/Haskell, we apply this notion of stability to analyse
four approaches to the instantiation of polymorphic types, concluding that the
most stable approach is lazy (instantiate a polytype only when absolutely
necessary) and shallow (instantiate only top-level type variables, not
variables that appear after explicit arguments).

    

### [<title>Custom Booster of XGBoost (e.g. NN model) - XGBoost</title>](https://discuss.xgboost.ai/t/custom-booster-of-xgboost-e-g-nn-model/2356/1)