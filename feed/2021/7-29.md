
## 2021-7-29

### [[2107.13268] Q-Learning for Conflict Resolution in B5G Network Automation](http://arxiv.org/abs/2107.13268)


  Network automation is gaining significant attention in the development of B5G
networks, primarily for reducing operational complexity, expenditures and
improving network efficiency. Concurrently operating closed loops aiming for
individual optimization targets may cause conflicts which, left unresolved,
would lead to significant degradation in network Key Performance Indicators
(KPIs), thereby resulting in sub-optimal network performance. Centralized
coordination, albeit optimal, is impractical in large scale networks and for
time-critical applications. Decentralized approaches are therefore envisaged in
the evolution to B5G and subsequently, 6G networks. This work explores
pervasive intelligence for conflict resolution in network automation, as an
alternative to centralized orchestration. A Q-Learning decentralized approach
to network automation is proposed, and an application to network slice
auto-scaling is designed and evaluated. Preliminary results highlight the
potential of the proposed scheme and justify further research work in this
direction.

    

### [[2107.13360] Quantum Technologies in the Telecommunications Industry](http://arxiv.org/abs/2107.13360)


  Quantum based technologies have been fundamental in our world. After
producing the laser and the transistor, the devices that have shaped our modern
information society, the possibilities enabled by the ability to create and
manipulate individual quantum states opens the door to a second quantum
revolution. In this paper we explore the possibilities that these new
technologies bring to the Telecommu-nications industry

    

### [[2107.13374] 5G Multi-access Edge Computing: Security, Dependability, and Performance](http://arxiv.org/abs/2107.13374)


  The main innovation of the Fifth Generation (5G) of mobile networks is the
ability to provide novel services with new and stricter requirements. One of
the technologies that enable the new 5G services is the Multi-access Edge
Computing (MEC). MEC is a system composed of multiple devices with computing
and storage capabilities that are deployed at the edge of the network, i.e.,
close to the end users. MEC reduces latency and enables contextual information
and real-time awareness of the local environment. MEC also allows cloud
offloading and the reduction of traffic congestion. Performance is not the only
requirement that the new 5G services have. New mission-critical applications
also require high security and dependability. These three aspects (security,
dependability, and performance) are rarely addressed together. This survey
fills this gap and presents 5G MEC by addressing all these three aspects.
First, we overview the background knowledge on MEC by referring to the current
standardization efforts. Second, we individually present each aspect by
introducing the related taxonomy (important for the not expert on the aspect),
the state of the art, and the challenges on 5G MEC. Finally, we discuss the
challenges of jointly addressing the three aspects.

    

### [[2107.13440] Finding Better Precoding in Massive MIMO using Optimization Approach](http://arxiv.org/abs/2107.13440)


  The paper studies the multi-user precoding problem as a non-convex
optimization problem for wireless MIMO systems. In our work, we approximate the
target Spectral Efficiency function with a novel computationally simpler
function. Then, we reduce the precoding problem to an unconstrained
optimization task using a special differential projection method and solve it
by the Quasi-Newton L-BFGS iterative procedure to achieve gains in capacity. We
are testing the proposed approach in several scenarios generated using Quadriga
-- open-source software for generating realistic radio channel impulse
response. Our method shows monotonic improvement over heuristic methods with
reasonable computation time. The proposed L-BFGS optimization scheme is novel
in this area and shows a significant advantage over the standard approaches.

    

### [[2107.13504] Inferring Multiple Relationships between ASes using Graph Convolutional Network](http://arxiv.org/abs/2107.13504)


  Precisely understanding the business relationships between Autonomous Systems
(ASes) is essential for studying the Internet structure. So far, many inference
algorithms have been proposed to classify the AS relationships, which mainly
focus on Peer-Peer (P2P) and Provider-Customer (P2C) binary classification and
achieved excellent results. However, there are other types of AS relationships
in actual scenarios, i.e., the businessbased sibling and structure-based
exchange relationships, that were neglected in the previous research. These
relationships are usually difficult to be inferred by existing algorithms
because there is no discrimination on the designed features compared to the P2P
or P2C relationships.
In this paper, we focus on the multi-classification of AS relationships for
the first time. We first summarize the differences between AS relationships
under the structural and attribute features, and the reasons why multiple
relationships are difficult to be inferred. We then introduce new features and
propose a Graph Convolutional Network (GCN) framework, AS-GCN, to solve this
multi-classification problem under complex scene. The framework takes into
account the global network structure and local link features concurrently. The
experiments on real Internet topological data validate the effectiveness of our
method, i.e., AS-GCN achieves comparable results on the easy binary
classification task, and outperforms a series of baselines on the more
difficult multi-classification task, with the overall accuracy above 95%.

    

### [[2007.06812] Can Encrypted DNS Be Fast?](http://arxiv.org/abs/2007.06812)


  In this paper, we study the performance of encrypted DNS protocols and
conventional DNS from thousands of home networks in the United States, over one
month in 2020. We perform these measurements from the homes of 2,693
participating panelists in the Federal Communications Commission's (FCC)
Measuring Broadband America program. We found that clients do not have to trade
DNS performance for privacy. For certain resolvers, DoT was able to perform
faster than DNS in median response times, even as latency increased. We also
found significant variation in DoH performance across recursive resolvers.
Based on these results, we recommend that DNS clients (e.g., web browsers)
should periodically conduct simple latency and response time measurements to
determine which protocol and resolver a client should use. No single DNS
protocol nor resolver performed the best for all clients.

    

### [[2102.06028] Synthesis of Winning Attacks on Communication Protocols using Supervisory Control Theory: Two Case Studies](http://arxiv.org/abs/2102.06028)


  There is an increasing need to study the vulnerability of communication
protocols in distributed systems to malicious attacks that attempt to violate
properties such as safety or liveness. In this paper, we propose a common
methodology for formal synthesis of successful attacks against two well-known
protocols, the Alternating Bit Protocol (ABP) and the Transmission Control
Protocol (TCP), where the attacker can always eventually win, called For-all
attacks. This generalizes previous work on the synthesis of There-exists
attacks for TCP, where the attacker can sometimes win. We model the ABP and TCP
protocols and system architecture by finite-state automata and employ the
supervisory control theory of discrete event systems to pose and solve the
synthesis of For-all attacks, where the attacker has partial observability and
controllability of the system events. We consider several scenarios of
person-in-themiddle attacks against ABP and TCP and present the results of
attack synthesis using our methodology for each case.

    

### [[2107.12997] Fully Homomorphically Encrypted Deep Learning as a Service](http://arxiv.org/abs/2107.12997)


  Fully Homomorphic Encryption (FHE) is a relatively recent advancement in the
field of privacy-preserving technologies. FHE allows for the arbitrary depth
computation of both addition and multiplication, and thus the application of
abelian/polynomial equations, like those found in deep learning algorithms.
This project investigates, derives, and proves how FHE with deep learning can
be used at scale, with relatively low time complexity, the problems that such a
system incurs, and mitigations/solutions for such problems. In addition, we
discuss how this could have an impact on the future of data privacy and how it
can enable data sharing across various actors in the agri-food supply chain,
hence allowing the development of machine learning-based systems. Finally, we
find that although FHE incurs a high spatial complexity cost, the time
complexity is within expected reasonable bounds, while allowing for absolutely
private predictions to be made, in our case for milk yield prediction.

    

### [[2107.13034] Dataset Distillation with Infinitely Wide Convolutional Networks](http://arxiv.org/abs/2107.13034)


  The effectiveness of machine learning algorithms arises from being able to
extract useful features from large amounts of data. As model and dataset sizes
increase, dataset distillation methods that compress large datasets into
significantly smaller yet highly performant ones will become valuable in terms
of training efficiency and useful feature extraction. To that end, we apply a
novel distributed kernel based meta-learning framework to achieve
state-of-the-art results for dataset distillation using infinitely wide
convolutional neural networks. For instance, using only 10 datapoints (0.02% of
original dataset), we obtain over 64% test accuracy on CIFAR-10 image
classification task, a dramatic improvement over the previous best test
accuracy of 40%. Our state-of-the-art results extend across many other settings
for MNIST, Fashion-MNIST, CIFAR-10, CIFAR-100, and SVHN. Furthermore, we
perform some preliminary analyses of our distilled datasets to shed light on
how they differ from naturally occurring data.

    

### [[2107.13045] A Case Study on Sampling Strategies for Evaluating Neural Sequential Item Recommendation Models](http://arxiv.org/abs/2107.13045)


  At the present time, sequential item recommendation models are compared by
calculating metrics on a small item subset (target set) to speed up
computation. The target set contains the relevant item and a set of negative
items that are sampled from the full item set. Two well-known strategies to
sample negative items are uniform random sampling and sampling by popularity to
better approximate the item frequency distribution in the dataset. Most
recently published papers on sequential item recommendation rely on sampling by
popularity to compare the evaluated models. However, recent work has already
shown that an evaluation with uniform random sampling may not be consistent
with the full ranking, that is, the model ranking obtained by evaluating a
metric using the full item set as target set, which raises the question whether
the ranking obtained by sampling by popularity is equal to the full ranking. In
this work, we re-evaluate current state-of-the-art sequential recommender
models from the point of view, whether these sampling strategies have an impact
on the final ranking of the models. We therefore train four recently proposed
sequential recommendation models on five widely known datasets. For each
dataset and model, we employ three evaluation strategies. First, we compute the
full model ranking. Then we evaluate all models on a target set sampled by the
two different sampling strategies, uniform random sampling and sampling by
popularity with the commonly used target set size of 100, compute the model
ranking for each strategy and compare them with each other. Additionally, we
vary the size of the sampled target set. Overall, we find that both sampling
strategies can produce inconsistent rankings compared with the full ranking of
the models. Furthermore, both sampling by popularity and uniform random
sampling do not consistently produce the same ranking ...

    

### [[2107.13054] Exceeding the Limits of Visual-Linguistic Multi-Task Learning](http://arxiv.org/abs/2107.13054)


  By leveraging large amounts of product data collected across hundreds of live
e-commerce websites, we construct 1000 unique classification tasks that share
similarly-structured input data, comprised of both text and images. These
classification tasks focus on learning the product hierarchy of different
e-commerce websites, causing many of them to be correlated. Adopting a
multi-modal transformer model, we solve these tasks in unison using multi-task
learning (MTL). Extensive experiments are presented over an initial 100-task
dataset to reveal best practices for "large-scale MTL" (i.e., MTL with more
than 100 tasks). From these experiments, a final, unified methodology is
derived, which is composed of both best practices and new proposals such as
DyPa, a simple heuristic for automatically allocating task-specific parameters
to tasks that could benefit from extra capacity. Using our large-scale MTL
methodology, we successfully train a single model across all 1000 tasks in our
dataset while using minimal task specific parameters, thereby showing that it
is possible to extend several orders of magnitude beyond current efforts in
MTL.

    

### [[2107.13059] Explicit Pairwise Factorized Graph Neural Network for Semi-Supervised Node Classification](http://arxiv.org/abs/2107.13059)


  Node features and structural information of a graph are both crucial for
semi-supervised node classification problems. A variety of graph neural network
(GNN) based approaches have been proposed to tackle these problems, which
typically determine output labels through feature aggregation. This can be
problematic, as it implies conditional independence of output nodes given
hidden representations, despite their direct connections in the graph. To learn
the direct influence among output nodes in a graph, we propose the Explicit
Pairwise Factorized Graph Neural Network (EPFGNN), which models the whole graph
as a partially observed Markov Random Field. It contains explicit pairwise
factors to model output-output relations and uses a GNN backbone to model
input-output relations. To balance model complexity and expressivity, the
pairwise factors have a shared component and a separate scaling coefficient for
each edge. We apply the EM algorithm to train our model, and utilize a
star-shaped piecewise likelihood for the tractable surrogate objective. We
conduct experiments on various datasets, which shows that our model can
effectively improve the performance for semi-supervised node classification on
graphs.

    

### [[2107.13066] Removing Operational Friction Using Process Mining: Challenges Provided by the Internet of Production (IoP)](http://arxiv.org/abs/2107.13066)


  Operational processes in production, logistics, material handling,
maintenance, etc., are supported by cyber-physical systems combining hardware
and software components. As a result, the digital and the physical world are
closely aligned, and it is possible to track operational processes in detail
(e.g., using sensors). The abundance of event data generated by today's
operational processes provides opportunities and challenges for process mining
techniques supporting process discovery, performance analysis, and conformance
checking. Using existing process mining tools, it is already possible to
automatically discover process models and uncover performance and compliance
problems. In the DFG-funded Cluster of Excellence "Internet of Production"
(IoP), process mining is used to create "digital shadows" to improve a wide
variety of operational processes. However, operational processes are dynamic,
distributed, and complex. Driven by the challenges identified in the IoP
cluster, we work on novel techniques for comparative process mining (comparing
process variants for different products at different locations at different
times), object-centric process mining (to handle processes involving different
types of objects that interact), and forward-looking process mining (to explore
"What if?" questions). By addressing these challenges, we aim to develop
valuable "digital shadows" that can be used to remove operational friction.

    

### [[2107.13067] A Deep Learning Algorithm for Piecewise Linear Interface Construction (PLIC)](http://arxiv.org/abs/2107.13067)


  Piecewise Linear Interface Construction (PLIC) is frequently used to
geometrically reconstruct fluid interfaces in Computational Fluid Dynamics
(CFD) modeling of two-phase flows. PLIC reconstructs interfaces from a scalar
field that represents the volume fraction of each phase in each computational
cell. Given the volume fraction and interface normal, the location of a linear
interface is uniquely defined. For a cubic computational cell (3D), the
position of the planar interface is determined by intersecting the cube with a
plane, such that the volume of the resulting truncated polyhedron cell is equal
to the volume fraction. Yet it is geometrically complex to find the exact
position of the plane, and it involves calculations that can be a computational
bottleneck of many CFD models. However, while the forward problem of 3D PLIC is
challenging, the inverse problem, of finding the volume of the truncated
polyhedron cell given a defined plane, is simple. In this work, we propose a
deep learning model for the solution to the forward problem of PLIC by only
making use of its inverse problem. The proposed model is up to several orders
of magnitude faster than traditional schemes, which significantly reduces the
computational bottleneck of PLIC in CFD simulations.

    

### [[2107.13068] End-to-End Balancing for Causal Continuous Treatment-Effect Estimation](http://arxiv.org/abs/2107.13068)


  We study the problem of observational causal inference with continuous
treatment. We focus on the challenge of estimating the causal response curve
for infrequently-observed treatment values. We design a new algorithm based on
the framework of entropy balancing which learns weights that directly maximize
causal inference accuracy using end-to-end optimization. Our weights can be
customized for different datasets and causal inference algorithms. We propose a
new theory for consistency of entropy balancing for continuous treatments.
Using synthetic and real-world data, we show that our proposed algorithm
outperforms the entropy balancing in terms of causal inference accuracy.

    

### [[2107.13074] Toward AI Assistants That Let Designers Design](http://arxiv.org/abs/2107.13074)


  AI for supporting designers needs to be rethought. It should aim to
cooperate, not automate, by supporting and leveraging the creativity and
problem-solving of designers. The challenge for such AI is how to infer
designers' goals and then help them without being needlessly disruptive. We
present AI-assisted design: a framework for creating such AI, built around
generative user models which enable reasoning about designers' goals,
reasoning, and capabilities.

    

### [[2107.13076] Interactive Storytelling for Children: A Case-study of Design and Development Considerations for Ethical Conversational AI](http://arxiv.org/abs/2107.13076)


  Conversational Artificial Intelligence (CAI) systems and Intelligent Personal
Assistants (IPA), such as Alexa, Cortana, Google Home and Siri are becoming
ubiquitous in our lives, including those of children, the implications of which
is receiving increased attention, specifically with respect to the effects of
these systems on children's cognitive, social and linguistic development.
Recent advances address the implications of CAI with respect to privacy,
safety, security, and access. However, there is a need to connect and embed the
ethical and technical aspects in the design. Using a case-study of a research
and development project focused on the use of CAI in storytelling for children,
this paper reflects on the social context within a specific case of technology
development, as substantiated and supported by argumentation from within the
literature. It describes the decision making process behind the recommendations
made on this case for their adoption in the creative industries. Further
research that engages with developers and stakeholders in the ethics of
storytelling through CAI is highlighted as a matter of urgency.

    

### [[2107.13078] A Payload Optimization Method for Federated Recommender Systems](http://arxiv.org/abs/2107.13078)


  We introduce the payload optimization method for federated recommender
systems (FRS). In federated learning (FL), the global model payload that is
moved between the server and users depends on the number of items to recommend.
The model payload grows when there is an increasing number of items. This
becomes challenging for an FRS if it is running in production mode. To tackle
the payload challenge, we formulated a multi-arm bandit solution that selected
part of the global model and transmitted it to all users. The selection process
was guided by a novel reward function suitable for FL systems. So far as we are
aware, this is the first optimization method that seeks to address item
dependent payloads. The method was evaluated using three benchmark
recommendation datasets. The empirical validation confirmed that the proposed
method outperforms the simpler methods that do not benefit from the bandits for
the purpose of item selection. In addition, we have demonstrated the usefulness
of our proposed method by rigorously evaluating the effects of a payload
reduction on the recommendation performance degradation. Our method achieved up
to a 90\% reduction in model payload, yielding only a $\sim$4\% - 8\% loss in
the recommendation performance for highly sparse datasets

    

### [[2107.13090] Policy Gradient Methods Find the Nash Equilibrium in N-player General-sum Linear-quadratic Games](http://arxiv.org/abs/2107.13090)


  We consider a general-sum N-player linear-quadratic game with stochastic
dynamics over a finite horizon and prove the global convergence of the natural
policy gradient method to the Nash equilibrium. In order to prove the
convergence of the method, we require a certain amount of noise in the system.
We give a condition, essentially a lower bound on the covariance of the noise
in terms of the model parameters, in order to guarantee convergence. We
illustrate our results with numerical experiments to show that even in
situations where the policy gradient method may not converge in the
deterministic setting, the addition of noise leads to convergence.

    

### [[2107.13093] Automated Human Cell Classification in Sparse Datasets using Few-Shot Learning](http://arxiv.org/abs/2107.13093)


  Classifying and analyzing human cells is a lengthy procedure, often involving
a trained professional. In an attempt to expedite this process, an active area
of research involves automating cell classification through use of deep
learning-based techniques. In practice, a large amount of data is required to
accurately train these deep learning models. However, due to the sparse human
cell datasets currently available, the performance of these models is typically
low. This study investigates the feasibility of using few-shot learning-based
techniques to mitigate the data requirements for accurate training. The study
is comprised of three parts: First, current state-of-the-art few-shot learning
techniques are evaluated on human cell classification. The selected techniques
are trained on a non-medical dataset and then tested on two out-of-domain,
human cell datasets. The results indicate that, overall, the test accuracy of
state-of-the-art techniques decreased by at least 30% when transitioning from a
non-medical dataset to a medical dataset. Second, this study evaluates the
potential benefits, if any, to varying the backbone architecture and training
schemes in current state-of-the-art few-shot learning techniques when used in
human cell classification. Even with these variations, the overall test
accuracy decreased from 88.66% on non-medical datasets to 44.13% at best on the
medical datasets. Third, this study presents future directions for using
few-shot learning in human cell classification. In general, few-shot learning
in its current state performs poorly on human cell classification. The study
proves that attempts to modify existing network architectures are not effective
and concludes that future research effort should be focused on improving
robustness towards out-of-domain testing using optimization-based or
self-supervised few-shot learning techniques.

    

### [[2107.13098] A Tale Of Two Long Tails](http://arxiv.org/abs/2107.13098)


  As machine learning models are increasingly employed to assist human
decision-makers, it becomes critical to communicate the uncertainty associated
with these model predictions. However, the majority of work on uncertainty has
focused on traditional probabilistic or ranking approaches - where the model
assigns low probabilities or scores to uncertain examples. While this captures
what examples are challenging for the model, it does not capture the underlying
source of the uncertainty. In this work, we seek to identify examples the model
is uncertain about and characterize the source of said uncertainty. We explore
the benefits of designing a targeted intervention - targeted data augmentation
of the examples where the model is uncertain over the course of training. We
investigate whether the rate of learning in the presence of additional
information differs between atypical and noisy examples? Our results show that
this is indeed the case, suggesting that well-designed interventions over the
course of training can be an effective way to characterize and distinguish
between different sources of uncertainty.

    

### [[2107.13109] Pixyz: a library for developing deep generative models](http://arxiv.org/abs/2107.13109)


  With the recent rapid progress in the study of deep generative models (DGMs),
there is a need for a framework that can implement them in a simple and generic
way. In this research, we focus on two features of the latest DGMs: (1) deep
neural networks are encapsulated by probability distributions and (2) models
are designed and learned based on an objective function. Taking these features
into account, we propose a new DGM library called Pixyz. We experimentally show
that our library is faster than existing probabilistic modeling languages in
learning simple DGMs and we show that our library can be used to implement
complex DGMs in a simple and concise manner, which is difficult to do with
existing libraries.

    

### [[2107.13124] Robust and Active Learning for Deep Neural Network Regression](http://arxiv.org/abs/2107.13124)


  We describe a gradient-based method to discover local error maximizers of a
deep neural network (DNN) used for regression, assuming the availability of an
"oracle" capable of providing real-valued supervision (a regression target) for
samples. For example, the oracle could be a numerical solver which,
operationally, is much slower than the DNN. Given a discovered set of local
error maximizers, the DNN is either fine-tuned or retrained in the manner of
active learning.

    

### [[2107.13132] Unsupervised Learning of Neurosymbolic Encoders](http://arxiv.org/abs/2107.13132)


  We present a framework for the unsupervised learning of neurosymbolic
encoders, i.e., encoders obtained by composing neural networks with symbolic
programs from a domain-specific language. Such a framework can naturally
incorporate symbolic expert knowledge into the learning process and lead to
more interpretable and factorized latent representations than fully neural
encoders. Also, models learned this way can have downstream impact, as many
analysis workflows can benefit from having clean programmatic descriptions. We
ground our learning algorithm in the variational autoencoding (VAE) framework,
where we aim to learn a neurosymbolic encoder in conjunction with a standard
decoder. Our algorithm integrates standard VAE-style training with modern
program synthesis techniques. We evaluate our method on learning latent
representations for real-world trajectory data from animal biology and sports
analytics. We show that our approach offers significantly better separation
than standard VAEs and leads to practical gains on downstream tasks.

    

### [[2107.13136] Insights from Generative Modeling for Neural Video Compression](http://arxiv.org/abs/2107.13136)


  While recent machine learning research has revealed connections between deep
generative models such as VAEs and rate-distortion losses used in learned
compression, most of this work has focused on images. In a similar spirit, we
view recently proposed neural video coding algorithms through the lens of deep
autoregressive and latent variable modeling. We present recent neural video
codecs as instances of a generalized stochastic temporal autoregressive
transform, and propose new avenues for further improvements inspired by
normalizing flows and structured priors. We propose several architectures that
yield state-of-the-art video compression performance on full-resolution video
and discuss their tradeoffs and ablations. In particular, we propose (i)
improved temporal autoregressive transforms, (ii) improved entropy models with
structured and temporal dependencies, and (iii) variable bitrate versions of
our algorithms. Since our improvements are compatible with a large class of
existing models, we provide further evidence that the generative modeling
viewpoint can advance the neural video coding field.

    

### [[2107.13148] Combining Machine Learning Classifiers for Stock Trading with Effective Feature Extraction](http://arxiv.org/abs/2107.13148)


  The unpredictability and volatility of the stock market render it challenging
to make a substantial profit using any generalized scheme. This paper intends
to discuss our machine learning model, which can make a significant amount of
profit in the US stock market by performing live trading in the Quantopian
platform while using resources free of cost. Our top approach was to use
ensemble learning with four classifiers: Gaussian Naive Bayes, Decision Tree,
Logistic Regression with L1 regularization and Stochastic Gradient Descent, to
decide whether to go long or short on a particular stock. Our best model
performed daily trade between July 2011 and January 2019, generating 54.35%
profit. Finally, our work showcased that mixtures of weighted classifiers
perform better than any individual predictor about making trading decisions in
the stock market.

    

### [[2107.13153] Homogeneous Architecture Augmentation for Neural Predictor](http://arxiv.org/abs/2107.13153)


  Neural Architecture Search (NAS) can automatically design well-performed
architectures of Deep Neural Networks (DNNs) for the tasks at hand. However,
one bottleneck of NAS is the prohibitively computational cost largely due to
the expensive performance evaluation. The neural predictors can directly
estimate the performance without any training of the DNNs to be evaluated, thus
have drawn increasing attention from researchers. Despite their popularity,
they also suffer a severe limitation: the shortage of annotated DNN
architectures for effectively training the neural predictors. In this paper, we
proposed Homogeneous Architecture Augmentation for Neural Predictor (HAAP) of
DNN architectures to address the issue aforementioned. Specifically, a
homogeneous architecture augmentation algorithm is proposed in HAAP to generate
sufficient training data taking the use of homogeneous representation.
Furthermore, the one-hot encoding strategy is introduced into HAAP to make the
representation of DNN architectures more effective. The experiments have been
conducted on both NAS-Benchmark-101 and NAS-Bench-201 dataset. The experimental
results demonstrate that the proposed HAAP algorithm outperforms the state of
the arts compared, yet with much less training data. In addition, the ablation
studies on both benchmark datasets have also shown the universality of the
homogeneous architecture augmentation.

    

### [[2107.13157] Retinal Microvasculature as Biomarker for Diabetes and Cardiovascular Diseases](http://arxiv.org/abs/2107.13157)


  Purpose: To demonstrate that retinal microvasculature per se is a reliable
biomarker for Diabetic Retinopathy (DR) and, by extension, cardiovascular
diseases. Methods: Deep Learning Convolutional Neural Networks (CNN) applied to
color fundus images for semantic segmentation of the blood vessels and severity
classification on both vascular and full images. Vessel reconstruction through
harmonic descriptors is also used as a smoothing and de-noising tool. The
mathematical background of the theory is also outlined. Results: For diabetic
patients, at least 93.8% of DR No-Refer vs. Refer classification can be related
to vasculature defects. As for the Non-Sight Threatening vs. Sight Threatening
case, the ratio is as high as 96.7%. Conclusion: In the case of DR, most of the
disease biomarkers are related topologically to the vasculature. Translational
Relevance: Experiments conducted on eye blood vasculature reconstruction as a
biomarker shows a strong correlation between vasculature shape and later stages
of DR.

    

### [[2107.13163] Statistically Meaningful Approximation: a Case Study on Approximating Turing Machines with Transformers](http://arxiv.org/abs/2107.13163)


  A common lens to theoretically study neural net architectures is to analyze
the functions they can approximate. However, the constructions from
approximation theory often have unrealistic aspects, for example, reliance on
infinite precision to memorize target function values, which make these results
potentially less meaningful. To address these issues, this work proposes a
formal definition of statistically meaningful approximation which requires the
approximating network to exhibit good statistical learnability. We present case
studies on statistically meaningful approximation for two classes of functions:
boolean circuits and Turing machines. We show that overparameterized
feedforward neural nets can statistically meaningfully approximate boolean
circuits with sample complexity depending only polynomially on the circuit
size, not the size of the approximating network. In addition, we show that
transformers can statistically meaningfully approximate Turing machines with
computation time bounded by $T$, requiring sample complexity polynomial in the
alphabet size, state space size, and $\log (T)$. Our analysis introduces new
tools for generalization bounds that provide much tighter sample complexity
guarantees than the typical VC-dimension or norm-based bounds, which may be of
independent interest.

    

### [[2107.13171] Learning with Multiclass AUC: Theory and Algorithms](http://arxiv.org/abs/2107.13171)


  The Area under the ROC curve (AUC) is a well-known ranking metric for
problems such as imbalanced learning and recommender systems. The vast majority
of existing AUC-optimization-based machine learning methods only focus on
binary-class cases, while leaving the multiclass cases unconsidered. In this
paper, we start an early trial to consider the problem of learning multiclass
scoring functions via optimizing multiclass AUC metrics. Our foundation is
based on the M metric, which is a well-known multiclass extension of AUC. We
first pay a revisit to this metric, showing that it could eliminate the
imbalance issue from the minority class pairs. Motivated by this, we propose an
empirical surrogate risk minimization framework to approximately optimize the M
metric. Theoretically, we show that: (i) optimizing most of the popular
differentiable surrogate losses suffices to reach the Bayes optimal scoring
function asymptotically; (ii) the training framework enjoys an imbalance-aware
generalization error bound, which pays more attention to the bottleneck samples
of minority classes compared with the traditional $O(\sqrt{1/N})$ result.
Practically, to deal with the low scalability of the computational operations,
we propose acceleration methods for three popular surrogate loss functions,
including the exponential loss, squared loss, and hinge loss, to speed up loss
and gradient evaluations. Finally, experimental results on 11 real-world
datasets demonstrate the effectiveness of our proposed framework.

    

### [[2107.13173] New Metrics to Evaluate the Performance and Fairness of Personalized Federated Learning](http://arxiv.org/abs/2107.13173)


  In Federated Learning (FL), the clients learn a single global model (FedAvg)
through a central aggregator. In this setting, the non-IID distribution of the
data across clients restricts the global FL model from delivering good
performance on the local data of each client. Personalized FL aims to address
this problem by finding a personalized model for each client. Recent works
widely report the average personalized model accuracy on a particular data
split of a dataset to evaluate the effectiveness of their methods. However,
considering the multitude of personalization approaches proposed, it is
critical to study the per-user personalized accuracy and the accuracy
improvements among users with an equitable notion of fairness. To address these
issues, we present a set of performance and fairness metrics intending to
assess the quality of personalized FL methods. We apply these metrics to four
recently proposed personalized FL methods, PersFL, FedPer, pFedMe, and
Per-FedAvg, on three different data splits of the CIFAR-10 dataset. Our
evaluations show that the personalized model with the highest average accuracy
across users may not necessarily be the fairest. Our code is available at
this https URL for public use.

    

### [[2107.13186] AutoML Meets Time Series Regression Design and Analysis of the AutoSeries Challenge](http://arxiv.org/abs/2107.13186)


  Analyzing better time series with limited human effort is of interest to
academia and industry. Driven by business scenarios, we organized the first
Automated Time Series Regression challenge (AutoSeries) for the WSDM Cup 2020.
We present its design, analysis, and post-hoc experiments. The code submission
requirement precluded participants from any manual intervention, testing
automated machine learning capabilities of solutions, across many datasets,
under hardware and time limitations. We prepared 10 datasets from diverse
application domains (sales, power consumption, air quality, traffic, and
parking), featuring missing data, mixed continuous and categorical variables,
and various sampling rates. Each dataset was split into a training and a test
sequence (which was streamed, allowing models to continuously adapt). The
setting of time series regression, differs from classical forecasting in that
covariates at the present time are known. Great strides were made by
participants to tackle this AutoSeries problem, as demonstrated by the jump in
performance from the sample submission, and post-hoc comparisons with
AutoGluon. Simple yet effective methods were used, based on feature
engineering, LightGBM, and random search hyper-parameter tuning, addressing all
aspects of the challenge. Our post-hoc analyses revealed that providing
additional time did not yield significant improvements. The winners' code was
open-sourced this https URL.

    

### [[2107.13191] Neural Network Approximation of Refinable Functions](http://arxiv.org/abs/2107.13191)


  In the desire to quantify the success of neural networks in deep learning and
other applications, there is a great interest in understanding which functions
are efficiently approximated by the outputs of neural networks. By now, there
exists a variety of results which show that a wide range of functions can be
approximated with sometimes surprising accuracy by these outputs. For example,
it is known that the set of functions that can be approximated with exponential
accuracy (in terms of the number of parameters used) includes, on one hand,
very smooth functions such as polynomials and analytic functions (see e.g.
\cite{E,S,Y}) and, on the other hand, very rough functions such as the
Weierstrass function (see e.g. \cite{EPGB,DDFHP}), which is nowhere
differentiable. In this paper, we add to the latter class of rough functions by
showing that it also includes refinable functions. Namely, we show that
refinable functions are approximated by the outputs of deep ReLU networks with
a fixed width and increasing depth with accuracy exponential in terms of their
number of parameters. Our results apply to functions used in the standard
construction of wavelets as well as to functions constructed via subdivision
algorithms in Computer Aided Geometric Design.

    

### [[2107.13200] An explainable two-dimensional single model deep learning approach for Alzheimer's disease diagnosis and brain atrophy localization](http://arxiv.org/abs/2107.13200)


  Early and accurate diagnosis of Alzheimer's disease (AD) and its prodromal
period mild cognitive impairment (MCI) is essential for the delayed disease
progression and the improved quality of patients'life. The emerging
computer-aided diagnostic methods that combine deep learning with structural
magnetic resonance imaging (sMRI) have achieved encouraging results, but some
of them are limit of issues such as data leakage and unexplainable diagnosis.
In this research, we propose a novel end-to-end deep learning approach for
automated diagnosis of AD and localization of important brain regions related
to the disease from sMRI data. This approach is based on a 2D single model
strategy and has the following differences from the current approaches: 1)
Convolutional Neural Network (CNN) models of different structures and
capacities are evaluated systemically and the most suitable model is adopted
for AD diagnosis; 2) a data augmentation strategy named Two-stage Random
RandAugment (TRRA) is proposed to alleviate the overfitting issue caused by
limited training data and to improve the classification performance in AD
diagnosis; 3) an explainable method of Grad-CAM++ is introduced to generate the
visually explainable heatmaps that localize and highlight the brain regions
that our model focuses on and to make our model more transparent. Our approach
has been evaluated on two publicly accessible datasets for two classification
tasks of AD vs. cognitively normal (CN) and progressive MCI (pMCI) vs. stable
MCI (sMCI). The experimental results indicate that our approach outperforms the
state-of-the-art approaches, including those using multi-model and 3D CNN
methods. The resultant localization heatmaps from our approach also highlight
the lateral ventricle and some disease-relevant regions of cortex, coincident
with the commonly affected regions during the development of AD.

    

### [[2107.13214] SONG: Self-Organizing Neural Graphs](http://arxiv.org/abs/2107.13214)


  Recent years have seen a surge in research on deep interpretable neural
networks with decision trees as one of the most commonly incorporated tools.
There are at least three advantages of using decision trees over logistic
regression classification models: they are easy to interpret since they are
based on binary decisions, they can make decisions faster, and they provide a
hierarchy of classes. However, one of the well-known drawbacks of decision
trees, as compared to decision graphs, is that decision trees cannot reuse the
decision nodes. Nevertheless, decision graphs were not commonly used in deep
learning due to the lack of efficient gradient-based training techniques. In
this paper, we fill this gap and provide a general paradigm based on Markov
processes, which allows for efficient training of the special type of decision
graphs, which we call Self-Organizing Neural Graphs (SONG). We provide an
extensive theoretical study of SONG, complemented by experiments conducted on
Letter, Connect4, MNIST, CIFAR, and TinyImageNet datasets, showing that our
method performs on par or better than existing decision models.

    

### [[2107.13217] DeepTeeth: A Teeth-photo Based Human Authentication System for Mobile and Hand-held Devices](http://arxiv.org/abs/2107.13217)


  This paper proposes teeth-photo, a new biometric modality for human
authentication on mobile and hand held devices. Biometrics samples are acquired
using the camera mounted on mobile device with the help of a mobile application
having specific markers to register the teeth area. Region of interest (RoI) is
then extracted using the markers and the obtained sample is enhanced using
contrast limited adaptive histogram equalization (CLAHE) for better visual
clarity. We propose a deep learning architecture and novel regularization
scheme to obtain highly discriminative embedding for small size RoI. Proposed
custom loss function was able to achieve perfect classification for the tiny
RoI of $75\times 75$ size. The model is end-to-end and few-shot and therefore
is very efficient in terms of time and energy requirements. The system can be
used in many ways including device unlocking and secure authentication. To the
best of our understanding, this is the first work on teeth-photo based
authentication for mobile device. Experiments have been conducted on an
in-house teeth-photo database collected using our application. The database is
made publicly available. Results have shown that the proposed system has
perfect accuracy.

    

### [[2107.13226] Multi-Graph Convolutional-Recurrent Neural Network (MGC-RNN) for Short-Term Forecasting of Transit Passenger Flow](http://arxiv.org/abs/2107.13226)


  Short-term forecasting of passenger flow is critical for transit management
and crowd regulation. Spatial dependencies, temporal dependencies,
inter-station correlations driven by other latent factors, and exogenous
factors bring challenges to the short-term forecasts of passenger flow of urban
rail transit networks. An innovative deep learning approach, Multi-Graph
Convolutional-Recurrent Neural Network (MGC-RNN) is proposed to forecast
passenger flow in urban rail transit systems to incorporate these complex
factors. We propose to use multiple graphs to encode the spatial and other
heterogenous inter-station correlations. The temporal dynamics of the
inter-station correlations are also modeled via the proposed multi-graph
convolutional-recurrent neural network structure. Inflow and outflow of all
stations can be collectively predicted with multiple time steps ahead via a
sequence to sequence(seq2seq) architecture. The proposed method is applied to
the short-term forecasts of passenger flow in Shenzhen Metro, China. The
experimental results show that MGC-RNN outperforms the benchmark algorithms in
terms of forecasting accuracy. Besides, it is found that the inter-station
driven by network distance, network structure, and recent flow patterns are
significant factors for passenger flow forecasting. Moreover, the architecture
of LSTM-encoder-decoder can capture the temporal dependencies well. In general,
the proposed framework could provide multiple views of passenger flow dynamics
for fine prediction and exhibit a possibility for multi-source heterogeneous
data fusion in the spatiotemporal forecast tasks.

    

### [[2107.13249] Bayesian Autoencoders for Drift Detection in Industrial Environments](http://arxiv.org/abs/2107.13249)


  Autoencoders are unsupervised models which have been used for detecting
anomalies in multi-sensor environments. A typical use includes training a
predictive model with data from sensors operating under normal conditions and
using the model to detect anomalies. Anomalies can come either from real
changes in the environment (real drift) or from faulty sensory devices (virtual
drift); however, the use of Autoencoders to distinguish between different
anomalies has not yet been considered. To this end, we first propose the
development of Bayesian Autoencoders to quantify epistemic and aleatoric
uncertainties. We then test the Bayesian Autoencoder using a real-world
industrial dataset for hydraulic condition monitoring. The system is injected
with noise and drifts, and we have found the epistemic uncertainty to be less
sensitive to sensor perturbations as compared to the reconstruction loss. By
observing the reconstructed signals with the uncertainties, we gain
interpretable insights, and these uncertainties offer a potential avenue for
distinguishing real and virtual drifts.

    

### [[2107.13252] Multi Agent System for Machine Learning Under Uncertainty in Cyber Physical Manufacturing System](http://arxiv.org/abs/2107.13252)


  Recent advancements in predictive machine learning has led to its application
in various use cases in manufacturing. Most research focused on maximising
predictive accuracy without addressing the uncertainty associated with it.
While accuracy is important, focusing primarily on it poses an overfitting
danger, exposing manufacturers to risk, ultimately hindering the adoption of
these techniques. In this paper, we determine the sources of uncertainty in
machine learning and establish the success criteria of a machine learning
system to function well under uncertainty in a cyber-physical manufacturing
system (CPMS) scenario. Then, we propose a multi-agent system architecture
which leverages probabilistic machine learning as a means of achieving such
criteria. We propose possible scenarios for which our proposed architecture is
useful and discuss future work. Experimentally, we implement Bayesian Neural
Networks for multi-tasks classification on a public dataset for the real-time
condition monitoring of a hydraulic system and demonstrate the usefulness of
the system by evaluating the probability of a prediction being accurate given
its uncertainty. We deploy these models using our proposed agent-based
framework and integrate web visualisation to demonstrate its real-time
feasibility.

    

### [[2107.13257] Towards Neural Schema Alignment for OpenStreetMap and Knowledge Graphs](http://arxiv.org/abs/2107.13257)


  OpenStreetMap (OSM) is one of the richest openly available sources of
volunteered geographic information. Although OSM includes various geographical
entities, their descriptions are highly heterogeneous, incomplete, and do not
follow any well-defined ontology. Knowledge graphs can potentially provide
valuable semantic information to enrich OSM entities. However, interlinking OSM
entities with knowledge graphs is inherently difficult due to the large,
heterogeneous, ambiguous, and flat OSM schema and the annotation sparsity. This
paper tackles the alignment of OSM tags with the corresponding knowledge graph
classes holistically by jointly considering the schema and instance layers. We
propose a novel neural architecture that capitalizes upon a shared latent space
for tag-to-class alignment created using linked entities in OSM and knowledge
graphs. Our experiments performed to align OSM datasets for several countries
with two of the most prominent openly available knowledge graphs, namely,
Wikidata and DBpedia, demonstrate that the proposed approach outperforms the
state-of-the-art schema alignment baselines by up to 53 percentage points in
terms of F1-score. The resulting alignment facilitates new semantic annotations
for over 10 million OSM entities worldwide, which is more than a 400% increase
compared to the existing semantic annotations in OSM.

    

### [[2107.13265] Learned Optimizers for Analytic Continuation](http://arxiv.org/abs/2107.13265)


  Traditional maximum entropy and sparsity-based algorithms for analytic
continuation often suffer from the ill-posed kernel matrix or demand tremendous
computation time for parameter tuning. Here we propose a neural network method
by convex optimization and replace the ill-posed inverse problem by a sequence
of well-conditioned surrogate problems. After training, the learned optimizers
are able to give a solution of high quality with low time cost and achieve
higher parameter efficiency than heuristic full-connected networks. The output
can also be used as a neural default model to improve the maximum entropy for
better performance. Our methods may be easily extended to other
high-dimensional inverse problems via large-scale pretraining.

    

### [[2107.13270] A Reflection on Learning from Data: Epistemology Issues and Limitations](http://arxiv.org/abs/2107.13270)


  Although learning from data is effective and has achieved significant
milestones, it has many challenges and limitations. Learning from data starts
from observations and then proceeds to broader generalizations. This framework
is controversial in science, yet it has achieved remarkable engineering
successes. This paper reflects on some epistemological issues and some of the
limitations of the knowledge discovered in data. The document discusses the
common perception that getting more data is the key to achieving better machine
learning models from theoretical and practical perspectives. The paper sheds
some light on the shortcomings of using generic mathematical theories to
describe the process. It further highlights the need for theories specialized
in learning from data. While more data leverages the performance of machine
learning models in general, the relation in practice is shown to be logarithmic
at its best; After a specific limit, more data stabilize or degrade the machine
learning models. Recent work in reinforcement learning showed that the trend is
shifting away from data-oriented approaches and relying more on algorithms. The
paper concludes that learning from data is hindered by many limitations. Hence
an approach that has an intensional orientation is needed.

    

### [[2107.13277] A Novel CropdocNet for Automated Potato Late Blight Disease Detection from the Unmanned Aerial Vehicle-based Hyperspectral Imagery](http://arxiv.org/abs/2107.13277)


  Late blight disease is one of the most destructive diseases in potato crop,
leading to serious yield losses globally. Accurate diagnosis of the disease at
early stage is critical for precision disease control and management. Current
farm practices in crop disease diagnosis are based on manual visual inspection,
which is costly, time consuming, subject to individual bias. Recent advances in
imaging sensors (e.g. RGB, multiple spectral and hyperspectral cameras), remote
sensing and machine learning offer the opportunity to address this challenge.
Particularly, hyperspectral imagery (HSI) combining with machine learning/deep
learning approaches is preferable for accurately identifying specific plant
diseases because the HSI consists of a wide range of high-quality reflectance
information beyond human vision, capable of capturing both spectral-spatial
information. The proposed method considers the potential disease specific
reflectance radiation variance caused by the canopy structural diversity,
introduces the multiple capsule layers to model the hierarchical structure of
the spectral-spatial disease attributes with the encapsulated features to
represent the various classes and the rotation invariance of the disease
attributes in the feature space. We have evaluated the proposed method with the
real UAV-based HSI data under the controlled field conditions. The
effectiveness of the hierarchical features has been quantitatively assessed and
compared with the existing representative machine learning/deep learning
methods. The experiment results show that the proposed model significantly
improves the accuracy performance when considering hierarchical-structure of
spectral-spatial features, comparing to the existing methods only using
spectral, or spatial or spectral-spatial features without consider
hierarchical-structure of spectral-spatial features.

    

### [[2107.13304] Bayesian Autoencoders: Analysing and Fixing the Bernoulli likelihood for Out-of-Distribution Detection](http://arxiv.org/abs/2107.13304)


  After an autoencoder (AE) has learnt to reconstruct one dataset, it might be
expected that the likelihood on an out-of-distribution (OOD) input would be
low. This has been studied as an approach to detect OOD inputs. Recent work
showed this intuitive approach can fail for the dataset pairs FashionMNIST vs
MNIST. This paper suggests this is due to the use of Bernoulli likelihood and
analyses why this is the case, proposing two fixes: 1) Compute the uncertainty
of likelihood estimate by using a Bayesian version of the AE. 2) Use
alternative distributions to model the likelihood.

    

### [[2107.13312] Effective Eigendecomposition based Graph Adaptation for Heterophilic Networks](http://arxiv.org/abs/2107.13312)


  Graph Neural Networks (GNNs) exhibit excellent performance when graphs have
strong homophily property, i.e. connected nodes have the same labels. However,
they perform poorly on heterophilic graphs. Several approaches address the
issue of heterophily by proposing models that adapt the graph by optimizing
task-specific loss function using labelled data. These adaptations are made
either via attention or by attenuating or enhancing various
low-frequency/high-frequency signals, as needed for the task at hand. More
recent approaches adapt the eigenvalues of the graph. One important
interpretation of this adaptation is that these models select/weigh the
eigenvectors of the graph. Based on this interpretation, we present an
eigendecomposition based approach and propose EigenNetwork models that improve
the performance of GNNs on heterophilic graphs. Performance improvement is
achieved by learning flexible graph adaptation functions that modulate the
eigenvalues of the graph. Regularization of these functions via parameter
sharing helps to improve the performance even more. Our approach achieves up to
11% improvement in performance over the state-of-the-art methods on
heterophilic graphs.

    

### [[2107.13319] Chance constrained conic-segmentation support vector machine with uncertain data](http://arxiv.org/abs/2107.13319)


  Support vector machines (SVM) is one of the well known supervised classes of
learning algorithms. Furthermore, the conic-segmentation SVM (CS-SVM) is a
natural multiclass analogue of the standard binary SVM, as CS-SVM models are
dealing with the situation where the exact values of the data points are known.
This paper studies CS-SVM when the data points are uncertain or mislabelled.
With some properties known for the distributions, a chance-constrained CS-SVM
approach is used to ensure the small probability of misclassification for the
uncertain data. The geometric interpretation is presented to show how CS-SVM
works. Finally, we present experimental results to investigate the chance
constrained CS-SVM's performance.

    

### [[2107.13346] Doing Great at Estimating CATE? On the Neglected Assumptions in Benchmark Comparisons of Treatment Effect Estimators](http://arxiv.org/abs/2107.13346)


  The machine learning toolbox for estimation of heterogeneous treatment
effects from observational data is expanding rapidly, yet many of its
algorithms have been evaluated only on a very limited set of semi-synthetic
benchmark datasets. In this paper, we show that even in arguably the simplest
setting -- estimation under ignorability assumptions -- the results of such
empirical evaluations can be misleading if (i) the assumptions underlying the
data-generating mechanisms in benchmark datasets and (ii) their interplay with
baseline algorithms are inadequately discussed. We consider two popular machine
learning benchmark datasets for evaluation of heterogeneous treatment effect
estimators -- the IHDP and ACIC2016 datasets -- in detail. We identify problems
with their current use and highlight that the inherent characteristics of the
benchmark datasets favor some algorithms over others -- a fact that is rarely
acknowledged but of immense relevance for interpretation of empirical results.
We close by discussing implications and possible next steps.

    

### [[2107.13349] Self-Supervised Hybrid Inference in State-Space Models](http://arxiv.org/abs/2107.13349)


  We perform approximate inference in state-space models that allow for
nonlinear higher-order Markov chains in latent space. The conditional
independencies of the generative model enable us to parameterize only an
inference model, which learns to estimate clean states in a self-supervised
manner using maximum likelihood. First, we propose a recurrent method that is
trained directly on noisy observations. Afterward, we cast the model such that
the optimization problem leads to an update scheme that backpropagates through
a recursion similar to the classical Kalman filter and smoother. In scientific
applications, domain knowledge can give a linear approximation of the latent
transition maps. We can easily incorporate this knowledge into our model,
leading to a hybrid inference approach. In contrast to other methods,
experiments show that the hybrid method makes the inferred latent states
physically more interpretable and accurate, especially in low-data regimes.
Furthermore, we do not rely on an additional parameterization of the generative
model or supervision via uncorrupted observations or ground truth latent
states. Despite our model's simplicity, we obtain competitive results on the
chaotic Lorenz system compared to a fully supervised approach and outperform a
method based on variational inference.

    

### [[2107.13353] Fast Wireless Sensor Anomaly Detection based on Data Stream in Edge Computing Enabled Smart Greenhouse](http://arxiv.org/abs/2107.13353)


  Edge computing enabled smart greenhouse is a representative application of
Internet of Things technology, which can monitor the environmental information
in real time and employ the information to contribute to intelligent
decision-making. In the process, anomaly detection for wireless sensor data
plays an important role. However, traditional anomaly detection algorithms
originally designed for anomaly detection in static data have not properly
considered the inherent characteristics of data stream produced by wireless
sensor such as infiniteness, correlations and concept drift, which may pose a
considerable challenge on anomaly detection based on data stream, and lead to
low detection accuracy and efficiency. First, data stream usually generates
quickly which means that it is infinite and enormous, so any traditional
off-line anomaly detection algorithm that attempts to store the whole dataset
or to scan the dataset multiple times for anomaly detection will run out of
memory space. Second, there exist correlations among different data streams,
which traditional algorithms hardly consider. Third, the underlying data
generation process or data distribution may change over time. Thus, traditional
anomaly detection algorithms with no model update will lose their effects.
Considering these issues, a novel method (called DLSHiForest) on basis of
Locality-Sensitive Hashing and time window technique in this paper is proposed
to solve these problems while achieving accurate and efficient detection.
Comprehensive experiments are executed using real-world agricultural greenhouse
dataset to demonstrate the feasibility of our approach. Experimental results
show that our proposal is practicable in addressing challenges of traditional
anomaly detection while ensuring accuracy and efficiency.

    

### [[2107.13361] Snippet Policy Network for Multi-class Varied-length ECG Early Classification](http://arxiv.org/abs/2107.13361)


  Arrhythmia detection from ECG is an important research subject in the
prevention and diagnosis of cardiovascular diseases. The prevailing studies
formulate arrhythmia detection from ECG as a time series classification
problem. Meanwhile, early detection of arrhythmia presents a real-world demand
for early prevention and diagnosis. In this paper, we address a problem of
cardiovascular disease early classification, which is a varied-length and
long-length time series early classification problem as well. For solving this
problem, we propose a deep reinforcement learning-based framework, namely
Snippet Policy Network (SPN), consisting of four modules, snippet generator,
backbone network, controlling agent, and discriminator. Comparing to the
existing approaches, the proposed framework features flexible input length,
solves the dual-optimization solution of the earliness and accuracy goals.
Experimental results demonstrate that SPN achieves an excellent performance of
over 80\% in terms of accuracy. Compared to the state-of-the-art methods, at
least 7% improvement on different metrics, including the precision, recall,
F1-score, and harmonic mean, is delivered by the proposed SPN. To the best of
our knowledge, this is the first work focusing on solving the cardiovascular
early classification problem based on varied-length ECG data. Based on these
excellent features from SPN, it offers a good exemplification for addressing
all kinds of varied-length time series early classification problems.

    

### [[2107.13379] Evaluating the Use of Reconstruction Error for Novelty Localization](http://arxiv.org/abs/2107.13379)


  The pixelwise reconstruction error of deep autoencoders is often utilized for
image novelty detection and localization under the assumption that pixels with
high error indicate which parts of the input image are unfamiliar and therefore
likely to be novel. This assumed correlation between pixels with high
reconstruction error and novel regions of input images has not been verified
and may limit the accuracy of these methods. In this paper we utilize saliency
maps to evaluate whether this correlation exists. Saliency maps reveal directly
how much a change in each input pixel would affect reconstruction loss, while
each pixel's reconstruction error may be attributed to many input pixels when
layers are fully connected. We compare saliency maps to reconstruction error
maps via qualitative visualizations as well as quantitative correspondence
between the top K elements of the maps for both novel and normal images. Our
results indicate that reconstruction error maps do not closely correlate with
the importance of pixels in the input images, making them insufficient for
novelty localization.

    

### [[2107.13389] SimROD: A Simple Adaptation Method for Robust Object Detection](http://arxiv.org/abs/2107.13389)


  This paper presents a Simple and effective unsupervised adaptation method for
Robust Object Detection (SimROD). To overcome the challenging issues of domain
shift and pseudo-label noise, our method integrates a novel domain-centric
augmentation method, a gradual self-labeling adaptation procedure, and a
teacher-guided fine-tuning mechanism. Using our method, target domain samples
can be leveraged to adapt object detection models without changing the model
architecture or generating synthetic data. When applied to image corruptions
and high-level cross-domain adaptation benchmarks, our method outperforms prior
baselines on multiple domain adaptation benchmarks. SimROD achieves new
state-of-the-art on standard real-to-synthetic and cross-camera setup
benchmarks. On the image corruption benchmark, models adapted with our method
achieved a relative robustness improvement of 15-25% AP50 on Pascal-C and 5-6%
AP on COCO-C and Cityscapes-C. On the cross-domain benchmark, our method
outperformed the best baseline performance by up to 8% AP50 on Comic dataset
and up to 4% on Watercolor dataset.

    

### [[2107.13393] Meaning Versus Information, Prediction Versus Memory, and Question Versus Answer](http://arxiv.org/abs/2107.13393)


  Brain science and artificial intelligence have made great progress toward the
understanding and engineering of the human mind. The progress has accelerated
significantly since the turn of the century thanks to new methods for probing
the brain (both structure and function), and rapid development in deep learning
research. However, despite these new developments, there are still many open
questions, such as how to understand the brain at the system level, and various
robustness issues and limitations of deep learning. In this informal essay, I
will talk about some of the concepts that are central to brain science and
artificial intelligence, such as information and memory, and discuss how a
different view on these concepts can help us move forward, beyond current
limits of our understanding in these fields.

    

### [[2107.13394] Nonlinear State Space Modeling and Control of the Impact of Patients' Modifiable Lifestyle Behaviors on the Emergence of Multiple Chronic Conditions](http://arxiv.org/abs/2107.13394)


  The emergence and progression of multiple chronic conditions (MCC) over time
often form a dynamic network that depends on patient's modifiable risk factors
and their interaction with non-modifiable risk factors and existing conditions.
Continuous time Bayesian networks (CTBNs) are effective methods for modeling
the complex network of MCC relationships over time. However, CTBNs are not able
to effectively formulate the dynamic impact of patient's modifiable risk
factors on the emergence and progression of MCC. Considering a functional CTBN
(FCTBN) to represent the underlying structure of the MCC relationships with
respect to individuals' risk factors and existing conditions, we propose a
nonlinear state-space model based on Extended Kalman filter (EKF) to capture
the dynamics of the patients' modifiable risk factors and existing conditions
on the MCC evolution over time. We also develop a tensor control chart to
dynamically monitor the effect of changes in the modifiable risk factors of
individual patients on the risk of new chronic conditions emergence. We
validate the proposed approach based on a combination of simulation and real
data from a dataset of 385 patients from Cameron County Hispanic Cohort (CCHC)
over multiple years. The dataset examines the emergence of 5 chronic conditions
(Diabetes, Obesity, Cognitive Impairment, Hyperlipidemia, and Hypertension)
based on 4 modifiable risk factors representing lifestyle behaviors (Diet,
Exercise, Smoking Habit, and Drinking Habit) and 3 non-modifiable risk factors,
including demographic information (Age, Gender, Education). The results
demonstrate the effectiveness of the proposed methodology for dynamic
prediction and monitoring of the risk of MCC emergence in individual patients.

    

### [[2107.13404] XFL: eXtreme Function Labeling](http://arxiv.org/abs/2107.13404)


  Reverse engineers would benefit from identifiers like function names, but
these are usually unavailable in binaries. Training a machine learning model to
predict function names automatically is promising but fundamentally hard due to
the enormous number of classes. In this paper, we introduce eXtreme Function
Labeling (XFL), an extreme multi-label learning approach to selecting
appropriate labels for binary functions. XFL splits function names into tokens,
treating each as an informative label akin to the problem of tagging texts in
natural language. To capture the semantics of binary code, we introduce DEXTER,
a novel function embedding that combines static analysis-based features with
local context from the call graph and global context from the entire binary. We
demonstrate that XFL outperforms state-of-the-art approaches to function
labeling on a dataset of over 10,000 binaries from the Debian project,
achieving a precision of 82.5%. We also study combinations of XFL with
different published embeddings for binary functions and show that DEXTER
consistently improves over the state of the art in information gain. As a
result, we are able to show that binary function labeling is best phrased in
terms of multi-label learning, and that binary function embeddings benefit from
moving beyond just learning from syntax.

    

### [[2107.13405] Automatic Unstructured Handwashing Recognition using Smartwatch to Reduce Contact Transmission of Pathogens](http://arxiv.org/abs/2107.13405)


  Current guidelines from the World Health Organization indicate that the
SARSCoV-2 coronavirus, which results in the novel coronavirus disease
(COVID-19), is transmitted through respiratory droplets or by contact. Contact
transmission occurs when contaminated hands touch the mucous membrane of the
mouth, nose, or eyes. Moreover, pathogens can also be transferred from one
surface to another by contaminated hands, which facilitates transmission by
indirect contact. Consequently, hands hygiene is extremely important to prevent
the spread of the SARSCoV-2 virus. Additionally, hand washing and/or hand
rubbing disrupts also the transmission of other viruses and bacteria that cause
common colds, flu and pneumonia, thereby reducing the overall disease burden.
The vast proliferation of wearable devices, such as smartwatches, containing
acceleration, rotation, magnetic field sensors, etc., together with the modern
technologies of artificial intelligence, such as machine learning and more
recently deep-learning, allow the development of accurate applications for
recognition and classification of human activities such as: walking, climbing
stairs, running, clapping, sitting, sleeping, etc. In this work we evaluate the
feasibility of an automatic system, based on current smartwatches, which is
able to recognize when a subject is washing or rubbing its hands, in order to
monitor parameters such as frequency and duration, and to evaluate the
effectiveness of the gesture. Our preliminary results show a classification
accuracy of about 95% and of about 94% for respectively deep and standard
learning techniques.

    

### [[2107.13419] Vowel-based Meeteilon dialect identification using a Random Forest classifier](http://arxiv.org/abs/2107.13419)


  This paper presents a vowel-based dialect identification system for
Meeteilon. For this work, a vowel dataset is created by using Meeteilon Speech
Corpora available at Linguistic Data Consortium for Indian Languages (LDC-IL).
Spectral features such as formant frequencies (F1, F1 and F3) and prosodic
features such as pitch (F0), energy, intensity and segment duration values are
extracted from monophthong vowel sounds. Random forest classifier, a decision
tree-based ensemble algorithm is used for classification of three major
dialects of Meeteilon namely, Imphal, Kakching and Sekmai. Model has shown an
average dialect identification performance in terms of accuracy of around
61.57%. The role of spectral and prosodic features are found to be significant
in Meeteilon dialect classification.

    

### [[2107.13423] A Signal Detection Scheme Based on Deep Learning in OFDM Systems](http://arxiv.org/abs/2107.13423)


  Channel estimation and signal detection are essential steps to ensure the
quality of end-to-end communication in orthogonal frequency-division
multiplexing (OFDM) systems. In this paper, we develop a DDLSD approach, i.e.,
Data-driven Deep Learning for Signal Detection in OFDM systems. First, the OFDM
system model is established. Then, the long short-term memory (LSTM) is
introduced into the OFDM system model. Wireless channel data is generated
through simulation, the preprocessed time series feature information is input
into the LSTM to complete the offline training. Finally, the trained model is
used for online recovery of transmitted signal. The difference between this
scheme and existing OFDM receiver is that explicit estimated channel state
information (CSI) is transformed into invisible estimated CSI, and the transmit
symbol is directly restored. Simulation results show that the DDLSD scheme
outperforms the existing traditional methods in terms of improving channel
estimation and signal detection performance.

    

### [[2107.13430] Kernel Density Estimation by Stagewise Algorithm with a Simple Dictionary](http://arxiv.org/abs/2107.13430)


  This paper studies kernel density estimation by stagewise minimization
algorithm with a simple dictionary on U-divergence. We randomly split an i.i.d.
sample into the two disjoint sets, one to be used for constructing the kernels
in the dictionary and the other for evaluating the estimator, and implement the
algorithm. The resulting estimator brings us data-adaptive weighting parameters
and bandwidth matrices, and realizes a sparse representation of kernel density
estimation. We present the non-asymptotic error bounds of our estimator and
confirm its performance by simulations compared with the direct plug-in
bandwidth matrices and the reduced set density estimator.

    

### [[2107.13433] Functorial String Diagrams for Reverse-Mode Automatic Differentiation](http://arxiv.org/abs/2107.13433)


  We enhance the calculus of string diagrams for monoidal categories with
hierarchical features in order to capture closed monoidal (and cartesian
closed) structure. Using this new syntax we formulate an automatic
differentiation algorithm for (applied) simply typed lambda calculus in the
style of [Pearlmutter and Siskind 2008] and we prove for the first time its
soundness. To give an efficient yet principled implementation of the AD
algorithm we define a sound and complete representation of hierarchical string
diagrams as a class of hierarchical hypergraphs we call hypernets.

    

### [[2107.13449] Self-learning Emulators and Eigenvector Continuation](http://arxiv.org/abs/2107.13449)


  Emulators that can bypass computationally expensive scientific calculations
with high accuracy and speed can enable new studies of fundamental science as
well as more potential applications. In this work we focus on solving a system
of constraint equations efficiently using a new machine learning approach that
we call self-learning emulation. A self-learning emulator is an active learning
protocol that can rapidly solve a system of equations over some range of
control parameters. The key ingredient is a fast estimate of the emulator error
that becomes progressively more accurate as the emulator improves. This
acceleration is possible because the emulator itself is used to estimate the
error, and we illustrate with two examples. The first uses cubic spline
interpolation to find the roots of a polynomial with variable coefficients. The
second example uses eigenvector continuation to find the eigenvectors and
eigenvalues of a large Hamiltonian matrix that depends on several control
parameters. We envision future applications of self-learning emulators for
solving systems of algebraic equations, linear and nonlinear differential
equations, and linear and nonlinear eigenvalue problems.

    

### [[2107.13459] Surrogate Model-Based Explainability Methods for Point Cloud NNs](http://arxiv.org/abs/2107.13459)


  In the field of autonomous driving and robotics, point clouds are showing
their excellent real-time performance as raw data from most of the mainstream
3D sensors. Therefore, point cloud neural networks have become a popular
research direction in recent years. So far, however, there has been little
discussion about the explainability of deep neural networks for point clouds.
In this paper, we propose new explainability approaches for point cloud deep
neural networks based on local surrogate model-based methods to show which
components make the main contribution to the classification. Moreover, we
propose a quantitative validation method for explainability methods of point
clouds which enhances the persuasive power of explainability by dropping the
most positive or negative contributing features and monitoring how the
classification scores of specific categories change. To enable an intuitive
explanation of misclassified instances, we display features with confounding
contributions. Our new explainability approach provides a fairly accurate, more
intuitive and widely applicable explanation for point cloud classification
tasks. Our code is available at this https URL


### [[2107.13467] Recursively Conditional Gaussian for Ordinal Unsupervised Domain Adaptation](http://arxiv.org/abs/2107.13467)


  There has been a growing interest in unsupervised domain adaptation (UDA) to
alleviate the data scalability issue, while the existing works usually focus on
classifying independently discrete labels. However, in many tasks (e.g.,
medical diagnosis), the labels are discrete and successively distributed. The
UDA for ordinal classification requires inducing non-trivial ordinal
distribution prior to the latent space. Target for this, the partially ordered
set (poset) is defined for constraining the latent vector. Instead of the
typically i.i.d. Gaussian latent prior, in this work, a recursively conditional
Gaussian (RCG) set is proposed for ordered constraint modeling, which admits a
tractable joint distribution prior. Furthermore, we are able to control the
density of content vectors that violate the poset constraint by a simple
"three-sigma rule". We explicitly disentangle the cross-domain images into a
shared ordinal prior induced ordinal content space and two separate
source/target ordinal-unrelated spaces, and the self-training is worked on the
shared space exclusively for ordinal-aware domain alignment. Extensive
experiments on UDA medical diagnoses and facial age estimation demonstrate its
effectiveness.

    

### [[2107.13469] Adversarial Unsupervised Domain Adaptation with Conditional and Label Shift: Infer, Align and Iterate](http://arxiv.org/abs/2107.13469)


  In this work, we propose an adversarial unsupervised domain adaptation (UDA)
approach with the inherent conditional and label shifts, in which we aim to
align the distributions w.r.t. both $p(x|y)$ and $p(y)$. Since the label is
inaccessible in the target domain, the conventional adversarial UDA assumes
$p(y)$ is invariant across domains, and relies on aligning $p(x)$ as an
alternative to the $p(x|y)$ alignment. To address this, we provide a thorough
theoretical and empirical analysis of the conventional adversarial UDA methods
under both conditional and label shifts, and propose a novel and practical
alternative optimization scheme for adversarial UDA. Specifically, we infer the
marginal $p(y)$ and align $p(x|y)$ iteratively in the training, and precisely
align the posterior $p(y|x)$ in testing. Our experimental results demonstrate
its effectiveness on both classification and segmentation UDA, and partial UDA.

    

### [[2107.13472] Reenvisioning Collaborative Filtering vs Matrix Factorization](http://arxiv.org/abs/2107.13472)


  Collaborative filtering models based on matrix factorization and learned
similarities using Artificial Neural Networks (ANNs) have gained significant
attention in recent years. This is, in part, because ANNs have demonstrated
good results in a wide variety of recommendation tasks. The introduction of
ANNs within the recommendation ecosystem has been recently questioned, raising
several comparisons in terms of efficiency and effectiveness. One aspect most
of these comparisons have in common is their focus on accuracy, neglecting
other evaluation dimensions important for the recommendation, such as novelty,
diversity, or accounting for biases. We replicate experiments from three papers
that compare Neural Collaborative Filtering (NCF) and Matrix Factorization
(MF), to extend the analysis to other evaluation dimensions. Our contribution
shows that the experiments are entirely reproducible, and we extend the study
including other accuracy metrics and two statistical hypothesis tests. We
investigated the Diversity and Novelty of the recommendations, showing that MF
provides a better accuracy also on the long tail, although NCF provides a
better item coverage and more diversified recommendations. We discuss the bias
effect generated by the tested methods. They show a relatively small bias, but
other recommendation baselines, with competitive accuracy performance,
consistently show to be less affected by this issue. This is the first work, to
the best of our knowledge, where several evaluation dimensions have been
explored for an array of SOTA algorithms covering recent adaptations of ANNs
and MF. Hence, we show the potential these techniques may have on
beyond-accuracy evaluation while analyzing the effect on reproducibility these
complementary dimensions may spark. Available at
this http URL


### [[2107.13473] The Portiloop: a deep learning-based open science tool for closed-loop brain stimulation](http://arxiv.org/abs/2107.13473)


  Electroencephalography (EEG) is a method of measuring the brain's electrical
activity, using non-invasive scalp electrodes. In this article, we propose the
Portiloop, a deep learning-based portable and low-cost device enabling the
neuroscience community to capture EEG, process it in real time, detect patterns
of interest, and respond with precisely-timed stimulation. The core of the
Portiloop is a System on Chip composed of an Analog to Digital Converter (ADC)
and a Field-Programmable Gate Array (FPGA). After being converted to digital by
the ADC, the EEG signal is processed in the FPGA. The FPGA contains an ad-hoc
Artificial Neural Network (ANN) with convolutional and recurrent units,
directly implemented in hardware. The output of the ANN is then used to trigger
the user-defined feedback. We use the Portiloop to develop a real-time sleep
spindle stimulating application, as a case study. Sleep spindles are a specific
type of transient oscillation ($\sim$2.5 s, 12-16 Hz) that are observed in EEG
recordings, and are related to memory consolidation during sleep. We tested the
Portiloop's capacity to detect and stimulate sleep spindles in real time using
an existing database of EEG sleep recordings. With 71% for both precision and
recall as compared with expert labels, the system is able to stimulate spindles
within $\sim$300 ms of their onset, enabling experimental manipulation of early
the entire spindle. The Portiloop can be extended to detect and stimulate other
neural events in EEG. It is fully available to the research community as an
open science project.

    

### [[2107.13477] Satisfiability and Synthesis Modulo Oracles](http://arxiv.org/abs/2107.13477)


  In classic program synthesis algorithms, such as counterexample-guided
inductive synthesis (CEGIS), the algorithms alternate between a synthesis phase
and an oracle (verification) phase. Many synthesis algorithms use a white-box
oracle based on satisfiability modulo theory (SMT) solvers to provide
counterexamples. But what if a white-box oracle is either not available or not
easy to work with? We present a framework for solving a general class of
oracle-guided synthesis problems which we term synthesis modulo oracles. In
this setting, oracles may be black boxes with a query-response interface
defined by the synthesis problem. As a necessary component of this framework,
we also formalize the problem of satisfiability modulo theories and oracles,
and present an algorithm for solving this problem. We implement a prototype
solver for satisfiability and synthesis modulo oracles and demonstrate that, by
using oracles that execute functions not easily modeled in SMT-constraints,
such as recursive functions or oracles that incorporate compilation and
execution of code, SMTO and SyMO are able to solve problems beyond the
abilities of standard SMT and synthesis solvers.

    

### [[2107.13490] MARViN -- Multiple Arithmetic Resolutions Vacillating in Neural Networks](http://arxiv.org/abs/2107.13490)


  Quantization is a technique for reducing deep neural networks (DNNs) training
and inference times, which is crucial for training in resource constrained
environments or time critical inference applications. State-of-the-art (SOTA)
quantization approaches focus on post-training quantization, i.e. quantization
of pre-trained DNNs for speeding up inference. Very little work on quantized
training exists, which neither al-lows dynamic intra-epoch precision switches
nor em-ploys an information theory based switching heuristic. Usually, existing
approaches require full precision refinement afterwards and enforce a global
word length across the whole DNN. This leads to suboptimal quantization
mappings and resource usage. Recognizing these limits, we introduce MARViN, a
new quantized training strategy using information theory-based intra-epoch
precision switching, which decides on a per-layer basis which precision should
be used in order to minimize quantization-induced information loss. Note that
any quantization must leave enough precision such that future learning steps do
not suffer from vanishing gradients. We achieve an average speedup of 1.86
compared to a float32 basis while limiting mean accuracy degradation on
AlexNet/ResNet to only -0.075%.

    

### [[2107.13491] Models of Computational Profiles to Study the Likelihood of DNN Metamorphic Test Cases](http://arxiv.org/abs/2107.13491)


  Neural network test cases are meant to exercise different reasoning paths in
an architecture and used to validate the prediction outcomes. In this paper, we
introduce "computational profiles" as vectors of neuron activation levels. We
investigate the distribution of computational profile likelihood of metamorphic
test cases with respect to the likelihood distributions of training, test and
error control cases. We estimate the non-parametric probability densities of
neuron activation levels for each distinct output class. Probabilities are
inferred using training cases only, without any additional knowledge about
metamorphic test cases. Experiments are performed by training a network on the
MNIST Fashion library of images and comparing prediction likelihoods with those
obtained from error control-data and from metamorphic test cases. Experimental
results show that the distributions of computational profile likelihood for
training and test cases are somehow similar, while the distribution of the
random-noise control-data is always remarkably lower than the observed one for
the training and testing sets. In contrast, metamorphic test cases show a
prediction likelihood that lies in an extended range with respect to training,
tests, and random noise. Moreover, the presented approach allows the
independent assessment of different training classes and experiments to show
that some of the classes are more sensitive to misclassifying metamorphic test
cases than other classes. In conclusion, metamorphic test cases represent very
aggressive tests for neural network architectures. Furthermore, since
metamorphic test cases force a network to misclassify those inputs whose
likelihood is similar to that of training cases, they could also be considered
as adversarial attacks that evade defenses based on computational profile
likelihood evaluation.

    

### [[2107.13505] Deep Recurrent Semi-Supervised EEG Representation Learning for Emotion Recognition](http://arxiv.org/abs/2107.13505)


  EEG-based emotion recognition often requires sufficient labeled training
samples to build an effective computational model. Labeling EEG data, on the
other hand, is often expensive and time-consuming. To tackle this problem and
reduce the need for output labels in the context of EEG-based emotion
recognition, we propose a semi-supervised pipeline to jointly exploit both
unlabeled and labeled data for learning EEG representations. Our
semi-supervised framework consists of both unsupervised and supervised
components. The unsupervised part maximizes the consistency between original
and reconstructed input data using an autoencoder, while simultaneously the
supervised part minimizes the cross-entropy between the input and output
labels. We evaluate our framework using both a stacked autoencoder and an
attention-based recurrent autoencoder. We test our framework on the large-scale
SEED EEG dataset and compare our results with several other popular
semi-supervised methods. Our semi-supervised framework with a deep
attention-based recurrent autoencoder consistently outperforms the benchmark
methods, even when small sub-sets (3\%, 5\% and 10\%) of the output labels are
available during training, achieving a new state-of-the-art semi-supervised
performance.

    

### [[2107.13507] The Reasonable Crowd: Towards evidence-based and interpretable models of driving behavior](http://arxiv.org/abs/2107.13507)


  Autonomous vehicles must balance a complex set of objectives. There is no
consensus on how they should do so, nor on a model for specifying a desired
driving behavior. We created a dataset to help address some of these questions
in a limited operating domain. The data consists of 92 traffic scenarios, with
multiple ways of traversing each scenario. Multiple annotators expressed their
preference between pairs of scenario traversals. We used the data to compare an
instance of a rulebook, carefully hand-crafted independently of the dataset,
with several interpretable machine learning models such as Bayesian networks,
decision trees, and logistic regression trained on the dataset. To compare
driving behavior, these models use scores indicating by how much different
scenario traversals violate each of 14 driving rules. The rules are
interpretable and designed by subject-matter experts. First, we found that
these rules were enough for these models to achieve a high classification
accuracy on the dataset. Second, we found that the rulebook provides high
interpretability without excessively sacrificing performance. Third, the data
pointed to possible improvements in the rulebook and the rules, and to
potential new rules. Fourth, we explored the interpretability vs performance
trade-off by also training non-interpretable models such as a random forest.
Finally, we make the dataset publicly available to encourage a discussion from
the wider community on behavior specification for AVs. Please find it at
this http URL.

    

### [[2107.13508] Uncertainty-Aware Credit Card Fraud Detection Using Deep Learning](http://arxiv.org/abs/2107.13508)


  Countless research works of deep neural networks (DNNs) in the task of credit
card fraud detection have focused on improving the accuracy of point
predictions and mitigating unwanted biases by building different network
architectures or learning models. Quantifying uncertainty accompanied by point
estimation is essential because it mitigates model unfairness and permits
practitioners to develop trustworthy systems which abstain from suboptimal
decisions due to low confidence. Explicitly, assessing uncertainties associated
with DNNs predictions is critical in real-world card fraud detection settings
for characteristic reasons, including (a) fraudsters constantly change their
strategies, and accordingly, DNNs encounter observations that are not generated
by the same process as the training distribution, (b) owing to the
time-consuming process, very few transactions are timely checked by
professional experts to update DNNs. Therefore, this study proposes three
uncertainty quantification (UQ) techniques named Monte Carlo dropout, ensemble,
and ensemble Monte Carlo dropout for card fraud detection applied on
transaction data. Moreover, to evaluate the predictive uncertainty estimates,
UQ confusion matrix and several performance metrics are utilized. Through
experimental results, we show that the ensemble is more effective in capturing
uncertainty corresponding to generated predictions. Additionally, we
demonstrate that the proposed UQ methods provide extra insight to the point
predictions, leading to elevate the fraud prevention process.

    

### [[2107.13522] Supervised Learning and the Finite-Temperature String Method for Computing Committor Functions and Reaction Rates](http://arxiv.org/abs/2107.13522)


  A central object in the computational studies of rare events is the committor
function. Though costly to compute, the committor function encodes complete
mechanistic information of the processes involving rare events, including
reaction rates and transition-state ensembles. Under the framework of
transition path theory (TPT), recent work [1] proposes an algorithm where a
feedback loop couples a neural network that models the committor function with
importance sampling, mainly umbrella sampling, which collects data needed for
adaptive training. In this work, we show additional modifications are needed to
improve the accuracy of the algorithm. The first modification adds elements of
supervised learning, which allows the neural network to improve its prediction
by fitting to sample-mean estimates of committor values obtained from short
molecular dynamics trajectories. The second modification replaces the
committor-based umbrella sampling with the finite-temperature string (FTS)
method, which enables homogeneous sampling in regions where transition pathways
are located. We test our modifications on low-dimensional systems with
non-convex potential energy where reference solutions can be found via
analytical or the finite element methods, and show how combining supervised
learning and the FTS method yields accurate computation of committor functions
and reaction rates. We also provide an error analysis for algorithms that use
the FTS method, using which reaction rates can be accurately estimated during
training with a small number of samples.

    

### [[2107.13530] Continual-wav2vec2: an Application of Continual Learning for Self-Supervised Automatic Speech Recognition](http://arxiv.org/abs/2107.13530)


  We present a method for continual learning of speech representations for
multiple languages using self-supervised learning (SSL) and applying these for
automatic speech recognition. There is an abundance of unannotated speech, so
creating self-supervised representations from raw audio and finetuning on a
small annotated datasets is a promising direction to build speech recognition
systems. Wav2vec models perform SSL on raw audio in a pretraining phase and
then finetune on a small fraction of annotated data. SSL models have produced
state of the art results for ASR. However, these models are very expensive to
pretrain with self-supervision. We tackle the problem of learning new language
representations continually from audio without forgetting a previous language
representation. We use ideas from continual learning to transfer knowledge from
a previous task to speed up pretraining a new language task. Our
continual-wav2vec2 model can decrease pretraining times by 32% when learning a
new language task, and learn this new audio-language representation without
forgetting previous language representation.

    

### [[2107.13545] ReLMM: Practical RL for Learning Mobile Manipulation Skills Using Only Onboard Sensors](http://arxiv.org/abs/2107.13545)


  In this paper, we study how robots can autonomously learn skills that require
a combination of navigation and grasping. Learning robotic skills in the real
world remains challenging without large-scale data collection and supervision.
Our aim is to devise a robotic reinforcement learning system for learning
navigation and manipulation together, in an \textit{autonomous} way without
human intervention, enabling continual learning under realistic assumptions.
Specifically, our system, ReLMM, can learn continuously on a real-world
platform without any environment instrumentation, without human intervention,
and without access to privileged information, such as maps, objects positions,
or a global view of the environment. Our method employs a modularized policy
with components for manipulation and navigation, where uncertainty over the
manipulation success drives exploration for the navigation controller, and the
manipulation module provides rewards for navigation. We evaluate our method on
a room cleanup task, where the robot must navigate to and pick up items of
scattered on the floor. After a grasp curriculum training phase, ReLMM can
learn navigation and grasping together fully automatically, in around 40 hours
of real-world training.

    

### [[1808.09670] Proximal boosting and variants](http://arxiv.org/abs/1808.09670)


  Gradient boosting is a prediction method that iteratively combines weak
learners to produce a complex and accurate model. From an optimization point of
view, the learning procedure of gradient boosting mimics a gradient descent on
a functional variable. This paper proposes to build upon the proximal point
algorithm, when the empirical risk to minimize is not differentiable, in order
to introduce a novel boosting approach, called proximal boosting. Besides being
motivated by non-differentiable optimization, the proposed algorithm benefits
from algorithmic improvements such as controlling the approximation error and
Nesterov's acceleration, in the same way as gradient boosting [Grubb and
Bagnell, 2011, Biau et al., 2018]. This leads to two variants, respectively
called residual proximal boosting and accelerated proximal boosting.
Theoretical convergence is proved for the first two procedures under different
hypotheses on the empirical risk and advantages of leveraging proximal methods
for boosting are illustrated by numerical experiments on simulated and
real-world data. In particular, we exhibit a favorable comparison over gradient
boosting regarding convergence rate and prediction accuracy.

    

### [[1811.11891] Manifold Coordinates with Physical Meaning](http://arxiv.org/abs/1811.11891)


  Manifold embedding algorithms map high-dimensional data down to coordinates
in a much lower-dimensional space. One of the aims of dimension reduction is to
find intrinsic coordinates that describe the data manifold. The coordinates
returned by the embedding algorithm are abstract, and finding their physical or
domain-related meaning is not formalized and often left to domain experts. This
paper studies the problem of recovering the meaning of the new low-dimensional
representation in an automatic, principled fashion. We propose a method to
explain embedding coordinates of a manifold as non-linear compositions of
functions from a user-defined dictionary. We show that this problem can be set
up as a sparse linear Group Lasso recovery problem, find sufficient recovery
conditions, and demonstrate its effectiveness on data.

    

### [[1901.09997] Quasi-Newton Methods for Machine Learning: Forget the Past, Just Sample](http://arxiv.org/abs/1901.09997)


  We present two sampled quasi-Newton methods (sampled LBFGS and sampled LSR1)
for solving empirical risk minimization problems that arise in machine
learning. Contrary to the classical variants of these methods that sequentially
build Hessian or inverse Hessian approximations as the optimization progresses,
our proposed methods sample points randomly around the current iterate at every
iteration to produce these approximations. As a result, the approximations
constructed make use of more reliable (recent and local) information, and do
not depend on past iterate information that could be significantly stale. Our
proposed algorithms are efficient in terms of accessed data points (epochs) and
have enough concurrency to take advantage of parallel/distributed computing
environments. We provide convergence guarantees for our proposed methods.
Numerical tests on a toy classification problem as well as on popular
benchmarking binary classification and neural network training tasks reveal
that the methods outperform their classical variants.

    

### [[1905.12278] An Inertial Newton Algorithm for Deep Learning](http://arxiv.org/abs/1905.12278)


  We introduce a new second-order inertial optimization method for machine
learning called INNA. It exploits the geometry of the loss function while only
requiring stochastic approximations of the function values and the generalized
gradients. This makes INNA fully implementable and adapted to large-scale
optimization problems such as the training of deep neural networks. The
algorithm combines both gradient-descent and Newton-like behaviors as well as
inertia. We prove the convergence of INNA for most deep learning problems. To
do so, we provide a well-suited framework to analyze deep learning loss
functions involving tame optimization in which we study a continuous dynamical
system together with its discrete stochastic approximations. We prove sublinear
convergence for the continuous-time differential inclusion which underlies our
algorithm. Additionally, we also show how standard optimization mini-batch
methods applied to non-smooth non-convex problems can yield a certain type of
spurious stationary points never discussed before. We address this issue by
providing a theoretical framework around the new idea of $D$-criticality; we
then give a simple asymptotic analysis of INNA. Our algorithm allows for using
an aggressive learning rate of $o(1/\log k)$. From an empirical viewpoint, we
show that INNA returns competitive results with respect to state of the art
(stochastic gradient descent, ADAGRAD, ADAM) on popular deep learning benchmark
problems.

    

### [[1906.00570] Clustering by Orthogonal NMF Model and Non-Convex Penalty Optimization](http://arxiv.org/abs/1906.00570)


  The non-negative matrix factorization (NMF) model with an additional
orthogonality constraint on one of the factor matrices, called the orthogonal
NMF (ONMF), has been found a promising clustering model and can outperform the
classical K-means. However, solving the ONMF model is a challenging
optimization problem because the coupling of the orthogonality and
non-negativity constraints introduces a mixed combinatorial aspect into the
problem due to the determination of the correct status of the variables
(positive or zero). Most of the existing methods directly deal with the
orthogonality constraint in its original form via various optimization
techniques, but are not scalable for large-scale problems. In this paper, we
propose a new ONMF based clustering formulation that equivalently transforms
the orthogonality constraint into a set of norm-based non-convex equality
constraints. We then apply a non-convex penalty (NCP) approach to add them to
the objective as penalty terms, leading to a problem that is efficiently
solvable. One smooth penalty formulation and one non-smooth penalty formulation
are respectively studied. We build theoretical conditions for the penalized
problems to provide feasible stationary solutions to the ONMF based clustering
problem, as well as proposing efficient algorithms for solving the penalized
problems of the two NCP methods. Experimental results based on both synthetic
and real datasets are presented to show that the proposed NCP methods are
computationally time efficient, and either match or outperform the existing
K-means and ONMF based methods in terms of the clustering performance.

    

### [[1907.01845] FairNAS: Rethinking Evaluation Fairness of Weight Sharing Neural Architecture Search](http://arxiv.org/abs/1907.01845)


  One of the most critical problems in weight-sharing neural architecture
search is the evaluation of candidate models within a predefined search space.
In practice, a one-shot supernet is trained to serve as an evaluator. A
faithful ranking certainly leads to more accurate searching results. However,
current methods are prone to making misjudgments. In this paper, we prove that
their biased evaluation is due to inherent unfairness in the supernet training.
In view of this, we propose two levels of constraints: expectation fairness and
strict fairness. Particularly, strict fairness ensures equal optimization
opportunities for all choice blocks throughout the training, which neither
overestimates nor underestimates their capacity. We demonstrate that this is
crucial for improving the confidence of models' ranking. Incorporating the
one-shot supernet trained under the proposed fairness constraints with a
multi-objective evolutionary search algorithm, we obtain various
state-of-the-art models, e.g., FairNAS-A attains 77.5% top-1 validation
accuracy on ImageNet. The models and their evaluation codes are made publicly
available online this http URL .

    

### [[2001.11988] Consensus-Based Optimization on the Sphere: Convergence to Global Minimizers and Machine Learning](http://arxiv.org/abs/2001.11988)


  We investigate the implementation of a new stochastic Kuramoto-Vicsek-type
model for global optimization of nonconvex functions on the sphere. This model
belongs to the class of Consensus-Based Optimization. In fact, particles move
on the sphere driven by a drift towards an instantaneous consensus point, which
is computed as a convex combination of particle locations, weighted by the cost
function according to Laplace's principle, and it represents an approximation
to a global minimizer. The dynamics is further perturbed by a random vector
field to favor exploration, whose variance is a function of the distance of the
particles to the consensus point. In particular, as soon as the consensus is
reached the stochastic component vanishes. The main results of this paper are
about the proof of convergence of the numerical scheme to global minimizers
provided conditions of well-preparation of the initial datum. The proof
combines previous results of mean-field limit with a novel asymptotic analysis,
and classical convergence results of numerical methods for SDE. We present
several numerical experiments, which show that the algorithm proposed in the
present paper scales well with the dimension and is extremely versatile. To
quantify the performances of the new approach, we show that the algorithm is
able to perform essentially as good as ad hoc state of the art methods in
challenging problems in signal processing and machine learning, namely the
phase retrieval problem and the robust subspace detection.

    

### [[2003.00826] Realistic River Image Synthesis using Deep Generative Adversarial Networks](http://arxiv.org/abs/2003.00826)


  In this paper, we demonstrated a practical application of realistic river
image generation using deep learning. Specifically, we explored a generative
adversarial network (GAN) model capable of generating high-resolution and
realistic river images that can be used to support modeling and analysis in
surface water estimation, river meandering, wetland loss, and other
hydrological research studies. First, we have created an extensive repository
of overhead river images to be used in training. Second, we incorporated the
Progressive Growing GAN (PGGAN), a network architecture that iteratively trains
smaller-resolution GANs to gradually build up to a very high resolution to
generate high quality (i.e., 1024x1024) synthetic river imagery. With simpler
GAN architectures, difficulties arose in terms of exponential increase of
training time and vanishing/exploding gradient issues, which the PGGAN
implementation seemed to significantly reduce. The results presented in this
study show great promise in generating high-quality images and capturing the
details of river structure and flow to support hydrological research, which
often requires extensive imagery for model performance.

    

### [[2006.02951] CiwGAN and fiwGAN: Encoding information in acoustic data to model lexical learning with Generative Adversarial Networks](http://arxiv.org/abs/2006.02951)


  How can deep neural networks encode information that corresponds to words in
human speech into raw acoustic data? This paper proposes two neural network
architectures for modeling unsupervised lexical learning from raw acoustic
inputs, ciwGAN (Categorical InfoWaveGAN) and fiwGAN (Featural InfoWaveGAN),
that combine a Deep Convolutional GAN architecture for audio data (WaveGAN;
arXiv:1705.07904) with an information theoretic extension of GAN -- InfoGAN
(arXiv:1606.03657), and propose a new latent space structure that can model
featural learning simultaneously with a higher level classification and allows
for a very low-dimension vector representation of lexical items. Lexical
learning is modeled as emergent from an architecture that forces a deep neural
network to output data such that unique information is retrievable from its
acoustic outputs. The networks trained on lexical items from TIMIT learn to
encode unique information corresponding to lexical items in the form of
categorical variables in their latent space. By manipulating these variables,
the network outputs specific lexical items. The network occasionally outputs
innovative lexical items that violate training data, but are linguistically
interpretable and highly informative for cognitive modeling and neural network
interpretability. Innovative outputs suggest that phonetic and phonological
representations learned by the network can be productively recombined and
directly paralleled to productivity in human speech: a fiwGAN network trained
on `suit' and `dark' outputs innovative `start', even though it never saw
`start' or even a [st] sequence in the training data. We also argue that
setting latent featural codes to values well beyond training range results in
almost categorical generation of prototypical lexical items and reveals
underlying values of each latent code.

    

### [[2006.07700] Defensive Approximation: Enhancing CNNs Security through Approximate Computing](http://arxiv.org/abs/2006.07700)


  In the past few years, an increasing number of machine-learning and deep
learning structures, such as Convolutional Neural Networks (CNNs), have been
applied to solving a wide range of real-life problems. However, these
architectures are vulnerable to adversarial attacks. In this paper, we propose
for the first time to use hardware-supported approximate computing to improve
the robustness of machine learning classifiers. We show that our approximate
computing implementation achieves robustness across a wide range of attack
scenarios. Specifically, for black-box and grey-box attack scenarios, we show
that successful adversarial attacks against the exact classifier have poor
transferability to the approximate implementation. Surprisingly, the robustness
advantages also apply to white-box attacks where the attacker has access to the
internal implementation of the approximate classifier. We explain some of the
possible reasons for this robustness through analysis of the internal operation
of the approximate implementation. Furthermore, our approximate computing model
maintains the same level in terms of classification accuracy, does not require
retraining, and reduces resource utilization and energy consumption of the CNN.
We conducted extensive experiments on a set of strong adversarial attacks; We
empirically show that the proposed implementation increases the robustness of a
LeNet-5 and an Alexnet CNNs by up to 99% and 87%, respectively for strong
grey-box adversarial attacks along with up to 67% saving in energy consumption
due to the simpler nature of the approximate logic. We also show that a
white-box attack requires a remarkably higher noise budget to fool the
approximate classifier, causing an average of 4db degradation of the PSNR of
the input image relative to the images that succeed in fooling the exact
classifier

    

### [[2007.00533] Partial Recovery in the Graph Alignment Problem](http://arxiv.org/abs/2007.00533)


  In this paper, we consider the graph alignment problem, which is the problem
of recovering, given two graphs, a one-to-one mapping between nodes that
maximizes edge overlap. This problem can be viewed as a noisy version of the
well-known graph isomorphism problem and appears in many applications,
including social network deanonymization and cellular biology. Our focus here
is on partial recovery, i.e., we look for a one-to-one mapping which is correct
on a fraction of the nodes of the graph rather than on all of them, and we
assume that the two input graphs to the problem are correlated
Erdős-Rényi graphs of parameters $(n,q,s)$. Our main contribution is then
to give necessary and sufficient conditions on $(n,q,s)$ under which partial
recovery is possible with high probability as the number of nodes $n$ goes to
infinity. In particular, we show that it is possible to achieve partial
recovery in the $nqs=\Theta(1)$ regime under certain additional assumptions. An
interesting byproduct of the analysis techniques we develop to obtain the
sufficiency result in the partial recovery setting is a tighter analysis of the
maximum likelihood estimator for the graph alignment problem, which leads to
improved sufficient conditions for exact recovery.

    

### [[2007.08032] When and how do CNNs generalize to out-of-distribution category-viewpoint combinations?](http://arxiv.org/abs/2007.08032)


  Object recognition and viewpoint estimation lie at the heart of visual
understanding. Recent works suggest that convolutional neural networks (CNNs)
fail to generalize to out-of-distribution (OOD) category-viewpoint
combinations, ie. combinations not seen during training. In this paper, we
investigate when and how such OOD generalization may be possible by evaluating
CNNs trained to classify both object category and 3D viewpoint on OOD
combinations, and identifying the neural mechanisms that facilitate such OOD
generalization. We show that increasing the number of in-distribution
combinations (ie. data diversity) substantially improves generalization to OOD
combinations, even with the same amount of training data. We compare learning
category and viewpoint in separate and shared network architectures, and
observe starkly different trends on in-distribution and OOD combinations, ie.
while shared networks are helpful in-distribution, separate networks
significantly outperform shared ones at OOD combinations. Finally, we
demonstrate that such OOD generalization is facilitated by the neural mechanism
of specialization, ie. the emergence of two types of neurons -- neurons
selective to category and invariant to viewpoint, and vice versa.

    

### [[2007.08093] Data-driven effective model shows a liquid-like deep learning](http://arxiv.org/abs/2007.08093)


  The geometric structure of an optimization landscape is argued to be
fundamentally important to support the success of deep neural network learning.
A direct computation of the landscape beyond two layers is hard. Therefore, to
capture the global view of the landscape, an interpretable model of the
network-parameter (or weight) space must be established. However, the model is
lacking so far. Furthermore, it remains unknown what the landscape looks like
for deep networks of binary synapses, which plays a key role in robust and
energy efficient neuromorphic computation. Here, we propose a statistical
mechanics framework by directly building a least structured model of the
high-dimensional weight space, considering realistic structured data,
stochastic gradient descent training, and the computational depth of neural
networks. We also consider whether the number of network parameters outnumbers
the number of supplied training data, namely, over- or under-parametrization.
Our least structured model reveals that the weight spaces of the
under-parametrization and over-parameterization cases belong to the same class,
in the sense that these weight spaces are well-connected without any
hierarchical clustering structure. In contrast, the shallow-network has a
broken weight space, characterized by a discontinuous phase transition, thereby
clarifying the benefit of depth in deep learning from the angle of high
dimensional geometry. Our effective model also reveals that inside a deep
network, there exists a liquid-like central part of the architecture in the
sense that the weights in this part behave as randomly as possible, providing
algorithmic implications. Our data-driven model thus provides a statistical
mechanics insight about why deep learning is unreasonably effective in terms of
the high-dimensional weight space, and how deep networks are different from
shallow ones.

    

### [[2009.12711] Local and non-local dependency learning and emergence of rule-like representations in speech data by Deep Convolutional Generative Adversarial Networks](http://arxiv.org/abs/2009.12711)


  This paper argues that training GANs on local and non-local dependencies in
speech data offers insights into how deep neural networks discretize continuous
data and how symbolic-like rule-based morphophonological processes emerge in a
deep convolutional architecture. Acquisition of speech has recently been
modeled as a dependency between latent space and data generated by GANs in
Beguš (2020b; arXiv:2006.03965), who models learning of a simple local
allophonic distribution. We extend this approach to test learning of local and
non-local phonological processes that include approximations of morphological
processes. We further parallel outputs of the model to results of a behavioral
experiment where human subjects are trained on the data used for training the
GAN network. Four main conclusions emerge: (i) the networks provide useful
information for computational models of speech acquisition even if trained on a
comparatively small dataset of an artificial grammar learning experiment; (ii)
local processes are easier to learn than non-local processes, which matches
both behavioral data in human subjects and typology in the world's languages.
This paper also proposes (iii) how we can actively observe the network's
progress in learning and explore the effect of training steps on learning
representations by keeping latent space constant across different training
steps. Finally, this paper shows that (iv) the network learns to encode the
presence of a prefix with a single latent variable; by interpolating this
variable, we can actively observe the operation of a non-local phonological
process. The proposed technique for retrieving learning representations has
general implications for our understanding of how GANs discretize continuous
speech data and suggests that rule-like generalizations in the training data
are represented as an interaction between variables in the network's latent
space.

    

### [[2010.00590] Quantifying social organization and political polarization in online platforms](http://arxiv.org/abs/2010.00590)


  Optimism about the Internet's potential to bring the world together has been
tempered by concerns about its role in inflaming the 'culture wars'. Via mass
selection into like-minded groups, online society may be becoming more
fragmented and polarized, particularly with respect to partisan differences.
However, our ability to measure the social makeup of online communities, and in
turn understand the social organization of online platforms, is limited by the
pseudonymous, unstructured, and large-scale nature of digital discussion. We
develop a neural embedding methodology to quantify the positioning of online
communities along social dimensions by leveraging large-scale patterns of
aggregate behaviour. Applying our methodology to 5.1B Reddit comments made in
10K communities over 14 years, we measure how the macroscale community
structure is organized with respect to age, gender, and U.S. political
partisanship. Examining political content, we find Reddit underwent a
significant polarization event around the 2016 U.S. presidential election, and
remained highly polarized for years afterward. Contrary to conventional wisdom,
however, individual-level polarization is rare; the system-level shift in 2016
was disproportionately driven by the arrival of new and newly political users.
Political polarization on Reddit is unrelated to previous activity on the
platform, and is instead temporally aligned with external events. We also
observe a stark ideological asymmetry, with the sharp increase in 2016 being
entirely attributable to changes in right-wing activity. Our methodology is
broadly applicable to the study of online interaction, and our findings have
implications for the design of online platforms, understanding the social
contexts of online behaviour, and quantifying the dynamics and mechanisms of
online polarization.

    

### [[2011.10687] HDR Environment Map Estimation for Real-Time Augmented Reality](http://arxiv.org/abs/2011.10687)


  We present a method to estimate an HDR environment map from a narrow
field-of-view LDR camera image in real-time. This enables perceptually
appealing reflections and shading on virtual objects of any material finish,
from mirror to diffuse, rendered into a real physical environment using
augmented reality. Our method is based on our efficient convolutional neural
network architecture, EnvMapNet, trained end-to-end with two novel losses,
ProjectionLoss for the generated image, and ClusterLoss for adversarial
training. Through qualitative and quantitative comparison to state-of-the-art
methods, we demonstrate that our algorithm reduces the directional error of
estimated light sources by more than 50%, and achieves 3.7 times lower Frechet
Inception Distance (FID). We further showcase a mobile application that is able
to run our neural network model in under 9 ms on an iPhone XS, and render in
real-time, visually coherent virtual objects in previously unseen real-world
environments.

    

### [[2012.14331] A method to integrate and classify normal distributions](http://arxiv.org/abs/2012.14331)


  Univariate and multivariate normal probability distributions are widely used
when modeling decisions under uncertainty. Computing the performance of such
models requires integrating these distributions over specific domains, which
can vary widely across models. Besides some special cases where these integrals
are easy to calculate, there exist no general analytical expressions, standard
numerical methods or software for these integrals. Here we present mathematical
results and open-source software that provide (i) the probability in any domain
of a normal in any dimensions with any parameters, (ii) the probability
density, cumulative distribution, and inverse cumulative distribution of any
function of a normal vector, (iii) the classification errors among any number
of normal distributions, the Bayes-optimal discriminability index and relation
to the operating characteristic, (iv) dimension reduction and visualizations
for such problems, and (v) tests for how reliably these methods may be used on
given data. We demonstrate these tools with vision research applications of
detecting occluding objects in natural scenes, and detecting camouflage.

    

### [[2101.02533] Towards Understanding Learning in Neural Networks with Linear Teachers](http://arxiv.org/abs/2101.02533)


  Can a neural network minimizing cross-entropy learn linearly separable data?
Despite progress in the theory of deep learning, this question remains
unsolved. Here we prove that SGD globally optimizes this learning problem for a
two-layer network with Leaky ReLU activations. The learned network can in
principle be very complex. However, empirical evidence suggests that it often
turns out to be approximately linear. We provide theoretical support for this
phenomenon by proving that if network weights converge to two weight clusters,
this will imply an approximately linear decision boundary. Finally, we show a
condition on the optimization that leads to weight clustering. We provide
empirical results that validate our theoretical analysis.

    

### [[2101.09747] Numerical issues in maximum likelihood parameter estimation for Gaussian process interpolation](http://arxiv.org/abs/2101.09747)


  This article investigates the origin of numerical issues in maximum
likelihood parameter estimation for Gaussian process (GP) interpolation and
investigates simple but effective strategies for improving commonly used
open-source software implementations. This work targets a basic problem but a
host of studies, particularly in the literature of Bayesian optimization, rely
on off-the-shelf GP implementations. For the conclusions of these studies to be
reliable and reproducible, robust GP implementations are critical.

    

### [[2102.01168] Reinforcement Learning for Decision-Making and Control in Power Systems: Tutorial, Review, and Vision](http://arxiv.org/abs/2102.01168)


  With large-scale integration of renewable generation and distributed energy
resources (DERs), modern power systems are confronted with new operational
challenges, such as growing complexity, increasing uncertainty, and aggravating
volatility. Meanwhile, more and more data are becoming available owing to the
widespread deployment of smart meters, smart sensors, and upgraded
communication networks. As a result, data-driven control techniques, especially
reinforcement learning (RL), have attracted surging attention in recent years.
In this paper, we provide a tutorial on various RL techniques and how they can
be applied to decision-making in power systems. We illustrate RL-based models
and solutions in three key applications, frequency regulation, voltage control,
and energy management. We conclude with three critical issues in the
application of RL, i.e., safety, scalability, and data. Several potential
future directions are discussed as well.

    

### [[2102.06758] On automatic extraction of on-street parking spaces using park-out events data](http://arxiv.org/abs/2102.06758)


  This article proposes two different approaches to automatically create a map
for valid on-street car parking spaces. For this, we use car sharing park-out
events data. The first one uses spatial aggregation and the second a machine
learning algorithm. For the former, we chose rasterization and road sectioning;
for the latter we chose decision trees. We compare the results of these
approaches and discuss their advantages and disadvantages. Furthermore, we show
our results for a neighborhood in the city of Berlin and report a
classification accuracy of 91.6\% on the original imbalanced data. Finally, we
discuss further work; from gathering more data over a longer period of time to
fitting spatial Gaussian densities to the data and the usage of apps for manual
validation and annotation of parking spaces to improve ground truth data.

    

### [[2102.13037] SPINN: Sparse, Physics-based, and partially Interpretable Neural Networks for PDEs](http://arxiv.org/abs/2102.13037)


  We introduce a class of Sparse, Physics-based, and partially Interpretable
Neural Networks (SPINN) for solving ordinary and partial differential equations
(PDEs). By reinterpreting a traditional meshless representation of solutions of
PDEs we develop a class of sparse neural network architectures that are
partially interpretable. The SPINN model we propose here serves as a seamless
bridge between two extreme modeling tools for PDEs, namely dense neural network
based methods like Physics Informed Neural Networks (PINNs) and traditional
mesh-free numerical methods, thereby providing a novel means to develop a new
class of hybrid algorithms that build on the best of both these viewpoints. A
unique feature of the SPINN model that distinguishes it from other neural
network based approximations proposed earlier is that it is (i) interpretable,
in a particular sense made precise in the work, and (ii) sparse in the sense
that it has much fewer connections than typical dense neural networks used for
PDEs. Further, the SPINN algorithm implicitly encodes mesh adaptivity and is
able to handle discontinuities in the solutions. In addition, we demonstrate
that Fourier series representations can also be expressed as a special class of
SPINN and propose generalized neural network analogues of Fourier
representations. We illustrate the utility of the proposed method with a
variety of examples involving ordinary differential equations, elliptic,
parabolic, hyperbolic and nonlinear partial differential equations, and an
example in fluid dynamics.

    

### [[2103.01205] Statistically Significant Stopping of Neural Network Training](http://arxiv.org/abs/2103.01205)


  The general approach taken when training deep learning classifiers is to save
the parameters after every few iterations, train until either a human observer
or a simple metric-based heuristic decides the network isn't learning anymore,
and then backtrack and pick the saved parameters with the best validation
accuracy. Simple methods are used to determine if a neural network isn't
learning anymore because, as long as it's well after the optimal values are
found, the condition doesn't impact the final accuracy of the model. However
from a runtime perspective, this is of great significance to the many cases
where numerous neural networks are trained simultaneously (e.g. hyper-parameter
tuning). Motivated by this, we introduce a statistical significance test to
determine if a neural network has stopped learning. This stopping criterion
appears to represent a happy medium compared to other popular stopping
criterions, achieving comparable accuracy to the criterions that achieve the
highest final accuracies in 77% or fewer epochs, while the criterions which
stop sooner do so with an appreciable loss to final accuracy. Additionally, we
use this as the basis of a new learning rate scheduler, removing the need to
manually choose learning rate schedules and acting as a quasi-line search,
achieving superior or comparable empirical performance to existing methods.

    

### [[2103.15244] Rethinking ResNets: Improved Stacking Strategies With High Order Schemes](http://arxiv.org/abs/2103.15244)


  Various deep neural network architectures (DNNs) maintain massive vital
records in computer vision. While drawing attention worldwide, the design of
the overall structure lacks general guidance. Based on the relationship between
DNN design and numerical differential equations, we performed a fair comparison
of the residual design with higher-order perspectives. We show that the widely
used DNN design strategy, constantly stacking a small design (usually 2-3
layers), could be easily improved, supported by solid theoretical knowledge and
with no extra parameters needed. We reorganise the residual design in
higher-order ways, which is inspired by the observation that many effective
networks can be interpreted as different numerical discretisations of
differential equations. The design of ResNet follows a relatively simple
scheme, which is Euler forward; however, the situation becomes complicated
rapidly while stacking. We suppose that stacked ResNet is somehow equalled to a
higher-order scheme; then, the current method of forwarding propagation might
be relatively weak compared with a typical high-order method such as
Runge-Kutta. We propose HO-ResNet to verify the hypothesis of widely used CV
benchmarks with sufficient experiments. Stable and noticeable increases in
performance are observed, and convergence and robustness are also improved. Our
stacking strategy improved ResNet-30 by 2.15 per cent and ResNet-58 by 2.35 per
cent on CIFAR-10, with the same settings and parameters. The proposed strategy
is fundamental and theoretical and can therefore be applied to any network as a
general guideline.

    

### [[2104.02604] Using Molecular Embeddings in QSAR Modeling: Does it Make a Difference?](http://arxiv.org/abs/2104.02604)


  With the consolidation of deep learning in drug discovery, several novel
algorithms for learning molecular representations have been proposed. Despite
the interest of the community in developing new methods for learning molecular
embeddings and their theoretical benefits, comparing molecular embeddings with
each other and with traditional representations is not straightforward, which
in turn hinders the process of choosing a suitable representation for QSAR
modeling. A reason behind this issue is the difficulty of conducting a fair and
thorough comparison of the different existing embedding approaches, which
requires numerous experiments on various datasets and training scenarios. To
close this gap, we reviewed the literature on methods for molecular embeddings
and reproduced three unsupervised and two supervised molecular embedding
techniques recently proposed in the literature. We compared these five methods
concerning their performance in QSAR scenarios using different classification
and regression datasets. We also compared these representations to traditional
molecular representations, namely molecular descriptors and fingerprints. As
opposed to the expected outcome, our experimental setup consisting of over
25,000 trained models and statistical tests revealed that the predictive
performance using molecular embeddings did not significantly surpass that of
traditional representations. While supervised embeddings yielded competitive
results compared to those using traditional molecular representations,
unsupervised embeddings tended to perform worse than traditional
representations. Our results highlight the need for conducting a careful
comparison and analysis of the different embedding techniques prior to using
them in drug design tasks, and motivate a discussion about the potential of
molecular embeddings in computer-aided drug design.

    

### [[2104.09079] A novel Time-frequency Transformer and its Application in Fault Diagnosis of Rolling Bearings](http://arxiv.org/abs/2104.09079)


  The scope of data-driven fault diagnosis models is greatly improved through
deep learning (DL). However, the classical convolution and recurrent structure
have their defects in computational efficiency and feature representation,
while the latest Transformer architecture based on attention mechanism has not
been applied in this field. To solve these problems, we propose a novel
time-frequency Transformer (TFT) model inspired by the massive success of
standard Transformer in sequence processing. Specially, we design a fresh
tokenizer and encoder module to extract effective abstractions from the
time-frequency representation (TFR) of vibration signals. On this basis, a new
end-to-end fault diagnosis framework based on time-frequency Transformer is
presented in this paper. Through the case studies on bearing experimental
datasets, we constructed the optimal Transformer structure and verified the
performance of the diagnostic method. The superiority of the proposed method is
demonstrated in comparison with the benchmark model and other state-of-the-art
methods.

    

### [[2105.00773] Approximate Bayesian Computation for an Explicit-Duration Hidden Markov Model of COVID-19 Hospital Trajectories](http://arxiv.org/abs/2105.00773)


  We address the problem of modeling constrained hospital resources in the
midst of the COVID-19 pandemic in order to inform decision-makers of future
demand and assess the societal value of possible interventions. For broad
applicability, we focus on the common yet challenging scenario where
patient-level data for a region of interest are not available. Instead, given
daily admissions counts, we model aggregated counts of observed resource use,
such as the number of patients in the general ward, in the intensive care unit,
or on a ventilator. In order to explain how individual patient trajectories
produce these counts, we propose an aggregate count explicit-duration hidden
Markov model, nicknamed the ACED-HMM, with an interpretable, compact
parameterization. We develop an Approximate Bayesian Computation approach that
draws samples from the posterior distribution over the model's transition and
duration parameters given aggregate counts from a specific location, thus
adapting the model to a region or individual hospital site of interest. Samples
from this posterior can then be used to produce future forecasts of any counts
of interest. Using data from the United States and the United Kingdom, we show
our mechanistic approach provides competitive probabilistic forecasts for the
future even as the dynamics of the pandemic shift. Furthermore, we show how our
model provides insight about recovery probabilities or length of stay
distributions, and we suggest its potential to answer challenging what-if
questions about the societal value of possible interventions.

    

### [[2105.01092] Process Model Forecasting Using Time Series Analysis of Event Sequence Data](http://arxiv.org/abs/2105.01092)


  Process analytics is an umbrella of data-driven techniques which includes
making predictions for individual process instances or overall process models.
At the instance level, various novel techniques have been recently devised,
tackling next activity, remaining time, and outcome prediction. At the model
level, there is a notable void. It is the ambition of this paper to fill this
gap. To this end, we develop a technique to forecast the entire process model
from historical event data. A forecasted model is a will-be process model
representing a probable future state of the overall process. Such a forecast
helps to investigate the consequences of drift and emerging bottlenecks. Our
technique builds on a representation of event data as multiple time series,
each capturing the evolution of a behavioural aspect of the process model, such
that corresponding forecasting techniques can be applied. Our implementation
demonstrates the accuracy of our technique on real-world event log data.

    

### [[2105.03941] Stronger Privacy for Federated Collaborative Filtering with Implicit Feedback](http://arxiv.org/abs/2105.03941)


  Recommender systems are commonly trained on centrally collected user
interaction data like views or clicks. This practice however raises serious
privacy concerns regarding the recommender's collection and handling of
potentially sensitive data. Several privacy-aware recommender systems have been
proposed in recent literature, but comparatively little attention has been
given to systems at the intersection of implicit feedback and privacy. To
address this shortcoming, we propose a practical federated recommender system
for implicit data under user-level local differential privacy (LDP). The
privacy-utility trade-off is controlled by parameters $\epsilon$ and $k$,
regulating the per-update privacy budget and the number of $\epsilon$-LDP
gradient updates sent by each user respectively. To further protect the user's
privacy, we introduce a proxy network to reduce the fingerprinting surface by
anonymizing and shuffling the reports before forwarding them to the
recommender. We empirically demonstrate the effectiveness of our framework on
the MovieLens dataset, achieving up to Hit Ratio with K=10 (HR@10) 0.68 on 50k
users with 5k items. Even on the full dataset, we show that it is possible to
achieve reasonable utility with HR@10>0.5 without compromising user privacy.

    

### [[2105.09637] Navigation Turing Test (NTT): Learning to Evaluate Human-Like Navigation](http://arxiv.org/abs/2105.09637)


  A key challenge on the path to developing agents that learn complex
human-like behavior is the need to quickly and accurately quantify
human-likeness. While human assessments of such behavior can be highly
accurate, speed and scalability are limited. We address these limitations
through a novel automated Navigation Turing Test (ANTT) that learns to predict
human judgments of human-likeness. We demonstrate the effectiveness of our
automated NTT on a navigation task in a complex 3D environment. We investigate
six classification models to shed light on the types of architectures best
suited to this task, and validate them against data collected through a human
NTT. Our best models achieve high accuracy when distinguishing true human and
agent behavior. At the same time, we show that predicting finer-grained human
assessment of agents' progress towards human-like behavior remains unsolved.
Our work takes an important step towards agents that more effectively learn
complex human-like behavior.

    

### [[2105.13807] Gym-$μ$RTS: Toward Affordable Full Game Real-time Strategy Games Research with Deep Reinforcement Learning](http://arxiv.org/abs/2105.13807)


  In recent years, researchers have achieved great success in applying Deep
Reinforcement Learning (DRL) algorithms to Real-time Strategy (RTS) games,
creating strong autonomous agents that could defeat professional players in
StarCraft~II. However, existing approaches to tackle full games have high
computational costs, usually requiring the use of thousands of GPUs and CPUs
for weeks. This paper has two main contributions to address this issue: 1) We
introduce Gym-$\mu$RTS (pronounced "gym-micro-RTS") as a fast-to-run RL
environment for full-game RTS research and 2) we present a collection of
techniques to scale DRL to play full-game $\mu$RTS as well as ablation studies
to demonstrate their empirical importance. Our best-trained bot can defeat
every $\mu$RTS bot we tested from the past $\mu$RTS competitions when working
in a single-map setting, resulting in a state-of-the-art DRL agent while only
taking about 60 hours of training using a single machine (one GPU, three vCPU,
16GB RAM). See the blog post at
this https URL
and the source code at this https URL


### [[2106.03004] Exploring the Limits of Out-of-Distribution Detection](http://arxiv.org/abs/2106.03004)


  Near out-of-distribution detection (OOD) is a major challenge for deep neural
networks. We demonstrate that large-scale pre-trained transformers can
significantly improve the state-of-the-art (SOTA) on a range of near OOD tasks
across different data modalities. For instance, on CIFAR-100 vs CIFAR-10 OOD
detection, we improve the AUROC from 85% (current SOTA) to more than 96% using
Vision Transformers pre-trained on ImageNet-21k. On a challenging genomics OOD
detection benchmark, we improve the AUROC from 66% to 77% using transformers
and unsupervised pre-training. To further improve performance, we explore the
few-shot outlier exposure setting where a few examples from outlier classes may
be available; we show that pre-trained transformers are particularly
well-suited for outlier exposure, and that the AUROC of OOD detection on
CIFAR-100 vs CIFAR-10 can be improved to 98.7% with just 1 image per OOD class,
and 99.46% with 10 images per OOD class. For multi-modal image-text pre-trained
transformers such as CLIP, we explore a new way of using just the names of
outlier classes as a sole source of information without any accompanying
images, and show that this outperforms previous SOTA on standard vision OOD
benchmark tasks.

    

### [[2106.03143] CAPE: Encoding Relative Positions with Continuous Augmented Positional Embeddings](http://arxiv.org/abs/2106.03143)


  Without positional information, attention-based transformer neural networks
are permutation-invariant. Absolute or relative positional embeddings are the
most popular ways to feed transformer models positional information. Absolute
positional embeddings are simple to implement, but suffer from generalization
issues when evaluating on sequences of different length than those seen at
training time. Relative positions are more robust to length change, but are
more complex to implement and yield inferior model throughput. In this paper,
we propose an augmentation-based approach (CAPE) for absolute positional
embeddings, which keeps the advantages of both absolute (simplicity and speed)
and relative position embeddings (better generalization). In addition, our
empirical evaluation on state-of-the-art models in machine translation, image
and speech recognition demonstrates that CAPE leads to better generalization
performance as well as increased stability with respect to training
hyper-parameters.

    

### [[2106.04982] Cooperative Online Learning](http://arxiv.org/abs/2106.04982)


  In this preliminary (and unpolished) version of the paper, we study an
asynchronous online learning setting with a network of agents. At each time
step, some of the agents are activated, requested to make a prediction, and pay
the corresponding loss. Some feedback is then revealed to these agents and is
later propagated through the network. We consider the case of full, bandit, and
semi-bandit feedback. In particular, we construct a reduction to delayed
single-agent learning that applies to both the full and the bandit feedback
case and allows to obtain regret guarantees for both settings. We complement
these results with a near-matching lower bound.

    

### [[2106.07112] User Acceptance of Gender Stereotypes in Automated Career Recommendations](http://arxiv.org/abs/2106.07112)


  Currently, there is a surge of interest in fair Artificial Intelligence (AI)
and Machine Learning (ML) research which aims to mitigate discriminatory bias
in AI algorithms, e.g. along lines of gender, age, and race. While most
research in this domain focuses on developing fair AI algorithms, in this work,
we show that a fair AI algorithm on its own may be insufficient to achieve its
intended results in the real world. Using career recommendation as a case
study, we build a fair AI career recommender by employing gender debiasing
machine learning techniques. Our offline evaluation showed that the debiased
recommender makes fairer career recommendations without sacrificing its
accuracy. Nevertheless, an online user study of more than 200 college students
revealed that participants on average prefer the original biased system over
the debiased system. Specifically, we found that perceived gender disparity is
a determining factor for the acceptance of a recommendation. In other words,
our results demonstrate we cannot fully address the gender bias issue in AI
recommendations without addressing the gender bias in humans.

    

### [[2106.12479] Classifying Textual Data with Pre-trained Vision Models through Transfer Learning and Data Transformations](http://arxiv.org/abs/2106.12479)


  Knowledge is acquired by humans through experience, and no boundary is set
between the kinds of knowledge or skill levels we can achieve on different
tasks at the same time. When it comes to Neural Networks, that is not the case,
the major breakthroughs in the field are extremely task and domain specific.
Vision and language are dealt with in separate manners, using separate methods
and different datasets. In this work, we propose to use knowledge acquired by
benchmark Vision Models which are trained on ImageNet to help a much smaller
architecture learn to classify text. After transforming the textual data
contained in the IMDB dataset to gray scale images. An analysis of different
domains and the Transfer Learning method is carried out. Despite the challenge
posed by the very different datasets, promising results are achieved. The main
contribution of this work is a novel approach which links large pretrained
models on both language and vision to achieve state-of-the-art results in
different sub-fields from the original task. Without needing high compute
capacity resources. Specifically, Sentiment Analysis is achieved after
transferring knowledge between vision and language models. BERT embeddings are
transformed into grayscale images, these images are then used as training
examples for pre-trained vision models such as VGG16 and ResNet
Index Terms: BERT, Convolutional Neural Networks, Domain Adaptation, image
classification, Natural Language Processing, t-SNE, text classification,
Transfer Learning

    

### [[2106.15093] Certifiable Machine Unlearning for Linear Models](http://arxiv.org/abs/2106.15093)


  Machine unlearning is the task of updating machine learning (ML) models after
a subset of the training data they were trained on is deleted. Methods for the
task are desired to combine effectiveness and efficiency, i.e., they should
effectively "unlearn" deleted data, but in a way that does not require
excessive computation effort (e.g., a full retraining) for a small amount of
deletions. Such a combination is typically achieved by tolerating some amount
of approximation in the unlearning. In addition, laws and regulations in the
spirit of "the right to be forgotten" have given rise to requirements for
certifiability, i.e., the ability to demonstrate that the deleted data has
indeed been unlearned by the ML model.
In this paper, we present an experimental study of the three state-of-the-art
approximate unlearning methods for linear models and demonstrate the trade-offs
between efficiency, effectiveness and certifiability offered by each method. In
implementing the study, we extend some of the existing works and describe a
common ML pipeline to compare and evaluate the unlearning methods on six
real-world datasets and a variety of settings. We provide insights into the
effect of the quantity and distribution of the deleted data on ML models and
the performance of each unlearning method in different settings. We also
propose a practical online strategy to determine when the accumulated error
from approximate unlearning is large enough to warrant a full retrain of the ML
model.

    

### [[2106.15281] On Board Volcanic Eruption Detection through CNNs and Satellite Multispectral Imagery](http://arxiv.org/abs/2106.15281)


  In recent years, the growth of Machine Learning (ML) algorithms has raised
the number of studies including their applicability in a variety of different
scenarios. Among all, one of the hardest ones is the aerospace, due to its
peculiar physical requirements. In this context, a feasibility study and a
first prototype for an Artificial Intelligence (AI) model to be deployed on
board satellites are presented in this work. As a case study, the detection of
volcanic eruptions has been investigated as a method to swiftly produce alerts
and allow immediate interventions. Two Convolutional Neural Networks (CNNs)
have been proposed and designed, showing how to efficiently implement them for
identifying the eruptions and at the same time adapting their complexity in
order to fit on board requirements.

    

### [[2107.00719] Toward Drug-Target Interaction Prediction via Ensemble Modeling and Transfer Learning](http://arxiv.org/abs/2107.00719)


  Drug-target interaction (DTI) prediction plays a crucial role in drug
discovery, and deep learning approaches have achieved state-of-the-art
performance in this field. We introduce an ensemble of deep learning models
(EnsembleDLM) for DTI prediction. EnsembleDLM only uses the sequence
information of chemical compounds and proteins, and it aggregates the
predictions from multiple deep neural networks. This approach not only achieves
state-of-the-art performance in Davis and KIBA datasets but also reaches
cutting-edge performance in the cross-domain applications across different
bio-activity types and different protein classes. We also demonstrate that
EnsembleDLM achieves a good performance (Pearson correlation coefficient and
concordance index > 0.8) in the new domain with approximately 50% transfer
learning data, i.e., the training set has twice as much data as the test set.

    

### [[2107.09047] Know Thyself: Transferable Visuomotor Control Through Robot-Awareness](http://arxiv.org/abs/2107.09047)


  Training visuomotor robot controllers from scratch on a new robot typically
requires generating large amounts of robot-specific data. Could we leverage
data previously collected on another robot to reduce or even completely remove
this need for robot-specific data? We propose a "robot-aware" solution paradigm
that exploits readily available robot "self-knowledge" such as proprioception,
kinematics, and camera calibration to achieve this. First, we learn modular
dynamics models that pair a transferable, robot-agnostic world dynamics module
with a robot-specific, analytical robot dynamics module. Next, we set up visual
planning costs that draw a distinction between the robot self and the world.
Our experiments on tabletop manipulation tasks in simulation and on real robots
demonstrate that these plug-in improvements dramatically boost the
transferability of visuomotor controllers, even permitting zero-shot transfer
onto new robots for the very first time. Project website:
this https URL


### [[2107.13386] SPOTS: An Accelerator for Sparse CNNs Leveraging General Matrix-Matrix Multiplication](http://arxiv.org/abs/2107.13386)


  This paper proposes a new hardware accelerator for sparse convolutional
neural networks (CNNs) by building a hardware unit to perform the Image to
Column (IM2COL) transformation of the input feature map coupled with a systolic
array-based general matrix-matrix multiplication (GEMM) unit. Our design
carefully overlaps the IM2COL transformation with the GEMM computation to
maximize parallelism. We propose a novel design for the IM2COL unit that uses a
set of distributed local memories connected by a ring network, which improves
energy efficiency and latency by streaming the input feature map only once. We
propose a tall systolic array for the GEMM unit while also providing the
ability to organize it as multiple small GEMM units, which enables our design
to handle a wide range of CNNs and their parameters. Further, our design
improves performance by effectively mapping the sparse data to the hardware
units by utilizing sparsity in both input feature maps and weights. Our
prototype, SPOTS, is on average 1.74X faster than Eyeriss. It is also 78X, and
12X more energy-efficient when compared to CPU and GPU implementations,
respectively.

    

### [[2107.13047] RingBFT: Resilient Consensus over Sharded Ring Topology](http://arxiv.org/abs/2107.13047)


  The recent surge in federated data-management applications has brought forth
concerns about the security of underlying data and the consistency of replicas
in the presence of malicious attacks. A prominent solution in this direction is
to employ a permissioned blockchain framework that is modeled around
traditional Byzantine Fault-Tolerant (BFT) consensus protocols. Any federated
application expects its data to be globally scattered to achieve faster access.
But, prior works have shown that traditional BFT protocols are slow and this
led to the rise of sharded-replicated blockchains. Existing BFT protocols for
these sharded blockchains are efficient if client transactions require access
to a single-shard, but face performance degradation if there is a cross-shard
transaction that requires access to multiple shards. However, cross-shard
transactions are common, and to resolve this dilemma, we present RingBFT, a
novel meta-BFT protocol for sharded blockchains. RingBFT requires shards to
adhere to the ring order, and follow the principle of process, forward, and
re-transmit while ensuring the communication between shards is linear. Our
evaluation of RingBFT against state-of-the-art sharding BFT protocols
illustrates that RingBFT achieves up to 25x higher throughput, easily scales to
nearly 500 globally distributed nodes, and achieves a peak throughput of 1.2
million txns/s.

    

### [[2107.13057] Neuromorphic scaling advantages for energy-efficient random walk computation](http://arxiv.org/abs/2107.13057)


  Computing stands to be radically improved by neuromorphic computing (NMC)
approaches inspired by the brain's incredible efficiency and capabilities. Most
NMC research, which aims to replicate the brain's computational structure and
architecture in man-made hardware, has focused on artificial intelligence;
however, less explored is whether this brain-inspired hardware can provide
value beyond cognitive tasks. We demonstrate that high-degree parallelism and
configurability of spiking neuromorphic architectures makes them well-suited to
implement random walks via discrete time Markov chains. Such random walks are
useful in Monte Carlo methods, which represent a fundamental computational tool
for solving a wide range of numerical computing tasks. Additionally, we show
how the mathematical basis for a probabilistic solution involving a class of
stochastic differential equations can leverage those simulations to provide
solutions for a range of broadly applicable computational tasks. Despite being
in an early development stage, we find that NMC platforms, at a sufficient
scale, can drastically reduce the energy demands of high-performance computing
(HPC) platforms.

    

### [[2107.13212] The Trip to The Enterprise Gourmet Data Product Marketplace through a Self-service Data Platform](http://arxiv.org/abs/2107.13212)


  Data Analytics provides core business reporting needs in many software
companies, acts as a source of truth for key information, and enables building
advanced solutions, e.g., predictive models, machine learning, real-time
recommendations, to grow the business.
A self-service, multi-tenant, API-first, and scalable data platform is the
foundational requirement in creating an enterprise data marketplace, which
enables the creation, publishing, and exchange of data products. Such a
marketplace enables the exploration and discovery of data products, further
providing high-level data governance and oversight on marketplace contents. In
this paper, we describe our way to the gourmet data product marketplace. We
cover the design principles, the implementation details, technology choices,
and the journey to build an enterprise data platform that meets the above
characteristics. The platform consists of ingestion, streaming, storage,
transformation, schema generation, fail-safe, data sharing, access management,
PII data automatic identification, self-service storage optimization
recommendations, and CI/CD integration.
We then show how the platform enables and operates the data marketplace,
facilitating the exchange of stable data products across users and tenants. We
motivate and show how we run scalable decentralized data governance. All of
this is built and run for Cimpress Technology (CT), which operates the Mass
Customization Platform for Cimpress and its businesses. The CT data platform
serves 1000s of users from different platform participants, with data sourced
from heterogeneous sources. Data is ingested at a rate of well over 1000
individual messages per second and serves more than 100k analytical queries
daily.

    

### [[2107.13309] $(1+ε)$-Approximate Shortest Paths in Dynamic Streams](http://arxiv.org/abs/2107.13309)


  Computing approximate shortest paths in the dynamic streaming setting is a
fundamental challenge that has been intensively studied during the last decade.
Currently existing solutions for this problem either build a sparse
multiplicative spanner of the input graph and compute shortest paths in the
spanner offline, or compute an exact single source BFS tree.
Solutions of the first type are doomed to incur a stretch-space tradeoff of
$2\kappa-1$ versus $n^{1+1/\kappa}$, for an integer parameter $\kappa$. (In
fact, existing solutions also incur an extra factor of $1+\epsilon$ in the
stretch for weighted graphs, and an additional factor of $\log^{O(1)}n$ in the
space.) The only existing solution of the second type uses $n^{1/2 -
O(1/\kappa)}$ passes over the stream (for space $O(n^{1+1/\kappa})$), and
applies only to unweighted graphs.
In this paper we show that $(1+\epsilon)$-approximate single-source shortest
paths can be computed in this setting with $\tilde{O}(n^{1+1/\kappa})$ space
using just \emph{constantly} many passes in unweighted graphs, and
polylogarithmically many passes in weighted graphs (assuming $\epsilon$ and
$\kappa$ are constant). Moreover, in fact, the same result applies for
multi-source shortest paths, as long as the number of sources is
$O(n^{1/\kappa})$.
We achieve these results by devising efficient dynamic streaming
constructions of $(1 + \epsilon, \beta)$-spanners and hopsets. We believe that
these constructions are of independent interest.

    

### [[2107.13317] C3O: Collaborative Cluster Configuration Optimization for Distributed Data Processing in Public Clouds](http://arxiv.org/abs/2107.13317)


  Distributed dataflow systems enable data-parallel processing of large
datasets on clusters. Public cloud providers offer a large variety and quantity
of resources that can be used for such clusters. Yet, selecting appropriate
cloud resources for dataflow jobs - that neither lead to bottlenecks nor to low
resource utilization - is often challenging, even for expert users such as data
engineers.
We present C3O, a collaborative system for optimizing data processing cluster
configurations in public clouds based on shared historical runtime data. The
shared data is utilized for predicting the runtimes of data processing jobs on
different possible cluster configurations, using specialized regression models.
These models take the diverse execution contexts of different users into
account and exhibit mean absolute errors below 3% in our experimental
evaluation with 930 unique Spark jobs.

    

### [[2107.13320] A Case Study on the Stability of Performance Tests for Serverless Applications](http://arxiv.org/abs/2107.13320)


  Context. While in serverless computing, application resource management and
operational concerns are generally delegated to the cloud provider, ensuring
that serverless applications meet their performance requirements is still a
responsibility of the developers. Performance testing is a commonly used
performance assessment practice; however, it traditionally requires visibility
of the resource environment.
Objective. In this study, we investigate whether performance tests of
serverless applications are stable, that is, if their results are reproducible,
and what implications the serverless paradigm has for performance tests.
Method. We conduct a case study where we collect two datasets of performance
test results: (a) repetitions of performance tests for varying memory size and
load intensities and (b) three repetitions of the same performance test every
day for ten months.
Results. We find that performance tests of serverless applications are
comparatively stable if conducted on the same day. However, we also observe
short-term performance variations and frequent long-term performance changes.
Conclusion. Performance tests for serverless applications can be stable;
however, the serverless model impacts the planning, execution, and analysis of
performance tests.

    

### [[2107.13500] Accelerating advection for atmospheric modelling on Xilinx and Intel FPGAs](http://arxiv.org/abs/2107.13500)


  Reconfigurable architectures, such as FPGAs, enable the execution of code at
the electronics level, avoiding the assumptions imposed by the general purpose
black-box micro-architectures of CPUs and GPUs. Such tailored execution can
result in increased performance and power efficiency, and as the HPC community
moves towards exascale an important question is the role such hardware
technologies can play in future supercomputers.
In this paper we explore the porting of the PW advection kernel, an important
code component used in a variety of atmospheric simulations and accounting for
around 40\% of the runtime of the popular Met Office NERC Cloud model (MONC).
Building upon previous work which ported this kernel to an older generation of
Xilinx FPGA, we target latest generation Xilinx Alveo U280 and Intel Stratix 10
FPGAs. Exploring the development of a dataflow design which is performance
portable between vendors, we then describe implementation differences between
the tool chains and compare kernel performance between FPGA hardware. This is
followed by a more general performance comparison, scaling up the number of
kernels on the Xilinx Alveo and Intel Stratix 10, against a 24 core Xeon
Platinum Cascade Lake CPU and NVIDIA Tesla V100 GPU. When overlapping the
transfer of data to and from the boards with compute, the FPGA solutions
considerably outperform the CPU and, whilst falling short of the GPU in terms
of performance, demonstrate power usage benefits, with the Alveo being
especially power efficient. The result of this work is a comparison and set of
design techniques that apply both to this specific atmospheric advection kernel
on Xilinx and Intel FPGAs, and that are also of interest more widely when
looking to accelerate HPC codes on a variety of reconfigurable architectures.

    

### [[2107.13502] A Secure and Multi-objective Virtual Machine Placement Framework for Cloud Data Centre](http://arxiv.org/abs/2107.13502)


  To facilitate cost-effective and elastic computing benefits to the cloud
users, the energy-efficient and secure allocation of virtual machines (VMs)
plays a significant role at the data centre. The inefficient VM Placement (VMP)
and sharing of common physical machines among multiple users leads to resource
wastage, excessive power consumption, increased inter-communication cost and
security breaches. To address the aforementioned challenges, a novel secure and
multi-objective virtual machine placement (SM-VMP) framework is proposed with
an efficient VM migration. The proposed framework ensures an energy-efficient
distribution of physical resources among VMs that emphasizes secure and timely
execution of user application by reducing inter-communication delay. The VMP is
carried out by applying the proposed Whale Optimization Genetic Algorithm
(WOGA), inspired by whale evolutionary optimization and non-dominated sorting
based genetic algorithms. The performance evaluation for static and dynamic VMP
and comparison with recent state-of-the-arts observed a notable reduction in
shared servers, inter-communication cost, power consumption and execution time
up to 28.81%, 25.7%, 35.9% and 82.21%, respectively and increased resource
utilization up to 30.21%.

    

### [[2107.13520] Exponentiation Using Laplace Expansion](http://arxiv.org/abs/2107.13520)


  This article derives an equation for exponentiation that can be used for
calculating exponents using a parallel computing architecture.

    

### [[1905.13600] Tracking in Order to Recover: Detectable Recovery of Lock-Free Data Structures](http://arxiv.org/abs/1905.13600)


  This paper presents the tracking approach for deriving detectably recoverable
(and thus also durable) implementations of many widely-used concurrent data
structures. Such data structures, satisfying detectable recovery, are appealing
for emerging systems featuring byte-addressable non-volatile main memory
(NVRAM), whose persistence allows to efficiently resurrect failed processes
after crashes. Detectable recovery ensures that after a crash, every executed
operation is able to recover and return a correct response, and that the state
of the data structure is not corrupted. Info-Structure Based (ISB)-tracking
amends descriptor objects used in existing lock-free helping schemes with
additional fields that track an operation's progress towards completion and
persists these fields to memory in order to ensure detectable recovery.
ISB-tracking avoids full-fledged logging and tracks the progress of concurrent
operations in a per-process manner, thus reducing the cost of ensuring
detectable recovery. We have applied ISB-tracking to derive detectably
recoverable implementations of a queue, a linked list, a binary search tree,
and an exchanger. Experimental results show the feasibility of the technique.

    

### [[2011.10436] Locally Solvable Tasks and the Limitations of Valency Arguments](http://arxiv.org/abs/2011.10436)


  An elegant strategy for proving impossibility results in distributed
computing was introduced in the celebrated FLP consensus impossibility proof.
This strategy is local in nature as at each stage, one configuration of a
hypothetical protocol for consensus is considered, together with future
valencies of possible extensions. This proof strategy has been used in numerous
situations related to consensus, leading one to wonder why it has not been used
in impossibility results of two other well-known tasks: set agreement and
renaming. This paper provides an explanation of why impossibility proofs of
these tasks have been of a global nature. It shows that a protocol can always
solve such tasks locally, in the following sense. Given a configuration and all
its future valencies, if a single successor configuration is selected, then the
protocol can reveal all decisions in this branch of executions, satisfying the
task specification. This result is shown for both set agreement and renaming,
implying that there are no local impossibility proofs for these tasks.

    

### [[2107.13031] Red Dragon AI at TextGraphs 2021 Shared Task: Multi-Hop Inference Explanation Regeneration by Matching Expert Ratings](http://arxiv.org/abs/2107.13031)


  Creating explanations for answers to science questions is a challenging task
that requires multi-hop inference over a large set of fact sentences. This
year, to refocus the Textgraphs Shared Task on the problem of gathering
relevant statements (rather than solely finding a single 'correct path'), the
WorldTree dataset was augmented with expert ratings of 'relevance' of
statements to each overall explanation. Our system, which achieved second place
on the Shared Task leaderboard, combines initial statement retrieval; language
models trained to predict the relevance scores; and ensembling of a number of
the resulting rankings. Our code implementation is made available at
this https URL


### [[2107.13083] Is Object Detection Necessary for Human-Object Interaction Recognition?](http://arxiv.org/abs/2107.13083)


  This paper revisits human-object interaction (HOI) recognition at image level
without using supervisions of object location and human pose. We name it
detection-free HOI recognition, in contrast to the existing
detection-supervised approaches which rely on object and keypoint detections to
achieve state of the art. With our method, not only the detection supervision
is evitable, but superior performance can be achieved by properly using
image-text pre-training (such as CLIP) and the proposed Log-Sum-Exp Sign
(LSE-Sign) loss function. Specifically, using text embeddings of class labels
to initialize the linear classifier is essential for leveraging the CLIP
pre-trained image encoder. In addition, LSE-Sign loss facilitates learning from
multiple labels on an imbalanced dataset by normalizing gradients over all
classes in a softmax format. Surprisingly, our detection-free solution achieves
60.5 mAP on the HICO dataset, outperforming the detection-supervised state of
the art by 13.4 mAP

    

### [[2107.13085] On Improving the Backjump Level in PB Solvers](http://arxiv.org/abs/2107.13085)


  Current PB solvers implement many techniques inspired by the CDCL
architecture of modern SAT solvers, so as to benefit from its practical
efficiency. However, they also need to deal with the fact that many of the
properties leveraged by this architecture are no longer true when considering
PB constraints. In this paper, we focus on one of these properties, namely the
optimality of the so-called first unique implication point (1-UIP). While it is
well known that learning the first assertive clause produced during conflict
analysis ensures to perform the highest possible backjump in a SAT solver, we
show that there is no such guarantee in the presence of PB constraints. We also
introduce and evaluate different approaches designed to improve the backjump
level identified during conflict analysis by allowing to continue the analysis
after reaching the 1-UIP. Our experiments show that sub-optimal backjumps are
fairly common in PB solvers, even though their impact on the solver is not
clear.

    

### [[2107.13144] Content-aware Directed Propagation Network with Pixel Adaptive Kernel Attention](http://arxiv.org/abs/2107.13144)


  Convolutional neural networks (CNNs) have been not only widespread but also
achieved noticeable results on numerous applications including image
classification, restoration, and generation. Although the weight-sharing
property of convolutions makes them widely adopted in various tasks, its
content-agnostic characteristic can also be considered a major drawback. To
solve this problem, in this paper, we propose a novel operation, called pixel
adaptive kernel attention (PAKA). PAKA provides directivity to the filter
weights by multiplying spatially varying attention from learnable features. The
proposed method infers pixel-adaptive attention maps along the channel and
spatial directions separately to address the decomposed model with fewer
parameters. Our method is trainable in an end-to-end manner and applicable to
any CNN-based models. In addition, we propose an improved information
aggregation module with PAKA, called the hierarchical PAKA module (HPM). We
demonstrate the superiority of our HPM by presenting state-of-the-art
performance on semantic segmentation compared to the conventional information
aggregation modules. We validate the proposed method through additional
ablation studies and visualizing the effect of PAKA providing directivity to
the weights of convolutions. We also show the generalizability of the proposed
method by applying it to multi-modal tasks especially color-guided depth map
super-resolution.

    

### [[2107.13165] Towards Emotion-Aware Agents For Negotiation Dialogues](http://arxiv.org/abs/2107.13165)


  Negotiation is a complex social interaction that encapsulates emotional
encounters in human decision-making. Virtual agents that can negotiate with
humans are useful in pedagogy and conversational AI. To advance the development
of such agents, we explore the prediction of two important subjective goals in
a negotiation - outcome satisfaction and partner perception. Specifically, we
analyze the extent to which emotion attributes extracted from the negotiation
help in the prediction, above and beyond the individual difference variables.
We focus on a recent dataset in chat-based negotiations, grounded in a
realistic camping scenario. We study three degrees of emotion dimensions -
emoticons, lexical, and contextual by leveraging affective lexicons and a
state-of-the-art deep learning architecture. Our insights will be helpful in
designing adaptive negotiation agents that interact through realistic
communication interfaces.

    

### [[2107.13179] Conflict Detection in IoT-based Smart Homes](http://arxiv.org/abs/2107.13179)


  We propose a novel framework that detects conflicts in IoT-based smart homes.
Conflicts may arise during interactions between the resident and IoT services
in smart homes. We propose a generic knowledge graph to represent the relations
between IoT services and environment entities. We also profile a generic
knowledge graph to a specific smart home setting based on the context
information. We propose a conflict taxonomy to capture different types of
conflicts in a single resident smart home setting. A conflict detection
algorithm is proposed to identify potential conflicts using the profiled
knowledge graph. We conduct a set of experiments on real datasets and
synthesized datasets to validate the effectiveness and efficiency of our
proposed approach.

    

### [[2107.13181] Packet Routing with Graph Attention Multi-agent Reinforcement Learning](http://arxiv.org/abs/2107.13181)


  Packet routing is a fundamental problem in communication networks that
decides how the packets are directed from their source nodes to their
destination nodes through some intermediate nodes. With the increasing
complexity of network topology and highly dynamic traffic demand, conventional
model-based and rule-based routing schemes show significant limitations, due to
the simplified and unrealistic model assumptions, and lack of flexibility and
adaption. Adding intelligence to the network control is becoming a trend and
the key to achieving high-efficiency network operation. In this paper, we
develop a model-free and data-driven routing strategy by leveraging
reinforcement learning (RL), where routers interact with the network and learn
from the experience to make some good routing configurations for the future.
Considering the graph nature of the network topology, we design a multi-agent
RL framework in combination with Graph Neural Network (GNN), tailored to the
routing problem. Three deployment paradigms, centralized, federated, and
cooperated learning, are explored respectively. Simulation results demonstrate
that our algorithm outperforms some existing benchmark algorithms in terms of
packet transmission delay and affordable load.

    

### [[2107.13261] Improving Multi-View Stereo via Super-Resolution](http://arxiv.org/abs/2107.13261)


  Today, Multi-View Stereo techniques are able to reconstruct robust and
detailed 3D models, especially when starting from high-resolution images.
However, there are cases in which the resolution of input images is relatively
low, for instance, when dealing with old photos, or when hardware constrains
the amount of data that can be acquired. In this paper, we investigate if, how,
and how much increasing the resolution of such input images through
Super-Resolution techniques reflects in quality improvements of the
reconstructed 3D models, despite the artifacts that sometimes this may
generate. We show that applying a Super-Resolution step before recovering the
depth maps in most cases leads to a better 3D model both in the case of
PatchMatch-based and deep-learning-based algorithms. The use of
Super-Resolution improves especially the completeness of reconstructed models
and turns out to be particularly effective in the case of textured scenes.

    

### [[2107.13271] Spatial Uncertainty-Aware Semi-Supervised Crowd Counting](http://arxiv.org/abs/2107.13271)


  Semi-supervised approaches for crowd counting attract attention, as the fully
supervised paradigm is expensive and laborious due to its request for a large
number of images of dense crowd scenarios and their annotations. This paper
proposes a spatial uncertainty-aware semi-supervised approach via regularized
surrogate task (binary segmentation) for crowd counting problems. Different
from existing semi-supervised learning-based crowd counting methods, to exploit
the unlabeled data, our proposed spatial uncertainty-aware teacher-student
framework focuses on high confident regions' information while addressing the
noisy supervision from the unlabeled data in an end-to-end manner.
Specifically, we estimate the spatial uncertainty maps from the teacher model's
surrogate task to guide the feature learning of the main task (density
regression) and the surrogate task of the student model at the same time.
Besides, we introduce a simple yet effective differential transformation layer
to enforce the inherent spatial consistency regularization between the main
task and the surrogate task in the student model, which helps the surrogate
task to yield more reliable predictions and generates high-quality uncertainty
maps. Thus, our model can also address the task-level perturbation problems
that occur spatial inconsistency between the primary and surrogate tasks in the
student model. Experimental results on four challenging crowd counting datasets
demonstrate that our method achieves superior performance to the
state-of-the-art semi-supervised methods.

    

### [[2107.13296] Checking Patch Behaviour against Test Specification](http://arxiv.org/abs/2107.13296)


  Towards predicting patch correctness in APR, we propose a simple, but novel
hypothesis on how the link between the patch behaviour and failing test
specifications can be drawn: similar failing test cases should require similar
patches. We then propose BATS, an unsupervised learning-based system to predict
patch correctness by checking patch Behaviour Against failing Test
Specification. BATS exploits deep representation learning models for code and
patches: for a given failing test case, the yielded embedding is used to
compute similarity metrics in the search for historical similar test cases in
order to identify the associated applied patches, which are then used as a
proxy for assessing generated patch correctness. Experimentally, we first
validate our hypothesis by assessing whether ground-truth developer patches
cluster together in the same way that their associated failing test cases are
clustered. Then, after collecting a large dataset of 1278 plausible patches
(written by developers or generated by some 32 APR tools), we use BATS to
predict correctness: BATS achieves an AUC between 0.557 to 0.718 and a recall
between 0.562 and 0.854 in identifying correct patches. Compared against
previous work, we demonstrate that our approach outperforms state-of-the-art
performance in patch correctness prediction, without the need for large labeled
patch datasets in contrast with prior machine learning-based approaches. While
BATS is constrained by the availability of similar test cases, we show that it
can still be complementary to existing approaches: used in conjunction with a
recent approach implementing supervised learning, BATS improves the overall
recall in detecting correct patches. We finally show that BATS can be
complementary to the state-of-the-art PATCH-SIM dynamic approach of identifying
the correct patches for APR tools.

    

### [[2107.13306] Tab2Know: Building a Knowledge Base from Tables in Scientific Papers](http://arxiv.org/abs/2107.13306)


  Tables in scientific papers contain a wealth of valuable knowledge for the
scientific enterprise. To help the many of us who frequently consult this type
of knowledge, we present Tab2Know, a new end-to-end system to build a Knowledge
Base (KB) from tables in scientific papers. Tab2Know addresses the challenge of
automatically interpreting the tables in papers and of disambiguating the
entities that they contain. To solve these problems, we propose a pipeline that
employs both statistical-based classifiers and logic-based reasoning. First,
our pipeline applies weakly supervised classifiers to recognize the type of
tables and columns, with the help of a data labeling system and an ontology
specifically designed for our purpose. Then, logic-based reasoning is used to
link equivalent entities (via sameAs links) in different tables. An empirical
evaluation of our approach using a corpus of papers in the Computer Science
domain has returned satisfactory performance. This suggests that ours is a
promising step to create a large-scale KB of scientific knowledge.

    

### [[2107.13329] Exploring and mining attributed sequences of interactions](http://arxiv.org/abs/2107.13329)


  We are faced with data comprised of entities interacting over time: this can
be individuals meeting, customers buying products, machines exchanging packets
on the IP network, among others. Capturing the dynamics as well as the
structure of these interactions is of crucial importance for analysis. These
interactions can almost always be labeled with content: group belonging,
reviews of products, abstracts, etc. We model these stream of interactions as
stream graphs, a recent framework to model interactions over time. Formal
Concept Analysis provides a framework for analyzing concepts evolving within a
context. Considering graphs as the context, it has recently been applied to
perform closed pattern mining on social graphs. In this paper, we are
interested in pattern mining in sequences of interactions. After recalling and
extending notions from formal concept analysis on graphs to stream graphs, we
introduce algorithms to enumerate closed patterns on a labeled stream graph,
and introduce a way to select relevant closed patterns. We run experiments on
two real-world datasets of interactions among students and citations between
authors, and show both the feasibility and the relevance of our method.

    

### [[2107.13355] A Computer Vision-Based Approach for Driver Distraction Recognition using Deep Learning and Genetic Algorithm Based Ensemble](http://arxiv.org/abs/2107.13355)


  As the proportion of road accidents increases each year, driver distraction
continues to be an important risk component in road traffic injuries and
deaths. The distractions caused by the increasing use of mobile phones and
other wireless devices pose a potential risk to road safety. Our current study
aims to aid the already existing techniques in driver posture recognition by
improving the performance in the driver distraction classification problem. We
present an approach using a genetic algorithm-based ensemble of six independent
deep neural architectures, namely, AlexNet, VGG-16, EfficientNet B0, Vanilla
CNN, Modified DenseNet, and InceptionV3 + BiLSTM. We test it on two
comprehensive datasets, the AUC Distracted Driver Dataset, on which our
technique achieves an accuracy of 96.37%, surpassing the previously obtained
95.98%, and on the State Farm Driver Distraction Dataset, on which we attain an
accuracy of 99.75%. The 6-Model Ensemble gave an inference time of 0.024
seconds as measured on our machine with Ubuntu 20.04(64-bit) and GPU as GeForce
GTX 1080.

    

### [[2107.13356] Value-Based Reinforcement Learning for Continuous Control Robotic Manipulation in Multi-Task Sparse Reward Settings](http://arxiv.org/abs/2107.13356)


  Learning continuous control in high-dimensional sparse reward settings, such
as robotic manipulation, is a challenging problem due to the number of samples
often required to obtain accurate optimal value and policy estimates. While
many deep reinforcement learning methods have aimed at improving sample
efficiency through replay or improved exploration techniques, state of the art
actor-critic and policy gradient methods still suffer from the hard exploration
problem in sparse reward settings. Motivated by recent successes of value-based
methods for approximating state-action values, like RBF-DQN, we explore the
potential of value-based reinforcement learning for learning continuous robotic
manipulation tasks in multi-task sparse reward settings. On robotic
manipulation tasks, we empirically show RBF-DQN converges faster than current
state of the art algorithms such as TD3, SAC, and PPO. We also perform ablation
studies with RBF-DQN and have shown that some enhancement techniques for
vanilla Deep Q learning such as Hindsight Experience Replay (HER) and
Prioritized Experience Replay (PER) can also be applied to RBF-DQN. Our
experimental analysis suggests that value-based approaches may be more
sensitive to data augmentation and replay buffer sample techniques than
policy-gradient methods, and that the benefits of these methods for robot
manipulation are heavily dependent on the transition dynamics of generated
subgoal states.

    

### [[2107.13377] Growing knowledge culturally across generations to solve novel, complex tasks](http://arxiv.org/abs/2107.13377)


  Knowledge built culturally across generations allows humans to learn far more
than an individual could glean from their own experience in a lifetime.
Cultural knowledge in turn rests on language: language is the richest record of
what previous generations believed, valued, and practiced. The power and
mechanisms of language as a means of cultural learning, however, are not well
understood. We take a first step towards reverse-engineering cultural learning
through language. We developed a suite of complex high-stakes tasks in the form
of minimalist-style video games, which we deployed in an iterated learning
paradigm. Game participants were limited to only two attempts (two lives) to
beat each game and were allowed to write a message to a future participant who
read the message before playing. Knowledge accumulated gradually across
generations, allowing later generations to advance further in the games and
perform more efficient actions. Multigenerational learning followed a
strikingly similar trajectory to individuals learning alone with an unlimited
number of lives. These results suggest that language provides a sufficient
medium to express and accumulate the knowledge people acquire in these diverse
tasks: the dynamics of the environment, valuable goals, dangerous risks, and
strategies for success. The video game paradigm we pioneer here is thus a rich
test bed for theories of cultural transmission and learning from language.

    

### [[2107.13435] MWP-BERT: A Strong Baseline for Math Word Problems](http://arxiv.org/abs/2107.13435)


  Math word problem (MWP) solving is the task of transforming a sequence of
natural language problem descriptions to executable math equations. An MWP
solver not only needs to understand complex scenarios described in the problem
texts, but also identify the key mathematical variables and associate text
descriptions with math equation logic. Although recent sequence modeling MWP
solvers have gained credits on the math-text contextual understanding,
pre-trained language models (PLM) have not been explored for solving MWP,
considering that PLM trained over free-form texts is limited in representing
text references to mathematical logic. In this work, we introduce MWP-BERT to
obtain pre-trained token representations that capture the alignment between
text description and mathematical logic. Additionally, we introduce a
keyword-based prompt matching method to address the MWPs requiring common-sense
knowledge. On a benchmark Math23K dataset and a new Ape210k dataset, we show
that MWP-BERT outperforms the strongest baseline model by 5-10% improvement on
accuracy.

    

### [[2107.13454] Artificial Intelligence in Healthcare: Lost In Translation?](http://arxiv.org/abs/2107.13454)


  Artificial intelligence (AI) in healthcare is a potentially revolutionary
tool to achieve improved healthcare outcomes while reducing overall health
costs. While many exploratory results hit the headlines in recent years there
are only few certified and even fewer clinically validated products available
in the clinical setting. This is a clear indication of failing translation due
to shortcomings of the current approach to AI in healthcare. In this work, we
highlight the major areas, where we observe current challenges for translation
in AI in healthcare, namely precision medicine, reproducible science, data
issues and algorithms, causality, and product development. For each field, we
outline possible solutions for these challenges. Our work will lead to improved
translation of AI in healthcare products into the clinical setting

    

### [[2107.13461] Marine Vehicles Localization Using Grid Cells for Path Integration](http://arxiv.org/abs/2107.13461)


  Autonomous Underwater Vehicles (AUVs) are platforms used for research and
exploration of marine environments. However, these types of vehicles face many
challenges that hinder their widespread use in the industry. One of the main
limitations is obtaining accurate position estimation, due to the lack of GPS
signal underwater. This estimation is usually done with Kalman filters.
However, new developments in the neuroscience field have shed light on the
mechanisms by which mammals are able to obtain a reliable estimation of their
current position based on external and internal motion cues. A new type of
neuron, called Grid cells, has been shown to be part of path integration system
in the brain. In this article, we show how grid cells can be used for obtaining
a position estimation of underwater vehicles. The model of grid cells used
requires only the linear velocities together with heading orientation and
provides a reliable estimation of the vehicle's position. We provide simulation
results for an AUV which show the feasibility of our proposed methodology.

    

### [[2107.13498] Toward Integrated Human-machine Intelligence for Civil Engineering: An Interdisciplinary Perspective](http://arxiv.org/abs/2107.13498)


  The purpose of this paper is to examine the opportunities and barriers of
Integrated Human-Machine Intelligence (IHMI) in civil engineering. Integrating
artificial intelligence's high efficiency and repeatability with humans'
adaptability in various contexts can advance timely and reliable
decision-making during civil engineering projects and emergencies. Successful
cases in other domains, such as biomedical science, healthcare, and
transportation, showed the potential of IHMI in data-driven, knowledge-based
decision-making in numerous civil engineering applications. However, whether
the industry and academia are ready to embrace the era of IHMI and maximize its
benefit to the industry is still questionable due to several knowledge gaps.
This paper thus calls for future studies in exploring the value, method, and
challenges of applying IHMI in civil engineering. Our systematic review of the
literature and motivating cases has identified four knowledge gaps in achieving
effective IHMI in civil engineering. First, it is unknown what types of tasks
in the civil engineering domain can be assisted by AI and to what extent.
Second, the interface between human and AI in civil engineering-related tasks
need more precise and formal definition. Third, the barriers that impede
collecting detailed behavioral data from humans and contextual environments
deserve systematic classification and prototyping. Lastly, it is unknown what
expected and unexpected impacts will IHMI have on the AEC industry and
entrepreneurship. Analyzing these knowledge gaps led to a list of identified
research questions. This paper will lay the foundation for identifying relevant
studies to form a research roadmap to address the four knowledge gaps
identified.

    

### [[2107.13509] The Who in Explainable AI: How AI Background Shapes Perceptions of AI Explanations](http://arxiv.org/abs/2107.13509)


  Explainability of AI systems is critical for users to take informed actions
and hold systems accountable. While "opening the opaque box" is important,
understanding who opens the box can govern if the Human-AI interaction is
effective. In this paper, we conduct a mixed-methods study of how two different
groups of whos--people with and without a background in AI--perceive different
types of AI explanations. These groups were chosen to look at how disparities
in AI backgrounds can exacerbate the creator-consumer gap. We quantitatively
share what the perceptions are along five dimensions: confidence, intelligence,
understandability, second chance, and friendliness. Qualitatively, we highlight
how the AI background influences each group's interpretations and elucidate why
the differences might exist through the lenses of appropriation and cognitive
heuristics. We find that (1) both groups had unwarranted faith in numbers, to
different extents and for different reasons, (2) each group found explanatory
values in different explanations that went beyond the usage we designed them
for, and (3) each group had different requirements of what counts as humanlike
explanations. Using our findings, we discuss potential negative consequences
such as harmful manipulation of user trust and propose design interventions to
mitigate them. By bringing conscious awareness to how and why AI backgrounds
shape perceptions of potential creators and consumers in XAI, our work takes a
formative step in advancing a pluralistic Human-centered Explainable AI
discourse.

    

### [[2007.14009] The Minimum Description Length Principle for Pattern Mining: A Survey](http://arxiv.org/abs/2007.14009)


  This is about the Minimum Description Length (MDL) principle applied to
pattern mining. The length of this description is kept to the minimum.
Mining patterns is a core task in data analysis and, beyond issues of
efficient enumeration, the selection of patterns constitutes a major challenge.
The MDL principle, a model selection method grounded in information theory, has
been applied to pattern mining with the aim to obtain compact high-quality sets
of patterns. After giving an outline of relevant concepts from information
theory and coding, as well as of work on the theory behind the MDL and similar
principles, we review MDL-based methods for mining various types of data and
patterns. Finally, we open a discussion on some issues regarding these methods,
and highlight currently active related data analysis problems.

    

### [[2012.00257] Confluence: A Robust Non-IoU Alternative to Non-Maxima Suppression in Object Detection](http://arxiv.org/abs/2012.00257)


  Confluence is a novel non-Intersection over Union (IoU) alternative to
Non-Maxima Suppression (NMS) in bounding box post-processing in object
detection. It overcomes the inherent limitations of IoU-based NMS variants to
provide a more stable, consistent predictor of bounding box clustering by using
a normalized Manhattan Distance inspired proximity metric to represent bounding
box clustering. Unlike Greedy and Soft NMS, it does not rely solely on
classification confidence scores to select optimal bounding boxes, instead
selecting the box which is closest to every other box within a given cluster
and removing highly confluent neighboring boxes. Confluence is experimentally
validated on the MS COCO and CrowdHuman benchmarks, improving Average Precision
by up to 2.3-3.8% and Average Recall by up to 5.3-7.2% when compared against
de-facto standard and state of the art NMS variants. Quantitative results are
supported by extensive qualitative analysis and threshold sensitivity analysis
experiments support the conclusion that Confluence is more robust than NMS
variants. Confluence represents a paradigm shift in bounding box processing,
with potential to replace IoU in bounding box regression processes.

    

### [[2012.14173] Playing to distraction: towards a robust training of CNN classifiers through visual explanation techniques](http://arxiv.org/abs/2012.14173)


  The field of deep learning is evolving in different directions, with still
the need for more efficient training strategies. In this work, we present a
novel and robust training scheme that integrates visual explanation techniques
in the learning process. Unlike the attention mechanisms that focus on the
relevant parts of images, we aim to improve the robustness of the model by
making it pay attention to other regions as well. Broadly speaking, the idea is
to distract the classifier in the learning process to force it to focus not
only on relevant regions but also on those that, a priori, are not so
informative for the discrimination of the class. We tested the proposed
approach by embedding it into the learning process of a convolutional neural
network for the analysis and classification of two well-known datasets, namely
Stanford cars and FGVC-Aircraft. Furthermore, we evaluated our model on a
real-case scenario for the classification of egocentric images, allowing us to
obtain relevant information about peoples' lifestyles. In particular, we work
on the challenging EgoFoodPlaces dataset, achieving state-of-the-art results
with a lower level of complexity. The obtained results indicate the suitability
of our proposed training scheme for image classification, improving the
robustness of the final model.

    

### [[2101.04257] Transforming Multi-Conditioned Generation from Meaning Representation](http://arxiv.org/abs/2101.04257)


  In task-oriented conversation systems, natural language generation systems
that generate sentences with specific information related to conversation flow
are useful. Our study focuses on language generation by considering various
information representing the meaning of utterances as multiple conditions of
generation. NLG from meaning representations, the conditions for sentence
meaning, generally goes through two steps: sentence planning and surface
realization. However, we propose a simple one-stage framework to generate
utterances directly from MR (Meaning Representation). Our model is based on
GPT2 and generates utterances with flat conditions on slot and value pairs,
which does not need to determine the structure of the sentence. We evaluate
several systems in the E2E dataset with 6 automatic metrics. Our system is a
simple method, but it demonstrates comparable performance to previous systems
in automated metrics. In addition, using only 10\% of the data set without any
other techniques, our model achieves comparable performance, and shows the
possibility of performing zero-shot generation and expanding to other datasets.

    

### [[2103.04019] Indoor Future Person Localization from an Egocentric Wearable Camera](http://arxiv.org/abs/2103.04019)


  Accurate prediction of future person location and movement trajectory from an
egocentric wearable camera can benefit a wide range of applications, such as
assisting visually impaired people in navigation, and the development of
mobility assistance for people with disability. In this work, a new egocentric
dataset was constructed using a wearable camera, with 8,250 short clips of a
targeted person either walking 1) toward, 2) away, or 3) across the camera
wearer in indoor environments, or 4) staying still in the scene, and 13,817
person bounding boxes were manually labelled. Apart from the bounding boxes,
the dataset also contains the estimated pose of the targeted person as well as
the IMU signal of the wearable camera at each time point. An LSTM-based
encoder-decoder framework was designed to predict the future location and
movement trajectory of the targeted person in this egocentric setting.
Extensive experiments have been conducted on the new dataset, and have shown
that the proposed method is able to reliably and better predict future person
location and trajectory in egocentric videos captured by the wearable camera
compared to three baselines.

    

### [[2104.02284] Text-guided Legal Knowledge Graph Reasoning](http://arxiv.org/abs/2104.02284)


  Recent years have witnessed the prosperity of legal artificial intelligence
with the development of technologies. In this paper, we propose a novel legal
application of legal provision prediction (LPP), which aims to predict the
related legal provisions of affairs. We formulate this task as a challenging
knowledge graph completion problem, which requires not only text understanding
but also graph reasoning. To this end, we propose a novel text-guided graph
reasoning approach. We collect amounts of real-world legal provision data from
the Guangdong government service website and construct a legal dataset called
LegalLPP. Extensive experimental results on the dataset show that our approach
achieves better performance compared with baselines. The code and dataset are
available in \url{this https URL} for reproducibility.

    

### [[2104.04039] Plug-and-Blend: A Framework for Controllable Story Generation with Blended Control Codes](http://arxiv.org/abs/2104.04039)


  Large pre-trained neural language models (LM) have very powerful text
generation capabilities. However, in practice, they are hard to control for
creative purposes. We describe a Plug-and-Play controllable language generation
framework, Plug-and-Blend, that allows a human user to input multiple control
codes (topics). In the context of automated story generation, this allows a
human user loose or fine-grained control of the topics and transitions between
them that will appear in the generated story, and can even allow for
overlapping, blended topics. Automated evaluations show our framework, working
with different generative LMs, controls the generation towards given
continuous-weighted control codes while keeping the generated sentences fluent,
demonstrating strong blending capability. A human participant evaluation shows
that the generated stories are observably transitioning between two topics.

    

### [[2104.06982] To Trust or Not to Trust a Regressor: Estimating and Explaining Trustworthiness of Regression Predictions](http://arxiv.org/abs/2104.06982)


  In hybrid human-AI systems, users need to decide whether or not to trust an
algorithmic prediction while the true error in the prediction is unknown. To
accommodate such settings, we introduce RETRO-VIZ, a method for (i) estimating
and (ii) explaining trustworthiness of regression predictions. It consists of
RETRO, a quantitative estimate of the trustworthiness of a prediction, and VIZ,
a visual explanation that helps users identify the reasons for the (lack of)
trustworthiness of a prediction. We find that RETRO-scores negatively correlate
with prediction error across 117 experimental settings, indicating that RETRO
provides a useful measure to distinguish trustworthy predictions from
untrustworthy ones. In a user study with 41 participants, we find that
VIZ-explanations help users identify whether a prediction is trustworthy or
not: on average, 95.1% of participants correctly select the more trustworthy
prediction, given a pair of predictions. In addition, an average of 75.6% of
participants can accurately describe why a prediction seems to be (not)
trustworthy. Finally, we find that the vast majority of users subjectively
experience RETRO-VIZ as a useful tool to assess the trustworthiness of
algorithmic predictions.

    

### [[2106.12226] Spatio-Temporal SAR-Optical Data Fusion for Cloud Removal via a Deep Hierarchical Model](http://arxiv.org/abs/2106.12226)


  The abundance of clouds, located both spatially and temporally, often makes
remote sensing (RS) applications with optical images difficult or even
impossible to perform. Traditional cloud removing techniques have been studied
for years, and recently, Machine Learning (ML)-based approaches have also been
considered. In this manuscript, a novel method for the restoration of
clouds-corrupted optical images is presented, able to generate the whole
optical scene of interest, not only the cloudy pixels, and based on a Joint
Data Fusion paradigm, where three deep neural networks are hierarchically
combined. Spatio-temporal features are separately extracted by a conditional
Generative Adversarial Network (cGAN) and by a Convolutional Long Short-Term
Memory (ConvLSTM), from Synthetic Aperture Radar (SAR) data and optical
time-series of data respectively, and then combined with a U-shaped network.
The use of time-series of data has been rarely explored in the state of the art
for this peculiar objective, and moreover existing models do not combine both
spatio-temporal domains and SAR-optical imagery. Quantitative and qualitative
results have shown a good ability of the proposed method in producing
cloud-free images, by also preserving the details and outperforming the cGAN
and the ConvLSTM when individually used. Both the code and the dataset have
been implemented from scratch and made available to interested researchers for
further analysis and investigation.

    

### [[2107.08959] T-RECS: A Simulation Tool to Study the Societal Impact of Recommender Systems](http://arxiv.org/abs/2107.08959)


  Simulation has emerged as a popular method to study the long-term societal
consequences of recommender systems. This approach allows researchers to
specify their theoretical model explicitly and observe the evolution of
system-level outcomes over time. However, performing simulation-based studies
often requires researchers to build their own simulation environments from the
ground up, which creates a high barrier to entry, introduces room for
implementation error, and makes it difficult to disentangle whether observed
outcomes are due to the model or the implementation.
We introduce T-RECS, an open-sourced Python package designed for researchers
to simulate recommendation systems and other types of sociotechnical systems in
which an algorithm mediates the interactions between multiple stakeholders,
such as users and content creators. To demonstrate the flexibility of T-RECS,
we perform a replication of two prior simulation-based research on
sociotechnical systems. We additionally show how T-RECS can be used to generate
novel insights with minimal overhead. Our tool promotes reproducibility in this
area of research, provides a unified language for simulating sociotechnical
systems, and removes the friction of implementing simulations from scratch.

    

### [[2107.13166] Performance Analysis of Dual-Hop THz Transmission Systems over $α$-$μ$ Fading Channels with Pointing Errors](http://arxiv.org/abs/2107.13166)


  In this paper, the performance of a dual-hop relaying terahertz (THz)
wireless communication system is investigated. In particular, the behaviors of
the two THz hops are determined by three factors, which are the deterministic
path loss, the fading effects, and pointing errors. Assuming that both THz
links are subject to the $\alpha$-$\mu$ fading with pointing errors, we derive
exact expressions for the cumulative distribution function (CDF) and
probability density function (PDF) of the end-to-end signal-to-noise ratio
(SNR). Relying on the CDF and PDF, important performance metrics are evaluated,
such as the outage probability, average bit error rate, and average channel
capacity. Moreover, the asymptotic analyses are presented to obtain more
insights. Results show that the dual-hop relaying scheme has better performance
than the single THz link. The system's diversity order is
$\min\left\{\frac{\phi_1}{2},\frac{\alpha_1\mu_1}{2},\phi_2,\alpha_2\mu_2\right\}$,
where $\alpha_i$ and $\mu_i$ represent the fading parameters of the $i$-th THz
link for $i\in(1,2)$, and $\phi_i$ denotes the pointing error parameter. In
addition, we extend the analysis to a multi-relay cooperative system and derive
the asymptotic symbol error rate expressions. Results demonstrate that the
diversity order of the multi-relay system is
$K\min\left\{\frac{\phi_1}{2},\frac{\alpha_1\mu_1}{2},\phi_2,\alpha_2\mu_2\right\}$,
where $K$ is the number of relays. Finally, the derived analytical expressions
are verified by Monte Carlo simulation.

    

### [[2107.13072] The Probabilistic Termination Tool Amber](http://arxiv.org/abs/2107.13072)


  We describe the Amber tool for proving and refuting the termination of a
class of probabilistic while-programs with polynomial arithmetic, in a fully
automated manner. Amber combines martingale theory with properties of
asymptotic bounding functions and implements relaxed versions of existing
probabilistic termination proof rules to prove/disprove (positive) almost sure
termination of probabilistic loops. Amber supports programs parameterized by
symbolic constants and drawing from common probability distributions. Our
experimental comparisons give practical evidence of Amber outperforming
existing state-of-the-art tools.

    

### [[2107.13101] Papaya: Global Typestate Analysis of Aliased Objects Extended Version](http://arxiv.org/abs/2107.13101)


  Typestates are state machines used in object-oriented programming to specify
and verify correct order of method calls on an object. To avoid inconsistent
object states, typestates enforce linear typing, which eliminates - or at best
limits - aliasing. However, aliasing is an important feature in programming,
and the state-of-the-art on typestates is too restrictive if we want typestates
to be adopted in real-world software systems.
In this paper, we present a type system for an object-oriented language with
typestate annotations, which allows for unrestricted aliasing, and as opposed
to previous approaches it does not require linearity constraints. The typestate
analysis is global and tracks objects throughout the entire program graph,
which ensures that well-typed programs conform and complete the declared
protocols. We implement our framework in the Scala programming language and
illustrate our approach using a running example that shows the interplay
between typestates and aliases.

    

### [[2107.13242] Type theories in category theory](http://arxiv.org/abs/2107.13242)


  We introduce basic notions in category theory to type theorists, including
comprehension categories, categories with attributes, contextual categories,
type categories, and categories with families along with additional discussions
that are not very closely related to type theories by listing definitions,
lemmata, and remarks. By doing so, this introduction becomes more friendly as a
referential material to be read in random order (instead of from the beginning
to the end). In the end, we list some mistakes made in the early versions of
this introduction.
The interpretation of common type formers in dependent type theories are
discussed based on existing categorical constructions instead of mechanically
derived from their type theoretical definition. Non-dependent type formers
include unit, products (as fiber products), and functions (as fiber exponents),
and dependent ones include extensional equalities (as equalizers), dependent
products, and the universe of (all) propositions (as the subobject classifier).

    

### [[2107.13347] Semantics for Variational Quantum Programming](http://arxiv.org/abs/2107.13347)


  We consider a programming language that can manipulate both classical and
quantum information. Our language is type-safe and designed for variational
quantum programming, which is a hybrid classical-quantum computational
paradigm. The classical subsystem of the language is the Probabilistic FixPoint
Calculus (PFPC), which is a lambda calculus with mixed-variance recursive
types, term recursion and probabilistic choice. The quantum subsystem is a
first-order linear type system that can manipulate quantum information. The two
subsystems are related by mixed classical/quantum terms that specify how
classical probabilistic effects are induced by quantum measurements, and
conversely, how classical (probabilistic) programs can influence the quantum
dynamics. We also describe a sound and computationally adequate denotational
semantics for the language. Classical probabilistic effects are interpreted
using a recently-described commutative probabilistic monad on DCPO. Quantum
effects and resources are interpreted in a category of von Neumann algebras
that we show is enriched over (continuous) domains. This strong sense of
enrichment allows us to develop novel semantic methods that we use to interpret
the relationship between the quantum and classical probabilistic effects. By
doing so we provide the first denotational analysis that relates models of
classical probabilistic programming to models of quantum programming.

    

### [[2101.09031] CutLang V2: towards a unified Analysis Description Language](http://arxiv.org/abs/2101.09031)


  We will present the latest developments in CutLang, the runtime interpreter
of a recently-developed analysis description language (ADL) for collider data
analysis. ADL is a domain-specific, declarative language that describes the
contents of an analysis in a standard and unambiguous way, independent of any
computing framework. In ADL, analyses are written in human-readable plain text
files, separating object, variable and event selection definitions in blocks,
with a syntax that includes mathematical and logical operations, comparison and
optimisation operators, reducers, four-vector algebra and commonly used
functions. Adopting ADLs would bring numerous benefits to the LHC experimental
and phenomenological communities, ranging from analysis preservation beyond the
lifetimes of experiments or analysis software to facilitating the abstraction,
design, visualization, validation, combination, reproduction, interpretation
and overall communication of the analysis contents. Since their initial
release, ADL and CutLang have been used for implementing and running numerous
LHC analyses. In this process, the original syntax from CutLang v1 has been
modified for better ADL compatibility, and the interpreter has been adapted to
work with that syntax, resulting in the current release v2. Furthermore,
CutLang has been enhanced to handle object combinatorics, to include tables and
weights, to save events at any analysis stage, to benefit from
multi-core/multi-CPU hardware among other improvements. In this contribution,
these and other enhancements are discussed in details. In addition, real life
examples from LHC analyses are presented together with a user manual.

    