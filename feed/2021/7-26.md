
## 2021-7-26

### [<title>关于)贵阳开劳保用品发票-贵阳本地宝o1 - DockOne.io</title>](http://dockone.io/question/857715)

### [<title>关于厦门如何可以开建筑业发票-厦门本地宝v0 - DockOne.io</title>](http://dockone.io/question/857714)

### [<title>关于)兰州开劳保用品发票-兰州本地宝z2 - DockOne.io</title>](http://dockone.io/question/857713)

### [<title>关于福建如何可以开建筑业发票-福建本地宝n4 - DockOne.io</title>](http://dockone.io/question/857712)

### [<title>关于贵阳如何可以开建筑业发票-贵阳本地宝y9 - DockOne.io</title>](http://dockone.io/question/857711)

### [<title>关于)长沙开劳保用品发票-长沙本地宝d4 - DockOne.io</title>](http://dockone.io/question/857710)

### [<title>关于贵州如何可以开建筑业发票-贵州本地宝k9 - DockOne.io</title>](http://dockone.io/question/857709)

### [<title>关于)哈尔滨开劳保用品发票-哈尔滨本地宝u3 - DockOne.io</title>](http://dockone.io/question/857708)

### [<title>关于兰州如何可以开建筑业发票-兰州本地宝w7 - DockOne.io</title>](http://dockone.io/question/857707)

### [<title>关于)石家庄开劳保用品发票-石家庄本地宝b0 - DockOne.io</title>](http://dockone.io/question/857706)

### [<title>关于甘肃如何可以开建筑业发票-甘肃本地宝h6 - DockOne.io</title>](http://dockone.io/question/857705)

### [<title>关于)济南开劳保用品发票-济南本地宝p0 - DockOne.io</title>](http://dockone.io/question/857704)

### [<title>关于)沈阳开劳保用品发票-沈阳本地宝v5 - DockOne.io</title>](http://dockone.io/question/857703)

### [<title>关于长沙如何可以开建筑业发票-长沙本地宝q2 - DockOne.io</title>](http://dockone.io/question/857702)

### [<title>关于)长春开劳保用品发票-长春本地宝n3 - DockOne.io</title>](http://dockone.io/question/857701)

### [<title>关于湖南如何可以开建筑业发票-湖南本地宝i6 - DockOne.io</title>](http://dockone.io/question/857700)

### [<title>关于)海口开劳保用品发票-海口本地宝a1 - DockOne.io</title>](http://dockone.io/question/857699)

### [<title>关于哈尔滨如何可以开建筑业发票-哈尔滨本地宝v1 - DockOne.io</title>](http://dockone.io/question/857698)

### [<title>关于)三亚开劳保用品发票-三亚本地宝m3 - DockOne.io</title>](http://dockone.io/question/857697)

### [<title>关于黑龙江如何可以开建筑业发票-黑龙江本地宝z3 - DockOne.io</title>](http://dockone.io/question/857696)

### [[2107.10938] BGP-Multipath Routing in the Internet](http://arxiv.org/abs/2107.10938)


  BGP-Multipath (BGP-M) is a multipath routing technique for load balancing.
Distinct from other techniques deployed at a router inside an Autonomous System
(AS), BGP-M is deployed at a border router that has installed multiple
inter-domain border links to a neighbour AS. It uses the equal-cost multi-path
(ECMP) function of a border router to share traffic to a destination prefix on
different border links. Despite recent research interests in multipath routing,
there is little study on BGP-M.
Here we provide the first measurement and a comprehensive analysis of BGP-M
routing in the Internet. We extracted information on BGP-M from query data
collected from Looking Glass (LG) servers. We revealed that BGP-M has already
been extensively deployed and used in the Internet. A particular example is
Hurricane Electric (AS6939), a Tier-1 network operator, which has implemented
>1,000 cases of BGP-M at 69 of its border routers to prefixes in 611 of its
neighbour ASes, including many hyper-giant ASes and large content providers, on
both IPv4 and IPv6 Internet. We examined the distribution and operation of
BGP-M. We also ran traceroute using RIPE Atlas to infer the routing paths, the
schemes of traffic allocation, and the delay on border links. This study
provided the state-of-the-art knowledge on BGP-M with novel insights into the
unique features and the distinct advantages of BGP-M as an effective and
readily available technique for load balancing.

    

### [[2107.11078] HURRA! Human readable router anomaly detection](http://arxiv.org/abs/2107.11078)


  This paper presents HURRA, a system that aims to reduce the time spent by
human operators in the process of network troubleshooting. To do so, it
comprises two modules that are plugged after any anomaly detection algorithm:
(i) a first attention mechanism, that ranks the present features in terms of
their relation with the anomaly and (ii) a second module able to incorporates
previous expert knowledge seamlessly, without any need of human interaction nor
decisions. We show the efficacy of these simple processes on a collection of
real router datasets obtained from tens of ISPs which exhibit a rich variety of
anomalies and very heterogeneous set of KPIs, on which we gather manually
annotated ground truth by the operator solving the troubleshooting ticket. Our
experimental evaluation shows that (i) the proposed system is effective in
achieving high levels of agreement with the expert, that (ii) even a simple
statistical approach is able to extracting useful information from expert
knowledge gained in past cases to further improve performance and finally that
(iii) the main difficulty in live deployment concerns the automated selection
of the anomaly detection algorithm and the tuning of its hyper-parameters.

    

### [[2107.11084] SNAC: An Unbiased Metric Evaluating Topology Recognize Ability of Network Alignment](http://arxiv.org/abs/2107.11084)


  Network alignment is a problem of finding the node mapping between similar
networks. It links the data from separate sources and is widely studied in
bioinformation and social network fields. The critical difference between
network alignment and exact graph matching is that the network alignment
considers node mapping in non-isomorphic graphs with error tolerance.
Researchers usually utilize AC (accuracy) to measure the performance of network
alignments which comparing each output element with the benchmark directly.
However, this metric neglects that some nodes are naturally indistinguishable
even in single graphs (e.g., nodes have the same neighbors) and no need to
distinguish across graphs. Such neglect leads to the underestimation of models.
We propose an unbiased metric for network alignment that takes
indistinguishable nodes into consideration to address this problem. Our
detailed experiments with different scales on both synthetic and real-world
datasets demonstrate that the proposed metric correctly reflects the deviation
of result mapping from benchmark mapping as standard metric AC does. Comparing
with the AC, the proposed metric effectively blocks the effect of
indistinguishable nodes and retains stability under increasing
indistinguishable nodes.

    

### [[2107.10868] Local SGD Optimizes Overparameterized Neural Networks in Polynomial Time](http://arxiv.org/abs/2107.10868)


  In this paper we prove that Local (S)GD (or FedAvg) can optimize two-layer
neural networks with Rectified Linear Unit (ReLU) activation function in
polynomial time. Despite the established convergence theory of Local SGD on
optimizing general smooth functions in communication-efficient distributed
optimization, its convergence on non-smooth ReLU networks still eludes full
theoretical understanding. The key property used in many Local SGD analysis on
smooth function is gradient Lipschitzness, so that the gradient on local models
will not drift far away from that on averaged model. However, this decent
property does not hold in networks with non-smooth ReLU activation function. We
show that, even though ReLU network does not admit gradient Lipschitzness
property, the difference between gradients on local models and average model
will not change too much, under the dynamics of Local SGD. We validate our
theoretical results via extensive experiments. This work is the first to show
the convergence of Local SGD on non-smooth functions, and will shed lights on
the optimization theory of federated training of deep neural networks.

    

### [[2107.10869] Filament Plots for Data Visualization](http://arxiv.org/abs/2107.10869)


  We construct a computationally inexpensive 3D extension of Andrew's plots by
considering curves generated by Frenet-Serret equations and induced by
optimally smooth 2D Andrew's plots. We consider linear isometries from a
Euclidean data space to infinite dimensional spaces of 2D curves, and
parametrize the linear isometries that produce (on average) optimally smooth
curves over a given dataset. This set of optimal isometries admits many degrees
of freedom, and (using recent results on generalized Gauss sums) we identify a
particular a member of this set which admits an asymptotic projective "tour"
property. Finally, we consider the unit-length 3D curves (filaments) induced by
these 2D Andrew's plots, where the linear isometry property preserves distances
as "relative total square curvatures". This work concludes by illustrating
filament plots for several datasets. Code is available at
this https URL


### [[2107.10870] Multiclass versus Binary Differentially Private PAC Learning](http://arxiv.org/abs/2107.10870)


  We show a generic reduction from multiclass differentially private PAC
learning to binary private PAC learning. We apply this transformation to a
recently proposed binary private PAC learner to obtain a private multiclass
learner with sample complexity that has a polynomial dependence on the
multiclass Littlestone dimension and a poly-logarithmic dependence on the
number of classes. This yields an exponential improvement in the dependence on
both parameters over learners from previous work. Our proof extends the notion
of $\Psi$-dimension defined in work of Ben-David et al. [JCSS '95] to the
online setting and explores its general properties.

    

### [[2107.10873] On the Certified Robustness for Ensemble Models and Beyond](http://arxiv.org/abs/2107.10873)


  Recent studies show that deep neural networks (DNN) are vulnerable to
adversarial examples, which aim to mislead DNNs by adding perturbations with
small magnitude. To defend against such attacks, both empirical and theoretical
defense approaches have been extensively studied for a single ML model. In this
work, we aim to analyze and provide the certified robustness for ensemble ML
models, together with the sufficient and necessary conditions of robustness for
different ensemble protocols. Although ensemble models are shown more robust
than a single model empirically; surprisingly, we find that in terms of the
certified robustness the standard ensemble models only achieve marginal
improvement compared to a single model. Thus, to explore the conditions that
guarantee to provide certifiably robust ensemble ML models, we first prove that
diversified gradient and large confidence margin are sufficient and necessary
conditions for certifiably robust ensemble models under the model-smoothness
assumption. We then provide the bounded model-smoothness analysis based on the
proposed Ensemble-before-Smoothing strategy. We also prove that an ensemble
model can always achieve higher certified robustness than a single base model
under mild conditions. Inspired by the theoretical findings, we propose the
lightweight Diversity Regularized Training (DRT) to train certifiably robust
ensemble ML models. Extensive experiments show that our DRT enhanced ensembles
can consistently achieve higher certified robustness than existing single and
ensemble ML models, demonstrating the state-of-the-art certified L2-robustness
on MNIST, CIFAR-10, and ImageNet datasets.

    

### [[2107.10878] Bagging, optimized dynamic mode decomposition (BOP-DMD) for robust, stable forecasting with spatial and temporal uncertainty-quantification](http://arxiv.org/abs/2107.10878)


  Dynamic mode decomposition (DMD) provides a regression framework for
adaptively learning a best-fit linear dynamics model over snapshots of
temporal, or spatio-temporal, data. A diversity of regression techniques have
been developed for producing the linear model approximation whose solutions are
exponentials in time. For spatio-temporal data, DMD provides low-rank and
interpretable models in the form of dominant modal structures along with their
exponential/oscillatory behavior in time. The majority of DMD algorithms,
however, are prone to bias errors from noisy measurements of the dynamics,
leading to poor model fits and unstable forecasting capabilities. The optimized
DMD algorithm minimizes the model bias with a variable projection optimization,
thus leading to stabilized forecasting capabilities. Here, the optimized DMD
algorithm is improved by using statistical bagging methods whereby a single set
of snapshots is used to produce an ensemble of optimized DMD models. The
outputs of these models are averaged to produce a bagging, optimized dynamic
mode decomposition (BOP-DMD). BOP-DMD not only improves performance, it also
robustifies the model and provides both spatial and temporal uncertainty
quantification (UQ). Thus unlike currently available DMD algorithms, BOP-DMD
provides a stable and robust model for probabilistic, or Bayesian forecasting
with comprehensive UQ metrics.

    

### [[2107.10879] Discovering Sparse Interpretable Dynamics from Partial Observations](http://arxiv.org/abs/2107.10879)


  Identifying the governing equations of a nonlinear dynamical system is key to
both understanding the physical features of the system and constructing an
accurate model of the dynamics that generalizes well beyond the available data.
We propose a machine learning framework for discovering these governing
equations using only partial observations, combining an encoder for state
reconstruction with a sparse symbolic model. Our tests show that this method
can successfully reconstruct the full system state and identify the underlying
dynamics for a variety of ODE and PDE systems.

    

### [[2107.10882] Size doesn't matter: predicting physico- or biochemical properties based on dozens of molecules](http://arxiv.org/abs/2107.10882)


  The use of machine learning in chemistry has become a common practice. At the
same time, despite the success of modern machine learning methods, the lack of
data limits their use. Using a transfer learning methodology can help solve
this problem. This methodology assumes that a model built on a sufficient
amount of data captures general features of the chemical compound structure on
which it was trained and that the further reuse of these features on a dataset
with a lack of data will greatly improve the quality of the new model. In this
paper, we develop this approach for small organic molecules, implementing
transfer learning with graph convolutional neural networks. The paper shows a
significant improvement in the performance of models for target properties with
a lack of data. The effects of the dataset composition on model quality and the
applicability domain of the resulting models are also considered.

    

### [[2107.10884] Structured second-order methods via natural gradient descent](http://arxiv.org/abs/2107.10884)


  In this paper, we propose new structured second-order methods and structured
adaptive-gradient methods obtained by performing natural-gradient descent on
structured parameter spaces. Natural-gradient descent is an attractive approach
to design new algorithms in many settings such as gradient-free,
adaptive-gradient, and second-order methods. Our structured methods not only
enjoy a structural invariance but also admit a simple expression. Finally, we
test the efficiency of our proposed methods on both deterministic non-convex
problems and deep learning problems.

    

### [[2107.10901] A reinforcement learning approach to resource allocation in genomic selection](http://arxiv.org/abs/2107.10901)


  Genomic selection (GS) is a technique that plant breeders use to select
individuals to mate and produce new generations of species. Allocation of
resources is a key factor in GS. At each selection cycle, breeders are facing
the choice of budget allocation to make crosses and produce the next generation
of breeding parents. Inspired by recent advances in reinforcement learning for
AI problems, we develop a reinforcement learning-based algorithm to
automatically learn to allocate limited resources across different generations
of breeding. We mathematically formulate the problem in the framework of Markov
Decision Process (MDP) by defining state and action spaces. To avoid the
explosion of the state space, an integer linear program is proposed that
quantifies the trade-off between resources and time. Finally, we propose a
value function approximation method to estimate the action-value function and
then develop a greedy policy improvement technique to find the optimal
resources. We demonstrate the effectiveness of the proposed method in enhancing
genetic gain using a case study with realistic data.

    

### [[2107.10931] Domain Generalization under Conditional and Label Shifts via Variational Bayesian Inference](http://arxiv.org/abs/2107.10931)


  In this work, we propose a domain generalization (DG) approach to learn on
several labeled source domains and transfer knowledge to a target domain that
is inaccessible in training. Considering the inherent conditional and label
shifts, we would expect the alignment of $p(x|y)$ and $p(y)$. However, the
widely used domain invariant feature learning (IFL) methods relies on aligning
the marginal concept shift w.r.t. $p(x)$, which rests on an unrealistic
assumption that $p(y)$ is invariant across domains. We thereby propose a novel
variational Bayesian inference framework to enforce the conditional
distribution alignment w.r.t. $p(x|y)$ via the prior distribution matching in a
latent space, which also takes the marginal label shift w.r.t. $p(y)$ into
consideration with the posterior alignment. Extensive experiments on various
benchmarks demonstrate that our framework is robust to the label shift and the
cross-domain accuracy is significantly improved, thereby achieving superior
performance over the conventional IFL counterparts.

    

### [[2107.10932] FNetAR: Mixing Tokens with Autoregressive Fourier Transforms](http://arxiv.org/abs/2107.10932)


  In this note we examine the autoregressive generalization of the FNet
algorithm, in which self-attention layers from the standard Transformer
architecture are substituted with a trivial sparse-uniformsampling procedure
based on Fourier transforms. Using the Wikitext-103 benchmark, we
demonstratethat FNetAR retains state-of-the-art performance (25.8 ppl) on the
task of causal language modelingcompared to a Transformer-XL baseline (24.2
ppl) with only half the number self-attention layers,thus providing further
evidence for the superfluity of deep neural networks with heavily
compoundedattention mechanisms. The autoregressive Fourier transform could
likely be used for parameterreduction on most Transformer-based time-series
prediction models.

    

### [[2107.10935] DeepTitle -- Leveraging BERT to generate Search Engine Optimized Headlines](http://arxiv.org/abs/2107.10935)


  Automated headline generation for online news articles is not a trivial task
- machine generated titles need to be grammatically correct, informative,
capture attention and generate search traffic without being "click baits" or
"fake news". In this paper we showcase how a pre-trained language model can be
leveraged to create an abstractive news headline generator for German language.
We incorporate state of the art fine-tuning techniques for abstractive text
summarization, i.e. we use different optimizers for the encoder and decoder
where the former is pre-trained and the latter is trained from scratch. We
modify the headline generation to incorporate frequently sought keywords
relevant for search engine optimization. We conduct experiments on a German
news data set and achieve a ROUGE-L-gram F-score of 40.02. Furthermore, we
address the limitations of ROUGE for measuring the quality of text
summarization by introducing a sentence similarity metric and human evaluation.

    

### [[2107.10939] What are you optimizing for? Aligning Recommender Systems with Human Values](http://arxiv.org/abs/2107.10939)


  We describe cases where real recommender systems were modified in the service
of various human values such as diversity, fairness, well-being, time well
spent, and factual accuracy. From this we identify the current practice of
values engineering: the creation of classifiers from human-created data with
value-based labels. This has worked in practice for a variety of issues, but
problems are addressed one at a time, and users and other stakeholders have
seldom been involved. Instead, we look to AI alignment work for approaches that
could learn complex values directly from stakeholders, and identify four major
directions: useful measures of alignment, participatory design and operation,
interactive value learning, and informed deliberative judgments.

    

### [[2107.10955] Linear Polytree Structural Equation Models: Structural Learning and Inverse Correlation Estimation](http://arxiv.org/abs/2107.10955)


  We are interested in the problem of learning the directed acyclic graph (DAG)
when data are generated from a linear structural equation model (SEM) and the
causal structure can be characterized by a polytree. Specially, under both
Gaussian and sub-Gaussian models, we study the sample size conditions for the
well-known Chow-Liu algorithm to exactly recover the equivalence class of the
polytree, which is uniquely represented by a CPDAG. We also study the error
rate for the estimation of the inverse correlation matrix under such models.
Our theoretical findings are illustrated by comprehensive numerical
simulations, and experiments on benchmark data also demonstrate the robustness
of the method when the ground truth graphical structure can only be
approximated by a polytree.

    

### [[2107.10957] Ego-GNNs: Exploiting Ego Structures in Graph Neural Networks](http://arxiv.org/abs/2107.10957)


  Graph neural networks (GNNs) have achieved remarkable success as a framework
for deep learning on graph-structured data. However, GNNs are fundamentally
limited by their tree-structured inductive bias: the WL-subtree kernel
formulation bounds the representational capacity of GNNs, and polynomial-time
GNNs are provably incapable of recognizing triangles in a graph. In this work,
we propose to augment the GNN message-passing operations with information
defined on ego graphs (i.e., the induced subgraph surrounding each node). We
term these approaches Ego-GNNs and show that Ego-GNNs are provably more
powerful than standard message-passing GNNs. In particular, we show that
Ego-GNNs are capable of recognizing closed triangles, which is essential given
the prominence of transitivity in real-world graphs. We also motivate our
approach from the perspective of graph signal processing as a form of multiplex
graph convolution. Experimental results on node classification using synthetic
and real data highlight the achievable performance gains using this approach.

    

### [[2107.10960] Implicit Rate-Constrained Optimization of Non-decomposable Objectives](http://arxiv.org/abs/2107.10960)


  We consider a popular family of constrained optimization problems arising in
machine learning that involve optimizing a non-decomposable evaluation metric
with a certain thresholded form, while constraining another metric of interest.
Examples of such problems include optimizing the false negative rate at a fixed
false positive rate, optimizing precision at a fixed recall, optimizing the
area under the precision-recall or ROC curves, etc. Our key idea is to
formulate a rate-constrained optimization that expresses the threshold
parameter as a function of the model parameters via the Implicit Function
theorem. We show how the resulting optimization problem can be solved using
standard gradient based methods. Experiments on benchmark datasets demonstrate
the effectiveness of our proposed method over existing state-of-the art
approaches for these problems.

    

### [[2107.10963] Compositional Models: Multi-Task Learning and Knowledge Transfer with Modular Networks](http://arxiv.org/abs/2107.10963)


  Conditional computation and modular networks have been recently proposed for
multitask learning and other problems as a way to decompose problem solving
into multiple reusable computational blocks. We propose a new approach for
learning modular networks based on the isometric version of ResNet with all
residual blocks having the same configuration and the same number of
parameters. This architectural choice allows adding, removing and changing the
order of residual blocks. In our method, the modules can be invoked repeatedly
and allow knowledge transfer to novel tasks by adjusting the order of
computation. This allows soft weight sharing between tasks with only a small
increase in the number of parameters. We show that our method leads to
interpretable self-organization of modules in case of multi-task learning,
transfer learning and domain adaptation while achieving competitive results on
those tasks. From practical perspective, our approach allows to: (a) reuse
existing modules for learning new task by adjusting the computation order, (b)
use it for unsupervised multi-source domain adaptation to illustrate that
adaptation to unseen data can be achieved by only manipulating the order of
pretrained modules, (c) show how our approach can be used to increase accuracy
of existing architectures for image classification tasks such as ImageNet,
without any parameter increase, by reusing the same block multiple times.

    

### [[2107.10970] The decomposition of the higher-order homology embedding constructed from the $k$-Laplacian](http://arxiv.org/abs/2107.10970)


  The null space of the $k$-th order Laplacian $\mathbf{\mathcal L}_k$, known
as the {\em $k$-th homology vector space}, encodes the non-trivial topology of
a manifold or a network. Understanding the structure of the homology embedding
can thus disclose geometric or topological information from the data. The study
of the null space embedding of the graph Laplacian $\mathbf{\mathcal L}_0$ has
spurred new research and applications, such as spectral clustering algorithms
with theoretical guarantees and estimators of the Stochastic Block Model. In
this work, we investigate the geometry of the $k$-th homology embedding and
focus on cases reminiscent of spectral clustering. Namely, we analyze the {\em
connected sum} of manifolds as a perturbation to the direct sum of their
homology embeddings. We propose an algorithm to factorize the homology
embedding into subspaces corresponding to a manifold's simplest topological
components. The proposed framework is applied to the {\em shortest homologous
loop detection} problem, a problem known to be NP-hard in general. Our spectral
loop detection algorithm scales better than existing methods and is effective
on diverse data such as point clouds and images.

    

### [[2107.10971] Adaptively Weighted Top-N Recommendation for Organ Matching](http://arxiv.org/abs/2107.10971)


  Reducing the shortage of organ donations to meet the demands of patients on
the waiting list has being a major challenge in organ transplantation. Because
of the shortage, organ matching decision is the most critical decision to
assign the limited viable organs to the most suitable patients. Currently,
organ matching decisions were only made by matching scores calculated via
scoring models, which are built by the first principles. However, these models
may disagree with the actual post-transplantation matching performance (e.g.,
patient's post-transplant quality of life (QoL) or graft failure measurements).
In this paper, we formulate the organ matching decision-making as a top-N
recommendation problem and propose an Adaptively Weighted Top-N Recommendation
(AWTR) method. AWTR improves performance of the current scoring models by using
limited actual matching performance in historical data set as well as the
collected covariates from organ donors and patients. AWTR sacrifices the
overall recommendation accuracy by emphasizing the recommendation and ranking
accuracy for top-N matched patients. The proposed method is validated in a
simulation study, where KAS [60] is used to simulate the organ-patient
recommendation response. The results show that our proposed method outperforms
seven state-of-the-art top-N recommendation benchmark methods.

    

### [[2107.10976] Federated Learning Versus Classical Machine Learning: A Convergence Comparison](http://arxiv.org/abs/2107.10976)


  In the past few decades, machine learning has revolutionized data processing
for large scale applications. Simultaneously, increasing privacy threats in
trending applications led to the redesign of classical data training models. In
particular, classical machine learning involves centralized data training,
where the data is gathered, and the entire training process executes at the
central server. Despite significant convergence, this training involves several
privacy threats on participants' data when shared with the central cloud
server. To this end, federated learning has achieved significant importance
over distributed data training. In particular, the federated learning allows
participants to collaboratively train the local models on local data without
revealing their sensitive information to the central cloud server. In this
paper, we perform a convergence comparison between classical machine learning
and federated learning on two publicly available datasets, namely,
logistic-regression-MNIST dataset and image-classification-CIFAR-10 dataset.
The simulation results demonstrate that federated learning achieves higher
convergence within limited communication rounds while maintaining participants'
anonymity. We hope that this research will show the benefits and help federated
learning to be implemented widely.

    

### [[2107.10977] Tsformer: Time series Transformer for tourism demand forecasting](http://arxiv.org/abs/2107.10977)


  AI-based methods have been widely applied to tourism demand forecasting.
However, current AI-based methods are short of the ability to process long-term
dependency, and most of them lack interpretability. The Transformer used
initially for machine translation shows an incredible ability to long-term
dependency processing. Based on the Transformer, we proposed a time series
Transformer (Tsformer) with Encoder-Decoder architecture for tourism demand
forecasting. The proposed Tsformer encodes long-term dependency with encoder,
captures short-term dependency with decoder, and simplifies the attention
interactions under the premise of highlighting dominant attention through a
series of attention masking mechanisms. These improvements make the multi-head
attention mechanism process the input sequence according to the time
relationship, contributing to better interpretability. What's more, the context
processing ability of the Encoder-Decoder architecture allows adopting the
calendar of days to be forecasted to enhance the forecasting performance.
Experiments conducted on the Jiuzhaigou valley and Siguniang mountain tourism
demand datasets with other nine baseline methods indicate that the proposed
Tsformer outperformed all baseline models in the short-term and long-term
tourism demand forecasting tasks. Moreover, ablation studies demonstrate that
the adoption of the calendar of days to be forecasted contributes to the
forecasting performance of the proposed Tsformer. For better interpretability,
the attention weight matrix visualization is performed. It indicates that the
Tsformer concentrates on seasonal features and days close to days to be
forecast in short-term forecasting.

    

### [[2107.10980] Economic Recession Prediction Using Deep Neural Network](http://arxiv.org/abs/2107.10980)


  We investigate the effectiveness of different machine learning methodologies
in predicting economic cycles. We identify the deep learning methodology of
Bi-LSTM with Autoencoder as the most accurate model to forecast the beginning
and end of economic recessions in the U.S. We adopt commonly-available macro
and market-condition features to compare the ability of different machine
learning models to generate good predictions both in-sample and out-of-sample.
The proposed model is flexible and dynamic when both predictive variables and
model coefficients vary over time. It provided good out-of-sample predictions
for the past two recessions and early warning about the COVID-19 recession.

    

### [[2107.10989] Estimating Predictive Uncertainty Under Program Data Distribution Shift](http://arxiv.org/abs/2107.10989)


  Deep learning (DL) techniques have achieved great success in predictive
accuracy in a variety of tasks, but deep neural networks (DNNs) are shown to
produce highly overconfident scores for even abnormal samples. Well-defined
uncertainty indicates whether a model's output should (or should not) be
trusted and thus becomes critical in real-world scenarios which typically
involves shifted input distributions due to many factors. Existing uncertainty
approaches assume that testing samples from a different data distribution would
induce unreliable model predictions thus have higher uncertainty scores. They
quantify model uncertainty by calibrating DL model's confidence of a given
input and evaluate the effectiveness in computer vision (CV) and natural
language processing (NLP)-related tasks. However, their methodologies'
reliability may be compromised under programming tasks due to difference in
data representations and shift patterns. In this paper, we first define three
different types of distribution shift in program data and build a large-scale
shifted Java dataset. We implement two common programming language tasks on our
dataset to study the effect of each distribution shift on DL model performance.
We also propose a large-scale benchmark of existing state-of-the-art predictive
uncertainty on programming tasks and investigate their effectiveness under data
distribution shift. Experiments show that program distribution shift does
degrade the DL model performance to varying degrees and that existing
uncertainty methods all present certain limitations in quantifying uncertainty
on program dataset.

    

### [[2107.10991] A novel meta-learning initialization method for physics-informed neural networks](http://arxiv.org/abs/2107.10991)


  Physics-informed neural networks (PINNs) have been widely used to solve
various scientific computing problems. However, large training costs limit
PINNs for some real-time applications. Although some works have been proposed
to improve the training efficiency of PINNs, few consider the influence of
initialization. To this end, we propose a New Reptile initialization based
Physics-Informed Neural Network (NRPINN). The original Reptile algorithm is a
meta-learning initialization method based on labeled data. PINNs can be trained
with less labeled data or even without any labeled data by adding partial
differential equations (PDEs) as a penalty term into the loss function.
Inspired by this idea, we propose the new Reptile initialization to sample more
tasks from the parameterized PDEs and adapt the penalty term of the loss. The
new Reptile initialization can acquire initialization parameters from related
tasks by supervised, unsupervised, and semi-supervised learning. Then, PINNs
with initialization parameters can efficiently solve PDEs. Besides, the new
Reptile initialization can also be used for the variants of PINNs. Finally, we
demonstrate and verify the NRPINN considering both forward problems, including
solving Poisson, Burgers, and Schrödinger equations, as well as inverse
problems, where unknown parameters in the PDEs are estimated. Experimental
results show that the NRPINN training is much faster and achieves higher
accuracy than PINNs with other initialization methods.

    

### [[2107.10996] Communication Efficiency in Federated Learning: Achievements and Challenges](http://arxiv.org/abs/2107.10996)


  Federated Learning (FL) is known to perform Machine Learning tasks in a
distributed manner. Over the years, this has become an emerging technology
especially with various data protection and privacy policies being imposed FL
allows performing machine learning tasks whilst adhering to these challenges.
As with the emerging of any new technology, there are going to be challenges
and benefits. A challenge that exists in FL is the communication costs, as FL
takes place in a distributed environment where devices connected over the
network have to constantly share their updates this can create a communication
bottleneck. In this paper, we present a survey of the research that is
performed to overcome the communication constraints in an FL setting.

    

### [[2107.11003] Model Selection for Offline Reinforcement Learning: Practical Considerations for Healthcare Settings](http://arxiv.org/abs/2107.11003)


  Reinforcement learning (RL) can be used to learn treatment policies and aid
decision making in healthcare. However, given the need for generalization over
complex state/action spaces, the incorporation of function approximators (e.g.,
deep neural networks) requires model selection to reduce overfitting and
improve policy performance at deployment. Yet a standard validation pipeline
for model selection requires running a learned policy in the actual
environment, which is often infeasible in a healthcare setting. In this work,
we investigate a model selection pipeline for offline RL that relies on
off-policy evaluation (OPE) as a proxy for validation performance. We present
an in-depth analysis of popular OPE methods, highlighting the additional
hyperparameters and computational requirements (fitting/inference of auxiliary
models) when used to rank a set of candidate policies. We compare the utility
of different OPE methods as part of the model selection pipeline in the context
of learning to treat patients with sepsis. Among all the OPE methods we
considered, fitted Q evaluation (FQE) consistently leads to the best validation
ranking, but at a high computational cost. To balance this trade-off between
accuracy of ranking and computational efficiency, we propose a simple two-stage
approach to accelerate model selection by avoiding potentially unnecessary
computation. Our work serves as a practical guide for offline RL model
selection and can help RL practitioners select policies using real-world
datasets. To facilitate reproducibility and future extensions, the code
accompanying this paper is available online at
this https URL.

    

### [[2107.11011] VisDA-2021 Competition Universal Domain Adaptation to Improve Performance on Out-of-Distribution Data](http://arxiv.org/abs/2107.11011)


  Progress in machine learning is typically measured by training and testing a
model on the same distribution of data, i.e., the same domain. This
over-estimates future accuracy on out-of-distribution data. The Visual Domain
Adaptation (VisDA) 2021 competition tests models' ability to adapt to novel
test distributions and handle distributional shift. We set up unsupervised
domain adaptation challenges for image classifiers and will evaluate adaptation
to novel viewpoints, backgrounds, modalities and degradation in quality. Our
challenge draws on large-scale publicly available datasets but constructs the
evaluation across domains, rather that the traditional in-domain bench-marking.
Furthermore, we focus on the difficult "universal" setting where, in addition
to input distribution drift, methods may encounter missing and/or novel classes
in the target dataset. Performance will be measured using a rigorous protocol,
comparing to state-of-the-art domain adaptation methods with the help of
established metrics. We believe that the competition will encourage further
improvement in machine learning methods' ability to handle realistic data in
many deployment scenarios.

    

### [[2107.11022] AD-GAN: End-to-end Unsupervised Nuclei Segmentation with Aligned Disentangling Training](http://arxiv.org/abs/2107.11022)


  We consider unsupervised cell nuclei segmentation in this paper. Exploiting
the recently-proposed unpaired image-to-image translation between cell nuclei
images and randomly synthetic masks, existing approaches, e.g., CycleGAN, have
achieved encouraging results. However, these methods usually take a two-stage
pipeline and fail to learn end-to-end in cell nuclei images. More seriously,
they could lead to the lossy transformation problem, i.e., the content
inconsistency between the original images and the corresponding segmentation
output. To address these limitations, we propose a novel end-to-end
unsupervised framework called Aligned Disentangling Generative Adversarial
Network (AD-GAN). Distinctively, AD-GAN introduces representation
disentanglement to separate content representation (the underling spatial
structure) from style representation (the rendering of the structure). With
this framework, spatial structure can be preserved explicitly, enabling a
significant reduction of macro-level lossy transformation. We also propose a
novel training algorithm able to align the disentangled content in the latent
space to reduce micro-level lossy transformation. Evaluations on real-world 2D
and 3D datasets show that AD-GAN substantially outperforms the other comparison
methods and the professional software both quantitatively and qualitatively.
Specifically, the proposed AD-GAN leads to significant improvement over the
current best unsupervised methods by an average 17.8% relatively (w.r.t. the
metric DICE) on four cell nuclei datasets. As an unsupervised method, AD-GAN
even performs competitive with the best supervised models, taking a further
leap towards end-to-end unsupervised nuclei segmentation.

    

### [[2107.11042] Deep Learning Based Reconstruction of Total Solar Irradiance](http://arxiv.org/abs/2107.11042)


  The Earth's primary source of energy is the radiant energy generated by the
Sun, which is referred to as solar irradiance, or total solar irradiance (TSI)
when all of the radiation is measured. A minor change in the solar irradiance
can have a significant impact on the Earth's climate and atmosphere. As a
result, studying and measuring solar irradiance is crucial in understanding
climate changes and solar variability. Several methods have been developed to
reconstruct total solar irradiance for long and short periods of time; however,
they are physics-based and rely on the availability of data, which does not go
beyond 9,000 years. In this paper we propose a new method, called TSInet, to
reconstruct total solar irradiance by deep learning for short and long periods
of time that span beyond the physical models' data availability. On the data
that are available, our method agrees well with the state-of-the-art
physics-based reconstruction models. To our knowledge, this is the first time
that deep learning has been used to reconstruct total solar irradiance for more
than 9,000 years.

    

### [[2107.11045] Ensemble of Convolution Neural Networks on Heterogeneous Signals for Sleep Stage Scoring](http://arxiv.org/abs/2107.11045)


  Over the years, several approaches have tried to tackle the problem of
performing an automatic scoring of the sleeping stages. Although any
polysomnography usually collects over a dozen of different signals, this
particular problem has been mainly tackled by using only the
Electroencephalograms presented in those records. On the other hand, the other
recorded signals have been mainly ignored by most works. This paper explores
and compares the convenience of using additional signals apart from
electroencephalograms. More specifically, this work uses the SHHS-1 dataset
with 5,804 patients containing an electromyogram recorded simultaneously as two
electroencephalograms. To compare the results, first, the same architecture has
been evaluated with different input signals and all their possible
combinations. These tests show how, using more than one signal especially if
they are from different sources, improves the results of the classification.
Additionally, the best models obtained for each combination of one or more
signals have been used in ensemble models and, its performance has been
compared showing the convenience of using these multi-signal models to improve
the classification. The best overall model, an ensemble of Depth-wise
Separational Convolutional Neural Networks, has achieved an accuracy of 86.06\%
with a Cohen's Kappa of 0.80 and a $F_{1}$ of 0.77. Up to date, those are the
best results on the complete dataset and it shows a significant improvement in
the precision and recall for the most uncommon class in the dataset.

    

### [[2107.11046] Learning the structure of wind: A data-driven nonlocal turbulence model for the atmospheric boundary layer](http://arxiv.org/abs/2107.11046)


  We develop a novel data-driven approach to modeling the atmospheric boundary
layer. This approach leads to a nonlocal, anisotropic synthetic turbulence
model which we refer to as the deep rapid distortion (DRD) model. Our approach
relies on an operator regression problem which characterizes the best fitting
candidate in a general family of nonlocal covariance kernels parameterized in
part by a neural network. This family of covariance kernels is expressed in
Fourier space and is obtained from approximate solutions to the Navier--Stokes
equations at very high Reynolds numbers. Each member of the family incorporates
important physical properties such as mass conservation and a realistic energy
cascade. The DRD model can be calibrated with noisy data from field
experiments. After calibration, the model can be used to generate synthetic
turbulent velocity fields. To this end, we provide a new numerical method based
on domain decomposition which delivers scalable, memory-efficient turbulence
generation with the DRD model as well as others. We demonstrate the robustness
of our approach with both filtered and noisy data coming from the 1968 Air
Force Cambridge Research Laboratory Kansas experiments. Using this data, we
witness exceptional accuracy with the DRD model, especially when compared to
the International Electrotechnical Commission standard.

    

### [[2107.11049] MCDAL: Maximum Classifier Discrepancy for Active Learning](http://arxiv.org/abs/2107.11049)


  Recent state-of-the-art active learning methods have mostly leveraged
Generative Adversarial Networks (GAN) for sample acquisition; however, GAN is
usually known to suffer from instability and sensitivity to hyper-parameters.
In contrast to these methods, we propose in this paper a novel active learning
framework that we call Maximum Classifier Discrepancy for Active Learning
(MCDAL) which takes the prediction discrepancies between multiple classifiers.
In particular, we utilize two auxiliary classification layers that learn
tighter decision boundaries by maximizing the discrepancies among them.
Intuitively, the discrepancies in the auxiliary classification layers'
predictions indicate the uncertainty in the prediction. In this regard, we
propose a novel method to leverage the classifier discrepancies for the
acquisition function for active learning. We also provide an interpretation of
our idea in relation to existing GAN based active learning methods and domain
adaptation frameworks. Moreover, we empirically demonstrate the utility of our
approach where the performance of our approach exceeds the state-of-the-art
methods on several image classification and semantic segmentation datasets in
active learning setups.

    

### [[2107.11053] An Adaptive State Aggregation Algorithm for Markov Decision Processes](http://arxiv.org/abs/2107.11053)


  Value iteration is a well-known method of solving Markov Decision Processes
(MDPs) that is simple to implement and boasts strong theoretical convergence
guarantees. However, the computational cost of value iteration quickly becomes
infeasible as the size of the state space increases. Various methods have been
proposed to overcome this issue for value iteration in large state and action
space MDPs, often at the price, however, of generalizability and algorithmic
simplicity. In this paper, we propose an intuitive algorithm for solving MDPs
that reduces the cost of value iteration updates by dynamically grouping
together states with similar cost-to-go values. We also prove that our
algorithm converges almost surely to within \(2\varepsilon / (1 - \gamma)\) of
the true optimal value in the \(\ell^\infty\) norm, where \(\gamma\) is the
discount factor and aggregated states differ by at most \(\varepsilon\).
Numerical experiments on a variety of simulated environments confirm the
robustness of our algorithm and its ability to solve MDPs with much cheaper
updates especially as the scale of the MDP problem increases.

    

### [[2107.11056] Improving the Generalization of Meta-learning on Unseen Domains via Adversarial Shift](http://arxiv.org/abs/2107.11056)


  Meta-learning provides a promising way for learning to efficiently learn and
achieves great success in many applications. However, most meta-learning
literature focuses on dealing with tasks from a same domain, making it brittle
to generalize to tasks from the other unseen domains. In this work, we address
this problem by simulating tasks from the other unseen domains to improve the
generalization and robustness of meta-learning method. Specifically, we propose
a model-agnostic shift layer to learn how to simulate the domain shift and
generate pseudo tasks, and develop a new adversarial learning-to-learn
mechanism to train it. Based on the pseudo tasks, the meta-learning model can
learn cross-domain meta-knowledge, which can generalize well on unseen domains.
We conduct extensive experiments under the domain generalization setting.
Experimental results demonstrate that the proposed shift layer is applicable to
various meta-learning frameworks. Moreover, our method also leads to
state-of-the-art performance on different cross-domain few-shot classification
benchmarks and produces good results on cross-domain few-shot regression.

    

### [[2107.11059] LocalGLMnet: interpretable deep learning for tabular data](http://arxiv.org/abs/2107.11059)


  Deep learning models have gained great popularity in statistical modeling
because they lead to very competitive regression models, often outperforming
classical statistical models such as generalized linear models. The
disadvantage of deep learning models is that their solutions are difficult to
interpret and explain, and variable selection is not easily possible because
deep learning models solve feature engineering and variable selection
internally in a nontransparent way. Inspired by the appealing structure of
generalized linear models, we propose a new network architecture that shares
similar features as generalized linear models, but provides superior predictive
power benefiting from the art of representation learning. This new architecture
allows for variable selection of tabular data and for interpretation of the
calibrated deep learning model, in fact, our approach provides an additive
decomposition in the spirit of Shapley values and integrated gradients.

    

### [[2107.11077] Reservoir Computing Approach for Gray Images Segmentation](http://arxiv.org/abs/2107.11077)


  The paper proposes a novel approach for gray scale images segmentation. It is
based on multiple features extraction from single feature per image pixel,
namely its intensity value, using Echo state network. The newly extracted
features -- reservoir equilibrium states -- reveal hidden image characteristics
that improve its segmentation via a clustering algorithm. Moreover, it was
demonstrated that the intrinsic plasticity tuning of reservoir fits its
equilibrium states to the original image intensity distribution thus allowing
for its better segmentation. The proposed approach is tested on the benchmark
image Lena.

    

### [[2107.11085] Data-driven deep density estimation](http://arxiv.org/abs/2107.11085)


  Density estimation plays a crucial role in many data analysis tasks, as it
infers a continuous probability density function (PDF) from discrete samples.
Thus, it is used in tasks as diverse as analyzing population data, spatial
locations in 2D sensor readings, or reconstructing scenes from 3D scans. In
this paper, we introduce a learned, data-driven deep density estimation (DDE)
to infer PDFs in an accurate and efficient manner, while being independent of
domain dimensionality or sample size. Furthermore, we do not require access to
the original PDF during estimation, neither in parametric form, nor as priors,
or in the form of many samples. This is enabled by training an unstructured
convolutional neural network on an infinite stream of synthetic PDFs, as
unbound amounts of synthetic training data generalize better across a deck of
natural PDFs than any natural finite training data will do. Thus, we hope that
our publicly available DDE method will be beneficial in many areas of data
analysis, where continuous models are to be estimated from discrete
observations.

    

### [[2107.11098] Generative adversarial networks in time series: A survey and taxonomy](http://arxiv.org/abs/2107.11098)


  Generative adversarial networks (GANs) studies have grown exponentially in
the past few years. Their impact has been seen mainly in the computer vision
field with realistic image and video manipulation, especially generation,
making significant advancements. While these computer vision advances have
garnered much attention, GAN applications have diversified across disciplines
such as time series and sequence generation. As a relatively new niche for
GANs, fieldwork is ongoing to develop high quality, diverse and private time
series data. In this paper, we review GAN variants designed for time series
related applications. We propose a taxonomy of discrete-variant GANs and
continuous-variant GANs, in which GANs deal with discrete time series and
continuous time series data. Here we showcase the latest and most popular
literature in this field; their architectures, results, and applications. We
also provide a list of the most popular evaluation metrics and their
suitability across applications. Also presented is a discussion of privacy
measures for these GANs and further protections and directions for dealing with
sensitive data. We aim to frame clearly and concisely the latest and
state-of-the-art research in this area and their applications to real-world
technologies.

    

### [[2107.11099] RGB Image Classification with Quantum Convolutional Ansaetze](http://arxiv.org/abs/2107.11099)


  With the rapid growth of qubit numbers and coherence times in quantum
hardware technology, implementing shallow neural networks on the so-called
Noisy Intermediate-Scale Quantum (NISQ) devices has attracted a lot of
interest. Many quantum (convolutional) circuit ansaetze are proposed for
grayscale images classification tasks with promising empirical results.
However, when applying these ansaetze on RGB images, the intra-channel
information that is useful for vision tasks is not extracted effectively. In
this paper, we propose two types of quantum circuit ansaetze to simulate
convolution operations on RGB images, which differ in the way how inter-channel
and intra-channel information are extracted. To the best of our knowledge, this
is the first work of a quantum convolutional circuit to deal with RGB images
effectively, with a higher test accuracy compared to the purely classical CNNs.
We also investigate the relationship between the size of quantum circuit ansatz
and the learnability of the hybrid quantum-classical convolutional neural
network. Through experiments based on CIFAR-10 and MNIST datasets, we
demonstrate that a larger size of the quantum circuit ansatz improves
predictive performance in multiclass classification tasks, providing useful
insights for near term quantum algorithm developments.

    

### [[2107.11107] Introducing: DeepHead, Wide-band Electromagnetic Imaging Paradigm](http://arxiv.org/abs/2107.11107)


  Electromagnetic medical imaging in the microwave regime is a hard problem
notorious for 1) instability 2) under-determinism. This two-pronged problem is
tackled with a two-pronged solution that uses double compression to maximally
utilizing the cheap unlabelled data to a) provide a priori information required
to ease under-determinism and b) reduce sensitivity of inference to the input.
The result is a stable solver with a high resolution output. DeepHead is a
fully data-driven implementation of the paradigm proposed in the context of
microwave brain imaging. It infers the dielectric distribution of the brain at
a desired single frequency while making use of an input that spreads over a
wide band of frequencies. The performance of the model is evaluated with both
simulations and human volunteers experiments. The inference made is juxtaposed
with ground-truth dielectric distribution in simulation case, and the golden
MRI / CT imaging modalities of the volunteers in real-world case.

    

### [[2107.11113] OLR 2021 Challenge: Datasets, Rules and Baselines](http://arxiv.org/abs/2107.11113)


  This paper introduces the sixth Oriental Language Recognition (OLR) 2021
Challenge, which intends to improve the performance of language recognition
systems and speech recognition systems within multilingual scenarios. The data
profile, four tasks, two baselines, and the evaluation principles are
introduced in this paper. In addition to the Language Identification (LID)
tasks, multilingual Automatic Speech Recognition (ASR) tasks are introduced to
OLR 2021 Challenge for the first time. The challenge this year focuses on more
practical and challenging problems, with four tasks: (1) constrained LID, (2)
unconstrained LID, (3) constrained multilingual ASR, (4) unconstrained
multilingual ASR. Baselines for LID tasks and multilingual ASR tasks are
provided, respectively. The LID baseline system is an extended TDNN x-vector
model constructed with Pytorch. A transformer-based end-to-end model is
provided as the multilingual ASR baseline system. These recipes will be online
published, and available for participants to construct their own LID or ASR
systems. The baseline results demonstrate that those tasks are rather
challenging and deserve more effort to achieve better performance.

    

### [[2107.11114] A comparison of combined data assimilation and machine learning methods for offline and online model error correction](http://arxiv.org/abs/2107.11114)


  Recent studies have shown that it is possible to combine machine learning
methods with data assimilation to reconstruct a dynamical system using only
sparse and noisy observations of that system. The same approach can be used to
correct the error of a knowledge-based model. The resulting surrogate model is
hybrid, with a statistical part supplementing a physical part. In practice, the
correction can be added as an integrated term (i.e. in the model resolvent) or
directly inside the tendencies of the physical model. The resolvent correction
is easy to implement. The tendency correction is more technical, in particular
it requires the adjoint of the physical model, but also more flexible. We use
the two-scale Lorenz model to compare the two methods. The accuracy in
long-range forecast experiments is somewhat similar between the surrogate
models using the resolvent correction and the tendency correction. By contrast,
the surrogate models using the tendency correction significantly outperform the
surrogate models using the resolvent correction in data assimilation
experiments. Finally, we show that the tendency correction opens the
possibility to make online model error correction, i.e. improving the model
progressively as new observations become available. The resulting algorithm can
be seen as a new formulation of weak-constraint 4D-Var. We compare online and
offline learning using the same framework with the two-scale Lorenz system, and
show that with online learning, it is possible to extract all the information
from sparse and noisy observations.

    

### [[2107.11136] High Dimensional Differentially Private Stochastic Optimization with Heavy-tailed Data](http://arxiv.org/abs/2107.11136)


  As one of the most fundamental problems in machine learning, statistics and
differential privacy, Differentially Private Stochastic Convex Optimization
(DP-SCO) has been extensively studied in recent years. However, most of the
previous work can only handle either regular data distribution or irregular
data in the low dimensional space case. To better understand the challenges
arising from irregular data distribution, in this paper we provide the first
study on the problem of DP-SCO with heavy-tailed data in the high dimensional
space. In the first part we focus on the problem over some polytope constraint
(such as the $\ell_1$-norm ball). We show that if the loss function is smooth
and its gradient has bounded second order moment, it is possible to get a (high
probability) error bound (excess population risk) of $\tilde{O}(\frac{\log
d}{(n\epsilon)^\frac{1}{3}})$ in the $\epsilon$-DP model, where $n$ is the
sample size and $d$ is the dimensionality of the underlying space. Next, for
LASSO, if the data distribution that has bounded fourth-order moments, we
improve the bound to $\tilde{O}(\frac{\log d}{(n\epsilon)^\frac{2}{5}})$ in the
$(\epsilon, \delta)$-DP model. In the second part of the paper, we study sparse
learning with heavy-tailed data. We first revisit the sparse linear model and
propose a truncated DP-IHT method whose output could achieve an error of
$\tilde{O}(\frac{s^{*2}\log d}{n\epsilon})$, where $s^*$ is the sparsity of the
underlying parameter. Then we study a more general problem over the sparsity
({\em i.e.,} $\ell_0$-norm) constraint, and show that it is possible to achieve
an error of $\tilde{O}(\frac{s^{*\frac{3}{2}}\log d}{n\epsilon})$, which is
also near optimal up to a factor of $\tilde{O}{(\sqrt{s^*})}$, if the loss
function is smooth and strongly convex.

    

### [[2107.11153] Constellation: Learning relational abstractions over objects for compositional imagination](http://arxiv.org/abs/2107.11153)


  Learning structured representations of visual scenes is currently a major
bottleneck to bridging perception with reasoning. While there has been exciting
progress with slot-based models, which learn to segment scenes into sets of
objects, learning configurational properties of entire groups of objects is
still under-explored. To address this problem, we introduce Constellation, a
network that learns relational abstractions of static visual scenes, and
generalises these abstractions over sensory particularities, thus offering a
potential basis for abstract relational reasoning. We further show that this
basis, along with language association, provides a means to imagine sensory
content in new ways. This work is a first step in the explicit representation
of visual relationships and using them for complex cognitive procedures.

    

### [[2107.11156] Teaching a neural network with non-tunable exciton-polariton nodes](http://arxiv.org/abs/2107.11156)


  In contrast to software simulations of neural networks, hardware or
neuromorphic implementations have often limited or no tunability. While such
networks promise great improvements in terms of speed and energy efficiency,
their performance is limited by the difficulty to apply efficient teaching. We
propose a system of non-tunable exciton-polariton nodes and an efficient
teaching method that relies on the precise measurement of the nonlinear node
response and the subsequent use of the backpropagation algorithm. We
demonstrate experimentally that the classification accuracy in the MNIST
handwritten digit benchmark is greatly improved compared to the case where
backpropagation is not used.

    

### [[2107.11167] Dynamic detection of mobile malware using smartphone data and machine learning](http://arxiv.org/abs/2107.11167)


  Mobile malware are malicious programs that target mobile devices. They are an
increasing problem, as seen in the rise of detected mobile malware samples per
year. The number of active smartphone users is expected to grow, stressing the
importance of research on the detection of mobile malware. Detection methods
for mobile malware exist but are still limited.
In this paper, we provide an overview of the performance of machine learning
(ML) techniques to detect malware on Android, without using privileged access.
The ML-classifiers use device information such as the CPU usage, battery usage,
and memory usage for the detection of 10 subtypes of Mobile Trojans on the
Android Operating System (OS).
We use a real-life dataset containing device and malware data from 47 users
for a year (2016). We examine which features, i.e. aspects, of a device, are
most important to monitor to detect (subtypes of) Mobile Trojans. The focus of
this paper is on dynamic hardware features. Using these dynamic features we
apply state-of-the-art machine learning classifiers: Random Forest, K-Nearest
Neighbour, and AdaBoost. We show classification results on different feature
sets, making a distinction between global device features, and specific app
features. None of the measured feature sets require privileged access.
Our results show that the Random Forest classifier performs best as a general
malware classifier: across 10 subtypes of Mobile Trojans, it achieves an F1
score of 0.73 with a False Positive Rate (FPR) of 0.009 and a False Negative
Rate (FNR) of 0.380. The Random Forest, K-Nearest Neighbours, and AdaBoost
classifiers achieve F1 scores above 0.72, an FPR below 0.02 and, an FNR below
0.33, when trained separately to detect each subtype of Mobile Trojans.

    

### [[2107.11170] Bias Loss for Mobile Neural Networks](http://arxiv.org/abs/2107.11170)


  Compact convolutional neural networks (CNNs) have witnessed exceptional
improvements in performance in recent years. However, they still fail to
provide the same predictive power as CNNs with a large number of parameters.
The diverse and even abundant features captured by the layers is an important
characteristic of these successful CNNs. However, differences in this
characteristic between large CNNs and their compact counterparts have rarely
been investigated. In compact CNNs, due to the limited number of parameters,
abundant features are unlikely to be obtained, and feature diversity becomes an
essential characteristic. Diverse features present in the activation maps
derived from a data point during model inference may indicate the presence of a
set of unique descriptors necessary to distinguish between objects of different
classes. In contrast, data points with low feature diversity may not provide a
sufficient amount of unique descriptors to make a valid prediction; we refer to
them as random predictions. Random predictions can negatively impact the
optimization process and harm the final performance. This paper proposes
addressing the problem raised by random predictions by reshaping the standard
cross-entropy to make it biased toward data points with a limited number of
unique descriptive features. Our novel Bias Loss focuses the training on a set
of valuable data points and prevents the vast number of samples with poor
learning features from misleading the optimization process. Furthermore, to
show the importance of diversity, we present a family of SkipNet models whose
architectures are brought to boost the number of unique descriptors in the last
layers. Our Skipnet-M can achieve 1% higher classification accuracy than
MobileNetV3 Large.

    

### [[2107.11181] VisMCA: A Visual Analytics System for Misclassification Correction and Analysis. VAST Challenge 2020, Mini-Challenge 2 Award: Honorable Mention for Detailed Analysis of Patterns of Misclassification](http://arxiv.org/abs/2107.11181)


  This paper presents VisMCA, an interactive visual analytics system that
supports deepening understanding in ML results, augmenting users' capabilities
in correcting misclassification, and providing an analysis of underlying
patterns, in response to the VAST Challenge 2020 Mini-Challenge 2. VisMCA
facilitates tracking provenance and provides a comprehensive view of object
detection results, easing re-labeling, and producing reliable, corrected data
for future training. Our solution implements multiple analytical views on
visual analysis to offer a deep insight for underlying pattern discovery.

    

### [[2107.11186] LARGE: Latent-Based Regression through GAN Semantics](http://arxiv.org/abs/2107.11186)


  We propose a novel method for solving regression tasks using few-shot or weak
supervision. At the core of our method is the fundamental observation that GANs
are incredibly successful at encoding semantic information within their latent
space, even in a completely unsupervised setting. For modern generative
frameworks, this semantic encoding manifests as smooth, linear directions which
affect image attributes in a disentangled manner. These directions have been
widely used in GAN-based image editing. We show that such directions are not
only linear, but that the magnitude of change induced on the respective
attribute is approximately linear with respect to the distance traveled along
them. By leveraging this observation, our method turns a pre-trained GAN into a
regression model, using as few as two labeled samples. This enables solving
regression tasks on datasets and attributes which are difficult to produce
quality supervision for. Additionally, we show that the same latent-distances
can be used to sort collections of images by the strength of given attributes,
even in the absence of explicit supervision. Extensive experimental evaluations
demonstrate that our method can be applied across a wide range of domains,
leverage multiple latent direction discovery frameworks, and achieve
state-of-the-art results in few-shot and low-supervision settings, even when
compared to methods designed to tackle a single task.

    

### [[2107.11191] Regularising Inverse Problems with Generative Machine Learning Models](http://arxiv.org/abs/2107.11191)


  Deep neural network approaches to inverse imaging problems have produced
impressive results in the last few years. In this paper, we consider the use of
generative models in a variational regularisation approach to inverse problems.
The considered regularisers penalise images that are far from the range of a
generative model that has learned to produce images similar to a training
dataset. We name this family \textit{generative regularisers}. The success of
generative regularisers depends on the quality of the generative model and so
we propose a set of desired criteria to assess models and guide future
research. In our numerical experiments, we evaluate three common generative
models, autoencoders, variational autoencoders and generative adversarial
networks, against our desired criteria. We also test three different generative
regularisers on the inverse problems of deblurring, deconvolution, and
tomography. We show that the success of solutions restricted to lie exactly in
the range of the generator is highly dependent on the ability of the generative
model but that allowing small deviations from the range of the generator
produces more consistent results.

    

### [[2107.11214] Human Pose Estimation from Sparse Inertial Measurements through Recurrent Graph Convolution](http://arxiv.org/abs/2107.11214)


  We propose the adjacency adaptive graph convolutional long-short term memory
network (AAGC-LSTM) for human pose estimation from sparse inertial
measurements, obtained from only 6 measurement units. The AAGC-LSTM combines
both spatial and temporal dependency in a single network operation. This is
made possible by equipping graph convolutions with adjacency adaptivity, which
also allows for learning unknown dependencies of the human body joints. To
further boost accuracy, we propose longitudinal loss weighting to consider
natural movement patterns, as well as body-aware contralateral data
augmentation. By combining these contributions, we are able to utilize the
inherent graph nature of the human body, and can thus outperform the state of
the art for human pose estimation from sparse inertial measurements.

    

### [[2107.11225] Wavelet Design in a Learning Framework](http://arxiv.org/abs/2107.11225)


  Wavelets have proven to be highly successful in several signal and image
processing applications. Wavelet design has been an active field of research
for over two decades, with the problem often being approached from an
analytical perspective. In this paper, we introduce a learning based approach
to wavelet design. We draw a parallel between convolutional autoencoders and
wavelet multiresolution approximation, and show how the learning angle provides
a coherent computational framework for addressing the design problem. We aim at
designing data-independent wavelets by training filterbank autoencoders, which
precludes the need for customized datasets. In fact, we use high-dimensional
Gaussian vectors for training filterbank autoencoders, and show that a
near-zero training loss implies that the learnt filters satisfy the perfect
reconstruction property with very high probability. Properties of a wavelet
such as orthogonality, compact support, smoothness, symmetry, and vanishing
moments can be incorporated by designing the autoencoder architecture
appropriately and with a suitable regularization term added to the mean-squared
error cost used in the learning process. Our approach not only recovers the
well known Daubechies family of orthogonal wavelets and the
Cohen-Daubechies-Feauveau family of symmetric biorthogonal wavelets, but also
learns wavelets outside these families.

    

### [[2107.11228] Taxonomizing local versus global structure in neural network loss landscapes](http://arxiv.org/abs/2107.11228)


  Viewing neural network models in terms of their loss landscapes has a long
history in the statistical mechanics approach to learning, and in recent years
it has received attention within machine learning proper. Among other things,
local metrics (such as the smoothness of the loss landscape) have been shown to
correlate with global properties of the model (such as good generalization).
Here, we perform a detailed empirical analysis of the loss landscape structure
of thousands of neural network models, systematically varying learning tasks,
model architectures, and/or quantity/quality of data. By considering a range of
metrics that attempt to capture different aspects of the loss landscape, we
demonstrate that the best test accuracy is obtained when: the loss landscape is
globally well-connected; ensembles of trained models are more similar to each
other; and models converge to locally smooth regions. We also show that
globally poorly-connected landscapes can arise when models are small or when
they are trained to lower quality data; and that, if the loss landscape is
globally poorly-connected, then training to zero loss can actually lead to
worse test accuracy. Based on these results, we develop a simple
one-dimensional model with load-like and temperature-like parameters, we
introduce the notion of an \emph{effective loss landscape} depending on these
parameters, and we interpret our results in terms of a \emph{rugged convexity}
of the loss landscape. When viewed through this lens, our detailed empirical
results shed light on phases of learning (and consequent double descent
behavior), fundamental versus incidental determinants of good generalization,
the role of load-like and temperature-like parameters in the learning process,
different influences on the loss landscape from model and data, and the
relationships between local and global metrics, all topics of recent interest.

    

### [[2107.11238] Exploring Deep Registration Latent Spaces](http://arxiv.org/abs/2107.11238)


  Explainability of deep neural networks is one of the most challenging and
interesting problems in the field. In this study, we investigate the topic
focusing on the interpretability of deep learning-based registration methods.
In particular, with the appropriate model architecture and using a simple
linear projection, we decompose the encoding space, generating a new basis, and
we empirically show that this basis captures various decomposed anatomically
aware geometrical transformations. We perform experiments using two different
datasets focusing on lungs and hippocampus MRI. We show that such an approach
can decompose the highly convoluted latent spaces of registration pipelines in
an orthogonal space with several interesting properties. We hope that this work
could shed some light on a better understanding of deep learning-based
registration methods.

    

### [[2107.11247] Effective and Interpretable fMRI Analysis via Functional Brain Network Generation](http://arxiv.org/abs/2107.11247)


  Recent studies in neuroscience show great potential of functional brain
networks constructed from fMRI data for popularity modeling and clinical
predictions. However, existing functional brain networks are noisy and unaware
of downstream prediction tasks, while also incompatible with recent powerful
machine learning models of GNNs. In this work, we develop an end-to-end
trainable pipeline to extract prominent fMRI features, generate brain networks,
and make predictions with GNNs, all under the guidance of downstream prediction
tasks. Preliminary experiments on the PNC fMRI data show the superior
effectiveness and unique interpretability of our framework.

    

### [[2107.11250] Multi-Channel Automatic Music Transcription Using Tensor Algebra](http://arxiv.org/abs/2107.11250)


  Music is an art, perceived in unique ways by every listener, coming from
acoustic signals. In the meantime, standards as musical scores exist to
describe it. Even if humans can make this transcription, it is costly in terms
of time and efforts, even more with the explosion of information consecutively
to the rise of the Internet. In that sense, researches are driven in the
direction of Automatic Music Transcription. While this task is considered
solved in the case of single notes, it is still open when notes superpose
themselves, forming chords. This report aims at developing some of the existing
techniques towards Music Transcription, particularly matrix factorization, and
introducing the concept of multi-channel automatic music transcription. This
concept will be explored with mathematical objects called tensors.

    

### [[2107.11253] State, global and local parameter estimation using local ensemble Kalman filters: applications to online machine learning of chaotic dynamics](http://arxiv.org/abs/2107.11253)


  Recent studies have shown that it is possible to combine machine learning
methods with data assimilation to reconstruct a dynamical system using only
sparse and noisy observations of that system. The same approach can be used to
correct the error of a knowledge-based model. The resulting surrogate model is
hybrid, with a statistical part supplementing a physical part. In practice, the
correction can be added as an integrated term (\textit{i.e.} in the model
resolvent) or directly inside the tendencies of the physical model. The
resolvent correction is easy to implement. The tendency correction is more
technical, in particular it requires the adjoint of the physical model, but
also more flexible. We use the two-scale Lorenz model to compare the two
methods. The accuracy in long-range forecast experiments is somewhat similar
between the surrogate models using the resolvent correction and the tendency
correction. By contrast, the surrogate models using the tendency correction
significantly outperform the surrogate models using the resolvent correction in
data assimilation experiments. Finally, we show that the tendency correction
opens the possibility to make online model error correction, \textit{i.e.}
improving the model progressively as new observations become available. The
resulting algorithm can be seen as a new formulation of weak-constraint 4D-Var.
We compare online and offline learning using the same framework with the
two-scale Lorenz system, and show that with online learning, it is possible to
extract all the information from sparse and noisy observations.

    

### [[2107.11275] A Differentiable Language Model Adversarial Attack on Text Classifiers](http://arxiv.org/abs/2107.11275)


  Robustness of huge Transformer-based models for natural language processing
is an important issue due to their capabilities and wide adoption. One way to
understand and improve robustness of these models is an exploration of an
adversarial attack scenario: check if a small perturbation of an input can fool
a model.
Due to the discrete nature of textual data, gradient-based adversarial
methods, widely used in computer vision, are not applicable per~se. The
standard strategy to overcome this issue is to develop token-level
transformations, which do not take the whole sentence into account.
In this paper, we propose a new black-box sentence-level attack. Our method
fine-tunes a pre-trained language model to generate adversarial examples. A
proposed differentiable loss function depends on a substitute classifier score
and an approximate edit distance computed via a deep learning model.
We show that the proposed attack outperforms competitors on a diverse set of
NLP problems for both computed metrics and human evaluation. Moreover, due to
the usage of the fine-tuned language model, the generated adversarial examples
are hard to detect, thus current models are not robust. Hence, it is difficult
to defend from the proposed attack, which is not the case for other attacks.

    

### [[2107.11277] Machine Learning with a Reject Option: A survey](http://arxiv.org/abs/2107.11277)


  Machine learning models always make a prediction, even when it is likely to
be inaccurate. This behavior should be avoided in many decision support
applications, where mistakes can have severe consequences. Albeit already
studied in 1970, machine learning with a reject option recently gained
interest. This machine learning subfield enables machine learning models to
abstain from making a prediction when likely to make a mistake.
This survey aims to provide an overview on machine learning with a reject
option. We introduce the conditions leading to two types of rejection,
ambiguity and novelty rejection. Moreover, we define the existing architectures
for models with a reject option, describe the standard learning strategies to
train such models and relate traditional machine learning techniques to
rejection. Additionally, we review strategies to evaluate a model's predictive
and rejective quality. Finally, we provide examples of relevant application
domains and show how machine learning with rejection relates to other machine
learning research areas.

    

### [[2107.11291] Human Pose Regression with Residual Log-likelihood Estimation](http://arxiv.org/abs/2107.11291)


  Heatmap-based methods dominate in the field of human pose estimation by
modelling the output distribution through likelihood heatmaps. In contrast,
regression-based methods are more efficient but suffer from inferior
performance. In this work, we explore maximum likelihood estimation (MLE) to
develop an efficient and effective regression-based methods. From the
perspective of MLE, adopting different regression losses is making different
assumptions about the output density function. A density function closer to the
true distribution leads to a better regression performance. In light of this,
we propose a novel regression paradigm with Residual Log-likelihood Estimation
(RLE) to capture the underlying output distribution. Concretely, RLE learns the
change of the distribution instead of the unreferenced underlying distribution
to facilitate the training process. With the proposed reparameterization
design, our method is compatible with off-the-shelf flow models. The proposed
method is effective, efficient and flexible. We show its potential in various
human pose estimation tasks with comprehensive experiments. Compared to the
conventional regression paradigm, regression with RLE bring 12.4 mAP
improvement on MSCOCO without any test-time overhead. Moreover, for the first
time, especially on multi-person pose estimation, our regression method is
superior to the heatmap-based methods. Our code is available at
this https URL


### [[2107.11304] Finite-Bit Quantization For Distributed Algorithms With Linear Convergence](http://arxiv.org/abs/2107.11304)


  This paper studies distributed algorithms for (strongly convex) composite
optimization problems over mesh networks, subject to quantized communications.
Instead of focusing on a specific algorithmic design, we propose a black-box
model casting distributed algorithms in the form of fixed-point iterates,
converging at linear rate. The algorithmic model is coupled with a novel
(random) Biased Compression (BC-)rule on the quantizer design, which preserves
linear convergence. A new quantizer coupled with a communication-efficient
encoding scheme is also proposed, which efficiently implements the BC-rule
using a finite number of bits. This contrasts with most of existing
quantization rules, whose implementation calls for an infinite number of bits.
A unified communication complexity analysis is developed for the black-box
model, determining the average number of bit required to reach a solution of
the optimization problem within the required accuracy. Numerical results
validate our theoretical findings and show that distributed algorithms equipped
with the proposed quantizer have more favorable communication complexity than
algorithms using existing quantization rules.

    

### [[2107.11320] Tackling the Overestimation of Forest Carbon with Deep Learning and Aerial Imagery](http://arxiv.org/abs/2107.11320)


  Forest carbon offsets are increasingly popular and can play a significant
role in financing climate mitigation, forest conservation, and reforestation.
Measuring how much carbon is stored in forests is, however, still largely done
via expensive, time-consuming, and sometimes unaccountable field measurements.
To overcome these limitations, many verification bodies are leveraging machine
learning (ML) algorithms to estimate forest carbon from satellite or aerial
imagery. Aerial imagery allows for tree species or family classification, which
improves the satellite imagery-based forest type classification. However,
aerial imagery is significantly more expensive to collect and it is unclear by
how much the higher resolution improves the forest carbon estimation. This
proposal paper describes the first systematic comparison of forest carbon
estimation from aerial imagery, satellite imagery, and ground-truth field
measurements via deep learning-based algorithms for a tropical reforestation
project. Our initial results show that forest carbon estimates from satellite
imagery can overestimate above-ground biomass by more than 10-times for
tropical reforestation projects. The significant difference between aerial and
satellite-derived forest carbon measurements shows the potential for aerial
imagery-based ML algorithms and raises the importance to extend this study to a
global benchmark between options for carbon measurements.

    

### [[2107.11327] Structack: Structure-based Adversarial Attacks on Graph Neural Networks](http://arxiv.org/abs/2107.11327)


  Recent work has shown that graph neural networks (GNNs) are vulnerable to
adversarial attacks on graph data. Common attack approaches are typically
informed, i.e. they have access to information about node attributes such as
labels and feature vectors. In this work, we study adversarial attacks that are
uninformed, where an attacker only has access to the graph structure, but no
information about node attributes. Here the attacker aims to exploit structural
knowledge and assumptions, which GNN models make about graph data. In
particular, literature has shown that structural node centrality and similarity
have a strong influence on learning with GNNs. Therefore, we study the impact
of centrality and similarity on adversarial attacks on GNNs. We demonstrate
that attackers can exploit this information to decrease the performance of GNNs
by focusing on injecting links between nodes of low similarity and,
surprisingly, low centrality. We show that structure-based uninformed attacks
can approach the performance of informed attacks, while being computationally
more efficient. With our paper, we present a new attack strategy on GNNs that
we refer to as Structack. Structack can successfully manipulate the performance
of GNNs with very limited information while operating under tight computational
constraints. Our work contributes towards building more robust machine learning
approaches on graphs.

    

### [[2107.11333] Robust Adaptive Submodular Maximization](http://arxiv.org/abs/2107.11333)


  Most of existing studies on adaptive submodular optimization focus on the
average-case, i.e., their objective is to find a policy that maximizes the
expected utility over a known distribution of realizations. However, a policy
that has a good average-case performance may have very poor performance under
the worst-case realization. In this study, we propose to study two variants of
adaptive submodular optimization problems, namely, worst-case adaptive
submodular maximization and robust submodular maximization. The first problem
aims to find a policy that maximizes the worst-case utility and the latter one
aims to find a policy, if any, that achieves both near optimal average-case
utility and worst-case utility simultaneously. We introduce a new class of
stochastic functions, called \emph{worst-case submodular function}. For the
worst-case adaptive submodular maximization problem subject to a $p$-system
constraint, we develop an adaptive worst-case greedy policy that achieves a
$\frac{1}{p+1}$ approximation ratio against the optimal worst-case utility if
the utility function is worst-case submodular. For the robust adaptive
submodular maximization problem subject to a cardinality constraint, if the
utility function is both worst-case submodular and adaptive submodular, we
develop a hybrid adaptive policy that achieves an approximation close to
$1-e^{-\frac{1}{2}}$ under both worst case setting and average case setting
simultaneously. We also describe several applications of our theoretical
results, including pool-base active learning, stochastic submodular set cover
and adaptive viral marketing.

    

### [[2107.11350] Heteroscedastic Temporal Variational Autoencoder For Irregularly Sampled Time Series](http://arxiv.org/abs/2107.11350)


  Irregularly sampled time series commonly occur in several domains where they
present a significant challenge to standard deep learning models. In this
paper, we propose a new deep learning framework for probabilistic interpolation
of irregularly sampled time series that we call the Heteroscedastic Temporal
Variational Autoencoder (HeTVAE). HeTVAE includes a novel input layer to encode
information about input observation sparsity, a temporal VAE architecture to
propagate uncertainty due to input sparsity, and a heteroscedastic output layer
to enable variable uncertainty in output interpolations. Our results show that
the proposed architecture is better able to reflect variable uncertainty
through time due to sparse and irregular sampling than a range of baseline and
traditional models, as well as recently proposed deep latent variable models
that use homoscedastic output layers.

    

### [[2107.11357] Joint Shapley values: a measure of joint feature importance](http://arxiv.org/abs/2107.11357)


  The Shapley value is one of the most widely used model-agnostic measures of
feature importance in explainable AI: it has clear axiomatic foundations, is
guaranteed to uniquely exist, and has a clear interpretation as a feature's
average effect on a model's prediction. We introduce joint Shapley values,
which directly extend the Shapley axioms. This preserves the classic Shapley
value's intuitions: joint Shapley values measure a set of features' average
effect on a model's prediction. We prove the uniqueness of joint Shapley
values, for any order of explanation. Results for games show that joint Shapley
values present different insights from existing interaction indices, which
assess the effect of a feature within a set of features. Deriving joint Shapley
values in ML attribution problems thus gives us the first measure of the joint
effect of sets of features on model predictions. In a dataset with binary
features, we present a presence-adjusted method for calculating global values
that retains the efficiency property.

    

### [[2107.11359] Rethinking Hard-Parameter Sharing in Multi-Task Learning](http://arxiv.org/abs/2107.11359)


  Hard parameter sharing in multi-task learning (MTL) allows tasks to share
some of model parameters, reducing storage cost and improving prediction
accuracy. The common sharing practice is to share bottom layers of a deep
neural network among tasks while using separate top layers for each task. In
this work, we revisit this common practice via an empirical study on
fine-grained image classification tasks and make two surprising observations.
(1) Using separate bottom-layer parameters could achieve significantly better
performance than the common practice and this phenomenon holds for different
number of tasks jointly trained on different backbone architectures with
different quantity of task-specific parameters. (2) A multi-task model with a
small proportion of task-specific parameters from bottom layers can achieve
competitive performance with independent models trained on each task separately
and outperform a state-of-the-art MTL framework. Our observations suggest that
people rethink the current sharing paradigm and adopt the new strategy of using
separate bottom-layer parameters as a stronger baseline for model design in
MTL.

    

### [[2107.11371] Optimum Risk Portfolio and Eigen Portfolio: A Comparative Analysis Using Selected Stocks from the Indian Stock Market](http://arxiv.org/abs/2107.11371)


  Designing an optimum portfolio that allocates weights to its constituent
stocks in a way that achieves the best trade-off between the return and the
risk is a challenging research problem. The classical mean-variance theory of
portfolio proposed by Markowitz is found to perform sub-optimally on the
real-world stock market data since the error in estimation for the expected
returns adversely affects the performance of the portfolio. This paper presents
three approaches to portfolio design, viz, the minimum risk portfolio, the
optimum risk portfolio, and the Eigen portfolio, for seven important sectors of
the Indian stock market. The daily historical prices of the stocks are scraped
from Yahoo Finance website from January 1, 2016, to December 31, 2020. Three
portfolios are built for each of the seven sectors chosen for this study, and
the portfolios are analyzed on the training data based on several metrics such
as annualized return and risk, weights assigned to the constituent stocks, the
correlation heatmaps, and the principal components of the Eigen portfolios.
Finally, the optimum risk portfolios and the Eigen portfolios for all sectors
are tested on their return over a period of a six-month period. The
performances of the portfolios are compared and the portfolio yielding the
higher return for each sector is identified.

    

### [[1905.10686] Empirical Risk Minimization in the Interpolating Regime with Application to Neural Network Learning](http://arxiv.org/abs/1905.10686)


  A common strategy to train deep neural networks (DNNs) is to use very large
architectures and to train them until they (almost) achieve zero training
error. Empirically observed good generalization performance on test data, even
in the presence of lots of label noise, corroborate such a procedure. On the
other hand, in statistical learning theory it is known that over-fitting models
may lead to poor generalization properties, occurring in e.g. empirical risk
minimization (ERM) over too large hypotheses classes. Inspired by this
contradictory behavior, so-called interpolation methods have recently received
much attention, leading to consistent and optimally learning methods for some
local averaging schemes with zero training error. However, there is no
theoretical analysis of interpolating ERM-like methods so far. We take a step
in this direction by showing that for certain, large hypotheses classes, some
interpolating ERMs enjoy very good statistical guarantees while others fail in
the worst sense. Moreover, we show that the same phenomenon occurs for DNNs
with zero training error and sufficiently large architectures.

    

### [[1911.11134] Rigging the Lottery: Making All Tickets Winners](http://arxiv.org/abs/1911.11134)


  Many applications require sparse neural networks due to space or inference
time restrictions. There is a large body of work on training dense networks to
yield sparse networks for inference, but this limits the size of the largest
trainable sparse model to that of the largest trainable dense model. In this
paper we introduce a method to train sparse neural networks with a fixed
parameter count and a fixed computational cost throughout training, without
sacrificing accuracy relative to existing dense-to-sparse training methods. Our
method updates the topology of the sparse network during training by using
parameter magnitudes and infrequent gradient calculations. We show that this
approach requires fewer floating-point operations (FLOPs) to achieve a given
level of accuracy compared to prior techniques. We demonstrate state-of-the-art
sparse training results on a variety of networks and datasets, including
ResNet-50, MobileNets on Imagenet-2012, and RNNs on WikiText-103. Finally, we
provide some insights into why allowing the topology to change during the
optimization can overcome local minima encountered when the topology remains
static. Code used in our work can be found in this http URL.

    

### [[1912.00781] Experiments with a PCCoder extension](http://arxiv.org/abs/1912.00781)


  Recent research in synthesis of programs written in some Domain Specific
Language (DSL) by means of neural networks from a limited set of inputs-output
correspondences such as DeepCoder and its PCCoder reimplementation/optimization
proved the efficiency of this kind of approach to automatic program generation
in a DSL language that although limited in scope is universal in the sense that
programs can be translated to basically any programming language. We experiment
with the extension of the DSL of DeepCoder/PCCoder with symbols IFI and IFL
which denote functional expressions of the If ramification (test) instruction
for types Int and List. We notice an increase (doubling) of the size of the
training set, the number of parameters of the trained neural network and of the
time spent looking for the program synthesized from limited sets of
inputs-output correspondences. The result is positive in the sense of
preserving the accuracy of applying synthesis on randomly generated test sets.

    

### [[2001.03148] Regularity and stability of feedback relaxed controls](http://arxiv.org/abs/2001.03148)


  This paper proposes a relaxed control regularization with general exploration
rewards to design robust feedback controls for multi-dimensional
continuous-time stochastic exit time problems. We establish that the
regularized control problem admits a Hölder continuous feedback control,
and demonstrate that both the value function and the feedback control of the
regularized control problem are Lipschitz stable with respect to parameter
perturbations. Moreover, we show that a pre-computed feedback relaxed control
has a robust performance in a perturbed system, and derive a first-order
sensitivity equation for both the value function and optimal feedback relaxed
control. These stability results provide a theoretical justification for recent
reinforcement learning heuristics that including an exploration reward in the
optimization objective leads to more robust decision making. We finally prove
first-order monotone convergence of the value functions for relaxed control
problems with vanishing exploration parameters, which subsequently enables us
to construct the pure exploitation strategy of the original control problem
based on the feedback relaxed controls.

    

### [[2002.01586] A Precise High-Dimensional Asymptotic Theory for Boosting and Minimum-$\ell_1$-Norm Interpolated Classifiers](http://arxiv.org/abs/2002.01586)


  This paper establishes a precise high-dimensional asymptotic theory for
boosting on separable data, taking statistical and computational perspectives.
We consider a high-dimensional setting where the number of features (weak
learners) $p$ scales with the sample size $n$, in an overparametrized regime.
Under a class of statistical models, we provide an exact analysis of the
generalization error of boosting when the algorithm interpolates the training
data and maximizes the empirical $\ell_1$-margin. Further, we explicitly pin
down the relation between the boosting test error and the optimal Bayes error,
as well as the proportion of active features at interpolation (with zero
initialization). In turn, these precise characterizations answer certain
questions raised in \cite{breiman1999prediction, schapire1998boosting}
surrounding boosting, under assumed data generating processes. At the heart of
our theory lies an in-depth study of the maximum-$\ell_1$-margin, which can be
accurately described by a new system of non-linear equations; to analyze this
margin, we rely on Gaussian comparison techniques and develop a novel uniform
deviation argument. Our statistical and computational arguments can handle (1)
any finite-rank spiked covariance model for the feature distribution and (2)
variants of boosting corresponding to general $\ell_q$-geometry, $q \in [1,
2]$. As a final component, via the Lindeberg principle, we establish a
universality result showcasing that the scaled $\ell_1$-margin (asymptotically)
remains the same, whether the covariates used for boosting arise from a
non-linear random feature model or an appropriately linearized model with
matching moments.

    

### [[2002.10673] On the simplicity and conditioning of low rank semidefinite programs](http://arxiv.org/abs/2002.10673)


  Low rank matrix recovery problems appear widely in statistics, combinatorics,
and imaging. One celebrated method for solving these problems is to formulate
and solve a semidefinite program (SDP). It is often known that the exact
solution to the SDP with perfect data recovers the solution to the original low
rank matrix recovery problem. It is more challenging to show that an
approximate solution to the SDP formulated with noisy problem data acceptably
solves the original problem; arguments are usually ad hoc for each problem
setting, and can be complex.
In this note, we identify a set of conditions that we call simplicity that
limit the error due to noisy problem data or incomplete convergence. In this
sense, simple SDPs are robust: simple SDPs can be (approximately) solved
efficiently at scale; and the resulting approximate solutions, even with noisy
data, can be trusted. Moreover, we show that simplicity holds generically, and
also for many structured low rank matrix recovery problems, including the
stochastic block model, $\mathbb{Z}_2$ synchronization, and matrix completion.
Formally, we call an SDP simple if it has a surjective constraint map, admits a
unique primal and dual solution pair, and satisfies strong duality and strict
complementarity.
However, simplicity is not a panacea: we show the Burer-Monteiro formulation
of the SDP may have spurious second-order critical points, even for a simple
SDP with a rank 1 solution.

    

### [[2003.04919] Integrating Scientific Knowledge with Machine Learning for Engineering and Environmental Systems](http://arxiv.org/abs/2003.04919)


  There is a growing consensus that solutions to complex science and
engineering problems require novel methodologies that are able to integrate
traditional physics-based modeling approaches with state-of-the-art machine
learning (ML) techniques. This paper provides a structured overview of such
techniques. Application-centric objective areas for which these approaches have
been applied are summarized, and then classes of methodologies used to
construct physics-guided ML models and hybrid physics-ML frameworks are
described. We then provide a taxonomy of these existing techniques, which
uncovers knowledge gaps and potential crossovers of methods between disciplines
that can serve as ideas for future research.

    

### [[2003.13001] Zeroth-Order Regularized Optimization (ZORO): Approximately Sparse Gradients and Adaptive Sampling](http://arxiv.org/abs/2003.13001)


  We consider the problem of minimizing a high-dimensional objective function,
which may include a regularization term, using (possibly noisy) evaluations of
the function. Such optimization is also called derivative-free, zeroth-order,
or black-box optimization. We propose a new $\textbf{Z}$eroth-$\textbf{O}$rder
$\textbf{R}$egularized $\textbf{O}$ptimization method, dubbed ZORO. When the
underlying gradient is approximately sparse at an iterate, ZORO needs very few
objective function evaluations to obtain a new iterate that decreases the
objective function. We achieve this with an adaptive, randomized gradient
estimator, followed by an inexact proximal-gradient scheme. Under a novel
approximately sparse gradient assumption and various different convex settings,
we show the (theoretical and empirical) convergence rate of ZORO is only
logarithmically dependent on the problem dimension. Numerical experiments show
that ZORO outperforms the existing methods with similar assumptions, on both
synthetic and real datasets.

    

### [[2004.11184] Learning Stable Adaptive Explicit Differentiable Predictive Control for Unknown Linear Systems](http://arxiv.org/abs/2004.11184)


  We present differentiable predictive control (DPC), a method for learning
constrained adaptive neural control policies and dynamical models of unknown
linear systems. DPC presents an approximate data-driven solution approach to
the explicit Model Predictive Control (MPC) problem as a scalable alternative
to computationally expensive multiparametric programming solvers. DPC is
formulated as a constrained deep learning problem whose architecture is
inspired by the structure of classical MPC. The optimization of the neural
control policy is based on automatic differentiation of the MPC-inspired loss
function through a differentiable closed-loop system model. This novel solution
approach can optimize adaptive neural control policies for time-varying
references while obeying state and input constraints without the prior need of
an MPC controller. We show that DPC can learn to stabilize constrained neural
control policies for systems with unstable dynamics. Moreover, we provide
sufficient conditions for asymptotic stability of generic closed-loop system
dynamics with neural feedback policies. In simulation case studies, we assess
the performance of the proposed DPC method in terms of reference tracking,
robustness, and computational and memory footprints compared against classical
model-based and data-driven control approaches. We demonstrate that DPC scales
linearly with problem size, compared to exponential scalability of classical
explicit MPC based on multiparametric programming.

    

### [[2005.09525] Toward Automated Classroom Observation: Multimodal Machine Learning to Estimate CLASS Positive Climate and Negative Climate](http://arxiv.org/abs/2005.09525)


  In this work we present a multi-modal machine learning-based system, which we
call ACORN, to analyze videos of school classrooms for the Positive Climate
(PC) and Negative Climate (NC) dimensions of the CLASS observation protocol
that is widely used in educational research. ACORN uses convolutional neural
networks to analyze spectral audio features, the faces of teachers and
students, and the pixels of each image frame, and then integrates this
information over time using Temporal Convolutional Networks. The audiovisual
ACORN's PC and NC predictions have Pearson correlations of $0.55$ and $0.63$
with ground-truth scores provided by expert CLASS coders on the UVA Toddler
dataset (cross-validation on $n=300$ 15-min video segments), and a purely
auditory ACORN predicts PC and NC with correlations of $0.36$ and $0.41$ on the
MET dataset (test set of $n=2000$ videos segments). These numbers are similar
to inter-coder reliability of human coders. Finally, using Graph Convolutional
Networks we make early strides (AUC=$0.70$) toward predicting the specific
moments (45-90sec clips) when the PC is particularly weak/strong. Our findings
inform the design of automatic classroom observation and also more general
video activity recognition and summary recognition systems.

    

### [[2006.01512] A fast and simple modification of Newton's method helping to avoid saddle points](http://arxiv.org/abs/2006.01512)


  We propose in this paper New Q-Newton's method. The update rule for the
simplest version is $x_{n+1}=x_n-w_n$ where
$w_n=pr_{A_n,+}(v_n)-pr_{A_n,-}(v_n)$, with $A_n=\nabla
^2f(x_n)+\delta_n||\nabla f(x_n)||^this http URL$ and $v_n=A_n^{-1}.\nabla f(x_n)$. Here
$\delta_n$ is an appropriate real number so that $A_n$ is invertible, and
$pr_{A_n,\pm}$ are projections to the vector subspaces generated by
eigenvectors of positive (correspondingly negative) eigenvalues of $A_n$.
The main result of this paper roughly says that if $f$ is $C^3$ and a
sequence $\{x_n\}$, constructed by the New Q-Newton's method from a random
initial point $x_0$, {\bf converges}, then the limit point is a critical point
and is not a saddle point, and the convergence rate is the same as that of
Newton's method. At the end of the paper, we present some issues (saddle points
and convergence) one faces when implementing Newton's method and modifications
into Deep Neural Networks. We test the good performance of New Q-Newton's
method against algorithms such as Newton's method, BFGS, Adaptive Cubic
Regularization, Random damping Newton's method and Inertial Newton's method, as
well as Unbounded Two-way Backtracking Gradient Descent. The experiments cover
both realistic settings (such as a toy model of protein folding and stochastic
optimization) as well as various benchmark test functions.

    

### [[2006.09973] Breaking Type Safety in Go: An Empirical Study on the Usage of the unsafe Package](http://arxiv.org/abs/2006.09973)


  A decade after its first release, the Go programming language has become a
major programming language in the development landscape. While praised for its
clean syntax and C-like performance, Go also contains a strong static
type-system that prevents arbitrary type casting and arbitrary memory access,
making the language type-safe by design. However, to give developers the
possibility of implementing low-level code, Go ships with a special package
called unsafe that offers developers a way around the type-safety of Go
programs. The package gives greater flexibility to developers but comes at a
higher risk of runtime errors, chances of non-portability, and the loss of
compatibility guarantees for future versions of Go.
In this paper, we present the first large-scale study on the usage of the
unsafe package in 2,438 popular Go projects. Our investigation shows that
unsafe is used in 24% of Go projects, motivated primarily by communicating with
operating systems and C code, but is also commonly used as a source of
performance optimization. Developers are willing to use unsafe to break
language specifications (e.g., string immutability) for better performance and
6% of analyzed projects that use unsafe perform risky pointer conversions that
can lead to program crashes and unexpected behavior. Furthermore, we report a
series of real issues faced by projects that use unsafe, from crashing errors
and non-deterministic behavior to having their deployment restricted from
certain popular environments. Our findings can be used to understand how and
why developers break type-safety in Go, and help motivate further tools and
language development that could make the usage of unsafe in Go even safer.

    

### [[2007.01350] Uncertainty Prediction for Deep Sequential Regression Using Meta Models](http://arxiv.org/abs/2007.01350)


  Generating high quality uncertainty estimates for sequential regression,
particularly deep recurrent networks, remains a challenging and open problem.
Existing approaches often make restrictive assumptions (such as stationarity)
yet still perform poorly in practice, particularly in presence of real world
non-stationary signals and drift. This paper describes a flexible method that
can generate symmetric and asymmetric uncertainty estimates, makes no
assumptions about stationarity, and outperforms competitive baselines on both
drift and non drift scenarios. This work helps make sequential regression more
effective and practical for use in real-world applications, and is a powerful
new addition to the modeling toolbox for sequential uncertainty quantification
in general.

    

### [[2009.07101] Approximate spectral clustering using both reference vectors and topology of the network generated by growing neural gas](http://arxiv.org/abs/2009.07101)


  Spectral clustering (SC) is one of the most popular clustering methods and
often outperforms traditional clustering methods. SC uses the eigenvectors of a
Laplacian matrix calculated from a similarity matrix of a dataset. SC has
serious drawbacks: the significant increases in the time complexity derived
from the computation of eigenvectors and the memory space complexity to store
the similarity matrix. To address the issues, I develop a new approximate
spectral clustering using the network generated by growing neural gas (GNG),
called ASC with GNG in this study. ASC with GNG uses not only reference vectors
for vector quantization but also the topology of the network for extraction of
the topological relationship between data points in a dataset. ASC with GNG
calculates the similarity matrix from both the reference vectors and the
topology of the network generated by GNG. Using the network generated from a
dataset by GNG, ASC with GNG achieves to reduce the computational and space
complexities and improve clustering quality. In this study, I demonstrate that
ASC with GNG effectively reduces the computational time. Moreover, this study
shows that ASC with GNG provides equal to or better clustering performance than
SC.

    

### [[2009.10337] Learning Task-Agnostic Action Spaces for Movement Optimization](http://arxiv.org/abs/2009.10337)


  We propose a novel method for exploring the dynamics of physically based
animated characters, and learning a task-agnostic action space that makes
movement optimization easier. Like several previous papers, we parameterize
actions as target states, and learn a short-horizon goal-conditioned low-level
control policy that drives the agent's state towards the targets. Our novel
contribution is that with our exploration data, we are able to learn the
low-level policy in a generic manner and without any reference movement data.
Trained once for each agent or simulation environment, the policy improves the
efficiency of optimizing both trajectories and high-level policies across
multiple tasks and optimization algorithms. We also contribute novel
visualizations that show how using target states as actions makes optimized
trajectories more robust to disturbances; this manifests as wider optima that
are easy to find. Due to its simplicity and generality, our proposed approach
should provide a building block that can improve a large variety of movement
optimization methods and applications.

    

### [[2011.11284] Peeking inside the Black Box: Interpreting Deep Learning Models for Exoplanet Atmospheric Retrievals](http://arxiv.org/abs/2011.11284)


  Deep learning algorithms are growing in popularity in the field of
exoplanetary science due to their ability to model highly non-linear relations
and solve interesting problems in a data-driven manner. Several works have
attempted to perform fast retrievals of atmospheric parameters with the use of
machine learning algorithms like deep neural networks (DNNs). Yet, despite
their high predictive power, DNNs are also infamous for being 'black boxes'. It
is their apparent lack of explainability that makes the astrophysics community
reluctant to adopt them. What are their predictions based on? How confident
should we be in them? When are they wrong and how wrong can they be? In this
work, we present a number of general evaluation methodologies that can be
applied to any trained model and answer questions like these. In particular, we
train three different popular DNN architectures to retrieve atmospheric
parameters from exoplanet spectra and show that all three achieve good
predictive performance. We then present an extensive analysis of the
predictions of DNNs, which can inform us - among other things - of the
credibility limits for atmospheric parameters for a given instrument and model.
Finally, we perform a perturbation-based sensitivity analysis to identify to
which features of the spectrum the outcome of the retrieval is most sensitive.
We conclude that for different molecules, the wavelength ranges to which the
DNN's predictions are most sensitive, indeed coincide with their characteristic
absorption regions. The methodologies presented in this work help to improve
the evaluation of DNNs and to grant interpretability to their predictions.

    

### [[2011.13000] Ax-BxP: Approximate Blocked Computation for Precision-Reconfigurable Deep Neural Network Acceleration](http://arxiv.org/abs/2011.13000)


  Precision scaling has emerged as a popular technique to optimize the compute
and storage requirements of Deep Neural Networks (DNNs). Efforts toward
creating ultra-low-precision (sub-8-bit) DNNs suggest that the minimum
precision required to achieve a given network-level accuracy varies
considerably across networks, and even across layers within a network,
requiring support for variable precision in DNN hardware. Previous proposals
such as bit-serial hardware incur high overheads, significantly diminishing the
benefits of lower precision. To efficiently support precision
re-configurability in DNN accelerators, we introduce an approximate computing
method wherein DNN computations are performed block-wise (a block is a group of
bits) and re-configurability is supported at the granularity of blocks. Results
of block-wise computations are composed in an approximate manner to enable
efficient re-configurability. We design a DNN accelerator that embodies
approximate blocked computation and propose a method to determine a suitable
approximation configuration for a given DNN. By varying the approximation
configurations across DNNs, we achieve 1.17x-1.73x and 1.02x-2.04x improvement
in system energy and performance respectively, over an 8-bit fixed-point (FxP8)
baseline, with negligible loss in classification accuracy. Further, by varying
the approximation configurations across layers and data-structures within DNNs,
we achieve 1.25x-2.42x and 1.07x-2.95x improvement in system energy and
performance respectively, with negligible accuracy loss.

    

### [[2012.10713] Fundamental Limits and Tradeoffs in Invariant Representation Learning](http://arxiv.org/abs/2012.10713)


  Many machine learning applications, e.g., privacy-preserving learning,
algorithmic fairness and domain adaptation/generalization, involve learning the
so-called invariant representations that achieve two competing goals: To
maximize information or accuracy with respect to a target while simultaneously
maximizing invariance or independence with respect to a set of protected
features (e.g.\ for fairness, privacy, etc). Despite its abundant applications
in the aforementioned domains, theoretical understanding on the limits and
tradeoffs of invariant representations is still severely lacking. In this
paper, we provide an information theoretic analysis of this general and
important problem under both classification and regression settings. In both
cases, we analyze the inherent tradeoffs between accuracy and invariance by
providing a geometric characterization of the feasible region in the
information plane, where we connect the geometric properties of this feasible
region to the fundamental limitations of the tradeoff problem. In the
regression setting, we further give a complete and exact characterization of
the frontier between accuracy and invariance. Although our contributions are
mainly theoretical, we also demonstrate the practical applications of our
results in certifying the suboptimality of certain representation learning
algorithms in both classification and regression tasks. Our results shed new
light on this fundamental problem by providing insights on the interplay
between accuracy and invariance. These results deepen our understanding of this
fundamental problem and may be useful in guiding the design of future
representation learning algorithms.

    

### [[2012.13349] Solving Mixed Integer Programs Using Neural Networks](http://arxiv.org/abs/2012.13349)


  Mixed Integer Programming (MIP) solvers rely on an array of sophisticated
heuristics developed with decades of research to solve large-scale MIP
instances encountered in practice. Machine learning offers to automatically
construct better heuristics from data by exploiting shared structure among
instances in the data. This paper applies learning to the two key sub-tasks of
a MIP solver, generating a high-quality joint variable assignment, and bounding
the gap in objective value between that assignment and an optimal one. Our
approach constructs two corresponding neural network-based components, Neural
Diving and Neural Branching, to use in a base MIP solver such as SCIP. Neural
Diving learns a deep neural network to generate multiple partial assignments
for its integer variables, and the resulting smaller MIPs for un-assigned
variables are solved with SCIP to construct high quality joint assignments.
Neural Branching learns a deep neural network to make variable selection
decisions in branch-and-bound to bound the objective value gap with a small
tree. This is done by imitating a new variant of Full Strong Branching we
propose that scales to large instances using GPUs. We evaluate our approach on
six diverse real-world datasets, including two Google production datasets and
MIPLIB, by training separate neural networks on each. Most instances in all the
datasets combined have $10^3-10^6$ variables and constraints after presolve,
which is significantly larger than previous learning approaches. Comparing
solvers with respect to primal-dual gap averaged over a held-out set of
instances, the learning-augmented SCIP is 2x to 10x better on all datasets
except one on which it is $10^5$x better, at large time limits. To the best of
our knowledge, ours is the first learning approach to demonstrate such large
improvements over SCIP on both large-scale real-world application datasets and
MIPLIB.

    

### [[2101.06069] Mining Data Impressions from Deep Models as Substitute for the Unavailable Training Data](http://arxiv.org/abs/2101.06069)


  Pretrained deep models hold their learnt knowledge in the form of model
parameters. These parameters act as "memory" for the trained models and help
them generalize well on unseen data. However, in absence of training data, the
utility of a trained model is merely limited to either inference or better
initialization towards a target task. In this paper, we go further and extract
synthetic data by leveraging the learnt model parameters. We dub them "Data
Impressions", which act as proxy to the training data and can be used to
realize a variety of tasks. These are useful in scenarios where only the
pretrained models are available and the training data is not shared (e.g., due
to privacy or sensitivity concerns). We show the applicability of data
impressions in solving several computer vision tasks such as unsupervised
domain adaptation, continual learning as well as knowledge distillation. We
also study the adversarial robustness of lightweight models trained via
knowledge distillation using these data impressions. Further, we demonstrate
the efficacy of data impressions in generating data-free Universal Adversarial
Perturbations (UAPs) with better fooling rates. Extensive experiments performed
on benchmark datasets demonstrate competitive performance achieved using data
impressions in absence of original training data.

    

### [[2101.08074] Flocking and Collision Avoidance for a Dynamic Squad of Fixed-Wing UAVs Using Deep Reinforcement Learning](http://arxiv.org/abs/2101.08074)


  Developing the flocking behavior for a dynamic squad of fixed-wing UAVs is
still a challenge due to kinematic complexity and environmental uncertainty. In
this paper, we deal with the decentralized flocking and collision avoidance
problem through deep reinforcement learning (DRL). Specifically, we formulate a
decentralized DRL-based decision making framework from the perspective of every
follower, where a collision avoidance mechanism is integrated into the flocking
controller. Then, we propose a novel reinforcement learning algorithm PS-CACER
for training a shared control policy for all the followers. Besides, we design
a plug-n-play embedding module based on convolutional neural networks and the
attention mechanism. As a result, the variable-length system state can be
encoded into a fixed-length embedding vector, which makes the learned DRL
policy independent with the number and the order of followers. Finally,
numerical simulation results demonstrate the effectiveness of the proposed
method, and the learned policies can be directly transferred to semi-physical
simulation without any parameter finetuning.

    

### [[2101.11520] Supervised Tree-Wasserstein Distance](http://arxiv.org/abs/2101.11520)


  To measure the similarity of documents, the Wasserstein distance is a
powerful tool, but it requires a high computational cost. Recently, for fast
computation of the Wasserstein distance, methods for approximating the
Wasserstein distance using a tree metric have been proposed. These tree-based
methods allow fast comparisons of a large number of documents; however, they
are unsupervised and do not learn task-specific distances. In this work, we
propose the Supervised Tree-Wasserstein (STW) distance, a fast, supervised
metric learning method based on the tree metric. Specifically, we rewrite the
Wasserstein distance on the tree metric by the parent-child relationships of a
tree and formulate it as a continuous optimization problem using a contrastive
loss. Experimentally, we show that the STW distance can be computed fast, and
improves the accuracy of document classification tasks. Furthermore, the STW
distance is formulated by matrix multiplications, runs on a GPU, and is
suitable for batch processing. Therefore, we show that the STW distance is
extremely efficient when comparing a large number of documents.

    

### [[2103.00083] Deep Quantile Aggregation](http://arxiv.org/abs/2103.00083)


  Conditional quantile estimation is a key statistical learning challenge
motivated by the need to quantify uncertainty in predictions or to model a
diverse population without being overly reductive. As such, many models have
been developed for this problem. Adopting a meta viewpoint, we propose a
general framework (inspired by neural network optimization) for aggregating any
number of conditional quantile models in order to boost predictive accuracy. We
consider weighted ensembling strategies of increasing flexibility where the
weights may vary over individual models, quantile levels, and feature values.
An appeal of our approach is its portability: we ensure that estimated
quantiles at adjacent levels do not cross by applying simple transformations
through which gradients can be backpropagated, and this allows us to leverage
the modern deep learning toolkit for building quantile ensembles. Our
experiments confirm that ensembling can lead to big gains in accuracy, even
when the constituent models are themselves powerful and flexible.

    

### [[2103.00959] CogDL: Toolkit for Deep Learning on Graphs](http://arxiv.org/abs/2103.00959)


  Deep learning on graphs has attracted tremendous attention from the graph
learning community in recent years. It has been widely used in several
real-world applications such as social network analysis and recommender
systems. In this paper, we introduce CogDL, an extensive toolkit for deep
learning on graphs that allows researchers and developers to easily conduct
experiments and build applications. It provides standard training and
evaluation for the most important tasks in the graph domain, including node
classification, graph classification, etc. For each task, it provides
implementations of state-of-the-art models. The models in our toolkit are
divided into two major parts, graph embedding methods and graph neural
networks. Most of the graph embedding methods learn node-level or graph-level
representations in an unsupervised way and preserves the graph properties such
as structural information, while graph neural networks capture node features
and work in semi-supervised or self-supervised settings. All models implemented
in our toolkit can be easily reproducible for leaderboard results. Most models
in CogDL are developed on top of PyTorch, and users can leverage the advantages
of PyTorch to implement their own models. Furthermore, we demonstrate the
effectiveness of CogDL for real-world applications in AMiner, a large academic
mining system.

    

### [[2103.02559] Minimum-Distortion Embedding](http://arxiv.org/abs/2103.02559)


  We consider the vector embedding problem. We are given a finite set of items,
with the goal of assigning a representative vector to each one, possibly under
some constraints (such as the collection of vectors being standardized, i.e.,
have zero mean and unit covariance). We are given data indicating that some
pairs of items are similar, and optionally, some other pairs are dissimilar.
For pairs of similar items, we want the corresponding vectors to be near each
other, and for dissimilar pairs, we want the corresponding vectors to not be
near each other, measured in Euclidean distance. We formalize this by
introducing distortion functions, defined for some pairs of the items. Our goal
is to choose an embedding that minimizes the total distortion, subject to the
constraints. We call this the minimum-distortion embedding (MDE) problem.
The MDE framework is simple but general. It includes a wide variety of
embedding methods, such as spectral embedding, principal component analysis,
multidimensional scaling, dimensionality reduction methods (like Isomap and
UMAP), force-directed layout, and others. It also includes new embeddings, and
provides principled ways of validating historical and new embeddings alike.
We develop a projected quasi-Newton method that approximately solves MDE
problems and scales to large data sets. We implement this method in PyMDE, an
open-source Python package. In PyMDE, users can select from a library of
distortion functions and constraints or specify custom ones, making it easy to
rapidly experiment with different embeddings. Our software scales to data sets
with millions of items and tens of millions of distortion functions. To
demonstrate our method, we compute embeddings for several real-world data sets,
including images, an academic co-author network, US county demographic data,
and single-cell mRNA transcriptomes.

    

### [[2104.03428] Generating Multi-type Temporal Sequences to Mitigate Class-imbalanced Problem](http://arxiv.org/abs/2104.03428)


  From the ad network standpoint, a user's activity is a multi-type sequence of
temporal events consisting of event types and time intervals. Understanding
user patterns in ad networks has received increasing attention from the machine
learning community. Particularly, the problems of fraud detection, Conversion
Rate (CVR), and Click-Through Rate (CTR) prediction are of interest. However,
the class imbalance between major and minor classes in these tasks can bias a
machine learning model leading to poor performance. This study proposes using
two multi-type (continuous and discrete) training approaches for GANs to deal
with the limitations of traditional GANs in passing the gradient updates for
discrete tokens. First, we used the Reinforcement Learning (RL)-based training
approach and then, an approximation of the multinomial distribution
parameterized in terms of the softmax function (Gumble-Softmax). Our extensive
experiments based on synthetic data have shown the trained generator can
generate sequences with desired properties measured by multiple criteria.

    

### [[2104.03466] Learning Graph Structures with Transformer for Multivariate Time Series Anomaly Detection in IoT](http://arxiv.org/abs/2104.03466)


  Many real-world IoT systems, which include a variety of internet-connected
sensory devices, produce substantial amounts of multivariate time series data.
Meanwhile, vital IoT infrastructures like smart power grids and water
distribution networks are frequently targeted by cyber-attacks, making anomaly
detection an important study topic. Modeling such relatedness is, nevertheless,
unavoidable for any efficient and effective anomaly detection system, given the
intricate topological and nonlinear connections that are originally unknown
among sensors. Furthermore, detecting anomalies in multivariate time series is
difficult due to their temporal dependency and stochasticity. This paper
presented GTA, a new framework for multivariate time series anomaly detection
that involves automatically learning a graph structure, graph convolution, and
modeling temporal dependency using a Transformer-based architecture. The
connection learning policy, which is based on the Gumbel-softmax sampling
approach to learn bi-directed links among sensors directly, is at the heart of
learning graph structure. To describe the anomaly information flow between
network nodes, we introduced a new graph convolution called Influence
Propagation convolution. In addition, to tackle the quadratic complexity
barrier, we suggested a multi-branch attention mechanism to replace the
original multi-head self-attention method. Extensive experiments on four
publicly available anomaly detection benchmarks further demonstrate the
superiority of our approach over alternative state-of-the-arts.

    

### [[2104.07932] Interval-censored Hawkes processes](http://arxiv.org/abs/2104.07932)


  This work builds a novel point process and tools to use the Hawkes process
with interval-censored data. Such data records the aggregated counts of events
solely during specific time intervals -- such as the number of patients
admitted to the hospital or the volume of vehicles passing traffic loop
detectors -- and not the exact occurrence time of the events. First, we
establish the Mean Behavior Poisson (MBP) process, a novel Poisson process with
a direct parameter correspondence to the popular self-exciting Hawkes process.
The event intensity function of the MBP is the expected intensity over all
possible Hawkes realizations with the same parameter set. We fit MBP in the
interval-censored setting using an interval-censored Poisson log-likelihood
(IC-LL). We use the parameter equivalence to uncover the parameters of the
associated Hawkes process. Second, we introduce two novel exogenous functions
to distinguish the exogenous from the endogenous events. We propose the
multi-impulse exogenous function when the exogenous events are observed as
event time and the latent homogeneous Poisson process exogenous function when
the exogenous events are presented as interval-censored volumes. Third, we
provide several approximation methods to estimate the intensity and compensator
function of MBP when no analytical solution exists. Fourth and finally, we
connect the interval-censored loss of MBP to a broader class of Bregman
divergence-based functions. Using the connection, we show that the current
state of the art in popularity estimation (Hawkes Intensity Process (HIP)
(Rizoiu et al.,2017b)) is a particular case of the MBP process. We verify our
models through empirical testing on synthetic data and real-world data. We find
that on real-world datasets that our MBP process outperforms HIP for the task
of popularity prediction.

    

### [[2105.02498] Why Approximate Matrix Square Root Outperforms Accurate SVD in Global Covariance Pooling?](http://arxiv.org/abs/2105.02498)


  Global covariance pooling (GCP) aims at exploiting the second-order
statistics of the convolutional feature. Its effectiveness has been
demonstrated in boosting the classification performance of Convolutional Neural
Networks (CNNs). Singular Value Decomposition (SVD) is used in GCP to compute
the matrix square root. However, the approximate matrix square root calculated
using Newton-Schulz iteration \cite{li2018towards} outperforms the accurate one
computed via SVD \cite{li2017second}. We empirically analyze the reason behind
the performance gap from the perspectives of data precision and gradient
smoothness. Various remedies for computing smooth SVD gradients are
investigated. Based on our observation and analyses, a hybrid training protocol
is proposed for SVD-based GCP meta-layers such that competitive performances
can be achieved against Newton-Schulz iteration. Moreover, we propose a new GCP
meta-layer that uses SVD in the forward pass, and Padé Approximants in the
backward propagation to compute the gradients. The proposed meta-layer has been
integrated into different CNN models and achieves state-of-the-art performances
on both large-scale and fine-grained datasets.

    

### [[2105.05222] Including Signed Languages in Natural Language Processing](http://arxiv.org/abs/2105.05222)


  Signed languages are the primary means of communication for many deaf and
hard of hearing individuals. Since signed languages exhibit all the fundamental
linguistic properties of natural language, we believe that tools and theories
of Natural Language Processing (NLP) are crucial towards its modeling. However,
existing research in Sign Language Processing (SLP) seldom attempt to explore
and leverage the linguistic organization of signed languages. This position
paper calls on the NLP community to include signed languages as a research area
with high social and scientific impact. We first discuss the linguistic
properties of signed languages to consider during their modeling. Then, we
review the limitations of current SLP models and identify the open challenges
to extend NLP to signed languages. Finally, we urge (1) the adoption of an
efficient tokenization method; (2) the development of linguistically-informed
models; (3) the collection of real-world signed language data; (4) the
inclusion of local signed language communities as an active and leading voice
in the direction of research.

    

### [[2105.07965] Learn to Intervene: An Adaptive Learning Policy for Restless Bandits in Application to Preventive Healthcare](http://arxiv.org/abs/2105.07965)


  In many public health settings, it is important for patients to adhere to
health programs, such as taking medications and periodic health checks.
Unfortunately, beneficiaries may gradually disengage from such programs, which
is detrimental to their health. A concrete example of gradual disengagement has
been observed by an organization that carries out a free automated call-based
program for spreading preventive care information among pregnant women. Many
women stop picking up calls after being enrolled for a few months. To avoid
such disengagements, it is important to provide timely interventions. Such
interventions are often expensive and can be provided to only a small fraction
of the beneficiaries. We model this scenario as a restless multi-armed bandit
(RMAB) problem, where each beneficiary is assumed to transition from one state
to another depending on the intervention. Moreover, since the transition
probabilities are unknown a priori, we propose a Whittle index based Q-Learning
mechanism and show that it converges to the optimal solution. Our method
improves over existing learning-based methods for RMABs on multiple benchmarks
from literature and also on the maternal healthcare dataset.

    

### [[2105.11646] Structured Convolutional Kernel Networks for Airline Crew Scheduling](http://arxiv.org/abs/2105.11646)


  Motivated by the needs from an airline crew scheduling application, we
introduce structured convolutional kernel networks (Struct-CKN), which combine
CKNs from Mairal et al. (2014) in a structured prediction framework that
supports constraints on the outputs. CKNs are a particular kind of
convolutional neural networks that approximate a kernel feature map on training
data, thus combining properties of deep learning with the non-parametric
flexibility of kernel methods. Extending CKNs to structured outputs allows us
to obtain useful initial solutions on a flight-connection dataset that can be
further refined by an airline crew scheduling solver. More specifically, we use
a flight-based network modeled as a general conditional random field capable of
incorporating local constraints in the learning process. Our experiments
demonstrate that this approach yields significant improvements for the
large-scale crew pairing problem (50,000 flights per month) over standard
approaches, reducing the solution cost by 17% (a gain of millions of dollars)
and the cost of global constraints by 97%.

    

### [[2105.11697] PyTorch, Explain! A Python library for Logic Explained Networks](http://arxiv.org/abs/2105.11697)


  "PyTorch, Explain!" is a Python module integrating a variety of
state-of-the-art approaches to provide logic explanations from neural networks.
This package focuses on bringing these methods to non-specialists. It has
minimal dependencies and it is distributed under the Apache 2.0 licence
allowing both academic and commercial use. Source code and documentation can be
downloaded from the github repository:
this https URL.

    

### [[2105.12342] A data-driven approach to beating SAA out-of-sample](http://arxiv.org/abs/2105.12342)


  While solutions of Distributionally Robust Optimization (DRO) problems can
sometimes have a higher out-of-sample expected reward than the Sample Average
Approximation (SAA), there is no guarantee. In this paper, we introduce the
class of Distributionally Optimistic Optimization (DOO) models, and show that
it is always possible to "beat" SAA out-of-sample if we consider not just
worst-case (DRO) models but also best-case (DOO) ones. We also show, however,
that this comes at a cost: Optimistic solutions are more sensitive to model
error than either worst-case or SAA optimizers, and hence are less robust.

    

### [[2105.13271] OpReg-Boost: Learning to Accelerate Online Algorithms with Operator Regression](http://arxiv.org/abs/2105.13271)


  This paper presents a new regularization approach -- termed OpReg-Boost -- to
boost the convergence and lessen the asymptotic error of online optimization
and learning algorithms. In particular, the paper considers online algorithms
for optimization problems with a time-varying (weakly) convex composite cost.
For a given online algorithm, OpReg-Boost learns the closest algorithmic map
that yields linear convergence; to this end, the learning procedure hinges on
the concept of operator regression. We show how to formalize the operator
regression problem and propose a computationally-efficient Peaceman-Rachford
solver that exploits a closed-form solution of simple quadratically-constrained
quadratic programs (QCQPs). Simulation results showcase the superior properties
of OpReg-Boost w.r.t. the more classical forward-backward algorithm, FISTA, and
Anderson acceleration, and with respect to its close relative
convex-regression-boost (CvxReg-Boost) which is also novel but less performing.

    

### [[2106.02965] Extracting Weighted Automata for Approximate Minimization in Language Modelling](http://arxiv.org/abs/2106.02965)


  In this paper we study the approximate minimization problem for language
modelling. We assume we are given some language model as a black box. The
objective is to obtain a weighted finite automaton (WFA) that fits within a
given size constraint and which mimics the behaviour of the original model
while minimizing some notion of distance between the black box and the
extracted WFA. We provide an algorithm for the approximate minimization of
black boxes trained for language modelling of sequential data over a one-letter
alphabet. By reformulating the problem in terms of Hankel matrices, we leverage
classical results on the approximation of Hankel operators, namely the
celebrated Adamyan-Arov-Krein (AAK) theory. This allows us to use the spectral
norm to measure the distance between the black box and the WFA. We provide
theoretical guarantees to study the potentially infinite-rank Hankel matrix of
the black box, without accessing the training data, and we prove that our
method returns an asymptotically-optimal approximation.

    

### [[2106.05852] Automatic Speech Recognition in Sanskrit: A New Speech Corpus and Modelling Insights](http://arxiv.org/abs/2106.05852)


  Automatic speech recognition (ASR) in Sanskrit is interesting, owing to the
various linguistic peculiarities present in the language. The Sanskrit language
is lexically productive, undergoes euphonic assimilation of phones at the word
boundaries and exhibits variations in spelling conventions and in
pronunciations. In this work, we propose the first large scale study of
automatic speech recognition (ASR) in Sanskrit, with an emphasis on the impact
of unit selection in Sanskrit ASR. In this work, we release a 78 hour ASR
dataset for Sanskrit, which faithfully captures several of the linguistic
characteristics expressed by the language. We investigate the role of different
acoustic model and language model units in ASR systems for Sanskrit. We also
propose a new modelling unit, inspired by the syllable level unit selection,
that captures character sequences from one vowel in the word to the next vowel.
We also highlight the importance of choosing graphemic representations for
Sanskrit and show the impact of this choice on word error rates (WER). Finally,
we extend these insights from Sanskrit ASR for building ASR systems in two
other Indic languages, Gujarati and Telugu. For both these languages, our
experimental results show that the use of phonetic based graphemic
representations in ASR results in performance improvements as compared to ASR
systems that use native scripts.

    

### [[2106.06498] An adaptive cognitive sensor node for ECG monitoring in the Internet of Medical Things](http://arxiv.org/abs/2106.06498)


  The Internet of Medical Things (IoMT) paradigm is becoming mainstream in
multiple clinical trials and healthcare procedures. Cardiovascular diseases
monitoring, usually involving electrocardiogram (ECG) traces analysis, is one
of the most promising and high-impact applications. Nevertheless, to fully
exploit the potential of IoMT in this domain, some steps forward are needed.
First, the edge-computing paradigm must be added to the picture. A certain
level of near-sensor processing has to be enabled, to improve the scalability,
portability, reliability, responsiveness of the IoMT nodes. Second, novel,
increasingly accurate, data analysis algorithms, such as those based on
artificial intelligence and Deep Learning, must be exploited. To reach these
objectives, designers and programmers of IoMT nodes, have to face challenging
optimization tasks, in order to execute fairly complex computing tasks on
low-power wearable and portable processing systems, with tight power and
battery lifetime budgets. In this work, we explore the implementation of a
cognitive data analysis algorithm, based on a convolutional neural network
trained to classify ECG waveforms, on a resource-constrained
microcontroller-based computing platform. To minimize power consumption, we add
an adaptivity layer that dynamically manages the hardware and software
configuration of the device to adapt it at runtime to the required operating
mode. Our experimental results show that adapting the node setup to the
workload at runtime can save up to 50% power consumption. Our optimized and
quantized neural network reaches an accuracy value higher than 97% for
arrhythmia disorders detection on MIT-BIH Arrhythmia dataset.

    

### [[2106.07688] Next Generation Reservoir Computing](http://arxiv.org/abs/2106.07688)


  Reservoir computing is a best-in-class machine learning algorithm for
processing information generated by dynamical systems using observed
time-series data. Importantly, it requires very small training data sets, uses
linear optimization, and thus requires minimal computing resources. However,
the algorithm uses randomly sampled matrices to define the underlying recurrent
neural network and has a multitude of metaparameters that must be optimized.
Recent results demonstrate the equivalence of reservoir computing to nonlinear
vector autoregression, which requires no random matrices, fewer metaparameters,
and provides interpretable results. Here, we demonstrate that nonlinear vector
autoregression excels at reservoir computing benchmark tasks and requires even
shorter training data sets and training time, heralding the next generation of
reservoir computing.

    

### [[2106.13423] Federated Graph Classification over Non-IID Graphs](http://arxiv.org/abs/2106.13423)


  Federated learning has emerged as an important paradigm for training machine
learning models in different domains. For graph-level tasks such as graph
classification, graphs can also be regarded as a special type of data samples,
which can be collected and stored in separate local systems. Similar to other
domains, multiple local systems, each holding a small set of graphs, may
benefit from collaboratively training a powerful graph mining model, such as
the popular graph neural networks (GNNs). To provide more motivation towards
such endeavors, we analyze real-world graphs from different domains to confirm
that they indeed share certain graph properties that are statistically
significant compared with random graphs. However, we also find that different
sets of graphs, even from the same domain or same dataset, are non-IID
regarding both graph structures and node features. To handle this, we propose a
graph clustered federated learning (GCFL) framework that dynamically finds
clusters of local systems based on the gradients of GNNs, and theoretically
justify that such clusters can reduce the structure and feature heterogeneity
among graphs owned by the local systems. Moreover, we observe the gradients of
GNNs to be rather fluctuating in GCFL which impedes high-quality clustering,
and design a gradient sequence-based clustering mechanism based on dynamic time
warping (GCFL+). Extensive experimental results and in-depth analysis
demonstrate the effectiveness of our proposed frameworks.

    

### [[2106.15561] A Survey on Neural Speech Synthesis](http://arxiv.org/abs/2106.15561)


  Text to speech (TTS), or speech synthesis, which aims to synthesize
intelligible and natural speech given text, is a hot research topic in speech,
language, and machine learning communities and has broad applications in the
industry. As the development of deep learning and artificial intelligence,
neural network-based TTS has significantly improved the quality of synthesized
speech in recent years. In this paper, we conduct a comprehensive survey on
neural TTS, aiming to provide a good understanding of current research and
future trends. We focus on the key components in neural TTS, including text
analysis, acoustic models and vocoders, and several advanced topics, including
fast TTS, low-resource TTS, robust TTS, expressive TTS, and adaptive TTS, etc.
We further summarize resources related to TTS (e.g., datasets, opensource
implementations) and discuss future research directions. This survey can serve
both academic researchers and industry practitioners working on TTS.

    

### [[2107.00992] Multimodal Representation for Neural Code Search](http://arxiv.org/abs/2107.00992)


  Semantic code search is about finding semantically relevant code snippets for
a given natural language query. In the state-of-the-art approaches, the
semantic similarity between code and query is quantified as the distance of
their representation in the shared vector space. In this paper, to improve the
vector space, we introduce tree-serialization methods on a simplified form of
AST and build the multimodal representation for the code data. We conduct
extensive experiments using a single corpus that is large-scale and
multi-language: CodeSearchNet. Our results show that both our tree-serialized
representations and multimodal learning model improve the performance of code
search. Last, we define intuitive quantification metrics oriented to the
completeness of semantic and syntactic information of the code data, to help
understand the experimental findings.

    

### [[2107.08966] Decoupling Exploration and Exploitation in Reinforcement Learning](http://arxiv.org/abs/2107.08966)


  Intrinsic rewards are commonly applied to improve exploration in
reinforcement learning. However, these approaches suffer from instability
caused by non-stationary reward shaping and strong dependency on
hyperparameters. In this work, we propose Decoupled RL (DeRL) which trains
separate policies for exploration and exploitation. DeRL can be applied with
on-policy and off-policy RL algorithms. We evaluate DeRL algorithms in two
sparse-reward environments with multiple types of intrinsic rewards. We show
that DeRL is more robust to scaling and speed of decay of intrinsic rewards and
converges to the same evaluation returns than intrinsically motivated baselines
in fewer interactions.

    

### [[2107.11089] A Resolution-Adaptive 8 mm$^\text{2}$ 9.98 Gb/s 39.7 pJ/b 32-Antenna All-Digital Spatial Equalizer for mmWave Massive MU-MIMO in 65nm CMOS](http://arxiv.org/abs/2107.11089)


  All-digital millimeter-wave (mmWave) massive multi-user multiple-input
multiple-output (MU-MIMO) receivers enable extreme data rates but require high
power consumption. In order to reduce power consumption, this paper presents
the first resolution-adaptive all-digital receiver ASIC that is able to adjust
the resolution of the data-converters and baseband-processing engine to the
instantaneous communication scenario. The scalable 32-antenna, 65 nm CMOS
receiver occupies a total area of 8 mm$^\text{2}$ and integrates
analog-to-digital converters (ADCs) with programmable gain and resolution,
beamspace channel estimation, and a resolution-adaptive processing-in-memory
spatial equalizer. With 6-bit ADC samples and a 4-bit spatial equalizer, our
ASIC achieves a throughput of 9.98 Gb/s while being at least 2x more
energy-efficient than state-of-the-art designs.

    

### [[2107.11336] Mitigating Power Attacks through Fine-Grained Instruction Reordering](http://arxiv.org/abs/2107.11336)


  Side-channel attacks are a security exploit that take advantage of
information leakage. They use measurement and analysis of physical parameters
to reverse engineer and extract secrets from a system. Power analysis attacks
in particular, collect a set of power traces from a computing device and use
statistical techniques to correlate this information with the attacked
application data and source code. Counter measures like just-in-time
compilation, random code injection and instruction descheduling obfuscate the
execution of instructions to reduce the security risk. Unfortunately, due to
the randomness and excess instructions executed by these solutions, they
introduce large overheads in performance, power and area.
In this work we propose a scheduling algorithm that dynamically reorders
instructions in an out-of-order processor to provide obfuscated execution and
mitigate power analysis attacks with little-to-no effect on the performance,
power or area of the processor. We exploit the time between operand
availability of critical instructions (slack) to create high-performance random
schedules without requiring additional instructions or static prescheduling.
Further, we perform an extended security analysis using different attacks. We
highlight the dangers of using incorrect adversarial assumptions, which can
often lead to a false sense of security. In that regard, our advanced security
metric demonstrates improvements of 34$\times$, while our basic security
evaluation shows results up to 261$\times$. Moreover, our system achieves
performance within 96% on average, of the baseline unprotected processor.

    

### [[2107.10881] Layer 2 Blockchain Scaling: a Survey](http://arxiv.org/abs/2107.10881)


  Blockchain technology is affected by massive limitations in scalability with
consequent repercussions on performance. This discussion aims at analyzing the
state of the art of current available Layer II solutions to overcome these
limitations, both focusing on theoretical and practical aspects and
highlighting the main differences among the examined frameworks. The structure
of the work is based on three major sections. In particular, the first one is
an introductory part about the technology, the scalability issue and Layer II
as a solution. The second section represents the core of the discussion and
consists of three different subsections, each with a detailed examination of
the respective solution (Lightning Network, Plasma, Rollups); the analysis of
each solution is based on how it affects five key aspects of blockchain
technology and Layer II: scalability, security, decentralization, privacy, fees
and micropayments (the last two are analyzed together given their high
correlation). Finally, the third section includes a tabular summary, followed
by a detailed description of a use-case specifically thought for a practical
evaluation of the presented frameworks. The results of the work met
expectations: all solutions effectively contribute to increasing scalability. A
crucial clarification is that none of the three dominates the others in all
possible fields of application, and the consequences in adopting each, are
different. Therefore, the choice depends on the application context, and a
trade-off must be found between the aspects previously mentioned.

    

### [[2107.10979] Rectifying Administrated ERC20 Tokens](http://arxiv.org/abs/2107.10979)


  The developers of Ethereum smart contracts often implement administrating
patterns, such as censoring certain users, creating or destroying balances on
demand, destroying smart contracts, or injecting arbitrary code. These routines
turn an ERC20 token into an administrated token - the type of Ethereum smart
contract that we scrutinize in this research. We discover that many smart
contracts are administrated, and the owners of these tokens carry lesser social
and legal responsibilities compared to the traditional centralized actors that
those tokens intend to disrupt. This entails two major problems: a) the owners
of the tokens have the ability to quickly steal all the funds and disappear
from the market; and b) if the private key of the owner's account is stolen,
all the assets might immediately turn into the property of the attacker. We
develop a pattern recognition framework based on 9 syntactic features
characterizing administrated ERC20 tokens, which we use to analyze existing
smart contracts deployed on Ethereum Mainnet. Our analysis of 84,062 unique
Ethereum smart contracts reveals that nearly 58% of them are administrated
ERC20 tokens, which accounts for almost 90% of all ERC20 tokens deployed on
Ethereum. To protect users from the frivolousness of unregulated token owners
without depriving the ability of these owners to properly manage their tokens,
we introduce SafelyAdministrated - a library that enforces a responsible
ownership and management of ERC20 tokens. The library introduces three
mechanisms: deferred maintenance, board of trustees and safe pause. We
implement and test SafelyAdministrated in the form of Solidity abstract
contract, which is ready to be used by the next generation of safely
administrated ERC20 tokens.

    

### [[2107.10987] Octo-Tiger's New Hydro Module and Performance Using HPX+CUDA on ORNL's Summit](http://arxiv.org/abs/2107.10987)


  Octo-Tiger is a code for modeling three-dimensional self-gravitating
astrophysical fluids. It was particularly designed for the study of dynamical
mass transfer between interacting binary stars. Octo-Tiger is parallelized for
distributed systems using the asynchronous many-task runtime system, the C++
standard library for parallelism and concurrency (HPX) and utilizes CUDA for
its gravity solver. Recently, we have remodeled Octo-Tiger's hydro solver to
use a three-dimensional reconstruction scheme. In addition, we have ported the
hydro solver to GPU using CUDA kernels. We present scaling results for the new
hydro kernels on ORNL's Summit machine using a Sedov-Taylor blast wave problem.
We also compare Octo-Tiger's new hydro scheme with its old hydro scheme, using
a rotating star as a test problem.

    

### [[2107.11144] Making Reads in BFT State Machine Replication Fast, Linearizable, and Live](http://arxiv.org/abs/2107.11144)


  Practical Byzantine Fault Tolerance (PBFT) is a seminal state machine
replication protocol that achieves a performance comparable to non-replicated
systems in realistic environments. A reason for such high performance is the
set of optimizations introduced in the protocol. One of these optimizations is
read-only requests, a particular type of client request which avoids running
the three-step agreement protocol and allows replicas to respond directly, thus
reducing the latency of reads from five to two communication steps. Given
PBFT's broad influence, its design and optimizations influenced many BFT
protocols and systems that followed, e.g., BFT-SMaRt. We show, for the first
time, that the read-only request optimization introduced in PBFT more than 20
years ago can violate its liveness. Notably, the problem affects not only the
optimized read-only operations but also standard, totally-ordered operations.
We show this weakness by presenting an attack in which a malicious leader
blocks correct clients and present two solutions for patching the protocol,
making read-only operations fast and correct. The two solutions were
implemented on BFT-SMaRt and evaluated in different scenarios, showing their
effectiveness in preventing the identified attack.

    

### [[2107.11331] How to Trust Strangers: Composition of Byzantine Quorum Systems](http://arxiv.org/abs/2107.11331)


  Trust is the basis of any distributed, fault-tolerant, or secure system. A
trust assumption specifies the failures that a system, such as a blockchain
network, can tolerate and determines the conditions under which it operates
correctly. In systems subject to Byzantine faults, the trust assumption is
usually specified through sets of processes that may fail together. Trust has
traditionally been symmetric, such that all processes in the system adhere to
the same, global assumption about potential faults. Recently, asymmetric trust
models have also been considered, especially in the context of blockchains,
where every participant is free to choose who to trust.
In both cases, it is an open question how to compose trust assumptions.
Consider two or more systems, run by different and possibly disjoint sets of
participants, with different assumptions about faults: how can they work
together? This work answers this question for the first time and offers
composition rules for symmetric and for asymmetric quorum systems. These rules
are static and do not require interaction or agreement on the new trust
assumption among the participants. Moreover, they ensure that if the original
systems allow for running a particular protocol (guaranteeing consistency and
availability), then so will the joint system. At the same time, the composed
system tolerates as many faults as possible, subject to the underlying
consistency and availability properties.
Reaching consensus with asymmetric trust in the model of personal Byzantine
quorum systems (Losa et al., DISC 2019) was shown to be impossible, if the
trust assumptions of the processes diverge from each other. With asymmetric
quorum systems, and by applying our composition rule, we show how consensus is
actually possible, even with the combination of disjoint sets of processes.

    

### [[2107.10969] Learning Quadruped Locomotion Policies with Reward Machines](http://arxiv.org/abs/2107.10969)


  Legged robots have been shown to be effective in navigating unstructured
environments. Although there has been much success in learning locomotion
policies for quadruped robots, there is little research on how to incorporate
human knowledge to facilitate this learning process. In this paper, we
demonstrate that human knowledge in the form of LTL formulas can be applied to
quadruped locomotion learning within a Reward Machine (RM) framework.
Experimental results in simulation show that our RM-based approach enables
easily defining diverse locomotion styles, and efficiently learning locomotion
policies of the defined styles.

    

### [[2107.10997] Resource Efficient Mountainous Skyline Extraction using Shallow Learning](http://arxiv.org/abs/2107.10997)


  Skyline plays a pivotal role in mountainous visual geo-localization and
localization/navigation of planetary rovers/UAVs and virtual/augmented reality
applications. We present a novel mountainous skyline detection approach where
we adapt a shallow learning approach to learn a set of filters to discriminate
between edges belonging to sky-mountain boundary and others coming from
different regions. Unlike earlier approaches, which either rely on extraction
of explicit feature descriptors and their classification, or fine-tuning
general scene parsing deep networks for sky segmentation, our approach learns
linear filters based on local structure analysis. At test time, for every
candidate edge pixel, a single filter is chosen from the set of learned filters
based on pixel's structure tensor, and then applied to the patch around it. We
then employ dynamic programming to solve the shortest path problem for the
resultant multistage graph to get the sky-mountain boundary. The proposed
approach is computationally faster than earlier methods while providing
comparable performance and is more suitable for resource constrained platforms
e.g., mobile devices, planetary rovers and UAVs. We compare our proposed
approach against earlier skyline detection methods using four different data
sets. Our code is available at
\url{this https URL}.

    

### [[2107.10998] Pruning Ternary Quantization](http://arxiv.org/abs/2107.10998)


  We propose pruning ternary quantization (PTQ), a simple, yet effective,
symmetric ternary quantization method. The method significantly compresses
neural network weights to a sparse ternary of [-1,0,1] and thus reduces
computational, storage, and memory footprints. We show that PTQ can convert
regular weights to ternary orthonormal bases by simply using pruning and L2
projection. In addition, we introduce a refined straight-through estimator to
finalize and stabilize the quantized weights. Our method can provide at most
46x compression ratio on the ResNet-18 structure, with an acceptable accuracy
of 65.36%, outperforming leading methods. Furthermore, PTQ can compress a
ResNet-18 model from 46 MB to 955KB (~48x) and a ResNet-50 model from 99 MB to
3.3MB (~30x), while the top-1 accuracy on ImageNet drops slightly from 69.7% to
65.3% and from 76.15% to 74.47%, respectively. Our method unifies pruning and
quantization and thus provides a range of size-accuracy trade-off.

    

### [[2107.11019] Generating Large-scale Dynamic Optimization Problem Instances Using the Generalized Moving Peaks Benchmark](http://arxiv.org/abs/2107.11019)


  This document describes the generalized moving peaks benchmark (GMPB) and how
it can be used to generate problem instances for continuous large-scale dynamic
optimization problems. It presents a set of 15 benchmark problems, the relevant
source code, and a performance indicator, designed for comparative studies and
competitions in large-scale dynamic optimization. Although its primary purpose
is to provide a coherent basis for running competitions, its generality allows
the interested reader to use this document as a guide to design customized
problem instances to investigate issues beyond the scope of the presented
benchmark suite. To this end, we explain the modular structure of the GMPB and
how its constituents can be assembled to form problem instances with a variety
of controllable characteristics ranging from unimodal to highly multimodal,
symmetric to highly asymmetric, smooth to highly irregular, and various degrees
of variable interaction and ill-conditioning.

    

### [[2107.11100] Malware Analysis with Artificial Intelligence and a Particular Attention on Results Interpretability](http://arxiv.org/abs/2107.11100)


  Malware detection and analysis are active research subjects in cybersecurity
over the last years. Indeed, the development of obfuscation techniques, as
packing, for example, requires special attention to detect recent variants of
malware. The usual detection methods do not necessarily provide tools to
interpret the results. Therefore, we propose a model based on the
transformation of binary files into grayscale image, which achieves an accuracy
rate of 88%. Furthermore, the proposed model can determine if a sample is
packed or encrypted with a precision of 85%. It allows us to analyze results
and act appropriately. Also, by applying attention mechanisms on detection
models, we have the possibility to identify which part of the files looks
suspicious. This kind of tool should be very useful for data analysts, it
compensates for the lack of interpretability of the common detection models,
and it can help to understand why some malicious files are undetected.

    

### [[2107.11150] User Preferences and the Shortest Path](http://arxiv.org/abs/2107.11150)


  Indoor navigation systems leverage shortest path algorithms to calculate
routes. In order to define the "shortest path", a cost function has to be
specified based on theories and heuristics in the application domain. For the
domain of indoor routing, we survey theories and criteria identified in the
literature as essential for human path planning. We drive quantitative
definitions and integrate them into a cost function that weights each of the
criteria separately. We then apply an exhaustive grid search to find weights
that lead to an ideal cost function. "Ideal" here is defined as guiding the
algorithm to plan routes that are most similar to those chosen by humans. To
explore which criteria should be taken into account in an improved pathfinding
algorithm, eleven different factors whose favorable impact on route selection
has been established in past research were considered. Each factor was included
separately in the Dijkstra algorithm and the similarity of thus calculated
routes to the actual routes chosen by students at the University of Regensburg
was determined. This allows for a quantitative assessment of the factors'
impact and further constitutes a way to directly compare them. A reduction of
the number of turns, streets, revolving doors, entryways, elevators as well as
the combination of the aforementioned factors was found to have a positive
effect and generate paths that were favored over the shortest path. Turns and
the combination of criteria turned out to be most impactful.

    

### [[2107.11201] A Fitness Landscape View on the Tuning of an Asynchronous Master-Worker EA for Nuclear Reactor Design](http://arxiv.org/abs/2107.11201)


  In the context of the introduction of intermittent renewable energies, we
propose to optimize the main variables of the control rods of a nuclear power
plant to improve its capability to load-follow. The design problem is a
black-box combinatorial optimization problem with expensive evaluation based on
a multi-physics simulator. Therefore, we use a parallel asynchronous
master-worker Evolutionary Algorithm scaling up to thousand computing units.
One main issue is the tuning of the algorithm parameters. A fitness landscape
analysis is conducted on this expensive real-world problem to show that it
would be possible to tune the mutation parameters according to the low-cost
estimation of the fitness landscape features.

    

### [[2107.11245] An Improved Algorithm of Robot Path Planning in Complex Environment Based on Double DQN](http://arxiv.org/abs/2107.11245)


  Deep Q Network (DQN) has several limitations when applied in planning a path
in environment with a number of dilemmas according to our experiment. The
reward function may be hard to model, and successful experience transitions are
difficult to find in experience replay. In this context, this paper proposes an
improved Double DQN (DDQN) to solve the problem by reference to A* and
Rapidly-Exploring Random Tree (RRT). In order to achieve the rich experiments
in experience replay, the initialization of robot in each training round is
redefined based on RRT strategy. In addition, reward for the free positions is
specially designed to accelerate the learning process according to the
definition of position cost in A*. The simulation experimental results validate
the efficiency of the improved DDQN, and robot could successfully learn the
ability of obstacle avoidance and optimal path planning in which DQN or DDQN
has no effect.

    

### [[2107.11252] Adversarial Reinforced Instruction Attacker for Robust Vision-Language Navigation](http://arxiv.org/abs/2107.11252)


  Language instruction plays an essential role in the natural language grounded
navigation tasks. However, navigators trained with limited human-annotated
instructions may have difficulties in accurately capturing key information from
the complicated instruction at different timesteps, leading to poor navigation
performance. In this paper, we exploit to train a more robust navigator which
is capable of dynamically extracting crucial factors from the long instruction,
by using an adversarial attacking paradigm. Specifically, we propose a Dynamic
Reinforced Instruction Attacker (DR-Attacker), which learns to mislead the
navigator to move to the wrong target by destroying the most instructive
information in instructions at different timesteps. By formulating the
perturbation generation as a Markov Decision Process, DR-Attacker is optimized
by the reinforcement learning algorithm to generate perturbed instructions
sequentially during the navigation, according to a learnable attack score.
Then, the perturbed instructions, which serve as hard samples, are used for
improving the robustness of the navigator with an effective adversarial
training strategy and an auxiliary self-supervised reasoning task. Experimental
results on both Vision-and-Language Navigation (VLN) and Navigation from Dialog
History (NDH) tasks show the superiority of our proposed method over
state-of-the-art methods. Moreover, the visualization analysis shows the
effectiveness of the proposed DR-Attacker, which can successfully attack
crucial information in the instructions at different timesteps. Code is
available at this https URL.

    

### [[1604.08612] Mysteries of Visual Experience](http://arxiv.org/abs/1604.08612)


  Science is a crowning glory of the human spirit and its applications remain
our best hope for social progress. But there are limitations to current science
and perhaps to any science. The general mind-body problem is known to be
intractable and currently mysterious. This is one of many deep problems that
are universally agreed to be beyond the current purview of Science, including
quantum phenomena, etc. But all of these famous unsolved problems are either
remote from everyday experience (entanglement, dark matter) or are hard to even
define sharply (phenomenology, consciousness, etc.).
In this note, we will consider some obvious computational problems in vision
that arise every time that we open our eyes and yet are demonstrably
incompatible with current theories of neural computation. The focus will be on
two related phenomena, known as the neural binding problem and the illusion of
a detailed stable visual world.

    

### [[1902.06123] Timeline-based planning: Expressiveness and Complexity](http://arxiv.org/abs/1902.06123)


  Timeline-based planning is an approach originally developed in the context of
space mission planning and scheduling, where problem domains are modelled as
systems made of a number of independent but interacting components, whose
behaviour over time, the timelines, is governed by a set of temporal
constraints. This approach is different from the action-based perspective of
common PDDL-like planning languages. Timeline-based systems have been
successfully deployed in a number of space missions and other domains. However,
despite this practical success, a thorough theoretical understanding of the
paradigm was missing.
This thesis fills this gap, providing the first detailed account of formal
and computational properties of the timeline-based approach to planning. In
particular, we show that a particularly restricted variant of the formalism is
already expressive enough to compactly capture action-based temporal planning
problems. Then, finding a solution plan for a timeline-based planning problem
is proved to be EXPSPACE-complete.
Then, we study the problem of timeline-based planning with uncertainty, that
include external components whose behaviour is not under the control of the
planned system. We identify a few issues in the state-of-the-art approach based
on flexible plans, proposing timeline-based games, a more general
game-theoretic formulation of the problem, that addresses those issues. We show
that winning strategies for such games can be found in doubly-exponential time.
Then, we study the expressiveness of the formalism from a logic point of
view, showing that (most of) timeline-based planning problems can be captured
by Bounded TPTL with Past, a fragment of TPTL+P that, unlike the latter, keeps
an EXPSPACE satisfiability problem. The logic is introduced and its
satisfiabilty problem is solved by extending a recent one-pass tree-shaped
tableau method for LTL.

    

### [[2010.13753] Handgun detection using combined human pose and weapon appearance](http://arxiv.org/abs/2010.13753)


  Closed-circuit television (CCTV) systems are essential nowadays to prevent
security threats or dangerous situations, in which early detection is crucial.
Novel deep learning-based methods have allowed to develop automatic weapon
detectors with promising results. However, these approaches are mainly based on
visual weapon appearance only. For handguns, body pose may be a useful cue,
especially in cases where the gun is barely visible. In this work, a novel
method is proposed to combine, in a single architecture, both weapon appearance
and human pose information. First, pose keypoints are estimated to extract hand
regions and generate binary pose images, which are the model inputs. Then, each
input is processed in different subnetworks and combined to produce the handgun
bounding box. Results obtained show that the combined model improves the
handgun detection state of the art, achieving from 4.23 to 18.9 AP points more
than the best previous approach.

    

### [[2012.01189] Classifying bacteria clones using attention-based deep multiple instance learning interpreted by persistence homology](http://arxiv.org/abs/2012.01189)


  In this work, we analyze if it is possible to distinguish between different
clones of the same bacteria species (Klebsiella pneumoniae) based only on
microscopic images. It is a challenging task, previously considered impossible
due to the high clones similarity. For this purpose, we apply a multi-step
algorithm with attention-based multiple instance learning. Except for obtaining
accuracy at the level of 0.9, we introduce extensive interpretability based on
CellProfiler and persistence homology, increasing the understandability and
trust in the model.

    

### [[2103.13460] Under Pressure: Learning to Detect Slip with Barometric Tactile Sensors](http://arxiv.org/abs/2103.13460)


  Despite the utility of tactile information, tactile sensors have yet to be
widely deployed in industrial robotics settings -- part of the challenge lies
in identifying slip and other key events from the tactile data stream. In this
paper, we present a learning-based method to detect slip using barometric
tactile sensors. Although these sensors have a low resolution, they have many
other desirable properties including high reliability and durability, a very
slim profile, and a low cost. We are able to achieve slip detection accuracies
of greater than 91% while being robust to the speed and direction of the slip
motion. Further, we test our detector on two robot manipulation tasks involving
common household objects and demonstrate successful generalization to
real-world scenarios not seen during training. We show that barometric tactile
sensing technology, combined with data-driven learning, is potentially suitable
for complex manipulation tasks such as slip compensation.

    

### [[2105.04067] Neural Graph Matching based Collaborative Filtering](http://arxiv.org/abs/2105.04067)


  User and item attributes are essential side-information; their interactions
(i.e., their co-occurrence in the sample data) can significantly enhance
prediction accuracy in various recommender systems. We identify two different
types of attribute interactions, inner interactions and cross interactions:
inner interactions are those between only user attributes or those between only
item attributes; cross interactions are those between user attributes and item
attributes. Existing models do not distinguish these two types of attribute
interactions, which may not be the most effective way to exploit the
information carried by the interactions. To address this drawback, we propose a
neural Graph Matching based Collaborative Filtering model (GMCF), which
effectively captures the two types of attribute interactions through modeling
and aggregating attribute interactions in a graph matching structure for
recommendation. In our model, the two essential recommendation procedures,
characteristic learning and preference matching, are explicitly conducted
through graph learning (based on inner interactions) and node matching (based
on cross interactions), respectively. Experimental results show that our model
outperforms state-of-the-art models. Further studies verify the effectiveness
of GMCF in improving the accuracy of recommendation.

    

### [[2106.15078] Don't Take It Literally: An Edit-Invariant Sequence Loss for Text Generation](http://arxiv.org/abs/2106.15078)


  Neural text generation models are typically trained by maximizing
log-likelihood with the sequence cross entropy loss, which encourages an exact
token-by-token match between a target sequence with a generated sequence. Such
training objective is sub-optimal when the target sequence not perfect, e.g.,
when the target sequence is corrupted with noises, or when only weak sequence
supervision is available. To address this challenge, we propose a novel
Edit-Invariant Sequence Loss (EISL), which computes the matching loss of a
target n-gram with all n-grams in the generated sequence. EISL draws
inspirations from convolutional networks (ConvNets) which are shift-invariant
to images, hence is robust to the shift of n-grams to tolerate edits in the
target sequences. Moreover, the computation of EISL is essentially a
convolution operation with target n-grams as kernels, which is easy to
implement with existing libraries. To demonstrate the effectiveness of EISL, we
conduct experiments on three tasks: machine translation with noisy target
sequences, unsupervised text style transfer, and non-autoregressive machine
translation. Experimental results show our method significantly outperforms
cross entropy loss on these three tasks.

    

### [[2107.00156] A Study of the Quality of Wikidata](http://arxiv.org/abs/2107.00156)


  Wikidata has been increasingly adopted by many communities for a wide variety
of applications, which demand high-quality knowledge to deliver successful
results. In this paper, we develop a framework to detect and analyze
low-quality statements in Wikidata by shedding light on the current practices
exercised by the community. We explore three indicators of data quality in
Wikidata, based on: 1) community consensus on the currently recorded knowledge,
assuming that statements that have been removed and not added back are
implicitly agreed to be of low quality; 2) statements that have been
deprecated; and 3) constraint violations in the data. We combine these
indicators to detect low-quality statements, revealing challenges with
duplicate entities, missing triples, violated type rules, and taxonomic
distinctions. Our findings complement ongoing efforts by the Wikidata community
to improve data quality, aiming to make it easier for users and editors to find
and correct mistakes.

    

### [[2104.14879] Optimal control policies for resource allocation in the Cloud: comparison between Markov decision process and heuristic approaches](http://arxiv.org/abs/2104.14879)


  We consider an auto-scaling technique in a cloud system where virtual
machines hosted on a physical node are turned on and off depending on the
queue's occupation (or thresholds), in order to minimise a global cost
integrating both energy consumption and performance. We propose several
efficient optimisation methods to find threshold values minimising this global
cost: local search heuristics coupled with aggregation of Markov chain and with
queues approximation techniques to reduce the execution time and improve the
accuracy. The second approach tackles the problem with a Markov Decision
Process (MDP) for which we proceed to a theoretical study and provide
theoretical comparison with the first approach. We also develop structured MDP
algorithms integrating hysteresis properties. We show that MDP algorithms
(value iteration, policy iteration) and especially structured MDP algorithms
outperform the devised heuristics, in terms of time execution and accuracy.
Finally, we propose a cost model for a real scenario of a cloud system to apply
our optimisation algorithms and show their relevance.

    

### [[2107.10936] Minimal Session Types for the π-calculus (Extended Version)](http://arxiv.org/abs/2107.10936)


  Session types enable the static verification of message-passing programs. A
session type specifies a channel's protocol as sequences of messages. Prior
work established a minimality result: every process typable with standard
session types can be compiled down to a process typable using minimal session
types: session types without sequencing construct. This result justifies
session types in terms of themselves; it holds for a higher-order session
\pi-calculus, where values are abstractions (functions from names to
processes).
This paper establishes a minimality result but now for the session
\pi-calculus, the language in which values are names and for which session
types have been more widely studied. This new minimality result for the session
\pi-calculus can be obtained by composing existing results. We develop
associated optimizations of this result, and establish its static and dynamic
correctness.

    

### [[2107.11280] Type-based Enforcement of Infinitary Trace Properties for Java](http://arxiv.org/abs/2107.11280)


  A common approach to improve software quality is to use programming
guidelines to avoid common kinds of errors. In this paper, we consider the
problem of enforcing guidelines for Featherweight Java (FJ). We formalize
guidelines as sets of finite or infinite execution traces and develop a
region-based type and effect system for FJ that can enforce such guidelines. We
build on the work by Erbatur, Hofmann and Zălinescu, who presented a type
system for verifying the finite event traces of terminating FJ programs. We
refine this type system, separating region typing from FJ typing, and use ideas
of Hofmann and Chen to extend it to capture also infinite traces produced by
non-terminating programs. Our type and effect system can express properties of
both finite and infinite traces and can compute information about the possible
infinite traces of FJ programs. Specifically, the set of infinite traces of a
method is constructed as the greatest fixed point of the operator which
calculates the possible traces of method bodies. Our type inference algorithm
is realized by working with the finitary abstraction of the system based on
Büchi automata.

    

### [[2107.11347] Comprehending nulls](http://arxiv.org/abs/2107.11347)


  The Nested Relational Calculus (NRC) has been an influential high-level query
language, providing power and flexibility while still allowing translation to
standard SQL queries. It has also been used as a basis for language-integrated
query in programming languages such as F#, Scala, and Links. However, SQL's
treatment of incomplete information, using nulls and three-valued logic, is not
compatible with `standard' NRC based on two-valued logic. Nulls are widely used
in practice for incomplete data, but the question of how to accommodate
SQL-style nulls and incomplete information in NRC, or integrate such queries
into a typed programming language, appears not to have been studied thoroughly.
In this paper we consider two approaches: an explicit approach in which option
types are used to represent (possibly) nullable primitive types, and an
implicit approach in which types are treated as possibly-null by default. We
give translations relating the implicit and explicit approaches, discuss
handling nulls in language integration, and sketch extensions of normalization
and conservativity results.

    