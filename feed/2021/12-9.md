
## 2021-12-9

### [[2112.04031] Estimating Quality of Transmission in a Live Production Network using Machine Learning](http://arxiv.org/abs/2112.04031)


  We demonstrate QoT estimation in a live network utilizing neural networks
trained on synthetic data spanning a large parameter space. The ML-model
predicts the measured lightpath performance with <0.5dB SNR error over a wide
configuration range.

    

### [[2112.04039] A QoT Estimation Method using EGN-assisted Machine Learning for Network Planning Applications](http://arxiv.org/abs/2112.04039)


  An ML model based on precomputed per-channel SCI is proposed. Due to its
superior accuracy over closed-form GN, an average SNR gain of 1.1 dB in an
end-to-end link optimization and a 40% reduction in required lightpaths to meet
traffic requests in a network planning scenario are shown.

    

### [[2112.04070] CoMP Enhanced Subcarrier and Power Allocation for Multi-Numerology based 5G-NR Networks](http://arxiv.org/abs/2112.04070)


  With proliferation of fifth generation (5G) new radio (NR) technology, it is
expected to meet the requirement of diverse traffic demands. We have designed a
coordinated multi-point (CoMP) enhanced flexible multi-numerology (MN) for
5G-NR networks to improve the network performance in terms of throughput and
latency. We have proposed a CoMP enhanced joint subcarrier and power allocation
(CESP) scheme which aims at maximizing sum rate under the considerations of
transmit power limitation and guaranteed quality-of-service (QoS) including
throughput and latency restrictions. By employing difference of two concave
functions (D.C.) approximation and abstract Lagrangian duality method, we
theoretically transform the original non-convex nonlinear problem into a
solvable maximization problem. Moreover, the convergence of our proposed CESP
algorithm with D.C. approximation is analytically derived with proofs, and is
further validated via numerical results. Simulation results demonstrated that
our proposed CESP algorithm outperforms the conventional non-CoMP and single
numerology mechanisms along with other existing benchmarks in terms of lower
latency and higher throughput under the scenarios of uniform and edge users.

    

### [[2112.04079] Flexible Functional Split for Processing Sharing and CoMP-Enhanced Mixed eMBB/URLLC Services in Beyond 5G Wireless Networks](http://arxiv.org/abs/2112.04079)


  With explosively escalating service demands, beyond fifth generation (B5G)
aims to realize various requirements for multi-service networks, i.e., mixed
enhanced mobile broadband (eMBB) and ultra-reliable low-latency communication
(URLLC) services. To flexibly serve diverse traffic, various functional split
options (FSOs) are specified by B5G protocols enabling different network
functions. In order to improve signal qualities for edge users, we consider
FSO-based coordinated multi-point (CoMP) transmission as a prominent technique
capable of supporting high traffic demands. However, due to conventional
confined hardware processing capability, a processing sharing (PS) model is
introduced to deal with high latency for multi-service FSO-based networks.
Therefore, it becomes essential to assign CoMP-enhanced functional split modes
under PS model. A more tractable FSO-based network in terms of ergodic rate and
reliability is derived by stochastic geometry approach. Moreover, we have
proposed CoMP-enhanced functional split mode allocation (CFSMA) scheme to
adaptively assign FSOs to provide enhanced mixed URLLC-eMBB services. The
simulation results have validated analytical derivation and demonstrated that
the proposed CFSMA scheme optimizes system spectrum efficiency while
guaranteeing stringent latency requirement.

    

### [[2112.04114] ESAFE: Enterprise Security and Forensics at Scale](http://arxiv.org/abs/2112.04114)


  Securing enterprise networks presents challenges in terms of both their size
and distributed structure. Data required to detect and characterize malicious
activities may be diffused and may be located across network and endpoint
devices. Further, cyber-relevant data routinely exceeds total available
storage, bandwidth, and analysis capability, often by several orders of
magnitude. Real-time detection of threats within or across very large
enterprise networks is not simply an issue of scale, but also a challenge due
to the variable nature of malicious activities and their presentations. The
system seeks to develop a hierarchy of cyber reasoning layers to detect
malicious behavior, characterize novel attack vectors and present an analyst
with a contextualized human-readable output from a series of machine learning
models. We developed machine learning algorithms for scalable throughput and
improved recall for our Multi-Resolution Joint Optimization for Enterprise
Security and Forensics (ESAFE) solution. This Paper will provide an overview of
ESAFE's Machine Learning Modules, Attack Ontologies, and Automated Smart Alert
generation which provide multi-layer reasoning over cross-correlated sensors
for analyst consumption.

    

### [[2112.04188] Beam Squint in Ultra-wideband mmWave Systems: RF Lens Array vs. Phase-Shifter-Based Array](http://arxiv.org/abs/2112.04188)


  In this article, we discuss the potential of radio frequency (RF) lens for
ultra-wideband millimeter-wave (mmWave) systems. In terms of the beam squint,
we compare the proposed RF lens antenna with the phase shifter-based array for
hybrid beamforming. To reduce the complexities for fully digital beamforming,
researchers have come up with RF lens-based hybrid beamforming. The use of
mmWave systems, however, causes an increase in bandwidth, which gives rise to
the beam squint phenomenon. We first find the causative factors for beam squint
in the dielectric RF lens antenna. Based on the beamforming gain at each
frequency, we verify that, in a specific situation, RF lens can be free of the
beam squint effect. We use 3D electromagnetic analysis software to numerically
interpret the beam squint of each antenna type. Based on the results, we
present the degraded spectral efficiency by system-level simulations with 3D
indoor ray tracing. Finally, to verify our analysis, we fabricate an actual RF
lens antenna and demonstrate the real performance using a mmWave, NI PXIe,
software-defined radio system.

    

### [[2112.04257] Tutorial on communication between access networks and the 5G core](http://arxiv.org/abs/2112.04257)


  Fifth-generation (5G) networks enable a variety of use cases, e.g.,
Ultra-Reliable and Low-Latency Communications, enhanced Mobile Broadband, and
massive Machine Type Communication. To explore the full potential of these use
cases, it is mandatory to understand the communication between User Equipment
(UE), Radio Access Network (RAN), and 5G Core (5GC), which support new network
concepts and paradigms. For example, network slicing plays a crucial role in
the communication system to address the challenges expected by the 5G networks.
3rd Generation Partnership Project has recently published Release 16, including
the protocols used to communicate between RANs and 5GC, i.e., Non-Access
Stratum (NAS) and NG Application Protocol (NGAP). The main goal of this article
is to present a comprehensive tutorial about NAS and NGAP specifications using
a didactic and practical approach. The tutorial describes the protocol stacks
and aspects of the functionality of these protocols in 5G networks, such as
authentication and identification procedures, data session establishment, and
allocation of resources. Moreover, we review message flows related to these
protocols in UE and Next Generation Node B (gNodeB) registration. To illustrate
the concepts presented in the tutorial, we introduce a 5GC tester that
implements NAS and NGAP for availing of three open-source 5GC projects on a
black-box testing methodology.

    

### [[2112.04263] Artificial Intelligence Powered Mobile Networks: From Cognition to Decision](http://arxiv.org/abs/2112.04263)


  Mobile networks (MN) are anticipated to provide unprecedented opportunities
to enable a new world of connected experiences and radically shift the way
people interact with everything. MN are becoming more and more complex, driven
by ever-increasingly complicated configuration issues and blossoming new
service requirements. This complexity poses significant challenges in
deployment, management, operation, optimization, and maintenance, since they
require a complete understanding and cognition of MN. Artificial intelligence
(AI), which deals with the simulation of intelligent behavior in computers, has
demonstrated enormous success in many application domains, suggesting its
potential in cognizing the state of MN and making intelligent decisions. In
this paper, we first propose an AI-powered mobile network architecture and
discuss challenges in terms of cognition complexity, decisions with
high-dimensional action space, and self-adaption to system dynamics. Then,
potential solutions that are associated with AI are discussed. Finally, we
propose a deep learning approach that directly maps the state of MN to
perceived QoS, integrating cognition with the decision. Our proposed approach
helps operators in making more intelligent decisions to guarantee QoS.
Meanwhile, the effectiveness and advantages of our proposed approach are
demonstrated on a real-world dataset, involving $31261$ users over $77$
stations within $5$ days.

    

### [[2112.04441] Autoencoder-based Communications with Reconfigurable Intelligent Surfaces](http://arxiv.org/abs/2112.04441)


  This paper presents a novel approach for the joint design of a reconfigurable
intelligent surface (RIS) and a transmitter-receiver pair that are trained
together as a set of deep neural networks (DNNs) to optimize the end-to-end
communication performance at the receiver. The RIS is a software-defined array
of unit cells that can be controlled in terms of the scattering and reflection
profiles to focus the incoming signals from the transmitter to the receiver.
The benefit of the RIS is to improve the coverage and spectral efficiency for
wireless communications by overcoming physical obstructions of the
line-of-sight (LoS) links. The selection process of the RIS beam codeword (out
of a pre-defined codebook) is formulated as a DNN, while the operations of the
transmitter-receiver pair are modeled as two DNNs, one for the encoder (at the
transmitter) and the other one for the decoder (at the receiver) of an
autoencoder, by accounting for channel effects including those induced by the
RIS in between. The underlying DNNs are jointly trained to minimize the symbol
error rate at the receiver. Numerical results show that the proposed design
achieves major gains in error performance with respect to various baseline
schemes, where no RIS is used or the selection of the RIS beam is separated
from the design of the transmitter-receiver pair.

    

### [[2112.03911] Dyadic Sex Composition and Task Classification Using fNIRS Hyperscanning Data](http://arxiv.org/abs/2112.03911)


  Hyperscanning with functional near-infrared spectroscopy (fNIRS) is an
emerging neuroimaging application that measures the nuanced neural signatures
underlying social interactions. Researchers have assessed the effect of sex and
task type (e.g., cooperation versus competition) on inter-brain coherence
during human-to-human interactions. However, no work has yet used deep
learning-based approaches to extract insights into sex and task-based
differences in an fNIRS hyperscanning context. This work proposes a
convolutional neural network-based approach to dyadic sex composition and task
classification for an extensive hyperscanning dataset with $N = 222$
participants. Inter-brain signal similarity computed using dynamic time warping
is used as the input data. The proposed approach achieves a maximum
classification accuracy of greater than $80$ percent, thereby providing a new
avenue for exploring and understanding complex brain behavior.

    

### [[2112.03912] RID-Noise: Towards Robust Inverse Design under Noisy Environments](http://arxiv.org/abs/2112.03912)


  From an engineering perspective, a design should not only perform well in an
ideal condition, but should also resist noises. Such a design methodology,
namely robust design, has been widely implemented in the industry for product
quality control. However, classic robust design requires a lot of evaluations
for a single design target, while the results of these evaluations could not be
reused for a new target. To achieve a data-efficient robust design, we propose
Robust Inverse Design under Noise (RID-Noise), which can utilize existing noisy
data to train a conditional invertible neural network (cINN). Specifically, we
estimate the robustness of a design parameter by its predictability, measured
by the prediction error of a forward neural network. We also define a
sample-wise weight, which can be used in the maximum weighted likelihood
estimation of an inverse model based on a cINN. With the visual results from
experiments, we clearly justify how RID-Noise works by learning the
distribution and robustness from data. Further experiments on several
real-world benchmark tasks with noises confirm that our method is more
effective than other state-of-the-art inverse design methods. Code and
supplementary is publicly available at
this https URL


### [[2112.03914] Synthetic Acute Hypotension and Sepsis Datasets Based on MIMIC-III and Published as Part of the Health Gym Project](http://arxiv.org/abs/2112.03914)


  These two synthetic datasets comprise vital signs, laboratory test results,
administered fluid boluses and vasopressors for 3,910 patients with acute
hypotension and for 2,164 patients with sepsis in the Intensive Care Unit
(ICU). The patient cohorts were built using previously published inclusion and
exclusion criteria and the data were created using Generative Adversarial
Networks (GANs) and the MIMIC-III Clinical Database. The risk of identity
disclosure associated with the release of these data was estimated to be very
low (0.045%). The datasets were generated and published as part of the Health
Gym, a project aiming to publicly distribute synthetic longitudinal health data
for developing machine learning algorithms (with a particular focus on offline
reinforcement learning) and for educational purposes.

    

### [[2112.03917] Scalable 3D Semantic Segmentation for Gun Detection in CT Scans](http://arxiv.org/abs/2112.03917)


  With the increased availability of 3D data, the need for solutions processing
those also increased rapidly. However, adding dimension to already reliably
accurate 2D approaches leads to immense memory consumption and higher
computational complexity. These issues cause current hardware to reach its
limitations, with most methods forced to reduce the input resolution
drastically. Our main contribution is a novel deep 3D semantic segmentation
method for gun detection in baggage CT scans that enables fast training and low
video memory consumption for high-resolution voxelized volumes. We introduce a
moving pyramid approach that utilizes multiple forward passes at inference time
for segmenting an instance.

    

### [[2112.03946] Generative Adversarial Network (GAN) and Enhanced Root Mean Square Error (ERMSE): Deep Learning for Stock Price Movement Prediction](http://arxiv.org/abs/2112.03946)


  The prediction of stock price movement direction is significant in financial
circles and academic. Stock price contains complex, incomplete, and fuzzy
information which makes it an extremely difficult task to predict its
development trend. Predicting and analysing financial data is a nonlinear,
time-dependent problem. With rapid development in machine learning and deep
learning, this task can be performed more effectively by a purposely designed
network. This paper aims to improve prediction accuracy and minimizing
forecasting error loss through deep learning architecture by using Generative
Adversarial Networks. It was proposed a generic model consisting of Phase-space
Reconstruction (PSR) method for reconstructing price series and Generative
Adversarial Network (GAN) which is a combination of two neural networks which
are Long Short-Term Memory (LSTM) as Generative model and Convolutional Neural
Network (CNN) as Discriminative model for adversarial training to forecast the
stock market. LSTM will generate new instances based on historical basic
indicators information and then CNN will estimate whether the data is predicted
by LSTM or is real. It was found that the Generative Adversarial Network (GAN)
has performed well on the enhanced root mean square error to LSTM, as it was
4.35% more accurate in predicting the direction and reduced processing time and
RMSE by 78 secs and 0.029, respectively. This study provides a better result in
the accuracy of the stock index. It seems that the proposed system concentrates
on minimizing the root mean square error and processing time and improving the
direction prediction accuracy, and provides a better result in the accuracy of
the stock index.

    

### [[2112.03968] Learning Theory Can (Sometimes) Explain Generalisation in Graph Neural Networks](http://arxiv.org/abs/2112.03968)


  In recent years, several results in the supervised learning setting suggested
that classical statistical learning-theoretic measures, such as VC dimension,
do not adequately explain the performance of deep learning models which
prompted a slew of work in the infinite-width and iteration regimes. However,
there is little theoretical explanation for the success of neural networks
beyond the supervised setting. In this paper we argue that, under some
distributional assumptions, classical learning-theoretic measures can
sufficiently explain generalization for graph neural networks in the
transductive setting. In particular, we provide a rigorous analysis of the
performance of neural networks in the context of transductive inference,
specifically by analysing the generalisation properties of graph convolutional
networks for the problem of node classification. While VC Dimension does result
in trivial generalisation error bounds in this setting as well, we show that
transductive Rademacher complexity can explain the generalisation properties of
graph convolutional networks for stochastic block models. We further use the
generalisation error bounds based on transductive Rademacher complexity to
demonstrate the role of graph convolutions and network architectures in
achieving smaller generalisation error and provide insights into when the graph
structure can help in learning. The findings of this paper could re-new the
interest in studying generalisation in neural networks in terms of
learning-theoretic measures, albeit in specific problems.

    

### [[2112.03975] Tailored neural networks for learning optimal value functions in MPC](http://arxiv.org/abs/2112.03975)


  Learning-based predictive control is a promising alternative to
optimization-based MPC. However, efficiently learning the optimal control
policy, the optimal value function, or the Q-function requires suitable
function approximators. Often, artificial neural networks (ANN) are considered
but choosing a suitable topology is also non-trivial. Against this background,
it has recently been shown that tailored ANN allow, in principle, to exactly
describe the optimal control policy in linear MPC by exploiting its piecewise
affine structure. In this paper, we provide a similar result for representing
the optimal value function and the Q-function that are both known to be
piecewise quadratic for linear MPC.

    

### [[2112.04002] SHRIMP: Sparser Random Feature Models via Iterative Magnitude Pruning](http://arxiv.org/abs/2112.04002)


  Sparse shrunk additive models and sparse random feature models have been
developed separately as methods to learn low-order functions, where there are
few interactions between variables, but neither offers computational
efficiency. On the other hand, $\ell_2$-based shrunk additive models are
efficient but do not offer feature selection as the resulting coefficient
vectors are dense. Inspired by the success of the iterative magnitude pruning
technique in finding lottery tickets of neural networks, we propose a new
method -- Sparser Random Feature Models via IMP (ShRIMP) -- to efficiently fit
high-dimensional data with inherent low-dimensional structure in the form of
sparse variable dependencies. Our method can be viewed as a combined process to
construct and find sparse lottery tickets for two-layer dense networks. We
explain the observed benefit of SHRIMP through a refined analysis on the
generalization error for thresholded Basis Pursuit and resulting bounds on
eigenvalues.
From function approximation experiments on both synthetic data and real-world
benchmark datasets, we show that SHRIMP obtains better than or competitive test
accuracy compared to state-of-art sparse feature and additive methods such as
SRFE-S, SSAM, and SALSA. Meanwhile, SHRIMP performs feature selection with low
computational complexity and is robust to the pruning rate, indicating a
robustness in the structure of the obtained subnetworks. We gain insight into
the lottery ticket hypothesis through SHRIMP by noting a correspondence between
our model and weight/neuron subnetworks.

    

### [[2112.04008] Multinational Address Parsing: A Zero-Shot Evaluation](http://arxiv.org/abs/2112.04008)


  Address parsing consists of identifying the segments that make up an address,
such as a street name or a postal code. Because of its importance for tasks
like record linkage, address parsing has been approached with many techniques,
the latest relying on neural networks. While these models yield notable
results, previous work on neural networks has only focused on parsing addresses
from a single source country. This paper explores the possibility of
transferring the address parsing knowledge acquired by training deep learning
models on some countries' addresses to others with no further training in a
zero-shot transfer learning setting. We also experiment using an attention
mechanism and a domain adversarial training algorithm in the same zero-shot
transfer setting to improve performance. Both methods yield state-of-the-art
performance for most of the tested countries while giving good results to the
remaining countries. We also explore the effect of incomplete addresses on our
best model, and we evaluate the impact of using incomplete addresses during
training. In addition, we propose an open-source Python implementation of some
of our trained models.

    

### [[2112.04013] A deep learning model for data-driven discovery of functional connectivity](http://arxiv.org/abs/2112.04013)


  Functional connectivity (FC) studies have demonstrated the overarching value
of studying the brain and its disorders through the undirected weighted graph
of fMRI correlation matrix. Most of the work with the FC, however, depends on
the way the connectivity is computed, and further depends on the manual
post-hoc analysis of the FC matrices. In this work we propose a deep learning
architecture BrainGNN that learns the connectivity structure as part of
learning to classify subjects. It simultaneously applies a graphical neural
network to this learned graph and learns to select a sparse subset of brain
regions important to the prediction task. We demonstrate the model's
state-of-the-art classification performance on a schizophrenia fMRI dataset and
demonstrate how introspection leads to disorder relevant findings. The graphs
learned by the model exhibit strong class discrimination and the sparse subset
of relevant regions are consistent with the schizophrenia literature.

    

### [[2112.04014] Unsupervised Representation Learning via Neural Activation Coding](http://arxiv.org/abs/2112.04014)


  We present neural activation coding (NAC) as a novel approach for learning
deep representations from unlabeled data for downstream applications. We argue
that the deep encoder should maximize its nonlinear expressivity on the data
for downstream predictors to take full advantage of its representation power.
To this end, NAC maximizes the mutual information between activation patterns
of the encoder and the data over a noisy communication channel. We show that
learning for a noise-robust activation code increases the number of distinct
linear regions of ReLU encoders, hence the maximum nonlinear expressivity. More
interestingly, NAC learns both continuous and discrete representations of data,
which we respectively evaluate on two downstream tasks: (i) linear
classification on CIFAR-10 and ImageNet-1K and (ii) nearest neighbor retrieval
on CIFAR-10 and FLICKR-25K. Empirical results show that NAC attains better or
comparable performance on both tasks over recent baselines including SimCLR and
DistillHash. In addition, NAC pretraining provides significant benefits to the
training of deep generative models. Our code is available at
this https URL.

    

### [[2112.04016] DeepFace-EMD: Re-ranking Using Patch-wise Earth Mover's Distance Improves Out-Of-Distribution Face Identification](http://arxiv.org/abs/2112.04016)


  Face identification (FI) is ubiquitous and drives many high-stake decisions
made by law enforcement. State-of-the-art FI approaches compare two images by
taking the cosine similarity between their image embeddings. Yet, such an
approach suffers from poor out-of-distribution (OOD) generalization to new
types of images (e.g., when a query face is masked, cropped, or rotated) not
included in the training set or the gallery. Here, we propose a re-ranking
approach that compares two faces using the Earth Mover's Distance on the deep,
spatial features of image patches. Our extra comparison stage explicitly
examines image similarity at a fine-grained level (e.g., eyes to eyes) and is
more robust to OOD perturbations and occlusions than traditional FI.
Interestingly, without finetuning feature extractors, our method consistently
improves the accuracy on all tested OOD queries: masked, cropped, rotated, and
adversarial while obtaining similar results on in-distribution images.

    

### [[2112.04023] Accelerating Understanding of Scientific Experiments with End to End Symbolic Regression](http://arxiv.org/abs/2112.04023)


  We consider the problem of learning free-form symbolic expressions from raw
data, such as that produced by an experiment in any scientific domain. Accurate
and interpretable models of scientific phenomena are the cornerstone of
scientific research. Simple yet interpretable models, such as linear or
logistic regression and decision trees often lack predictive accuracy.
Alternatively, accurate blackbox models such as deep neural networks provide
high predictive accuracy, but do not readily admit human understanding in a way
that would enrich the scientific theory of the phenomenon. Many great
breakthroughs in science revolve around the development of parsimonious
equational models with high predictive accuracy, such as Newton's laws,
universal gravitation, and Maxwell's equations. Previous work on automating the
search of equational models from data combine domain-specific heuristics as
well as computationally expensive techniques, such as genetic programming and
Monte-Carlo search. We develop a deep neural network (MACSYMA) to address the
symbolic regression problem as an end-to-end supervised learning problem.
MACSYMA can generate symbolic expressions that describe a dataset. The
computational complexity of the task is reduced to the feedforward computation
of a neural network. We train our neural network on a synthetic dataset
consisting of data tables of varying length and varying levels of noise, for
which the neural network must learn to produce the correct symbolic expression
token by token. Finally, we validate our technique by running on a public
dataset from behavioral science.

    

### [[2112.04033] Image classifiers can not be made robust to small perturbations](http://arxiv.org/abs/2112.04033)


  The sensitivity of image classifiers to small perturbations in the input is
often viewed as a defect of their construction. We demonstrate that this
sensitivity is a fundamental property of classifiers. For any arbitrary
classifier over the set of $n$-by-$n$ images, we show that for all but one
class it is possible to change the classification of all but a tiny fraction of
the images in that class with a tiny modification compared to the diameter of
the image space when measured in any $p$-norm, including the hamming distance.
We then examine how this phenomenon manifests in human visual perception and
discuss its implications for the design considerations of computer vision
systems.

    

### [[2112.04035] Relating transformers to models and neural representations of the hippocampal formation](http://arxiv.org/abs/2112.04035)


  Many deep neural network architectures loosely based on brain networks have
recently been shown to replicate neural firing patterns observed in the brain.
One of the most exciting and promising novel architectures, the Transformer
neural network, was developed without the brain in mind. In this work, we show
that transformers, when equipped with recurrent position encodings, replicate
the precisely tuned spatial representations of the hippocampal formation; most
notably place and grid cells. Furthermore, we show that this result is no
surprise since it is closely related to current hippocampal models from
neuroscience. We additionally show the transformer version offers dramatic
performance gains over the neuroscience version. This work continues to bind
computations of artificial and brain networks, offers a novel understanding of
the hippocampal-cortical interaction, and suggests how wider cortical areas may
perform complex tasks beyond current neuroscience models such as language
comprehension.

    

### [[2112.04036] DeepDiagnosis: Automatically Diagnosing Faults and Recommending Actionable Fixes in Deep Learning Programs](http://arxiv.org/abs/2112.04036)


  Deep Neural Networks (DNNs) are used in a wide variety of applications.
However, as in any software application, DNN-based apps are afflicted with
bugs. Previous work observed that DNN bug fix patterns are different from
traditional bug fix patterns. Furthermore, those buggy models are non-trivial
to diagnose and fix due to inexplicit errors with several options to fix them.
To support developers in locating and fixing bugs, we propose DeepDiagnosis, a
novel debugging approach that localizes the faults, reports error symptoms and
suggests fixes for DNN programs. In the first phase, our technique monitors a
training model, periodically checking for eight types of error conditions.
Then, in case of problems, it reports messages containing sufficient
information to perform actionable repairs to the model. In the evaluation, we
thoroughly examine 444 models -53 real-world from GitHub and Stack Overflow,
and 391 curated by AUTOTRAINER. DeepDiagnosis provides superior accuracy when
compared to UMLUAT and DeepLocalize. Our technique is faster than AUTOTRAINER
for fault localization. The results show that our approach can support
additional types of models, while state-of-the-art was only able to handle
classification ones. Our technique was able to report bugs that do not manifest
as numerical errors during training. Also, it can provide actionable insights
for fix whereas DeepLocalize can only report faults that lead to numerical
errors during training. DeepDiagnosis manifests the best capabilities of fault
detection, bug localization, and symptoms identification when compared to other
approaches.

    

### [[2112.04041] A Transferable Approach for Partitioning Machine Learning Models on Multi-Chip-Modules](http://arxiv.org/abs/2112.04041)


  Multi-Chip-Modules (MCMs) reduce the design and fabrication cost of machine
learning (ML) accelerators while delivering performance and energy efficiency
on par with a monolithic large chip. However, ML compilers targeting MCMs need
to solve complex optimization problems optimally and efficiently to achieve
this high performance. One such problem is the multi-chip partitioning problem
where compilers determine the optimal partitioning and placement of operations
in tensor computation graphs on chiplets in MCMs. Partitioning ML graphs for
MCMs is particularly hard as the search space grows exponentially with the
number of chiplets available and the number of nodes in the neural network.
Furthermore, the constraints imposed by the underlying hardware produce a
search space where valid solutions are extremely sparse. In this paper, we
present a strategy using a deep reinforcement learning (RL) framework to emit a
possibly invalid candidate partition that is then corrected by a constraint
solver. Using the constraint solver ensures that RL encounters valid solutions
in the sparse space frequently enough to converge with fewer samples as
compared to non-learned strategies. The architectural choices we make for the
policy network allow us to generalize across different ML graphs. Our
evaluation of a production-scale model, BERT, on real hardware reveals that the
partitioning generated using RL policy achieves 6.11% and 5.85% higher
throughput than random search and simulated annealing. In addition, fine-tuning
the pre-trained RL policy reduces the search time from 3 hours to only 9
minutes, while achieving the same throughput as training RL policy from
scratch.

    

### [[2112.04042] Vision-Cloud Data Fusion for ADAS: A Lane Change Prediction Case Study](http://arxiv.org/abs/2112.04042)


  With the rapid development of intelligent vehicles and Advanced
Driver-Assistance Systems (ADAS), a new trend is that mixed levels of human
driver engagements will be involved in the transportation system. Therefore,
necessary visual guidance for drivers is vitally important under this situation
to prevent potential risks. To advance the development of visual guidance
systems, we introduce a novel vision-cloud data fusion methodology, integrating
camera image and Digital Twin information from the cloud to help intelligent
vehicles make better decisions. Target vehicle bounding box is drawn and
matched with the help of the object detector (running on the ego-vehicle) and
position information (received from the cloud). The best matching result, a
79.2% accuracy under 0.7 intersection over union threshold, is obtained with
depth images served as an additional feature source. A case study on lane
change prediction is conducted to show the effectiveness of the proposed data
fusion methodology. In the case study, a multi-layer perceptron algorithm is
proposed with modified lane change prediction approaches. Human-in-the-loop
simulation results obtained from the Unity game engine reveal that the proposed
model can improve highway driving performance significantly in terms of safety,
comfort, and environmental sustainability.

    

### [[2112.04075] Active Sensing for Communications by Learning](http://arxiv.org/abs/2112.04075)


  This paper proposes a deep learning approach to a class of active sensing
problems in wireless communications in which an agent sequentially interacts
with an environment over a predetermined number of time frames to gather
information in order to perform a sensing or actuation task for maximizing some
utility function. In such an active learning setting, the agent needs to design
an adaptive sensing strategy sequentially based on the observations made so
far. To tackle such a challenging problem in which the dimension of historical
observations increases over time, we propose to use a long short-term memory
(LSTM) network to exploit the temporal correlations in the sequence of
observations and to map each observation to a fixed-size state information
vector. We then use a deep neural network (DNN) to map the LSTM state at each
time frame to the design of the next measurement step. Finally, we employ
another DNN to map the final LSTM state to the desired solution. We investigate
the performance of the proposed framework for adaptive channel sensing problems
in wireless communications. In particular, we consider the adaptive beamforming
problem for mmWave beam alignment and the adaptive reconfigurable intelligent
surface sensing problem for reflection alignment. Numerical results demonstrate
that the proposed deep active sensing strategy outperforms the existing
adaptive or nonadaptive sensing schemes.

    

### [[2112.04083] Best Arm Identification under Additive Transfer Bandits](http://arxiv.org/abs/2112.04083)


  We consider a variant of the best arm identification (BAI) problem in
multi-armed bandits (MAB) in which there are two sets of arms (source and
target), and the objective is to determine the best target arm while only
pulling source arms. In this paper, we study the setting when, despite the
means being unknown, there is a known additive relationship between the source
and target MAB instances. We show how our framework covers a range of
previously studied pure exploration problems and additionally captures new
problems. We propose and theoretically analyze an LUCB-style algorithm to
identify an $\epsilon$-optimal target arm with high probability. Our
theoretical analysis highlights aspects of this transfer learning problem that
do not arise in the typical BAI setup, and yet recover the LUCB algorithm for
single domain BAI as a special case.

    

### [[2112.04084] Hyper-parameter optimization based on soft actor critic and hierarchical mixture regularization](http://arxiv.org/abs/2112.04084)


  Hyper-parameter optimization is a crucial problem in machine learning as it
aims to achieve the state-of-the-art performance in any model. Great efforts
have been made in this field, such as random search, grid search, Bayesian
optimization. In this paper, we model hyper-parameter optimization process as a
Markov decision process, and tackle it with reinforcement learning. A novel
hyper-parameter optimization method based on soft actor critic and hierarchical
mixture regularization has been proposed. Experiments show that the proposed
method can obtain better hyper-parameters in a shorter time.

    

### [[2112.04085] KoopmanizingFlows: Diffeomorphically Learning Stable Koopman Operators](http://arxiv.org/abs/2112.04085)


  We propose a novel framework for constructing linear time-invariant (LTI)
models for data-driven representations of the Koopman operator for a class of
stable nonlinear dynamics. The Koopman operator (generator) lifts a
finite-dimensional nonlinear system to a possibly infinite-dimensional linear
feature space. To utilize it for modeling, one needs to discover
finite-dimensional representations of the Koopman operator. Learning suitable
features is challenging, as one needs to learn LTI features that are both
Koopman-invariant (evolve linearly under the dynamics) as well as relevant
(spanning the original state) - a generally unsupervised learning task. For a
theoretically well-founded solution to this problem, we propose learning
Koopman-invariant coordinates by composing a diffeomorphic learner with a
lifted aggregate system of a latent linear model. Using an unconstrained
parameterization of stable matrices along with the aforementioned feature
construction, we learn the Koopman operator features without assuming a
predefined library of functions or knowing the spectrum, while ensuring
stability regardless of the operator approximation accuracy. We demonstrate the
superior efficacy of the proposed method in comparison to a state-of-the-art
method on the well-known LASA handwriting dataset.

    

### [[2112.04094] The Effect of Model Size on Worst-Group Generalization](http://arxiv.org/abs/2112.04094)


  Overparameterization is shown to result in poor test accuracy on rare
subgroups under a variety of settings where subgroup information is known. To
gain a more complete picture, we consider the case where subgroup information
is unknown. We investigate the effect of model size on worst-group
generalization under empirical risk minimization (ERM) across a wide range of
settings, varying: 1) architectures (ResNet, VGG, or BERT), 2) domains (vision
or natural language processing), 3) model size (width or depth), and 4)
initialization (with pre-trained or random weights). Our systematic evaluation
reveals that increasing model size does not hurt, and may help, worst-group
test performance under ERM across all setups. In particular, increasing
pre-trained model size consistently improves performance on Waterbirds and
MultiNLI. We advise practitioners to use larger pre-trained models when
subgroup labels are unknown.

    

### [[2112.04100] Uncovering the Local Hidden Community Structure in Social Networks](http://arxiv.org/abs/2112.04100)


  Hidden community is a useful concept proposed recently for social network
analysis. To handle the rapid growth of network scale, in this work, we explore
the detection of hidden communities from the local perspective, and propose a
new method that detects and boosts each layer iteratively on a subgraph sampled
from the original network. We first expand the seed set from a single seed node
based on our modified local spectral method and detect an initial dominant
local community. Then we temporarily remove the members of this community as
well as their connections to other nodes, and detect all the neighborhood
communities in the remaining subgraph, including some "broken communities" that
only contain a fraction of members in the original network. The local community
and neighborhood communities form a dominant layer, and by reducing the edge
weights inside these communities, we weaken this layer's structure to reveal
the hidden layers. Eventually, we repeat the whole process and all communities
containing the seed node can be detected and boosted iteratively. We
theoretically show that our method can avoid some situations that a broken
community and the local community are regarded as one community in the
subgraph, leading to the inaccuracy on detection which can be caused by global
hidden community detection methods. Extensive experiments show that our method
could significantly outperform the state-of-the-art baselines designed for
either global hidden community detection or multiple local community detection.

    

### [[2112.04101] Learning Linear Models Using Distributed Iterative Hessian Sketching](http://arxiv.org/abs/2112.04101)


  This work considers the problem of learning the Markov parameters of a linear
system from observed data. Recent non-asymptotic system identification results
have characterized the sample complexity of this problem in the single and
multi-rollout setting. In both instances, the number of samples required in
order to obtain acceptable estimates can produce optimization problems with an
intractably large number of decision variables for a second-order algorithm. We
show that a randomized and distributed Newton algorithm based on
Hessian-sketching can produce $\epsilon$-optimal solutions and converges
geometrically. Moreover, the algorithm is trivially parallelizable. Our results
hold for a variety of sketching matrices and we illustrate the theory with
numerical examples.

    

### [[2112.04123] ShinRL: A Library for Evaluating RL Algorithms from Theoretical and Practical Perspectives](http://arxiv.org/abs/2112.04123)


  We present ShinRL, an open-source library specialized for the evaluation of
reinforcement learning (RL) algorithms from both theoretical and practical
perspectives. Existing RL libraries typically allow users to evaluate practical
performances of deep RL algorithms through returns. Nevertheless, these
libraries are not necessarily useful for analyzing if the algorithms perform as
theoretically expected, such as if Q learning really achieves the optimal Q
function. In contrast, ShinRL provides an RL environment interface that can
compute metrics for delving into the behaviors of RL algorithms, such as the
gap between learned and the optimal Q values and state visitation frequencies.
In addition, we introduce a flexible solver interface for evaluating both
theoretically justified algorithms (e.g., dynamic programming and tabular RL)
and practically effective ones (i.e., deep RL, typically with some additional
extensions and regularizations) in a consistent fashion. As a case study, we
show that how combining these two features of ShinRL makes it easier to analyze
the behavior of deep Q learning. Furthermore, we demonstrate that ShinRL can be
used to empirically validate recent theoretical findings such as the effect of
KL regularization for value iteration and for deep Q learning, and the
robustness of entropy-regularized policies to adversarial rewards. The source
code for ShinRL is available on GitHub: this https URL.

    

### [[2112.04137] Pareto Domain Adaptation](http://arxiv.org/abs/2112.04137)


  Domain adaptation (DA) attempts to transfer the knowledge from a labeled
source domain to an unlabeled target domain that follows different distribution
from the source. To achieve this, DA methods include a source classification
objective to extract the source knowledge and a domain alignment objective to
diminish the domain shift, ensuring knowledge transfer. Typically, former DA
methods adopt some weight hyper-parameters to linearly combine the training
objectives to form an overall objective. However, the gradient directions of
these objectives may conflict with each other due to domain shift. Under such
circumstances, the linear optimization scheme might decrease the overall
objective value at the expense of damaging one of the training objectives,
leading to restricted solutions. In this paper, we rethink the optimization
scheme for DA from a gradient-based perspective. We propose a Pareto Domain
Adaptation (ParetoDA) approach to control the overall optimization direction,
aiming to cooperatively optimize all training objectives. Specifically, to
reach a desirable solution on the target domain, we design a surrogate loss
mimicking target classification. To improve target-prediction accuracy to
support the mimicking, we propose a target-prediction refining mechanism which
exploits domain labels via Bayes' theorem. On the other hand, since prior
knowledge of weighting schemes for objectives is often unavailable to guide
optimization to approach the optimal solution on the target domain, we propose
a dynamic preference mechanism to dynamically guide our cooperative
optimization by the gradient of the surrogate loss on a held-out unlabeled
target dataset. Extensive experiments on image classification and semantic
segmentation benchmarks demonstrate the effectiveness of ParetoDA

    

### [[2112.04138] Contrastive Instruction-Trajectory Learning for Vision-Language Navigation](http://arxiv.org/abs/2112.04138)


  The vision-language navigation (VLN) task requires an agent to reach a target
with the guidance of natural language instruction. Previous works learn to
navigate step-by-step following an instruction. However, these works may fail
to discriminate the similarities and discrepancies across
instruction-trajectory pairs and ignore the temporal continuity of
sub-instructions. These problems hinder agents from learning distinctive
vision-and-language representations, harming the robustness and
generalizability of the navigation policy. In this paper, we propose a
Contrastive Instruction-Trajectory Learning (CITL) framework that explores
invariance across similar data samples and variance across different ones to
learn distinctive representations for robust navigation. Specifically, we
propose: (1) a coarse-grained contrastive learning objective to enhance
vision-and-language representations by contrasting semantics of full trajectory
observations and instructions, respectively; (2) a fine-grained contrastive
learning objective to perceive instructions by leveraging the temporal
information of the sub-instructions; (3) a pairwise sample-reweighting
mechanism for contrastive learning to mine hard samples and hence mitigate the
influence of data sampling bias in contrastive learning. Our CITL can be easily
integrated with VLN backbones to form a new learning paradigm and achieve
better generalizability in unseen environments. Extensive experiments show that
the model with CITL surpasses the previous state-of-the-art methods on R2R,
R4R, and RxR.

    

### [[2112.04153] Model-Value Inconsistency as a Signal for Epistemic Uncertainty](http://arxiv.org/abs/2112.04153)


  Using a model of the environment and a value function, an agent can construct
many estimates of a state's value, by unrolling the model for different lengths
and bootstrapping with its value function. Our key insight is that one can
treat this set of value estimates as a type of ensemble, which we call an
\emph{implicit value ensemble} (IVE). Consequently, the discrepancy between
these estimates can be used as a proxy for the agent's epistemic uncertainty;
we term this signal \emph{model-value inconsistency} or
\emph{self-inconsistency} for short. Unlike prior work which estimates
uncertainty by training an ensemble of many models and/or value functions, this
approach requires only the single model and value function which are already
being learned in most model-based reinforcement learning algorithms. We provide
empirical evidence in both tabular and function approximation settings from
pixels that self-inconsistency is useful (i) as a signal for exploration, (ii)
for acting safely under distribution shifts, and (iii) for robustifying
value-based planning with a model.

    

### [[2112.04165] Shortest Paths in Graphs with Matrix-Valued Edges: Concepts, Algorithm and Application to 3D Multi-Shape Analysis](http://arxiv.org/abs/2112.04165)


  Finding shortest paths in a graph is relevant for numerous problems in
computer vision and graphics, including image segmentation, shape matching, or
the computation of geodesic distances on discrete surfaces. Traditionally, the
concept of a shortest path is considered for graphs with scalar edge weights,
which makes it possible to compute the length of a path by adding up the
individual edge weights. Yet, graphs with scalar edge weights are severely
limited in their expressivity, since oftentimes edges are used to encode
significantly more complex interrelations. In this work we compensate for this
modelling limitation and introduce the novel graph-theoretic concept of a
shortest path in a graph with matrix-valued edges. To this end, we define a
meaningful way for quantifying the path length for matrix-valued edges, and we
propose a simple yet effective algorithm to compute the respective shortest
path. While our formalism is universal and thus applicable to a wide range of
settings in vision, graphics and beyond, we focus on demonstrating its merits
in the context of 3D multi-shape analysis.

    

### [[2112.04185] Transformaly -- Two (Feature Spaces) Are Better Than One](http://arxiv.org/abs/2112.04185)


  Anomaly detection is a well-established research area that seeks to identify
samples outside of a predetermined distribution. An anomaly detection pipeline
is comprised of two main stages: (1) feature extraction and (2) normality score
assignment. Recent papers used pre-trained networks for feature extraction
achieving state-of-the-art results. However, the use of pre-trained networks
does not fully-utilize the normal samples that are available at train time.
This paper suggests taking advantage of this information by using
teacher-student training. In our setting, a pretrained teacher network is used
to train a student network on the normal training samples. Since the student
network is trained only on normal samples, it is expected to deviate from the
teacher network in abnormal cases. This difference can serve as a complementary
representation to the pre-trained feature vector. Our method -- Transformaly --
exploits a pre-trained Vision Transformer (ViT) to extract both feature
vectors: the pre-trained (agnostic) features and the teacher-student
(fine-tuned) features. We report state-of-the-art AUROC results in both the
common unimodal setting, where one class is considered normal and the rest are
considered abnormal, and the multimodal setting, where all classes but one are
considered normal, and just one class is considered abnormal. The code is
available at this https URL.

    

### [[2112.04187] Pretrained Cost Model for Distributed Constraint Optimization Problems](http://arxiv.org/abs/2112.04187)


  Distributed Constraint Optimization Problems (DCOPs) are an important
subclass of combinatorial optimization problems, where information and controls
are distributed among multiple autonomous agents. Previously, Machine Learning
(ML) has been largely applied to solve combinatorial optimization problems by
learning effective heuristics. However, existing ML-based heuristic methods are
often not generalizable to different search algorithms. Most importantly, these
methods usually require full knowledge about the problems to be solved, which
are not suitable for distributed settings where centralization is not realistic
due to geographical limitations or privacy concerns. To address the generality
issue, we propose a novel directed acyclic graph representation schema for
DCOPs and leverage the Graph Attention Networks (GATs) to embed graph
representations. Our model, GAT-PCM, is then pretrained with optimally labelled
data in an offline manner, so as to construct effective heuristics to boost a
broad range of DCOP algorithms where evaluating the quality of a partial
assignment is critical, such as local search or backtracking search.
Furthermore, to enable decentralized model inference, we propose a distributed
embedding schema of GAT-PCM where each agent exchanges only embedded vectors,
and show its soundness and complexity. Finally, we demonstrate the
effectiveness of our model by combining it with a local search or a
backtracking search algorithm. Extensive empirical evaluations indicate that
the GAT-PCM-boosted algorithms significantly outperform the state-of-the-art
methods in various benchmarks. The pretrained model is available at
this https URL.

    

### [[2112.04193] Learnable Faster Kernel-PCA for Nonlinear Fault Detection: Deep Autoencoder-Based Realization](http://arxiv.org/abs/2112.04193)


  Kernel principal component analysis (KPCA) is a well-recognized nonlinear
dimensionality reduction method that has been widely used in nonlinear fault
detection tasks. As a kernel trick-based method, KPCA inherits two major
problems. First, the form and the parameters of the kernel function are usually
selected blindly, depending seriously on trial-and-error. As a result, there
may be serious performance degradation in case of inappropriate selections.
Second, at the online monitoring stage, KPCA has much computational burden and
poor real-time performance, because the kernel method requires to leverage all
the offline training data. In this work, to deal with the two drawbacks, a
learnable faster realization of the conventional KPCA is proposed. The core
idea is to parameterize all feasible kernel functions using the novel nonlinear
DAE-FE (deep autoencoder based feature extraction) framework and propose
DAE-PCA (deep autoencoder based principal component analysis) approach in
detail. The proposed DAE-PCA method is proved to be equivalent to KPCA but has
more advantage in terms of automatic searching of the most suitable nonlinear
high-dimensional space according to the inputs. Furthermore, the online
computational efficiency improves by approximately 100 times compared with the
conventional KPCA. With the Tennessee Eastman (TE) process benchmark, the
effectiveness and superiority of the proposed method is illustrated.

    

### [[2112.04197] A Fast Algorithm for PAC Combinatorial Pure Exploration](http://arxiv.org/abs/2112.04197)


  We consider the problem of Combinatorial Pure Exploration (CPE), which deals
with finding a combinatorial set or arms with a high reward, when the rewards
of individual arms are unknown in advance and must be estimated using arm
pulls. Previous algorithms for this problem, while obtaining sample complexity
reductions in many cases, are highly computationally intensive, thus making
them impractical even for mildly large problems. In this work, we propose a new
CPE algorithm in the PAC setting, which is computationally light weight, and so
can easily be applied to problems with tens of thousands of arms. This is
achieved since the proposed algorithm requires a very small number of
combinatorial oracle calls. The algorithm is based on successive acceptance of
arms, along with elimination which is based on the combinatorial structure of
the problem. We provide sample complexity guarantees for our algorithm, and
demonstrate in experiments its usefulness on large problems, whereas previous
algorithms are impractical to run on problems of even a few dozen arms. The
code for the algorithms and experiments is provided at
this https URL.

    

### [[2112.04213] Convergence Results For Q-Learning With Experience Replay](http://arxiv.org/abs/2112.04213)


  A commonly used heuristic in RL is experience replay
(e.g.~\citet{lin1993reinforcement, mnih2015human}), in which a learner stores
and re-uses past trajectories as if they were sampled online. In this work, we
initiate a rigorous study of this heuristic in the setting of tabular
Q-learning. We provide a convergence rate guarantee, and discuss how it
compares to the convergence of Q-learning depending on important parameters
such as the frequency and number of replay iterations. We also provide
theoretical evidence showing when we might expect this heuristic to strictly
improve performance, by introducing and analyzing a simple class of MDPs.
Finally, we provide some experiments to support our theoretical findings.

    

### [[2112.04214] Learning music audio representations via weak language supervision](http://arxiv.org/abs/2112.04214)


  Audio representations for music information retrieval are typically learned
via supervised learning in a task-specific fashion. Although effective at
producing state-of-the-art results, this scheme lacks flexibility with respect
to the range of applications a model can have and requires extensively
annotated datasets. In this work, we pose the question of whether it may be
possible to exploit weakly aligned text as the only supervisory signal to learn
general-purpose music audio representations. To address this question, we
design a multimodal architecture for music and language pre-training (MuLaP)
optimised via a set of proxy tasks. Weak supervision is provided in the form of
noisy natural language descriptions conveying the overall musical content of
the track. After pre-training, we transfer the audio backbone of the model to a
set of music audio classification and regression tasks. We demonstrate the
usefulness of our approach by comparing the performance of audio
representations produced by the same audio backbone with different training
strategies and show that our pre-training method consistently achieves
comparable or higher scores on all tasks and datasets considered. Our
experiments also confirm that MuLaP effectively leverages audio-caption pairs
to learn representations that are competitive with audio-only and cross-modal
self-supervised methods in the literature.

    

### [[2112.04215] Self-Supervised Models are Continual Learners](http://arxiv.org/abs/2112.04215)


  Self-supervised models have been shown to produce comparable or better visual
representations than their supervised counterparts when trained offline on
unlabeled data at scale. However, their efficacy is catastrophically reduced in
a Continual Learning (CL) scenario where data is presented to the model
sequentially. In this paper, we show that self-supervised loss functions can be
seamlessly converted into distillation mechanisms for CL by adding a predictor
network that maps the current state of the representations to their past state.
This enables us to devise a framework for Continual self-supervised visual
representation Learning that (i) significantly improves the quality of the
learned representations, (ii) is compatible with several state-of-the-art
self-supervised objectives, and (iii) needs little to no hyperparameter tuning.
We demonstrate the effectiveness of our approach empirically by training six
popular self-supervised models in various CL settings.

    

### [[2112.04216] Specializing Versatile Skill Libraries using Local Mixture of Experts](http://arxiv.org/abs/2112.04216)


  A long-cherished vision in robotics is to equip robots with skills that match
the versatility and precision of humans. For example, when playing table
tennis, a robot should be capable of returning the ball in various ways while
precisely placing it at the desired location. A common approach to model such
versatile behavior is to use a Mixture of Experts (MoE) model, where each
expert is a contextual motion primitive. However, learning such MoEs is
challenging as most objectives force the model to cover the entire context
space, which prevents specialization of the primitives resulting in rather
low-quality components. Starting from maximum entropy reinforcement learning
(RL), we decompose the objective into optimizing an individual lower bound per
mixture component. Further, we introduce a curriculum by allowing the
components to focus on a local context region, enabling the model to learn
highly accurate skill representations. To this end, we use local context
distributions that are adapted jointly with the expert primitives. Our lower
bound advocates an iterative addition of new components, where new components
will concentrate on local context regions not covered by the current MoE. This
local and incremental learning results in a modular MoE model of high accuracy
and versatility, where both properties can be scaled by adding more components
on the fly. We demonstrate this by an extensive ablation and on two challenging
simulated robot skill learning tasks. We compare our achieved performance to
LaDiPS and HiREPS, a known hierarchical policy search method for learning
diverse skills.

    

### [[2112.04219] Learning over All Stabilizing Nonlinear Controllers for a Partially-Observed Linear System](http://arxiv.org/abs/2112.04219)


  We propose a parameterization of nonlinear output feedback controllers for
linear dynamical systems based on a recently developed class of neural network
called the recurrent equilibrium network (REN), and a nonlinear version of the
Youla parameterization. Our approach guarantees the closed-loop stability of
partially observable linear dynamical systems without requiring any constraints
to be satisfied. This significantly simplifies model fitting as any
unconstrained optimization procedure can be applied whilst still maintaining
stability. We demonstrate our method on reinforcement learning tasks with both
exact and approximate gradient methods. Simulation studies show that our method
is significantly more scalable and significantly outperforms other approaches
in the same problem setting.

    

### [[2112.04223] Progressive Multi-stage Interactive Training in Mobile Network for Fine-grained Recognition](http://arxiv.org/abs/2112.04223)


  Fine-grained Visual Classification (FGVC) aims to identify objects from
subcategories. It is a very challenging task because of the subtle inter-class
differences. Existing research applies large-scale convolutional neural
networks or visual transformers as the feature extractor, which is extremely
computationally expensive. In fact, real-world scenarios of fine-grained
recognition often require a more lightweight mobile network that can be
utilized offline. However, the fundamental mobile network feature extraction
capability is weaker than large-scale models. In this paper, based on the
lightweight MobilenetV2, we propose a Progressive Multi-Stage Interactive
training method with a Recursive Mosaic Generator (RMG-PMSI). First, we propose
a Recursive Mosaic Generator (RMG) that generates images with different
granularities in different phases. Then, the features of different stages pass
through a Multi-Stage Interaction (MSI) module, which strengthens and
complements the corresponding features of different stages. Finally, using the
progressive training (P), the features extracted by the model in different
stages can be fully utilized and fused with each other. Experiments on three
prestigious fine-grained benchmarks show that RMG-PMSI can significantly
improve the performance with good robustness and transferability.

    

### [[2112.04229] Replay For Safety](http://arxiv.org/abs/2112.04229)


  Experience replay \citep{lin1993reinforcement, mnih2015human} is a widely
used technique to achieve efficient use of data and improved performance in RL
algorithms. In experience replay, past transitions are stored in a memory
buffer and re-used during learning. Various suggestions for sampling schemes
from the replay buffer have been suggested in previous works, attempting to
optimally choose those experiences which will most contribute to the
convergence to an optimal policy. Here, we give some conditions on the replay
sampling scheme that will ensure convergence, focusing on the well-known
Q-learning algorithm in the tabular setting. After establishing sufficient
conditions for convergence, we turn to suggest a slightly different usage for
experience replay - replaying memories in a biased manner as a means to change
the properties of the resulting policy. We initiate a rigorous study of
experience replay as a tool to control and modify the properties of the
resulting policy. In particular, we show that using an appropriate biased
sampling scheme can allow us to achieve a \emph{safe} policy. We believe that
using experience replay as a biasing mechanism that allows controlling the
resulting policy in desirable ways is an idea with promising potential for many
applications.

    

### [[2112.04236] Application of Deep Reinforcement Learning to Payment Fraud](http://arxiv.org/abs/2112.04236)


  The large variety of digital payment choices available to consumers today has
been a key driver of e-commerce transactions in the past decade. Unfortunately,
this has also given rise to cybercriminals and fraudsters who are constantly
looking for vulnerabilities in these systems by deploying increasingly
sophisticated fraud attacks. A typical fraud detection system employs standard
supervised learning methods where the focus is on maximizing the fraud recall
rate. However, we argue that such a formulation can lead to sub-optimal
solutions. The design requirements for these fraud models requires that they
are robust to the high-class imbalance in the data, adaptive to changes in
fraud patterns, maintain a balance between the fraud rate and the decline rate
to maximize revenue, and be amenable to asynchronous feedback since usually
there is a significant lag between the transaction and the fraud realization.
To achieve this, we formulate fraud detection as a sequential decision-making
problem by including the utility maximization within the model in the form of
the reward function. The historical decline rate and fraud rate define the
state of the system with a binary action space composed of approving or
declining the transaction. In this study, we primarily focus on utility
maximization and explore different reward functions to this end. The
performance of the proposed Reinforcement Learning system has been evaluated
for two publicly available fraud datasets using Deep Q-learning and compared
with different classifiers. We aim to address the rest of the issues in future
work.

    

### [[2112.04261] Efficient Batch Homomorphic Encryption for Vertically Federated XGBoost](http://arxiv.org/abs/2112.04261)


  More and more orgainizations and institutions make efforts on using external
data to improve the performance of AI services. To address the data privacy and
security concerns, federated learning has attracted increasing attention from
both academia and industry to securely construct AI models across multiple
isolated data providers. In this paper, we studied the efficiency problem of
adapting widely used XGBoost model in real-world applications to vertical
federated learning setting. State-of-the-art vertical federated XGBoost
frameworks requires large number of encryption operations and ciphertext
transmissions, which makes the model training much less efficient than training
XGBoost models locally. To bridge this gap, we proposed a novel batch
homomorphic encryption method to cut the cost of encryption-related computation
and transmission in nearly half. This is achieved by encoding the first-order
derivative and the second-order derivative into a single number for encryption,
ciphertext transmission, and homomorphic addition operations. The sum of
multiple first-order derivatives and second-order derivatives can be
simultaneously decoded from the sum of encoded values. We are motivated by the
batch idea in the work of BatchCrypt for horizontal federated learning, and
design a novel batch method to address the limitations of allowing quite few
number of negative numbers. The encode procedure of the proposed batch method
consists of four steps, including shifting, truncating, quantizing and
batching, while the decoding procedure consists of de-quantization and shifting
back. The advantages of our method are demonstrated through theoretical
analysis and extensive numerical experiments.

    

### [[2112.04267] Implicit Neural Representations for Image Compression](http://arxiv.org/abs/2112.04267)


  Recently Implicit Neural Representations (INRs) gained attention as a novel
and effective representation for various data types. Thus far, prior work
mostly focused on optimizing their reconstruction performance. This work
investigates INRs from a novel perspective, i.e., as a tool for image
compression. To this end, we propose the first comprehensive compression
pipeline based on INRs including quantization, quantization-aware retraining
and entropy coding. Encoding with INRs, i.e. overfitting to a data sample, is
typically orders of magnitude slower. To mitigate this drawback, we leverage
meta-learned initializations based on MAML to reach the encoding in fewer
gradient updates which also generally improves rate-distortion performance of
INRs. We find that our approach to source compression with INRs vastly
outperforms similar prior work, is competitive with common compression
algorithms designed specifically for images and closes the gap to
state-of-the-art learned approaches based on Rate-Distortion Autoencoders.
Moreover, we provide an extensive ablation study on the importance of
individual components of our method which we hope facilitates future research
on this novel approach to image compression.

    

### [[2112.04274] On the Use of Unrealistic Predictions in Hundreds of Papers Evaluating Graph Representations](http://arxiv.org/abs/2112.04274)


  Prediction using the ground truth sounds like an oxymoron in machine
learning. However, such an unrealistic setting was used in hundreds, if not
thousands of papers in the area of finding graph representations. To evaluate
the multi-label problem of node classification by using the obtained
representations, many works assume in the prediction stage that the number of
labels of each test instance is known. In practice such ground truth
information is rarely available, but we point out that such an inappropriate
setting is now ubiquitous in this research area. We detailedly investigate why
the situation occurs. Our analysis indicates that with unrealistic information,
the performance is likely over-estimated. To see why suitable predictions were
not used, we identify difficulties in applying some multi-label techniques. For
the use in future studies, we propose simple and effective settings without
using practically unknown information. Finally, we take this chance to conduct
a fair and serious comparison of major graph-representation learning methods on
multi-label node classification.

    

### [[2112.04282] Radar Occupancy Prediction with Lidar Supervision while Preserving Long-Range Sensing and Penetrating Capabilities](http://arxiv.org/abs/2112.04282)


  Radar shows great potential for autonomous driving by accomplishing
long-range sensing under diverse weather conditions. But radar is also a
particularly challenging sensing modality due to the radar noises. Recent works
have made enormous progress in classifying free and occupied spaces in radar
images by leveraging lidar label supervision. However, there are still several
unsolved issues. Firstly, the sensing distance of the results is limited by the
sensing range of lidar. Secondly, the performance of the results is degenerated
by lidar due to the physical sensing discrepancies between the two sensors. For
example, some objects visible to lidar are invisible to radar, and some objects
occluded in lidar scans are visible in radar images because of the radar's
penetrating capability. These sensing differences cause false positive and
penetrating capability degeneration, respectively.
In this paper, we propose training data preprocessing and polar sliding
window inference to solve the issues. The data preprocessing aims to reduce the
effect caused by radar-invisible measurements in lidar scans. The polar sliding
window inference aims to solve the limited sensing range issue by applying a
near-range trained network to the long-range region. Instead of using common
Cartesian representation, we propose to use polar representation to reduce the
shape dissimilarity between long-range and near-range data. We find that
extending a near-range trained network to long-range region inference in the
polar space has 4.2 times better IoU than in Cartesian space. Besides, the
polar sliding window inference can preserve the radar penetrating capability by
changing the viewpoint of the inference region, which makes some occluded
measurements seem non-occluded for a pretrained network.

    

### [[2112.04288] Non parametric estimation of causal populations in a counterfactual scenario](http://arxiv.org/abs/2112.04288)


  In causality, estimating the effect of a treatment without confounding
inference remains a major issue because requires to assess the outcome in both
case with and without treatment. Not being able to observe simultaneously both
of them, the estimation of potential outcome remains a challenging task. We
propose an innovative approach where the problem is reformulated as a missing
data model. The aim is to estimate the hidden distribution of \emph{causal
populations}, defined as a function of treatment and outcome. A Causal
Auto-Encoder (CAE), enhanced by a prior dependent on treatment and outcome
information, assimilates the latent space to the probability distribution of
the target populations. The features are reconstructed after being reduced to a
latent space and constrained by a mask introduced in the intermediate layer of
the network, containing treatment and outcome information.

    

### [[2112.04291] FastSGD: A Fast Compressed SGD Framework for Distributed Machine Learning](http://arxiv.org/abs/2112.04291)


  With the rapid increase of big data, distributed Machine Learning (ML) has
been widely applied in training large-scale models. Stochastic Gradient Descent
(SGD) is arguably the workhorse algorithm of ML. Distributed ML models trained
by SGD involve large amounts of gradient communication, which limits the
scalability of distributed ML. Thus, it is important to compress the gradients
for reducing communication. In this paper, we propose FastSGD, a Fast
compressed SGD framework for distributed ML. To achieve a high compression
ratio at a low cost, FastSGD represents the gradients as key-value pairs, and
compresses both the gradient keys and values in linear time complexity. For the
gradient value compression, FastSGD first uses a reciprocal mapper to transform
original values into reciprocal values, and then, it utilizes a logarithm
quantization to further reduce reciprocal values to small integers. Finally,
FastSGD filters reduced gradient integers by a given threshold. For the
gradient key compression, FastSGD provides an adaptive fine-grained delta
encoding method to store gradient keys with fewer bits. Extensive experiments
on practical ML models and datasets demonstrate that FastSGD achieves the
compression ratio up to 4 orders of magnitude, and accelerates the convergence
time up to 8x, compared with state-of-the-art methods.

    

### [[2112.04298] GCA-Net : Utilizing Gated Context Attention for Improving Image Forgery Localization and Detection](http://arxiv.org/abs/2112.04298)


  Forensic analysis depends on the identification of hidden traces from
manipulated images. Traditional neural networks fail in this task because of
their inability in handling feature attenuation and reliance on the dominant
spatial features. In this work we propose a novel Gated Context Attention
Network (GCA-Net) that utilizes the non-local attention block for global
context learning. Additionally, we utilize a gated attention mechanism in
conjunction with a dense decoder network to direct the flow of relevant
features during the decoding phase, allowing for precise localization. The
proposed attention framework allows the network to focus on relevant regions by
filtering the coarse features. Furthermore, by utilizing multi-scale feature
fusion and efficient learning strategies, GCA-Net can better handle the scale
variation of manipulated regions. We show that our method outperforms
state-of-the-art networks by an average of 4.2%-5.4% AUC on multiple benchmark
datasets. Lastly, we also conduct extensive ablation experiments to demonstrate
the method's robustness for image forensics.

    

### [[2112.04312] Geometry-Guided Progressive NeRF for Generalizable and Efficient Neural Human Rendering](http://arxiv.org/abs/2112.04312)


  In this work we develop a generalizable and efficient Neural Radiance Field
(NeRF) pipeline for high-fidelity free-viewpoint human body synthesis under
settings with sparse camera views. Though existing NeRF-based methods can
synthesize rather realistic details for human body, they tend to produce poor
results when the input has self-occlusion, especially for unseen humans under
sparse views. Moreover, these methods often require a large number of sampling
points for rendering, which leads to low efficiency and limits their real-world
applicability. To address these challenges, we propose a Geometry-guided
Progressive NeRF~(GP-NeRF). In particular, to better tackle self-occlusion, we
devise a geometry-guided multi-view feature integration approach that utilizes
the estimated geometry prior to integrate the incomplete information from input
views and construct a complete geometry volume for the target human body.
Meanwhile, for achieving higher rendering efficiency, we introduce a
geometry-guided progressive rendering pipeline, which leverages the geometric
feature volume and the predicted density values to progressively reduce the
number of sampling points and speed up the rendering process. Experiments on
the ZJU-MoCap and THUman datasets show that our method outperforms the
state-of-the-arts significantly across multiple generalization settings, while
the time cost is reduced >70% via applying our efficient progressive rendering
pipeline.

    

### [[2112.04314] Trainability for Universal GNNs Through Surgical Randomness](http://arxiv.org/abs/2112.04314)


  Message passing neural networks (MPNN) have provable limitations, which can
be overcome by universal networks. However, universal networks are typically
impractical. The only exception is random node initialization (RNI), a data
augmentation method that results in provably universal networks. Unfortunately,
RNI suffers from severe drawbacks such as slow convergence and high sensitivity
to changes in hyperparameters. We transfer powerful techniques from the
practical world of graph isomorphism testing to MPNNs, resolving these
drawbacks. This culminates in individualization-refinement node initialization
(IRNI). We replace the indiscriminate and haphazard randomness used in RNI by a
surgical incision of only a few random bits at well-selected nodes. Our novel
non-intrusive data-augmentation scheme maintains the networks' universality
while resolving the trainability issues. We formally prove the claimed
universality and corroborate experimentally -- on synthetic benchmarks sets
previously explicitly designed for that purpose -- that IRNI overcomes the
limitations of MPNNs. We also verify the practical efficacy of our approach on
the standard benchmark data sets PROTEINS and NCI1.

    

### [[2112.04319] Improving the Training of Graph Neural Networks with Consistency Regularization](http://arxiv.org/abs/2112.04319)


  Graph neural networks (GNNs) have achieved notable success in the
semi-supervised learning scenario. The message passing mechanism in graph
neural networks helps unlabeled nodes gather supervision signals from their
labeled neighbors. In this work, we investigate how consistency regularization,
one of widely adopted semi-supervised learning methods, can help improve the
performance of graph neural networks. We revisit two methods of consistency
regularization for graph neural networks. One is simple consistency
regularization (SCR), and the other is mean-teacher consistency regularization
(MCR). We combine the consistency regularization methods with two
state-of-the-art GNNs and conduct experiments on the ogbn-products dataset.
With the consistency regularization, the performance of state-of-the-art GNNs
can be improved by 0.3% on the ogbn-products dataset of Open Graph Benchmark
(OGB) both with and without external data.

    

### [[2112.04322] Multiway Ensemble Kalman Filter](http://arxiv.org/abs/2112.04322)


  In this work, we study the emergence of sparsity and multiway structures in
second-order statistical characterizations of dynamical processes governed by
partial differential equations (PDEs). We consider several state-of-the-art
multiway covariance and inverse covariance (precision) matrix estimators and
examine their pros and cons in terms of accuracy and interpretability in the
context of physics-driven forecasting when incorporated into the ensemble
Kalman filter (EnKF). In particular, we show that multiway data generated from
the Poisson and the convection-diffusion types of PDEs can be accurately
tracked via EnKF when integrated with appropriate covariance and precision
matrix estimators.

    

### [[2112.04324] Deep Learning and Mathematical Intuition: A Review of (Davies et al. 2021)](http://arxiv.org/abs/2112.04324)


  A recent paper by Davies et al (2021) describes how deep learning (DL)
technology was used to find plausible hypotheses that have led to two original
mathematical results: one in knot theory, one in representation theory. I argue
here that the significance and novelty of this application of DL technology to
mathematics is significantly overstated in the paper under review and has been
wildly overstated in some of the accounts in the popular science press. In the
knot theory result, the role of DL was small, and a conventional statistical
analysis would probably have sufficed. In the representation theory result, the
role of DL is much larger; however, it is not very different in kind from what
has been done in experimental mathematics for decades. Moreover, it is not
clear whether the distinctive features of DL that make it useful here will
apply across a wide range of mathematical problems. Finally, I argue that the
DL here "guides human intuition" is unhelpful and misleading; what the DL does
primarily does is to mark many possible conjectures as false and a few others
as possibly worthy of study.
Certainly the representation theory result represents an original and
interesting application of DL to mathematical research, but its larger
significance is uncertain.

    

### [[2112.04330] Estimation in Rotationally Invariant Generalized Linear Models via Approximate Message Passing](http://arxiv.org/abs/2112.04330)


  We consider the problem of signal estimation in generalized linear models
defined via rotationally invariant design matrices. Since these matrices can
have an arbitrary spectral distribution, this model is well suited to capture
complex correlation structures which often arise in applications. We propose a
novel family of approximate message passing (AMP) algorithms for signal
estimation, and rigorously characterize their performance in the
high-dimensional limit via a state evolution recursion. Assuming knowledge of
the design matrix spectrum, our rotationally invariant AMP has complexity of
the same order as the existing AMP for Gaussian matrices; it also recovers the
existing AMP as a special case. Numerical results showcase a performance close
to Vector AMP (which is conjectured to be Bayes-optimal in some settings), but
obtained with a much lower complexity, as the proposed algorithm does not
require a computationally expensive singular value decomposition.

    

### [[2112.04344] Does Structure Matter? Leveraging Data-to-Text Generation for Answering Complex Information Needs](http://arxiv.org/abs/2112.04344)


  In this work, our aim is to provide a structured answer in natural language
to a complex information need. Particularly, we envision using generative
models from the perspective of data-to-text generation. We propose the use of a
content selection and planning pipeline which aims at structuring the answer by
generating intermediate plans. The experimental evaluation is performed using
the TREC Complex Answer Retrieval (CAR) dataset. We evaluate both the generated
answer and its corresponding structure and show the effectiveness of
planning-based models in comparison to a text-to-text model.

    

### [[2112.04345] Burn After Reading: Online Adaptation for Cross-domain Streaming Data](http://arxiv.org/abs/2112.04345)


  In the context of online privacy, many methods propose complex privacy and
security preserving measures to protect sensitive data. In this paper, we argue
that: not storing any sensitive data is the best form of security. Thus we
propose an online framework that "burns after reading", i.e. each online sample
is immediately deleted after it is processed. Meanwhile, we tackle the
inevitable distribution shift between the labeled public data and unlabeled
private data as a problem of unsupervised domain adaptation. Specifically, we
propose a novel algorithm that aims at the most fundamental challenge of the
online adaptation setting--the lack of diverse source-target data pairs.
Therefore, we design a Cross-Domain Bootstrapping approach, called CroDoBo, to
increase the combined diversity across domains. Further, to fully exploit the
valuable discrepancies among the diverse combinations, we employ the training
strategy of multiple learners with co-supervision. CroDoBo achieves
state-of-the-art online performance on four domain adaptation benchmarks.

    

### [[2112.04351] Sentiment Analysis and Effect of COVID-19 Pandemic using College SubReddit Data](http://arxiv.org/abs/2112.04351)


  The COVID-19 pandemic has affected societies and human health and well-being
in various ways. In this study, we collected Reddit data from 2019
(pre-pandemic) and 2020 (pandemic) from the subreddits communities associated
with 8 universities, applied natural language processing (NLP) techniques, and
trained graphical neural networks with social media data, to study how the
pandemic has affected people's emotions and psychological states compared to
the pre-pandemic era. Specifically, we first applied a pre-trained Robustly
Optimized BERT pre-training approach (RoBERTa) to learn embedding from the
semantic information of Reddit messages and trained a graph attention network
(GAT) for sentiment classification. The usage of GAT allows us to leverage the
relational information among the messages during training. We then applied
subgroup-adaptive model stacking to combine the prediction probabilities from
RoBERTa and GAT to yield the final classification on sentiment. With the
manually labeled and model-predicted sentiment labels on the collected data, we
applied a generalized linear mixed-effects model to estimate the effects of
pandemic and online teaching on people's sentiment in a statistically
significant manner. The results suggest the odds of negative sentiments in 2020
is $14.6\%$ higher than the odds in 2019 ($p$-value $<0.001$), and the odds of
negative sentiments are $41.6\%$ higher with in-person teaching than with
online teaching in 2020 ($p$-value $=0.037$) in the studied population.

    

### [[2112.04364] Generalization Error Bounds for Iterative Recovery Algorithms Unfolded as Neural Networks](http://arxiv.org/abs/2112.04364)


  Motivated by the learned iterative soft thresholding algorithm (LISTA), we
introduce a general class of neural networks suitable for sparse reconstruction
from few linear measurements. By allowing a wide range of degrees of
weight-sharing between the layers, we enable a unified analysis for very
different neural network types, ranging from recurrent ones to networks more
similar to standard feedforward neural networks. Based on training samples, via
empirical risk minimization we aim at learning the optimal network parameters
and thereby the optimal network that reconstructs signals from their
low-dimensional linear measurements. We derive generalization bounds by
analyzing the Rademacher complexity of hypothesis classes consisting of such
deep networks, that also take into account the thresholding parameters. We
obtain estimates of the sample complexity that essentially depend only linearly
on the number of parameters and on the depth. We apply our main result to
obtain specific generalization bounds for several practical examples, including
different algorithms for (implicit) dictionary learning, and convolutional
neural networks.

    

### [[2112.04369] Adaptive R-Peak Detection on Wearable ECG Sensors for High-Intensity Exercise](http://arxiv.org/abs/2112.04369)


  Objective: Continuous monitoring of biosignals via wearable sensors has
quickly expanded in the medical and wellness fields. At rest, automatic
detection of vital parameters is generally accurate. However, in conditions
such as high-intensity exercise, sudden physiological changes occur to the
signals, compromising the robustness of standard algorithms. Methods: Our
method, called BayeSlope, is based on unsupervised learning, Bayesian
filtering, and non-linear normalization to enhance and correctly detect the R
peaks according to their expected positions in the ECG. Furthermore, as
BayeSlope is computationally heavy and can drain the device battery quickly, we
propose an online design that adapts its robustness to sudden physiological
changes, and its complexity to the heterogeneous resources of modern embedded
platforms. This method combines BayeSlope with a lightweight algorithm,
executed in cores with different capabilities, to reduce the energy consumption
while preserving the accuracy. Results: BayeSlope achieves an F1 score of 99.3%
in experiments during intense cycling exercise with 20 subjects. Additionally,
the online adaptive process achieves an F1 score of 99% across five different
exercise intensities, with a total energy consumption of 1.55+-0.54~mJ.
Conclusion: We propose a highly accurate and robust method, and a complete
energy-efficient implementation in a modern ultra-low-power embedded platform
to improve R peak detection in challenging conditions, such as during
high-intensity exercise. Significance: The experiments show that BayeSlope
outperforms a state-of-the-art algorithm up to 8.4% in F1 score, while our
online adaptive method can reach energy savings up to 38.7% on modern
heterogeneous wearable platforms.

    

### [[2112.04379] Player Modeling using Behavioral Signals in Competitive Online Games](http://arxiv.org/abs/2112.04379)


  Competitive online games use rating systems to match players with similar
skills to ensure a satisfying experience for players. In this paper, we focus
on the importance of addressing different aspects of playing behavior when
modeling players for creating match-ups. To this end, we engineer several
behavioral features from a dataset of over 75,000 battle royale matches and
create player models based on the retrieved features. We then use the created
models to predict ranks for different groups of players in the data. The
predicted ranks are compared to those of three popular rating systems. Our
results show the superiority of simple behavioral models over mainstream rating
systems. Some behavioral features provided accurate predictions for all groups
of players while others proved useful for certain groups of players. The
results of this study highlight the necessity of considering different aspects
of the player's behavior such as goals, strategy, and expertise when making
assignments.

    

### [[2112.04386] Which images to label for few-shot medical landmark detection?](http://arxiv.org/abs/2112.04386)


  The success of deep learning methods relies on the availability of
well-labeled large-scale datasets. However, for medical images, annotating such
abundant training data often requires experienced radiologists and consumes
their limited time. Few-shot learning is developed to alleviate this burden,
which achieves competitive performances with only several labeled data.
However, a crucial yet previously overlooked problem in few-shot learning is
about the selection of template images for annotation before learning, which
affects the final performance. We herein propose a novel Sample Choosing Policy
(SCP) to select "the most worthy" images for annotation, in the context of
few-shot medical landmark detection. SCP consists of three parts: 1)
Self-supervised training for building a pre-trained deep model to extract
features from radiological images, 2) Key Point Proposal for localizing
informative patches, and 3) Representative Score Estimation for searching the
most representative samples or templates. The advantage of SCP is demonstrated
by various experiments on three widely-used public datasets. For one-shot
medical landmark detection, its use reduces the mean radial errors on
Cephalometric and HandXray datasets by 14.2% (from 3.595mm to 3.083mm) and
35.5% (4.114mm to 2.653mm), respectively.

    

### [[2112.04388] A graph representation based on fluid diffusion model for multimodal data analysis: theoretical aspects and enhanced community detection](http://arxiv.org/abs/2112.04388)


  Representing data by means of graph structures identifies one of the most
valid approach to extract information in several data analysis applications.
This is especially true when multimodal datasets are investigated, as records
collected by means of diverse sensing strategies are taken into account and
explored. Nevertheless, classic graph signal processing is based on a model for
information propagation that is configured according to heat diffusion
mechanism. This system provides several constraints and assumptions on the data
properties that might be not valid for multimodal data analysis, especially
when large scale datasets collected from heterogeneous sources are considered,
so that the accuracy and robustness of the outcomes might be severely
jeopardized. In this paper, we introduce a novel model for graph definition
based on fluid diffusion. The proposed approach improves the ability of
graph-based data analysis to take into account several issues of modern data
analysis in operational scenarios, so to provide a platform for precise,
versatile, and efficient understanding of the phenomena underlying the records
under exam, and to fully exploit the potential provided by the diversity of the
records in obtaining a thorough characterization of the data and their
significance. In this work, we focus our attention to using this fluid
diffusion model to drive a community detection scheme, i.e., to divide
multimodal datasets into many groups according to similarity among nodes in an
unsupervised fashion. Experimental results achieved by testing real multimodal
datasets in diverse application scenarios show that our method is able to
strongly outperform state-of-the-art schemes for community detection in
multimodal data analysis.

    

### [[2112.04389] Mixed Membership Distribution-Free model](http://arxiv.org/abs/2112.04389)


  We consider the problem of detecting latent community information of mixed
membership weighted network in which nodes have mixed memberships and edges
connecting between nodes can be finite real numbers. We propose a general mixed
membership distribution-free model for this problem. The model has no
distribution constraints of edges but only the expected values, and can be
viewed as generalizations of some previous models. We use an efficient spectral
algorithm to estimate community memberships under the model. We also derive the
convergence rate of the proposed algorithm under the model using delicate
spectral analysis. We demonstrate the advantages of mixed membership
distribution-free model with applications to a small scale of simulated
networks when edges follow different distributions.

    

### [[2112.04404] Gaudí: Conversational Interactions with Deep Representations to Generate Image Collections](http://arxiv.org/abs/2112.04404)


  Based on recent advances in realistic language modeling (GPT-3) and
cross-modal representations (CLIP), Gaudí was developed to help designers
search for inspirational images using natural language. In the early stages of
the design process, with the goal of eliciting a client's preferred creative
direction, designers will typically create thematic collections of
inspirational images called "mood-boards". Creating a mood-board involves
sequential image searches which are currently performed using keywords or
images. Gaudí transforms this process into a conversation where the user is
gradually detailing the mood-board's theme. This representation allows our AI
to generate new search queries from scratch, straight from a project briefing,
following a theme hypothesized by GPT-3. Compared to previous computational
approaches to mood-board creation, to the best of our knowledge, ours is the
first attempt to represent mood-boards as the stories that designers tell when
presenting a creative direction to a client.

    

### [[2112.04417] What I Cannot Predict, I Do Not Understand: A Human-Centered Evaluation Framework for Explainability Methods](http://arxiv.org/abs/2112.04417)


  A multitude of explainability methods and theoretical evaluation scores have
been proposed. However, it is not yet known: (1) how useful these methods are
in real-world scenarios and (2) how well theoretical measures predict the
usefulness of these methods for practical use by a human. To fill this gap, we
conducted human psychophysics experiments at scale to evaluate the ability of
human participants (n=1,150) to leverage representative attribution methods to
learn to predict the decision of different image classifiers. Our results
demonstrate that theoretical measures used to score explainability methods
poorly reflect the practical usefulness of individual attribution methods in
real-world scenarios. Furthermore, the degree to which individual attribution
methods helped human participants predict classifiers' decisions varied widely
across categorization tasks and datasets.
Overall, our results highlight fundamental challenges for the field --
suggesting a critical need to develop better explainability methods and to
deploy human-centered evaluation approaches. We will make the code of our
framework available to ease the systematic evaluation of novel explainability
methods.

    

### [[2112.04424] Training Robust Zero-Shot Voice Conversion Models with Self-supervised Features](http://arxiv.org/abs/2112.04424)


  Unsupervised Zero-Shot Voice Conversion (VC) aims to modify the speaker
characteristic of an utterance to match an unseen target speaker without
relying on parallel training data. Recently, self-supervised learning of speech
representation has been shown to produce useful linguistic units without using
transcripts, which can be directly passed to a VC model. In this paper, we
showed that high-quality audio samples can be achieved by using a length
resampling decoder, which enables the VC model to work in conjunction with
different linguistic feature extractors and vocoders without requiring them to
operate on the same sequence length. We showed that our method can outperform
many baselines on the VCTK dataset. Without modifying the architecture, we
further demonstrated that a) using pairs of different audio segments from the
same speaker, b) adding a cycle consistency loss, and c) adding a speaker
classification loss can help to learn a better speaker embedding. Our model
trained on LibriTTS using these techniques achieves the best performance,
producing audio samples transferred well to the target speaker's voice, while
preserving the linguistic content that is comparable with actual human
utterances in terms of Character Error Rate.

    

### [[2112.04426] Improving language models by retrieving from trillions of tokens](http://arxiv.org/abs/2112.04426)


  We enhance auto-regressive language models by conditioning on document chunks
retrieved from a large corpus, based on local similarity with preceding tokens.
With a $2$ trillion token database, our Retrieval-Enhanced Transformer (RETRO)
obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite
using 25$\times$ fewer parameters. After fine-tuning, RETRO performance
translates to downstream knowledge-intensive tasks such as question answering.
RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked
cross-attention mechanism to predict tokens based on an order of magnitude more
data than what is typically consumed during training. We typically train RETRO
from scratch, yet can also rapidly RETROfit pre-trained transformers with
retrieval and still achieve good performance. Our work opens up new avenues for
improving language models through explicit memory at unprecedented scale.

    

### [[2112.04453] MLP Architectures for Vision-and-Language Modeling: An Empirical Study](http://arxiv.org/abs/2112.04453)


  We initiate the first empirical study on the use of MLP architectures for
vision-and-language (VL) fusion. Through extensive experiments on 5 VL tasks
and 5 robust VQA benchmarks, we find that: (i) Without pre-training, using MLPs
for multimodal fusion has a noticeable performance gap compared to
transformers; (ii) However, VL pre-training can help close the performance gap;
(iii) Instead of heavy multi-head attention, adding tiny one-head attention to
MLPs is sufficient to achieve comparable performance to transformers. Moreover,
we also find that the performance gap between MLPs and transformers is not
widened when being evaluated on the harder robust VQA benchmarks, suggesting
using MLPs for VL fusion can generalize roughly to a similar degree as using
transformers. These results hint that MLPs can effectively learn to align
vision and text features extracted from lower-level encoders without heavy
reliance on self-attention. Based on this, we ask an even bolder question: can
we have an all-MLP architecture for VL modeling, where both VL fusion and the
vision encoder are replaced with MLPs? Our result shows that an all-MLP VL
model is sub-optimal compared to state-of-the-art full-featured VL models when
both of them get pre-trained. However, pre-training an all-MLP can surprisingly
achieve a better average score than full-featured transformer models without
pre-training. This indicates the potential of large-scale pre-training of
MLP-like architectures for VL modeling and inspires the future research
direction on simplifying well-established VL modeling with less inductive
design bias. Our code is publicly available at:
this https URL


### [[2112.04459] Self-Supervised Speaker Verification with Simple Siamese Network and Self-Supervised Regularization](http://arxiv.org/abs/2112.04459)


  Training speaker-discriminative and robust speaker verification systems
without speaker labels is still challenging and worthwhile to explore. In this
study, we propose an effective self-supervised learning framework and a novel
regularization strategy to facilitate self-supervised speaker representation
learning. Different from contrastive learning-based self-supervised learning
methods, the proposed self-supervised regularization (SSReg) focuses
exclusively on the similarity between the latent representations of positive
data pairs. We also explore the effectiveness of alternative online data
augmentation strategies on both the time domain and frequency domain. With our
strong online data augmentation strategy, the proposed SSReg shows the
potential of self-supervised learning without using negative pairs and it can
significantly improve the performance of self-supervised speaker representation
learning with a simple Siamese network architecture. Comprehensive experiments
on the VoxCeleb datasets demonstrate that our proposed self-supervised approach
obtains a 23.4% relative improvement by adding the effective self-supervised
regularization and outperforms other previous works.

    

### [[2112.04461] Enhancing Counterfactual Classification via Self-Training](http://arxiv.org/abs/2112.04461)


  Unlike traditional supervised learning, in many settings only partial
feedback is available. We may only observe outcomes for the chosen actions, but
not the counterfactual outcomes associated with other alternatives. Such
settings encompass a wide variety of applications including pricing, online
marketing and precision medicine. A key challenge is that observational data
are influenced by historical policies deployed in the system, yielding a biased
data distribution. We approach this task as a domain adaptation problem and
propose a self-training algorithm which imputes outcomes with categorical
values for finite unseen actions in the observational data to simulate a
randomized trial through pseudolabeling, which we refer to as Counterfactual
Self-Training (CST). CST iteratively imputes pseudolabels and retrains the
model. In addition, we show input consistency loss can further improve CST
performance which is shown in recent theoretical analysis of pseudolabeling. We
demonstrate the effectiveness of the proposed algorithms on both synthetic and
real datasets.

    

### [[2112.04467] CoMPS: Continual Meta Policy Search](http://arxiv.org/abs/2112.04467)


  We develop a new continual meta-learning method to address challenges in
sequential multi-task learning. In this setting, the agent's goal is to achieve
high reward over any sequence of tasks quickly. Prior meta-reinforcement
learning algorithms have demonstrated promising results in accelerating the
acquisition of new tasks. However, they require access to all tasks during
training. Beyond simply transferring past experience to new tasks, our goal is
to devise continual reinforcement learning algorithms that learn to learn,
using their experience on previous tasks to learn new tasks more quickly. We
introduce a new method, continual meta-policy search (CoMPS), that removes this
limitation by meta-training in an incremental fashion, over each task in a
sequence, without revisiting prior tasks. CoMPS continuously repeats two
subroutines: learning a new task using RL and using the experience from RL to
perform completely offline meta-learning to prepare for subsequent task
learning. We find that CoMPS outperforms prior continual learning and
off-policy meta-reinforcement methods on several sequences of challenging
continuous control tasks.

    

### [[2112.04468] Revisiting Contrastive Learning through the Lens of Neighborhood Component Analysis: an Integrated Framework](http://arxiv.org/abs/2112.04468)


  As a seminal tool in self-supervised representation learning, contrastive
learning has gained unprecedented attention in recent years. In essence,
contrastive learning aims to leverage pairs of positive and negative samples
for representation learning, which relates to exploiting neighborhood
information in a feature space. By investigating the connection between
contrastive learning and neighborhood component analysis (NCA), we provide a
novel stochastic nearest neighbor viewpoint of contrastive learning and
subsequently propose a series of contrastive losses that outperform the
existing ones. Under our proposed framework, we show a new methodology to
design integrated contrastive losses that could simultaneously achieve good
accuracy and robustness on downstream tasks. With the integrated framework, we
achieve up to 6\% improvement on the standard accuracy and 17\% improvement on
the adversarial accuracy.

    

### [[2112.04470] Optimistic Rates: A Unifying Theory for Interpolation Learning and Regularization in Linear Regression](http://arxiv.org/abs/2112.04470)


  We study a localized notion of uniform convergence known as an "optimistic
rate" (Panchenko 2002; Srebro et al. 2010) for linear regression with Gaussian
data. Our refined analysis avoids the hidden constant and logarithmic factor in
existing results, which are known to be crucial in high-dimensional settings,
especially for understanding interpolation learning. As a special case, our
analysis recovers the guarantee from Koehler et al. (2021), which tightly
characterizes the population risk of low-norm interpolators under the benign
overfitting conditions. Our optimistic rate bound, though, also analyzes
predictors with arbitrary training error. This allows us to recover some
classical statistical guarantees for ridge and LASSO regression under random
designs, and helps us obtain a precise understanding of the excess risk of
near-interpolators in the over-parameterized regime.

    

### [[2112.04480] Exploring Temporal Granularity in Self-Supervised Video Representation Learning](http://arxiv.org/abs/2112.04480)


  This work presents a self-supervised learning framework named TeG to explore
Temporal Granularity in learning video representations. In TeG, we sample a
long clip from a video and a short clip that lies inside the long clip. We then
extract their dense temporal embeddings. The training objective consists of two
parts: a fine-grained temporal learning objective to maximize the similarity
between corresponding temporal embeddings in the short clip and the long clip,
and a persistent temporal learning objective to pull together global embeddings
of the two clips. Our study reveals the impact of temporal granularity with
three major findings. 1) Different video tasks may require features of
different temporal granularities. 2) Intriguingly, some tasks that are widely
considered to require temporal awareness can actually be well addressed by
temporally persistent features. 3) The flexibility of TeG gives rise to
state-of-the-art results on 8 video benchmarks, outperforming supervised
pre-training in most cases.

    

### [[1811.11419] Mixture Martingales Revisited with Applications to Sequential Tests and Confidence Intervals](http://arxiv.org/abs/1811.11419)


  This paper presents new deviation inequalities that are valid uniformly in
time under adaptive sampling in a multi-armed bandit model. The deviations are
measured using the Kullback-Leibler divergence in a given one-dimensional
exponential family, and may take into account several arms at a time. They are
obtained by constructing for each arm a mixture martingale based on a
hierarchical prior, and by multiplying those martingales. Our deviation
inequalities allow us to analyze stopping rules based on generalized likelihood
ratios for a large class of sequential identification problems, and to
construct tight confidence intervals for some functions of the means of the
arms.

    

### [[1901.01002] On Reproducing Kernel Banach Spaces: Generic Definitions and Unified Framework of Constructions](http://arxiv.org/abs/1901.01002)


  Recently, there has been emerging interest in constructing reproducing kernel
Banach spaces (RKBS) for applied and theoretical purposes such as machine
learning, sampling reconstruction, sparse approximation and functional
analysis. Existing constructions include the reflexive RKBS via a bilinear
form, the semi-inner-product RKBS, the RKBS with $\ell^1$ norm, the $p$-norm
RKBS via generalized Mercer kernels, etc. The definitions of RKBS and the
associated reproducing kernel in those references are dependent on the
construction. Moreover, relations among those constructions are unclear. We
explore a generic definition of RKBS and the reproducing kernel for RKBS that
is independent of construction. Furthermore, we propose a framework of
constructing RKBSs that unifies existing constructions mentioned above via a
continuous bilinear form and a pair of feature maps. A new class of Orlicz
RKBSs is proposed. Finally, we develop representer theorems for machine
learning in RKBSs constructed in our framework, which also unifies representer
theorems in existing RKBSs.

    

### [[1910.09573] Detecting Underspecification with Local Ensembles](http://arxiv.org/abs/1910.09573)


  We present local ensembles, a method for detecting underspecification -- when
many possible predictors are consistent with the training data and model class
-- at test time in a pre-trained model. Our method uses local second-order
information to approximate the variance of predictions across an ensemble of
models from the same class. We compute this approximation by estimating the
norm of the component of a test point's gradient that aligns with the
low-curvature directions of the Hessian, and provide a tractable method for
estimating this quantity. Experimentally, we show that our method is capable of
detecting when a pre-trained model is underspecified on test data, with
applications to out-of-distribution detection, detecting spurious correlates,
and active learning.

    

### [[2006.10919] On the effect of normalization layers on Differentially Private training of deep Neural networks](http://arxiv.org/abs/2006.10919)


  Differentially private stochastic gradient descent (DPSGD) is a variation of
stochastic gradient descent based on the Differential Privacy (DP) paradigm,
which can mitigate privacy threats that arise from the presence of sensitive
information in training data. However, one major drawback of training deep
neural networks with DPSGD is a reduction in the models accuracy. In this
paper, we study the effect of normalization layers on the performance of DPSGD.
We demonstrate that normalization layers significantly impact the utility of
deep neural networks with noisy parameters and should be considered essential
ingredients of training with DPSGD. In particular, we propose a novel method
for integrating batch normalization with DPSGD without incurring an additional
privacy loss. With our approach, we are able to train deeper networks and
achieve a better utility-privacy trade-off.

    

### [[2006.11440] Local Convolutions Cause an Implicit Bias towards High Frequency Adversarial Examples](http://arxiv.org/abs/2006.11440)


  Adversarial Attacks are still a significant challenge for neural networks.
Recent work has shown that adversarial perturbations typically contain
high-frequency features, but the root cause of this phenomenon remains unknown.
Inspired by theoretical work on linear full-width convolutional models, we
hypothesize that the local (i.e. bounded-width) convolutional operations
commonly used in current neural networks are implicitly biased to learn high
frequency features, and that this is one of the root causes of high frequency
adversarial examples. To test this hypothesis, we analyzed the impact of
different choices of linear and nonlinear architectures on the implicit bias of
the learned features and the adversarial perturbations, in both spatial and
frequency domains. We find that the high-frequency adversarial perturbations
are critically dependent on the convolution operation because the
spatially-limited nature of local convolutions induces an implicit bias towards
high frequency features. The explanation for the latter involves the Fourier
Uncertainty Principle: a spatially-limited (local in the space domain) filter
cannot also be frequency-limited (local in the frequency domain). Furthermore,
using larger convolution kernel sizes or avoiding convolutions (e.g. by using
Vision Transformers architecture) significantly reduces this high frequency
bias, but not the overall susceptibility to attacks. Looking forward, our work
strongly suggests that understanding and controlling the implicit bias of
architectures will be essential for achieving adversarial robustness.

    

### [[2006.15294] Gradient-based Editing of Memory Examples for Online Task-free Continual Learning](http://arxiv.org/abs/2006.15294)


  We explore task-free continual learning (CL), in which a model is trained to
avoid catastrophic forgetting in the absence of explicit task boundaries or
identities. Among many efforts on task-free CL, a notable family of approaches
are memory-based that store and replay a subset of training examples. However,
the utility of stored seen examples may diminish over time since CL models are
continually updated. Here, we propose Gradient based Memory EDiting (GMED), a
framework for editing stored examples in continuous input space via gradient
updates, in order to create more "challenging" examples for replay. GMED-edited
examples remain similar to their unedited forms, but can yield increased loss
in the upcoming model updates, thereby making the future replays more effective
in overcoming catastrophic forgetting. By construction, GMED can be seamlessly
applied in conjunction with other memory-based CL algorithms to bring further
improvement. Experiments validate the effectiveness of GMED, and our best
method significantly outperforms baselines and previous state-of-the-art on
five out of six datasets. Code can be found at this https URL.

    

### [[2009.12153] A Systematic Review on Model Watermarking for Neural Networks](http://arxiv.org/abs/2009.12153)


  Machine learning (ML) models are applied in an increasing variety of domains.
The availability of large amounts of data and computational resources
encourages the development of ever more complex and valuable models. These
models are considered intellectual property of the legitimate parties who have
trained them, which makes their protection against stealing, illegitimate
redistribution, and unauthorized application an urgent need. Digital
watermarking presents a strong mechanism for marking model ownership and,
thereby, offers protection against those threats. This work presents a taxonomy
identifying and analyzing different classes of watermarking schemes for ML
models. It introduces a unified threat model to allow structured reasoning on
and comparison of the effectiveness of watermarking methods in different
scenarios. Furthermore, it systematizes desired security requirements and
attacks against ML model watermarking. Based on that framework, representative
literature from the field is surveyed to illustrate the taxonomy. Finally,
shortcomings and general limitations of existing approaches are discussed, and
an outlook on future research directions is given.

    

### [[2011.02159] Reverse engineering learned optimizers reveals known and novel mechanisms](http://arxiv.org/abs/2011.02159)


  Learned optimizers are algorithms that can themselves be trained to solve
optimization problems. In contrast to baseline optimizers (such as momentum or
Adam) that use simple update rules derived from theoretical principles, learned
optimizers use flexible, high-dimensional, nonlinear parameterizations.
Although this can lead to better performance in certain settings, their inner
workings remain a mystery. How is a learned optimizer able to outperform a well
tuned baseline? Has it learned a sophisticated combination of existing
optimization techniques, or is it implementing completely new behavior? In this
work, we address these questions by careful analysis and visualization of
learned optimizers. We study learned optimizers trained from scratch on three
disparate tasks, and discover that they have learned interpretable mechanisms,
including: momentum, gradient clipping, learning rate schedules, and a new form
of learning rate adaptation. Moreover, we show how the dynamics of learned
optimizers enables these behaviors. Our results help elucidate the previously
murky understanding of how learned optimizers work, and establish tools for
interpreting future learned optimizers.

    

### [[2011.07423] Declarative Approaches to Counterfactual Explanations for Classification](http://arxiv.org/abs/2011.07423)


  We propose answer-set programs that specify and compute counterfactual
interventions on entities that are input on a classification model. In relation
to the outcome of the model, the resulting counterfactual entities serve as a
basis for the definition and computation of causality-based explanation scores
for the feature values in the entity under classification, namely
"responsibility scores". The approach and the programs can be applied with
black-box models, and also with models that can be specified as logic programs,
such as rule-based classifiers. The main focus of this work is on the
specification and computation of "best" counterfactual entities, i.e. those
that lead to maximum responsibility scores. From them one can read off the
explanations as maximum responsibility feature values in the original entity.
We also extend the programs to bring into the picture semantic or domain
knowledge. We show how the approach could be extended by means of probabilistic
methods, and how the underlying probability distributions could be modified
through the use of constraints. Several examples of programs written in the
syntax of the DLV ASP-solver, and run with it, are shown.

    

### [[2012.06168] OpenHoldem: An Open Toolkit for Large-Scale Imperfect-Information Game Research](http://arxiv.org/abs/2012.06168)


  Owning to the unremitting efforts by a few institutes, significant progress
has recently been made in designing superhuman AIs in No-limit Texas Hold'em
(NLTH), the primary testbed for large-scale imperfect-information game
research. However, it remains challenging for new researchers to study this
problem since there are no standard benchmarks for comparing with existing
methods, which seriously hinders further developments in this research area. In
this work, we present OpenHoldem, an integrated toolkit for large-scale
imperfect-information game research using NLTH. OpenHoldem makes three main
contributions to this research direction: 1) a standardized evaluation protocol
for thoroughly evaluating different NLTH AIs, 2) four publicly available strong
baselines for NLTH AI, and 3) an online testing platform with easy-to-use APIs
for public NLTH AI evaluation. We have released OpenHoldem at this http URL,
hoping it facilitates further studies on the unsolved theoretical and
computational issues in this area and cultivate crucial research problems like
opponent modeling and human-computer interactive learning.

    

### [[2101.06763] Multi-view Data Visualisation via Manifold Learning](http://arxiv.org/abs/2101.06763)


  Non-linear dimensionality reduction can be performed by \textit{manifold
learning} approaches, such as Stochastic Neighbour Embedding (SNE), Locally
Linear Embedding (LLE) and Isometric Feature Mapping (ISOMAP). These methods
aim to produce two or three latent embeddings, primarily to visualise the data
in intelligible representations. This manuscript proposes extensions of
Student's t-distributed SNE (t-SNE), LLE and ISOMAP, for dimensionality
reduction and visualisation of multi-view data. Multi-view data refers to
multiple types of data generated from the same samples. The proposed multi-view
approaches provide more comprehensible projections of the samples compared to
the ones obtained by visualising each data-view separately. Commonly
visualisation is used for identifying underlying patterns within the samples.
By incorporating the obtained low-dimensional embeddings from the multi-view
manifold approaches into the K-means clustering algorithm, it is shown that
clusters of the samples are accurately identified. Through the analysis of real
and synthetic data the proposed multi-SNE approach is found to have the best
performance. We further illustrate the applicability of the multi-SNE approach
for the analysis of multi-omics single-cell data, where the aim is to visualise
and identify cell heterogeneity and cell types in biological tissues relevant
to health and disease.

    

### [[2101.07295] Does Continual Learning = Catastrophic Forgetting?](http://arxiv.org/abs/2101.07295)


  Continual learning is known for suffering from catastrophic forgetting, a
phenomenon where earlier learned concepts are forgotten at the expense of more
recent samples. In this work, we challenge the assumption that continual
learning is inevitably associated with catastrophic forgetting by presenting a
set of tasks that surprisingly do not suffer from catastrophic forgetting when
learned continually. We provide evidence that these reconstruction-type tasks
exhibit positive forward transfer and that single-view 3D shape reconstruction
improves the performance on learned and novel categories over time. We provide
the novel analysis of knowledge transfer ability by looking at the output
distribution shift across sequential learning tasks. Finally, we show that the
robustness of these tasks leads to the potential of having a proxy
representation learning task for continual classification. The codebase,
dataset, and pre-trained models released with this article can be found at
this https URL.

    

### [[2102.09284] Reduced-Order Neural Network Synthesis with Robustness Guarantees](http://arxiv.org/abs/2102.09284)


  In the wake of the explosive growth in smartphones and cyberphysical systems,
there has been an accelerating shift in how data is generated away from
centralised data towards on-device generated data. In response, machine
learning algorithms are being adapted to run locally on board, potentially
hardware limited, devices to improve user privacy, reduce latency and be more
energy efficient. However, our understanding of how these device orientated
algorithms behave and should be trained is still fairly limited. To address
this issue, a method to automatically synthesize reduced-order neural networks
(having fewer neurons) approximating the input/output mapping of a larger one
is introduced. The reduced-order neural network's weights and biases are
generated from a convex semi-definite programme that minimises the worst-case
approximation error with respect to the larger network. Worst-case bounds for
this approximation error are obtained and the approach can be applied to a wide
variety of neural networks architectures. What differentiates the proposed
approach to existing methods for generating small neural networks, e.g.
pruning, is the inclusion of the worst-case approximation error directly within
the training cost function, which should add robustness. Numerical examples
highlight the potential of the proposed approach. The overriding goal of this
paper is to generalise recent results in the robustness analysis of neural
networks to a robust synthesis problem for their weights and biases.

    

### [[2103.01895] Adversarial Examples can be Effective Data Augmentation for Unsupervised Machine Learning](http://arxiv.org/abs/2103.01895)


  Adversarial examples causing evasive predictions are widely used to evaluate
and improve the robustness of machine learning models. However, current studies
focus on supervised learning tasks, relying on the ground-truth data label, a
targeted objective, or supervision from a trained classifier. In this paper, we
propose a framework of generating adversarial examples for unsupervised models
and demonstrate novel applications to data augmentation. Our framework exploits
a mutual information neural estimator as an information-theoretic similarity
measure to generate adversarial examples without supervision. We propose a new
MinMax algorithm with provable convergence guarantees for efficient generation
of unsupervised adversarial examples. Our framework can also be extended to
supervised adversarial examples. When using unsupervised adversarial examples
as a simple plug-in data augmentation tool for model retraining, significant
improvements are consistently observed across different unsupervised tasks and
datasets, including data reconstruction, representation learning, and
contrastive learning. Our results show novel methods and considerable
advantages in studying and improving unsupervised machine learning via
adversarial examples.

    

### [[2103.05895] WFA-IRL: Inverse Reinforcement Learning of Autonomous Behaviors Encoded as Weighted Finite Automata](http://arxiv.org/abs/2103.05895)


  This paper presents a method for learning logical task specifications and
cost functions from demonstrations. Linear temporal logic (LTL) formulas are
widely used to express complex objectives and constraints for autonomous
systems. Yet, such specifications may be challenging to construct by hand.
Instead, we consider demonstrated task executions, whose temporal logic
structure and transition costs need to be inferred by an autonomous agent. We
employ a spectral learning approach to extract a weighted finite automaton
(WFA), approximating the unknown logic structure of the task. Thereafter, we
define a product between the WFA for high-level task guidance and a labeled
Markov decision process for low-level control. An inverse reinforcement
learning (IRL) problem is considered to learn a cost function by
backpropagating the loss between agent and expert behaviors through the
planning algorithm. Our proposed model, termed WFA-IRL, is capable of
generalizing the execution of the inferred task specification in a suite of
MiniGrid environments.

    

### [[2104.04450] Unsupervised Class-Incremental Learning Through Confusion](http://arxiv.org/abs/2104.04450)


  While many works on Continual Learning have shown promising results for
mitigating catastrophic forgetting, they have relied on supervised training. To
successfully learn in a label-agnostic incremental setting, a model must
distinguish between learned and novel classes to properly include samples for
training. We introduce a novelty detection method that leverages network
confusion caused by training incoming data as a new class. We found that
incorporating a class-imbalance during this detection method substantially
enhances performance. The effectiveness of our approach is demonstrated across
a set of image classification benchmarks: MNIST, SVHN, CIFAR-10, CIFAR-100, and
CRIB.

    

### [[2106.02543] Accelerating Dynamical System Simulations with Contracting and Physics-Projected Neural-Newton Solvers](http://arxiv.org/abs/2106.02543)


  Recent advances in deep learning have allowed neural networks (NNs) to
successfully replace traditional numerical solvers in many applications, thus
enabling impressive computing gains. One such application is time domain
simulation, which is indispensable for the design, analysis and operation of
many engineering systems. Simulating dynamical systems with implicit
Newton-based solvers is a computationally heavy task, as it requires the
solution of a parameterized system of differential and algebraic equations at
each time step. A variety of NN-based methodologies have been shown to
successfully approximate the trajectories computed by numerical solvers at a
fraction of the time. However, few previous works have used NNs to model the
numerical solver itself. For the express purpose of accelerating time domain
simulation speeds, this paper proposes and explores two complementary
alternatives for modeling numerical solvers. First, we use a NN to mimic the
linear transformation provided by the inverse Jacobian in a single Newton step.
Using this procedure, we evaluate and project the exact, physics-based residual
error onto the NN mapping, thus leaving physics ``in the loop''. The resulting
tool, termed the Physics-pRojected Neural-Newton Solver (PRoNNS), is able to
achieve an extremely high degree of numerical accuracy at speeds which were
observed to be up to 31% faster than a Newton-based solver. In the second
approach, we model the Newton solver at the heart of an implicit Runge-Kutta
integrator as a contracting map iteratively seeking a fixed point on a time
domain trajectory. The associated recurrent NN simulation tool, termed the
Contracting Neural-Newton Solver (CoNNS), is embedded with training constraints
(via CVXPY Layers) which guarantee the mapping provided by the NN satisfies the
Banach fixed-point theorem.

    

### [[2106.05165] A Lyapunov-Based Methodology for Constrained Optimization with Bandit Feedback](http://arxiv.org/abs/2106.05165)


  In a wide variety of applications including online advertising, contractual
hiring, and wireless scheduling, the controller is constrained by a stringent
budget constraint on the available resources, which are consumed in a random
amount by each action, and a stochastic feasibility constraint that may impose
important operational limitations on decision-making. In this work, we consider
a general model to address such problems, where each action returns a random
reward, cost, and penalty from an unknown joint distribution, and the
decision-maker aims to maximize the total reward under a budget constraint $B$
on the total cost and a stochastic constraint on the time-average penalty. We
propose a novel low-complexity algorithm based on Lyapunov optimization
methodology, named ${\tt LyOn}$, and prove that for $K$ arms it achieves
$O(\sqrt{K B\log B})$ regret and zero constraint-violation when $B$ is
sufficiently large. The low computational cost and sharp performance bounds of
${\tt LyOn}$ suggest that Lyapunov-based algorithm design methodology can be
effective in solving constrained bandit optimization problems.

    

### [[2106.07880] Scaling Neural Tangent Kernels via Sketching and Random Features](http://arxiv.org/abs/2106.07880)


  The Neural Tangent Kernel (NTK) characterizes the behavior of infinitely-wide
neural networks trained under least squares loss by gradient descent. Recent
works also report that NTK regression can outperform finitely-wide neural
networks trained on small-scale datasets. However, the computational complexity
of kernel methods has limited its use in large-scale learning tasks. To
accelerate learning with NTK, we design a near input-sparsity time
approximation algorithm for NTK, by sketching the polynomial expansions of
arc-cosine kernels: our sketch for the convolutional counterpart of NTK (CNTK)
can transform any image using a linear runtime in the number of pixels.
Furthermore, we prove a spectral approximation guarantee for the NTK matrix, by
combining random features (based on leverage score sampling) of the arc-cosine
kernels with a sketching algorithm. We benchmark our methods on various
large-scale regression and classification tasks and show that a linear
regressor trained on our CNTK features matches the accuracy of exact CNTK on
CIFAR-10 dataset while achieving 150x speedup.

    

### [[2106.12026] The Neurally-Guided Shape Parser: Grammar-based Labeling of 3D Shape Regions with Approximate Inference](http://arxiv.org/abs/2106.12026)


  We propose the Neurally-Guided Shape Parser (NGSP), a method that learns how
to assign fine-grained semantic labels to regions of a 3D shape. NGSP solves
this problem via MAP inference, modeling the posterior probability of a label
assignment conditioned on an input shape with a learned likelihood function. To
make this search tractable, NGSP employs a neural guide network that learns to
approximate the posterior. NGSP finds high-probability label assignments by
first sampling proposals with the guide network and then evaluating each
proposal under the full likelihood. We evaluate NGSP on the task of
fine-grained semantic segmentation of manufactured 3D shapes from PartNet,
where shapes have been decomposed into regions that correspond to part instance
over-segmentations. We find that NGSP delivers significant performance
improvements over comparison methods that (i) use regions to group per-point
predictions, (ii) use regions as a self-supervisory signal or (iii) assign
labels to regions under alternative formulations. Further, we show that NGSP
maintains strong performance even with limited labeled data or as shape regions
undergo artificial corruption. Finally, we demonstrate that NGSP can be
directly applied to CAD shapes found in online repositories and validate its
effectiveness with a perceptual study.

    

### [[2110.01471] Fine-Grained Neural Network Explanation by Identifying Input Features with Predictive Information](http://arxiv.org/abs/2110.01471)


  One principal approach for illuminating a black-box neural network is feature
attribution, i.e. identifying the importance of input features for the
network's prediction. The predictive information of features is recently
proposed as a proxy for the measure of their importance. So far, the predictive
information is only identified for latent features by placing an information
bottleneck within the network. We propose a method to identify features with
predictive information in the input domain. The method results in fine-grained
identification of input features' information and is agnostic to network
architecture. The core idea of our method is leveraging a bottleneck on the
input that only lets input features associated with predictive latent features
pass through. We compare our method with several feature attribution methods
using mainstream feature attribution evaluation experiments. The code is
publicly available.

    

### [[2112.02856] Optimal No-Regret Learning in Strongly Monotone Games with Bandit Feedback](http://arxiv.org/abs/2112.02856)


  We consider online no-regret learning in unknown games with bandit feedback,
where each agent only observes its reward at each time -- determined by all
players' current joint action -- rather than its gradient. We focus on the
class of smooth and strongly monotone games and study optimal no-regret
learning therein. Leveraging self-concordant barrier functions, we first
construct an online bandit convex optimization algorithm and show that it
achieves the single-agent optimal regret of $\tilde{\Theta}(\sqrt{T})$ under
smooth and strongly-concave payoff functions. We then show that if each agent
applies this no-regret learning algorithm in strongly monotone games, the joint
action converges in \textit{last iterate} to the unique Nash equilibrium at a
rate of $\tilde{\Theta}(1/\sqrt{T})$. Prior to our work, the best-know
convergence rate in the same class of games is $O(1/T^{1/3})$ (achieved by a
different algorithm), thus leaving open the problem of optimal no-regret
learning algorithms (since the known lower bound is $\Omega(1/\sqrt{T})$). Our
results thus settle this open problem and contribute to the broad landscape of
bandit game-theoretical learning by identifying the first doubly optimal bandit
learning algorithm, in that it achieves (up to log factors) both optimal regret
in the single-agent learning and optimal last-iterate convergence rate in the
multi-agent learning. We also present results on several simulation studies --
Cournot competition, Kelly auctions, and distributed regularized logistic
regression -- to demonstrate the efficacy of our algorithm.

    

### [[2112.03227] CALVIN: A Benchmark for Language-conditioned Policy Learning for Long-horizon Robot Manipulation Tasks](http://arxiv.org/abs/2112.03227)


  General-purpose robots coexisting with humans in their environment must learn
to relate human language to their perceptions and actions to be useful in a
range of daily tasks. Moreover, they need to acquire a diverse repertoire of
general-purpose skills that allow composing long-horizon tasks by following
unconstrained language instructions. In this paper, we present CALVIN
(Composing Actions from Language and Vision), an open-source simulated
benchmark to learn long-horizon language-conditioned tasks. Our aim is to make
it possible to develop agents that can solve many robotic manipulation tasks
over a long horizon, from onboard sensors, and specified only via human
language. CALVIN tasks are more complex in terms of sequence length, action
space, and language than existing vision-and-language task datasets and
supports flexible specification of sensor suites. We evaluate the agents in
zero-shot to novel language instructions and to novel environments and objects.
We show that a baseline model based on multi-context imitation learning
performs poorly on CALVIN, suggesting that there is significant room for
developing innovative agents that learn to relate human language to their world
models with this benchmark.

    

### [[2102.05599] Improving Model-Based Reinforcement Learning with Internal State Representations through Self-Supervision](http://arxiv.org/abs/2102.05599)


  Using a model of the environment, reinforcement learning agents can plan
their future moves and achieve superhuman performance in board games like
Chess, Shogi, and Go, while remaining relatively sample-efficient. As
demonstrated by the MuZero Algorithm, the environment model can even be learned
dynamically, generalizing the agent to many more tasks while at the same time
achieving state-of-the-art performance. Notably, MuZero uses internal state
representations derived from real environment states for its predictions. In
this paper, we bind the model's predicted internal state representation to the
environment state via two additional terms: a reconstruction model loss and a
simpler consistency loss, both of which work independently and unsupervised,
acting as constraints to stabilize the learning process. Our experiments show
that this new integration of reconstruction model loss and simpler consistency
loss provide a significant performance increase in OpenAI Gym environments. Our
modifications also enable self-supervised pretraining for MuZero, so the
algorithm can learn about environment dynamics before a goal is made available.

    

### [[2112.03442] Approximating Nash Equilibrium in Random Graphical Games](http://arxiv.org/abs/2112.03442)


  Computing Nash equilibrium in multi-agent games is a longstanding challenge
at the interface of game theory and computer science. It is well known that a
general normal form game in N players and k strategies requires exponential
space simply to write down. This Curse of Multi-Agents prompts the study of
succinct games which can be written down efficiently. A canonical example of a
succinct game is the graphical game which models players as nodes in a graph
interacting with only their neighbors in direct analogy with markov random
fields. Graphical games have found applications in wireless, financial, and
social networks. However, computing the nash equilbrium of graphical games has
proven challenging. Even for polymatrix games, a model where payoffs to an
agent can be written as the sum of payoffs of interactions with the agent's
neighbors, it has been shown that computing an epsilon approximate nash
equilibrium is PPAD hard for epsilon smaller than a constant. The focus of this
work is to circumvent this computational hardness by considering average case
graph models i.e random graphs. We provide a quasipolynomial time approximation
scheme (QPTAS) for computing an epsilon approximate nash equilibrium of
polymatrix games on random graphs with edge density greater than poly(k,
1/epsilon, ln(N))$ with high probability. Furthermore, with the same runtime we
can compute an epsilon-approximate Nash equilibrium that epsilon-approximates
the maximum social welfare of any nash equilibrium of the game. Our primary
technical innovation is an "accelerated rounding" of a novel hierarchical
convex program for the nash equilibrium problem. Our accelerated rounding also
yields faster algorithms for Max-2CSP on the same family of random graphs,
which may be of independent interest.

    

### [[2112.04136] SeaPlace: Process Variation Aware Placement for Reliable Combinational Circuits against SETs and METs](http://arxiv.org/abs/2112.04136)


  Nowadays nanoscale combinational circuits are facing significant reliability
challenges including soft errors and process variations. This paper presents
novel process variation-aware placement strategies that include two algorithms
to increase the reliability of combinational circuits against both Single Event
Transients (SETs) and Multiple Event Transients (METs). The first proposed
algorithm is a global placement method (called SeaPlace-G) that places the
cells for hardening the circuit against SETs by solving a quadratic
formulation. Afterwards, a detailed placement algorithm (named SeaPlace-D) is
proposed to increase the circuit reliability against METs by solving a linear
programming optimization problem. Experimental results show that SeaPlace-G and
SeaPlace-D averagely achieve 41.78% and 32.04% soft error reliability
improvement against SET and MET, respectively. Moreover, when SeaPlace-D is
followed by SeaPlace-G, MET reduction can be improved by up to 53.3%.

    

### [[2112.03997] Designing a Real-Time IoT Data Streaming Testbed for Horizontally Scalable Analytical Platforms: Czech Post Case Study](http://arxiv.org/abs/2112.03997)


  There is a growing trend for enterprise-level Internet of Things (IoT)
applications requiring real-time horizontally scalable data processing
platforms. Real-time processing platforms receiving data streams from sensor
networks (e.g., autonomous and connected vehicles, smart security for
businesses and homes, smartwatches, fitness trackers, and other wearables)
require distributed MQTT brokers. This case study presents an IoT data
streaming testbed platform prepared for the Czech Post. The presented platform
has met the throughput requirement of 2 million messages per 24 hours
(comprising SMS and emails). The tested MQTT broker runs on a single virtual
node of a horizontally scalable testbed platform. Soon the Czech Post will
modernise its eServices to increase package deliveries aligned with eCommerce
and eGovernment demands. The presented testbed platform fulfils all
requirements, and it is also capable of processing thousands of messages per
second. The presented platform and concepts are transferable to healthcare
systems, transport operations, the automotive industry, and other domains such
as smart cities.

    

### [[2112.04088] SASG: Sparsification with Adaptive Stochastic Gradients for Communication-efficient Distributed Learning](http://arxiv.org/abs/2112.04088)


  Stochastic optimization algorithms implemented on distributed computing
architectures are increasingly used to tackle large-scale machine learning
applications. A key bottleneck in such distributed systems is the communication
overhead for exchanging information such as stochastic gradients between
different workers. Sparse communication with memory and the adaptive
aggregation methodology are two successful frameworks among the various
techniques proposed to address this issue. In this paper, we creatively exploit
the advantages of Sparse communication and Adaptive aggregated Stochastic
Gradients to design a communication-efficient distributed algorithm named SASG.
Specifically, we first determine the workers that need to communicate based on
the adaptive aggregation rule and then sparse this transmitted information.
Therefore, our algorithm reduces both the overhead of communication rounds and
the number of communication bits in the distributed system. We define an
auxiliary sequence and give convergence results of the algorithm with the help
of Lyapunov function analysis. Experiments on training deep neural networks
show that our algorithm can significantly reduce the number of communication
rounds and bits compared to the previous methods, with little or no impact on
training and testing accuracy.

    

### [[2112.04405] Improved Distributed Fractional Coloring Algorithms](http://arxiv.org/abs/2112.04405)


  We prove new bounds on the distributed fractional coloring problem in the
LOCAL model. Fractional $c$-colorings can be understood as multicolorings as
follows. For some natural numbers $p$ and $q$ such that $p/q\leq c$, each node
$v$ is assigned a set of at least $q$ colors from $\{1,\dots,p\}$ such that
adjacent nodes are assigned disjoint sets of colors. The minimum $c$ for which
a fractional $c$-coloring of a graph $G$ exists is called the fractional
chromatic number $\chi_f(G)$ of $G$.
Recently, [Bousquet, Esperet, and Pirot; SIROCCO '21] showed that for any
constant $\epsilon>0$, a fractional $(\Delta+\epsilon)$-coloring can be
computed in $\Delta^{O(\Delta)} + O(\Delta\cdot\log^* n)$ rounds. We show that
such a coloring can be computed in only $O(\log^2 \Delta)$ rounds, without any
dependency on $n$.
We further show that in $O\big(\frac{\log n}{\epsilon}\big)$ rounds, it is
possible to compute a fractional $(1+\epsilon)\chi_f(G)$-coloring, even if the
fractional chromatic number $\chi_f(G)$ is not known. That is, this problem can
be approximated arbitrarily well by an efficient algorithm in the LOCAL model.
For the standard coloring problem, it is only known that an $O\big(\frac{\log
n}{\log\log n}\big)$-approximation can be computed in polylogarithmic time in
the LOCAL model. We also show that our distributed fractional coloring
approximation algorithm is best possible. We show that in trees, which have
fractional chromatic number $2$, computing a fractional $(2+\epsilon)$-coloring
requires at least $\Omega\big(\frac{\log n}{\epsilon}\big)$ rounds.
We finally study fractional colorings of regular grids. In [Bousquet,
Esperet, and Pirot; SIROCCO '21], it is shown that in regular grids of bounded
dimension, a fractional $(2+\epsilon)$-coloring can be computed in time
$O(\log^* n)$. We show that such a coloring can even be computed in $O(1)$
rounds in the LOCAL model.

    

### [[2112.03984] Emotion-Cause Pair Extraction in Customer Reviews](http://arxiv.org/abs/2112.03984)


  Emotion-Cause Pair Extraction (ECPE) is a complex yet popular area in Natural
Language Processing due to its importance and potential applications in various
domains. In this report , we aim to present our work in ECPE in the domain of
online reviews. With a manually annotated dataset, we explore an algorithm to
extract emotion cause pairs using a neural network. In addition, we propose a
model using previous reference materials and combining emotion-cause pair
extraction with research in the domain of emotion-aware word embeddings, where
we send these embeddings into a Bi-LSTM layer which gives us the emotionally
relevant clauses. With the constraint of a limited dataset, we achieved . The
overall scope of our report comprises of a comprehensive literature review,
implementation of referenced methods for dataset construction and initial model
training, and modifying previous work in ECPE by proposing an improvement to
the pipeline, as well as algorithm development and implementation for the
specific domain of reviews.

    

### [[2112.04087] Improving Knowledge Graph Representation Learning by Structure Contextual Pre-training](http://arxiv.org/abs/2112.04087)


  Representation learning models for Knowledge Graphs (KG) have proven to be
effective in encoding structural information and performing reasoning over KGs.
In this paper, we propose a novel pre-training-then-fine-tuning framework for
knowledge graph representation learning, in which a KG model is firstly
pre-trained with triple classification task, followed by discriminative
fine-tuning on specific downstream tasks such as entity type prediction and
entity alignment. Drawing on the general ideas of learning deep contextualized
word representations in typical pre-trained language models, we propose SCoP to
learn pre-trained KG representations with structural and contextual triples of
the target triple encoded. Experimental results demonstrate that fine-tuning
SCoP not only outperforms results of baselines on a portfolio of downstream
tasks but also avoids tedious task-specific model design and parameter
training.

    

### [[2112.04145] A Review for Deep Reinforcement Learning in Atari:Benchmarks, Challenges, and Solutions](http://arxiv.org/abs/2112.04145)


  The Arcade Learning Environment (ALE) is proposed as an evaluation platform
for empirically assessing the generality of agents across dozens of Atari 2600
games. ALE offers various challenging problems and has drawn significant
attention from the deep reinforcement learning (RL) community. From Deep
Q-Networks (DQN) to Agent57, RL agents seem to achieve superhuman performance
in ALE. However, is this the case? In this paper, to explore this problem, we
first review the current evaluation metrics in the Atari benchmarks and then
reveal that the current evaluation criteria of achieving superhuman performance
are inappropriate, which underestimated the human performance relative to what
is possible. To handle those problems and promote the development of RL
research, we propose a novel Atari benchmark based on human world records
(HWR), which puts forward higher requirements for RL agents on both final
performance and learning efficiency. Furthermore, we summarize the
state-of-the-art (SOTA) methods in Atari benchmarks and provide benchmark
results over new evaluation metrics based on human world records. We concluded
that at least four open challenges hinder RL agents from achieving superhuman
performance from those new benchmark results. Finally, we also discuss some
promising ways to handle those problems.

    

### [[2112.04150] BA-Net: Bridge Attention for Deep Convolutional Neural Networks](http://arxiv.org/abs/2112.04150)


  In recent years, channel attention mechanism is widely investigated for its
great potential in improving the performance of deep convolutional neural
networks (CNNs). However, in most existing methods, only the output of the
adjacent convolution layer is fed to the attention layer for calculating the
channel weights. Information from other convolution layers is ignored. With
these observations, a simple strategy, named Bridge Attention Net (BA-Net), is
proposed for better channel attention mechanisms. The main idea of this design
is to bridge the outputs of the previous convolution layers through skip
connections for channel weights generation. BA-Net can not only provide richer
features to calculate channel weight when feedforward, but also multiply paths
of parameters updating when backforward. Comprehensive evaluation demonstrates
that the proposed approach achieves state-of-the-art performance compared with
the existing methods in regards to accuracy and speed. Bridge Attention
provides a fresh perspective on the design of neural network architectures and
shows great potential in improving the performance of the existing channel
attention mechanisms. The code is available at
\url{this https URL


### [[2112.04154] SNEAK: Synonymous Sentences-Aware Adversarial Attack on Natural Language Video Localization](http://arxiv.org/abs/2112.04154)


  Natural language video localization (NLVL) is an important task in the
vision-language understanding area, which calls for an in-depth understanding
of not only computer vision and natural language side alone, but more
importantly the interplay between both sides. Adversarial vulnerability has
been well-recognized as a critical security issue of deep neural network
models, which requires prudent investigation. Despite its extensive yet
separated studies in video and language tasks, current understanding of the
adversarial robustness in vision-language joint tasks like NLVL is less
developed. This paper therefore aims to comprehensively investigate the
adversarial robustness of NLVL models by examining three facets of
vulnerabilities from both attack and defense aspects. To achieve the attack
goal, we propose a new adversarial attack paradigm called synonymous
sentences-aware adversarial attack on NLVL (SNEAK), which captures the
cross-modality interplay between the vision and language sides.

    

### [[2112.04169] Equity Promotion in Online Resource Allocation](http://arxiv.org/abs/2112.04169)


  We consider online resource allocation under a typical non-profit setting,
where limited or even scarce resources are administered by a not-for-profit
organization like a government. We focus on the internal-equity by assuming
that arriving requesters are homogeneous in terms of their external factors
like demands but heterogeneous for their internal attributes like demographics.
Specifically, we associate each arriving requester with one or several groups
based on their demographics (i.e., race, gender, and age), and we aim to design
an equitable distributing strategy such that every group of requesters can
receive a fair share of resources proportional to a preset target ratio. We
present two LP-based sampling algorithms and investigate them both
theoretically (in terms of competitive-ratio analysis) and experimentally based
on real COVID-19 vaccination data maintained by the Minnesota Department of
Health. Both theoretical and numerical results show that our LP-based sampling
strategies can effectively promote equity, especially when the arrival
population is disproportionately represented, as observed in the early stage of
the COVID-19 vaccine rollout.

    

### [[2112.04231] Towards automation of threat modeling based on a semantic model of attack patterns and weaknesses](http://arxiv.org/abs/2112.04231)


  This works considers challenges of building and usage a formal knowledge base
(model), which unites the ATT&CK, CAPEC, CWE, CVE security enumerations. The
proposed model can be used to learn relations between attack techniques, attack
pattern, weaknesses, and vulnerabilities in order to build various threat
landscapes, in particular, for threat modeling. The model is created as an
ontology with freely available datasets in the OWL and RDF formats. The use of
ontologies is an alternative of structural and graph based approaches to
integrate the security enumerations. In this work we consider an approach of
threat modeling with the data components of ATT&CK based on the knowledge base
and an ontology driven threat modeling framework. Also, some evaluations are
made, how it can be possible to use the ontological approach of threat modeling
and which challenges this can be faced.

    

### [[2112.04286] TempAMLSI : Temporal Action Model Learning based on Grammar Induction](http://arxiv.org/abs/2112.04286)


  Hand-encoding PDDL domains is generally accepted as difficult, tedious and
error-prone. The difficulty is even greater when temporal domains have to be
encoded. Indeed, actions have a duration and their effects are not
instantaneous. In this paper, we present TempAMLSI, an algorithm based on the
AMLSI approach able to learn temporal domains. TempAMLSI is based on the
classical assumption done in temporal planning that it is possible to convert a
non-temporal domain into a temporal domain. TempAMLSI is the first approach
able to learn temporal domain with single hard envelope and Cushing's
intervals. We show experimentally that TempAMLSI is able to learn accurate
temporal domains, i.e., temporal domain that can be used directly to solve new
planning problem, with different forms of action concurrency.

    

### [[2112.04289] iRoPro: An interactive Robot Programming Framework](http://arxiv.org/abs/2112.04289)


  The great diversity of end-user tasks ranging from manufacturing environments
to personal homes makes pre-programming robots for general purpose applications
extremely challenging. In fact, teaching robots new actions from scratch that
can be reused for previously unseen tasks remains a difficult challenge and is
generally left up to robotics experts. In this work, we present iRoPro, an
interactive Robot Programming framework that allows end-users with little to no
technical background to teach a robot new reusable actions. We combine
Programming by Demonstration and Automated Planning techniques to allow the
user to construct the robot's knowledge base by teaching new actions by
kinesthetic demonstration. The actions are generalised and reused with a task
planner to solve previously unseen problems defined by the user. We implement
iRoPro as an end-to-end system on a Baxter Research Robot to simultaneously
teach low- and high-level actions by demonstration that the user can customise
via a Graphical User Interface to adapt to their specific use case. To evaluate
the feasibility of our approach, we first conducted pre-design experiments to
better understand the user's adoption of involved concepts and the proposed
robot programming process. We compare results with post-design experiments,
where we conducted a user study to validate the usability of our approach with
real end-users. Overall, we showed that users with different programming levels
and educational backgrounds can easily learn and use iRoPro and its robot
programming process.

    

### [[2112.04359] Ethical and social risks of harm from Language Models](http://arxiv.org/abs/2112.04359)


  This paper aims to help structure the risk landscape associated with
large-scale Language Models (LMs). In order to foster advances in responsible
innovation, an in-depth understanding of the potential risks posed by these
models is needed. A wide range of established and anticipated risks are
analysed in detail, drawing on multidisciplinary expertise and literature from
computer science, linguistics, and social sciences.
We outline six specific risk areas: I. Discrimination, Exclusion and
Toxicity, II. Information Hazards, III. Misinformation Harms, V. Malicious
Uses, V. Human-Computer Interaction Harms, VI. Automation, Access, and
Environmental Harms. The first area concerns the perpetuation of stereotypes,
unfair discrimination, exclusionary norms, toxic language, and lower
performance by social group for LMs. The second focuses on risks from private
data leaks or LMs correctly inferring sensitive information. The third
addresses risks arising from poor, false or misleading information including in
sensitive domains, and knock-on risks such as the erosion of trust in shared
information. The fourth considers risks from actors who try to use LMs to cause
harm. The fifth focuses on risks specific to LLMs used to underpin
conversational agents that interact with human users, including unsafe use,
manipulation or deception. The sixth discusses the risk of environmental harm,
job automation, and other challenges that may have a disparate effect on
different social groups or communities.
In total, we review 21 risks in-depth. We discuss the points of origin of
different risks and point to potential mitigation approaches. Lastly, we
discuss organisational responsibilities in implementing mitigations, and the
role of collaboration and participation. We highlight directions for further
research, particularly on expanding the toolkit for assessing and evaluating
the outlined risks in LMs.

    

### [[2112.04363] Geometry-Aware Fruit Grasping Estimation for Robotic Harvesting in Orchards](http://arxiv.org/abs/2112.04363)


  Field robotic harvesting is a promising technique in recent development of
agricultural industry. It is vital for robots to recognise and localise fruits
before the harvesting in natural orchards. However, the workspace of harvesting
robots in orchards is complex: many fruits are occluded by branches and leaves.
It is important to estimate a proper grasping pose for each fruit before
performing the manipulation. In this study, a geometry-aware network, A3N, is
proposed to perform end-to-end instance segmentation and grasping estimation
using both color and geometry sensory data from a RGB-D camera. Besides,
workspace geometry modelling is applied to assist the robotic manipulation.
Moreover, we implement a global-to-local scanning strategy, which enables
robots to accurately recognise and retrieve fruits in field environments with
two consumer-level RGB-D cameras. We also evaluate the accuracy and robustness
of proposed network comprehensively in experiments. The experimental results
show that A3N achieves 0.873 on instance segmentation accuracy, with an average
computation time of 35 ms. The average accuracy of grasping estimation is 0.61
cm and 4.8$^{\circ}$ in centre and orientation, respectively. Overall, the
robotic system that utilizes the global-to-local scanning and A3N, achieves
success rate of harvesting ranging from 70\% - 85\% in field harvesting
experiments.

    

### [[2112.04367] On visual self-supervision and its effect on model robustness](http://arxiv.org/abs/2112.04367)


  Recent self-supervision methods have found success in learning feature
representations that could rival ones from full supervision, and have been
shown to be beneficial to the model in several ways: for example improving
models robustness and out-of-distribution detection. In our paper, we conduct
an empirical study to understand more precisely in what way can self-supervised
learning - as a pre-training technique or part of adversarial training -
affects model robustness to $l_2$ and $l_{\infty}$ adversarial perturbations
and natural image corruptions. Self-supervision can indeed improve model
robustness, however it turns out the devil is in the details. If one simply
adds self-supervision loss in tandem with adversarial training, then one sees
improvement in accuracy of the model when evaluated with adversarial
perturbations smaller or comparable to the value of $\epsilon_{train}$ that the
robust model is trained with. However, if one observes the accuracy for
$\epsilon_{test} \ge \epsilon_{train}$, the model accuracy drops. In fact, the
larger the weight of the supervision loss, the larger the drop in performance,
i.e. harming the robustness of the model. We identify primary ways in which
self-supervision can be added to adversarial training, and observe that using a
self-supervised loss to optimize both network parameters and find adversarial
examples leads to the strongest improvement in model robustness, as this can be
viewed as a form of ensemble adversarial training. Although self-supervised
pre-training yields benefits in improving adversarial training as compared to
random weight initialization, we observe no benefit in model robustness or
accuracy if self-supervision is incorporated into adversarial training.

    

### [[2112.04368] Semantic TrueLearn: Using Semantic Knowledge Graphs in Recommendation Systems](http://arxiv.org/abs/2112.04368)


  In informational recommenders, many challenges arise from the need to handle
the semantic and hierarchical structure between knowledge areas. This work aims
to advance towards building a state-aware educational recommendation system
that incorporates semantic relatedness between knowledge topics, propagating
latent information across semantically related topics. We introduce a novel
learner model that exploits this semantic relatedness between knowledge
components in learning resources using the Wikipedia link graph, with the aim
to better predict learner engagement and latent knowledge in a lifelong
learning scenario. In this sense, Semantic TrueLearn builds a humanly intuitive
knowledge representation while leveraging Bayesian machine learning to improve
the predictive performance of the educational engagement. Our experiments with
a large dataset demonstrate that this new semantic version of TrueLearn
algorithm achieves statistically significant improvements in terms of
predictive performance with a simple extension that adds semantic awareness to
the model.

    

### [[2112.04387] Truth-tracking via Approval Voting: Size Matters](http://arxiv.org/abs/2112.04387)


  Epistemic social choice aims at unveiling a hidden ground truth given votes,
which are interpreted as noisy signals about it. We consider here a simple
setting where votes consist of approval ballots: each voter approves a set of
alternatives which they believe can possibly be the ground truth. Based on the
intuitive idea that more reliable votes contain fewer alternatives, we define
several noise models that are approval voting variants of the Mallows model.
The likelihood-maximizing alternative is then characterized as the winner of a
weighted approval rule, where the weight of a ballot decreases with its
cardinality. We have conducted an experiment on three image annotation
datasets; they conclude that rules based on our noise model outperform standard
approval voting; the best performance is obtained by a variant of the Condorcet
noise model.

    

### [[2112.04406] Adapting Procedural Content Generation to Player Personas Through Evolution](http://arxiv.org/abs/2112.04406)


  Automatically adapting game content to players opens new doors for game
development. In this paper we propose an architecture using persona agents and
experience metrics, which enables evolving procedurally generated levels
tailored for particular player personas. Using our game, "Grave Rave", we
demonstrate that this approach successfully adapts to four rule-based persona
agents over three different experience metrics. Furthermore, the adaptation is
shown to be specific in nature, meaning that the levels are persona-conscious,
and not just general optimizations with regard to the selected metric.

    

### [[2003.01899] Robust Active Preference Elicitation](http://arxiv.org/abs/2003.01899)


  We study the problem of eliciting the preferences of a decision-maker through
a moderate number of pairwise comparison queries to make them a high quality
recommendation for a specific problem. We are motivated by applications in high
stakes domains, such as when choosing a policy for allocating scarce resources
to satisfy basic needs (e.g., kidneys for transplantation or housing for those
experiencing homelessness) where a consequential recommendation needs to be
made from the (partially) elicited preferences. We model uncertainty in the
preferences as being set based and} investigate two settings: a) an offline
elicitation setting, where all queries are made at once, and b) an online
elicitation setting, where queries are selected sequentially over time in an
adaptive fashion. We propose robust optimization formulations of these problems
which integrate the preference elicitation and recommendation phases with aim
to either maximize worst-case utility or minimize worst-case regret, and study
their complexity. For the offline case, where active preference elicitation
takes the form of a two and half stage robust optimization problem with
decision-dependent information discovery, we provide an equivalent
reformulation in the form of a mixed-binary linear program which we solve via
column-and-constraint generation. For the online setting, where active
preference learning takes the form of a multi-stage robust optimization problem
with decision-dependent information discovery, we propose a conservative
solution approach. Numerical studies on synthetic data demonstrate that our
methods outperform state-of-the art approaches from the literature in terms of
worst-case rank, regret, and utility. We showcase how our methodology can be
used to assist a homeless services agency in choosing a policy for allocating
scarce housing resources of different types to people experiencing
homelessness.

    

### [[2102.04640] Rethinking the Optimization of Average Precision: Only Penalizing Negative Instances before Positive Ones is Enough](http://arxiv.org/abs/2102.04640)


  Optimising the approximation of Average Precision (AP) has been widely
studied for image retrieval. Limited by the definition of AP, such methods
consider both negative and positive instances ranking before each positive
instance. However, we claim that only penalizing negative instances before
positive ones is enough, because the loss only comes from these negative
instances. To this end, we propose a novel loss, namely Penalizing Negative
instances before Positive ones (PNP), which can directly minimize the number of
negative instances before each positive one. In addition, AP-based methods
adopt a fixed and sub-optimal gradient assignment strategy. Therefore, we
systematically investigate different gradient assignment solutions via
constructing derivative functions of the loss, resulting in PNP-I with
increasing derivative functions and PNP-D with decreasing ones. PNP-I focuses
more on the hard positive instances by assigning larger gradients to them and
tries to make all relevant instances closer. In contrast, PNP-D pays less
attention to such instances and slowly corrects them. For most real-world data,
one class usually contains several local clusters. PNP-I blindly gathers these
clusters while PNP-D keeps them as they were. Therefore, PNP-D is more
superior. Experiments on three standard retrieval datasets show consistent
results with the above analysis. Extensive evaluations demonstrate that PNP-D
achieves the state-of-the-art performance. Code is available at
this https URL


### [[2103.10206] DanceFormer: Music Conditioned 3D Dance Generation with Parametric Motion Transformer](http://arxiv.org/abs/2103.10206)


  Generating 3D dances from music is an emerged research task that benefits a
lot of applications in vision and graphics. Previous works treat this task as
sequence generation, however, it is challenging to render a music-aligned
long-term sequence with high kinematic complexity and coherent movements. In
this paper, we reformulate it by a two-stage process, ie, a key pose generation
and then an in-between parametric motion curve prediction, where the key poses
are easier to be synchronized with the music beats and the parametric curves
can be efficiently regressed to render fluent rhythm-aligned movements. We
named the proposed method as DanceFormer, which includes two cascading
kinematics-enhanced transformer-guided networks (called DanTrans) that tackle
each stage, respectively. Furthermore, we propose a large-scale music
conditioned 3D dance dataset, called PhantomDance, that is accurately labeled
by experienced animators rather than reconstruction or motion capture. This
dataset also encodes dances as key poses and parametric motion curves apart
from pose sequences, thus benefiting the training of our DanceFormer. Extensive
experiments demonstrate that the proposed method, even trained by existing
datasets, can generate fluent, performative, and music-matched 3D dances that
surpass previous works quantitatively and qualitatively. Moreover, the proposed
DanceFormer, together with the PhantomDance dataset, are seamlessly compatible
with industrial animation software, thus facilitating the adaptation for
various downstream applications.

    

### [[2103.11139] MogFace: Towards a Deeper Appreciation on Face Detection](http://arxiv.org/abs/2103.11139)


  Benefiting from the pioneering design of generic object detectors,
significant achievements have been made in the field of face detection.
Typically, the architectures of the backbone, feature pyramid layer and
detection head module within the face detector all assimilate the excellent
experience from general object detectors. However, several effective methods,
including label assignment and scale-level data augmentation strategy
\footnote{enriches the scale distribution of the training data to resolve scale
variance challenge.}, fail to maintain consistent superiority when applying on
the face detector directly. Concretely, the former strategy involves a vast
body of hyper-parameters and the latter one suffers from the challenge of scale
distribution bias between different detection tasks, which both limit their
generalization abilities. Furthermore, in order to provide accurate face
bounding boxes for facial down-stream tasks, the face detector imperatively
requires the elimination of false alarms. As a result, practical solutions on
label assignment, scale-level data augmentation and reducing false alarms are
necessary for advancing face detector. In this paper, we focus on resolving
three aforementioned challenges that exiting methods are difficult to finish
off and present a novel face detector, termed as MogFace. In our Mogface, three
key components, Adaptive Online Incremental Anchor Mining Strategy, Selective
Scale Enhancement Strategy and Hierarchical Context-Aware Module, are
separately proposed to boost the performance of face detection. Finally, to the
best of our knowledge, our MogFace is the best face detector on the Wider Face
leader-board, achieving all champions across different testing scenarios. The
code is available at this https URL


### [[2106.05963] Learning to See by Looking at Noise](http://arxiv.org/abs/2106.05963)


  Current vision systems are trained on huge datasets, and these datasets come
with costs: curation is expensive, they inherit human biases, and there are
concerns over privacy and usage rights. To counter these costs, interest has
surged in learning from cheaper data sources, such as unlabeled images. In this
paper we go a step further and ask if we can do away with real image datasets
entirely, instead learning from noise processes. We investigate a suite of
image generation models that produce images from simple random processes. These
are then used as training data for a visual representation learner with a
contrastive loss. We study two types of noise processes, statistical image
models and deep generative models under different random initializations. Our
findings show that it is important for the noise to capture certain structural
properties of real data but that good performance can be achieved even with
processes that are far from realistic. We also find that diversity is a key
property to learn good representations. Datasets, models, and code are
available at this https URL.

    

### [[2106.08482] Minimizing Communication while Maximizing Performance in Multi-Agent Reinforcement Learning](http://arxiv.org/abs/2106.08482)


  Inter-agent communication can significantly increase performance in
multi-agent tasks that require co-ordination to achieve a shared goal. Prior
work has shown that it is possible to learn inter-agent communication protocols
using multi-agent reinforcement learning and message-passing network
architectures. However, these models use an unconstrained broadcast
communication model, in which an agent communicates with all other agents at
every step, even when the task does not require it. In real-world applications,
where communication may be limited by system constraints like bandwidth, power
and network capacity, one might need to reduce the number of messages that are
sent. In this work, we explore a simple method of minimizing communication
while maximizing performance in multi-task learning: simultaneously optimizing
a task-specific objective and a communication penalty. We show that the
objectives can be optimized using Reinforce and the Gumbel-Softmax
reparameterization. We introduce two techniques to stabilize training: 50%
training and message forwarding. Training with the communication penalty on
only 50% of the episodes prevents our models from turning off their outgoing
messages. Second, repeating messages received previously helps models retain
information, and further improves performance. With these techniques, we show
that we can reduce communication by 75% with no loss of performance.

    

### [[2107.00650] CLIP-It! Language-Guided Video Summarization](http://arxiv.org/abs/2107.00650)


  A generic video summary is an abridged version of a video that conveys the
whole story and features the most important scenes. Yet the importance of
scenes in a video is often subjective, and users should have the option of
customizing the summary by using natural language to specify what is important
to them. Further, existing models for fully automatic generic summarization
have not exploited available language models, which can serve as an effective
prior for saliency. This work introduces CLIP-It, a single framework for
addressing both generic and query-focused video summarization, typically
approached separately in the literature. We propose a language-guided
multimodal transformer that learns to score frames in a video based on their
importance relative to one another and their correlation with a user-defined
query (for query-focused summarization) or an automatically generated dense
video caption (for generic video summarization). Our model can be extended to
the unsupervised setting by training without ground-truth supervision. We
outperform baselines and prior work by a significant margin on both standard
video summarization datasets (TVSum and SumMe) and a query-focused video
summarization dataset (QFVS). Particularly, we achieve large improvements in
the transfer setting, attesting to our method's strong generalization
capabilities.

    

### [[2112.04352] Efficient Data Race Detection of Async-Finish Programs Using Vector Clocks](http://arxiv.org/abs/2112.04352)


  Existing data race detectors for task-based programs incur large run time and
space overheads. The overheads arise because of frequent lookups in
fine-grained tree data structures to check whether two accesses can happen in
parallel.
This work shows how to efficiently apply vector clocks for dynamic race
detection of async-finish programs with locks. Our proposed technique,
FastRacer, builds on the FastTrack algorithm with per-task and per-variable
optimizations to reduce the size of vector clocks. FastRacer also exploits the
structured parallelism of async-finish programs to use a coarse-grained
encoding of the dynamic task inheritance to limit the metadata in the presence
of many concurrent readers. Our evaluation shows that FastRacer substantially
improves time and space overheads over FastTrack and is competitive with the
state-of-the-art data race detectors for async-finish programs with locks.

    

### [<title data-react-helmet="true">英伟达官方免费课程！学用皮克斯USD框架，在主流3D仿真和协同应用中大显身手！ - 知乎</title>](https://zhuanlan.zhihu.com/p/443147336)