
## 2021-7-28

### [<title>CPP API XGBoosterPredict cost too much time! - XGBoost</title>](https://discuss.xgboost.ai/t/cpp-api-xgboosterpredict-cost-too-much-time/97/11)

### [<title>CPP API XGBoosterPredict cost too much time! - XGBoost</title>](https://discuss.xgboost.ai/t/cpp-api-xgboosterpredict-cost-too-much-time/97/10)

### [[2107.12433] The Graph Neural Networking Challenge: A Worldwide Competition for Education in AI/ML for Networks](http://arxiv.org/abs/2107.12433)


  During the last decade, Machine Learning (ML) has increasingly become a hot
topic in the field of Computer Networks and is expected to be gradually adopted
for a plethora of control, monitoring and management tasks in real-world
deployments. This poses the need to count on new generations of students,
researchers and practitioners with a solid background in ML applied to
networks. During 2020, the International Telecommunication Union (ITU) has
organized the "ITU AI/ML in 5G challenge'', an open global competition that has
introduced to a broad audience some of the current main challenges in ML for
networks. This large-scale initiative has gathered 23 different challenges
proposed by network operators, equipment manufacturers and academia, and has
attracted a total of 1300+ participants from 60+ countries. This paper narrates
our experience organizing one of the proposed challenges: the "Graph Neural
Networking Challenge 2020''. We describe the problem presented to participants,
the tools and resources provided, some organization aspects and participation
statistics, an outline of the top-3 awarded solutions, and a summary with some
lessons learned during all this journey. As a result, this challenge leaves a
curated set of educational resources openly available to anyone interested in
the topic.

    

### [[2107.12765] Resource Optimization with Interference Coupling in Multi-IRS-assisted Multi-cell Systems](http://arxiv.org/abs/2107.12765)


  Deploying Intelligent reflecting surfaces (IRSs) to enhance wireless
transmission is a promising approach. In this paper, we investigate large-scale
multi-IRS-assisted multi-cell systems, where multiple IRSs are deployed in each
cell. Different from the full-buffer scenario, the mutual interference in our
system is not known a priori, and for this reason we apply the load coupling
model to analyze this system. The objective is to minimize the total resource
consumption subject to user demand requirement by optimizing the reflection
coefficients in the cells. The cells are highly coupled and the overall problem
is non-convex. To tackle this, we first investigate the single-cell case with
given interference, and propose a low-complexity algorithm based on the
Majorization-Minimization (MM) method to obtain a locally optimal solution.
Then, we embed this algorithm into an algorithmic framework for the overall
multi-cell problem, and prove its feasibility and convergence to a solution
that is at least locally optimal. Simulation results demonstrate the benefit of
IRS in time-frequency resource utilization in the multi-cell system.

    

### [[2107.12766] Beyond 5G: Big Data Processing for Better Spectrum Utilization](http://arxiv.org/abs/2107.12766)


  This article emphasizes the great potential of big data processing for
advanced user- and situation-oriented, so context-aware resource utilization in
future wireless networks. In particular, we consider the application of
dedicated, detailed, and rich-in-content maps and records called Radio Service
Maps, (RSM) for unlocking the spectrum opportunities in 6G networks. Due to the
characteristics of 5G, in the future, there will be a need for high convergence
of various types of wireless networks, such as cellular and the
Internet-of-Things (IoT) networks, which are steadily growing and consequently
considered as the studied use case in this work. We show that the 6G network
significantly benefits from effective Dynamic Spectrum management (DSM) based
on RSM which provides rich and accurate knowledge of the radio context; a
knowledge that is stored and processed within database-oriented subsystems
designed to support wireless networks for improving spectral efficiency. In
this article, we discuss context-aware RSM subsystem architecture and operation
for DSM in convergent 6G radio and IoT networks. By providing various
use-cases, we demonstrate that the accurate definition and access to the rich
context information lead to a significant improvement of the system
performance. In consequence, we also claim that efficient big-data processing
algorithms will be necessary for future applications.

    

### [[2107.12816] Dynamic Power and Frequency Allocation Scheme for Autonomous Platooning](http://arxiv.org/abs/2107.12816)


  In this paper, we consider the use of radio environment maps (REMs) in
vehicular dynamic spectrum access (VDSA) for vehicle platooning applications.
We propose an algorithm that dynamically allocates the frequency bands and
transmission power in the so-called TV white spaces (TVWS) for intra-platoon
messaging, intending to maximize the reliability of the communications,
simultaneously keeping the interference to the primary system below the
required threshold. The proposed solution is evaluated in simulations, with the
results indicating a significant increase in communications reliability with
VDSA.

    

### [[2107.12912] AToM: Active Topology Monitoring for the Bitcoin Peer-to-Peer Network](http://arxiv.org/abs/2107.12912)


  Over the past decade, the Bitcoin P2P network protocol has become a reference
model for all modern cryptocurrencies. While nodes in this network are known,
the connections among them are kept hidden, as it is commonly believed that
this helps protect from deanonymization and low-level attacks. However,
adversaries can bypass this limitation by inferring connections through side
channels. At the same time, the lack of topology information hinders the
analysis of the network, which is essential to improve efficiency and security.
In this paper, we thoroughly review network-level attacks and empirically show
that topology obfuscation is not an effective countermeasure. We then argue
that the benefits of an open topology potentially outweigh its risks, and
propose a protocol to reliably infer and monitor connections among reachable
nodes of the Bitcoin network. We formally analyze our protocol and
experimentally evaluate its accuracy in both trusted and untrusted settings.
Results show our system has a low impact on the network, and has precision and
recall are over 90% with up to 20% of malicious nodes in the network.

    

### [[2102.12737] IPFS and Friends: A Qualitative Comparison of Next Generation Peer-to-Peer Data Networks](http://arxiv.org/abs/2102.12737)


  Decentralized, distributed storage offers a way to reduce the impact of data
silos as often fostered by centralized cloud storage. While the intentions of
this trend are not new, the topic gained traction due to technological
advancements, most notably blockchain networks. As a consequence, we observe
that a new generation of peer-to-peer data networks emerges. In this survey
paper, we therefore provide a technical overview of the next generation data
networks. We use select data networks to introduce general concepts and to
emphasize new developments. Specifically, we provide a deeper outline of the
Interplanetary File System and a general overview of Swarm, the Hypercore
Protocol, SAFE, Storj, and Arweave. We identify common building blocks and
provide a qualitative comparison. From the overview, we derive future
challenges and research goals concerning data networks.

    

### [[2104.09462] Heads in the Clouds: Measuring the Implications of Universities Migrating to Public Clouds](http://arxiv.org/abs/2104.09462)


  With the emergence of mandatory remote education and work in universities due
to COVID-19, the `zoomification' of higher education, i.e., the migration of
universities to the clouds, reached the public discourse. Ongoing discussions
reason about how this shift will take control over students' data away from
universities, and may ultimately prevent privacy from being an attainable goal
for researchers and students alike. However, there has been no comprehensive
measurement of universities' use of public clouds and reliance on
Software-as-a-Service offerings to assess how far this migration has already
progressed.
In this paper, we perform a longitudinal study of the migration to public
clouds among universities in the U.S. and Europe, as well as institutions
listed in the Times Higher Education (THE) Top100 between January 2015 and June
2021. We find that cloud-adoption differs between countries, with one cluster
(Germany, France, Austria, Switzerland) showing a limited move to clouds, while
the other cluster (U.S., U.K., the Netherlands, THE Top100) frequently migrates
universities' core functions and services to public clouds -- starting long
before the COVID-19 pandemic. We attribute this clustering to several
socio-economic factors in the respective countries, including the general
culture of higher education and the administrative paradigm taken towards
running universities. We then analyze and interpret our results, finding that
the implications reach beyond individuals' privacy towards questions of
academic independence and integrity.

    

### [[2104.10536] Multi-RAT for IoT: The Potential in Combining LoRaWAN and NB-IoT](http://arxiv.org/abs/2104.10536)


  The broad range of requirements of Internet of Things applications has lead
to the development of several dedicated communication technologies, each
tailored to meet a specific feature set. A solution combining different
wireless technologies in one device, can overcome the disadvantages of any
individual technology. The design of such Multiple Radio Access Technology
solutions based on the diverse characteristics of the technologies offers
interesting opportunities. In this work we analyze the potential of combining
LoRaWAN and NB-IoT in a Multi-RAT solution for IoT. To that end we evaluate key
IoT node requirements in function of payload size and link quality: (1) energy
efficiency, (2) coverage, (3) payload size, (4) latency performance, (5)
Quality of Service, and (6) cost efficiency. Our theoretical assessment and
experimental validation of these IoT features show the merits of a Multi-RAT
solution. Notably, energy consumption in use cases with only sporadic large
payload requirements, can be improved by a factor of at least 4 with respect to
either single-mode technologies. Moreover, latency-critical messages can get
delivered on time and coverage can be extended elegantly where needed.

    

### [[2105.09687] Secure, Anonymity-Preserving and Lightweight Mutual Authentication and Key Agreement Protocol for Home Automation IoT Networks](http://arxiv.org/abs/2105.09687)


  Home automation Internet of Things (IoT) systems have recently become a
target for several types of attacks. In this paper, we present an
authentication and key agreement protocol for a home automation network based
on the ZigBee standard, which connects together a central controller and
several end devices. Our scheme performs mutual authentication between end
devices and the controller, which is followed by device-to-device
communication. The scheme achieves confidentiality, message integrity,
anonymity, unlinkability, forward and backward secrecy, and availability. Our
scheme uses only simple hash and XOR computations and symmetric key encryption,
and hence is resource-efficient. We show using a detailed security analysis and
numerical results that our proposed scheme provides better security and
anonymity, and is more efficient in terms of computation time, communication
cost, and storage cost than schemes proposed in prior works.

    

### [[2106.15262] MU-MIMO Grouping For Real-time Applications](http://arxiv.org/abs/2106.15262)


  Over the last decade, the bandwidth expansion and MU-MIMO spectral efficiency
have promised to increase data throughput by allowing concurrent communication
between one Access Point and multiple users. However, we are still a long way
from enjoying such MU-MIMO MAC protocol improvements for bandwidth hungry
applications such as video streaming in practical WiFi network settings due to
heterogeneous channel conditions and devices, unreliable transmissions, and
lack of useful feedback exchange among the lower and upper layers'
requirements. This paper introduces MuViS, a novel dual-phase optimization
framework that proposes a Quality of Experience (QoE) aware MU-MIMO
optimization for multi-user video streaming over IEEE 802.11ac. MuViS first
employs reinforcement learning to optimize the MU-MIMO user group and mode
selection for users based on their PHY/MAC layer characteristics. The video
bitrate is then optimized based on the user's mode (Multi-User (MU) or
Single-User (SU)). We present our design and its evaluation on smartphones and
laptops using 802.11ac WiFi. Our experimental results in various indoor
environments and configurations show a scalable framework that can support a
large number of users with streaming at high video rates and satisfying QoE
requirements.

    

### [[2107.12373] Relational Boosted Regression Trees](http://arxiv.org/abs/2107.12373)


  Many tasks use data housed in relational databases to train boosted
regression tree models. In this paper, we give a relational adaptation of the
greedy algorithm for training boosted regression trees. For the subproblem of
calculating the sum of squared residuals of the dataset, which dominates the
runtime of the boosting algorithm, we provide a $(1 + \epsilon)$-approximation
using the tensor sketch technique. Employing this approximation within the
relational boosted regression trees algorithm leads to learning similar model
parameters, but with asymptotically better runtime.

    

### [[2107.12375] Geometric Deep Learning on Molecular Representations](http://arxiv.org/abs/2107.12375)


  Geometric deep learning (GDL), which is based on neural network architectures
that incorporate and process symmetry information, has emerged as a recent
paradigm in artificial intelligence. GDL bears particular promise in molecular
modeling applications, in which various molecular representations with
different symmetry properties and levels of abstraction exist. This review
provides a structured and harmonized overview of molecular GDL, highlighting
its applications in drug discovery, chemical synthesis prediction, and quantum
chemistry. Emphasis is placed on the relevance of the learned molecular
features and their complementarity to well-established molecular descriptors.
This review provides an overview of current challenges and opportunities, and
presents a forecast of the future of GDL for molecular sciences.

    

### [[2107.12395] Constraining dark matter annihilation with cosmic ray antiprotons using neural networks](http://arxiv.org/abs/2107.12395)


  The interpretation of data from indirect detection experiments searching for
dark matter annihilations requires computationally expensive simulations of
cosmic-ray propagation. In this work we present a new method based on Recurrent
Neural Networks that significantly accelerates simulations of secondary and
dark matter Galactic cosmic ray antiprotons while achieving excellent accuracy.
This approach allows for an efficient profiling or marginalisation over the
nuisance parameters of a cosmic ray propagation model in order to perform
parameter scans for a wide range of dark matter models. We identify importance
sampling as particularly suitable for ensuring that the network is only
evaluated in well-trained parameter regions. We present resulting constraints
using the most recent AMS-02 antiproton data on several models of Weakly
Interacting Massive Particles. The fully trained networks are released as
DarkRayNet together with this work and achieve a speed-up of the runtime by at
least two orders of magnitude compared to conventional approaches.

    

### [[2107.12416] Asynchronous Distributed Reinforcement Learning for LQR Control via Zeroth-Order Block Coordinate Descent](http://arxiv.org/abs/2107.12416)


  Recently introduced distributed zeroth-order optimization (ZOO) algorithms
have shown their utility in distributed reinforcement learning (RL).
Unfortunately, in the gradient estimation process, almost all of them require
random samples with the same dimension as the global variable and/or require
evaluation of the global cost function, which may induce high estimation
variance for large-scale networks. In this paper, we propose a novel
distributed zeroth-order algorithm by leveraging the network structure inherent
in the optimization objective, which allows each agent to estimate its local
gradient by local cost evaluation independently, without use of any consensus
protocol. The proposed algorithm exhibits an asynchronous update scheme, and is
designed for stochastic non-convex optimization with a possibly non-convex
feasible domain based on the block coordinate descent method. The algorithm is
later employed as a distributed model-free RL algorithm for distributed linear
quadratic regulator design, where a learning graph is designed to describe the
required interaction relationship among agents in distributed learning. We
provide an empirical validation of the proposed algorithm to benchmark its
performance on convergence rate and variance against a centralized ZOO
algorithm.

    

### [[2107.12421] Parallel Surrogate-assisted Optimization Using Mesh Adaptive Direct Search](http://arxiv.org/abs/2107.12421)


  We consider computationally expensive blackbox optimization problems and
present a method that employs surrogate models and concurrent computing at the
search step of the mesh adaptive direct search (MADS) algorithm. Specifically,
we solve a surrogate optimization problem using locally weighted scatterplot
smoothing (LOWESS) models to find promising candidate points to be evaluated by
the blackboxes. We consider several methods for selecting promising points from
a large number of points. We conduct numerical experiments to assess the
performance of the modified MADS algorithm with respect to available CPU
resources by means of five engineering design problems.

    

### [[2107.12436] Feature Synergy, Redundancy, and Independence in Global Model Explanations using SHAP Vector Decomposition](http://arxiv.org/abs/2107.12436)


  We offer a new formalism for global explanations of pairwise feature
dependencies and interactions in supervised models. Building upon SHAP values
and SHAP interaction values, our approach decomposes feature contributions into
synergistic, redundant and independent components (S-R-I decomposition of SHAP
vectors). We propose a geometric interpretation of the components and formally
prove its basic properties. Finally, we demonstrate the utility of synergy,
redundancy and independence by applying them to a constructed data set and
model.

    

### [[2107.12438] Debiasing In-Sample Policy Performance for Small-Data, Large-Scale Optimization](http://arxiv.org/abs/2107.12438)


  Motivated by the poor performance of cross-validation in settings where data
are scarce, we propose a novel estimator of the out-of-sample performance of a
policy in data-driven optimization.Our approach exploits the optimization
problem's sensitivity analysis to estimate the gradient of the optimal
objective value with respect to the amount of noise in the data and uses the
estimated gradient to debias the policy's in-sample performance. Unlike
cross-validation techniques, our approach avoids sacrificing data for a test
set, utilizes all data when training and, hence, is well-suited to settings
where data are scarce. We prove bounds on the bias and variance of our
estimator for optimization problems with uncertain linear objectives but known,
potentially non-convex, feasible regions. For more specialized optimization
problems where the feasible region is ``weakly-coupled" in a certain sense, we
prove stronger results. Specifically, we provide explicit high-probability
bounds on the error of our estimator that hold uniformly over a policy class
and depends on the problem's dimension and policy class's complexity. Our
bounds show that under mild conditions, the error of our estimator vanishes as
the dimension of the optimization problem grows, even if the amount of
available data remains small and constant. Said differently, we prove our
estimator performs well in the small-data, large-scale regime. Finally, we
numerically compare our proposed method to state-of-the-art approaches through
a case-study on dispatching emergency medical response services using real
data. Our method provides more accurate estimates of out-of-sample performance
and learns better-performing policies.

    

### [[2107.12445] Towards Low-Latency Energy-Efficient Deep SNNs via Attention-Guided Compression](http://arxiv.org/abs/2107.12445)


  Deep spiking neural networks (SNNs) have emerged as a potential alternative
to traditional deep learning frameworks, due to their promise to provide
increased compute efficiency on event-driven neuromorphic hardware. However, to
perform well on complex vision applications, most SNN training frameworks yield
large inference latency which translates to increased spike activity and
reduced energy efficiency. Hence,minimizing average spike activity while
preserving accuracy indeep SNNs remains a significant challenge and
opportunity.This paper presents a non-iterative SNN training technique
thatachieves ultra-high compression with reduced spiking activitywhile
maintaining high inference accuracy. In particular, our framework first uses
the attention-maps of an un compressed meta-model to yield compressed ANNs.
This step can be tuned to support both irregular and structured channel pruning
to leverage computational benefits over a broad range of platforms. The
framework then performs sparse-learning-based supervised SNN training using
direct inputs. During the training, it jointly optimizes the SNN weight,
threshold, and leak parameters to drastically minimize the number of time steps
required while retaining compression. To evaluate the merits of our approach,
we performed experiments with variants of VGG and ResNet, on both CIFAR-10 and
CIFAR-100, and VGG16 on Tiny-ImageNet.The SNN models generated through the
proposed technique yield SOTA compression ratios of up to 33.4x with no
significant drops in accuracy compared to baseline unpruned counterparts.
Compared to existing SNN pruning methods, we achieve up to 8.3x higher
compression with improved accuracy.

    

### [[2107.12452] Accelerated Gradient Descent Learning over Multiple Access Fading Channels](http://arxiv.org/abs/2107.12452)


  We consider a distributed learning problem in a wireless network, consisting
of N distributed edge devices and a parameter server (PS). The objective
function is a sum of the edge devices' local loss functions, who aim to train a
shared model by communicating with the PS over multiple access channels (MAC).
This problem has attracted a growing interest in distributed sensing systems,
and more recently in federated learning, known as over-the-air computation. In
this paper, we develop a novel Accelerated Gradient-descent Multiple Access
(AGMA) algorithm that uses momentum-based gradient signals over noisy fading
MAC to improve the convergence rate as compared to existing methods.
Furthermore, AGMA does not require power control or beamforming to cancel the
fading effect, which simplifies the implementation complexity. We analyze AGMA
theoretically, and establish a finite-sample bound of the error for both convex
and strongly convex loss functions with Lipschitz gradient. For the strongly
convex case, we show that AGMA approaches the best-known linear convergence
rate as the network increases. For the convex case, we show that AGMA
significantly improves the sub-linear convergence rate as compared to existing
methods. Finally, we present simulation results using real datasets that
demonstrate better performance by AGMA.

    

### [[2107.12455] Combining Reward and Rank Signals for Slate Recommendation](http://arxiv.org/abs/2107.12455)


  We consider the problem of slate recommendation, where the recommender system
presents a user with a collection or slate composed of K recommended items at
once. If the user finds the recommended items appealing then the user may click
and the recommender system receives some feedback. Two pieces of information
are available to the recommender system: was the slate clicked? (the reward),
and if the slate was clicked, which item was clicked? (rank). In this paper, we
formulate several Bayesian models that incorporate the reward signal (Reward
model), the rank signal (Rank model), or both (Full model), for
non-personalized slate recommendation. In our experiments, we analyze
performance gains of the Full model and show that it achieves significantly
lower error as the number of products in the catalog grows or as the slate size
increases.

    

### [[2107.12460] Don't Sweep your Learning Rate under the Rug: A Closer Look at Cross-modal Transfer of Pretrained Transformers](http://arxiv.org/abs/2107.12460)


  Self-supervised pre-training of large-scale transformer models on text
corpora followed by finetuning has achieved state-of-the-art on a number of
natural language processing tasks. Recently, Lu et al. (2021, arXiv:2103.05247)
claimed that frozen pretrained transformers (FPTs) match or outperform training
from scratch as well as unfrozen (fine-tuned) pretrained transformers in a set
of transfer tasks to other modalities. In our work, we find that this result
is, in fact, an artifact of not tuning the learning rates. After carefully
redesigning the empirical setup, we find that when tuning learning rates
properly, pretrained transformers do outperform or match training from scratch
in all of our tasks, but only as long as the entire model is finetuned. Thus,
while transfer from pretrained language models to other modalities does indeed
provide gains and hints at exciting possibilities for future work, properly
tuning hyperparameters is important for arriving at robust findings.

    

### [[2107.12466] High-Dimensional Distribution Generation Through Deep Neural Networks](http://arxiv.org/abs/2107.12466)


  We show that every $d$-dimensional probability distribution of bounded
support can be generated through deep ReLU networks out of a $1$-dimensional
uniform input distribution. What is more, this is possible without incurring a
cost - in terms of approximation error measured in Wasserstein-distance -
relative to generating the $d$-dimensional target distribution from $d$
independent random variables. This is enabled by a vast generalization of the
space-filling approach discovered in (Bailey & Telgarsky, 2018). The
construction we propose elicits the importance of network depth in driving the
Wasserstein distance between the target distribution and its neural network
approximation to zero. Finally, we find that, for histogram target
distributions, the number of bits needed to encode the corresponding generative
network equals the fundamental limit for encoding probability distributions as
dictated by quantization theory.

    

### [[2107.12480] Circular-Symmetric Correlation Layer based on FFT](http://arxiv.org/abs/2107.12480)


  Despite the vast success of standard planar convolutional neural networks,
they are not the most efficient choice for analyzing signals that lie on an
arbitrarily curved manifold, such as a cylinder. The problem arises when one
performs a planar projection of these signals and inevitably causes them to be
distorted or broken where there is valuable information. We propose a
Circular-symmetric Correlation Layer (CCL) based on the formalism of
roto-translation equivariant correlation on the continuous group $S^1 \times
\mathbb{R}$, and implement it efficiently using the well-known Fast Fourier
Transform (FFT) algorithm. We showcase the performance analysis of a general
network equipped with CCL on various recognition and classification tasks and
datasets. The PyTorch package implementation of CCL is provided online.

    

### [[2107.12490] LEGATO: A LayerwisE Gradient AggregaTiOn Algorithm for Mitigating Byzantine Attacks in Federated Learning](http://arxiv.org/abs/2107.12490)


  Federated learning has arisen as a mechanism to allow multiple participants
to collaboratively train a model without sharing their data. In these settings,
participants (workers) may not trust each other fully; for instance, a set of
competitors may collaboratively train a machine learning model to detect fraud.
The workers provide local gradients that a central server uses to update a
global model. This global model can be corrupted when Byzantine workers send
malicious gradients, which necessitates robust methods for aggregating
gradients that mitigate the adverse effects of Byzantine inputs. Existing
robust aggregation algorithms are often computationally expensive and only
effective under strict assumptions. In this paper, we introduce LayerwisE
Gradient AggregatTiOn (LEGATO), an aggregation algorithm that is, by contrast,
scalable and generalizable. Informed by a study of layer-specific responses of
gradients to Byzantine attacks, LEGATO employs a dynamic gradient reweighing
scheme that is novel in its treatment of gradients based on layer-specific
robustness. We show that LEGATO is more computationally efficient than multiple
state-of-the-art techniques and more generally robust across a variety of
attack settings in practice. We also demonstrate LEGATO's benefits for gradient
descent convergence in the absence of an attack.

    

### [[2107.12501] Adversarial Random Forest Classifier for Automated Game Design](http://arxiv.org/abs/2107.12501)


  Autonomous game design, generating games algorithmically, has been a longtime
goal within the technical games research field. However, existing autonomous
game design systems have relied in large part on human-authoring for game
design knowledge, such as fitness functions in search-based methods. In this
paper, we describe an experiment to attempt to learn a human-like fitness
function for autonomous game design in an adversarial manner. While our
experimental work did not meet our expectations, we present an analysis of our
system and results that we hope will be informative to future autonomous game
design research.

    

### [[2107.12506] TaikoNation: Patterning-focused Chart Generation for Rhythm Action Games](http://arxiv.org/abs/2107.12506)


  Generating rhythm game charts from songs via machine learning has been a
problem of increasing interest in recent years. However, all existing systems
struggle to replicate human-like patterning: the placement of game objects in
relation to each other to form congruent patterns based on events in the song.
Patterning is a key identifier of high quality rhythm game content, seen as a
necessary component in human rankings. We establish a new approach for chart
generation that produces charts with more congruent, human-like patterning than
seen in prior work.

    

### [[2107.12514] Language Grounding with 3D Objects](http://arxiv.org/abs/2107.12514)


  Seemingly simple natural language requests to a robot are generally
underspecified, for example "Can you bring me the wireless mouse?" When viewing
mice on the shelf, the number of buttons or presence of a wire may not be
visible from certain angles or positions. Flat images of candidate mice may not
provide the discriminative information needed for "wireless". The world, and
objects in it, are not flat images but complex 3D shapes. If a human requests
an object based on any of its basic properties, such as color, shape, or
texture, robots should perform the necessary exploration to accomplish the
task. In particular, while substantial effort and progress has been made on
understanding explicitly visual attributes like color and category,
comparatively little progress has been made on understanding language about
shapes and contours. In this work, we introduce a novel reasoning task that
targets both visual and non-visual language about 3D objects. Our new
benchmark, ShapeNet Annotated with Referring Expressions (SNARE), requires a
model to choose which of two objects is being referenced by a natural language
description. We introduce several CLIP-based models for distinguishing objects
and demonstrate that while recent advances in jointly modeling vision and
language are useful for robotic language understanding, it is still the case
that these models are weaker at understanding the 3D nature of objects --
properties which play a key role in manipulation. In particular, we find that
adding view estimation to language grounding models improves accuracy on both
SNARE and when identifying objects referred to in language on a robot platform.

    

### [[2107.12521] Restricted Boltzmann Machine and Deep Belief Network: Tutorial and Survey](http://arxiv.org/abs/2107.12521)


  This is a tutorial and survey paper on Boltzmann Machine (BM), Restricted
Boltzmann Machine (RBM), and Deep Belief Network (DBN). We start with the
required background on probabilistic graphical models, Markov random field,
Gibbs sampling, statistical physics, Ising model, and the Hopfield network.
Then, we introduce the structures of BM and RBM. The conditional distributions
of visible and hidden variables, Gibbs sampling in RBM for generating
variables, training BM and RBM by maximum likelihood estimation, and
contrastive divergence are explained. Then, we discuss different possible
discrete and continuous distributions for the variables. We introduce
conditional RBM and how it is trained. Finally, we explain deep belief network
as a stack of RBM models. This paper on Boltzmann machines can be useful in
various fields including data science, statistics, neural computation, and
statistical physics.

    

### [[2107.12524] Ensemble Learning For Mega Man Level Generation](http://arxiv.org/abs/2107.12524)


  Procedural content generation via machine learning (PCGML) is the process of
procedurally generating game content using models trained on existing game
content. PCGML methods can struggle to capture the true variance present in
underlying data with a single model. In this paper, we investigated the use of
ensembles of Markov chains for procedurally generating \emph{Mega Man} levels.
We conduct an initial investigation of our approach and evaluate it on measures
of playability and stylistic similarity in comparison to a non-ensemble,
existing Markov chain approach.

    

### [[2107.12525] Proof: Accelerating Approximate Aggregation Queries with Expensive Predicates](http://arxiv.org/abs/2107.12525)


  Given a dataset $\mathcal{D}$, we are interested in computing the mean of a
subset of $\mathcal{D}$ which matches a predicate. \algname leverages
stratified sampling and proxy models to efficiently compute this statistic
given a sampling budget $N$. In this document, we theoretically analyze
\algname and show that the MSE of the estimate decays at rate $O(N_1^{-1} +
N_2^{-1} + N_1^{1/2}N_2^{-3/2})$, where $N=K \cdot N_1+N_2$ for some integer
constant $K$ and $K \cdot N_1$ and $N_2$ represent the number of samples used
in Stage 1 and Stage 2 of \algname respectively. Hence, if a constant fraction
of the total sample budget $N$ is allocated to each stage, we will achieve a
mean squared error of $O(N^{-1})$ which matches the rate of mean squared error
of the optimal stratified sampling algorithm given a priori knowledge of the
predicate positive rate and standard deviation per stratum.

    

### [[2107.12527] Physics-Enforced Modeling for Insertion Loss of Transmission Lines by Deep Neural Networks](http://arxiv.org/abs/2107.12527)


  In this paper, we investigate data-driven parameterized modeling of insertion
loss for transmission lines with respect to design parameters. We first show
that direct application of neural networks can lead to non-physics models with
negative insertion loss. To mitigate this problem, we propose two deep learning
solutions. One solution is to add a regulation term, which represents the
passive condition, to the final loss function to enforce the negative quantity
of insertion loss. In the second method, a third-order polynomial expression is
defined first, which ensures positiveness, to approximate the insertion loss,
then DeepONet neural network structure, which was proposed recently for
function and system modeling, was employed to model the coefficients of
polynomials. The resulting neural network is applied to predict the
coefficients of the polynomial expression. The experimental results on an
open-sourced SI/PI database of a PCB design show that both methods can ensure
the positiveness for the insertion loss. Furthermore, both methods can achieve
similar prediction results, while the polynomial-based DeepONet method is
faster than DeepONet based method in training time.

    

### [[2107.12530] Convergence of Deep ReLU Networks](http://arxiv.org/abs/2107.12530)


  We explore convergence of deep neural networks with the popular ReLU
activation function, as the depth of the networks tends to infinity. To this
end, we introduce the notion of activation domains and activation matrices of a
ReLU network. By replacing applications of the ReLU activation function by
multiplications with activation matrices on activation domains, we obtain an
explicit expression of the ReLU network. We then identify the convergence of
the ReLU networks as convergence of a class of infinite products of matrices.
Sufficient and necessary conditions for convergence of these infinite products
of matrices are studied. As a result, we establish necessary conditions for
ReLU networks to converge that the sequence of weight matrices converges to the
identity matrix and the sequence of the bias vectors converges to zero as the
depth of ReLU networks increases to infinity. Moreover, we obtain sufficient
conditions in terms of the weight matrices and bias vectors at hidden layers
for pointwise convergence of deep ReLU networks. These results provide
mathematical insights to the design strategy of the well-known deep residual
networks in image classification.

    

### [[2107.12532] Generating Lode Runner Levels by Learning Player Paths with LSTMs](http://arxiv.org/abs/2107.12532)


  Machine learning has been a popular tool in many different fields, including
procedural content generation. However, procedural content generation via
machine learning (PCGML) approaches can struggle with controllability and
coherence. In this paper, we attempt to address these problems by learning to
generate human-like paths, and then generating levels based on these paths. We
extract player path data from gameplay video, train an LSTM to generate new
paths based on this data, and then generate game levels based on this path
data. We demonstrate that our approach leads to more coherent levels for the
game Lode Runner in comparison to an existing PCGML approach.

    

### [[2107.12533] Toward Co-creative Dungeon Generation via Transfer Learning](http://arxiv.org/abs/2107.12533)


  Co-creative Procedural Content Generation via Machine Learning (PCGML) refers
to systems where a PCGML agent and a human work together to produce output
content. One of the limitations of co-creative PCGML is that it requires
co-creative training data for a PCGML agent to learn to interact with humans.
However, acquiring this data is a difficult and time-consuming process. In this
work, we propose approximating human-AI interaction data and employing transfer
learning to adapt learned co-creative knowledge from one game to a different
game. We explore this approach for co-creative Zelda dungeon room generation.

    

### [[2107.12547] Probing neural networks with t-SNE, class-specific projections and a guided tour](http://arxiv.org/abs/2107.12547)


  We use graphical methods to probe neural nets that classify images. Plots of
t-SNE outputs at successive layers in a network reveal increasingly organized
arrangement of the data points. They can also reveal how a network can diminish
or even forget about within-class structure as the data proceeds through
layers. We use class-specific analogues of principal components to visualize
how succeeding layers separate the classes. These allow us to sort images from
a given class from most typical to least typical (in the data) and they also
serve as very useful projection coordinates for data visualization. We find
them especially useful when defining versions guided tours for animated data
visualization.

    

### [[2107.12562] Cross-speaker Style Transfer with Prosody Bottleneck in Neural Speech Synthesis](http://arxiv.org/abs/2107.12562)


  Cross-speaker style transfer is crucial to the applications of multi-style
and expressive speech synthesis at scale. It does not require the target
speakers to be experts in expressing all styles and to collect corresponding
recordings for model training. However, the performances of existing style
transfer methods are still far behind real application needs. The root causes
are mainly twofold. Firstly, the style embedding extracted from single
reference speech can hardly provide fine-grained and appropriate prosody
information for arbitrary text to synthesize. Secondly, in these models the
content/text, prosody, and speaker timbre are usually highly entangled, it's
therefore not realistic to expect a satisfied result when freely combining
these components, such as to transfer speaking style between speakers. In this
paper, we propose a cross-speaker style transfer text-to-speech (TTS) model
with explicit prosody bottleneck. The prosody bottleneck builds up the kernels
accounting for speaking style robustly, and disentangles the prosody from
content and speaker timbre, therefore guarantees high quality cross-speaker
style transfer. Evaluation result shows the proposed method even achieves
on-par performance with source speaker's speaker-dependent (SD) model in
objective measurement of prosody, and significantly outperforms the cycle
consistency and GMVAE-based baselines in objective and subjective evaluations.

    

### [[2107.12571] CFLOW-AD: Real-Time Unsupervised Anomaly Detection with Localization via Conditional Normalizing Flows](http://arxiv.org/abs/2107.12571)


  Unsupervised anomaly detection with localization has many practical
applications when labeling is infeasible and, moreover, when anomaly examples
are completely missing in the train data. While recently proposed models for
such data setup achieve high accuracy metrics, their complexity is a limiting
factor for real-time processing. In this paper, we propose a real-time model
and analytically derive its relationship to prior methods. Our CFLOW-AD model
is based on a conditional normalizing flow framework adopted for anomaly
detection with localization. In particular, CFLOW-AD consists of a
discriminatively pretrained encoder followed by a multi-scale generative
decoders where the latter explicitly estimate likelihood of the encoded
features. Our approach results in a computationally and memory-efficient model:
CFLOW-AD is faster and smaller by a factor of 10x than prior state-of-the-art
with the same input setting. Our experiments on the MVTec dataset show that
CFLOW-AD outperforms previous methods by 0.36% AUROC in detection task, by
1.12% AUROC and 2.5% AUPRO in localization task, respectively. We open-source
our code with fully reproducible experiments.

    

### [[2107.12580] Pointer Value Retrieval: A new benchmark for understanding the limits of neural network generalization](http://arxiv.org/abs/2107.12580)


  The successes of deep learning critically rely on the ability of neural
networks to output meaningful predictions on unseen data -- generalization. Yet
despite its criticality, there remain fundamental open questions on how neural
networks generalize. How much do neural networks rely on memorization -- seeing
highly similar training examples -- and how much are they capable of
human-intelligence styled reasoning -- identifying abstract rules underlying
the data? In this paper we introduce a novel benchmark, Pointer Value Retrieval
(PVR) tasks, that explore the limits of neural network generalization. While
PVR tasks can consist of visual as well as symbolic inputs, each with varying
levels of difficulty, they all have a simple underlying rule. One part of the
PVR task input acts as a pointer, giving the location of a different part of
the input, which forms the value (and output). We demonstrate that this task
structure provides a rich testbed for understanding generalization, with our
empirical study showing large variations in neural network performance based on
dataset size, task complexity and model architecture. The interaction of
position, values and the pointer rule also allow the development of nuanced
tests of generalization, by introducing distribution shift and increasing
functional complexity. These reveal both subtle failures and surprising
successes, suggesting many promising directions of exploration on this
benchmark.

    

### [[2107.12591] Combining Probabilistic Logic and Deep Learning for Self-Supervised Learning](http://arxiv.org/abs/2107.12591)


  Deep learning has proven effective for various application tasks, but its
applicability is limited by the reliance on annotated examples. Self-supervised
learning has emerged as a promising direction to alleviate the supervision
bottleneck, but existing work focuses on leveraging co-occurrences in unlabeled
data for task-agnostic representation learning, as exemplified by masked
language model pretraining. In this chapter, we explore task-specific
self-supervision, which leverages domain knowledge to automatically annotate
noisy training examples for end applications, either by introducing labeling
functions for annotating individual instances, or by imposing constraints over
interdependent label decisions. We first present deep probabilistic logic(DPL),
which offers a unifying framework for task-specific self-supervision by
composing probabilistic logic with deep learning. DPL represents unknown labels
as latent variables and incorporates diverse self-supervision using
probabilistic logic to train a deep neural network end-to-end using variational
EM. Next, we present self-supervised self-supervision(S4), which adds to DPL
the capability to learn new self-supervision automatically. Starting from an
initial seed self-supervision, S4 iteratively uses the deep neural network to
propose new self supervision. These are either added directly (a form of
structured self-training) or verified by a human expert (as in feature-based
active learning). Experiments on real-world applications such as biomedical
machine reading and various text classification tasks show that task-specific
self-supervision can effectively leverage domain expertise and often match the
accuracy of supervised methods with a tiny fraction of human effort.

    

### [[2107.12619] Uniformity in Heterogeneity:Diving Deep into Count Interval Partition for Crowd Counting](http://arxiv.org/abs/2107.12619)


  Recently, the problem of inaccurate learning targets in crowd counting draws
increasing attention. Inspired by a few pioneering work, we solve this problem
by trying to predict the indices of pre-defined interval bins of counts instead
of the count values themselves. However, an inappropriate interval setting
might make the count error contributions from different intervals extremely
imbalanced, leading to inferior counting performance. Therefore, we propose a
novel count interval partition criterion called Uniform Error Partition (UEP),
which always keeps the expected counting error contributions equal for all
intervals to minimize the prediction risk. Then to mitigate the inevitably
introduced discretization errors in the count quantization process, we propose
another criterion called Mean Count Proxies (MCP). The MCP criterion selects
the best count proxy for each interval to represent its count value during
inference, making the overall expected discretization error of an image nearly
negligible. As far as we are aware, this work is the first to delve into such a
classification task and ends up with a promising solution for count interval
partition. Following the above two theoretically demonstrated criterions, we
propose a simple yet effective model termed Uniform Error Partition Network
(UEPNet), which achieves state-of-the-art performance on several challenging
datasets. The codes will be available at:
this https URL.

    

### [[2107.12626] Unsupervised Deep Anomaly Detection for Multi-Sensor Time-Series Signals](http://arxiv.org/abs/2107.12626)


  Nowadays, multi-sensor technologies are applied in many fields, e.g., Health
Care (HC), Human Activity Recognition (HAR), and Industrial Control System
(ICS). These sensors can generate a substantial amount of multivariate
time-series data. Unsupervised anomaly detection on multi-sensor time-series
data has been proven critical in machine learning researches. The key challenge
is to discover generalized normal patterns by capturing spatial-temporal
correlation in multi-sensor data. Beyond this challenge, the noisy data is
often intertwined with the training data, which is likely to mislead the model
by making it hard to distinguish between the normal, abnormal, and noisy data.
Few of previous researches can jointly address these two challenges. In this
paper, we propose a novel deep learning-based anomaly detection algorithm
called Deep Convolutional Autoencoding Memory network (CAE-M). We first build a
Deep Convolutional Autoencoder to characterize spatial dependence of
multi-sensor data with a Maximum Mean Discrepancy (MMD) to better distinguish
between the noisy, normal, and abnormal data. Then, we construct a Memory
Network consisting of linear (Autoregressive Model) and non-linear predictions
(Bidirectional LSTM with Attention) to capture temporal dependence from
time-series data. Finally, CAE-M jointly optimizes these two subnetworks. We
empirically compare the proposed approach with several state-of-the-art anomaly
detection methods on HAR and HC datasets. Experimental results demonstrate that
our proposed model outperforms these existing methods.

    

### [[2107.12628] Energy-Based Open-World Uncertainty Modeling for Confidence Calibration](http://arxiv.org/abs/2107.12628)


  Confidence calibration is of great importance to the reliability of decisions
made by machine learning systems. However, discriminative classifiers based on
deep neural networks are often criticized for producing overconfident
predictions that fail to reflect the true correctness likelihood of
classification accuracy. We argue that such an inability to model uncertainty
is mainly caused by the closed-world nature in softmax: a model trained by the
cross-entropy loss will be forced to classify input into one of $K$ pre-defined
categories with high probability. To address this problem, we for the first
time propose a novel $K$+1-way softmax formulation, which incorporates the
modeling of open-world uncertainty as the extra dimension. To unify the
learning of the original $K$-way classification task and the extra dimension
that models uncertainty, we propose a novel energy-based objective function,
and moreover, theoretically prove that optimizing such an objective essentially
forces the extra dimension to capture the marginal data distribution. Extensive
experiments show that our approach, Energy-based Open-World Softmax
(EOW-Softmax), is superior to existing state-of-the-art methods in improving
confidence calibration.

    

### [[2107.12631] Learning to Estimate RIS-Aided mmWave Channels](http://arxiv.org/abs/2107.12631)


  Inspired by the remarkable learning and prediction performance of deep neural
networks (DNNs), we apply one special type of DNN framework, known as
model-driven deep unfolding neural network, to reconfigurable intelligent
surface (RIS)-aided millimeter wave (mmWave) single-input multiple-output
(SIMO) systems. We focus on uplink cascaded channel estimation, where known and
fixed base station combining and RIS phase control matrices are considered for
collecting observations. To boost the estimation performance and reduce the
training overhead, the inherent channel sparsity of mmWave channels is
leveraged in the deep unfolding method. It is verified that the proposed deep
unfolding network architecture can outperform the least squares (LS) method
with a relatively smaller training overhead and online computational
complexity.

    

### [[2107.12636] Exploring Sequence Feature Alignment for Domain Adaptive Detection Transformers](http://arxiv.org/abs/2107.12636)


  Detection transformers have recently shown promising object detection results
and attracted increasing attention. However, how to develop effective domain
adaptation techniques to improve its cross-domain performance remains
unexplored and unclear. In this paper, we delve into this topic and empirically
find that direct feature distribution alignment on the CNN backbone only brings
limited improvements, as it does not guarantee domain-invariant sequence
features in the transformer for prediction. To address this issue, we propose a
novel Sequence Feature Alignment (SFA) method that is specially designed for
the adaptation of detection transformers. Technically, SFA consists of a domain
query-based feature alignment (DQFA) module and a token-wise feature alignment
(TDA) module. In DQFA, a novel domain query is used to aggregate and align
global context from the token sequence of both domains. DQFA reduces the domain
discrepancy in global feature representations and object relations when
deploying in the transformer encoder and decoder, respectively. Meanwhile, TDA
aligns token features in the sequence from both domains, which reduces the
domain gaps in local and instance-level feature representations in the
transformer encoder and decoder, respectively. Besides, a novel bipartite
matching consistency loss is proposed to enhance the feature discriminability
for robust object detection. Experiments on three challenging benchmarks show
that SFA outperforms state-of-the-art domain adaptive object detection methods.
Code has been made available at: this https URL.

    

### [[2107.12651] Greedy Gradient Ensemble for Robust Visual Question Answering](http://arxiv.org/abs/2107.12651)


  Language bias is a critical issue in Visual Question Answering (VQA), where
models often exploit dataset biases for the final decision without considering
the image information. As a result, they suffer from performance drop on
out-of-distribution data and inadequate visual explanation. Based on
experimental analysis for existing robust VQA methods, we stress the language
bias in VQA that comes from two aspects, i.e., distribution bias and shortcut
bias. We further propose a new de-bias framework, Greedy Gradient Ensemble
(GGE), which combines multiple biased models for unbiased base model learning.
With the greedy strategy, GGE forces the biased models to over-fit the biased
data distribution in priority, thus makes the base model pay more attention to
examples that are hard to solve by biased models. The experiments demonstrate
that our method makes better use of visual information and achieves
state-of-the-art performance on diagnosing dataset VQA-CP without using extra
annotations.

    

### [[2107.12654] Co-Transport for Class-Incremental Learning](http://arxiv.org/abs/2107.12654)


  Traditional learning systems are trained in closed-world for a fixed number
of classes, and need pre-collected datasets in advance. However, new classes
often emerge in real-world applications and should be learned incrementally.
For example, in electronic commerce, new types of products appear daily, and in
a social media community, new topics emerge frequently. Under such
circumstances, incremental models should learn several new classes at a time
without forgetting. We find a strong correlation between old and new classes in
incremental learning, which can be applied to relate and facilitate different
learning stages mutually. As a result, we propose CO-transport for class
Incremental Learning (COIL), which learns to relate across incremental tasks
with the class-wise semantic relationship. In detail, co-transport has two
aspects: prospective transport tries to augment the old classifier with optimal
transported knowledge as fast model adaptation. Retrospective transport aims to
transport new class classifiers backward as old ones to overcome forgetting.
With these transports, COIL efficiently adapts to new tasks, and stably resists
forgetting. Experiments on benchmark and real-world multimedia datasets
validate the effectiveness of our proposed method.

    

### [[2107.12657] Continual Learning with Neuron Activation Importance](http://arxiv.org/abs/2107.12657)


  Continual learning is a concept of online learning with multiple sequential
tasks. One of the critical barriers of continual learning is that a network
should learn a new task keeping the knowledge of old tasks without access to
any data of the old tasks. In this paper, we propose a neuron activation
importance-based regularization method for stable continual learning regardless
of the order of tasks. We conduct comprehensive experiments on existing
benchmark data sets to evaluate not just the stability and plasticity of our
method with improved classification accuracy also the robustness of the
performance along the changes of task order.

    

### [[2107.12673] COPS: Controlled Pruning Before Training Starts](http://arxiv.org/abs/2107.12673)


  State-of-the-art deep neural network (DNN) pruning techniques, applied
one-shot before training starts, evaluate sparse architectures with the help of
a single criterion -- called pruning score. Pruning weights based on a solitary
score works well for some architectures and pruning rates but may also fail for
other ones. As a common baseline for pruning scores, we introduce the notion of
a generalized synaptic score (GSS). In this work we do not concentrate on a
single pruning criterion, but provide a framework for combining arbitrary GSSs
to create more powerful pruning strategies. These COmbined Pruning Scores
(COPS) are obtained by solving a constrained optimization problem. Optimizing
for more than one score prevents the sparse network to overly specialize on an
individual task, thus COntrols Pruning before training Starts. The
combinatorial optimization problem given by COPS is relaxed on a linear program
(LP). This LP is solved analytically and determines a solution for COPS.
Furthermore, an algorithm to compute it for two scores numerically is proposed
and evaluated. Solving COPS in such a way has lower complexity than the best
general LP solver. In our experiments we compared pruning with COPS against
state-of-the-art methods for different network architectures and image
classification tasks and obtained improved results.

    

### [[2107.12674] Vision-Guided Forecasting -- Visual Context for Multi-Horizon Time Series Forecasting](http://arxiv.org/abs/2107.12674)


  Autonomous driving gained huge traction in recent years, due to its potential
to change the way we commute. Much effort has been put into trying to estimate
the state of a vehicle. Meanwhile, learning to forecast the state of a vehicle
ahead introduces new capabilities, such as predicting dangerous situations.
Moreover, forecasting brings new supervision opportunities by learning to
predict richer a context, expressed by multiple horizons. Intuitively, a video
stream originated from a front-facing camera is necessary because it encodes
information about the upcoming road. Besides, historical traces of the
vehicle's states give more context. In this paper, we tackle multi-horizon
forecasting of vehicle states by fusing the two modalities. We design and
experiment with 3 end-to-end architectures that exploit 3D convolutions for
visual features extraction and 1D convolutions for features extraction from
speed and steering angle traces. To demonstrate the effectiveness of our
method, we perform extensive experiments on two publicly available real-world
datasets, Comma2k19 and the Udacity challenge. We show that we are able to
forecast a vehicle's state to various horizons, while outperforming the current
state-of-the-art results on the related task of driving state estimation. We
examine the contribution of vision features, and find that a model fed with
vision features achieves an error that is 56.6% and 66.9% of the error of a
model that doesn't use those features, on the Udacity and Comma2k19 datasets
respectively.

    

### [[2107.12677] Deep Variational Models for Collaborative Filtering-based Recommender Systems](http://arxiv.org/abs/2107.12677)


  Deep learning provides accurate collaborative filtering models to improve
recommender system results. Deep matrix factorization and their related
collaborative neural networks are the state-of-art in the field; nevertheless,
both models lack the necessary stochasticity to create the robust, continuous,
and structured latent spaces that variational autoencoders exhibit. On the
other hand, data augmentation through variational autoencoder does not provide
accurate results in the collaborative filtering field due to the high sparsity
of recommender systems. Our proposed models apply the variational concept to
inject stochasticity in the latent space of the deep architecture, introducing
the variational technique in the neural collaborative filtering field. This
method does not depend on the particular model used to generate the latent
representation. In this way, this approach can be applied as a plugin to any
current and future specific models. The proposed models have been tested using
four representative open datasets, three different quality measures, and
state-of-art baselines. The results show the superiority of the proposed
approach in scenarios where the variational enrichment exceeds the injected
noise effect. Additionally, a framework is provided to enable the
reproducibility of the conducted experiments.

    

### [[2107.12679] MFAGAN: A Compression Framework for Memory-Efficient On-Device Super-Resolution GAN](http://arxiv.org/abs/2107.12679)


  Generative adversarial networks (GANs) have promoted remarkable advances in
single-image super-resolution (SR) by recovering photo-realistic images.
However, high memory consumption of GAN-based SR (usually generators) causes
performance degradation and more energy consumption, hindering the deployment
of GAN-based SR into resource-constricted mobile devices. In this paper, we
propose a novel compression framework \textbf{M}ulti-scale \textbf{F}eature
\textbf{A}ggregation Net based \textbf{GAN} (MFAGAN) for reducing the memory
access cost of the generator. First, to overcome the memory explosion of dense
connections, we utilize a memory-efficient multi-scale feature aggregation net
as the generator. Second, for faster and more stable training, our method
introduces the PatchGAN discriminator. Third, to balance the student
discriminator and the compressed generator, we distill both the generator and
the discriminator. Finally, we perform a hardware-aware neural architecture
search (NAS) to find a specialized SubGenerator for the target mobile phone.
Benefiting from these improvements, the proposed MFAGAN achieves up to
\textbf{8.3}$\times$ memory saving and \textbf{42.9}$\times$ computation
reduction, with only minor visual quality degradation, compared with ESRGAN.
Empirical studies also show $\sim$\textbf{70} milliseconds latency on Qualcomm
Snapdragon 865 chipset.

    

### [[2107.12685] On the Role of Optimization in Double Descent: A Least Squares Study](http://arxiv.org/abs/2107.12685)


  Empirically it has been observed that the performance of deep neural networks
steadily improves as we increase model size, contradicting the classical view
on overfitting and generalization. Recently, the double descent phenomena has
been proposed to reconcile this observation with theory, suggesting that the
test error has a second descent when the model becomes sufficiently
overparameterized, as the model size itself acts as an implicit regularizer. In
this paper we add to the growing body of work in this space, providing a
careful study of learning dynamics as a function of model size for the least
squares scenario. We show an excess risk bound for the gradient descent
solution of the least squares objective. The bound depends on the smallest
non-zero eigenvalue of the covariance matrix of the input features, via a
functional form that has the double descent behavior. This gives a new
perspective on the double descent curves reported in the literature. Our
analysis of the excess risk allows to decouple the effect of optimization and
generalization error. In particular, we find that in case of noiseless
regression, double descent is explained solely by optimization-related
quantities, which was missed in studies focusing on the Moore-Penrose
pseudoinverse solution. We believe that our derivation provides an alternative
view compared to existing work, shedding some light on a possible cause of this
phenomena, at least in the considered least squares setting. We empirically
explore if our predictions hold for neural networks, in particular whether the
covariance of intermediary hidden activations has a similar behavior as the one
predicted by our derivations.

    

### [[2107.12698] Source-Agnostic Gravitational-Wave Detection with Recurrent Autoencoders](http://arxiv.org/abs/2107.12698)


  We present an application of anomaly detection techniques based on deep
recurrent autoencoders to the problem of detecting gravitational wave signals
in laser interferometers. Trained on noise data, this class of algorithms could
detect signals using an unsupervised strategy, i.e., without targeting a
specific kind of source. We develop a custom architecture to analyze the data
from two interferometers. We compare the obtained performance to that obtained
with other autoencoder architectures and with a convolutional classifier. The
unsupervised nature of the proposed strategy comes with a cost in terms of
accuracy, when compared to more traditional supervised techniques. On the other
hand, there is a qualitative gain in generalizing the experimental sensitivity
beyond the ensemble of pre-computed signal templates. The recurrent autoencoder
outperforms other autoencoders based on different architectures. The class of
recurrent autoencoders presented in this paper could complement the search
strategy employed for gravitational wave detection and extend the reach of the
ongoing detection campaigns.

    

### [[2107.12706] Improving ClusterGAN Using Self-AugmentedInformation Maximization of Disentangling LatentSpaces](http://arxiv.org/abs/2107.12706)


  The Latent Space Clustering in Generative adversarial networks (ClusterGAN)
method has been successful with high-dimensional data. However, the method
assumes uniformlydistributed priors during the generation of modes, which isa
restrictive assumption in real-world data and cause loss ofdiversity in the
generated modes. In this paper, we proposeself-augmentation information
maximization improved Clus-terGAN (SIMI-ClusterGAN) to learn the distinctive
priorsfrom the data. The proposed SIMI-ClusterGAN consists offour deep neural
networks: self-augmentation prior network,generator, discriminator and
clustering inference autoencoder.The proposed method has been validated using
seven bench-mark data sets and has shown improved performance overstate-of-the
art methods. To demonstrate the superiority ofSIMI-ClusterGAN performance on
imbalanced dataset, wehave discussed two imbalanced conditions on MNIST
datasetswith one-class imbalance and three classes imbalanced cases.The results
highlight the advantages of SIMI-ClusterGAN.

    

### [[2107.12723] Stability & Generalisation of Gradient Descent for Shallow Neural Networks without the Neural Tangent Kernel](http://arxiv.org/abs/2107.12723)


  We revisit on-average algorithmic stability of Gradient Descent (GD) for
training overparameterised shallow neural networks and prove new generalisation
and excess risk bounds without the Neural Tangent Kernel (NTK) or
Polyak-Łojasiewicz (PL) assumptions. In particular, we show oracle type
bounds which reveal that the generalisation and excess risk of GD is controlled
by an interpolating network with the shortest GD path from initialisation (in a
sense, an interpolating network with the smallest relative norm). While this
was known for kernelised interpolants, our proof applies directly to networks
trained by GD without intermediate kernelisation. At the same time, by relaxing
oracle inequalities developed here we recover existing NTK-based risk bounds in
a straightforward way, which demonstrates that our analysis is tighter.
Finally, unlike most of the NTK-based analyses we focus on regression with
label noise and show that GD with early stopping is consistent.

    

### [[2107.12734] ENHANCE (ENriching Health data by ANnotations of Crowd and Experts): A case study for skin lesion classification](http://arxiv.org/abs/2107.12734)


  We present ENHANCE, an open dataset with multiple annotations to complement
the existing ISIC and PH2 skin lesion classification datasets. This dataset
contains annotations of visual ABC (asymmetry, border, colour) features from
non-expert annotation sources: undergraduate students, crowd workers from
Amazon MTurk and classic image processing algorithms. In this paper we first
analyse the correlations between the annotations and the diagnostic label of
the lesion, as well as study the agreement between different annotation
sources. Overall we find weak correlations of non-expert annotations with the
diagnostic label, and low agreement between different annotation sources. We
then study multi-task learning (MTL) with the annotations as additional labels,
and show that non-expert annotations can improve (ensembles of)
state-of-the-art convolutional neural networks via MTL. We hope that our
dataset can be used in further research into multiple annotations and/or MTL.
All data and models are available on Github:
this https URL.

    

### [[2107.12770] Comparing Prophet and Deep Learning to ARIMA in Forecasting Wholesale Food Prices](http://arxiv.org/abs/2107.12770)


  Setting sale prices correctly is of great importance for firms, and the study
and forecast of prices time series is therefore a relevant topic not only from
a data science perspective but also from an economic and applicative one. In
this paper we exhamine different techniques to forecast the sale prices of
three food products applied by an Italian food wholesaler, as a step towards
the automation of pricing tasks usually taken care by human workforce. We
consider ARIMA models and compare them to Prophet, a scalable forecasting tool
developed by Facebook and based on a generalized additive model, and to deep
learning models based on Long Short--Term Memory (LSTM) and Convolutional
Neural Networks (CNNs). ARIMA models are frequently used in econometric
analyses, providing a good bechmark for the problem under study. Our results
indicate that ARIMA performs similarly to LSTM neural networks for the problem
under study, while the combination of CNNs and LSTMs attains the best overall
accuracy, but requires more time to be tuned. On the contrary, Prophet is very
fast to use, but less accurate.

    

### [[2107.12775] Realistic Ultrasound Image Synthesis for Improved Classification of Liver Disease](http://arxiv.org/abs/2107.12775)


  With the success of deep learning-based methods applied in medical image
analysis, convolutional neural networks (CNNs) have been investigated for
classifying liver disease from ultrasound (US) data. However, the scarcity of
available large-scale labeled US data has hindered the success of CNNs for
classifying liver disease from US data. In this work, we propose a novel
generative adversarial network (GAN) architecture for realistic diseased and
healthy liver US image synthesis. We adopt the concept of stacking to
synthesize realistic liver US data. Quantitative and qualitative evaluation is
performed on 550 in-vivo B-mode liver US images collected from 55 subjects. We
also show that the synthesized images, together with real in vivo data, can be
used to significantly improve the performance of traditional CNN architectures
for Nonalcoholic fatty liver disease (NAFLD) classification.

    

### [[2107.12780] Physics-constrained Deep Learning for Robust Inverse ECG Modeling](http://arxiv.org/abs/2107.12780)


  The rapid developments in advanced sensing and imaging bring about a
data-rich environment, facilitating the effective modeling, monitoring, and
control of complex systems. For example, the body-sensor network captures
multi-channel information pertinent to the electrical activity of the heart
(i.e., electrocardiograms (ECG)), which enables medical scientists to monitor
and detect abnormal cardiac conditions. However, the high-dimensional sensing
data are generally complexly structured and realizing the full data potential
depends to a great extent on advanced analytical and predictive methods. This
paper presents a physics-constrained deep learning (P-DL) framework for
high-dimensional inverse ECG modeling. This method integrates the physical laws
of the complex system with the advanced deep learning infrastructure for
effective prediction of the system dynamics. The proposed P-DL approach is
implemented to solve the inverse ECG model and predict the time-varying
distribution of electric potentials in the heart from the ECG data measured by
the body-surface sensor network. Experimental results show that the proposed
P-DL method significantly outperforms existing methods that are commonly used
in current practice.

    

### [[2107.12783] Statistical Guarantees for Fairness Aware Plug-In Algorithms](http://arxiv.org/abs/2107.12783)


  A plug-in algorithm to estimate Bayes Optimal Classifiers for fairness-aware
binary classification has been proposed in (Menon & Williamson, 2018). However,
the statistical efficacy of their approach has not been established. We prove
that the plug-in algorithm is statistically consistent. We also derive finite
sample guarantees associated with learning the Bayes Optimal Classifiers via
the plug-in algorithm. Finally, we propose a protocol that modifies the plug-in
approach, so as to simultaneously guarantee fairness and differential privacy
with respect to a binary feature deemed sensitive.

    

### [[2107.12791] Clickbait Detection in YouTube Videos](http://arxiv.org/abs/2107.12791)


  YouTube videos often include captivating descriptions and intriguing
thumbnails designed to increase the number of views, and thereby increase the
revenue for the person who posted the video. This creates an incentive for
people to post clickbait videos, in which the content might deviate
significantly from the title, description, or thumbnail. In effect, users are
tricked into clicking on clickbait videos. In this research, we consider the
challenging problem of detecting clickbait YouTube videos. We experiment with
multiple state-of-the-art machine learning techniques using a variety of
textual features.

    

### [[2107.12794] Short-Term Electricity Price Forecasting based on Graph Convolution Network and Attention Mechanism](http://arxiv.org/abs/2107.12794)


  In electricity markets, locational marginal price (LMP) forecasting is
particularly important for market participants in making reasonable bidding
strategies, managing potential trading risks, and supporting efficient system
planning and operation. Unlike existing methods that only consider LMPs'
temporal features, this paper tailors a spectral graph convolutional network
(GCN) to greatly improve the accuracy of short-term LMP forecasting. A
three-branch network structure is then designed to match the structure of LMPs'
compositions. Such kind of network can extract the spatial-temporal features of
LMPs, and provide fast and high-quality predictions for all nodes
simultaneously. The attention mechanism is also implemented to assign varying
importance weights between different nodes and time slots. Case studies based
on the IEEE-118 test system and real-world data from the PJM validate that the
proposed model outperforms existing forecasting models in accuracy, and
maintains a robust performance by avoiding extreme errors.

    

### [[2107.12797] Wasserstein-Splitting Gaussian Process Regression for Heterogeneous Online Bayesian Inference](http://arxiv.org/abs/2107.12797)


  Gaussian processes (GPs) are a well-known nonparametric Bayesian inference
technique, but they suffer from scalability problems for large sample sizes,
and their performance can degrade for non-stationary or spatially heterogeneous
data. In this work, we seek to overcome these issues through (i) employing
variational free energy approximations of GPs operating in tandem with online
expectation propagation steps; and (ii) introducing a local splitting step
which instantiates a new GP whenever the posterior distribution changes
significantly as quantified by the Wasserstein metric over posterior
distributions. Over time, then, this yields an ensemble of sparse GPs which may
be updated incrementally, and adapts to locality, heterogeneity, and
non-stationarity in training data.

    

### [[2107.12800] Deep Reinforcement Learning for L3 Slice Localization in Sarcopenia Assessment](http://arxiv.org/abs/2107.12800)


  Sarcopenia is a medical condition characterized by a reduction in muscle mass
and function. A quantitative diagnosis technique consists of localizing the CT
slice passing through the middle of the third lumbar area (L3) and segmenting
muscles at this level. In this paper, we propose a deep reinforcement learning
method for accurate localization of the L3 CT slice. Our method trains a
reinforcement learning agent by incentivizing it to discover the right
position. Specifically, a Deep Q-Network is trained to find the best policy to
follow for this problem. Visualizing the training process shows that the agent
mimics the scrolling of an experienced radiologist. Extensive experiments
against other state-of-the-art deep learning based methods for L3 localization
prove the superiority of our technique which performs well even with limited
amount of data and annotations.

    

### [[2107.12801] Robust Optimization Framework for Training Shallow Neural Networks Using Reachability Method](http://arxiv.org/abs/2107.12801)


  In this paper, a robust optimization framework is developed to train shallow
neural networks based on reachability analysis of neural networks. To
characterize noises of input data, the input training data is disturbed in the
description of interval sets. Interval-based reachability analysis is then
performed for the hidden layer. With the reachability analysis results, a
robust optimization training method is developed in the framework of robust
least-square problems. Then, the developed robust least-square problem is
relaxed to a semidefinite programming problem. It has been shown that the
developed robust learning method can provide better robustness against
perturbations at the price of loss of training accuracy to some extent. At
last, the proposed method is evaluated on a robot arm model learning example.

    

### [[2107.12808] Open-Ended Learning Leads to Generally Capable Agents](http://arxiv.org/abs/2107.12808)


  In this work we create agents that can perform well beyond a single,
individual task, that exhibit much wider generalisation of behaviour to a
massive, rich space of challenges. We define a universe of tasks within an
environment domain and demonstrate the ability to train agents that are
generally capable across this vast space and beyond. The environment is
natively multi-agent, spanning the continuum of competitive, cooperative, and
independent games, which are situated within procedurally generated physical 3D
worlds. The resulting space is exceptionally diverse in terms of the challenges
posed to agents, and as such, even measuring the learning progress of an agent
is an open research problem. We propose an iterative notion of improvement
between successive generations of agents, rather than seeking to maximise a
singular objective, allowing us to quantify progress despite tasks being
incomparable in terms of achievable rewards. We show that through constructing
an open-ended learning process, which dynamically changes the training task
distributions and training objectives such that the agent never stops learning,
we achieve consistent learning of new behaviours. The resulting agent is able
to score reward in every one of our humanly solvable evaluation levels, with
behaviour generalising to many held-out points in the universe of tasks.
Examples of this zero-shot generalisation include good performance on Hide and
Seek, Capture the Flag, and Tag. Through analysis and hand-authored probe tasks
we characterise the behaviour of our agent, and find interesting emergent
heuristic behaviours such as trial-and-error experimentation, simple tool use,
option switching, and cooperation. Finally, we demonstrate that the general
capabilities of this agent could unlock larger scale transfer of behaviour
through cheap finetuning.

    

### [[2107.12809] Bayesian Optimisation for Sequential Experimental Design with Applications in Additive Manufacturing](http://arxiv.org/abs/2107.12809)


  Bayesian optimization (BO) is an approach to globally optimizing black-box
objective functions that are expensive to evaluate. BO-powered experimental
design has found wide application in materials science, chemistry, experimental
physics, drug development, etc. This work aims to bring attention to the
benefits of applying BO in designing experiments and to provide a BO manual,
covering both methodology and software, for the convenience of anyone who wants
to apply or learn BO. In particular, we briefly explain the BO technique,
review all the applications of BO in additive manufacturing, compare and
exemplify the features of different open BO libraries, unlock new potential
applications of BO to other types of data (e.g., preferential output). This
article is aimed at readers with some understanding of Bayesian methods, but
not necessarily with knowledge of additive manufacturing; the software
performance overview and implementation instructions are instrumental for any
experimental-design practitioner. Moreover, our review in the field of additive
manufacturing highlights the current knowledge and technological trends of BO.

    

### [[2107.12824] A Low-Cost Neural ODE with Depthwise Separable Convolution for Edge Domain Adaptation on FPGAs](http://arxiv.org/abs/2107.12824)


  Although high-performance deep neural networks are in high demand in edge
environments, computation resources are strictly limited in edge devices, and
light-weight neural network techniques, such as Depthwise Separable Convolution
(DSC), have been developed. ResNet is one of conventional deep neural network
models that stack a lot of layers and parameters for a higher accuracy. To
reduce the parameter size of ResNet, by utilizing a similarity to ODE (Ordinary
Differential Equation), Neural ODE repeatedly uses most of weight parameters
instead of having a lot of different parameters. Thus, Neural ODE becomes
significantly small compared to that of ResNet so that it can be implemented in
resource-limited edge devices. In this paper, a combination of Neural ODE and
DSC, called dsODENet, is designed and implemented for FPGAs (Field-Programmable
Gate Arrays). dsODENet is then applied to edge domain adaptation as a practical
use case and evaluated with image classification datasets. It is implemented on
Xilinx ZCU104 board and evaluated in terms of domain adaptation accuracy,
training speed, FPGA resource utilization, and speedup rate compared to a
software execution. The results demonstrate that dsODENet is comparable to or
slightly better than our baseline Neural ODE implementation in terms of domain
adaptation accuracy, while the total parameter size without pre- and
post-processing layers is reduced by 54.2% to 79.8%. The FPGA implementation
accelerates the prediction tasks by 27.9 times faster than a software
implementation.

    

### [[2107.12825] Individual Survival Curves with Conditional Normalizing Flows](http://arxiv.org/abs/2107.12825)


  Survival analysis, or time-to-event modelling, is a classical statistical
problem that has garnered a lot of interest for its practical use in
epidemiology, demographics or actuarial sciences. Recent advances on the
subject from the point of view of machine learning have been concerned with
precise per-individual predictions instead of population studies, driven by the
rise of individualized medicine. We introduce here a conditional normalizing
flow based estimate of the time-to-event density as a way to model highly
flexible and individualized conditional survival distributions. We use a novel
hierarchical formulation of normalizing flows to enable efficient fitting of
flexible conditional distributions without overfitting and show how the
normalizing flow formulation can be efficiently adapted to the censored
setting. We experimentally validate the proposed approach on a synthetic
dataset as well as four open medical datasets and an example of a common
financial problem.

    

### [[2107.12826] Adversarial Stacked Auto-Encoders for Fair Representation Learning](http://arxiv.org/abs/2107.12826)


  Training machine learning models with the only accuracy as a final goal may
promote prejudices and discriminatory behaviors embedded in the data. One
solution is to learn latent representations that fulfill specific fairness
metrics. Different types of learning methods are employed to map data into the
fair representational space. The main purpose is to learn a latent
representation of data that scores well on a fairness metric while maintaining
the usability for the downstream task. In this paper, we propose a new fair
representation learning approach that leverages different levels of
representation of data to tighten the fairness bounds of the learned
representation. Our results show that stacking different auto-encoders and
enforcing fairness at different latent spaces result in an improvement of
fairness compared to other existing approaches.

    

### [[2107.12838] Graph Autoencoders for Embedding Learning in Brain Networks and Major Depressive Disorder Identification](http://arxiv.org/abs/2107.12838)


  Brain functional connectivity (FC) reveals biomarkers for identification of
various neuropsychiatric disorders. Recent application of deep neural networks
(DNNs) to connectome-based classification mostly relies on traditional
convolutional neural networks using input connectivity matrices on a regular
Euclidean grid. We propose a graph deep learning framework to incorporate the
non-Euclidean information about graph structure for classifying functional
magnetic resonance imaging (fMRI)- derived brain networks in major depressive
disorder (MDD). We design a novel graph autoencoder (GAE) architecture based on
the graph convolutional networks (GCNs) to embed the topological structure and
node content of large-sized fMRI networks into low-dimensional latent
representations. In network construction, we employ the Ledoit-Wolf (LDW)
shrinkage method to estimate the high-dimensional FC metrics efficiently from
fMRI data. We consider both supervised and unsupervised approaches for the
graph embedded learning. The learned embeddings are then used as feature inputs
for a deep fully-connected neural network (FCNN) to discriminate MDD from
healthy controls. Evaluated on a resting-state fMRI MDD dataset with 43
subjects, results show that the proposed GAE-FCNN model significantly
outperforms several state-of-the-art DNN methods for brain connectome
classification, achieving accuracy of 72.50% using the LDW-FC metrics as node
features. The graph embeddings of fMRI FC networks learned by the GAE also
reveal apparent group differences between MDD and HC. Our new framework
demonstrates feasibility of learning graph embeddings on brain networks to
provide discriminative information for diagnosis of brain disorders.

    

### [[2107.12847] Learning Local Recurrent Models for Human Mesh Recovery](http://arxiv.org/abs/2107.12847)


  We consider the problem of estimating frame-level full human body meshes
given a video of a person with natural motion dynamics. While much progress in
this field has been in single image-based mesh estimation, there has been a
recent uptick in efforts to infer mesh dynamics from video given its role in
alleviating issues such as depth ambiguity and occlusions. However, a key
limitation of existing work is the assumption that all the observed motion
dynamics can be modeled using one dynamical/recurrent model. While this may
work well in cases with relatively simplistic dynamics, inference with
in-the-wild videos presents many challenges. In particular, it is typically the
case that different body parts of a person undergo different dynamics in the
video, e.g., legs may move in a way that may be dynamically different from
hands (e.g., a person dancing). To address these issues, we present a new
method for video mesh recovery that divides the human mesh into several local
parts following the standard skeletal model. We then model the dynamics of each
local part with separate recurrent models, with each model conditioned
appropriately based on the known kinematic structure of the human body. This
results in a structure-informed local recurrent learning architecture that can
be trained in an end-to-end fashion with available annotations. We conduct a
variety of experiments on standard video mesh recovery benchmark datasets such
as Human3.6M, MPI-INF-3DHP, and 3DPW, demonstrating the efficacy of our design
of modeling local dynamics as well as establishing state-of-the-art results
based on standard evaluation metrics.

    

### [[2107.12855] Neural Network Branch-and-Bound for Neural Network Verification](http://arxiv.org/abs/2107.12855)


  Many available formal verification methods have been shown to be instances of
a unified Branch-and-Bound (BaB) formulation. We propose a novel machine
learning framework that can be used for designing an effective branching
strategy as well as for computing better lower bounds. Specifically, we learn
two graph neural networks (GNN) that both directly treat the network we want to
verify as a graph input and perform forward-backward passes through the GNN
layers. We use one GNN to simulate the strong branching heuristic behaviour and
another to compute a feasible dual solution of the convex relaxation, thereby
providing a valid lower bound.
We provide a new verification dataset that is more challenging than those
used in the literature, thereby providing an effective alternative for testing
algorithmic improvements for verification. Whilst using just one of the GNNs
leads to a reduction in verification time, we get optimal performance when
combining the two GNN approaches. Our combined framework achieves a 50\%
reduction in both the number of branches and the time required for verification
on various convolutional networks when compared to several state-of-the-art
verification methods. In addition, we show that our GNN models generalize well
to harder properties on larger unseen networks.

    

### [[2107.12869] A Simplified Framework for Air Route Clustering Based on ADS-B Data](http://arxiv.org/abs/2107.12869)


  The volume of flight traffic gets increasing over the time, which makes the
strategic traffic flow management become one of the challenging problems since
it requires a lot of computational resources to model entire traffic data. On
the other hand, Automatic Dependent Surveillance - Broadcast (ADS-B) technology
has been considered as a promising data technology to provide both flight crews
and ground control staff the necessary information safely and efficiently about
the position and velocity of the airplanes in a specific area. In the attempt
to tackle this problem, we presented in this paper a simplified framework that
can support to detect the typical air routes between airports based on ADS-B
data. Specifically, the flight traffic will be classified into major groups
based on similarity measures, which helps to reduce the number of flight paths
between airports. As a matter of fact, our framework can be taken into account
to reduce practically the computational cost for air flow optimization and
evaluate the operational performance. Finally, in order to illustrate the
potential applications of our proposed framework, an experiment was performed
using ADS-B traffic flight data of three different pairs of airports. The
detected typical routes between each couple of airports show promising results
by virtue of combining two indices for measuring the clustering performance and
incorporating human judgment into the visual inspection.

    

### [[2107.12871] Model Free Barrier Functions via Implicit Evading Maneuvers](http://arxiv.org/abs/2107.12871)


  This paper demonstrates that in some cases the safety override arising from
the use of a barrier function can be needlessly restrictive. In particular, we
examine the case of fixed wing collision avoidance and show that when using a
barrier function, there are cases where two fixed wing aircraft can come closer
to colliding than if there were no barrier function at all. In addition, we
construct cases where the barrier function labels the system as unsafe even
when the vehicles start arbitrarily far apart. In other words, the barrier
function ensures safety but with unnecessary costs to performance. We therefore
introduce model free barrier functions which take a data driven approach to
creating a barrier function. We demonstrate the effectiveness of model free
barrier functions in a collision avoidance simulation of two fixed-wing
aircraft.

    

### [[2107.12878] Linear Prediction Residual for Efficient Diagnosis of Parkinson's Disease from Gait](http://arxiv.org/abs/2107.12878)


  Parkinson's Disease (PD) is a chronic and progressive neurological disorder
that results in rigidity, tremors and postural instability. There is no
definite medical test to diagnose PD and diagnosis is mostly a clinical
exercise. Although guidelines exist, about 10-30% of the patients are wrongly
diagnosed with PD. Hence, there is a need for an accurate, unbiased and fast
method for diagnosis. In this study, we propose LPGNet, a fast and accurate
method to diagnose PD from gait. LPGNet uses Linear Prediction Residuals (LPR)
to extract discriminating patterns from gait recordings and then uses a 1D
convolution neural network with depth-wise separable convolutions to perform
diagnosis. LPGNet achieves an AUC of 0.91 with a 21 times speedup and about 99%
lesser parameters in the model compared to the state of the art. We also
undertake an analysis of various cross-validation strategies used in literature
in PD diagnosis from gait and find that most methods are affected by some form
of data leakage between various folds which leads to unnecessarily large models
and inflated performance due to overfitting. The analysis clears the path for
future works in correctly evaluating their methods.

    

### [[2107.12889] Improved-Mask R-CNN: Towards an Accurate Generic MSK MRI instance segmentation platform (Data from the Osteoarthritis Initiative)](http://arxiv.org/abs/2107.12889)


  Objective assessment of Magnetic Resonance Imaging (MRI) scans of
osteoarthritis (OA) can address the limitation of the current OA assessment.
Segmentation of bone, cartilage, and joint fluid is necessary for the OA
objective assessment. Most of the proposed segmentation methods are not
performing instance segmentation and suffer from class imbalance problems. This
study deployed Mask R-CNN instance segmentation and improved it (improved-Mask
R-CNN (iMaskRCNN)) to obtain a more accurate generalized segmentation for
OA-associated tissues. Training and validation of the method were performed
using 500 MRI knees from the Osteoarthritis Initiative (OAI) dataset and 97 MRI
scans of patients with symptomatic hip OA. Three modifications to Mask R-CNN
yielded the iMaskRCNN: adding a 2nd ROIAligned block, adding an extra decoder
layer to the mask-header, and connecting them by a skip connection. The results
were assessed using Hausdorff distance, dice score, and coefficients of
variation (CoV). The iMaskRCNN led to improved bone and cartilage segmentation
compared to Mask RCNN as indicated with the increase in dice score from 95% to
98% for the femur, 95% to 97% for tibia, 71% to 80% for femoral cartilage, and
81% to 82% for tibial cartilage. For the effusion detection, dice improved with
iMaskRCNN 72% versus MaskRCNN 71%. The CoV values for effusion detection
between Reader1 and Mask R-CNN (0.33), Reader1 and iMaskRCNN (0.34), Reader2
and Mask R-CNN (0.22), Reader2 and iMaskRCNN (0.29) are close to CoV between
two readers (0.21), indicating a high agreement between the human readers and
both Mask R-CNN and iMaskRCNN. Mask R-CNN and iMaskRCNN can reliably and
simultaneously extract different scale articular tissues involved in OA,
forming the foundation for automated assessment of OA. The iMaskRCNN results
show that the modification improved the network performance around the edges.

    

### [[2107.12910] Sparse Bayesian Deep Learning for Dynamic System Identification](http://arxiv.org/abs/2107.12910)


  This paper proposes a sparse Bayesian treatment of deep neural networks
(DNNs) for system identification. Although DNNs show impressive approximation
ability in various fields, several challenges still exist for system
identification problems. First, DNNs are known to be too complex that they can
easily overfit the training data. Second, the selection of the input regressors
for system identification is nontrivial. Third, uncertainty quantification of
the model parameters and predictions are necessary. The proposed Bayesian
approach offers a principled way to alleviate the above challenges by marginal
likelihood/model evidence approximation and structured group sparsity-inducing
priors construction. The identification algorithm is derived as an iterative
regularized optimization procedure that can be solved as efficiently as
training typical DNNs. Furthermore, a practical calculation approach based on
the Monte-Carlo integration method is derived to quantify the uncertainty of
the parameters and predictions. The effectiveness of the proposed Bayesian
approach is demonstrated on several linear and nonlinear systems identification
benchmarks with achieving good and competitive simulation accuracy.

    

### [[2107.12915] Initial Foundation for Predicting Individual Earthquake's Location and Magnitude by Using Glass-Box Physics Rule Learner](http://arxiv.org/abs/2107.12915)


  Although researchers accumulated knowledge about seismogenesis and
decades-long earthquake data, predicting imminent individual earthquakes at a
specific time and location remains a long-standing enigma. This study
hypothesizes that the observed data conceal the hidden rules which may be
unraveled by a novel glass-box (as opposed to black-box) physics rule learner
(GPRL) framework. Without any predefined earthquake-related mechanisms or
statistical laws, GPRL's two essentials, convolved information index and
transparent link function, seek generic expressions of rules directly from
data. GPRL's training with 10-years data appears to identify plausible rules,
suggesting a combination of the pseudo power and the pseudo vorticity of
released energy in the lithosphere. Independent feasibility test supports the
promising role of the unraveled rules in predicting earthquakes' magnitudes and
their specific locations. The identified rules and GPRL are in their infancy
requiring substantial improvement. Still, this study hints at the existence of
the data-guided hidden pathway to imminent individual earthquake prediction.

    

### [[2107.12917] Experiments on Properties of Hidden Structures of Sparse Neural Networks](http://arxiv.org/abs/2107.12917)


  Sparsity in the structure of Neural Networks can lead to less energy
consumption, less memory usage, faster computation times on convenient
hardware, and automated machine learning. If sparsity gives rise to certain
kinds of structure, it can explain automatically obtained features during
learning.
We provide insights into experiments in which we show how sparsity can be
achieved through prior initialization, pruning, and during learning, and answer
questions on the relationship between the structure of Neural Networks and
their performance. This includes the first work of inducing priors from network
theory into Recurrent Neural Networks and an architectural performance
prediction during a Neural Architecture Search. Within our experiments, we show
how magnitude class blinded pruning achieves 97.5% on MNIST with 80%
compression and re-training, which is 0.5 points more than without compression,
that magnitude class uniform pruning is significantly inferior to it and how a
genetic search enhanced with performance prediction achieves 82.4% on CIFAR10.
Further, performance prediction for Recurrent Networks learning the Reber
grammar shows an $R^2$ of up to 0.81 given only structural information.

    

### [[2107.12919] Transfer Learning in Electronic Health Records through Clinical Concept Embedding](http://arxiv.org/abs/2107.12919)


  Deep learning models have shown tremendous potential in learning
representations, which are able to capture some key properties of the data.
This makes them great candidates for transfer learning: Exploiting
commonalities between different learning tasks to transfer knowledge from one
task to another. Electronic health records (EHR) research is one of the domains
that has witnessed a growing number of deep learning techniques employed for
learning clinically-meaningful representations of medical concepts (such as
diseases and medications). Despite this growth, the approaches to benchmark and
assess such learned representations (or, embeddings) is under-investigated;
this can be a big issue when such embeddings are shared to facilitate transfer
learning. In this study, we aim to (1) train some of the most prominent disease
embedding techniques on a comprehensive EHR data from 3.1 million patients, (2)
employ qualitative and quantitative evaluation techniques to assess these
embeddings, and (3) provide pre-trained disease embeddings for transfer
learning. This study can be the first comprehensive approach for clinical
concept embedding evaluation and can be applied to any embedding techniques and
for any EHR concept.

    

### [[2107.12931] Persistent Reinforcement Learning via Subgoal Curricula](http://arxiv.org/abs/2107.12931)


  Reinforcement learning (RL) promises to enable autonomous acquisition of
complex behaviors for diverse agents. However, the success of current
reinforcement learning algorithms is predicated on an often under-emphasised
requirement -- each trial needs to start from a fixed initial state
distribution. Unfortunately, resetting the environment to its initial state
after each trial requires substantial amount of human supervision and extensive
instrumentation of the environment which defeats the purpose of autonomous
reinforcement learning. In this work, we propose Value-accelerated Persistent
Reinforcement Learning (VaPRL), which generates a curriculum of initial states
such that the agent can bootstrap on the success of easier tasks to efficiently
learn harder tasks. The agent also learns to reach the initial states proposed
by the curriculum, minimizing the reliance on human interventions into the
learning. We observe that VaPRL reduces the interventions required by three
orders of magnitude compared to episodic RL while outperforming prior
state-of-the art methods for reset-free RL both in terms of sample efficiency
and asymptotic performance on a variety of simulated robotics problems.

    

### [[2107.12940] Finding Failures in High-Fidelity Simulation using Adaptive Stress Testing and the Backward Algorithm](http://arxiv.org/abs/2107.12940)


  Validating the safety of autonomous systems generally requires the use of
high-fidelity simulators that adequately capture the variability of real-world
scenarios. However, it is generally not feasible to exhaustively search the
space of simulation scenarios for failures. Adaptive stress testing (AST) is a
method that uses reinforcement learning to find the most likely failure of a
system. AST with a deep reinforcement learning solver has been shown to be
effective in finding failures across a range of different systems. This
approach generally involves running many simulations, which can be very
expensive when using a high-fidelity simulator. To improve efficiency, we
present a method that first finds failures in a low-fidelity simulator. It then
uses the backward algorithm, which trains a deep neural network policy using a
single expert demonstration, to adapt the low-fidelity failures to
high-fidelity. We have created a series of autonomous vehicle validation case
studies that represent some of the ways low-fidelity and high-fidelity
simulators can differ, such as time discretization. We demonstrate in a variety
of case studies that this new AST approach is able to find failures with
significantly fewer high-fidelity simulation steps than are needed when just
running AST directly in high-fidelity. As a proof of concept, we also
demonstrate AST on NVIDIA's DriveSim simulator, an industry state-of-the-art
high-fidelity simulator for finding failures in autonomous vehicles.

    

### [[2107.12942] Reinforcement Learning with Formal Performance Metrics for Quadcopter Attitude Control under Non-nominal Contexts](http://arxiv.org/abs/2107.12942)


  We explore the reinforcement learning approach to designing controllers by
extensively discussing the case of a quadcopter attitude controller. We provide
all details allowing to reproduce our approach, starting with a model of the
dynamics of a crazyflie 2.0 under various nominal and non-nominal conditions,
including partial motor failures and wind gusts. We develop a robust form of a
signal temporal logic to quantitatively evaluate the vehicle's behavior and
measure the performance of controllers. The paper thoroughly describes the
choices in training algorithms, neural net architecture, hyperparameters,
observation space in view of the different performance metrics we have
introduced. We discuss the robustness of the obtained controllers, both to
partial loss of power for one rotor and to wind gusts and finish by drawing
conclusions on practical controller design by reinforcement learning.

    

### [[2107.12957] Learning Numeric Optimal Differentially Private Truncated Additive Mechanisms](http://arxiv.org/abs/2107.12957)


  Differentially private (DP) mechanisms face the challenge of providing
accurate results while protecting their inputs: the privacy-utility trade-off.
A simple but powerful technique for DP adds noise to sensitivity-bounded query
outputs to blur the exact query output: additive mechanisms. While a vast body
of work considers infinitely wide noise distributions, some applications (e.g.,
real-time operating systems) require hard bounds on the deviations from the
real query, and only limited work on such mechanisms exist. An additive
mechanism with truncated noise (i.e., with bounded range) can offer such hard
bounds. We introduce a gradient-descent-based tool to learn truncated noise for
additive mechanisms with strong utility bounds while simultaneously optimizing
for differential privacy under sequential composition, i.e., scenarios where
multiple noisy queries on the same data are revealed. Our method can learn
discrete noise patterns and not only hyper-parameters of a predefined
probability distribution. For sensitivity bounded mechanisms, we show that it
is sufficient to consider symmetric and that\new{, for from the mean
monotonically falling noise,} ensuring privacy for a pair of representative
query outputs guarantees privacy for all pairs of inputs (that differ in one
element). We find that the utility-privacy trade-off curves of our generated
noise are remarkably close to truncated Gaussians and even replicate their
shape for $l_2$ utility-loss. For a low number of compositions, we also
improved DP-SGD (sub-sampling). Moreover, we extend Moments Accountant to
truncated distributions, allowing to incorporate mechanism output events with
varying input-dependent zero occurrence probability.

    

### [[2107.12958] Verifiable Coded Computing: Towards Fast, Secure and Private Distributed Machine Learning](http://arxiv.org/abs/2107.12958)


  Stragglers, Byzantine workers, and data privacy are the main bottlenecks in
distributed cloud computing. Several prior works proposed coded computing
strategies to jointly address all three challenges. They require either a large
number of workers, a significant communication cost or a significant
computational complexity to tolerate malicious workers. Much of the overhead in
prior schemes comes from the fact that they tightly couple coding for all three
problems into a single framework. In this work, we propose Verifiable Coded
Computing (VCC) framework that decouples Byzantine node detection challenge
from the straggler tolerance. VCC leverages coded computing just for handling
stragglers and privacy, and then uses an orthogonal approach of verifiable
computing to tackle Byzantine nodes. Furthermore, VCC dynamically adapts its
coding scheme to tradeoff straggler tolerance with Byzantine protection and
vice-versa. We evaluate VCC on compute intensive distributed logistic
regression application. Our experiments show that VCC speeds up the
conventional uncoded implementation of distributed logistic regression by
$3.2\times-6.9\times$, and also improves the test accuracy by up to $12.6\%$.

    

### [[2107.12964] A Physiologically-adapted Gold Standard for Arousal During a Stress Induced Scenario](http://arxiv.org/abs/2107.12964)


  Emotion is an inherently subjective psychophysiological human-state and to
produce an agreed-upon representation (gold standard) for continuous emotion
requires a time-consuming and costly training procedure of multiple human
annotators. There is strong evidence in the literature that physiological
signals are sufficient objective markers for states of emotion, particularly
arousal. In this contribution, we utilise a dataset which includes continuous
emotion and physiological signals - Heartbeats per Minute (BPM), Electrodermal
Activity (EDA), and Respiration-rate - captured during a stress induced
scenario (Trier Social Stress Test). We utilise a Long Short-Term Memory,
Recurrent Neural Network to explore the benefit of fusing these physiological
signals with arousal as the target, learning from various audio, video, and
textual based features. We utilise the state-of-the-art MuSe-Toolbox to
consider both annotation delay and inter-rater agreement weighting when fusing
the target signals. An improvement in Concordance Correlation Coefficient (CCC)
is seen across features sets when fusing EDA with arousal, compared to the
arousal only gold standard results. Additionally, BERT-based textual features'
results improved for arousal plus all physiological signals, obtaining up to
.3344 CCC compared to .2118 CCC for arousal only. Multimodal fusion also
improves overall CCC with audio plus video features obtaining up to .6157 CCC
to recognize arousal plus EDA and BPM.

    

### [[2107.12970] A Data-driven feature selection and machine-learning model benchmark for the prediction of longitudinal dispersion coefficient](http://arxiv.org/abs/2107.12970)


  Longitudinal Dispersion(LD) is the dominant process of scalar transport in
natural streams. An accurate prediction on LD coefficient(Dl) can produce a
performance leap in related simulation. The emerging machine learning(ML)
techniques provide a self-adaptive tool for this problem. However, most of the
existing studies utilize an unproved quaternion feature set, obtained through
simple theoretical deduction. Few studies have put attention on its reliability
and rationality. Besides, due to the lack of comparative comparison, the proper
choice of ML models in different scenarios still remains unknown. In this
study, the Feature Gradient selector was first adopted to distill the local
optimal feature sets directly from multivariable data. Then, a global optimal
feature set (the channel width, the flow velocity, the channel slope and the
cross sectional area) was proposed through numerical comparison of the
distilled local optimums in performance with representative ML models. The
channel slope is identified to be the key parameter for the prediction of LDC.
Further, we designed a weighted evaluation metric which enables comprehensive
model comparison. With the simple linear model as the baseline, a benchmark of
single and ensemble learning models was provided. Advantages and disadvantages
of the methods involved were also discussed. Results show that the support
vector machine has significantly better performance than other models. Decision
tree is not suitable for this problem due to poor generalization ability.
Notably, simple models show superiority over complicated model on this
low-dimensional problem, for their better balance between regression and
generalization.

    

### [[2107.12972] Channel-Wise Early Stopping without a Validation Set via NNK Polytope Interpolation](http://arxiv.org/abs/2107.12972)


  State-of-the-art neural network architectures continue to scale in size and
deliver impressive generalization results, although this comes at the expense
of limited interpretability. In particular, a key challenge is to determine
when to stop training the model, as this has a significant impact on
generalization. Convolutional neural networks (ConvNets) comprise
high-dimensional feature spaces formed by the aggregation of multiple channels,
where analyzing intermediate data representations and the model's evolution can
be challenging owing to the curse of dimensionality. We present channel-wise
DeepNNK (CW-DeepNNK), a novel channel-wise generalization estimate based on
non-negative kernel regression (NNK) graphs with which we perform local
polytope interpolation on low-dimensional channels. This method leads to
instance-based interpretability of both the learned data representations and
the relationship between channels. Motivated by our observations, we use
CW-DeepNNK to propose a novel early stopping criterion that (i) does not
require a validation set, (ii) is based on a task performance metric, and (iii)
allows stopping to be reached at different points for each channel. Our
experiments demonstrate that our proposed method has advantages as compared to
the standard criterion based on validation set performance.

    

### [[2107.12975] Cross-architecture Tuning of Silicon and SiGe-based Quantum Devices Using Machine Learning](http://arxiv.org/abs/2107.12975)


  The potential of Si and SiGe-based devices for the scaling of quantum
circuits is tainted by device variability. Each device needs to be tuned to
operation conditions. We give a key step towards tackling this variability with
an algorithm that, without modification, is capable of tuning a 4-gate Si
FinFET, a 5-gate GeSi nanowire and a 7-gate SiGe heterostructure double quantum
dot device from scratch. We achieve tuning times of 30, 10, and 92 minutes,
respectively. The algorithm also provides insight into the parameter space
landscape for each of these devices. These results show that overarching
solutions for the tuning of quantum devices are enabled by machine learning.

    

### [[2107.12977] The social dilemma in AI development and why we have to solve it](http://arxiv.org/abs/2107.12977)


  While the demand for ethical artificial intelligence (AI) systems increases,
the number of unethical uses of AI accelerates, even though there is no
shortage of ethical guidelines. We argue that a main underlying cause for this
is that AI developers face a social dilemma in AI development ethics,
preventing the widespread adaptation of ethical best practices. We define the
social dilemma for AI development and describe why the current crisis in AI
development ethics cannot be solved without relieving AI developers of their
social dilemma. We argue that AI development must be professionalised to
overcome the social dilemma, and discuss how medicine can be used as a template
in this process.

    

### [[2107.12978] Optimizing Operating Points for High Performance Lesion Detection and Segmentation Using Lesion Size Reweighting](http://arxiv.org/abs/2107.12978)


  There are many clinical contexts which require accurate detection and
segmentation of all focal pathologies (e.g. lesions, tumours) in patient
images. In cases where there are a mix of small and large lesions, standard
binary cross entropy loss will result in better segmentation of large lesions
at the expense of missing small ones. Adjusting the operating point to
accurately detect all lesions generally leads to oversegmentation of large
lesions. In this work, we propose a novel reweighing strategy to eliminate this
performance gap, increasing small pathology detection performance while
maintaining segmentation accuracy. We show that our reweighing strategy vastly
outperforms competing strategies based on experiments on a large scale,
multi-scanner, multi-center dataset of Multiple Sclerosis patient images.

    

### [[1711.03639] Small-loss bounds for online learning with partial information](http://arxiv.org/abs/1711.03639)


  We consider the problem of adversarial (non-stochastic) online learning with
partial information feedback, where at each round, a decision maker selects an
action from a finite set of alternatives. We develop a black-box approach for
such problems where the learner observes as feedback only losses of a subset of
the actions that includes the selected action. When losses of actions are
non-negative, under the graph-based feedback model introduced by Mannor and
Shamir, we offer algorithms that attain the so called "small-loss" $o(\alpha
L^{\star})$ regret bounds with high probability, where $\alpha$ is the
independence number of the graph, and $L^{\star}$ is the loss of the best
action. Prior to our work, there was no data-dependent guarantee for general
feedback graphs even for pseudo-regret (without dependence on the number of
actions, i.e. utilizing the increased information feedback). Taking advantage
of the black-box nature of our technique, we extend our results to many other
applications such as semi-bandits (including routing in networks), contextual
bandits (even with an infinite comparator class), as well as learning with
slowly changing (shifting) comparators.
In the special case of classical bandit and semi-bandit problems, we provide
optimal small-loss, high-probability guarantees of
$\tilde{O}(\sqrt{dL^{\star}})$ for actual regret, where $d$ is the number of
actions, answering open questions of Neu. Previous bounds for bandits and
semi-bandits were known only for pseudo-regret and only in expectation. We also
offer an optimal $\tilde{O}(\sqrt{\kappa L^{\star}})$ regret guarantee for
fixed feedback graphs with clique-partition number at most $\kappa$.

    

### [[1905.05976] Information criteria for non-normalized models](http://arxiv.org/abs/1905.05976)


  Many statistical models are given in the form of non-normalized densities
with an intractable normalization constant. Since maximum likelihood estimation
is computationally intensive for these models, several estimation methods have
been developed which do not require explicit computation of the normalization
constant, such as noise contrastive estimation (NCE) and score matching.
However, model selection methods for general non-normalized models have not
been proposed so far. In this study, we develop information criteria for
non-normalized models estimated by NCE or score matching. They are
approximately unbiased estimators of discrepancy measures for non-normalized
models. Simulation results and applications to real data demonstrate that the
proposed criteria enable selection of the appropriate non-normalized model in a
data-driven manner.

    

### [[2001.05759] A Methodology guided by Decision Trees Ensemble and Smart Data for Imbalanced Big Data](http://arxiv.org/abs/2001.05759)


  Differences in data size per class, also known as imbalanced data
distribution, have become a common problem affecting data quality. Big Data
scenarios pose a new challenge to traditional imbalanced classification
algorithms, since they are not prepared to work with such amount of data. Split
data strategies and lack of data in the minority class due to the use of
MapReduce paradigm have posed new challenges for tackling the imbalance between
classes in Big Data scenarios. Ensembles have shown to be able to successfully
address imbalanced data problems. Smart Data refers to data of enough quality
to achieve high performance models. The combination of ensembles and Smart
Data, achieved through Big Data preprocessing, should be a great synergy. In
this paper, we propose a novel methodology based on Decision Trees Ensemble
with Smart Data for addressing the imbalanced classification problem in Big
Data domains, namely DeTE_SD methodology. This methodology is based on the
learning of different decision trees using distributed quality data for the
ensemble process. This quality data is achieved by fusing Random
Discretization, Principal Components Analysis and clustering-based Random
Oversampling for obtaining different Smart Data versions of the original data.
Experiments carried out in 21 binary adapted datasets have shown that our
methodology outperforms Random Forest.

    

### [[2001.07620] EdgeNets:Edge Varying Graph Neural Networks](http://arxiv.org/abs/2001.07620)


  Driven by the outstanding performance of neural networks in the structured
Euclidean domain, recent years have seen a surge of interest in developing
neural networks for graphs and data supported on graphs. The graph is leveraged
at each layer of the neural network as a parameterization to capture detail at
the node level with a reduced number of parameters and computational
complexity. Following this rationale, this paper puts forth a general framework
that unifies state-of-the-art graph neural networks (GNNs) through the concept
of EdgeNet. An EdgeNet is a GNN architecture that allows different nodes to use
different parameters to weigh the information of different neighbors. By
extrapolating this strategy to more iterations between neighboring nodes, the
EdgeNet learns edge- and neighbor-dependent weights to capture local detail.
This is a general linear and local operation that a node can perform and
encompasses under one formulation all existing graph convolutional neural
networks (GCNNs) as well as graph attention networks (GATs). In writing
different GNN architectures with a common language, EdgeNets highlight specific
architecture advantages and limitations, while providing guidelines to improve
their capacity without compromising their local implementation. An interesting
conclusion is the unification of GCNNs and GATs -- approaches that have been so
far perceived as separate. In particular, we show that GATs are GCNNs on a
graph that is learned from the features. This particularization opens the doors
to develop alternative attention mechanisms for improving discriminatory power.

    

### [[2003.07450] Spectral Graph Attention Network with Fast Eigen-approximation](http://arxiv.org/abs/2003.07450)


  Variants of Graph Neural Networks (GNNs) for representation learning have
been proposed recently and achieved fruitful results in various fields. Among
them, Graph Attention Network (GAT) first employs a self-attention strategy to
learn attention weights for each edge in the spatial domain. However, learning
the attentions over edges can only focus on the local information of graphs and
greatly increases the computational costs. In this paper, we first introduce
the attention mechanism in the spectral domain of graphs and present Spectral
Graph Attention Network (SpGAT) that learns representations for different
frequency components regarding weighted filters and graph wavelets bases. In
this way, SpGAT can better capture global patterns of graphs in an efficient
manner with much fewer learned parameters than that of GAT. Further, to reduce
the computational cost of SpGAT brought by the eigen-decomposition, we propose
a fast approximation variant SpGAT-Cheby. We thoroughly evaluate the
performance of SpGAT and SpGAT-Cheby in semi-supervised node classification
tasks and verify the effectiveness of the learned attentions in the spectral
domain.

    

### [[2004.06230] Power Constrained Bandits](http://arxiv.org/abs/2004.06230)


  Contextual bandits often provide simple and effective personalization in
decision making problems, making them popular tools to deliver personalized
interventions in mobile health as well as other health applications. However,
when bandits are deployed in the context of a scientific study -- e.g. a
clinical trial to test if a mobile health intervention is effective -- the aim
is not only to personalize for an individual, but also to determine, with
sufficient statistical power, whether or not the system's intervention is
effective. It is essential to assess the effectiveness of the intervention
before broader deployment for better resource allocation. The two objectives
are often deployed under different model assumptions, making it hard to
determine how achieving the personalization and statistical power affect each
other. In this work, we develop general meta-algorithms to modify existing
algorithms such that sufficient power is guaranteed while still improving each
user's well-being. We also demonstrate that our meta-algorithms are robust to
various model mis-specifications possibly appearing in statistical studies,
thus providing a valuable tool to study designers.

    

### [[2005.07473] Predicting User Emotional Tone in Mental Disorder Online Communities](http://arxiv.org/abs/2005.07473)


  In recent years, Online Social Networks have become an important medium for
people who suffer from mental disorders to share moments of hardship, and
receive emotional and informational support. In this work, we analyze how
discussions in Reddit communities related to mental disorders can help improve
the health conditions of their users. Using the emotional tone of users'
writing as a proxy for emotional state, we uncover relationships between user
interactions and state changes. First, we observe that authors of negative
posts often write rosier comments after engaging in discussions, indicating
that users' emotional state can improve due to social support. Second, we build
models based on SOTA text embedding techniques and RNNs to predict shifts in
emotional tone. This differs from most of related work, which focuses primarily
on detecting mental disorders from user activity. We demonstrate the
feasibility of accurately predicting the users' reactions to the interactions
experienced in these platforms, and present some examples which illustrate that
the models are correctly capturing the effects of comments on the author's
emotional tone. Our models hold promising implications for interventions to
provide support for people struggling with mental illnesses.

    

### [[2005.14220] Task-Based Information Compression for Multi-Agent Communication Problems with Channel Rate Constraints](http://arxiv.org/abs/2005.14220)


  A collaborative task is assigned to a multiagent system (MAS) in which agents
are allowed to communicate. The MAS runs over an underlying Markov decision
process and its task is to maximize the averaged sum of discounted one-stage
rewards. Although knowing the global state of the environment is necessary for
the optimal action selection of the MAS, agents are limited to individual
observations. The inter-agent communication can tackle the issue of local
observability, however, the limited rate of the inter-agent communication
prevents the agent from acquiring the precise global state information. To
overcome this challenge, agents need to communicate their observations in a
compact way such that the MAS compromises the minimum possible sum of rewards.
We show that this problem is equivalent to a form of rate-distortion problem
which we call the task-based information compression. We introduce a scheme for
task-based information compression titled State aggregation for information
compression (SAIC), for which a state aggregation algorithm is analytically
designed. The SAIC is shown to be capable of achieving near-optimal performance
in terms of the achieved sum of discounted rewards. The proposed algorithm is
applied to a rendezvous problem and its performance is compared with several
benchmarks. Numerical experiments confirm the superiority of the proposed
algorithm.

    

### [[2006.01791] SaliencyMix: A Saliency Guided Data Augmentation Strategy for Better Regularization](http://arxiv.org/abs/2006.01791)


  Advanced data augmentation strategies have widely been studied to improve the
generalization ability of deep learning models. Regional dropout is one of the
popular solutions that guides the model to focus on less discriminative parts
by randomly removing image regions, resulting in improved regularization.
However, such information removal is undesirable. On the other hand, recent
strategies suggest to randomly cut and mix patches and their labels among
training images, to enjoy the advantages of regional dropout without having any
pointless pixel in the augmented images. We argue that such random selection
strategies of the patches may not necessarily represent sufficient information
about the corresponding object and thereby mixing the labels according to that
uninformative patch enables the model to learn unexpected feature
representation. Therefore, we propose SaliencyMix that carefully selects a
representative image patch with the help of a saliency map and mixes this
indicative patch with the target image, thus leading the model to learn more
appropriate feature representation. SaliencyMix achieves the best known top-1
error of 21.26% and 20.09% for ResNet-50 and ResNet-101 architectures on
ImageNet classification, respectively, and also improves the model robustness
against adversarial perturbations. Furthermore, models that are trained with
SaliencyMix help to improve the object detection performance. Source code is
available at this https URL.

    

### [[2008.05938] RGB cameras failures and their effects in autonomous driving applications](http://arxiv.org/abs/2008.05938)


  RGB cameras are one of the most relevant sensors for autonomous driving
applications. It is undeniable that failures of vehicle cameras may compromise
the autonomous driving task, possibly leading to unsafe behaviors when images
that are subsequently processed by the driving system are altered. To support
the definition of safe and robust vehicle architectures and intelligent
systems, in this paper we define the failure modes of a vehicle camera,
together with an analysis of effects and known mitigations. Further, we build a
software library for the generation of the corresponding failed images and we
feed them to six object detectors for mono and stereo cameras and to the
self-driving agent of an autonomous driving simulator. The resulting
misbehaviors with respect to operating with clean images allow a better
understanding of failures effects and the related safety risks in image-based
applications.

    

### [[2010.02871] LETI: Latency Estimation Tool and Investigation of Neural Networks inference on Mobile GPU](http://arxiv.org/abs/2010.02871)


  A lot of deep learning applications are desired to be run on mobile devices.
Both accuracy and inference time are meaningful for a lot of them. While the
number of FLOPs is usually used as a proxy for neural network latency, it may
be not the best choice. In order to obtain a better approximation of latency,
research community uses look-up tables of all possible layers for latency
calculation for the final prediction of the inference on mobile CPU. It
requires only a small number of experiments. Unfortunately, on mobile GPU this
method is not applicable in a straight-forward way and shows low precision. In
this work, we consider latency approximation on mobile GPU as a data and
hardware-specific problem. Our main goal is to construct a convenient latency
estimation tool for investigation(LETI) of neural network inference and
building robust and accurate latency prediction models for each specific task.
To achieve this goal, we build open-source tools which provide a convenient way
to conduct massive experiments on different target devices focusing on mobile
GPU. After evaluation of the dataset, we learn the regression model on
experimental data and use it for future latency prediction and analysis. We
experimentally demonstrate the applicability of such an approach on a subset of
popular NAS-Benchmark 101 dataset and also evaluate the most popular neural
network architectures for two mobile GPUs. As a result, we construct latency
prediction model with good precision on the target evaluation subset. We
consider LETI as a useful tool for neural architecture search or massive
latency evaluation. The project is available at this https URL


### [[2011.10464] A Reputation Mechanism Is All You Need: Collaborative Fairness and Adversarial Robustness in Federated Learning](http://arxiv.org/abs/2011.10464)


  Federated learning (FL) is an emerging practical framework for effective and
scalable machine learning among multiple participants, such as end users,
organizations and companies. However, most existing FL or distributed learning
frameworks have not well addressed two important issues together: collaborative
fairness and adversarial robustness (e.g. free-riders and malicious
participants). In conventional FL, all participants receive the global model
(equal rewards), which might be unfair to the high-contributing participants.
Furthermore, due to the lack of a safeguard mechanism, free-riders or malicious
adversaries could game the system to access the global model for free or to
sabotage it. In this paper, we propose a novel Robust and Fair Federated
Learning (RFFL) framework to achieve collaborative fairness and adversarial
robustness simultaneously via a reputation mechanism. RFFL maintains a
reputation for each participant by examining their contributions via their
uploaded gradients (using vector similarity) and thus identifies
non-contributing or malicious participants to be removed. Our approach
differentiates itself by not requiring any auxiliary/validation dataset.
Extensive experiments on benchmark datasets show that RFFL can achieve high
fairness and is very robust to different types of adversaries while achieving
competitive predictive accuracy.

    

### [[2011.12379] Invariant Representation Learning for Treatment Effect Estimation](http://arxiv.org/abs/2011.12379)


  The defining challenge for causal inference from observational data is the
presence of `confounders', covariates that affect both treatment assignment and
the outcome. To address this challenge, practitioners collect and adjust for
the covariates, hoping that they adequately correct for confounding. However,
including every observed covariate in the adjustment runs the risk of including
`bad controls', variables that induce bias when they are conditioned on. The
problem is that we do not always know which variables in the covariate set are
safe to adjust for and which are not. To address this problem, we develop
Nearly Invariant Causal Estimation (NICE). NICE uses invariant risk
minimization (IRM) [Arj19] to learn a representation of the covariates that,
under some assumptions, strips out bad controls but preserves sufficient
information to adjust for confounding. Adjusting for the learned
representation, rather than the covariates themselves, avoids the induced bias
and provides valid causal inferences. We evaluate NICE on both synthetic and
semi-synthetic data. When the covariates contain unknown collider variables and
other bad controls, NICE performs better than adjusting for all the covariates.

    

### [[2011.12423] Stochastic sparse adversarial attacks](http://arxiv.org/abs/2011.12423)


  This paper introduces stochastic sparse adversarial attacks (SSAA), simple,
fast and purely noise-based targeted and untargeted $L_0$ attacks of neural
network classifiers (NNC). SSAA are devised by exploiting a simple small-time
expansion idea widely used for Markov processes and offer new examples of $L_0$
attacks whose studies have been limited. They are designed to solve the known
scalability issue of the family of Jacobian-based saliency maps attacks to
large datasets and they succeed in solving it. Experiments on small and large
datasets (CIFAR-10 and ImageNet) illustrate further advantages of SSAA in
comparison with the-state-of-the-art methods. For instance, in the untargeted
case, our method called Voting Folded Gaussian Attack (VFGA) scales efficiently
to ImageNet and achieves a significantly lower $L_0$ score than SparseFool (up
to $\frac{2}{5}$ lower) while being faster. Moreover, VFGA achieves better
$L_0$ scores on ImageNet than Sparse-RS when both attacks are fully successful
on a large number of samples. Codes are publicly available through the link
this https URL


### [[2012.03370] Competition in Cross-situational Word Learning: A Computational Study](http://arxiv.org/abs/2012.03370)


  Children learn word meanings by tapping into the commonalities across
different situations in which words are used and overcome the high level of
uncertainty involved in early word learning experiences. We propose a modeling
framework to investigate the role of mutual exclusivity bias - asserting
one-to-one mappings between words and their meanings - in reducing uncertainty
in word learning. In a set of computational studies, we show that to
successfully learn word meanings in the face of uncertainty, a learner needs to
use two types of competition: words competing for association to a referent
when learning from an observation and referents competing for a word when the
word is used. Our work highlights the importance of an algorithmic-level
analysis to shed light on the utility of different mechanisms that can
implement the same computational-level theory.

    

### [[2012.03902] Generative Adversarial User Privacy in Lossy Single-Server Information Retrieval](http://arxiv.org/abs/2012.03902)


  We propose to extend the concept of private information retrieval by allowing
for distortion in the retrieval process and relaxing the perfect privacy
requirement at the same time. In particular, we study the tradeoff between
download rate, distortion, and user privacy leakage, and show that in the limit
of large file sizes this trade-off can be captured via a novel
information-theoretical formulation for datasets with a known distribution.
Moreover, for scenarios where the statistics of the dataset is unknown, we
propose a new deep learning framework by leveraging a generative adversarial
network approach, which allows the user to learn efficient schemes from the
data itself, minimizing the download cost. We evaluate the performance of the
scheme on a synthetic Gaussian dataset as well as on both the MNIST and
CIFAR-10 datasets. For the MNIST dataset, the data-driven approach
significantly outperforms a non-learning based scheme which combines source
coding with multiple file download, while the CIFAR-10 performance is notably
better.

    

### [[2012.05754] Optimal Thompson Sampling strategies for support-aware CVaR bandits](http://arxiv.org/abs/2012.05754)


  In this paper we study a multi-arm bandit problem in which the quality of
each arm is measured by the Conditional Value at Risk (CVaR) at some level
alpha of the reward distribution. While existing works in this setting mainly
focus on Upper Confidence Bound algorithms, we introduce a new Thompson
Sampling approach for CVaR bandits on bounded rewards that is flexible enough
to solve a variety of problems grounded on physical resources. Building on a
recent work by Riou & Honda (2020), we introduce B-CVTS for continuous bounded
rewards and M-CVTS for multinomial distributions. On the theoretical side, we
provide a non-trivial extension of their analysis that enables to theoretically
bound their CVaR regret minimization performance. Strikingly, our results show
that these strategies are the first to provably achieve asymptotic optimality
in CVaR bandits, matching the corresponding asymptotic lower bounds for this
setting. Further, we illustrate empirically the benefit of Thompson Sampling
approaches both in a realistic environment simulating a use-case in agriculture
and on various synthetic examples.

    

### [[2012.06384] An AI-Assisted Design Method for Topology Optimization Without Pre-Optimized Training Data](http://arxiv.org/abs/2012.06384)


  Topology optimization is widely used by engineers during the initial product
development process to get a first possible geometry design. The
state-of-the-art is the iterative calculation, which requires both time and
computational power. Some newly developed methods use artificial intelligence
to accelerate the topology optimization. These require conventionally
pre-optimized data and therefore are dependent on the quality and number of
available data. This paper proposes an AI-assisted design method for topology
optimization, which does not require pre-optimized data. The designs are
provided by an artificial neural network, the predictor, on the basis of
boundary conditions and degree of filling (the volume percentage filled by
material) as input data. In the training phase, geometries generated on the
basis of random input data are evaluated with respect to given criteria. The
results of those evaluations flow into an objective function which is minimized
by adapting the predictor's parameters. After the training is completed, the
presented AI-assisted design procedure supplies geometries which are similar to
the ones generated by conventional topology optimizers, but requires a small
fraction of the computational effort required by those algorithms. We
anticipate our paper to be a starting point for AI-based methods that requires
data, that is hard to compute or not available.

    

### [[2012.06575] Entropy Maximization and Meta Classification for Out-Of-Distribution Detection in Semantic Segmentation](http://arxiv.org/abs/2012.06575)


  Deep neural networks (DNNs) for the semantic segmentation of images are
usually trained to operate on a predefined closed set of object classes. This
is in contrast to the "open world" setting where DNNs are envisioned to be
deployed to. From a functional safety point of view, the ability to detect
so-called "out-of-distribution" (OoD) samples, i.e., objects outside of a DNN's
semantic space, is crucial for many applications such as automated driving. A
natural baseline approach to OoD detection is to threshold on the pixel-wise
softmax entropy. We present a two-step procedure that significantly improves
that approach. Firstly, we utilize samples from the COCO dataset as OoD proxy
and introduce a second training objective to maximize the softmax entropy on
these samples. Starting from pretrained semantic segmentation networks we
re-train a number of DNNs on different in-distribution datasets and
consistently observe improved OoD detection performance when evaluating on
completely disjoint OoD datasets. Secondly, we perform a transparent
post-processing step to discard false positive OoD samples by so-called "meta
classification". To this end, we apply linear models to a set of hand-crafted
metrics derived from the DNN's softmax probabilities. In our experiments we
consistently observe a clear additional gain in OoD detection performance,
cutting down the number of detection errors by up to 52% when comparing the
best baseline with our results. We achieve this improvement sacrificing only
marginally in original segmentation performance. Therefore, our method
contributes to safer DNNs with more reliable overall system performance.

    

### [[2012.08158] Frozen-to-Paraffin: Categorization of Histological Frozen Sections by the Aid of Paraffin Sections and Generative Adversarial Networks](http://arxiv.org/abs/2012.08158)


  In contrast to paraffin sections, frozen sections can be quickly generated
during surgical interventions. This procedure allows surgeons to wait for
histological findings during the intervention to base intra-operative decisions
on the outcome of the histology. However, compared to paraffin sections, the
quality of frozen sections is typically lower, leading to a higher ratio of
miss-classification. In this work, we investigated the effect of the section
type on automated decision support approaches for classification of thyroid
cancer. This was enabled by a data set consisting of pairs of sections for
individual patients. Moreover, we investigated, whether a frozen-to-paraffin
translation could help to optimize classification scores. Finally, we propose a
specific data augmentation strategy to deal with a small amount of training
data and to increase classification accuracy even further.

    

### [[2101.11508] Effects of Image Size on Deep Learning](http://arxiv.org/abs/2101.11508)


  This paper presents the evaluation of effects of image size on deep learning
performance via semantic segmentation of magnetic resonance heart images with
U-net for fully automated quantification of myocardial infarction. Both
non-extra pixel and extra pixel interpolation algorithms are used to change the
size of images in datasets of interest. Extra class labels, in interpolated
ground truth segmentation images, are removed using thresholding, median
filtering, and subtraction strategies. Common class metrics are used to
evaluate the quality of semantic segmentation with U-net against the ground
truth segmentation while arbitrary threshold, comparison of the sums, and sums
of differences between medical experts and fully automated results are options
used to estimate the relationship between medical experts-based quantification
and fully automated quantification results.

    

### [[2102.00310] Symmetry-Aware Reservoir Computing](http://arxiv.org/abs/2102.00310)


  We demonstrate that matching the symmetry properties of a reservoir computer
(RC) to the data being processed dramatically increases its processing power.
We apply our method to the parity task, a challenging benchmark problem that
highlights inversion and permutation symmetries, and to a chaotic system
inference task that presents an inversion symmetry rule. For the parity task,
our symmetry-aware RC obtains zero error using an exponentially reduced neural
network and training data, greatly speeding up the time to result and
outperforming hand crafted artificial neural networks. When both symmetries are
respected, we find that the network size $N$ necessary to obtain zero error for
50 different RC instances scales linearly with the parity-order $n$. Moreover,
some symmetry-aware RC instances perform a zero error classification with only
$N=1$ for $n\leq7$. Furthermore, we show that a symmetry-aware RC only needs a
training data set with size on the order of $(n+n/2)$ to obtain such
performance, an exponential reduction in comparison to a regular RC which
requires a training data set with size on the order of $n2^n$ to contain all
$2^n$ possible $n-$bit-long sequences. For the inference task, we show that a
symmetry-aware RC presents a normalized root-mean-square error three
orders-of-magnitude smaller than regular RCs. For both tasks, our RC approach
respects the symmetries by adjusting only the input and the output layers, and
not by problem-based modifications to the neural network. We anticipate that
generalizations of our procedure can be applied in information processing for
problems with known symmetries.

    

### [[2102.00473] Information fusion between knowledge and data in Bayesian network structure learning](http://arxiv.org/abs/2102.00473)


  Bayesian Networks (BNs) have become a powerful technology for reasoning under
uncertainty, particularly in areas that require causal assumptions that enable
us to simulate the effect of intervention. The graphical structure of these
models can be determined by causal knowledge, learnt from data, or a
combination of both. While it seems plausible that the best approach in
constructing a causal graph involves combining knowledge with machine learning,
this approach remains underused in practice. We implement and evaluate 10
knowledge approaches with application to different case studies and BN
structure learning algorithms available in the open-source Bayesys structure
learning system. The approaches enable us to specify pre-existing knowledge
that can be obtained from heterogeneous sources, to constrain or guide
structure learning. Each approach is assessed in terms of structure learning
effectiveness and efficiency, including graphical accuracy, model fitting,
complexity, and runtime; making this the first paper that provides a
comparative evaluation of a wide range of knowledge approaches for BN structure
learning. Because the value of knowledge depends on what data are available, we
illustrate the results both with limited and big data. While the overall
results show that knowledge becomes less important with big data due to higher
learning accuracy rendering knowledge less important, some of the knowledge
approaches are actually found to be more important with big data. Amongst the
main conclusions is the observation that reduced search space obtained from
knowledge does not always imply reduced computational complexity, perhaps
because the relationships implied by the data and knowledge are in tension.

    

### [[2102.04896] Formalising the Use of the Activation Function in Neural Inference](http://arxiv.org/abs/2102.04896)


  We investigate how the activation function can be used to describe neural
firing in an abstract way, and in turn, why it works well in artificial neural
networks. We discuss how a spike in a biological neurone belongs to a
particular universality class of phase transitions in statistical physics. We
then show that the artificial neurone is, mathematically, a mean field model of
biological neural membrane dynamics, which arises from modelling spiking as a
phase transition. This allows us to treat selective neural firing in an
abstract way, and formalise the role of the activation function in perceptron
learning. The resultant statistical physical model allows us to recover the
expressions for some known activation functions as various special cases. Along
with deriving this model and specifying the analogous neural case, we analyse
the phase transition to understand the physics of neural network learning.
Together, it is shown that there is not only a biological meaning, but a
physical justification, for the emergence and performance of typical activation
functions; implications for neural learning and inference are also discussed.

    

### [[2102.10205] CKNet: A Convolutional Neural Network Based on Koopman Operator for Modeling Latent Dynamics from Pixels](http://arxiv.org/abs/2102.10205)


  With the development of end-to-end control based on deep learning, it is
important to study new system modeling techniques to realize dynamics modeling
with high-dimensional inputs. In this paper, a novel Koopman-based deep
convolutional network, called CKNet, is proposed to identify latent dynamics
from raw pixels. CKNet learns an encoder and decoder to play the role of the
Koopman eigenfunctions and modes, respectively. The Koopman eigenvalues can be
approximated by eigenvalues of the learned state transition matrix. The
deterministic convolutional Koopman network (DCKNet) and the variational
convolutional Koopman network (VCKNet) are proposed to span some subspace for
approximating the Koopman operator respectively. Because CKNet is trained under
the constraints of the Koopman theory, the identified latent dynamics is in a
linear form and has good interpretability. Besides, the state transition and
control matrices are trained as trainable tensors so that the identified
dynamics is also time-invariant. We also design an auxiliary weight term for
reducing multi-step linearity and prediction losses. Experiments were conducted
on two offline trained and four online trained nonlinear forced dynamical
systems with continuous action spaces in Gym and Mujoco environment
respectively, and the results show that identified dynamics are adequate for
approximating the latent dynamics and generating clear images. Especially for
offline trained cases, this work confirms CKNet from a novel perspective that
we visualize the evolutionary processes of the latent states and the Koopman
eigenfunctions with DCKNet and VCKNet separately to each task based on the same
episode and results demonstrate that different approaches learn similar
features in shapes.

    

### [[2102.12040] Active Learning to Classify Macromolecular Structures in situ for Less Supervision in Cryo-Electron Tomography](http://arxiv.org/abs/2102.12040)


  Motivation: Cryo-Electron Tomography (cryo-ET) is a 3D bioimaging tool that
visualizes the structural and spatial organization of macromolecules at a
near-native state in single cells, which has broad applications in life
science. However, the systematic structural recognition and recovery of
macromolecules captured by cryo-ET are difficult due to high structural
complexity and imaging limits. Deep learning based subtomogram classification
have played critical roles for such tasks. As supervised approaches, however,
their performance relies on sufficient and laborious annotation on a large
training dataset.
Results: To alleviate this major labeling burden, we proposed a Hybrid Active
Learning (HAL) framework for querying subtomograms for labelling from a large
unlabeled subtomogram pool. Firstly, HAL adopts uncertainty sampling to select
the subtomograms that have the most uncertain predictions. Moreover, to
mitigate the sampling bias caused by such strategy, a discriminator is
introduced to judge if a certain subtomogram is labeled or unlabeled and
subsequently the model queries the subtomogram that have higher probabilities
to be unlabeled. Additionally, HAL introduces a subset sampling strategy to
improve the diversity of the query set, so that the information overlap is
decreased between the queried batches and the algorithmic efficiency is
improved. Our experiments on subtomogram classification tasks using both
simulated and real data demonstrate that we can achieve comparable testing
performance (on average only 3% accuracy drop) by using less than 30% of the
labeled subtomograms, which shows a very promising result for subtomogram
classification task with limited labeling resources.

    

### [[2102.12833] Diffusion Earth Mover's Distance and Distribution Embeddings](http://arxiv.org/abs/2102.12833)


  We propose a new fast method of measuring distances between large numbers of
related high dimensional datasets called the Diffusion Earth Mover's Distance
(EMD). We model the datasets as distributions supported on common data graph
that is derived from the affinity matrix computed on the combined data. In such
cases where the graph is a discretization of an underlying Riemannian closed
manifold, we prove that Diffusion EMD is topologically equivalent to the
standard EMD with a geodesic ground distance. Diffusion EMD can be computed in
$\tilde{O}(n)$ time and is more accurate than similarly fast algorithms such as
tree-based EMDs. We also show Diffusion EMD is fully differentiable, making it
amenable to future uses in gradient-descent frameworks such as deep neural
networks. Finally, we demonstrate an application of Diffusion EMD to single
cell data collected from 210 COVID-19 patient samples at Yale New Haven
Hospital. Here, Diffusion EMD can derive distances between patients on the
manifold of cells at least two orders of magnitude faster than equally accurate
methods. This distance matrix between patients can be embedded into a higher
level patient manifold which uncovers structure and heterogeneity in patients.
More generally, Diffusion EMD is applicable to all datasets that are massively
collected in parallel in many medical and biological systems.

    

### [[2103.00147] Statistical Measures For Defining Curriculum Scoring Function](http://arxiv.org/abs/2103.00147)


  Curriculum learning is a training strategy that sorts the training examples
by some measure of their difficulty and gradually exposes them to the learner
to improve the network performance. Motivated by our insights from implicit
curriculum ordering, we first introduce a simple curriculum learning strategy
that uses statistical measures such as standard deviation and entropy values to
score the difficulty of data points for real image classification tasks. We
empirically show its improvements in performance with convolutional and
fully-connected neural networks on multiple real image datasets. We also
propose and study the performance of a dynamic curriculum learning algorithm.
Our dynamic curriculum algorithm tries to reduce the distance between the
network weight and an optimal weight at any training step by greedily sampling
examples with gradients that are directed towards the optimal weight. Further,
we use our algorithms to discuss why curriculum learning is helpful.

    

### [[2103.08265] Constant Random Perturbations Provide Adversarial Robustness with Minimal Effect on Accuracy](http://arxiv.org/abs/2103.08265)


  This paper proposes an attack-independent (non-adversarial training)
technique for improving adversarial robustness of neural network models, with
minimal loss of standard accuracy. We suggest creating a neighborhood around
each training example, such that the label is kept constant for all inputs
within that neighborhood. Unlike previous work that follows a similar
principle, we apply this idea by extending the training set with multiple
perturbations for each training example, drawn from within the neighborhood.
These perturbations are model independent, and remain constant throughout the
entire training process. We analyzed our method empirically on MNIST, SVHN, and
CIFAR-10, under different attacks and conditions. Results suggest that the
proposed approach improves standard accuracy over other defenses while having
increased robustness compared to vanilla adversarial training.

    

### [[2103.16074] PointBA: Towards Backdoor Attacks in 3D Point Cloud](http://arxiv.org/abs/2103.16074)


  3D deep learning has been increasingly more popular for a variety of tasks
including many safety-critical applications. However, recently several works
raise the security issues of 3D deep nets. Although most of these works
consider adversarial attacks, we identify that backdoor attack is indeed a more
serious threat to 3D deep learning systems but remains unexplored. We present
the backdoor attacks in 3D with a unified framework that exploits the unique
properties of 3D data and networks. In particular, we design two attack
approaches: the poison-label attack and the clean-label attack. The first one
is straightforward and effective in practice, while the second one is more
sophisticated assuming there are certain data inspections. The attack
algorithms are mainly motivated and developed by 1) the recent discovery of 3D
adversarial samples which demonstrate the vulnerability of 3D deep nets under
spatial transformations; 2) the proposed feature disentanglement technique that
manipulates the feature of the data through optimization methods and its
potential to embed a new task. Extensive experiments show the efficacy of the
poison-label attack with over 95% success rate across several 3D datasets and
models, and the ability of clean-label attack against data filtering with
around 50% success rate. Our proposed backdoor attack in 3D point cloud is
expected to perform as a baseline for improving the robustness of 3D deep
models.

    

### [[2104.00355] Speech Resynthesis from Discrete Disentangled Self-Supervised Representations](http://arxiv.org/abs/2104.00355)


  We propose using self-supervised discrete representations for the task of
speech resynthesis. To generate disentangled representation, we separately
extract low-bitrate representations for speech content, prosodic information,
and speaker identity. This allows to synthesize speech in a controllable
manner. We analyze various state-of-the-art, self-supervised representation
learning methods and shed light on the advantages of each method while
considering reconstruction quality and disentanglement properties.
Specifically, we evaluate the F0 reconstruction, speaker identification
performance (for both resynthesis and voice conversion), recordings'
intelligibility, and overall quality using subjective human evaluation. Lastly,
we demonstrate how these representations can be used for an ultra-lightweight
speech codec. Using the obtained representations, we can get to a rate of 365
bits per second while providing better speech quality than the baseline
methods. Audio samples can be found under the following link:
this http URL.

    

### [[2104.00415] Learning with Neural Tangent Kernels in Near Input Sparsity Time](http://arxiv.org/abs/2104.00415)


  The Neural Tangent Kernel (NTK) characterizes the behavior of infinitely wide
neural nets trained under least squares loss by gradient descent. However,
despite its importance, the super-quadratic runtime of kernel methods limits
the use of NTK in large-scale learning tasks. To accelerate kernel machines
with NTK, we propose a near input sparsity time algorithm that maps the input
data to a randomized low-dimensional feature space so that the inner product of
the transformed data approximates their NTK evaluation. Our transformation
works by sketching the polynomial expansions of arc-cosine kernels.
Furthermore, we propose a feature map for approximating the convolutional
counterpart of the NTK, which can transform any image using a runtime that is
only linear in the number of pixels. We show that in standard large-scale
regression and classification tasks a linear regressor trained on our features
outperforms trained Neural Nets and Nystrom approximation of NTK kernel.

    

### [[2104.02477] Deep Transfer Learning based COVID-19 Detection in Cough, Breath and Speech using Bottleneck Features](http://arxiv.org/abs/2104.02477)


  We present an experimental investigation into the automatic detection of
COVID-19 from coughs, breaths and speech as this type of screening is
non-contact, does not require specialist medical expertise or laboratory
facilities and can easily be deployed on inexpensive consumer hardware.
Smartphone recordings of cough, breath and speech from subjects around the
globe are used for classification by seven standard machine learning
classifiers using leave-$p$-out cross-validation to provide a promising
baseline performance.
Then, a diverse dataset of 10.29 hours of cough, sneeze, speech and noise
audio recordings are used to pre-train a CNN, LSTM and Resnet50 classifier and
fine tuned the model to enhance the performance even further.
We have also extracted the bottleneck features from these pre-trained models
by removing the final-two layers and used them as an input to the LR, SVM, MLP
and KNN classifiers to detect COVID-19 signature.
The highest AUC of 0.98 was achieved using a transfer learning based Resnet50
architecture on coughs from Coswara dataset.
The highest AUC of 0.94 and 0.92 was achieved from an SVM run on the
bottleneck features extracted from the breaths from Coswara dataset and speech
recordings from ComParE dataset.
We conclude that among all vocal audio, coughs carry the strongest COVID-19
signature followed by breath and speech and using transfer learning improves
the classifier performance with higher AUC and lower variance across the
cross-validation folds.
Although these signatures are not perceivable by human ear, machine learning
based COVID-19 detection is possible from vocal audio recorded via smartphone.

    

### [[2104.09630] Quaternion Generative Adversarial Networks](http://arxiv.org/abs/2104.09630)


  Latest Generative Adversarial Networks (GANs) are gathering outstanding
results through a large-scale training, thus employing models composed of
millions of parameters requiring extensive computational capabilities. Building
such huge models undermines their replicability and increases the training
instability. Moreover, multi-channel data, such as images or audio, are usually
processed by realvalued convolutional networks that flatten and concatenate the
input, often losing intra-channel spatial relations. To address these issues
related to complexity and information loss, we propose a family of
quaternion-valued generative adversarial networks (QGANs). QGANs exploit the
properties of quaternion algebra, e.g., the Hamilton product, that allows to
process channels as a single entity and capture internal latent relations,
while reducing by a factor of 4 the overall number of parameters. We show how
to design QGANs and to extend the proposed approach even to advanced models.We
compare the proposed QGANs with real-valued counterparts on several image
generation benchmarks. Results show that QGANs are able to obtain better FID
scores than real-valued GANs and to generate visually pleasing images.
Furthermore, QGANs save up to 75% of the training parameters. We believe these
results may pave the way to novel, more accessible, GANs capable of improving
performance and saving computational resources.

    

### [[2104.12827] Learning-based decentralized offloading decision making in an adversarial environment](http://arxiv.org/abs/2104.12827)


  Vehicular fog computing (VFC) pushes the cloud computing capability to the
distributed fog nodes at the edge of the Internet, enabling compute-intensive
and latency-sensitive computing services for vehicles through task offloading.
However, a heterogeneous mobility environment introduces uncertainties in terms
of resource supply and demand, which are inevitable bottlenecks for the optimal
offloading decision. Also, these uncertainties bring extra challenges to task
offloading under the oblivious adversary attack and data privacy risks. In this
article, we develop a new adversarial online learning algorithm with bandit
feedback based on the adversarial multi-armed bandit theory, to enable scalable
and low-complexity offloading decision making. Specifically, we focus on
optimizing fog node selection with the aim of minimizing the offloading service
costs in terms of delay and energy. The key is to implicitly tune the
exploration bonus in the selection process and the assessment rules of the
designed algorithm, taking into account volatile resource supply and demand. We
theoretically prove that the input-size dependent selection rule allows to
choose a suitable fog node without exploring the sub-optimal actions, and also
an appropriate score patching rule allows to quickly adapt to evolving
circumstances, which reduce variance and bias simultaneously, thereby achieving
a better exploitation-exploration balance. Simulation results verify the
effectiveness and robustness of the proposed algorithm.

    

### [[2105.03464] Estimating Parkinsonism Severity in Natural Gait Videos of Older Adults with Dementia](http://arxiv.org/abs/2105.03464)


  Drug-induced parkinsonism affects many older adults with dementia, often
causing gait disturbances. New advances in vision-based human pose-estimation
have opened possibilities for frequent and unobtrusive analysis of gait in
residential settings. This work proposes novel spatial-temporal graph
convolutional network (ST-GCN) architectures and training procedures to predict
clinical scores of parkinsonism in gait from video of individuals with
dementia. We propose a two-stage training approach consisting of a
self-supervised pretraining stage that encourages the ST-GCN model to learn
about gait patterns before predicting clinical scores in the finetuning stage.
The proposed ST-GCN models are evaluated on joint trajectories extracted from
video and are compared against traditional (ordinal, linear, random forest)
regression models and temporal convolutional network baselines. Three 2D human
pose-estimation libraries (OpenPose, Detectron, AlphaPose) and the Microsoft
Kinect (2D and 3D) are used to extract joint trajectories of 4787 natural
walking bouts from 53 older adults with dementia. A subset of 399 walks from 14
participants is annotated with scores of parkinsonism severity on the gait
criteria of the Unified Parkinson's Disease Rating Scale (UPDRS) and the
Simpson-Angus Scale (SAS). Our results demonstrate that ST-GCN models operating
on 3D joint trajectories extracted from the Kinect consistently outperform all
other models and feature sets. Prediction of parkinsonism scores in natural
walking bouts of unseen participants remains a challenging task, with the best
models achieving macro-averaged F1-scores of 0.53 +/- 0.03 and 0.40 +/- 0.02
for UPDRS-gait and SAS-gait, respectively. Pre-trained model and demo code for
this work is available:
this https URL.

    

### [[2105.12033] Model-Constrained Deep Learning Approaches for Inverse Problems](http://arxiv.org/abs/2105.12033)


  Deep Learning (DL), in particular deep neural networks (DNN), by design is
purely data-driven and in general does not require physics. This is the
strength of DL but also one of its key limitations when applied to science and
engineering problems in which underlying physical properties (such as
stability, conservation, and positivity) and desired accuracy need to be
achieved. DL methods in their original forms are not capable of respecting the
underlying mathematical models or achieving desired accuracy even in big-data
regimes. On the other hand, many data-driven science and engineering problems,
such as inverse problems, typically have limited experimental or observational
data, and DL would overfit the data in this case. Leveraging information
encoded in the underlying mathematical models, we argue, not only compensates
missing information in low data regimes but also provides opportunities to
equip DL methods with the underlying physics and hence obtaining higher
accuracy. This short communication introduces several model-constrained DL
approaches (including both feed-forward DNN and autoencoders) that are capable
of learning not only information hidden in the training data but also in the
underlying mathematical models to solve inverse problems. We present and
provide intuitions for our formulations for general nonlinear problems. For
linear inverse problems and linear networks, the first order optimality
conditions show that our model-constrained DL approaches can learn information
encoded in the underlying mathematical models, and thus can produce consistent
or equivalent inverse solutions, while naive purely data-based counterparts
cannot.

    

### [[2106.11548] Adaptive Learning Rate and Momentum for Training Deep Neural Networks](http://arxiv.org/abs/2106.11548)


  Recent progress on deep learning relies heavily on the quality and efficiency
of training algorithms. In this paper, we develop a fast training method
motivated by the nonlinear Conjugate Gradient (CG) framework. We propose the
Conjugate Gradient with Quadratic line-search (CGQ) method. On the one hand, a
quadratic line-search determines the step size according to current loss
landscape. On the other hand, the momentum factor is dynamically updated in
computing the conjugate gradient parameter (like Polak-Ribiere). Theoretical
results to ensure the convergence of our method in strong convex settings is
developed. And experiments in image classification datasets show that our
method yields faster convergence than other local solvers and has better
generalization capability (test set accuracy). One major advantage of the paper
method is that tedious hand tuning of hyperparameters like the learning rate
and momentum is avoided.

    

### [[2106.13493] Online Self-Attentive Gated RNNs for Real-Time Speaker Separation](http://arxiv.org/abs/2106.13493)


  Deep neural networks have recently shown great success in the task of blind
source separation, both under monaural and binaural settings. Although these
methods were shown to produce high-quality separations, they were mainly
applied under offline settings, in which the model has access to the full input
signal while separating the signal. In this study, we convert a non-causal
state-of-the-art separation model into a causal and real-time model and
evaluate its performance under both online and offline settings. We compare the
performance of the proposed model to several baseline methods under anechoic,
noisy, and noisy-reverberant recording conditions while exploring both monaural
and binaural inputs and outputs. Our findings shed light on the relative
difference between causal and non-causal models when performing separation. Our
stateful implementation for online separation leads to a minor drop in
performance compared to the offline model; 0.8dB for monaural inputs and 0.3dB
for binaural inputs while reaching a real-time factor of 0.65. Samples can be
found under the following link:
this https URL.

    

### [[2106.13948] Core Challenges in Embodied Vision-Language Planning](http://arxiv.org/abs/2106.13948)


  Recent advances in the areas of multimodal machine learning and artificial
intelligence (AI) have led to the development of challenging tasks at the
intersection of Computer Vision, Natural Language Processing, and Embodied AI.
Whereas many approaches and previous survey pursuits have characterised one or
two of these dimensions, there has not been a holistic analysis at the center
of all three. Moreover, even when combinations of these topics are considered,
more focus is placed on describing, e.g., current architectural methods, as
opposed to also illustrating high-level challenges and opportunities for the
field. In this survey paper, we discuss Embodied Vision-Language Planning
(EVLP) tasks, a family of prominent embodied navigation and manipulation
problems that jointly use computer vision and natural language. We propose a
taxonomy to unify these tasks and provide an in-depth analysis and comparison
of the new and current algorithmic approaches, metrics, simulated environments,
as well as the datasets used for EVLP tasks. Finally, we present the core
challenges that we believe new EVLP works should seek to address, and we
advocate for task construction that enables model generalizability and furthers
real-world deployment.

    

### [[2106.15292] Adaptive Sample Selection for Robust Learning under Label Noise](http://arxiv.org/abs/2106.15292)


  Deep Neural Networks (DNNs) have been shown to be susceptible to memorization
or overfitting in the presence of noisily labelled data. For the problem of
robust learning under such noisy data, several algorithms have been proposed. A
prominent class of algorithms rely on sample selection strategies, motivated by
curriculum learning. For example, many algorithms use the `small loss trick'
wherein a fraction of samples with loss values below a certain threshold are
selected for training. These algorithms are sensitive to such thresholds, and
it is difficult to fix or learn these thresholds. Often, these algorithms also
require information such as label noise rates which are typically unavailable
in practice. In this paper, we propose a data-dependent, adaptive sample
selection strategy that relies only on batch statistics of a given mini-batch
to provide robustness against label noise. The algorithm does not have any
additional hyperparameters for sample selection, does not need any information
on noise rates, and does not need access to separate data with clean labels. We
empirically demonstrate the effectiveness of our algorithm on benchmark
datasets.

    

### [[2107.12346] Beyond Voice Identity Conversion: Manipulating Voice Attributes by Adversarial Learning of Structured Disentangled Representations](http://arxiv.org/abs/2107.12346)


  Voice conversion (VC) consists of digitally altering the voice of an
individual to manipulate part of its content, primarily its identity, while
maintaining the rest unchanged. Research in neural VC has accomplished
considerable breakthroughs with the capacity to falsify a voice identity using
a small amount of data with a highly realistic rendering. This paper goes
beyond voice identity and presents a neural architecture that allows the
manipulation of voice attributes (e.g., gender and age). Leveraging the latest
advances on adversarial learning of structured speech representation, a novel
structured neural network is proposed in which multiple auto-encoders are used
to encode speech as a set of idealistically independent linguistic and
extra-linguistic representations, which are learned adversariarly and can be
manipulated during VC. Moreover, the proposed architecture is time-synchronized
so that the original voice timing is preserved during conversion which allows
lip-sync applications. Applied to voice gender conversion on the real-world
VCTK dataset, our proposed architecture can learn successfully
gender-independent representation and convert the voice gender with a very high
efficiency and naturalness.

    

### [[2107.12102] Global optimization using random embeddings](http://arxiv.org/abs/2107.12102)


  We propose a random-subspace algorithmic framework for global optimization of
Lipschitz-continuous objectives, and analyse its convergence using novel tools
from conic integral geometry. X-REGO randomly projects, in a sequential or
simultaneous manner, the high-dimensional original problem into low-dimensional
subproblems that can then be solved with any global, or even local,
optimization solver. We estimate the probability that the randomly-embedded
subproblem shares (approximately) the same global optimum as the original
problem. This success probability is then used to show convergence of X-REGO to
an approximate global solution of the original problem, under weak assumptions
on the problem (having a strictly feasible global solution) and on the solver
(guaranteed to find an approximate global solution of the reduced problem with
sufficiently high probability). In the particular case of unconstrained
objectives with low effective dimension, that only vary over a low-dimensional
subspace, we propose an X-REGO variant that explores random subspaces of
increasing dimension until finding the effective dimension of the problem,
leading to X-REGO globally converging after a finite number of embeddings,
proportional to the effective dimension. We show numerically that this variant
efficiently finds both the effective dimension and an approximate global
minimizer of the original problem.

    

### [[2107.12295] A Unified Deep Model of Learning from both Data and Queries for Cardinality Estimation](http://arxiv.org/abs/2107.12295)


  Cardinality estimation is a fundamental problem in database systems. To
capture the rich joint data distributions of a relational table, most of the
existing work either uses data as unsupervised information or uses query
workload as supervised information. Very little work has been done to use both
types of information, and cannot fully make use of both types of information to
learn the joint data distribution. In this work, we aim to close the gap
between data-driven and query-driven methods by proposing a new unified deep
autoregressive model, UAE, that learns the joint data distribution from both
the data and query workload. First, to enable using the supervised query
information in the deep autoregressive model, we develop differentiable
progressive sampling using the Gumbel-Softmax trick. Second, UAE is able to
utilize both types of information to learn the joint data distribution in a
single model. Comprehensive experimental results demonstrate that UAE achieves
single-digit multiplicative error at tail, better accuracies over
state-of-the-art methods, and is both space and time efficient.

    

### [[2107.12922] Design Space Exploration of Sparse Accelerators for Deep Neural Networks](http://arxiv.org/abs/2107.12922)


  Novel architectures for deep learning exploit both activation and weight
sparsity to improve the performance of DNN inference. However, this speedup
usually brings non-negligible overheads which diminish the efficiency of such
designs when running dense models. These overheads specifically are exacerbated
for low precision accelerators with optimized SRAM size per core. This paper
examines the design space trade-offs of such accelerators aiming to achieve
competitive performance and efficiency metrics for all four combinations of
dense or sparse activation/weight tensors. To do so, we systematically examine
overheads of supporting sparsity on top of an optimized dense core. These
overheads are modeled based on parameters that indicate how a multiplier can
borrow a nonzero operation from the neighboring multipliers or future cycles.
As a result of this exploration, we identify a few promising designs that
perform better than prior work. Our findings suggest that even a best design
targeting dual sparsity yields 20%-30% drop in power efficiency when performing
on single sparse models, i.e., those with only sparse weight or sparse
activation tensors. We introduce novel techniques to reuse resources of the
same core to maintain high performance and efficiency when running single
sparsity or dense models. We call this hybrid design Griffin. Griffin is 1.2,
3.0, 3.1, and 1.4X more power efficient than state-of-the-art sparse
architectures, for dense, weight-only sparse, activation-only sparse, and dual
sparse models, respectively.

    

### [[2107.12371] An FPGA cached sparse matrix vector product (SpMV) for unstructured computational fluid dynamics simulations](http://arxiv.org/abs/2107.12371)


  Field Programmable Gate Arrays generate algorithmic specific architectures
that improve the code's FLOP per watt ratio. Such devices are re-gaining
interest due to the rise of new tools that facilitate their programming, such
as OmpSs. The computational fluid dynamics community is always investigating
new architectures that can improve its algorithm's performance. Commonly, those
algorithms have a low arithmetic intensity and only reach a small percentage of
the peak performance. The sparse matrix-vector multiplication is one of the
most time-consuming operations on unstructured simulations. The matrix's
sparsity pattern determines the indirect memory accesses of the multiplying
vector. This data path is hard to predict, making traditional implementations
fail. In this work, we present an FPGA architecture that maximizes the vector's
re-usability by introducing a cache-like architecture. The cache is implemented
as a circular list that maintains the BRAM vector components while needed.
Following this strategy, up to 16 times of acceleration is obtained compared to
a naive implementation of the algorithm.

    

### [[2107.12486] AI Multi-Tenancy on Edge: Concurrent Deep Learning Model Executions and Dynamic Model Placements on Edge Devices](http://arxiv.org/abs/2107.12486)


  Many real-world applications are widely adopting the edge computing paradigm
due to its low latency and better privacy protection. With notable success in
AI and deep learning (DL), edge devices and AI accelerators play a crucial role
in deploying DL inference services at the edge of the Internet. While prior
works quantified various edge devices' efficiency, most studies focused on the
performance of edge devices with single DL tasks. Therefore, there is an urgent
need to investigate AI multi-tenancy on edge devices, required by many advanced
DL applications for edge computing.
This work investigates two techniques - concurrent model executions and
dynamic model placements - for AI multi-tenancy on edge devices. With image
classification as an example scenario, we empirically evaluate AI multi-tenancy
on various edge devices, AI accelerators, and DL frameworks to identify its
benefits and limitations. Our results show that multi-tenancy significantly
improves DL inference throughput by up to 3.3x -- 3.8x on Jetson TX2. These AI
multi-tenancy techniques also open up new opportunities for flexible deployment
of multiple DL services on edge devices and AI accelerators.

    

### [[2107.12519] Proactive Composition of Mobile IoT Energy Services](http://arxiv.org/abs/2107.12519)


  We propose a novel proactive composition framework of wireless energy
services in a crowdsourced IoT environment. We define a new model for energy
services and requests that includes providers' and consumers' mobility patterns
and energy usage behavior. The proposed composition approach leverages the
mobility and energy usage behavior to generate energy services and requests
proactively. Preliminary experimental results demonstrate the effectiveness of
generating proactive energy requests and composing proactive services.

    

### [[2107.12563] Parallel Detection for Efficient Video Analytics at the Edge](http://arxiv.org/abs/2107.12563)


  Deep Neural Network (DNN) trained object detectors are widely deployed in
many mission-critical systems for real time video analytics at the edge, such
as autonomous driving and video surveillance. A common performance requirement
in these mission-critical edge services is the near real-time latency of online
object detection on edge devices. However, even with well-trained DNN object
detectors, the online detection quality at edge may deteriorate for a number of
reasons, such as limited capacity to run DNN object detection models on
heterogeneous edge devices, and detection quality degradation due to random
frame dropping when the detection processing rate is significantly slower than
the incoming video frame rate. This paper addresses these problems by
exploiting multi-model multi-device detection parallelism for fast object
detection in edge systems with heterogeneous edge devices. First, we analyze
the performance bottleneck of running a well-trained DNN model at edge for real
time online object detection. We use the offline detection as a reference
model, and examine the root cause by analyzing the mismatch among the incoming
video streaming rate, video processing rate for object detection, and output
rate for real time detection visualization of video streaming. Second, we study
performance optimizations by exploiting multi-model detection parallelism. We
show that the model-parallel detection approach can effectively speed up the
FPS detection processing rate, minimizing the FPS disparity with the incoming
video frame rate on heterogeneous edge devices. We evaluate the proposed
approach using SSD300 and YOLOv3 on benchmark videos of different video stream
rates. The results show that exploiting multi-model detection parallelism can
speed up the online object detection processing rate and deliver near real-time
object detection performance for efficient video analytics at edge.

    

### [[2107.12581] Average-Case Analysis of Greedy Matching for D2D Resource Sharing](http://arxiv.org/abs/2107.12581)


  Given the proximity of many wireless users and their diversity in consuming
local resources (e.g., data-plans, computation and even energy resources),
device-to-device (D2D) resource sharing is a promising approach towards
realizing a sharing economy. In the resulting networked economy, $n$ users
segment themselves into sellers and buyers that need to be efficiently matched
locally. This paper adopts an easy-to-implement greedy matching algorithm with
distributed fashion and only sub-linear $O(\log n)$ parallel complexity, which
offers a great advantage compared to the optimal but computational-expensive
centralized matching. But is it efficient compared to the optimal matching?
Extensive simulations indicate that in a large number of practical cases the
average loss is no more than $10\%$, a far better result than the $50\%$ loss
bound in the worst case. However, there is no rigorous average-case analysis in
the literature to back up such encouraging findings, which is a fundamental
step towards supporting the practical use of greedy matching in D2D sharing.
This paper is the first to present the rigorous average analysis of certain
representative classes of graphs with random parameters, by proposing a new
asymptotic methodology. For typical 2D grids with random matching weights we
rigorously prove that our greedy algorithm performs better than $84.9\%$ of the
optimal, while for typical Erdos-Renyi random graphs we prove a lower bound of
$79\%$ when the graph is neither dense nor sparse. Finally, we use realistic
data to show that our random graph models approximate well D2D sharing networks
encountered in practice.

    

### [[2107.12603] Federated Learning Meets Natural Language Processing: A Survey](http://arxiv.org/abs/2107.12603)


  Federated Learning aims to learn machine learning models from multiple
decentralized edge devices (e.g. mobiles) or servers without sacrificing local
data privacy. Recent Natural Language Processing techniques rely on deep
learning and large pre-trained language models. However, both big deep neural
and language models are trained with huge amounts of data which often lies on
the server side. Since text data is widely originated from end users, in this
work, we look into recent NLP models and techniques which use federated
learning as the learning framework. Our survey discusses major challenges in
federated natural language processing, including the algorithm challenges,
system challenges as well as the privacy issues. We also provide a critical
review of the existing Federated NLP evaluation methods and tools. Finally, we
highlight the current research gaps and future directions.

    

### [[2107.12720] Efficient Parallel Graph Trimming by Arc-Consistency](http://arxiv.org/abs/2107.12720)


  Given a large data graph, trimming techniques can reduce the search space by
removing vertices without outgoing edges. One application is to speed up the
parallel decomposition of graphs into strongly connected components (SCC
decomposition), which is a fundamental step for analyzing graphs. We observe
that graph trimming is essentially a kind of arc-consistency problem, and AC-3,
AC-4, and AC-6 are the most relevant arc-consistency algorithms for application
to graph trimming. The existing parallel graph trimming methods require
worst-case $\mathcal O(nm)$ time and worst-case $\mathcal O(n)$ space for
graphs with $n$ vertices and $m$ edges. We call these parallel AC-3-based as
they are much like the AC-3 algorithm. In this work, we propose AC-4-based and
AC-6-based trimming methods. That is, AC-4-based trimming has an improved
worst-case time of $\mathcal O(n+m)$ but requires worst-case space of $\mathcal
O(n+m)$; compared with AC-4-based trimming, AC-6-based has the same worst-case
time of $\mathcal O(n+m)$ but an improved worst-case space of $\mathcal O(n)$.
We parallelize the AC-4-based and AC-6-based algorithms to be suitable for
shared-memory multi-core machines. The algorithms are designed to minimize
synchronization overhead. For these algorithms, we also prove the correctness
and analyze time complexities with the work-depth model. In experiments, we
compare these three parallel trimming algorithms over a variety of real and
synthetic graphs. Specifically, for the maximum number of traversed edges per
worker by using 16 workers, AC-3-based traverses up to 58.3 and 36.5 times more
edges than AC-6-based trimming and AC-4-based trimming, respectively.

    

### [[2107.12740] Edge service resource allocation strategy based on intelligent prediction](http://arxiv.org/abs/2107.12740)


  Artificial intelligence is one of the important technologies for industrial
applications, but it requires a lot of computing resources and sensor data to
support it. With the development of edge computing and the Internet of Things,
artificial intelligence are playing an increasingly important role in the field
of edge services. Therefore, how to make intelligent algorithms provide better
services and the development of the Internet of Things has become an
increasingly important topic. This paper focuses on the application of edge
service distribution strategy, and proposes an edge service distribution
strategy based on intelligent prediction, which reduces the bandwidth
consumption of edge service providers and minimizes the cost of edge service
providers. In addition, this article uses the real data provided by the Wangsu
Technology Company and an improved long and short term memory prediction method
to dynamically change the bandwidth, and achieves better optimization of
resources allocation comparing with actual industrial applications.The
simulation results show that our intelligent prediction can achieve good
results, and the mechanism can achieve higher resource utilization.

    

### [[2107.12788] On the data persistency of replicated erasure codes in distributed storage systems](http://arxiv.org/abs/2107.12788)


  This paper studies the fundamental problem of data persistency for a general
family of redundancy schemes in distributed storage systems, called replicated
erasure codes. Namely, we analyze two strategies of replicated erasure codes
distribution: random and symmetric. For both strategies we derive closed
analytical and asymptotic formulas for expected data persistency despite nodes
failure.

    

### [[2107.12807] HPTMT: Operator-Based Architecture for ScalableHigh-Performance Data-Intensive Frameworks](http://arxiv.org/abs/2107.12807)


  Data-intensive applications impact many domains, and their steadily
increasing size and complexity demands high-performance, highly usable
environments. We integrate a set of ideas developed in various data science and
data engineering frameworks. They employ a set of operators on specific data
abstractions that include vectors, matrices, tensors, graphs, and tables. Our
key concepts are inspired from systems like MPI, HPF (High-Performance
Fortran), NumPy, Pandas, Spark, Modin, PyTorch, TensorFlow, RAPIDS(NVIDIA), and
OneAPI (Intel). Further, it is crucial to support different languages in
everyday use in the Big Data arena, including Python, R, C++, and Java. We note
the importance of Apache Arrow and Parquet for enabling language agnostic high
performance and interoperability. In this paper, we propose High-Performance
Tensors, Matrices and Tables (HPTMT), an operator-based architecture for
data-intensive applications, and identify the fundamental principles needed for
performance and usability success. We illustrate these principles by a
discussion of examples using our software environments, Cylon and Twister2 that
embody HPTMT.

    

### [[2107.12867] From Library Portability to Para-rehosting: Natively Executing Microcontroller Software on Commodity Hardware](http://arxiv.org/abs/2107.12867)


  Finding bugs in microcontroller (MCU) firmware is challenging, even for
device manufacturers who own the source code. The MCU runs different
instruction sets than x86 and exposes a very different development environment.
This invalidates many existing sophisticated software testing tools on x86. To
maintain a unified developing and testing environment, a straightforward way is
to re-compile the source code into the native executable for a commodity
machine (called rehosting). However, ad-hoc re-hosting is a daunting and
tedious task and subject to many issues (library-dependence, kernel-dependence
and hardware-dependence). In this work, we systematically explore the
portability problem of MCU software and propose pararehosting to ease the
porting process. Specifically, we abstract and implement a portable MCU (PMCU)
using the POSIX interface. It models common functions of the MCU cores. For
peripheral specific logic, we propose HAL-based peripheral function
replacement, in which high-level hardware functions are replaced with an
equivalent backend driver on the host. These backend drivers are invoked by
well-designed para-APIs and can be reused across many MCU OSs. We categorize
common HAL functions into four types and implement templates for quick backend
development. Using the proposed approach, we have successfully rehosted nine
MCU OSs including the widely deployed Amazon FreeRTOS, ARM Mbed OS, Zephyr and
LiteOS. To demonstrate the superiority of our approach in terms of security
testing, we used off-the-shelf dynamic analysis tools (AFL and ASAN) against
the rehosted programs and discovered 28 previously-unknown bugs, among which 5
were confirmed by CVE and the other 19 were confirmed by vendors at the time of
writing.

    

### [[2107.12981] Cross-Referencing Method for Scalable Public Blockchain](http://arxiv.org/abs/2107.12981)


  We previously proposed a cross-referencing method for enabling multiple
peer-to-peer network domains to manage their own public blockchains and
periodically exchanging the state of the latest fixed block in the blockchain
with hysteresis signatures among all the domains via an upper network layer. In
this study, we evaluated the effectiveness of our method from three theoretical
viewpoints: decentralization, scalability, and tamper resistance. We show that
the performance of the entire system can be improved because transactions and
blocks are distributed only inside the domain. We argue that the transaction
processing capacity will increase to 56,000 transactions per second, which is
as much as that of a VISA credit card system. The capacity is also evaluated by
multiplying the number of domains by the average reduction in
transaction-processing time due to the increase in block size and reduction in
the block-generation-time interval by domain partition. For tamper resistance,
each domain has evidence of the hysteresis signatures of the other domains in
the blockchain. We introduce two types of tamper-resistance-improvement ratios
as evaluation measures of tamper resistance for a blockchain and theoretically
explain how tamper resistance is improved using our cross-referencing method.
With our method, tamper resistance improves as the number of domains increases.
The proposed system of 1,000 domains are 3-10 times more tamper-resistant than
that of 100 domains, and the capacity is 10 times higher. We conclude that our
method enables a more scalable and tamper-resistant public blockchain balanced
with decentralization.

    

### [[2002.02516] Breaking the $O(\sqrt n)$-Bit Barrier: Byzantine Agreement with Polylog Bits Per Party](http://arxiv.org/abs/2002.02516)


  Byzantine agreement (BA), the task of $n$ parties to agree on one of their
input bits in the face of malicious agents, is a powerful primitive that lies
at the core of a vast range of distributed protocols. Interestingly, in
protocols with the best overall communication, the demands of the parties are
highly unbalanced: the amortized cost is $\tilde O(1)$ bits per party, but some
parties must send $\Omega(n)$ bits. In best known balanced protocols, the
overall communication is sub-optimal, with each party communicating $\tilde
O(\sqrt{n})$. In this work, we ask whether asymmetry is inherent for optimizing
total communication. Our contributions in this line are as follows:
1) We define a cryptographic primitive, succinctly reconstructed distributed
signatures (SRDS), that suffices for constructing $\tilde O(1)$ balanced BA. We
provide two constructions of SRDS from different cryptographic and Public-Key
Infrastructure (PKI) assumptions.
2) The SRDS-based BA follows a paradigm of boosting from "almost-everywhere"
agreement to full agreement, and does so in a single round. We prove that PKI
setup and cryptographic assumptions are necessary for such protocols in which
every party sends $o(n)$ messages.
3) We further explore connections between a natural approach toward attaining
SRDS and average-case succinct non-interactive argument systems (SNARGs) for a
particular type of NP-Complete problems (generalizing Subset-Sum and
Subset-Product).
Our results provide new approaches forward, as well as limitations and
barriers, towards minimizing per-party communication of BA. In particular, we
construct the first two BA protocols with $\tilde O(1)$ balanced communication,
offering a tradeoff between setup and cryptographic assumptions, and answering
an open question presented by King and Saia (DISC'09).

    

### [[2009.03987] Time-Optimal Construction of Overlay Networks](http://arxiv.org/abs/2009.03987)


  We show how to construct an overlay network of constant degree and diameter
$O(\log n)$ in time $O(\log n)$ starting from an arbitrary weakly connected
graph. We assume a synchronous communication network in which nodes can send
messages to nodes they know the identifier of, and new connections can be
established by sending node identifiers. If the initial network's graph is
weakly connected and has constant degree, then our algorithm constructs the
desired topology with each node sending and receiving only $O(\log n)$ messages
in each round in time $O(\log n)$, w.h.p., which beats the currently best
$O(\log^{3/2} n)$ time algorithm of [Götte et al., SIROCCO'19]. Since the
problem cannot be solved faster than by using pointer jumping for $O(\log n)$
rounds (which would even require each node to communicate $\Omega(n)$ bits),
our algorithm is asymptotically optimal. We achieve this speedup by using short
random walks to repeatedly establish random connections between the nodes that
quickly reduce the conductance of the graph using an observation of [Kwok and
Lau, APPROX'14]. Additionally, we show how our algorithm can be used to
efficiently solve graph problems in \emph{hybrid networks} [Augustine et al.,
SODA'20]. Motivated by the idea that nodes possess two different modes of
communication, we assume that communication of the \emph{initial} edges is
unrestricted, whereas only polylogarithmically many messages can be
communicated over edges that have been established throughout an algorithm's
execution. For an (undirected) graph $G$ with arbitrary degree, we show how to
compute connected components, a spanning tree, and biconnected components in
time $O(\log n)$, w.h.p. Furthermore, we show how to compute an MIS in time
$O(\log d + \log \log n)$, w.h.p., where $d$ is the initial degree of $G$.

    

### [[2102.12825] Revisiting Optimal Resilience of Fast Byzantine Consensus (Extended Version)](http://arxiv.org/abs/2102.12825)


  It is a common belief that Byzantine fault-tolerant solutions for consensus
are significantly slower than their crash fault-tolerant counterparts. Indeed,
in PBFT, the most widely known Byzantine fault-tolerant consensus protocol, it
takes three message delays to decide a value, in contrast with just two in
Paxos. This motivates the search for fast Byzantine consensus algorithms that
can produce decisions after just two message delays \emph{in the common case},
e.g., under the assumption that the current leader is correct and not suspected
by correct processes. The (optimal) two-step latency comes with the cost of
lower resilience: fast Byzantine consensus requires more processes to tolerate
the same number of faults. In particular, $5f+1$ processes were claimed to be
necessary to tolerate $f$ Byzantine failures.
In this paper, we present a fast Byzantine consensus algorithm that relies on
just $5f-1$ processes. Moreover, we show that $5f-1$ is the tight lower bound,
correcting a mistake in the earlier work. While the difference of just $2$
processes may appear insignificant for large values of $f$, it can be crucial
for systems of a smaller scale. In particular, for $f=1$, our algorithm
requires only $4$ processes, which is optimal for any (not necessarily fast)
partially synchronous Byzantine consensus algorithm.

    

### [[2103.02928] Straggler Mitigation through Unequal Error Protection for Distributed Approximate Matrix Multiplication](http://arxiv.org/abs/2103.02928)


  Large-scale machine learning and data mining methods routinely distribute
computations across multiple agents to parallelize processing. The time
required for the computations at the agents is affected by the availability of
local resources and/or poor channel conditions giving rise to the "straggler
problem". As a remedy to this problem, we employ Unequal Error Protection (UEP)
codes to obtain an approximation of the matrix product in the distributed
computation setting to provide higher protection for the blocks with higher
effect on the final result. We characterize the performance of the proposed
approach from a theoretical perspective by bounding the expected reconstruction
error for matrices with uncorrelated entries. We also apply the proposed coding
strategy to the computation of the back-propagation step in the training of a
Deep Neural Network (DNN) for an image classification task in the evaluation of
the gradients. Our numerical experiments show that it is indeed possible to
obtain significant improvements in the overall time required to achieve the DNN
training convergence by producing approximation of matrix products using UEP
codes in the presence of stragglers.

    

### [[2105.08706] Durable Queues: The Second Amendment](http://arxiv.org/abs/2105.08706)


  We consider durable data structures for non-volatile main memory, such as the
new Intel Optane memory architecture. Substantial recent work has concentrated
on making concurrent data structures durable with low overhead, by adding a
minimal number of blocking persist operations (i.e., flushes and fences). In
this work we show that focusing on minimizing the number of persist
instructions is important, but not enough. We show that access to flushed
content is of high cost due to cache invalidation in current architectures.
Given this finding, we present a design of the queue data structure that
properly takes care of minimizing blocking persist operations as well as
minimizing access to flushed content. The proposed design outperforms
state-of-the-art durable queues.
We start by providing a durable version of the Michael Scott queue (MSQ). We
amend MSQ by adding a minimal number of persist instructions, fewer than in
available durable queues, and meeting the theoretical lower bound on the number
of blocking persist operations. We then proceed with a second amendment to this
design, that eliminates accesses to flushed data. Evaluation shows that the
second amendment yields substantial performance improvement, outperforming the
state of the art and demonstrating the importance of reduced accesses to
flushed content. The presented queues are durably linearizable and lock-free.
Finally, we discuss the theoretical optimal number of accesses to flushed
content.

    

### [[2106.00565] Robust and accurate fine-grain power models for embedded systems with no on-chip PMU](http://arxiv.org/abs/2106.00565)


  This paper presents a novel approach to event-based power modelling for
embedded platforms that do not have a Performance Monitoring Unit (PMU). The
method involves complementing the target hardware platform, where the physical
power data is measured, with another platform on which the CPU performance
data, that is needed for model generation, can be collected. The methodology is
used to generate accurate fine-grain power models for the the Gaisler GR712RC
dual-core LEON3 fault-tolerant SPARC processor with on-board power sensors and
no PMU. A Kintex UltraScale FPGA is used as the support platform to obtain the
required CPU performance data, by running a soft-core representation of the
dual-core LEON3 as on the GR712RC but with a PMU implementation. Both platforms
execute the same benchmark set and data collection is synchronised using
per-sample timestamps so that the power sensor data from the GR712RC board can
be matched to the PMU data from the FPGA. The synchronised samples are then
processed by the Robust Energy and Power Predictor Selection (REPPS) software
in order to generate power models. The models achieve less than 2% power
estimation error when validated on an industrial use-case and can successfully
follow program phases, which makes them suitable for runtime power profiling.

    

### [[2107.12422] Towards Efficient Tensor Decomposition-Based DNN Model Compression with Optimization Framework](http://arxiv.org/abs/2107.12422)


  Advanced tensor decomposition, such as Tensor train (TT) and Tensor ring
(TR), has been widely studied for deep neural network (DNN) model compression,
especially for recurrent neural networks (RNNs). However, compressing
convolutional neural networks (CNNs) using TT/TR always suffers significant
accuracy loss. In this paper, we propose a systematic framework for tensor
decomposition-based model compression using Alternating Direction Method of
Multipliers (ADMM). By formulating TT decomposition-based model compression to
an optimization problem with constraints on tensor ranks, we leverage ADMM
technique to systemically solve this optimization problem in an iterative way.
During this procedure, the entire DNN model is trained in the original
structure instead of TT format, but gradually enjoys the desired low tensor
rank characteristics. We then decompose this uncompressed model to TT format
and fine-tune it to finally obtain a high-accuracy TT-format DNN model. Our
framework is very general, and it works for both CNNs and RNNs, and can be
easily modified to fit other tensor decomposition approaches. We evaluate our
proposed framework on different DNN models for image classification and video
recognition tasks. Experimental results show that our ADMM-based TT-format
models demonstrate very high compression performance with high accuracy.
Notably, on CIFAR-100, with 2.3X and 2.4X compression ratios, our models have
1.96% and 2.21% higher top-1 accuracy than the original ResNet-20 and
ResNet-32, respectively. For compressing ResNet-18 on ImageNet, our model
achieves 2.47X FLOPs reduction without accuracy loss.

    

### [[2107.12473] Adversarial Attacks with Time-Scale Representations](http://arxiv.org/abs/2107.12473)


  We propose a novel framework for real-time black-box universal attacks which
disrupts activations of early convolutional layers in deep learning models. Our
hypothesis is that perturbations produced in the wavelet space disrupt early
convolutional layers more effectively than perturbations performed in the time
domain. The main challenge in adversarial attacks is to preserve low frequency
image content while minimally changing the most meaningful high frequency
content. To address this, we formulate an optimization problem using time-scale
(wavelet) representations as a dual space in three steps. First, we project
original images into orthonormal sub-spaces for low and high scales via wavelet
coefficients. Second, we perturb wavelet coefficients for high scale projection
using a generator network. Third, we generate new adversarial images by
projecting back the original coefficients from the low scale and the perturbed
coefficients from the high scale sub-space. We provide a theoretical framework
that guarantees a dual mapping from time and time-scale domain representations.
We compare our results with state-of-the-art black-box attacks from
generative-based and gradient-based models. We also verify efficacy against
multiple defense methods such as JPEG compression, Guided Denoiser and
Comdefend. Our results show that wavelet-based perturbations consistently
outperform time-based attacks thus providing new insights into vulnerabilities
of deep learning models and could potentially lead to robust architectures or
new defense and attack mechanisms by leveraging time-scale representations.

    

### [[2107.12477] Decision Making Using Rough Set based Spanning Sets for a Decision System](http://arxiv.org/abs/2107.12477)


  Rough Set based concepts of Span and Spanning Sets were recently proposed to
deal with uncertainties in data. Here, this paper, presents novel concepts for
generic decision-making process using Rough Set based span for a decision
table. Majority of problems in Artificial Intelligence deal with decision
making. This paper provides real life applications of proposed Rough Set based
span for decision tables. Here, novel concept of span for a decision table is
proposed, illustrated with real life example of flood relief and rescue team
assignment. Its uses, applications and properties are explored. The key
contribution of paper is primarily to study decision making using Rough Set
based Span for a decision tables, as against an information system in prior
works. Here, the main contribution is that decision classes are automatically
learned by the technique of Rough Set based span, for a particular problem,
hence automating the decision-making process. These decision-making tools based
on span can guide an expert in taking decisions in tough and time-bound
situations.

    

### [[2107.12512] H3D-Net: Few-Shot High-Fidelity 3D Head Reconstruction](http://arxiv.org/abs/2107.12512)


  Recent learning approaches that implicitly represent surface geometry using
coordinate-based neural representations have shown impressive results in the
problem of multi-view 3D reconstruction. The effectiveness of these techniques
is, however, subject to the availability of a large number (several tens) of
input views of the scene, and computationally demanding optimizations. In this
paper, we tackle these limitations for the specific problem of few-shot full 3D
head reconstruction, by endowing coordinate-based representations with a
probabilistic shape prior that enables faster convergence and better
generalization when using few input images (down to three). First, we learn a
shape model of 3D heads from thousands of incomplete raw scans using implicit
representations. At test time, we jointly overfit two coordinate-based neural
networks to the scene, one modeling the geometry and another estimating the
surface radiance, using implicit differentiable rendering. We devise a
two-stage optimization strategy in which the learned prior is used to
initialize and constrain the geometry during an initial optimization phase.
Then, the prior is unfrozen and fine-tuned to the scene. By doing this, we
achieve high-fidelity head reconstructions, including hair and shoulders, and
with a high level of detail that consistently outperforms both state-of-the-art
3D Morphable Models methods in the few-shot scenario, and non-parametric
methods when large sets of views are available.

    

### [[2107.12536] A Data-Driven Biophysical Computational Model of Parkinson's Disease based on Marmoset Monkeys](http://arxiv.org/abs/2107.12536)


  In this work we propose a new biophysical computational model of brain
regions relevant to Parkinson's Disease based on local field potential data
collected from the brain of marmoset monkeys. Parkinson's disease is a
neurodegenerative disorder, linked to the death of dopaminergic neurons at the
substantia nigra pars compacta, which affects the normal dynamics of the basal
ganglia-thalamus-cortex neuronal circuit of the brain. Although there are
multiple mechanisms underlying the disease, a complete description of those
mechanisms and molecular pathogenesis are still missing, and there is still no
cure. To address this gap, computational models that resemble neurobiological
aspects found in animal models have been proposed. In our model, we performed a
data-driven approach in which a set of biologically constrained parameters is
optimised using differential evolution. Evolved models successfully resembled
single-neuron mean firing rates and spectral signatures of local field
potentials from healthy and parkinsonian marmoset brain data. As far as we are
concerned, this is the first computational model of Parkinson's Disease based
on simultaneous electrophysiological recordings from seven brain regions of
Marmoset monkeys. Results show that the proposed model could facilitate the
investigation of the mechanisms of PD and support the development of techniques
that can indicate new therapies. It could also be applied to other
computational neuroscience problems in which biological data could be used to
fit multi-scale models of brain circuits.

    

### [[2107.12540] A Neurorobotics Approach to Behaviour Selection based on Human Activity Recognition](http://arxiv.org/abs/2107.12540)


  Behaviour selection has been an active research topic for robotics, in
particular in the field of human-robot interaction. For a robot to interact
effectively and autonomously with humans, the coupling between techniques for
human activity recognition, based on sensing information, and robot behaviour
selection, based on decision-making mechanisms, is of paramount importance.
However, most approaches to date consist of deterministic associations between
the recognised activities and the robot behaviours, neglecting the uncertainty
inherent to sequential predictions in real-time applications. In this paper, we
address this gap by presenting a neurorobotics approach based on computational
models that resemble neurophysiological aspects of living beings. This
neurorobotics approach was compared to a non-bioinspired, heuristics-based
approach. To evaluate both approaches, a robot simulation is developed, in
which a mobile robot has to accomplish tasks according to the activity being
performed by the inhabitant of an intelligent home. The outcomes of each
approach were evaluated according to the number of correct outcomes provided by
the robot. Results revealed that the neurorobotics approach is advantageous,
especially considering the computational models based on more complex animals.

    

### [[2107.12544] Human-Level Reinforcement Learning through Theory-Based Modeling, Exploration, and Planning](http://arxiv.org/abs/2107.12544)


  Reinforcement learning (RL) studies how an agent comes to achieve reward in
an environment through interactions over time. Recent advances in machine RL
have surpassed human expertise at the world's oldest board games and many
classic video games, but they require vast quantities of experience to learn
successfully -- none of today's algorithms account for the human ability to
learn so many different tasks, so quickly. Here we propose a new approach to
this challenge based on a particularly strong form of model-based RL which we
call Theory-Based Reinforcement Learning, because it uses human-like intuitive
theories -- rich, abstract, causal models of physical objects, intentional
agents, and their interactions -- to explore and model an environment, and plan
effectively to achieve task goals. We instantiate the approach in a video game
playing agent called EMPA (the Exploring, Modeling, and Planning Agent), which
performs Bayesian inference to learn probabilistic generative models expressed
as programs for a game-engine simulator, and runs internal simulations over
these models to support efficient object-based, relational exploration and
heuristic planning. EMPA closely matches human learning efficiency on a suite
of 90 challenging Atari-style video games, learning new games in just minutes
of game play and generalizing robustly to new game situations and new levels.
The model also captures fine-grained structure in people's exploration
trajectories and learning dynamics. Its design and behavior suggest a way
forward for building more general human-like AI systems.

    

### [[2107.12576] CCGL: Contrastive Cascade Graph Learning](http://arxiv.org/abs/2107.12576)


  Supervised learning, while prevalent for information cascade modeling, often
requires abundant labeled data in training, and the trained model is not easy
to generalize across tasks and datasets. Semi-supervised learning facilitates
unlabeled data for cascade understanding in pre-training. It often learns
fine-grained feature-level representations, which can easily result in
overfitting for downstream tasks. Recently, contrastive self-supervised
learning is designed to alleviate these two fundamental issues in linguistic
and visual tasks. However, its direct applicability for cascade modeling,
especially graph cascade related tasks, remains underexplored. In this work, we
present Contrastive Cascade Graph Learning (CCGL), a novel framework for
cascade graph representation learning in a contrastive, self-supervised, and
task-agnostic way. In particular, CCGL first designs an effective data
augmentation strategy to capture variation and uncertainty. Second, it learns a
generic model for graph cascade tasks via self-supervised contrastive
pre-training using both unlabeled and labeled data. Third, CCGL learns a
task-specific cascade model via fine-tuning using labeled data. Finally, to
make the model transferable across datasets and cascade applications, CCGL
further enhances the model via distillation using a teacher-student
architecture. We demonstrate that CCGL significantly outperforms its supervised
and semi-supervised counterpartsfor several downstream tasks.

    

### [[2107.12578] Dual Slot Selector via Local Reliability Verification for Dialogue State Tracking](http://arxiv.org/abs/2107.12578)


  The goal of dialogue state tracking (DST) is to predict the current dialogue
state given all previous dialogue contexts. Existing approaches generally
predict the dialogue state at every turn from scratch. However, the
overwhelming majority of the slots in each turn should simply inherit the slot
values from the previous turn. Therefore, the mechanism of treating slots
equally in each turn not only is inefficient but also may lead to additional
errors because of the redundant slot value generation. To address this problem,
we devise the two-stage DSS-DST which consists of the Dual Slot Selector based
on the current turn dialogue, and the Slot Value Generator based on the
dialogue history. The Dual Slot Selector determines each slot whether to update
slot value or to inherit the slot value from the previous turn from two
aspects: (1) if there is a strong relationship between it and the current turn
dialogue utterances; (2) if a slot value with high reliability can be obtained
for it through the current turn dialogue. The slots selected to be updated are
permitted to enter the Slot Value Generator to update values by a hybrid
method, while the other slots directly inherit the values from the previous
turn. Empirical results show that our method achieves 56.93%, 60.73%, and
58.04% joint accuracy on MultiWOZ 2.0, MultiWOZ 2.1, and MultiWOZ 2.2 datasets
respectively and achieves a new state-of-the-art performance with significant
improvements.

    

### [[2107.12585] Nearest Neighborhood-Based Deep Clustering for Source Data-absent Unsupervised Domain Adaptation](http://arxiv.org/abs/2107.12585)


  In the classic setting of unsupervised domain adaptation (UDA), the labeled
source data are available in the training phase. However, in many real-world
scenarios, owing to some reasons such as privacy protection and information
security, the source data is inaccessible, and only a model trained on the
source domain is available. This paper proposes a novel deep clustering method
for this challenging task. Aiming at the dynamical clustering at feature-level,
we introduce extra constraints hidden in the geometric structure between data
to assist the process. Concretely, we propose a geometry-based constraint,
named semantic consistency on the nearest neighborhood (SCNNH), and use it to
encourage robust clustering. To reach this goal, we construct the nearest
neighborhood for every target data and take it as the fundamental clustering
unit by building our objective on the geometry. Also, we develop a more
SCNNH-compliant structure with an additional semantic credibility constraint,
named semantic hyper-nearest neighborhood (SHNNH). After that, we extend our
method to this new geometry. Extensive experiments on three challenging UDA
datasets indicate that our method achieves state-of-the-art results. The
proposed method has significant improvement on all datasets (as we adopt SHNNH,
the average accuracy increases by over 3.0\% on the large-scaled dataset). Code
is available at this https URL.

    

### [[2107.12595] Template-based Chatbot for Agriculture Related FAQs](http://arxiv.org/abs/2107.12595)


  Agriculture is the fundamental industry of the society, which is the basis of
food supply and an important source of employment and GDP increase. However,
the insufficient expert can not fulfill the demand of farmers. To address this
problem, we design a chatbot to answer frequently asked questions in the
Agriculture field. Template-based questions will be answered by AIML while LSA
is used for other service-based questions. This chatbot will assist farmers by
dealing with industry problems conveniently and efficiently.

    

### [[2107.12598] Identify Apple Leaf Diseases Using Deep Learning Algorithm](http://arxiv.org/abs/2107.12598)


  Agriculture is an essential industry in the both society and economy of a
country. However, the pests and diseases cause a great amount of reduction in
agricultural production while there is not sufficient guidance for farmers to
avoid this disaster. To address this problem, we apply CNNs to plant disease
recognition by building a classification model. Within the dataset of 3,642
images of apple leaves, We use a pre-trained image classification model
Restnet34 based on a Convolutional neural network (CNN) with the Fastai
framework in order to save the training time. Overall, the accuracy of
classification is 93.765%.

    

### [[2107.12708] QA Dataset Explosion: A Taxonomy of NLP Resources for Question Answering and Reading Comprehension](http://arxiv.org/abs/2107.12708)


  Alongside huge volumes of research on deep learning models in NLP in the
recent years, there has been also much work on benchmark datasets needed to
track modeling progress. Question answering and reading comprehension have been
particularly prolific in this regard, with over 80 new datasets appearing in
the past two years. This study is the largest survey of the field to date. We
provide an overview of the various formats and domains of the current
resources, highlighting the current lacunae for future work. We further discuss
the current classifications of ``reasoning types" in question answering and
propose a new taxonomy. We also discuss the implications of over-focusing on
English, and survey the current monolingual resources for other languages and
multilingual resources. The study is aimed at both practitioners looking for
pointers to the wealth of existing data, and at researchers working on new
resources.

    

### [[2107.12711] Inclusion, equality and bias in designing online mass deliberative platforms](http://arxiv.org/abs/2107.12711)


  Designers of online deliberative platforms aim to counter the degrading
quality of online debates and eliminate online discrimination based on class,
race or gender. Support technologies such as machine learning and natural
language processing open avenues for widening the circle of people involved in
deliberation, moving from small groups to ``crowd'' scale. Some design features
of large-scale online discussion systems allow larger numbers of people to
discuss shared problems, enhance critical thinking, and formulate solutions.
However, scaling up deliberation is challenging. We review the
transdisciplinary literature on the design of digital mass-deliberation
platforms and examine the commonly featured design aspects (e.g., argumentation
support, automated facilitation, and gamification). We find that the literature
is heavily focused on developing technical fixes for scaling up deliberation,
with a heavy western influence on design and test users skew young and highly
educated. Contrastingly, there is a distinct lack of discussion on the nature
of the design process, the inclusion of stakeholders and issues relating to
inclusion, which may unwittingly perpetuate bias. Another tendency of
deliberation platforms is to nudge participants to desired forms of
argumentation, and simplifying definitions of good and bad arguments to fit
algorithmic purposes. Few studies bridge disciplines between deliberative
theory, design and engineering. As a result, scaling up deliberation will
likely advance in separate systemic siloes. We make design and process
recommendations to correct this course and suggest avenues for future research.

    

### [[2107.12806] Towards Industrial Private AI: A two-tier framework for data and model security](http://arxiv.org/abs/2107.12806)


  With the advances in 5G and IoT devices, the industries are vastly adopting
artificial intelligence (AI) techniques for improving classification and
prediction-based services. However, the use of AI also raises concerns
regarding data privacy and security that can be misused or leaked. Private AI
was recently coined to address the data security issue by combining AI with
encryption techniques but existing studies have shown that model inversion
attacks can be used to reverse engineer the images from model parameters. In
this regard, we propose a federated learning and encryption-based private
(FLEP) AI framework that provides two-tier security for data and model
parameters in an IIoT environment. We proposed a three-layer encryption method
for data security and provided a hypothetical method to secure the model
parameters. Experimental results show that the proposed method achieves better
encryption quality at the expense of slightly increased execution time. We also
highlighted several open issues and challenges regarding the FLEP AI
framework's realization.

    

### [[2107.12845] A Storytelling Robot managing Persuasive and Ethical Stances via ACT-R: an Exploratory Study](http://arxiv.org/abs/2107.12845)


  We present a storytelling robot, controlled via the ACT-R cognitive
architecture, able to adopt different persuasive techniques and ethical stances
while conversing about some topics concerning COVID-19. The main contribution
of the paper consists in the proposal of a needs-driven model that guides and
evaluates, during the dialogue, the use (if any) of persuasive techniques
available in the agent procedural memory. The portfolio of persuasive
techniques tested in such a model ranges from the use of storytelling, to
framing techniques and rhetorical-based arguments. To the best of our
knowledge, this represents the first attempt of building a persuasive agent
able to integrate a mix of explicitly grounded cognitive assumptions about
dialogue management, storytelling and persuasive techniques as well as ethical
attitudes. The paper presents the results of an exploratory evaluation of the
system on 63 participants

    

### [[2107.12851] Task and Situation Structures for Service Agent Planning](http://arxiv.org/abs/2107.12851)


  Everyday tasks are characterized by their varieties and variations, and
frequently are not clearly specified to service agents. This paper presents a
comprehensive approach to enable a service agent to deal with everyday tasks in
open, uncontrolled environments. We introduce a generic structure for
representing tasks, and another structure for representing situations. Based on
the two newly introduced structures, we present a methodology of situation
handling that avoids hard-coding domain rules while improving the scalability
of real-world task planning systems.

    

### [[2107.12873] PDF-Malware: An Overview on Threats, Detection and Evasion Attacks](http://arxiv.org/abs/2107.12873)


  In the recent years, Portable Document Format, commonly known as PDF, has
become a democratized standard for document exchange and dissemination. This
trend has been due to its characteristics such as its flexibility and
portability across platforms. The widespread use of PDF has installed a false
impression of inherent safety among benign users. However, the characteristics
of PDF motivated hackers to exploit various types of vulnerabilities, overcome
security safeguards, thereby making the PDF format one of the most efficient
malicious code attack vectors. Therefore, efficiently detecting malicious PDF
files is crucial for information security. Several analysis techniques has been
proposed in the literature, be it static or dynamic, to extract the main
features that allow the discrimination of malware files from benign ones. Since
classical analysis techniques may be limited in case of zero-days,
machine-learning based techniques have emerged recently as an automatic
PDF-malware detection method that is able to generalize from a set of training
samples. These techniques are themselves facing the challenge of evasion
attacks where a malicious PDF is transformed to look benign. In this work, we
give an overview on the PDF-malware detection problem. We give a perspective on
the new challenges and emerging solutions.

    

### [[2107.12877] Efficient TBox Reasoning with Value Restrictions using the $\mathcal{FL}_{o}$wer reasoner](http://arxiv.org/abs/2107.12877)


  The inexpressive Description Logic (DL) $\mathcal{FL}_0$, which has
conjunction and value restriction as its only concept constructors, had fallen
into disrepute when it turned out that reasoning in $\mathcal{FL}_0$ w.r.t.
general TBoxes is ExpTime-complete, i.e., as hard as in the considerably more
expressive logic $\mathcal{ALC}$. In this paper, we rehabilitate
$\mathcal{FL}_0$ by presenting a dedicated subsumption algorithm for
$\mathcal{FL}_0$, which is much simpler than the tableau-based algorithms
employed by highly optimized DL reasoners. Our experiments show that the
performance of our novel algorithm, as prototypically implemented in our
$\mathcal{FL}_o$wer reasoner, compares very well with that of the highly
optimized reasoners. $\mathcal{FL}_o$wer can also deal with ontologies written
in the extension $\mathcal{FL}_{\bot}$ of $\mathcal{FL}_0$ with the top and the
bottom concept by employing a polynomial-time reduction, shown in this paper,
which eliminates top and bottom. We also investigate the complexity of
reasoning in DLs related to the Horn-fragments of $\mathcal{FL}_0$ and
$\mathcal{FL}_{\bot}$.

    

### [[2107.12895] Emotion Recognition under Consideration of the Emotion Component Process Model](http://arxiv.org/abs/2107.12895)


  Emotion classification in text is typically performed with neural network
models which learn to associate linguistic units with emotions. While this
often leads to good predictive performance, it does only help to a limited
degree to understand how emotions are communicated in various domains. The
emotion component process model (CPM) by Scherer (2005) is an interesting
approach to explain emotion communication. It states that emotions are a
coordinated process of various subcomponents, in reaction to an event, namely
the subjective feeling, the cognitive appraisal, the expression, a
physiological bodily reaction, and a motivational action tendency. We
hypothesize that these components are associated with linguistic realizations:
an emotion can be expressed by describing a physiological bodily reaction ("he
was trembling"), or the expression ("she smiled"), etc. We annotate existing
literature and Twitter emotion corpora with emotion component classes and find
that emotions on Twitter are predominantly expressed by event descriptions or
subjective reports of the feeling, while in literature, authors prefer to
describe what characters do, and leave the interpretation to the reader. We
further include the CPM in a multitask learning model and find that this
supports the emotion categorization. The annotated corpora are available at
this https URL.

    

### [[2107.12921] Angel's Girl for Blind Painters: an Efficient Painting Navigation System Validated by Multimodal Evaluation Approach](http://arxiv.org/abs/2107.12921)


  For people who ardently love painting but unfortunately have visual
impairments, holding a paintbrush to create a work is a very difficult task.
People in this special group are eager to pick up the paintbrush, like Leonardo
da Vinci, to create and make full use of their own talents. Therefore, to
maximally bridge this gap, we propose a painting navigation system to assist
blind people in painting and artistic creation. The proposed system is composed
of cognitive system and guidance system. The system adopts drawing board
positioning based on QR code, brush navigation based on target detection and
bush real-time positioning. Meanwhile, this paper uses human-computer
interaction on the basis of voice and a simple but efficient position
information coding rule. In addition, we design a criterion to efficiently
judge whether the brush reaches the target or not. According to the
experimental results, the thermal curves extracted from the faces of testers
show that it is relatively well accepted by blindfolded and even blind testers.
With the prompt frequency of 1s, the painting navigation system performs best
with the completion degree of 89% with SD of 8.37% and overflow degree of 347%
with SD of 162.14%. Meanwhile, the excellent and good types of brush tip
trajectory account for 74%, and the relative movement distance is 4.21 with SD
of 2.51. This work demonstrates that it is practicable for the blind people to
feel the world through the brush in their hands. In the future, we plan to
deploy Angle's Eyes on the phone to make it more portable. The demo video of
the proposed painting navigation system is available at:
this https URL.

    

### [[2107.12979] Predictive Coding: a Theoretical and Experimental Review](http://arxiv.org/abs/2107.12979)


  Predictive coding offers a potentially unifying account of cortical function
-- postulating that the core function of the brain is to minimize prediction
errors with respect to a generative model of the world. The theory is closely
related to the Bayesian brain framework and, over the last two decades, has
gained substantial influence in the fields of theoretical and cognitive
neuroscience. A large body of research has arisen based on both empirically
testing improved and extended theoretical and mathematical models of predictive
coding, as well as in evaluating their potential biological plausibility for
implementation in the brain and the concrete neurophysiological and
psychological predictions made by the theory. Despite this enduring popularity,
however, no comprehensive review of predictive coding theory, and especially of
recent developments in this field, exists. Here, we provide a comprehensive
review both of the core mathematical structure and logic of predictive coding,
thus complementing recent tutorials in the literature. We also review a wide
range of classic and recent work within the framework, ranging from the
neurobiologically realistic microcircuits that could implement predictive
coding, to the close relationship between predictive coding and the widely-used
backpropagation of error algorithm, as well as surveying the close
relationships between predictive coding and modern machine learning techniques.

    

### [[2009.09405] Scale-Localized Abstract Reasoning](http://arxiv.org/abs/2009.09405)


  We consider the abstract relational reasoning task, which is commonly used as
an intelligence test. Since some patterns have spatial rationales, while others
are only semantic, we propose a multi-scale architecture that processes each
query in multiple resolutions. We show that indeed different rules are solved
by different resolutions and a combined multi-scale approach outperforms the
existing state of the art in this task on all benchmarks by 5-54%. The success
of our method is shown to arise from multiple novelties. First, it searches for
relational patterns in multiple resolutions, which allows it to readily detect
visual relations, such as location, in higher resolution, while allowing the
lower resolution module to focus on semantic relations, such as shape type.
Second, we optimize the reasoning network of each resolution proportionally to
its performance, hereby we motivate each resolution to specialize on the rules
for which it performs better than the others and ignore cases that are already
solved by the other resolutions. Third, we propose a new way to pool
information along the rows and the columns of the illustration-grid of the
query. Our work also analyses the existing benchmarks, demonstrating that the
RAVEN dataset selects the negative examples in a way that is easily exploited.
We, therefore, propose a modified version of the RAVEN dataset, named
RAVEN-FAIR. Our code and pretrained models are available at
this https URL.

    

### [[2102.08811] Deep Learning for Market by Order Data](http://arxiv.org/abs/2102.08811)


  Market by order (MBO) data - a detailed feed of individual trade instructions
for a given stock on an exchange - is arguably one of the most granular sources
of microstructure information. While limit order books (LOBs) are implicitly
derived from it, MBO data is largely neglected by current academic literature
which focuses primarily on LOB modelling. In this paper, we demonstrate the
utility of MBO data for forecasting high-frequency price movements, providing
an orthogonal source of information to LOB snapshots and expanding the universe
of alpha discovery. We provide the first predictive analysis on MBO data by
carefully introducing the data structure and presenting a specific
normalisation scheme to consider level information in order books and to allow
model training with multiple instruments. Through forecasting experiments using
deep neural networks, we show that while MBO-driven and LOB-driven models
individually provide similar performance, ensembles of the two can lead to
improvements in forecasting accuracy - indicating that MBO data is additive to
LOB-based features.

    

### [[2106.03548] Auction-based and Distributed Optimization Approaches for Scheduling Observations in Satellite Constellations with Exclusive Orbit Portions](http://arxiv.org/abs/2106.03548)


  We investigate the use of multi-agent allocation techniques on problems
related to Earth observation scenarios with multiple users and satellites. We
focus on the problem of coordinating users having reserved exclusive orbit
portions and one central planner having several requests that may use some
intervals of these exclusives. We define this problem as Earth Observation
Satellite Constellation Scheduling Problem (EOSCSP) and map it to a Mixed
Integer Linear Program. As to solve EOSCSP, we propose market-based techniques
and a distributed problem solving technique based on Distributed Constraint
Optimization (DCOP), where agents cooperate to allocate requests without
sharing their own schedules. These contributions are experimentally evaluated
on randomly generated EOSCSP instances based on real large-scale or highly
conflicting observation order books.

    

### [[2107.12550] Accelerated Multiple Precision Direct Method and Mixed Precision Iterative Refinement on Python Programming Environment](http://arxiv.org/abs/2107.12550)


  Current Python programming environment does not have any reliable and
efficient multiple precision floating-point (MPF) arithmetic except ``mpmath"
and ``gmpy2" packages based on GNU MP(GMP) and MPFR libraries. Although it is
well known that multi-component-type MPF library can be utilized for middle
length precision arithmetic under 200 bits, they are not widely used on Python
environment. In this paper, we describe our accelerated MPF direct method with
AVX2 techniques and its application to mixed precision iterative refinement
combined with mpmath, and demonstrate their efficiency on x86\_64 computational
environments.

    

### [[2107.12567] Guided Optimization for Image Processing Pipelines](http://arxiv.org/abs/2107.12567)


  Writing high-performance image processing code is challenging and
labor-intensive. The Halide programming language simplifies this task by
decoupling high-level algorithms from "schedules" which optimize their
implementation. However, even with this abstraction, it is still challenging
for Halide programmers to understand complicated scheduling strategies and
productively write valid, optimized schedules. To address this, we propose a
programming support method called "guided optimization." Guided optimization
provides programmers a set of valid optimization options and interactive
feedback about their current choices, which enables them to comprehend and
efficiently optimize image processing code without the time-consuming
trial-and-error process of traditional text editors. We implemented a
proof-of-concept system, Roly-poly, which integrates guided optimization,
program visualization, and schedule cost estimation to support the
comprehension and development of efficient Halide image processing code. We
conducted a user study with novice Halide programmers and confirmed that
Roly-poly and its guided optimization was informative, increased productivity,
and resulted in higher-performing schedules in less time.

    

### [[2107.12568] Version Space Algebras are Acyclic Tree Automata](http://arxiv.org/abs/2107.12568)


  Version space algebras are ways of representing spaces of programs which can
be combined using union, intersection, and cross-product/``join" operators. In
their reified form as ASTs with explicit union and join nodes, they have the
ability to compactly represent exponentially-large spaces of programs, owing to
which they have become become the most popular approach to enumerative program
synthesis since the introduction of FlashFill in 2010. We present a linear-time
semantics-preserving constructive embedding from version space algebras into
nondeterministic finite tree automata, showing that the former are but a
special case of the latter. Combined with recent results finding a
correspondence between e-graphs and minimal deterministic tree automata, this
shows that tree automata are strict generalizations of all recent major
approaches to efficiently representing large spaces of programs by sharing.

    

### [[2107.12850] Guidelines on Minimum Standards for Developer Verification of Software](http://arxiv.org/abs/2107.12850)


  Executive Order (EO) 14028, "Improving the Nation's Cybersecurity," 12 May
2021, directs the National Institute of Standards and Technology (NIST) to
recommend minimum standards for software testing within 60 days. This document
describes eleven recommendations for software verification techniques as well
as providing supplemental information about the techniques and references for
further information. It recommends the following techniques:
Threat modeling to look for design-level security issues
Automated testing for consistency and to minimize human effort
Static code scanning to look for top bugs
Heuristic tools to look for possible hardcoded secrets
Use of built-in checks and protections
"Black box" test cases
Code-based structural test cases
Historical test cases
Fuzzing
Web app scanners, if applicable
Address included code (libraries, packages, services)
The document does not address the totality of software verification, but
instead recommends techniques that are broadly applicable and form the minimum
standards.
The document was developed by NIST in consultation with the National Security
Agency. Additionally, we received input from numerous outside organizations
through papers submitted to a NIST workshop on the Executive Order held in
early June, 2021 and discussion at the workshop as well as follow up with
several of the submitters.

    

### [[2107.12909] So You Want to Analyze Scheme Programs With Datalog?](http://arxiv.org/abs/2107.12909)


  Static analysis approximates the results of a program by examining only its
syntax. For example, control-flow analysis (CFA) determines which syntactic
lambdas (for functional languages) or (for object-oriented) methods may be
invoked at each call site within a program. Rich theoretical results exist
studying control flow analysis for Scheme-like languages, but implementations
are often complex and specialized. By contrast, object-oriented languages (Java
in particular) enjoy high-precision control-flow analyses that scale to
thousands (or more) of lines of code. State-of-the-art implementations (such as
DOOP on Soufflé) structure the analysis using Horn-SAT (Datalog) to enable
compilation of the analysis to efficient implementations such as
high-performance relational algebra kernels. In this paper, we present an
implementation of control-flow analysis for a significant subset of Scheme
(including set!, call/cc, and primitive operations) using the Soufflé Datalog
engine. We present an evaluation on a worst-case term demonstrating the
polynomial complexity of our m-CFA and remark upon scalability results using
Soufflé.

    

### [<title>How is XGBRFRegressor intended to work with early stopping? - RFC - XGBoost</title>](https://discuss.xgboost.ai/t/how-is-xgbrfregressor-intended-to-work-with-early-stopping/2391/1)