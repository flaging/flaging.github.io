
## 2021-11-24

### [[2111.11441] pmSensing: A Participatory Sensing Network for Predictive Monitoring of Particulate Matter](http://arxiv.org/abs/2111.11441)


  This work presents a proposal for a wireless sensor network for participatory
sensing, with IoT sensing devices developed especially for monitoring and
predicting air quality, as alternatives of high cost meteorological stations.
The system, called pmSensing, aims to measure particulate material. A
validation is done by comparing the data collected by the prototype with data
from stations. The comparison shows that the results are close, which can
enable low-cost solutions to the problem. The system still presents a
predictive analysis using recurrent neural networks, in this case the LSTM-RNN,
where the predictions presented high accuracy in relation to the real data.

    

### [[2111.11608] Optimal Content Caching and Recommendation with Age of Information](http://arxiv.org/abs/2111.11608)


  Content caching at the network edge has been considered an effective way of
mitigating backhaul load and improving user experience. Caching efficiency can
be enhanced by content recommendation and by keeping the information fresh. To
the best of our knowledge, there is no work that jointly takes into account
these aspects. By content recommendation, a requested content that is not in
the cache can be alternatively satisfied by a related cached content
recommended by the system. Information freshness can be quantified by age of
information (AoI). We address, optimal scheduling of cache updates for a
time-slotted system accounting for content recommendation and AoI. For each
content, there are requests that need to be satisfied, and there is a cost
function capturing the freshness of information. We present the following
contributions. First, we prove that the problem is NP-hard. Second, we derive
an integer linear formulation, by which the optimal solution can be obtained
for small-scale scenarios. Third, we develop an algorithm based on Lagrangian
decomposition. Fourth, we develop efficient algorithms for solving the
resulting subproblems. Our algorithm computes a bound that can be used to
evaluate the performance of any suboptimal solution. Finally, we conduct
simulations to show the effectiveness of our algorithm in comparison to a
greedy schedule.

    

### [[2111.11650] Aerial Intelligent Reflecting Surface Enabled Terahertz Covert Communications in Beyond-5G Internet of Things](http://arxiv.org/abs/2111.11650)


  Unmanned aerial vehicles (UAVs) are envisioned to be extensively employed for
assisting wireless communications in Internet of Things (IoT) applications. On
the other hand, terahertz (THz) enabled intelligent reflecting surface (IRS) is
expected to be one of the core enabling technologies for forthcoming beyond-5G
wireless communications that promise a broad range of data-demand applications.
In this paper, we propose a UAV-mounted IRS (UIRS) communication system over
THz bands for confidential data dissemination from an access point (AP) towards
multiple ground user equipments (UEs) in IoT networks. Specifically, the AP
intends to send data to the scheduled UE, while unscheduled UEs may pose
potential adversaries. To protect information messages and the privacy of the
scheduled UE, we aim to devise an energy-efficient multi-UAV covert
communication scheme, where the UIRS is for reliable data transmissions, and an
extra UAV is utilized as a cooperative jammer generating artificial noise (AN)
to degrade unscheduled UEs detection. We then formulate a novel minimum average
energy efficiency (mAEE) optimization problem, targetting to improve the covert
throughput and reduce UAVs' propulsion energy consumption subject to the
covertness requirement, which is determined analytically. Since the
optimization problem is non-convex, we tackle it via the block successive
convex approximation (BSCA) approach to iteratively solve a sequence of
approximated convex sub-problems, designing the binary user scheduling, AP's
power allocation, maximum AN jamming power, IRS beamforming, and both UAVs'
trajectory planning. Finally, we present a low-complex overall algorithm for
system performance enhancement with complexity and convergence analysis.
Numerical results are provided to verify our analysis and demonstrate
significant outperformance of our design over other existing benchmark schemes.

    

### [[2111.11714] Challenges and Opportunities in Securing the Industrial Internet of Things](http://arxiv.org/abs/2111.11714)


  Given the tremendous success of the Internet of Things in interconnecting
consumer devices, we observe a natural trend to likewise interconnect devices
in industrial settings, referred to as Industrial Internet of Things or
Industry 4.0. While this coupling of industrial components provides many
benefits, it also introduces serious security challenges. Although sharing many
similarities with the consumer Internet of Things, securing the Industrial
Internet of Things introduces its own challenges but also opportunities, mainly
resulting from a longer lifetime of components and a larger scale of networks.
In this paper, we identify the unique security goals and challenges of the
Industrial Internet of Things, which, unlike consumer deployments, mainly
follow from safety and productivity requirements. To address these security
goals and challenges, we provide a comprehensive survey of research efforts to
secure the Industrial Internet of Things, discuss their applicability, and
analyze their security benefits.

    

### [[2111.11856] Spatio-Temporal Split Learning for Autonomous Aerial Surveillance using Urban Air Mobility (UAM) Networks](http://arxiv.org/abs/2111.11856)


  Autonomous surveillance unmanned aerial vehicles (UAVs) are deployed to
observe the streets of the city for any suspicious activities. This paper
utilizes surveillance UAVs for the purpose of detecting the presence of a fire
in the streets. An extensive database is collected from UAV surveillance
drones. With the aid of artificial intelligence (AI), fire stations can swiftly
identify the presence of a fire emerging in the neighborhood. Spatio-temporal
split learning is applied to this scenario to preserve privacy and globally
train a fire classification model. Fires are hazardous natural disasters that
can spread very quickly. Swift identification of fire is required to deploy
firefighters to the scene. In order to do this, strong communication between
the UAV and the central server where the deep learning process occurs is
required. Improving communication resilience is integral to enhancing a safe
experience on the roads. Therefore, this paper explores the adequate number of
clients and data ratios for split learning in this UAV setting, as well as the
required network infrastructure.

    

### [[2111.11885] SACRIFICE: A Secure Road Condition Monitoring Scheme over Fog-based VANETs](http://arxiv.org/abs/2111.11885)


  With the rapid growth of Vehicular Ad-Hoc Networks (VANETs), huge amounts of
road condition data are constantly being generated and sent to the cloud for
processing. However, this introduces a significant load on the network
bandwidth causing delay in the network and for a time-critical application like
VANET such delay may have severe impact on real-time traffic management. This
delay maybe reduced by offloading some computational tasks to devices as close
as possible to the vehicles. Further, security and privacy of vehicles is
another important concern in such applications. Thus, this paper proposes a
secure road condition monitoring scheme for fog-based vehicular networks. It
considers Road-Side Units (RSUs) with computational capabilities, which act as
intermediate fog nodes in between vehicles and cloud to reduce delay in
decision making and thereby increases scalability. Apart from fulfilling basic
security features, our scheme also supports advanced features like
unlinkability and untraceability. A detailed security analysis proves that the
proposed scheme can handle both internal and external adversaries. We also
justify the efficiency of our scheme through theoretical overhead analysis.
Finally, the scheme is simulated using an integrated SUMO and NS-3 platform to
show its feasibility for practical implementations while not compromising on
the network performance. The results show an average 80.96\% and 37.5\%
improvement in execution time and end-to-end delay respectively while
maintaining at par results for packet delivery ratio over a state-of-the-art
scheme.

    

### [[2111.11912] No Free Lunch: Balancing Learning and Exploitation at the Network Edge](http://arxiv.org/abs/2111.11912)


  Over the last few years, the DRL paradigm has been widely adopted for 5G and
beyond network optimization because of its extreme adaptability to many
different scenarios. However, collecting and processing learning data entail a
significant cost in terms of communication and computational resources, which
is often disregarded in the networking literature. In this work, we analyze the
cost of learning in a resource-constrained system, defining an optimization
problem in which training a DRL agent makes it possible to improve the resource
allocation strategy but also reduces the number of available resources. Our
simulation results show that the cost of learning can be critical when
evaluating DRL schemes on the network edge and that assuming a cost-free
learning model can lead to significantly overestimating performance.

    

### [[2111.12064] Semantic-Aware Collaborative Deep Reinforcement Learning Over Wireless Cellular Networks](http://arxiv.org/abs/2111.12064)


  Collaborative deep reinforcement learning (CDRL) algorithms in which multiple
agents can coordinate over a wireless network is a promising approach to enable
future intelligent and autonomous systems that rely on real-time
decision-making in complex dynamic environments. Nonetheless, in practical
scenarios, CDRL faces many challenges due to the heterogeneity of agents and
their learning tasks, different environments, time constraints of the learning,
and resource limitations of wireless networks. To address these challenges, in
this paper, a novel semantic-aware CDRL method is proposed to enable a group of
heterogeneous untrained agents with semantically-linked DRL tasks to
collaborate efficiently across a resource-constrained wireless cellular
network. To this end, a new heterogeneous federated DRL (HFDRL) algorithm is
proposed to select the best subset of semantically relevant DRL agents for
collaboration. The proposed approach then jointly optimizes the training loss
and wireless bandwidth allocation for the cooperating selected agents in order
to train each agent within the time limit of its real-time task. Simulation
results show the superior performance of the proposed algorithm compared to
state-of-the-art baselines.

    

### [[2104.00972] Resource-aware Time Series Imaging Classification for Wireless Link Layer Anomalies](http://arxiv.org/abs/2104.00972)


  The number of end devices that use the last mile wireless connectivity is
dramatically increasing with the rise of smart infrastructures and require
reliable functioning to support smooth and efficient business processes. To
efficiently manage such massive wireless networks, more advanced and accurate
network monitoring and malfunction detection solutions are required. In this
paper, we perform a first time analysis of image-based representation
techniques for wireless anomaly detection using recurrence plots and Gramian
angular fields and propose a new deep learning architecture enabling accurate
anomaly detection. We elaborate on the design considerations for developing a
resource aware architecture and propose a new model using time-series to image
transformation using recurrence plots. We show that the proposed model a)
outperforms the one based on Grammian angular fields by up to 14 percentage
points, b) outperforms classical ML models using dynamic time warping by up to
24 percentage points, c) outperforms or performs on par with mainstream
architectures such as AlexNet and VGG11 while having <10 times their weights
and up to $\approx$8\% of their computational complexity and d) outperforms the
state of the art in the respective application area by up to 55 percentage
points. Finally, we also explain on randomly chosen examples how the classifier
takes decisions.

    

### [[2111.11435] Precise Learning of Source Code Contextual Semantics via Hierarchical Dependence Structure and Graph Attention Networks](http://arxiv.org/abs/2111.11435)


  Deep learning is being used extensively in a variety of software engineering
tasks, e.g., program classification and defect prediction. Although the
technique eliminates the required process of feature engineering, the
construction of source code model significantly affects the performance on
those tasks. Most recent works was mainly focused on complementing AST-based
source code models by introducing contextual dependencies extracted from CFG.
However, all of them pay little attention to the representation of basic
blocks, which are the basis of contextual dependencies.
In this paper, we integrated AST and CFG and proposed a novel source code
model embedded with hierarchical dependencies. Based on that, we also designed
a neural network that depends on the graph attention mechanism.Specifically, we
introduced the syntactic structural of the basic block, i.e., its corresponding
AST, in source code model to provide sufficient information and fill the gap.
We have evaluated this model on three practical software engineering tasks and
compared it with other state-of-the-art methods. The results show that our
model can significantly improve the performance. For example, compared to the
best performing baseline, our model reduces the scale of parameters by 50\% and
achieves 4\% improvement on accuracy on program classification task.

    

### [[2111.11438] Fink: early supernovae Ia classification using active learning](http://arxiv.org/abs/2111.11438)


  We describe how the Fink broker early supernova Ia classifier optimizes its
ML classifications by employing an active learning (AL) strategy. We
demonstrate the feasibility of implementation of such strategies in the current
Zwicky Transient Facility (ZTF) public alert data stream. We compare the
performance of two AL strategies: uncertainty sampling and random sampling. Our
pipeline consists of 3 stages: feature extraction, classification and learning
strategy. Starting from an initial sample of 10 alerts (5 SN Ia and 5 non-Ia),
we let the algorithm identify which alert should be added to the training
sample. The system is allowed to evolve through 300 iterations. Our data set
consists of 23 840 alerts from the ZTF with confirmed classification via
cross-match with SIMBAD database and the Transient name server (TNS), 1 600 of
which were SNe Ia (1 021 unique objects). The data configuration, after the
learning cycle was completed, consists of 310 alerts for training and 23 530
for testing. Averaging over 100 realizations, the classifier achieved 89%
purity and 54% efficiency. From 01/November/2020 to 31/October/2021 Fink has
applied its early supernova Ia module to the ZTF stream and communicated
promising SN Ia candidates to the TNS. From the 535 spectroscopically
classified Fink candidates, 459 (86%) were proven to be SNe Ia. Our results
confirm the effectiveness of active learning strategies for guiding the
construction of optimal training samples for astronomical classifiers. It
demonstrates in real data that the performance of learning algorithms can be
highly improved without the need of extra computational resources or
overwhelmingly large training samples. This is, to our knowledge, the first
application of AL to real alerts data.

    

### [[2111.11439] Predicting Osteoarthritis Progression in Radiographs via Unsupervised Representation Learning](http://arxiv.org/abs/2111.11439)


  Osteoarthritis (OA) is the most common joint disorder affecting substantial
proportions of the global population, primarily the elderly. Despite its
individual and socioeconomic burden, the onset and progression of OA can still
not be reliably predicted. Aiming to fill this diagnostic gap, we introduce an
unsupervised learning scheme based on generative models to predict the future
development of OA based on knee joint radiographs. Using longitudinal data from
osteoarthritis studies, we explore the latent temporal trajectory to predict a
patient's future radiographs up to the eight-year follow-up visit. Our model
predicts the risk of progression towards OA and surpasses its supervised
counterpart whose input was provided by seven experienced radiologists. With
the support of the model, sensitivity, specificity, positive predictive value,
and negative predictive value increased significantly from 42.1% to 51.6%, from
72.3% to 88.6%, from 28.4% to 57.6%, and from 83.9% to 88.4%, respectively,
while without such support, radiologists performed only slightly better than
random guessing. Our predictive model improves predictions on OA onset and
progression, despite requiring no human annotation in the training phase.

    

### [[2111.11477] Prediction Model for Mortality Analysis of Pregnant Women Affected With COVID-19](http://arxiv.org/abs/2111.11477)


  COVID-19 pandemic is an ongoing global pandemic which has caused
unprecedented disruptions in the public health sector and global economy. The
virus, SARS-CoV-2 is responsible for the rapid transmission of coronavirus
disease. Due to its contagious nature, the virus can easily infect an
unprotected and exposed individual from mild to severe symptoms. The study of
the virus effects on pregnant mothers and neonatal is now a concerning issue
globally among civilians and public health workers considering how the virus
will affect the mother and the neonates health. This paper aims to develop a
predictive model to estimate the possibility of death for a COVID-diagnosed
mother based on documented symptoms: dyspnea, cough, rhinorrhea, arthralgia,
and the diagnosis of pneumonia. The machine learning models that have been used
in our study are support vector machine, decision tree, random forest, gradient
boosting, and artificial neural network. The models have provided impressive
results and can accurately predict the mortality of pregnant mothers with a
given input.The precision rate for 3 models(ANN, Gradient Boost, Random Forest)
is 100% The highest accuracy score(Gradient Boosting,ANN) is 95%,highest
recall(Support Vector Machine) is 92.75% and highest f1 score(Gradient
Boosting,ANN) is 94.66%. Due to the accuracy of the model, pregnant mother can
expect immediate medical treatment based on their possibility of death due to
the virus. The model can be utilized by health workers globally to list down
emergency patients, which can ultimately reduce the death rate of COVID-19
diagnosed pregnant mothers.

    

### [[2111.11482] Graph Neural Networks with Parallel Neighborhood Aggregations for Graph Classification](http://arxiv.org/abs/2111.11482)


  We focus on graph classification using a graph neural network (GNN) model
that precomputes the node features using a bank of neighborhood aggregation
graph operators arranged in parallel. These GNN models have a natural advantage
of reduced training and inference time due to the precomputations but are also
fundamentally different from popular GNN variants that update node features
through a sequential neighborhood aggregation procedure during training. We
provide theoretical conditions under which a generic GNN model with parallel
neighborhood aggregations (PA-GNNs, in short) are provably as powerful as the
well-known Weisfeiler-Lehman (WL) graph isomorphism test in discriminating
non-isomorphic graphs. Although PA-GNN models do not have an apparent
relationship with the WL test, we show that the graph embeddings obtained from
these two methods are injectively related. We then propose a specialized PA-GNN
model, called SPIN, which obeys the developed conditions. We demonstrate via
numerical experiments that the developed model achieves state-of-the-art
performance on many diverse real-world datasets while maintaining the
discriminative power of the WL test and the computational advantage of
preprocessing graphs before the training process.

    

### [[2111.11485] A Free Lunch from the Noise: Provable and Practical Exploration for Representation Learning](http://arxiv.org/abs/2111.11485)


  Representation learning lies at the heart of the empirical success of deep
learning for dealing with the curse of dimensionality. However, the power of
representation learning has not been fully exploited yet in reinforcement
learning (RL), due to i), the trade-off between expressiveness and
tractability; and ii), the coupling between exploration and representation
learning. In this paper, we first reveal the fact that under some noise
assumption in the stochastic control model, we can obtain the linear spectral
feature of its corresponding Markov transition operator in closed-form for
free. Based on this observation, we propose Spectral Dynamics Embedding
(SPEDE), which breaks the trade-off and completes optimistic exploration for
representation learning by exploiting the structure of the noise. We provide
rigorous theoretical analysis of SPEDE, and demonstrate the practical superior
performance over the existing state-of-the-art empirical algorithms on several
benchmarks.

    

### [[2111.11487] A Comparison of State-of-the-Art Techniques for Generating Adversarial Malware Binaries](http://arxiv.org/abs/2111.11487)


  We consider the problem of generating adversarial malware by a cyber-attacker
where the attacker's task is to strategically modify certain bytes within
existing binary malware files, so that the modified files are able to evade a
malware detector such as machine learning-based malware classifier. We have
evaluated three recent adversarial malware generation techniques using binary
malware samples drawn from a single, publicly available malware data set and
compared their performances for evading a machine-learning based malware
classifier called MalConv. Our results show that among the compared techniques,
the most effective technique is the one that strategically modifies bytes in a
binary's header. We conclude by discussing the lessons learned and future
research directions on the topic of adversarial malware generation.

    

### [[2111.11509] Blockchain-based Recommender Systems: Applications, Challenges and Future Opportunities](http://arxiv.org/abs/2111.11509)


  Recommender systems have been widely used in different application domains
including energy-preservation, e-commerce, healthcare, social media, etc. Such
applications require the analysis and mining of massive amounts of various
types of user data, including demographics, preferences, social interactions,
etc. in order to develop accurate and precise recommender systems. Such
datasets often include sensitive information, yet most recommender systems are
focusing on the models' accuracy and ignore issues related to security and the
users' privacy. Despite the efforts to overcome these problems using different
risk reduction techniques, none of them has been completely successful in
ensuring cryptographic security and protection of the users' private
information. To bridge this gap, the blockchain technology is presented as a
promising strategy to promote security and privacy preservation in recommender
systems, not only because of its security and privacy salient features, but
also due to its resilience, adaptability, fault tolerance and trust
characteristics. This paper presents a holistic review of blockchain-based
recommender systems covering challenges, open issues and solutions.
Accordingly, a well-designed taxonomy is introduced to describe the security
and privacy challenges, overview existing frameworks and discuss their
applications and benefits when using blockchain before indicating opportunities
for future research.

    

### [[2111.11510] Bootstrap Your Flow](http://arxiv.org/abs/2111.11510)


  Normalising flows are flexible, parameterized distributions that can be used
to approximate expectations from intractable distributions via importance
sampling. However, current flow-based approaches are limited on challenging
targets where they either suffer from mode seeking behaviour or high variance
in the training loss, or rely on samples from the target distribution, which
may not be available. To address these challenges, we combine flows with
annealed importance sampling (AIS), while using the $\alpha$-divergence as our
objective, in a novel training procedure, FAB (Flow AIS Bootstrap). Thereby,
the flow and AIS to improve each other in a bootstrapping manner. We
demonstrate that FAB can be used to produce accurate approximations to complex
target distributions, including Boltzmann distributions, in problems where
previous flow-based methods fail.

    

### [[2111.11514] On Data-centric Myths](http://arxiv.org/abs/2111.11514)


  The community lacks theory-informed guidelines for building good data sets.
We analyse theoretical directions relating to what aspects of the data matter
and conclude that the intuitions derived from the existing literature are
incorrect and misleading. Using empirical counter-examples, we show that 1)
data dimension should not necessarily be minimised and 2) when manipulating
data, preserving the distribution is inessential. This calls for a more
data-aware theoretical understanding. Although not explored in this work, we
propose the study of the impact of data modification on learned representations
as a promising research direction.

    

### [[2111.11520] Zero-Shot Open-Book Question Answering](http://arxiv.org/abs/2111.11520)


  Open book question answering is a subset of question answering tasks where
the system aims to find answers in a given set of documents (open-book) and
common knowledge about a topic. This article proposes a solution for answering
natural language questions from a corpus of Amazon Web Services (AWS) technical
documents with no domain-specific labeled data (zero-shot). These questions can
have yes-no-none answers, short answers, long answers, or any combination of
the above. This solution comprises a two-step architecture in which a retriever
finds the right document and an extractor finds the answers in the retrieved
document. We are introducing a new test dataset for open-book QA based on real
customer questions on AWS technical documentation. After experimenting with
several information retrieval systems and extractor models based on extractive
language models, the solution attempts to find the yes-no-none answers and text
answers in the same pass. The model is trained on the The Stanford Question
Answering Dataset - SQuAD (Rajpurkaret al., 2016) and Natural Questions
(Kwiatkowski et al., 2019) datasets. We were able to achieve 49% F1 and 39%
exact match score (EM) end-to-end with no domain-specific training.

    

### [[2111.11523] Learnable Structural Semantic Readout for Graph Classification](http://arxiv.org/abs/2111.11523)


  With the great success of deep learning in various domains, graph neural
networks (GNNs) also become a dominant approach to graph classification. By the
help of a global readout operation that simply aggregates all node (or
node-cluster) representations, existing GNN classifiers obtain a graph-level
representation of an input graph and predict its class label using the
representation. However, such global aggregation does not consider the
structural information of each node, which results in information loss on the
global structure. Particularly, it limits the discrimination power by enforcing
the same weight parameters of the classifier for all the node representations;
in practice, each of them contributes to target classes differently depending
on its structural semantic. In this work, we propose structural semantic
readout (SSRead) to summarize the node representations at the position-level,
which allows to model the position-specific weight parameters for
classification as well as to effectively capture the graph semantic relevant to
the global structure. Given an input graph, SSRead aims to identify
structurally-meaningful positions by using the semantic alignment between its
nodes and structural prototypes, which encode the prototypical features of each
position. The structural prototypes are optimized to minimize the alignment
cost for all training graphs, while the other GNN parameters are trained to
predict the class labels. Our experimental results demonstrate that SSRead
significantly improves the classification performance and interpretability of
GNN classifiers while being compatible with a variety of aggregation functions,
GNN architectures, and learning frameworks.

    

### [[2111.11525] Component Transfer Learning for Deep RL Based on Abstract Representations](http://arxiv.org/abs/2111.11525)


  In this work we investigate a specific transfer learning approach for deep
reinforcement learning in the context where the internal dynamics between two
tasks are the same but the visual representations differ. We learn a
low-dimensional encoding of the environment, meant to capture summarizing
abstractions, from which the internal dynamics and value functions are learned.
Transfer is then obtained by freezing the learned internal dynamics and value
functions, thus reusing the shared low-dimensional embedding space. When
retraining the encoder for transfer, we make several observations: (i) in some
cases, there are local minima that have small losses but a mismatching
embedding space, resulting in poor task performance and (ii) in the absence of
local minima, the output of the encoder converges in our experiments to the
same embedding space, which leads to a fast and efficient transfer as compared
to learning from scratch. The local minima are caused by the reduced degree of
freedom of the optimization process caused by the frozen models. We also find
that the transfer performance is heavily reliant on the base model; some base
models often result in a successful transfer, whereas other base models often
result in a failing transfer.

    

### [[2111.11537] Machine Learning for Mars Exploration](http://arxiv.org/abs/2111.11537)


  Risk to human astronauts and interplanetary distance causing slow and limited
communication drives scientists to pursue an autonomous approach to exploring
distant planets, such as Mars. A portion of exploration of Mars has been
conducted through the autonomous collection and analysis of Martian data by
spacecraft such as the Mars rovers and the Mars Express Orbiter. The autonomy
used on these Mars exploration spacecraft and on Earth to analyze data
collected by these vehicles mainly consist of machine learning, a field of
artificial intelligence where algorithms collect data and self-improve with the
data. Additional applications of machine learning techniques for Mars
exploration have potential to resolve communication limitations and human risks
of interplanetary exploration. In addition, analyzing Mars data with machine
learning has the potential to provide a greater understanding of Mars in
numerous domains such as its climate, atmosphere, and potential future
habitation. To explore further utilizations of machine learning techniques for
Mars exploration, this paper will first summarize the general features and
phenomena of Mars to provide a general overview of the planet, elaborate upon
uncertainties of Mars that would be beneficial to explore and understand,
summarize every current or previous usage of machine learning techniques in the
exploration of Mars, explore implementations of machine learning that will be
utilized in future Mars exploration missions, and explore machine learning
techniques used in Earthly domains to provide solutions to the previously
described uncertainties of Mars.

    

### [[2111.11542] Depth Without the Magic: Inductive Bias of Natural Gradient Descent](http://arxiv.org/abs/2111.11542)


  In gradient descent, changing how we parametrize the model can lead to
drastically different optimization trajectories, giving rise to a surprising
range of meaningful inductive biases: identifying sparse classifiers or
reconstructing low-rank matrices without explicit regularization. This implicit
regularization has been hypothesised to be a contributing factor to good
generalization in deep learning. However, natural gradient descent is
approximately invariant to reparameterization, it always follows the same
trajectory and finds the same optimum. The question naturally arises: What
happens if we eliminate the role of parameterization, which solution will be
found, what new properties occur? We characterize the behaviour of natural
gradient flow in deep linear networks for separable classification under
logistic loss and deep matrix factorization. Some of our findings extend to
nonlinear neural networks with sufficient but finite over-parametrization. We
demonstrate that there exist learning problems where natural gradient descent
fails to generalize, while gradient descent with the right architecture
performs well.

    

### [[2111.11547] Camera Measurement of Physiological Vital Signs](http://arxiv.org/abs/2111.11547)


  The need for remote tools for healthcare monitoring has never been more
apparent. Camera measurement of vital signs leverages imaging devices to
compute physiological changes by analyzing images of the human body. Building
on advances in optics, machine learning, computer vision and medicine these
techniques have progressed significantly since the invention of digital
cameras. This paper presents a comprehensive survey of camera measurement of
physiological vital signs, describing they vital signs that can be measured and
the computational techniques for doing so. I cover both clinical and
non-clinical applications and the challenges that need to be overcome for these
applications to advance from proofs-of-concept. Finally, I describe the current
resources (datasets and code) available to the research community and provide a
comprehensive webpage (this https URL) with links to these
resource and a categorized list of all the papers referenced in this article.

    

### [[2111.11550] Dynamic Regret for Strongly Adaptive Methods and Optimality of Online KRR](http://arxiv.org/abs/2111.11550)


  We consider the framework of non-stationary Online Convex Optimization where
a learner seeks to control its dynamic regret against an arbitrary sequence of
comparators. When the loss functions are strongly convex or exp-concave, we
demonstrate that Strongly Adaptive (SA) algorithms can be viewed as a
principled way of controlling dynamic regret in terms of path variation $V_T$
of the comparator sequence. Specifically, we show that SA algorithms enjoy
$\tilde O(\sqrt{TV_T} \vee \log T)$ and $\tilde O(\sqrt{dTV_T} \vee d\log T)$
dynamic regret for strongly convex and exp-concave losses respectively without
apriori knowledge of $V_T$. The versatility of the principled approach is
further demonstrated by the novel results in the setting of learning against
bounded linear predictors and online regression with Gaussian kernels.
Under a related setting, the second component of the paper addresses an open
question posed by Zhdanov and Kalnishkan (2010) that concerns online kernel
regression with squared error losses. We derive a new lower bound on a certain
penalized regret which establishes the near minimax optimality of online Kernel
Ridge Regression (KRR). Our lower bound can be viewed as an RKHS extension to
the lower bound derived in Vovk (2001) for online linear regression in finite
dimensions.

    

### [[2111.11554] KML: Using Machine Learning to Improve Storage Systems](http://arxiv.org/abs/2111.11554)


  Operating systems include many heuristic algorithms designed to improve
overall storage performance and throughput. Because such heuristics cannot work
well for all conditions and workloads, system designers resorted to exposing
numerous tunable parameters to users -- essentially burdening users with
continually optimizing their own storage systems and applications. Storage
systems are usually responsible for most latency in I/O heavy applications, so
even a small overall latency improvement can be significant. Machine learning
(ML) techniques promise to learn patterns, generalize from them, and enable
optimal solutions that adapt to changing workloads. We propose that ML
solutions become a first-class component in OSs and replace manual heuristics
to optimize storage systems dynamically. In this paper, we describe our
proposed ML architecture, called KML. We developed a prototype KML architecture
and applied it to two problems: optimal readahead and NFS read-size values. Our
experiments show that KML consumes little OS resources, adds negligible
latency, and yet can learn patterns that can improve I/O throughput by as much
as 2.3x or 15x for the two use cases respectively -- even for complex,
never-before-seen, concurrently running mixed workloads on different storage
devices.

    

### [[2111.11556] FLIX: A Simple and Communication-Efficient Alternative to Local Methods in Federated Learning](http://arxiv.org/abs/2111.11556)


  Federated Learning (FL) is an increasingly popular machine learning paradigm
in which multiple nodes try to collaboratively learn under privacy,
communication and multiple heterogeneity constraints. A persistent problem in
federated learning is that it is not clear what the optimization objective
should be: the standard average risk minimization of supervised learning is
inadequate in handling several major constraints specific to federated
learning, such as communication adaptivity and personalization control. We
identify several key desiderata in frameworks for federated learning and
introduce a new framework, FLIX, that takes into account the unique challenges
brought by federated learning. FLIX has a standard finite-sum form, which
enables practitioners to tap into the immense wealth of existing (potentially
non-local) methods for distributed optimization. Through a smart initialization
that does not require any communication, FLIX does not require the use of local
steps but is still provably capable of performing dissimilarity regularization
on par with local methods. We give several algorithms for solving the FLIX
formulation efficiently under communication constraints. Finally, we
corroborate our theoretical results with extensive experimentation.

    

### [[2111.11576] Building Goal-Oriented Dialogue Systems with Situated Visual Context](http://arxiv.org/abs/2111.11576)


  Most popular goal-oriented dialogue agents are capable of understanding the
conversational context. However, with the surge of virtual assistants with
screen, the next generation of agents are required to also understand screen
context in order to provide a proper interactive experience, and better
understand users' goals. In this paper, we propose a novel multimodal
conversational framework, where the dialogue agent's next action and their
arguments are derived jointly conditioned both on the conversational and the
visual context. Specifically, we propose a new model, that can reason over the
visual context within a conversation and populate API arguments with visual
entities given the user query. Our model can recognize visual features such as
color and shape as well as the metadata based features such as price or star
rating associated with a visual entity. In order to train our model, due to a
lack of suitable multimodal conversational datasets, we also propose a novel
multimodal dialog simulator to generate synthetic data and also collect
realistic user data from MTurk to improve model robustness. The proposed model
achieves a reasonable 85% model accuracy, without high inference latency. We
also demonstrate the proposed approach in a prototypical furniture shopping
experience for a multimodal virtual assistant.

    

### [[2111.11581] Automatic Mapping of the Best-Suited DNN Pruning Schemes for Real-Time Mobile Acceleration](http://arxiv.org/abs/2111.11581)


  Weight pruning is an effective model compression technique to tackle the
challenges of achieving real-time deep neural network (DNN) inference on mobile
devices. However, prior pruning schemes have limited application scenarios due
to accuracy degradation, difficulty in leveraging hardware acceleration, and/or
restriction on certain types of DNN layers. In this paper, we propose a
general, fine-grained structured pruning scheme and corresponding compiler
optimizations that are applicable to any type of DNN layer while achieving high
accuracy and hardware inference performance. With the flexibility of applying
different pruning schemes to different layers enabled by our compiler
optimizations, we further probe into the new problem of determining the
best-suited pruning scheme considering the different acceleration and accuracy
performance of various pruning schemes. Two pruning scheme mapping methods, one
is search-based and the other is rule-based, are proposed to automatically
derive the best-suited pruning regularity and block size for each layer of any
given DNN. Experimental results demonstrate that our pruning scheme mapping
methods, together with the general fine-grained structured pruning scheme,
outperform the state-of-the-art DNN optimization framework with up to
2.48$\times$ and 1.73$\times$ DNN inference acceleration on CIFAR-10 and
ImageNet dataset without accuracy loss.

    

### [[2111.11595] Semi-Supervised Learning with Taxonomic Labels](http://arxiv.org/abs/2111.11595)


  We propose techniques to incorporate coarse taxonomic labels to train image
classifiers in fine-grained domains. Such labels can often be obtained with a
smaller effort for fine-grained domains such as the natural world where
categories are organized according to a biological taxonomy. On the Semi-iNat
dataset consisting of 810 species across three Kingdoms, incorporating Phylum
labels improves the Species level classification accuracy by 6% in a transfer
learning setting using ImageNet pre-trained models. Incorporating the
hierarchical label structure with a state-of-the-art semi-supervised learning
algorithm called FixMatch improves the performance further by 1.3%. The
relative gains are larger when detailed labels such as Class or Order are
provided, or when models are trained from scratch. However, we find that most
methods are not robust to the presence of out-of-domain data from novel
classes. We propose a technique to select relevant data from a large collection
of unlabeled images guided by the hierarchy which improves the robustness.
Overall, our experiments show that semi-supervised learning with coarse
taxonomic labels are practical for training classifiers in fine-grained
domains.

    

### [[2111.11616] Using mixup as regularization and tuning hyper-parameters for ResNets](http://arxiv.org/abs/2111.11616)


  While novel computer vision architectures are gaining traction, the impact of
model architectures is often related to changes or exploring in training
methods. Identity mapping-based architectures ResNets and DenseNets have
promised path-breaking results in the image classification task and are go-to
methods for even now if the data given is fairly limited. Considering the ease
of training with limited resources this work revisits the ResNets and improves
the ResNet50 \cite{resnets} by using mixup data-augmentation as regularization
and tuning the hyper-parameters.

    

### [[2111.11623] A Modular Framework for Centrality and Clustering in Complex Networks](http://arxiv.org/abs/2111.11623)


  The structure of many complex networks includes edge directionality and
weights on top of their topology. Network analysis that can seamlessly consider
combination of these properties are desirable. In this paper, we study two
important such network analysis techniques, namely, centrality and clustering.
An information-flow based model is adopted for clustering, which itself builds
upon an information theoretic measure for computing centrality. Our principal
contributions include a generalized model of Markov entropic centrality with
the flexibility to tune the importance of node degrees, edge weights and
directions, with a closed-form asymptotic analysis. It leads to a novel
two-stage graph clustering algorithm. The centrality analysis helps reason
about the suitability of our approach to cluster a given graph, and determine
`query' nodes, around which to explore local community structures, leading to
an agglomerative clustering mechanism. The entropic centrality computations are
amortized by our clustering algorithm, making it computationally efficient:
compared to prior approaches using Markov entropic centrality for clustering,
our experiments demonstrate multiple orders of magnitude of speed-up. Our
clustering algorithm naturally inherits the flexibility to accommodate edge
directionality, as well as different interpretations and interplay between edge
weights and node degrees. Overall, this paper thus not only makes significant
theoretical and conceptual contributions, but also translates the findings into
artifacts of practical relevance, yielding new, effective and scalable
centrality computations and graph clustering algorithms, whose efficacy has
been validated through extensive benchmarking experiments.

    

### [[2111.11632] Lossless Compression with Probabilistic Circuits](http://arxiv.org/abs/2111.11632)


  Despite extensive progress on image generation, deep generative models are
suboptimal when applied to lossless compression. For example, models such as
VAEs suffer from a compression cost overhead due to their latent variables that
can only be partially eliminated with elaborated schemes such as bits-back
coding, resulting in oftentimes poor single-sample compression rates. To
overcome such problems, we establish a new class of tractable lossless
compression models that permit efficient encoding and decoding: Probabilistic
Circuits (PCs). These are a class of neural networks involving $|p|$
computational units that support efficient marginalization over arbitrary
subsets of the $D$ feature dimensions, enabling efficient arithmetic coding. We
derive efficient encoding and decoding schemes that both have time complexity
$\mathcal{O} (\log(D) \cdot |p|)$, where a naive scheme would have linear costs
in $D$ and $|p|$, making the approach highly scalable. Empirically, our
PC-based (de)compression algorithm runs 5-20x faster than neural compression
algorithms that achieve similar bitrates. By scaling up the traditional PC
structure learning pipeline, we achieved state-of-the-art results on image
datasets such as MNIST. Furthermore, PCs can be naturally integrated with
existing neural compression algorithms to improve the performance of these base
models on natural image datasets. Our results highlight the potential impact
that non-standard learning architectures may have on neural data compression.

    

### [[2111.11638] Network In Graph Neural Network](http://arxiv.org/abs/2111.11638)


  Graph Neural Networks (GNNs) have shown success in learning from graph
structured data containing node/edge feature information, with application to
social networks, recommendation, fraud detection and knowledge graph reasoning.
In this regard, various strategies have been proposed in the past to improve
the expressiveness of GNNs. For example, one straightforward option is to
simply increase the parameter size by either expanding the hid-den dimension or
increasing the number of GNN layers. However, wider hidden layers can easily
lead to overfitting, and incrementally adding more GNN layers can potentially
result in this http URL this paper, we present a model-agnostic
methodology, namely Network In Graph Neural Network (NGNN ), that allows
arbitrary GNN models to increase their model capacity by making the model
deeper. However, instead of adding or widening GNN layers, NGNN deepens a GNN
model by inserting non-linear feedforward neural network layer(s) within each
GNN layer. An analysis of NGNN as applied to a GraphSage base GNN on
ogbn-products data demonstrate that it can keep the model stable against either
node feature or graph structure perturbations. Furthermore, wide-ranging
evaluation results on both node classification and link prediction tasks show
that NGNN works reliably across diverse GNN architectures.For instance, it
improves the test accuracy of GraphSage on the ogbn-products by 1.6% and
improves the hits@100 score of SEAL on ogbl-ppa by 7.08% and the hits@20 score
of GraphSage+Edge-Attr on ogbl-ppi by 6.22%. And at the time of this
submission, it achieved two first places on the OGB link prediction
leaderboard.

    

### [[2111.11639] Isolation forests: looking beyond tree depth](http://arxiv.org/abs/2111.11639)


  The isolation forest algorithm for outlier detection exploits a simple yet
effective observation: if taking some multivariate data and making uniformly
random cuts across the feature space recursively, it will take fewer such
random cuts for an outlier to be left alone in a given subspace as compared to
regular observations. The original idea proposed an outlier score based on the
tree depth (number of random cuts) required for isolation, but experiments here
show that using information about the size of the feature space taken and the
number of points assigned to it can result in improved results in many
situations without any modification to the tree structure, especially in the
presence of categorical features.

    

### [[2111.11647] Inducing Functions through Reinforcement Learning without Task Specification](http://arxiv.org/abs/2111.11647)


  We report a bio-inspired framework for training a neural network through
reinforcement learning to induce high level functions within the network. Based
on the interpretation that animals have gained their cognitive functions such
as object recognition - without ever being specifically trained for - as a
result of maximizing their fitness to the environment, we place our agent in an
environment where developing certain functions may facilitate decision making.
The experimental results show that high level functions, such as image
classification and hidden variable estimation, can be naturally and
simultaneously induced without any pre-training or specifying them.

    

### [[2111.11652] CoDiM: Learning with Noisy Labels via Contrastive Semi-Supervised Learning](http://arxiv.org/abs/2111.11652)


  Labels are costly and sometimes unreliable. Noisy label learning,
semi-supervised learning, and contrastive learning are three different
strategies for designing learning processes requiring less annotation cost.
Semi-supervised learning and contrastive learning have been recently
demonstrated to improve learning strategies that address datasets with noisy
labels. Still, the inner connections between these fields as well as the
potential to combine their strengths together have only started to emerge. In
this paper, we explore further ways and advantages to fuse them. Specifically,
we propose CSSL, a unified Contrastive Semi-Supervised Learning algorithm, and
CoDiM (Contrastive DivideMix), a novel algorithm for learning with noisy
labels. CSSL leverages the power of classical semi-supervised learning and
contrastive learning technologies and is further adapted to CoDiM, which learns
robustly from multiple types and levels of label noise. We show that CoDiM
brings consistent improvements and achieves state-of-the-art results on
multiple benchmarks.

    

### [[2111.11654] Weight Pruning and Uncertainty in Radio Galaxy Classification](http://arxiv.org/abs/2111.11654)


  In this work we use variational inference to quantify the degree of epistemic
uncertainty in model predictions of radio galaxy classification and show that
the level of model posterior variance for individual test samples is correlated
with human uncertainty when labelling radio galaxies. We explore the model
performance and uncertainty calibration for a variety of different weight
priors and suggest that a sparse prior produces more well-calibrated
uncertainty estimates. Using the posterior distributions for individual
weights, we show that signal-to-noise ratio (SNR) ranking allows pruning of the
fully-connected layers to the level of 30\% without significant loss of
performance, and that this pruning increases the predictive uncertainty in the
model. Finally we show that, like other work in this field, we experience a
cold posterior effect. We examine whether adapting the cost function in our
model to accommodate model misspecification can compensate for this effect, but
find that it does not make a significant difference. We also examine the effect
of principled data augmentation and find that it improves upon the baseline but
does not compensate for the observed effect fully. We interpret this as the
cold posterior effect being due to the overly effective curation of our
training sample leading to likelihood misspecification, and raise this as a
potential issue for Bayesian deep learning approaches to radio galaxy
classification in future.

    

### [[2111.11655] Multi-task manifold learning for small sample size datasets](http://arxiv.org/abs/2111.11655)


  In this study, we develop a method for multi-task manifold learning. The
method aims to improve the performance of manifold learning for multiple tasks,
particularly when each task has a small number of samples. Furthermore, the
method also aims to generate new samples for new tasks, in addition to new
samples for existing tasks. In the proposed method, we use two different types
of information transfer: instance transfer and model transfer. For instance
transfer, datasets are merged among similar tasks, whereas for model transfer,
the manifold models are averaged among similar tasks. For this purpose, the
proposed method consists of a set of generative manifold models corresponding
to the tasks, which are integrated into a general model of a fiber bundle. We
applied the proposed method to artificial datasets and face image sets, and the
results showed that the method was able to estimate the manifolds, even for a
tiny number of samples.

    

### [[2111.11676] RIO: Rotation-equivariance supervised learning of robust inertial odometry](http://arxiv.org/abs/2111.11676)


  This paper introduces rotation-equivariance as a self-supervisor to train
inertial odometry models. We demonstrate that the self-supervised scheme
provides a powerful supervisory signal at training phase as well as at
inference stage. It reduces the reliance on massive amounts of labeled data for
training a robust model and makes it possible to update the model using various
unlabeled data. Further, we propose adaptive Test-Time Training (TTT) based on
uncertainty estimations in order to enhance the generalizability of the
inertial odometry to various unseen data. We show in experiments that the
Rotation-equivariance-supervised Inertial Odometry (RIO) trained with 30% data
achieves on par performance with a model trained with the whole database.
Adaptive TTT improves models performance in all cases and makes more than 25%
improvements under several scenarios.

    

### [[2111.11702] Deep learning-based fast solver of the shallow water equations](http://arxiv.org/abs/2111.11702)


  Fast and reliable prediction of river flow velocities is important in many
applications, including flood risk management. The shallow water equations
(SWEs) are commonly used for this purpose. However, traditional numerical
solvers of the SWEs are computationally expensive and require high-resolution
riverbed profile measurement (bathymetry). In this work, we propose a two-stage
process in which, first, using the principal component geostatistical approach
(PCGA) we estimate the probability density function of the bathymetry from flow
velocity measurements, and then use machine learning (ML) algorithms to obtain
a fast solver for the SWEs. The fast solver uses realizations from the
posterior bathymetry distribution and takes as input the prescribed range of
BCs. The first stage allows us to predict flow velocities without direct
measurement of the bathymetry. Furthermore, we augment the bathymetry posterior
distribution to a more general class of distributions before providing them as
inputs to ML algorithm in the second stage. This allows the solver to
incorporate future direct bathymetry measurements into the flow velocity
prediction for improved accuracy, even if the bathymetry changes over time
compared to its original indirect estimation. We propose and benchmark three
different solvers, referred to as PCA-DNN (principal component analysis-deep
neural network), SE (supervised encoder), and SVE (supervised variational
encoder), and validate them on the Savannah river, Augusta, GA. Our results
show that the fast solvers are capable of predicting flow velocities for
different bathymetry and BCs with good accuracy, at a computational cost that
is significantly lower than the cost of solving the full boundary value problem
with traditional methods.

    

### [[2111.11703] A Contextual Latent Space Model: Subsequence Modulation in Melodic Sequence](http://arxiv.org/abs/2111.11703)


  Some generative models for sequences such as music and text allow us to edit
only subsequences, given surrounding context sequences, which plays an
important part in steering generation interactively. However, editing
subsequences mainly involves randomly resampling subsequences from a possible
generation space. We propose a contextual latent space model (CLSM) in order
for users to be able to explore subsequence generation with a sense of
direction in the generation space, e.g., interpolation, as well as exploring
variations -- semantically similar possible subsequences. A context-informed
prior and decoder constitute the generative model of CLSM, and a context
position-informed encoder is the inference model. In experiments, we use a
monophonic symbolic music dataset, demonstrating that our contextual latent
space is smoother in interpolation than baselines, and the quality of generated
samples is superior to baseline models. The generation examples are available
online.

    

### [[2111.11709] A Multi-Stage model based on YOLOv3 for defect detection in PV panels based on IR and Visible Imaging by Unmanned Aerial Vehicle](http://arxiv.org/abs/2111.11709)


  As solar capacity installed worldwide continues to grow, there is an
increasing awareness that advanced inspection systems are becoming of utmost
importance to schedule smart interventions and minimize downtime likelihood. In
this work we propose a novel automatic multi-stage model to detect panel
defects on aerial images captured by unmanned aerial vehicle by using the
YOLOv3 network and Computer Vision techniques. The model combines detections of
panels and defects to refine its accuracy. The main novelties are represented
by its versatility to process either thermographic or visible images and detect
a large variety of defects and its portability to both rooftop and
ground-mounted PV systems and different panel types. The proposed model has
been validated on two big PV plants in the south of Italy with an outstanding
AP@0.5 exceeding 98% for panel detection, a remarkable AP@0.4 (AP@0.5) of
roughly 88.3% (66.95%) for hotspots by means of infrared thermography and a
mAP@0.5 of almost 70% in the visible spectrum for detection of anomalies
including panel shading induced by soiling and bird dropping, delamination,
presence of puddles and raised rooftop panels. An estimation of the soiling
coverage is also predicted. Finally an analysis of the influence of the
different YOLOv3's output scales on the detection is discussed.

    

### [[2111.11710] Link Analysis meets Ontologies: Are Embeddings the Answer?](http://arxiv.org/abs/2111.11710)


  The increasing amounts of semantic resources offer valuable storage of human
knowledge; however, the probability of wrong entries increases with the
increased size. The development of approaches that identify potentially
spurious parts of a given knowledge base is thus becoming an increasingly
important area of interest. In this work, we present a systematic evaluation of
whether structure-only link analysis methods can already offer a scalable means
to detecting possible anomalies, as well as potentially interesting novel
relation candidates. Evaluating thirteen methods on eight different semantic
resources, including Gene Ontology, Food Ontology, Marine Ontology and similar,
we demonstrated that structure-only link analysis could offer scalable anomaly
detection for a subset of the data sets. Further, we demonstrated that by
considering symbolic node embedding, explanations of the predictions (links)
could be obtained, making this branch of methods potentially more valuable than
the black-box only ones. To our knowledge, this is currently one of the most
extensive systematic studies of the applicability of different types of link
analysis methods across semantic resources from different domains.

    

### [[2111.11711] Sample Efficient Imitation Learning via Reward Function Trained in Advance](http://arxiv.org/abs/2111.11711)


  Imitation learning (IL) is a framework that learns to imitate expert behavior
from demonstrations. Recently, IL shows promising results on high dimensional
and control tasks. However, IL typically suffers from sample inefficiency in
terms of environment interaction, which severely limits their application to
simulated domains. In industrial applications, learner usually have a high
interaction cost, the more interactions with environment, the more damage it
causes to the environment and the learner itself. In this article, we make an
effort to improve sample efficiency by introducing a novel scheme of inverse
reinforcement learning. Our method, which we call \textit{Model Reward Function
Based Imitation Learning} (MRFIL), uses an ensemble dynamic model as a reward
function, what is trained with expert demonstrations. The key idea is to
provide the agent with an incentive to match the demonstrations over a long
horizon, by providing a positive reward upon encountering states in line with
the expert demonstration distribution. In addition, we demonstrate the
convergence guarantee for new objective function. Experimental results show
that our algorithm reaches the competitive performance and significantly
reducing the environment interactions compared to IL methods.

    

### [[2111.11719] Variational encoder geostatistical analysis (VEGAS) with an application to large scale riverine bathymetry](http://arxiv.org/abs/2111.11719)


  Estimation of riverbed profiles, also known as bathymetry, plays a vital role
in many applications, such as safe and efficient inland navigation, prediction
of bank erosion, land subsidence, and flood risk management. The high cost and
complex logistics of direct bathymetry surveys, i.e., depth imaging, have
encouraged the use of indirect measurements such as surface flow velocities.
However, estimating high-resolution bathymetry from indirect measurements is an
inverse problem that can be computationally challenging. Here, we propose a
reduced-order model (ROM) based approach that utilizes a variational
autoencoder (VAE), a type of deep neural network with a narrow layer in the
middle, to compress bathymetry and flow velocity information and accelerate
bathymetry inverse problems from flow velocity measurements. In our
application, the shallow-water equations (SWE) with appropriate boundary
conditions (BCs), e.g., the discharge and/or the free surface elevation,
constitute the forward problem, to predict flow velocity. Then, ROMs of the
SWEs are constructed on a nonlinear manifold of low dimensionality through a
variational encoder. Estimation with uncertainty quantification (UQ) is
performed on the low-dimensional latent space in a Bayesian setting. We have
tested our inversion approach on a one-mile reach of the Savannah River, GA,
USA. Once the neural network is trained (offline stage), the proposed technique
can perform the inversion operation orders of magnitude faster than traditional
inversion methods that are commonly based on linear projections, such as
principal component analysis (PCA), or the principal component geostatistical
approach (PCGA). Furthermore, tests show that the algorithm can estimate the
bathymetry with good accuracy even with sparse flow velocity measurements.

    

### [[2111.11743] Independent Learning in Stochastic Games](http://arxiv.org/abs/2111.11743)


  Reinforcement learning (RL) has recently achieved tremendous successes in
many artificial intelligence applications. Many of the forefront applications
of RL involve multiple agents, e.g., playing chess and Go games, autonomous
driving, and robotics. Unfortunately, the framework upon which classical RL
builds is inappropriate for multi-agent learning, as it assumes an agent's
environment is stationary and does not take into account the adaptivity of
other agents. In this review paper, we present the model of stochastic games
for multi-agent learning in dynamic environments. We focus on the development
of simple and independent learning dynamics for stochastic games: each agent is
myopic and chooses best-response type actions to other agents' strategy without
any coordination with her opponent. There has been limited progress on
developing convergent best-response type independent learning dynamics for
stochastic games. We present our recently proposed simple and independent
learning dynamics that guarantee convergence in zero-sum stochastic games,
together with a review of other contemporaneous algorithms for dynamic
multi-agent learning in this setting. Along the way, we also reexamine some
classical results from both the game theory and RL literature, to situate both
the conceptual contributions of our independent learning dynamics, and the
mathematical novelties of our analysis. We hope this review paper serves as an
impetus for the resurgence of studying independent and natural learning
dynamics in game theory, for the more challenging settings with a dynamic
environment.

    

### [[2111.11758] Understanding the Impact of Data Distribution on Q-learning with Function Approximation](http://arxiv.org/abs/2111.11758)


  In this work, we focus our attention on the study of the interplay between
the data distribution and Q-learning-based algorithms with function
approximation. We provide a theoretical and empirical analysis as to why
different properties of the data distribution can contribute to regulating
sources of algorithmic instability. First, we revisit theoretical bounds on the
performance of approximate dynamic programming algorithms. Second, we provide a
novel four-state MDP that highlights the impact of the data distribution in the
performance of a Q-learning algorithm with function approximation, both in
online and offline settings. Finally, we experimentally assess the impact of
the data distribution properties in the performance of an offline deep
Q-network algorithm. Our results show that: (i) the data distribution needs to
possess certain properties in order to robustly learn in an offline setting,
namely low distance to the distributions induced by optimal policies of the MDP
and high coverage over the state-action space; and (ii) high entropy data
distributions can contribute to mitigating sources of algorithmic instability.

    

### [[2111.11763] Uncertainty estimation under model misspecification in neural network regression](http://arxiv.org/abs/2111.11763)


  Although neural networks are powerful function approximators, the underlying
modelling assumptions ultimately define the likelihood and thus the hypothesis
class they are parameterizing. In classification, these assumptions are minimal
as the commonly employed softmax is capable of representing any categorical
distribution. In regression, however, restrictive assumptions on the type of
continuous distribution to be realized are typically placed, like the dominant
choice of training via mean-squared error and its underlying Gaussianity
assumption. Recently, modelling advances allow to be agnostic to the type of
continuous distribution to be modelled, granting regression the flexibility of
classification models. While past studies stress the benefit of such flexible
regression models in terms of performance, here we study the effect of the
model choice on uncertainty estimation. We highlight that under model
misspecification, aleatoric uncertainty is not properly captured, and that a
Bayesian treatment of a misspecified model leads to unreliable epistemic
uncertainty estimates. Overall, our study provides an overview on how modelling
choices in regression may influence uncertainty estimation and thus any
downstream decision making process.

    

### [[2111.11768] Schedule Based Temporal Difference Algorithms](http://arxiv.org/abs/2111.11768)


  Learning the value function of a given policy from data samples is an
important problem in Reinforcement Learning. TD($\lambda$) is a popular class
of algorithms to solve this problem. However, the weights assigned to different
$n$-step returns in TD($\lambda$), controlled by the parameter $\lambda$,
decrease exponentially with increasing $n$. In this paper, we present a
$\lambda$-schedule procedure that generalizes the TD($\lambda$) algorithm to
the case when the parameter $\lambda$ could vary with time-step. This allows
flexibility in weight assignment, i.e., the user can specify the weights
assigned to different $n$-step returns by choosing a sequence $\{\lambda_t\}_{t
\geq 1}$. Based on this procedure, we propose an on-policy algorithm -
TD($\lambda$)-schedule, and two off-policy algorithms - GTD($\lambda$)-schedule
and TDC($\lambda$)-schedule, respectively. We provide proofs of almost sure
convergence for all three algorithms under a general Markov noise framework.

    

### [[2111.11771] A self-training framework for glaucoma grading in OCT B-scans](http://arxiv.org/abs/2111.11771)


  In this paper, we present a self-training-based framework for glaucoma
grading using OCT B-scans under the presence of domain shift. Particularly, the
proposed two-step learning methodology resorts to pseudo-labels generated
during the first step to augment the training dataset on the target domain,
which is then used to train the final target model. This allows transferring
knowledge-domain from the unlabeled data. Additionally, we propose a novel
glaucoma-specific backbone which introduces residual and attention modules via
skip-connections to refine the embedding features of the latent space. By doing
this, our model is capable of improving state-of-the-art from a quantitative
and interpretability perspective. The reported results demonstrate that the
proposed learning strategy can boost the performance of the model on the target
dataset without incurring in additional annotation steps, by using only labels
from the source examples. Our model consistently outperforms the baseline by
1-3% across different metrics and bridges the gap with respect to training the
model on the labeled target data.

    

### [[2111.11789] End-to-End Optimized Arrhythmia Detection Pipeline using Machine Learning for Ultra-Edge Devices](http://arxiv.org/abs/2111.11789)


  Atrial fibrillation (AF) is the most prevalent cardiac arrhythmia worldwide,
with 2% of the population affected. It is associated with an increased risk of
strokes, heart failure and other heart-related complications. Monitoring
at-risk individuals and detecting asymptomatic AF could result in considerable
public health benefits, as individuals with asymptomatic AF could take
preventive measures with lifestyle changes. With increasing affordability to
wearables, personalized health care is becoming more accessible. These
personalized healthcare solutions require accurate classification of
bio-signals while being computationally inexpensive. By making inferences
on-device, we avoid issues inherent to cloud-based systems such as latency and
network connection dependency. We propose an efficient pipeline for real-time
Atrial Fibrillation Detection with high accuracy that can be deployed in
ultra-edge devices. The feature engineering employed in this research catered
to optimizing the resource-efficient classifier used in the proposed pipeline,
which was able to outperform the best performing standard ML model by
$10^5\times$ in terms of memory footprint with a mere trade-off of 2%
classification accuracy. We also obtain higher accuracy of approximately 6%
while consuming 403$\times$ lesser memory and being 5.2$\times$ faster compared
to the previous state-of-the-art (SoA) embedded implementation.

    

### [[2111.11798] Composing Partial Differential Equations with Physics-Aware Neural Networks](http://arxiv.org/abs/2111.11798)


  We introduce a compositional physics-aware neural network (FINN) for learning
spatiotemporal advection-diffusion processes. FINN implements a new way of
combining the learning abilities of artificial neural networks with physical
and structural knowledge from numerical simulation by modeling the constituents
of partial differential equations (PDEs) in a compositional manner. Results on
both one- and two-dimensional PDEs (Burger's, diffusion-sorption,
diffusion-reaction, Allen-Cahn) demonstrate FINN's superior modeling accuracy
and excellent out-of-distribution generalization ability beyond initial and
boundary conditions. With only one tenth of the number of parameters on
average, FINN outperforms pure machine learning and other state-of-the-art
physics-aware models in all cases -- often even by multiple orders of
magnitude. Moreover, FINN outperforms a calibrated physical model when
approximating sparse real-world data in a diffusion-sorption scenario,
confirming its generalization abilities and showing explanatory potential by
revealing the unknown retardation factor of the observed process.

    

### [[2111.11802] Pruning Self-attentions into Convolutional Layers in Single Path](http://arxiv.org/abs/2111.11802)


  Vision Transformers (ViTs) have achieved impressive performance over various
computer vision tasks. However, modeling global correlations with multi-head
self-attention (MSA) layers leads to two widely recognized issues: the massive
computational resource consumption and the lack of intrinsic inductive bias for
modeling local visual patterns. One unified solution is to search whether to
replace some MSA layers with convolution-like inductive biases that are
computationally efficient via neural architecture search (NAS) based pruning
methods. However, maintaining MSA and different candidate convolutional
operations as separate trainable paths gives rise to expensive search cost and
challenging optimization. Instead, we propose a novel weight-sharing scheme
between MSA and convolutional operations and cast the search problem as finding
which subset of parameters to use in each MSA layer. The weight-sharing scheme
further allows us to devise an automatic Single-Path Vision Transformer pruning
method (SPViT) to quickly prune the pre-trained ViTs into accurate and compact
hybrid models with significantly reduced search cost, given target efficiency
constraints. We conduct extensive experiments on two representative ViT models
showing our method achieves a favorable accuracy-efficiency trade-off. Code is
available at this https URL.

    

### [[2111.11828] Variance Reduction in Deep Learning: More Momentum is All You Need](http://arxiv.org/abs/2111.11828)


  Variance reduction (VR) techniques have contributed significantly to
accelerating learning with massive datasets in the smooth and strongly convex
setting (Schmidt et al., 2017; Johnson & Zhang, 2013; Roux et al., 2012).
However, such techniques have not yet met the same success in the realm of
large-scale deep learning due to various factors such as the use of data
augmentation or regularization methods like dropout (Defazio & Bottou, 2019).
This challenge has recently motivated the design of novel variance reduction
techniques tailored explicitly for deep learning (Arnold et al., 2019; Ma &
Yarats, 2018). This work is an additional step in this direction. In
particular, we exploit the ubiquitous clustering structure of rich datasets
used in deep learning to design a family of scalable variance reduced
optimization procedures by combining existing optimizers (e.g., SGD+Momentum,
Quasi Hyperbolic Momentum, Implicit Gradient Transport) with a multi-momentum
strategy (Yuan et al., 2019). Our proposal leads to faster convergence than
vanilla methods on standard benchmark datasets (e.g., CIFAR and ImageNet). It
is robust to label noise and amenable to distributed optimization. We provide a
parallel implementation in JAX.

    

### [[2111.11840] Local Permutation Equivariance For Graph Neural Networks](http://arxiv.org/abs/2111.11840)


  In this work we develop a new method, named locally permutation-equivariant
graph neural networks, which provides a framework for building graph neural
networks that operate on local node neighbourhoods, through sub-graphs, while
using permutation equivariant update functions. Message passing neural networks
have been shown to be limited in their expressive power and recent approaches
to over come this either lack scalability or require structural information to
be encoded into the feature space. The general framework presented here
overcomes the scalability issues associated with global permutation
equivariance by operating on sub-graphs through restricted representations. In
addition, we prove that there is no loss of expressivity by using restricted
representations. Furthermore, the proposed framework only requires a choice of
$k$-hops for creating sub-graphs and a choice of representation space to be
used for each layer, which makes the method easily applicable across a range of
graph based domains. We experimentally validate the method on a range of graph
benchmark classification tasks, demonstrating either state-of-the-art results
or very competitive results on all benchmarks. Further, we demonstrate that the
use of local update functions offers a significant improvement in GPU memory
over global methods.

    

### [[2111.11846] Predicting High-Flow Nasal Cannula Failure in an ICU Using a Recurrent Neural Network with Transfer Learning and Input Data Perseveration: A Retrospective Analysis](http://arxiv.org/abs/2111.11846)


  High Flow Nasal Cannula (HFNC) provides non-invasive respiratory support for
critically ill children who may tolerate it more readily than other
Non-Invasive (NIV) techniques. Timely prediction of HFNC failure can provide an
indication for increasing respiratory support. This work developed and compared
machine learning models to predict HFNC failure. A retrospective study was
conducted using EMR of patients admitted to a tertiary pediatric ICU from
January 2010 to February 2020. A Long Short-Term Memory (LSTM) model was
trained to generate a continuous prediction of HFNC failure. Performance was
assessed using the area under the receiver operating curve (AUROC) at various
times following HFNC initiation. The sensitivity, specificity, positive and
negative predictive values (PPV, NPV) of predictions at two hours after HFNC
initiation were also evaluated. These metrics were also computed in a cohort
with primarily respiratory diagnoses. 834 HFNC trials [455 training, 173
validation, 206 test] met the inclusion criteria, of which 175 [103, 30, 42]
(21.0%) escalated to NIV or intubation. The LSTM models trained with transfer
learning generally performed better than the LR models, with the best LSTM
model achieving an AUROC of 0.78, vs 0.66 for the LR, two hours after
initiation. Machine learning models trained using EMR data were able to
identify children at risk for failing HFNC within 24 hours of initiation. LSTM
models that incorporated transfer learning, input data perseveration and
ensembling showed improved performance than the LR and standard LSTM models.

    

### [[2111.11848] Time Series Prediction about Air Quality using LSTM-Based Models: A Systematic Mapping](http://arxiv.org/abs/2111.11848)


  This systematic mapping study investigates the use of Long short-term memory
networks to predict time series data about air quality, trying to understand
the reasons, characteristics and methods available in the scientific
literature, identify gaps in the researched area and potential approaches that
can be exploited on later studies.

    

### [[2111.11850] Incentive Mechanisms for Federated Learning: From Economic and Game Theoretic Perspective](http://arxiv.org/abs/2111.11850)


  Federated learning (FL) becomes popular and has shown great potentials in
training large-scale machine learning (ML) models without exposing the owners'
raw data. In FL, the data owners can train ML models based on their local data
and only send the model updates rather than raw data to the model owner for
aggregation. To improve learning performance in terms of model accuracy and
training completion time, it is essential to recruit sufficient participants.
Meanwhile, the data owners are rational and may be unwilling to participate in
the collaborative learning process due to the resource consumption. To address
the issues, there have been various works recently proposed to motivate the
data owners to contribute their resources. In this paper, we provide a
comprehensive review for the economic and game theoretic approaches proposed in
the literature to design various schemes for stimulating data owners to
participate in FL training process. In particular, we first present the
fundamentals and background of FL, economic theories commonly used in incentive
mechanism design. Then, we review applications of game theory and economic
approaches applied for incentive mechanisms design of FL. Finally, we highlight
some open issues and future research directions concerning incentive mechanism
design of FL.

    

### [[2111.11858] Asteroid Flyby Cycler Trajectory Design Using Deep Neural Networks](http://arxiv.org/abs/2111.11858)


  Asteroid exploration has been attracting more attention in recent years.
Nevertheless, we have just visited tens of asteroids while we have discovered
more than one million bodies. As our current observation and knowledge should
be biased, it is essential to explore multiple asteroids directly to better
understand the remains of planetary building materials. One of the mission
design solutions is utilizing asteroid flyby cycler trajectories with multiple
Earth gravity assists. An asteroid flyby cycler trajectory design problem is a
subclass of global trajectory optimization problems with multiple flybys,
involving a trajectory optimization problem for a given flyby sequence and a
combinatorial optimization problem to decide the sequence of the flybys. As the
number of flyby bodies grows, the computation time of this optimization problem
expands maliciously. This paper presents a new method to design asteroid flyby
cycler trajectories utilizing a surrogate model constructed by deep neural
networks approximating trajectory optimization results. Since one of the
bottlenecks of machine learning approaches is to generate massive trajectory
databases, we propose an efficient database generation strategy by introducing
pseudo-asteroids satisfying the Karush-Kuhn-Tucker conditions. The numerical
result applied to JAXA's DESTINY+ mission shows that the proposed method can
significantly reduce the computational time for searching asteroid flyby
sequences.

    

### [[2111.11862] Inferring User Facial Affect in Work-like Settings](http://arxiv.org/abs/2111.11862)


  Unlike the six basic emotions of happiness, sadness, fear, anger, disgust and
surprise, modelling and predicting dimensional affect in terms of valence
(positivity - negativity) and arousal (intensity) has proven to be more
flexible, applicable and useful for naturalistic and real-world settings. In
this paper, we aim to infer user facial affect when the user is engaged in
multiple work-like tasks under varying difficulty levels (baseline, easy, hard
and stressful conditions), including (i) an office-like setting where they
undertake a task that is less physically demanding but requires greater mental
strain; (ii) an assembly-line-like setting that requires the usage of fine
motor skills; and (iii) an office-like setting representing teleworking and
teleconferencing. In line with this aim, we first design a study with different
conditions and gather multimodal data from 12 subjects. We then perform several
experiments with various machine learning models and find that: (i) the display
and prediction of facial affect vary from non-working to working settings; (ii)
prediction capability can be boosted by using datasets captured in a work-like
context; and (iii) segment-level (spectral representation) information is
crucial in improving the facial affect prediction.

    

### [[2111.11863] Explainable Deep Image Classifiers for Skin Lesion Diagnosis](http://arxiv.org/abs/2111.11863)


  A key issue in critical contexts such as medical diagnosis is the
interpretability of the deep learning models adopted in decision-making
systems. Research in eXplainable Artificial Intelligence (XAI) is trying to
solve this issue. However, often XAI approaches are only tested on generalist
classifier and do not represent realistic problems such as those of medical
diagnosis. In this paper, we analyze a case study on skin lesion images where
we customize an existing XAI approach for explaining a deep learning model able
to recognize different types of skin lesions. The explanation is formed by
synthetic exemplar and counter-exemplar images of skin lesion and offers the
practitioner a way to highlight the crucial traits responsible for the
classification decision. A survey conducted with domain experts, beginners and
unskilled people proof that the usage of explanations increases the trust and
confidence in the automatic decision system. Also, an analysis of the latent
space adopted by the explainer unveils that some of the most frequent skin
lesion classes are distinctly separated. This phenomenon could derive from the
intrinsic characteristics of each class and, hopefully, can provide support in
the resolution of the most frequent misclassifications by human experts.

    

### [[2111.11868] Multi-agent Bayesian Deep Reinforcement Learning for Microgrid Energy Management under Communication Failures](http://arxiv.org/abs/2111.11868)


  Microgrids (MGs) are important players for the future transactive energy
systems where a number of intelligent Internet of Things (IoT) devices interact
for energy management in the smart grid. Although there have been many works on
MG energy management, most studies assume a perfect communication environment,
where communication failures are not considered. In this paper, we consider the
MG as a multi-agent environment with IoT devices in which AI agents exchange
information with their peers for collaboration. However, the collaboration
information may be lost due to communication failures or packet loss. Such
events may affect the operation of the whole MG. To this end, we propose a
multi-agent Bayesian deep reinforcement learning (BA-DRL) method for MG energy
management under communication failures. We first define a multi-agent
partially observable Markov decision process (MA-POMDP) to describe agents
under communication failures, in which each agent can update its beliefs on the
actions of its peers. Then, we apply a double deep Q-learning (DDQN)
architecture for Q-value estimation in BA-DRL, and propose a belief-based
correlated equilibrium for the joint-action selection of multi-agent BA-DRL.
Finally, the simulation results show that BA-DRL is robust to both power supply
uncertainty and communication failure uncertainty. BA-DRL has 4.1% and 10.3%
higher reward than Nash Deep Q-learning (Nash-DQN) and alternating direction
method of multipliers (ADMM) respectively under 1% communication failure
probability.

    

### [[2111.11869] Machine unlearning via GAN](http://arxiv.org/abs/2111.11869)


  Machine learning models, especially deep models, may unintentionally remember
information about their training data. Malicious attackers can thus pilfer some
property about training data by attacking the model via membership inference
attack or model inversion attack. Some regulations, such as the EU's GDPR, have
enacted "The Right to Be Forgotten" to protect users' data privacy, enhancing
individuals' sovereignty over their data. Therefore, removing training data
information from a trained model has become a critical issue. In this paper, we
present a GAN-based algorithm to delete data in deep models, which
significantly improves deleting speed compared to retraining from scratch,
especially in complicated scenarios. We have experimented on five commonly used
datasets, and the experimental results show the efficiency of our method.

    

### [[2111.11870] DBIA: Data-free Backdoor Injection Attack against Transformer Networks](http://arxiv.org/abs/2111.11870)


  Recently, transformer architecture has demonstrated its significance in both
Natural Language Processing (NLP) and Computer Vision (CV) tasks. Though other
network models are known to be vulnerable to the backdoor attack, which embeds
triggers in the model and controls the model behavior when the triggers are
presented, little is known whether such an attack is still valid on the
transformer models and if so, whether it can be done in a more cost-efficient
manner. In this paper, we propose DBIA, a novel data-free backdoor attack
against the CV-oriented transformer networks, leveraging the inherent attention
mechanism of transformers to generate triggers and injecting the backdoor using
the poisoned surrogate dataset. We conducted extensive experiments based on
three benchmark transformers, i.e., ViT, DeiT and Swin Transformer, on two
mainstream image classification tasks, i.e., CIFAR10 and ImageNet. The
evaluation results demonstrate that, consuming fewer resources, our approach
can embed backdoors with a high success rate and a low impact on the
performance of the victim transformers. Our code is available at
https://anonymous.4open.science/r/DBIA-825D.

    

### [[2111.11874] Is this IoT Device Likely to be Secure? Risk Score Prediction for IoT Devices Using Gradient Boosting Machines](http://arxiv.org/abs/2111.11874)


  Security risk assessment and prediction are critical for organisations
deploying Internet of Things (IoT) devices. An absolute minimum requirement for
enterprises is to verify the security risk of IoT devices for the reported
vulnerabilities in the National Vulnerability Database (NVD). This paper
proposes a novel risk prediction for IoT devices based on publicly available
information about them. Our solution provides an easy and cost-efficient
solution for enterprises of all sizes to predict the security risk of deploying
new IoT devices. After an extensive analysis of the NVD records over the past
eight years, we have created a unique, systematic, and balanced dataset for
vulnerable IoT devices, including key technical features complemented with
functional and descriptive features available from public resources. We then
use machine learning classification models such as Gradient Boosting Decision
Trees (GBDT) over this dataset and achieve 71% prediction accuracy in
classifying the severity of device vulnerability score.

    

### [[2111.11875] Functional Model of Residential Consumption Elasticity under Dynamic Tariffs](http://arxiv.org/abs/2111.11875)


  One of the major barriers for the retailers is to understand the consumption
elasticity they can expect from their contracted demand response (DR) clients.
The current trend of DR products provided by retailers are not
consumer-specific, which poses additional barriers for the active engagement of
consumers in these programs. The elasticity of consumers demand behavior varies
from individual to individual. The utility will benefit from knowing more
accurately how changes in its prices will modify the consumption pattern of its
clients. This work proposes a functional model for the consumption elasticity
of the DR contracted consumers. The model aims to determine the load adjustment
the DR consumers can provide to the retailers or utilities for different price
levels. The proposed model uses a Bayesian probabilistic approach to identify
the actual load adjustment an individual contracted client can provide for
different price levels it can experience. The developed framework provides the
retailers or utilities with a tool to obtain crucial information on how an
individual consumer will respond to different price levels. This approach is
able to quantify the likelihood with which the consumer reacts to a DR signal
and identify the actual load adjustment an individual contracted DR client
provides for different price levels they can experience. This information can
be used to maximize the control and reliability of the services the retailer or
utility can offer to the System Operators.

    

### [[2111.11879] Weakly-Supervised Cloud Detection with Fixed-Point GANs](http://arxiv.org/abs/2111.11879)


  The detection of clouds in satellite images is an essential preprocessing
task for big data in remote sensing. Convolutional neural networks (CNNs) have
greatly advanced the state-of-the-art in the detection of clouds in satellite
images, but existing CNN-based methods are costly as they require large amounts
of training images with expensive pixel-level cloud labels. To alleviate this
cost, we propose Fixed-Point GAN for Cloud Detection (FCD), a weakly-supervised
approach. Training with only image-level labels, we learn fixed-point
translation between clear and cloudy images, so only clouds are affected during
translation. Doing so enables our approach to predict pixel-level cloud labels
by translating satellite images to clear ones and setting a threshold to the
difference between the two images. Moreover, we propose FCD+, where we exploit
the label-noise robustness of CNNs to refine the prediction of FCD, leading to
further improvements. We demonstrate the effectiveness of our approach on the
Landsat-8 Biome cloud detection dataset, where we obtain performance close to
existing fully-supervised methods that train with expensive pixel-level labels.
By fine-tuning our FCD+ with just 1% of the available pixel-level labels, we
match the performance of fully-supervised methods.

    

### [[2111.11932] Modelling Direct Messaging Networks with Multiple Recipients for Cyber Deception](http://arxiv.org/abs/2111.11932)


  Cyber deception is emerging as a promising approach to defending networks and
systems against attackers and data thieves. However, despite being relatively
cheap to deploy, the generation of realistic content at scale is very costly,
due to the fact that rich, interactive deceptive technologies are largely
hand-crafted. With recent improvements in Machine Learning, we now have the
opportunity to bring scale and automation to the creation of realistic and
enticing simulated content. In this work, we propose a framework to automate
the generation of email and instant messaging-style group communications at
scale. Such messaging platforms within organisations contain a lot of valuable
information inside private communications and document attachments, making them
an enticing target for an adversary. We address two key aspects of simulating
this type of system: modelling when and with whom participants communicate, and
generating topical, multi-party text to populate simulated conversation
threads. We present the LogNormMix-Net Temporal Point Process as an approach to
the first of these, building upon the intensity-free modeling approach of
Shchur et al.~\cite{shchur2019intensity} to create a generative model for
unicast and multi-cast communications. We demonstrate the use of fine-tuned,
pre-trained language models to generate convincing multi-party conversation
threads. A live email server is simulated by uniting our LogNormMix-Net TPP (to
generate the communication timestamp, sender and recipients) with the language
model, which generates the contents of the multi-party email threads. We
evaluate the generated content with respect to a number of realism-based
properties, that encourage a model to learn to generate content that will
engage the attention of an adversary to achieve a deception outcome.

    

### [[2111.11946] Is Shapley Explanation for a model unique?](http://arxiv.org/abs/2111.11946)


  Shapley value has recently become a popular way to explain the predictions of
complex and simple machine learning models. This paper is discusses the factors
that influence Shapley value. In particular, we explore the relationship
between the distribution of a feature and its Shapley value. We extend our
analysis by discussing the difference that arises in Shapley explanation for
different predicted outcomes from the same model. Our assessment is that
Shapley value for particular feature not only depends on its expected mean but
on other moments as well such as variance and there are disagreements for
baseline prediction, disagreements for signs and most important feature for
different outcomes such as probability, log odds, and binary decision generated
using same linear probability model (logit/probit). These disagreements not
only stay for local explainability but also affect the global feature
importance. We conclude that there is no unique Shapley explanation for a given
model. It varies with model outcome (Probability/Log-odds/binary decision such
as accept vs reject) and hence model application.

    

### [[2111.11954] Depth induces scale-averaging in overparameterized linear Bayesian neural networks](http://arxiv.org/abs/2111.11954)


  Inference in deep Bayesian neural networks is only fully understood in the
infinite-width limit, where the posterior flexibility afforded by increased
depth washes out and the posterior predictive collapses to a shallow Gaussian
process. Here, we interpret finite deep linear Bayesian neural networks as
data-dependent scale mixtures of Gaussian process predictors across output
channels. We leverage this observation to study representation learning in
these networks, allowing us to connect limiting results obtained in previous
studies within a unified framework. In total, these results advance our
analytical understanding of how depth affects inference in a simple class of
Bayesian neural networks.

    

### [[2111.11956] ptype-cat: Inferring the Type and Values of Categorical Variables](http://arxiv.org/abs/2111.11956)


  Type inference is the task of identifying the type of values in a data column
and has been studied extensively in the literature. Most existing type
inference methods support data types such as Boolean, date, float, integer and
string. However, these methods do not consider non-Boolean categorical
variables, where there are more than two possible values encoded by integers or
strings. Therefore, such columns are annotated either as integer or string
rather than categorical, and need to be transformed into categorical manually
by the user. In this paper, we propose a probabilistic type inference method
that can identify the general categorical data type (including non-Boolean
variables). Additionally, we identify the possible values of each categorical
variable by adapting the existing type inference method ptype. Combining these
methods, we present ptype-cat which achieves better results than existing
applicable solutions.

    

### [[2111.11959] Identifying the Units of Measurement in Tabular Data](http://arxiv.org/abs/2111.11959)


  We consider the problem of identifying the units of measurement in a data
column that contains both numeric values and unit symbols in each row, e.g.,
"5.2 l", "7 pints". In this case we seek to identify the dimension of the
column (e.g. volume) and relate the unit symbols to valid units (e.g. litre,
pint) obtained from a knowledge graph. Below we present PUC, a Probabilistic
Unit Canonicalizer that can accurately identify the units of measurement,
extract semantic descriptions of quantitative data columns and canonicalize
their entries. We present the first messy real-world tabular datasets annotated
for units of measurement, which can enable and accelerate the research in this
area. Our experiments on these datasets show that PUC achieves better results
than existing solutions.

    

### [[2111.11964] Reviewing continual learning from the perspective of human-level intelligence](http://arxiv.org/abs/2111.11964)


  Humans' continual learning (CL) ability is closely related to Stability
Versus Plasticity Dilemma that describes how humans achieve ongoing learning
capacity and preservation for learned information. The notion of CL has always
been present in artificial intelligence (AI) since its births. This paper
proposes a comprehensive review of CL. Different from previous reviews that
mainly focus on the catastrophic forgetting phenomenon in CL, this paper
surveys CL from a more macroscopic perspective based on the Stability Versus
Plasticity mechanism. Analogous to biological counterpart, "smart" AI agents
are supposed to i) remember previously learned information (information
retrospection); ii) infer on new information continuously (information
prospection:); iii) transfer useful information (information transfer), to
achieve high-level CL. According to the taxonomy, evaluation metrics,
algorithms, applications as well as some open issues are then introduced. Our
main contributions concern i) rechecking CL from the level of artificial
general intelligence; ii) providing a detailed and extensive overview on CL
topics; iii) presenting some novel ideas on the potential development of CL.

    

### [[2111.11971] Tree density estimation](http://arxiv.org/abs/2111.11971)


  We study the problem of density estimation for a random vector ${\boldsymbol
X}$ in $\mathbb R^d$ with probability density $f(\boldsymbol x)$. For a
spanning tree $T$ defined on the vertex set $\{1,\dots ,d\}$, the tree density
$f_{T}$ is a product of bivariate conditional densities. The optimal spanning
tree $T^*$ is the spanning tree $T$, for which the Kullback-Leibler divergence
of $f$ and $f_{T}$ is the smallest. From i.i.d. data we identify the optimal
tree $T^*$ and computationally efficiently construct a tree density estimate
$f_n$ such that, without any regularity conditions on the density $f$, one has
that $\lim_{n\to \infty} \int |f_n(\boldsymbol x)-f_{T^*}(\boldsymbol
x)|d\boldsymbol x=0$ a.s. For Lipschitz continuous $f$ with bounded support,
$\mathbb E\{ \int |f_n(\boldsymbol x)-f_{T^*}(\boldsymbol x)|d\boldsymbol
x\}=O(n^{-1/4})$.

    

### [[2111.11980] Scalable Learning for Optimal Load Shedding Under Power Grid Emergency Operations](http://arxiv.org/abs/2111.11980)


  Effective and timely responses to unexpected contingencies are crucial for
enhancing the resilience of power grids. Given the fast, complex process of
cascading propagation, corrective actions such as optimal load shedding (OLS)
are difficult to attain in large-scale networks due to the computation
complexity and communication latency issues. This work puts forth an innovative
learning-for-OLS approach by constructing the optimal decision rules of load
shedding under a variety of potential contingency scenarios through offline
neural network (NN) training. Notably, the proposed NN-based OLS decisions are
fully decentralized, enabling individual load centers to quickly react to the
specific contingency using readily available local measurements. Numerical
studies on the IEEE 14-bus system have demonstrated the effectiveness of our
scalable OLS design for real-time responses to severe grid emergency events.

    

### [[2111.11986] HERO: Hessian-Enhanced Robust Optimization for Unifying and Improving Generalization and Quantization Performance](http://arxiv.org/abs/2111.11986)


  With the recent demand of deploying neural network models on mobile and edge
devices, it is desired to improve the model's generalizability on unseen
testing data, as well as enhance the model's robustness under fixed-point
quantization for efficient deployment. Minimizing the training loss, however,
provides few guarantees on the generalization and quantization performance. In
this work, we fulfill the need of improving generalization and quantization
performance simultaneously by theoretically unifying them under the framework
of improving the model's robustness against bounded weight perturbation and
minimizing the eigenvalues of the Hessian matrix with respect to model weights.
We therefore propose HERO, a Hessian-enhanced robust optimization method, to
minimize the Hessian eigenvalues through a gradient-based training process,
simultaneously improving the generalization and quantization performance. HERO
enables up to a 3.8% gain on test accuracy, up to 30% higher accuracy under 80%
training label perturbation, and the best post-training quantization accuracy
across a wide range of precision, including a >10% accuracy improvement over
SGD-trained models for common model architectures on various datasets.

    

### [[2111.11992] Sparse Fusion for Multimodal Transformers](http://arxiv.org/abs/2111.11992)


  Multimodal classification is a core task in human-centric machine learning.
We observe that information is highly complementary across modalities, thus
unimodal information can be drastically sparsified prior to multimodal fusion
without loss of accuracy. To this end, we present Sparse Fusion Transformers
(SFT), a novel multimodal fusion method for transformers that performs
comparably to existing state-of-the-art methods while having greatly reduced
memory footprint and computation cost. Key to our idea is a sparse-pooling
block that reduces unimodal token sets prior to cross-modality modeling.
Evaluations are conducted on multiple multimodal benchmark datasets for a wide
range of classification tasks. State-of-the-art performance is obtained on
multiple benchmarks under similar experiment conditions, while reporting up to
six-fold reduction in computational cost and memory requirements. Extensive
ablation studies showcase our benefits of combining sparsification and
multimodal learning over naive approaches. This paves the way for enabling
multimodal learning on low-resource devices.

    

### [[2111.11998] Appliance Level Short-term Load Forecasting via Recurrent Neural Network](http://arxiv.org/abs/2111.11998)


  Accurate load forecasting is critical for electricity market operations and
other real-time decision-making tasks in power systems. This paper considers
the short-term load forecasting (STLF) problem for residential customers within
a community. Existing STLF work mainly focuses on forecasting the aggregated
load for either a feeder system or a single customer, but few efforts have been
made on forecasting the load at individual appliance level. In this work, we
present an STLF algorithm for efficiently predicting the power consumption of
individual electrical appliances. The proposed method builds upon a powerful
recurrent neural network (RNN) architecture in deep learning, termed as long
short-term memory (LSTM). As each appliance has uniquely repetitive consumption
patterns, the patterns of prediction error will be tracked such that past
prediction errors can be used for improving the final prediction performance.
Numerical tests on real-world load datasets demonstrate the improvement of the
proposed method over existing LSTM-based method and other benchmark approaches.

    

### [[2111.12024] Adversarial Sampling for Solving Differential Equations with Neural Networks](http://arxiv.org/abs/2111.12024)


  Neural network-based methods for solving differential equations have been
gaining traction. They work by improving the differential equation residuals of
a neural network on a sample of points in each iteration. However, most of them
employ standard sampling schemes like uniform or perturbing equally spaced
points. We present a novel sampling scheme which samples points adversarially
to maximize the loss of the current solution estimate. A sampler architecture
is described along with the loss terms used for training. Finally, we
demonstrate that this scheme outperforms pre-existing schemes by comparing both
on a number of problems.

    

### [[2111.12034] Adversarial machine learning for protecting against online manipulation](http://arxiv.org/abs/2111.12034)


  Adversarial examples are inputs to a machine learning system that result in
an incorrect output from that system. Attacks launched through this type of
input can cause severe consequences: for example, in the field of image
recognition, a stop signal can be misclassified as a speed limit
indication.However, adversarial examples also represent the fuel for a flurry
of research directions in different domains and applications. Here, we give an
overview of how they can be profitably exploited as powerful tools to build
stronger learning models, capable of better-withstanding attacks, for two
crucial tasks: fake news and social bot detection.

    

### [[2111.12038] Learning Symbolic Rules for Reasoning in Quasi-Natural Language](http://arxiv.org/abs/2111.12038)


  Symbolic reasoning, rule-based symbol manipulation, is a hallmark of human
intelligence. However, rule-based systems have had limited success competing
with learning-based systems outside formalized domains such as automated
theorem proving. We hypothesize that this is due to the manual construction of
rules in past attempts. In this work, we ask how we can build a rule-based
system that can reason with natural language input but without the manual
construction of rules. We propose MetaQNL, a "Quasi-Natural" language that can
express both formal logic and natural language sentences, and MetaInduce, a
learning algorithm that induces MetaQNL rules from training data consisting of
questions and answers, with or without intermediate reasoning steps. Our
approach achieves state-of-the-art accuracy on multiple reasoning benchmarks;
it learns compact models with much less data and produces not only answers but
also checkable proofs. Further, experiments on a real-world morphological
analysis benchmark show that it is possible for our method to handle noise and
ambiguity. Code will be released at this https URL.

    

### [[2111.12045] Adaptive Multi-Goal Exploration](http://arxiv.org/abs/2111.12045)


  We introduce a generic strategy for provably efficient multi-goal
exploration. It relies on AdaGoal, a novel goal selection scheme that is based
on a simple constrained optimization problem, which adaptively targets goal
states that are neither too difficult nor too easy to reach according to the
agent's current knowledge. We show how AdaGoal can be used to tackle the
objective of learning an $\epsilon$-optimal goal-conditioned policy for all the
goal states that are reachable within $L$ steps in expectation from a reference
state $s_0$ in a reward-free Markov decision process. In the tabular case with
$S$ states and $A$ actions, our algorithm requires $\tilde{O}(L^3 S A
\epsilon^{-2})$ exploration steps, which is nearly minimax optimal. We also
readily instantiate AdaGoal in linear mixture Markov decision processes, which
yields the first goal-oriented PAC guarantee with linear function
approximation. Beyond its strong theoretical guarantees, AdaGoal is anchored in
the high-level algorithmic structure of existing methods for goal-conditioned
deep reinforcement learning.

    

### [[2111.12050] Simple Stochastic and Online Gradient DescentAlgorithms for Pairwise Learning](http://arxiv.org/abs/2111.12050)


  Pairwise learning refers to learning tasks where the loss function depends on
a pair of instances. It instantiates many important machine learning tasks such
as bipartite ranking and metric learning. A popular approach to handle
streaming data in pairwise learning is an online gradient descent (OGD)
algorithm, where one needs to pair the current instance with a buffering set of
previous instances with a sufficiently large size and therefore suffers from a
scalability issue. In this paper, we propose simple stochastic and online
gradient descent methods for pairwise learning. A notable difference from the
existing studies is that we only pair the current instance with the previous
one in building a gradient direction, which is efficient in both the storage
and computational complexity. We develop novel stability results, optimization,
and generalization error bounds for both convex and nonconvex as well as both
smooth and nonsmooth problems. We introduce novel techniques to decouple the
dependency of models and the previous instance in both the optimization and
generalization analysis. Our study resolves an open question on developing
meaningful generalization bounds for OGD using a buffering set with a very
small fixed size. We also extend our algorithms and stability analysis to
develop differentially private SGD algorithms for pairwise learning which
significantly improves the existing results.

    

### [[2111.12055] Generating GPU Compiler Heuristics using Reinforcement Learning](http://arxiv.org/abs/2111.12055)


  GPU compilers are complex software programs with many optimizations specific
to target hardware. These optimizations are often controlled by heuristics
hand-designed by compiler experts using time- and resource-intensive processes.
In this paper, we developed a GPU compiler autotuning framework that uses
off-policy deep reinforcement learning to generate heuristics that improve the
frame rates of graphics applications. Furthermore, we demonstrate the
resilience of these learned heuristics to frequent compiler updates by
analyzing their stability across a year of code check-ins without retraining.
We show that our machine learning-based compiler autotuning framework matches
or surpasses the frame rates for 98% of graphics benchmarks with an average
uplift of 1.6% up to 15.8%.

    

### [[2111.12056] Forget-SVGD: Particle-Based Bayesian Federated Unlearning](http://arxiv.org/abs/2111.12056)


  Variational particle-based Bayesian learning methods have the advantage of
not being limited by the bias affecting more conventional parametric
techniques. This paper proposes to leverage the flexibility of non-parametric
Bayesian approximate inference to develop a novel Bayesian federated unlearning
method, referred to as Forget-Stein Variational Gradient Descent (Forget-SVGD).
Forget-SVGD builds on SVGD - a particle-based approximate Bayesian inference
scheme using gradient-based deterministic updates - and on its distributed
(federated) extension known as Distributed SVGD (DSVGD). Upon the completion of
federated learning, as one or more participating agents request for their data
to be "forgotten", Forget-SVGD carries out local SVGD updates at the agents
whose data need to be "unlearned", which are interleaved with communication
rounds with a parameter server. The proposed method is validated via
performance comparisons with non-parametric schemes that train from scratch by
excluding data to be forgotten, as well as with existing parametric Bayesian
unlearning methods.

    

### [[2111.12062] DABS: A Domain-Agnostic Benchmark for Self-Supervised Learning](http://arxiv.org/abs/2111.12062)


  Self-supervised learning algorithms, including BERT and SimCLR, have enabled
significant strides in fields like natural language processing, computer
vision, and speech processing. However, these algorithms are domain-specific,
meaning that new self-supervised learning algorithms must be developed for each
new setting, including myriad healthcare, scientific, and multimodal domains.
To catalyze progress toward domain-agnostic methods, we introduce DABS: a
Domain-Agnostic Benchmark for Self-supervised learning. To perform well on
DABS, an algorithm is evaluated on seven diverse domains: natural images,
multichannel sensor data, English text, speech recordings, multilingual text,
chest x-rays, and images with text descriptions. Each domain contains an
unlabeled dataset for pretraining; the model is then is scored based on its
downstream performance on a set of labeled tasks in the domain. We also present
e-Mix and ShED: two baseline domain-agnostic algorithms; their relatively
modest performance demonstrates that significant progress is needed before
self-supervised learning is an out-of-the-box solution for arbitrary domains.
Code for benchmark datasets and baseline algorithms is available at
this https URL.

    

### [[2111.12066] Physics Informed Neural Networks for Control Oriented Thermal Modeling of Buildings](http://arxiv.org/abs/2111.12066)


  This paper presents a data-driven modeling approach for developing
control-oriented thermal models of buildings. These models are developed with
the objective of reducing energy consumption costs while controlling the indoor
temperature of the building within required comfort limits. To combine the
interpretability of white/gray box physics models and the expressive power of
neural networks, we propose a physics informed neural network approach for this
modeling task. Along with measured data and building parameters, we encode the
neural networks with the underlying physics that governs the thermal behavior
of these buildings. Thus, realizing a model that is guided by physics, aids in
modeling the temporal evolution of room temperature and power consumption as
well as the hidden state, i.e., the temperature of building thermal mass for
subsequent time steps. The main research contributions of this work are: (1) we
propose two variants of physics informed neural network architectures for the
task of control-oriented thermal modeling of buildings, (2) we show that
training these architectures is data-efficient, requiring less training data
compared to conventional, non-physics informed neural networks, and (3) we show
that these architectures achieve more accurate predictions than conventional
neural networks for longer prediction horizons. We test the prediction
performance of the proposed architectures using simulated and real-word data to
demonstrate (2) and (3) and show that the proposed physics informed neural
network architectures can be used for this control-oriented modeling problem.

    

### [[2111.12071] Minimizing subject-dependent calibration for BCI with Riemannian transfer learning](http://arxiv.org/abs/2111.12071)


  Calibration is still an important issue for user experience in Brain-Computer
Interfaces (BCI). Common experimental designs often involve a lengthy training
period that raises the cognitive fatigue, before even starting to use the BCI.
Reducing or suppressing this subject-dependent calibration is possible by
relying on advanced machine learning techniques, such as transfer learning.
Building on Riemannian BCI, we present a simple and effective scheme to train a
classifier on data recorded from different subjects, to reduce the calibration
while preserving good performances. The main novelty of this paper is to
propose a unique approach that could be applied on very different paradigms. To
demonstrate the robustness of this approach, we conducted a meta-analysis on
multiple datasets for three BCI paradigms: event-related potentials (P300),
motor imagery and SSVEP. Relying on the MOABB open source framework to ensure
the reproducibility of the experiments and the statistical analysis, the
results clearly show that the proposed approach could be applied on any kind of
BCI paradigm and in most of the cases to significantly improve the classifier
reliability. We point out some key features to further improve transfer
learning methods.

    

### [[2111.12083] VISTA 2.0: An Open, Data-driven Simulator for Multimodal Sensing and Policy Learning for Autonomous Vehicles](http://arxiv.org/abs/2111.12083)


  Simulation has the potential to transform the development of robust
algorithms for mobile agents deployed in safety-critical scenarios. However,
the poor photorealism and lack of diverse sensor modalities of existing
simulation engines remain key hurdles towards realizing this potential. Here,
we present VISTA, an open source, data-driven simulator that integrates
multiple types of sensors for autonomous vehicles. Using high fidelity,
real-world datasets, VISTA represents and simulates RGB cameras, 3D LiDAR, and
event-based cameras, enabling the rapid generation of novel viewpoints in
simulation and thereby enriching the data available for policy learning with
corner cases that are difficult to capture in the physical world. Using VISTA,
we demonstrate the ability to train and test perception-to-control policies
across each of the sensor types and showcase the power of this approach via
deployment on a full scale autonomous vehicle. The policies learned in VISTA
exhibit sim-to-real transfer without modification and greater robustness than
those trained exclusively on real-world data.

    

### [[1810.02244] Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks](http://arxiv.org/abs/1810.02244)


  In recent years, graph neural networks (GNNs) have emerged as a powerful
neural architecture to learn vector representations of nodes and graphs in a
supervised, end-to-end fashion. Up to now, GNNs have only been evaluated
empirically -- showing promising results. The following work investigates GNNs
from a theoretical point of view and relates them to the $1$-dimensional
Weisfeiler-Leman graph isomorphism heuristic ($1$-WL). We show that GNNs have
the same expressiveness as the $1$-WL in terms of distinguishing non-isomorphic
(sub-)graphs. Hence, both algorithms also have the same shortcomings. Based on
this, we propose a generalization of GNNs, so-called $k$-dimensional GNNs
($k$-GNNs), which can take higher-order graph structures at multiple scales
into account. These higher-order structures play an essential role in the
characterization of social networks and molecule graphs. Our experimental
evaluation confirms our theoretical findings as well as confirms that
higher-order information is useful in the task of graph classification and
regression.

    

### [[1905.10395] Leader Stochastic Gradient Descent for Distributed Training of Deep Learning Models: Extension](http://arxiv.org/abs/1905.10395)


  We consider distributed optimization under communication constraints for
training deep learning models. We propose a new algorithm, whose parameter
updates rely on two forces: a regular gradient step, and a corrective direction
dictated by the currently best-performing worker (leader). Our method differs
from the parameter-averaging scheme EASGD in a number of ways: (i) our
objective formulation does not change the location of stationary points
compared to the original optimization problem; (ii) we avoid convergence
decelerations caused by pulling local workers descending to different local
minima to each other (i.e. to the average of their parameters); (iii) our
update by design breaks the curse of symmetry (the phenomenon of being trapped
in poorly generalizing sub-optimal solutions in symmetric non-convex
landscapes); and (iv) our approach is more communication efficient since it
broadcasts only parameters of the leader rather than all workers. We provide
theoretical analysis of the batch version of the proposed algorithm, which we
call Leader Gradient Descent (LGD), and its stochastic variant (LSGD). Finally,
we implement an asynchronous version of our algorithm and extend it to the
multi-leader setting, where we form groups of workers, each represented by its
own local leader (the best performer in a group), and update each worker with a
corrective direction comprised of two attractive forces: one to the local, and
one to the global leader (the best performer among all workers). The
multi-leader setting is well-aligned with current hardware architecture, where
local workers forming a group lie within a single computational node and
different groups correspond to different nodes. For training convolutional
neural networks, we empirically demonstrate that our approach compares
favorably to state-of-the-art baselines.

    

### [[1907.09064] Performance-Complexity Tradeoffs in Greedy Weak Submodular Maximization with Random Sampling](http://arxiv.org/abs/1907.09064)


  Many problems in signal processing and machine learning can be formalized as
weak submodular optimization tasks. For such problems, a simple greedy
algorithm (\textsc{Greedy}) is guaranteed to find a solution achieving the
objective with a value no worse than $1-e^{-1/c}$ of the optimal, where $c$ is
the multiplicative weak-submodularity constant. Due to the high cost of
querying large-scale systems, the complexity of \textsc{Greedy} becomes
prohibitive in contemporary applications. In this work, we study the tradeoff
between performance and complexity when one resorts to random sampling
strategies to reduce the query complexity of \textsc{Greedy}. Specifically, we
quantify the effect of uniform sampling strategies on \textsc{Greedy}'s
performance through two metrics: (i) probability of identifying an optimal
subset, and (ii) suboptimality with respect to the optimal solution. The latter
implies that uniform sampling strategies with a fixed sampling size achieve a
non-trivial approximation factor; however, we show that with overwhelming
probability, these methods fail to find the optimal subset. Our analysis shows
that the failure of uniform sampling strategies with fixed sample size can be
circumvented by successively increasing the size of the search space. Building
upon this insight, we propose a simple progressive stochastic greedy algorithm
and study its approximation guarantees. Moreover, we demonstrate effectiveness
of the proposed method in dimensionality reduction applications and feature
selection tasks for clustering and object tracking.

    

### [[2001.09598] FakeLocator: Robust Localization of GAN-Based Face Manipulations](http://arxiv.org/abs/2001.09598)


  Full face synthesis and partial face manipulation by virtue of the generative
adversarial networks (GANs) and its variants have raised wide public concerns.
In the multi-media forensics area, detecting and ultimately locating the image
forgery has become an imperative task. In this work, we investigate the
architecture of existing GAN-based face manipulation methods and observe that
the imperfection of upsampling methods therewithin could be served as an
important asset for GAN-synthesized fake image detection and forgery
localization. Based on this basic observation, we have proposed a novel
approach, termed FakeLocator, to obtain high localization accuracy, at full
resolution, on manipulated facial images. To the best of our knowledge, this is
the very first attempt to solve the GAN-based fake localization problem with a
gray-scale fakeness map that preserves more information of fake regions. To
improve the universality of FakeLocator across multifarious facial attributes,
we introduce an attention mechanism to guide the training of the model. To
improve the universality of FakeLocator across different DeepFake methods, we
propose partial data augmentation and single sample clustering on the training
images. Experimental results on popular FaceForensics++, DFFD datasets and
seven different state-of-the-art GAN-based face generation methods have shown
the effectiveness of our method. Compared with the baselines, our method
performs better on various metrics. Moreover, the proposed method is robust
against various real-world facial image degradations such as JPEG compression,
low-resolution, noise, and blur.

    

### [[2003.02929] Flexible Bayesian Nonlinear Model Configuration](http://arxiv.org/abs/2003.02929)


  Regression models are used in a wide range of applications providing a
powerful scientific tool for researchers from different fields. Linear, or
simple parametric, models are often not sufficient to describe complex
relationships between input variables and a response. Such relationships can be
better described through flexible approaches such as neural networks, but this
results in less interpretable models and potential overfitting. Alternatively,
specific parametric nonlinear functions can be used, but the specification of
such functions is in general complicated. In this paper, we introduce a
flexible approach for the construction and selection of highly flexible
nonlinear parametric regression models. Nonlinear features are generated
hierarchically, similarly to deep learning, but have additional flexibility on
the possible types of features to be considered. This flexibility, combined
with variable selection, allows us to find a small set of important features
and thereby more interpretable models. Within the space of possible functions,
a Bayesian approach, introducing priors for functions based on their
complexity, is considered. A genetically modified mode jumping Markov chain
Monte Carlo algorithm is adopted to perform Bayesian inference and estimate
posterior probabilities for model averaging. In various applications, we
illustrate how our approach is used to obtain meaningful nonlinear models.
Additionally, we compare its predictive performance with several machine
learning algorithms.

    

### [[2004.01570] A New Method to Compare the Interpretability of Rule-based Algorithms](http://arxiv.org/abs/2004.01570)


  Interpretability is becoming increasingly important for predictive model
analysis. Unfortunately, as remarked by many authors, there is still no
consensus regarding this notion. The goal of this paper is to propose the
definition of a score that allows to quickly compare interpretable algorithms.
This definition consists of three terms, each one being quantitatively measured
with a simple formula: predictivity, stability and simplicity. While
predictivity has been extensively studied to measure the accuracy of predictive
algorithms, stability is based on the Dice-Sorensen index for comparing two
rule sets generated by an algorithm using two independent samples. The
simplicity is based on the sum of the lengths of the rules derived from the
predictive model. The proposed score is a weighted sum of the three terms
mentioned above. We use this score to compare the interpretability of a set of
rule-based algorithms and tree-based algorithms for the regression case and for
the classification case.

    

### [[2006.02554] Generalized Penalty for Circular Coordinate Representation](http://arxiv.org/abs/2006.02554)


  Topological Data Analysis (TDA) provides novel approaches that allow us to
analyze the geometrical shapes and topological structures of a dataset. As one
important application, TDA can be used for data visualization and dimension
reduction. We follow the framework of circular coordinate representation, which
allows us to perform dimension reduction and visualization for high-dimensional
datasets on a torus using persistent cohomology. In this paper, we propose a
method to adapt the circular coordinate framework to take into account the
roughness of circular coordinates in change-point and high-dimensional
applications. We use a generalized penalty function instead of an $L_{2}$
penalty in the traditional circular coordinate algorithm. We provide simulation
experiments and real data analysis to support our claim that circular
coordinates with generalized penalty will detect the change in high-dimensional
datasets under different sampling schemes while preserving the topological
structures.

    

### [[2008.10526] Stochastic Multi-level Composition Optimization Algorithms with Level-Independent Convergence Rates](http://arxiv.org/abs/2008.10526)


  In this paper, we study smooth stochastic multi-level composition
optimization problems, where the objective function is a nested composition of
$T$ functions. We assume access to noisy evaluations of the functions and their
gradients, through a stochastic first-order oracle. For solving this class of
problems, we propose two algorithms using moving-average stochastic estimates,
and analyze their convergence to an $\epsilon$-stationary point of the problem.
We show that the first algorithm, which is a generalization of
\cite{GhaRuswan20} to the $T$ level case, can achieve a sample complexity of
$\mathcal{O}(1/\epsilon^6)$ by using mini-batches of samples in each iteration.
By modifying this algorithm using linearized stochastic estimates of the
function values, we improve the sample complexity to
$\mathcal{O}(1/\epsilon^4)$. {\color{black}This modification not only removes
the requirement of having a mini-batch of samples in each iteration, but also
makes the algorithm parameter-free and easy to implement}. To the best of our
knowledge, this is the first time that such an online algorithm designed for
the (un)constrained multi-level setting, obtains the same sample complexity of
the smooth single-level setting, under standard assumptions (unbiasedness and
boundedness of the second moments) on the stochastic first-order oracle.

    

### [[2009.00606] Semi-Supervised Empirical Risk Minimization: Using unlabeled data to improve prediction](http://arxiv.org/abs/2009.00606)


  We present a general methodology for using unlabeled data to design semi
supervised learning (SSL) variants of the Empirical Risk Minimization (ERM)
learning process. Focusing on generalized linear regression, we analyze of the
effectiveness of our SSL approach in improving prediction performance. The key
ideas are carefully considering the null model as a competitor, and utilizing
the unlabeled data to determine signal-noise combinations where SSL outperforms
both supervised learning and the null model. We then use SSL in an adaptive
manner based on estimation of the signal and noise. In the special case of
linear regression with Gaussian covariates, we prove that the non-adaptive SSL
version is in fact not capable of improving on both the supervised estimator
and the null model simultaneously, beyond a negligible O(1/n) term. On the
other hand, the adaptive model presented in this work, can achieve a
substantial improvement over both competitors simultaneously, under a variety
of settings. This is shown empirically through extensive simulations, and
extended to other scenarios, such as non-Gaussian covariates, misspecified
linear regression, or generalized linear regression with non-linear link
functions.

    

### [[2011.00159] Learning Strategies in Decentralized Matching Markets under Uncertain Preferences](http://arxiv.org/abs/2011.00159)


  We study the problem of decision-making in the setting of a scarcity of
shared resources when the preferences of agents are unknown a priori and must
be learned from data. Taking the two-sided matching market as a running
example, we focus on the decentralized setting, where agents do not share their
learned preferences with a central authority. Our approach is based on the
representation of preferences in a reproducing kernel Hilbert space, and a
learning algorithm for preferences that accounts for uncertainty due to the
competition among the agents in the market. Under regularity conditions, we
show that our estimator of preferences converges at a minimax optimal rate.
Given this result, we derive optimal strategies that maximize agents' expected
payoffs and we calibrate the uncertain state by taking opportunity costs into
account. We also derive an incentive-compatibility property and show that the
outcome from the learned strategies has a stability property. Finally, we prove
a fairness property that asserts that there exists no justified envy according
to the learned strategies.

    

### [[2102.07115] Sliced Multi-Marginal Optimal Transport](http://arxiv.org/abs/2102.07115)


  Multi-marginal optimal transport enables one to compare multiple probability
measures, which increasingly finds application in multi-task learning problems.
One practical limitation of multi-marginal transport is computational
scalability in the number of measures, samples and dimensionality. In this
work, we propose a multi-marginal optimal transport paradigm based on random
one-dimensional projections, whose (generalized) distance we term the sliced
multi-marginal Wasserstein distance. To construct this distance, we introduce a
characterization of the one-dimensional multi-marginal Kantorovich problem and
use it to highlight a number of properties of the sliced multi-marginal
Wasserstein distance. In particular, we show that (i) the sliced multi-marginal
Wasserstein distance is a (generalized) metric that induces the same topology
as the standard Wasserstein distance, (ii) it admits a dimension-free sample
complexity, (iii) it is tightly connected with the problem of barycentric
averaging under the sliced-Wasserstein metric. We conclude by illustrating the
sliced multi-marginal Wasserstein on multi-task density estimation and
multi-dynamics reinforcement learning problems.

    

### [[2102.10362] Factored Policy Gradients: Leveraging Structure for Efficient Learning in MOMDPs](http://arxiv.org/abs/2102.10362)


  Policy gradient methods can solve complex tasks but often fail when the
dimensionality of the action-space or objective multiplicity grow very large.
This occurs, in part, because the variance on score-based gradient estimators
scales quadratically. In this paper, we address this problem through a factor
baseline which exploits independence structure encoded in a novel action-target
influence network. Factored policy gradients (FPGs), which follow, provide a
common framework for analysing key state-of-the-art algorithms, are shown to
generalise traditional policy gradients, and yield a principled way of
incorporating prior knowledge of a problem domain's generative processes. We
provide an analysis of the proposed estimator and identify the conditions under
which variance is reduced. The algorithmic aspects of FPGs are discussed,
including optimal policy factorisation, as characterised by minimum biclique
coverings, and the implications for the bias-variance trade-off of incorrectly
specifying the network. Finally, we demonstrate the performance advantages of
our algorithm on large-scale bandit and traffic intersection problems,
providing a novel contribution to the latter in the form of a spatial
approximation.

    

### [[2103.00484] Deepfakes Generation and Detection: State-of-the-art, open challenges, countermeasures, and way forward](http://arxiv.org/abs/2103.00484)


  Easy access to audio-visual content on social media, combined with the
availability of modern tools such as Tensorflow or Keras, open-source trained
models, and economical computing infrastructure, and the rapid evolution of
deep-learning (DL) methods, especially Generative Adversarial Networks (GAN),
have made it possible to generate deepfakes to disseminate disinformation,
revenge porn, financial frauds, hoaxes, and to disrupt government functioning.
The existing surveys have mainly focused on the detection of deepfake images
and videos. This paper provides a comprehensive review and detailed analysis of
existing tools and machine learning (ML) based approaches for deepfake
generation and the methodologies used to detect such manipulations for both
audio and visual deepfakes. For each category of deepfake, we discuss
information related to manipulation approaches, current public datasets, and
key standards for the performance evaluation of deepfake detection techniques
along with their results. Additionally, we also discuss open challenges and
enumerate future directions to guide future researchers on issues that need to
be considered to improve the domains of both deepfake generation and detection.
This work is expected to assist the readers in understanding the creation and
detection mechanisms of deepfakes, along with their current limitations and
future direction.

    

### [[2103.11774] Clustered Hierarchical Anomaly and Outlier Detection Algorithms](http://arxiv.org/abs/2103.11774)


  Anomaly and outlier detection is a long-standing problem in machine learning.
In some cases, anomaly detection is easy, such as when data are drawn from
well-characterized distributions such as the Gaussian. However, when data
occupy high-dimensional spaces, anomaly detection becomes more difficult. We
present CLAM (Clustered Learning of Approximate Manifolds), a manifold mapping
technique in any metric space. CLAM begins with a fast hierarchical clustering
technique and then induces a graph from the cluster tree, based on overlapping
clusters as selected using several geometric and topological features. Using
these graphs, we implement CHAODA (Clustered Hierarchical Anomaly and Outlier
Detection Algorithms), exploring various properties of the graphs and their
constituent clusters to find outliers. CHAODA employs a form of transfer
learning based on a training set of datasets, and applies this knowledge to a
separate test set of datasets of different cardinalities, dimensionalities, and
domains. On 24 publicly available datasets, we compare CHAODA (by measure of
ROC AUC) to a variety of state-of-the-art unsupervised anomaly-detection
algorithms. Six of the datasets are used for training. CHAODA outperforms other
approaches on 16 of the remaining 18 datasets. CLAM and CHAODA scale to large,
high-dimensional "big data" anomaly-detection problems, and generalize across
datasets and distance functions. Source code to CLAM and CHAODA are freely
available on GitHub at this https URL.

    

### [[2104.06486] MS2: Multi-Document Summarization of Medical Studies](http://arxiv.org/abs/2104.06486)


  To assess the effectiveness of any medical intervention, researchers must
conduct a time-intensive and highly manual literature review. NLP systems can
help to automate or assist in parts of this expensive process. In support of
this goal, we release MS^2 (Multi-Document Summarization of Medical Studies), a
dataset of over 470k documents and 20k summaries derived from the scientific
literature. This dataset facilitates the development of systems that can assess
and aggregate contradictory evidence across multiple studies, and is the first
large-scale, publicly available multi-document summarization dataset in the
biomedical domain. We experiment with a summarization system based on BART,
with promising early results. We formulate our summarization inputs and targets
in both free text and structured forms and modify a recently proposed metric to
assess the quality of our system's generated summaries. Data and models are
available at this https URL


### [[2104.14526] Scaling and Scalability: Provable Nonconvex Low-Rank Tensor Estimation from Incomplete Measurements](http://arxiv.org/abs/2104.14526)


  Tensors, which provide a powerful and flexible model for representing
multi-attribute data and multi-way interactions, play an indispensable role in
modern data science across various fields in science and engineering. A
fundamental task is to faithfully recover the tensor from highly incomplete
measurements in a statistically and computationally efficient manner.
Harnessing the low-rank structure of tensors in the Tucker decomposition, this
paper develops a scaled gradient descent (ScaledGD) algorithm to directly
recover the tensor factors with tailored spectral initializations, and shows
that it provably converges at a linear rate independent of the condition number
of the ground truth tensor for two canonical problems -- tensor completion and
tensor regression -- as soon as the sample size is above the order of $n^{3/2}$
ignoring other parameter dependencies, where $n$ is the dimension of the
tensor. This leads to an extremely scalable approach to low-rank tensor
estimation compared with prior art, which suffers from at least one of the
following drawbacks: extreme sensitivity to ill-conditioning, high
per-iteration costs in terms of memory and computation, or poor sample
complexity guarantees. To the best of our knowledge, ScaledGD is the first
algorithm that achieves near-optimal statistical and computational complexities
simultaneously for low-rank tensor completion with the Tucker decomposition.
Our algorithm highlights the power of appropriate preconditioning in
accelerating nonconvex statistical estimation, where the iteration-varying
preconditioners promote desirable invariance properties of the trajectory with
respect to the underlying symmetry in low-rank tensor factorization.

    

### [[2105.00899] Fully Learnable Deep Wavelet Transform for Unsupervised Monitoring of High-Frequency Time Series](http://arxiv.org/abs/2105.00899)


  High-Frequency (HF) signals are ubiquitous in the industrial world and are of
great use for monitoring of industrial assets. Most deep learning tools are
designed for inputs of fixed and/or very limited size and many successful
applications of deep learning to the industrial context use as inputs extracted
features, which is a manually and often arduously obtained compact
representation of the original signal. In this paper, we propose a fully
unsupervised deep learning framework that is able to extract a meaningful and
sparse representation of raw HF signals. We embed in our architecture important
properties of the fast discrete wavelet transformation (FDWT) such as (1) the
cascade algorithm, (2) the conjugate quadrature filter property that links
together the wavelet, the scaling and transposed filter functions, and (3) the
coefficient denoising. Using deep learning, we make this architecture fully
learnable: both the wavelet bases and the wavelet coefficient denoising are
learnable. To achieve this objective, we propose a new activation function that
performs a learnable hard-thresholding of the wavelet coefficients. With our
framework, the denoising FDWT becomes a fully learnable unsupervised tool that
does neither require any type of pre- nor post-processing, nor any prior
knowledge on wavelet transform. We demonstrate the benefits of embedding all
these properties on three machine-learning tasks performed on open source sound
datasets. We perform an ablation study of the impact of each property on the
performance of the architecture, achieve results well above baseline and
outperform other state-of-the-art methods.

    

### [[2105.14594] Sparse Uncertainty Representation in Deep Learning with Inducing Weights](http://arxiv.org/abs/2105.14594)


  Bayesian neural networks and deep ensembles represent two modern paradigms of
uncertainty quantification in deep learning. Yet these approaches struggle to
scale mainly due to memory inefficiency issues, since they require parameter
storage several times higher than their deterministic counterparts. To address
this, we augment the weight matrix of each layer with a small number of
inducing weights, thereby projecting the uncertainty quantification into such
low dimensional spaces. We further extend Matheron's conditional Gaussian
sampling rule to enable fast weight sampling, which enables our inference
method to maintain reasonable run-time as compared with ensembles. Importantly,
our approach achieves competitive performance to the state-of-the-art in
prediction and uncertainty estimation tasks with fully connected neural
networks and ResNets, while reducing the parameter size to $\leq 24.3\%$ of
that of a $single$ neural network.

    

### [[2106.01770] Bayesian Classifier Fusion with an Explicit Model of Correlation](http://arxiv.org/abs/2106.01770)


  Combining the outputs of multiple classifiers or experts into a single
probabilistic classification is a fundamental task in machine learning with
broad applications from classifier fusion to expert opinion pooling. Here we
present a hierarchical Bayesian model of probabilistic classifier fusion based
on a new correlated Dirichlet distribution. This distribution explicitly models
positive correlations between marginally Dirichlet-distributed random vectors
thereby allowing explicit modeling of correlations between base classifiers or
experts. The proposed model naturally accommodates the classic Independent
Opinion Pool and other independent fusion algorithms as special cases. It is
evaluated by uncertainty reduction and correctness of fusion on synthetic and
real-world data sets. We show that a change in performance of the fused
classifier due to uncertainty reduction can be Bayes optimal even for highly
correlated base classifiers.

    

### [[2106.03253] Tabular Data: Deep Learning is Not All You Need](http://arxiv.org/abs/2106.03253)


  A key element in solving real-life data science problems is selecting the
types of models to use. Tree ensemble models (such as XGBoost) are usually
recommended for classification and regression problems with tabular data.
However, several deep learning models for tabular data have recently been
proposed, claiming to outperform XGBoost for some use cases. This paper
explores whether these deep models should be a recommended option for tabular
data by rigorously comparing the new deep models to XGBoost on various
datasets. In addition to systematically comparing their performance, we
consider the tuning and computation they require. Our study shows that XGBoost
outperforms these deep models across the datasets, including the datasets used
in the papers that proposed the deep models. We also demonstrate that XGBoost
requires much less tuning. On the positive side, we show that an ensemble of
deep models and XGBoost performs better on these datasets than XGBoost alone.

    

### [[2106.04727] ParChain: A Framework for Parallel Hierarchical Agglomerative Clustering using Nearest-Neighbor Chain](http://arxiv.org/abs/2106.04727)


  This paper studies the hierarchical clustering problem, where the goal is to
produce a dendrogram that represents clusters at varying scales of a data set.
We propose the ParChain framework for designing parallel hierarchical
agglomerative clustering (HAC) algorithms, and using the framework we obtain
novel parallel algorithms for the complete linkage, average linkage, and Ward's
linkage criteria. Compared to most previous parallel HAC algorithms, which
require quadratic memory, our new algorithms require only linear memory, and
are scalable to large data sets. ParChain is based on our parallelization of
the nearest-neighbor chain algorithm, and enables multiple clusters to be
merged on every round. We introduce two key optimizations that are critical for
efficiency: a range query optimization that reduces the number of distance
computations required when finding nearest neighbors of clusters, and a caching
optimization that stores a subset of previously computed distances, which are
likely to be reused.
Experimentally, we show that our highly-optimized implementations using 48
cores with two-way hyper-threading achieve 5.8--110.1x speedup over
state-of-the-art parallel HAC algorithms and achieve 13.75--54.23x
self-relative speedup. Compared to state-of-the-art algorithms, our algorithms
require up to 237.3x less space. Our algorithms are able to scale to data set
sizes with tens of millions of points, which existing algorithms are not able
to handle.

    

### [[2106.05234] Do Transformers Really Perform Bad for Graph Representation?](http://arxiv.org/abs/2106.05234)


  The Transformer architecture has become a dominant choice in many domains,
such as natural language processing and computer vision. Yet, it has not
achieved competitive performance on popular leaderboards of graph-level
prediction compared to mainstream GNN variants. Therefore, it remains a mystery
how Transformers could perform well for graph representation learning. In this
paper, we solve this mystery by presenting Graphormer, which is built upon the
standard Transformer architecture, and could attain excellent results on a
broad range of graph representation learning tasks, especially on the recent
OGB Large-Scale Challenge. Our key insight to utilizing Transformer in the
graph is the necessity of effectively encoding the structural information of a
graph into the model. To this end, we propose several simple yet effective
structural encoding methods to help Graphormer better model graph-structured
data. Besides, we mathematically characterize the expressive power of
Graphormer and exhibit that with our ways of encoding the structural
information of graphs, many popular GNN variants could be covered as the
special cases of Graphormer.

    

### [[2106.05964] Fair Classification with Adversarial Perturbations](http://arxiv.org/abs/2106.05964)


  We study fair classification in the presence of an omniscient adversary that,
given an $\eta$, is allowed to choose an arbitrary $\eta$-fraction of the
training samples and arbitrarily perturb their protected attributes. The
motivation comes from settings in which protected attributes can be incorrect
due to strategic misreporting, malicious actors, or errors in imputation; and
prior approaches that make stochastic or independence assumptions on errors may
not satisfy their guarantees in this adversarial setting. Our main contribution
is an optimization framework to learn fair classifiers in this adversarial
setting that comes with provable guarantees on accuracy and fairness. Our
framework works with multiple and non-binary protected attributes, is designed
for the large class of linear-fractional fairness metrics, and can also handle
perturbations besides protected attributes. We prove near-tightness of our
framework's guarantees for natural hypothesis classes: no algorithm can have
significantly better accuracy and any algorithm with better fairness must have
lower accuracy. Empirically, we evaluate the classifiers produced by our
framework for statistical rate on real-world and synthetic datasets for a
family of adversaries.

    

### [[2106.09913] Iterative Feature Matching: Toward Provable Domain Generalization with Logarithmic Environments](http://arxiv.org/abs/2106.09913)


  Domain generalization aims at performing well on unseen test environments
with data from a limited number of training environments. Despite a
proliferation of proposal algorithms for this task, assessing their performance
both theoretically and empirically is still very challenging. Distributional
matching algorithms such as (Conditional) Domain Adversarial Networks [Ganin et
al., 2016, Long et al., 2018] are popular and enjoy empirical success, but they
lack formal guarantees. Other approaches such as Invariant Risk Minimization
(IRM) require a prohibitively large number of training environments -- linear
in the dimension of the spurious feature space $d_s$ -- even on simple data
models like the one proposed by [Rosenfeld et al., 2021]. Under a variant of
this model, we show that both ERM and IRM cannot generalize with $o(d_s)$
environments. We then present an iterative feature matching algorithm that is
guaranteed with high probability to yield a predictor that generalizes after
seeing only $O(\log d_s)$ environments. Our results provide the first
theoretical justification for a family of distribution-matching algorithms
widely used in practice under a concrete nontrivial data model.

    

### [[2111.07737] Progress in Self-Certified Neural Networks](http://arxiv.org/abs/2111.07737)


  A learning method is self-certified if it uses all available data to
simultaneously learn a predictor and certify its quality with a tight
statistical certificate that is valid on unseen data. Recent work has shown
that neural network models trained by optimising PAC-Bayes bounds lead not only
to accurate predictors, but also to tight risk certificates, bearing promise
towards achieving self-certified learning. In this context, learning and
certification strategies based on PAC-Bayes bounds are especially attractive
due to their ability to leverage all data to learn a posterior and
simultaneously certify its risk with a tight numerical certificate. In this
paper, we assess the progress towards self-certification in probabilistic
neural networks learnt by PAC-Bayes inspired objectives. We empirically compare
(on 4 classification datasets) classical test set bounds for deterministic
predictors and a PAC-Bayes bound for randomised self-certified predictors. We
first show that both of these generalisation bounds are not too far from
out-of-sample test set errors. We then show that in data starvation regimes,
holding out data for the test set bounds adversely affects generalisation
performance, while self-certified strategies based on PAC-Bayes bounds do not
suffer from this drawback, proving that they might be a suitable choice for the
small data regime. We also find that probabilistic neural networks learnt by
PAC-Bayes inspired objectives lead to certificates that can be surprisingly
competitive with commonly used test set bounds.

    

### [[2111.11133] L-Verse: Bidirectional Generation Between Image and Text](http://arxiv.org/abs/2111.11133)


  Far beyond learning long-range interactions of natural language, transformers
are becoming the de-facto standard for many vision tasks with their power and
scalabilty. Especially with cross-modal tasks between image and text, vector
quantized variational autoencoders (VQ-VAEs) are widely used to make a raw RGB
image into a sequence of feature vectors. To better leverage the correlation
between image and text, we propose L-Verse, a novel architecture consisting of
feature-augmented variational autoencoder (AugVAE) and bidirectional
auto-regressive transformer (BiART) for text-to-image and image-to-text
generation. Our AugVAE shows the state-of-the-art reconstruction performance on
ImageNet1K validation set, along with the robustness to unseen images in the
wild. Unlike other models, BiART can distinguish between image (or text) as a
conditional reference and a generation target. L-Verse can be directly used for
image-to-text or text-to-image generation tasks without any finetuning or extra
object detection frameworks. In quantitative and qualitative experiments,
L-Verse shows impressive results against previous methods in both image-to-text
and text-to-image generation on MS-COCO Captions. We furthermore assess the
scalability of L-Verse architecture on Conceptual Captions and present the
initial results of bidirectional vision-language representation learning on
general domain. Codes available at: this https URL


### [[2111.11236] Nanorobot queue: Cooperative treatment of cancer based on team member communication and image processing](http://arxiv.org/abs/2111.11236)


  Although nanorobots have been used as clinical prescriptions for work such as
gastroscopy, and even photoacoustic tomography technology has been proposed to
control nanorobots to deliver drugs at designated delivery points in real time,
and there are cases of eliminating "superbacteria" in blood through nanorobots,
most technologies are immature, either with low efficiency or low accuracy,
Either it can not be mass produced, so the most effective way to treat cancer
diseases at this stage is through chemotherapy and radiotherapy. Patients are
suffering and can not be cured. Therefore, this paper proposes an ideal model
of a treatment method that can completely cure cancer, a cooperative treatment
method based on nano robot queue through team member communication and computer
vision image classification (target detection).

    

### [[2111.11406] Anomaly-resistant Graph Neural Networks via Neural Architecture Search](http://arxiv.org/abs/2111.11406)


  In general, Graph Neural Networks(GNN) have been using a message passing
method to aggregate and summarize information about neighbors to express their
information. Nonetheless, previous studies have shown that the performance of
graph neural networks becomes vulnerable when there are abnormal nodes in the
neighborhood due to this message passing method. In this paper, inspired by the
Neural Architecture Search method, we present an algorithm that recognizes
abnormal nodes and automatically excludes them from information aggregation.
Experiments on various real worlds datasets show that our proposed Neural
Architecture Search-based Anomaly Resistance Graph Neural Network (NASAR-GNN)
is actually effective.

    

### [[2102.05167] Scheduling the NASA Deep Space Network with Deep Reinforcement Learning](http://arxiv.org/abs/2102.05167)


  With three complexes spread evenly across the Earth, NASA's Deep Space
Network (DSN) is the primary means of communications as well as a significant
scientific instrument for dozens of active missions around the world. A rapidly
rising number of spacecraft and increasingly complex scientific instruments
with higher bandwidth requirements have resulted in demand that exceeds the
network's capacity across its 12 antennae. The existing DSN scheduling process
operates on a rolling weekly basis and is time-consuming; for a given week,
generation of the final baseline schedule of spacecraft tracking passes takes
roughly 5 months from the initial requirements submission deadline, with
several weeks of peer-to-peer negotiations in between. This paper proposes a
deep reinforcement learning (RL) approach to generate candidate DSN schedules
from mission requests and spacecraft ephemeris data with demonstrated
capability to address real-world operational constraints. A deep RL agent is
developed that takes mission requests for a given week as input, and interacts
with a DSN scheduling environment to allocate tracks such that its reward
signal is maximized. A comparison is made between an agent trained using
Proximal Policy Optimization and its random, untrained counterpart. The results
represent a proof-of-concept that, given a well-shaped reward signal, a deep RL
agent can learn the complex heuristics used by experts to schedule the DSN. A
trained agent can potentially be used to generate candidate schedules to
bootstrap the scheduling process and thus reduce the turnaround cycle for DSN
scheduling.

    

### [[2111.11744] A Customized NoC Architecture to Enable Highly Localized Computing-On-the-Move DNN Dataflow](http://arxiv.org/abs/2111.11744)


  The ever-increasing computation complexity of fastgrowing Deep Neural
Networks (DNNs) has requested new computing paradigms to overcome the memory
wall in conventional Von Neumann computing architectures. The emerging
Computing-In-Memory (CIM) architecture has been a promising candidate to
accelerate neural network computing. However, data movement between CIM arrays
may still dominate the total power consumption in conventional designs. This
paper proposes a flexible CIM processor architecture named Domino and
"Computing-On-the-Move" (COM) dataflow, to enable stream computing and local
data access to significantly reduce data movement energy. Meanwhile, Domino
employs customized distributed instruction scheduling within Network-on-Chip
(NoC) to implement inter-memory computing and attain mapping flexibility. The
evaluation with prevailing DNN models shows that Domino achieves
1.77-to-2.37$\times$ power efficiency over several state-of-the-art CIM
accelerators and improves the throughput by 1.28-to-13.16$\times$.

    

### [[2111.11838] Design of Many-Core Big Little μBrain for Energy-Efficient Embedded Neuromorphic Computing](http://arxiv.org/abs/2111.11838)


  As spiking-based deep learning inference applications are increasing in
embedded systems, these systems tend to integrate neuromorphic accelerators
such as $\mu$Brain to improve energy efficiency. We propose a $\mu$Brain-based
scalable many-core neuromorphic hardware design to accelerate the computations
of spiking deep convolutional neural networks (SDCNNs). To increase energy
efficiency, cores are designed to be heterogeneous in terms of their neuron and
synapse capacity (big cores have higher capacity than the little ones), and
they are interconnected using a parallel segmented bus interconnect, which
leads to lower latency and energy compared to a traditional mesh-based
Network-on-Chip (NoC). We propose a system software framework called SentryOS
to map SDCNN inference applications to the proposed design. SentryOS consists
of a compiler and a run-time manager. The compiler compiles an SDCNN
application into subnetworks by exploiting the internal architecture of big and
little $\mu$Brain cores. The run-time manager schedules these sub-networks onto
cores and pipeline their execution to improve throughput. We evaluate the
proposed big little many-core neuromorphic design and the system software
framework with five commonlyused SDCNN inference applications and show that the
proposed solution reduces energy (between 37% and 98%), reduces latency
(between 9% and 25%), and increases application throughput (between 20% and
36%). We also show that SentryOS can be easily extended for other spiking
neuromorphic accelerators.

    

### [[2111.11460] On the Local Communication Complexity of Counting and Modular Arithmetic](http://arxiv.org/abs/2111.11460)


  In standard number-in-hand multi-party communication complexity, performance
is measured as the total number of bits transmitted globally in the network. In
this paper, we study a variation called local communication complexity in which
performance instead measures the maximum number of bits sent or received at any
one player. We focus on a simple model where $n$ players, each with one input
bit, execute a protocol by exchanging messages to compute a function on the $n$
input bits. We ask what can and cannot be solved with a small local
communication complexity in this setting. We begin by establishing a
non-trivial lower bound on the local complexity for a specific function by
proving that counting the number of $1$'s among the first $17$ input bits
distributed among the participants requires a local complexity strictly greater
than $1$. We further investigate whether harder counting problems of this type
can yield stronger lower bounds, providing a largely negative answer by showing
that constant local complexity is sufficient to count the number $1$ bits over
the entire input, and therefore compute any symmetric function. In addition to
counting, we show that both sorting and searching can be computed in constant
local complexity. We then use the counting solution as a subroutine to
demonstrate that constant local complexity is also sufficient to compute many
standard modular arithmetic operations on two operands, including: comparisons,
addition, subtraction, multiplication, division, and exponentiation. Finally we
establish that function $GCD(x,y)$ where $x$ and $y$ are in the range $[1,n]$
has local complexity of $O(1)$. Our work highlights both new techniques for
proving lower bounds on this metric and the power of even a small amount of
local communication.

    

### [[2111.11562] Reliable Actors with Retry Orchestration](http://arxiv.org/abs/2111.11562)


  Enterprise cloud developers have to build applications that are resilient to
failures and interruptions. We advocate for, formalize, implement, and evaluate
a simple, albeit effective, fault-tolerant programming model for the cloud
based on actors, reliable message delivery, and retry orchestration. Our model
guarantees that (1) failed actor invocations are retried until success, (2) in
a distributed chain of invocations only the last one may be retried, (3)
pending synchronous invocations with a failed caller are automatically
cancelled. These guarantees make it possible to productively develop
fault-tolerant distributed applications ranging from classic problems of
concurrency theory to complex enterprise applications. Built as a service mesh,
our runtime system can interface application components written in any
programming language and scale with the application. We measure overhead
relative to reliable message queues. Using an application inspired by a typical
enterprise scenario, we assess fault tolerance and the impact of fault recovery
on application performance.

    

### [[2111.11682] Locality Sensitive Hash Aggregated Nonlinear Neighbourhood Matrix Factorization for Online Sparse Big Data Analysis](http://arxiv.org/abs/2111.11682)


  Matrix factorization (MF) can extract the low-rank features and integrate the
information of the data manifold distribution from high-dimensional data, which
can consider the nonlinear neighbourhood information. Thus, MF has drawn wide
attention for low-rank analysis of sparse big data, e.g., Collaborative
Filtering (CF) Recommender Systems, Social Networks, and Quality of Service.
However, the following two problems exist: 1) huge computational overhead for
the construction of the Graph Similarity Matrix (GSM), and 2) huge memory
overhead for the intermediate GSM. Therefore, GSM-based MF, e.g., kernel MF,
graph regularized MF, etc., cannot be directly applied to the low-rank analysis
of sparse big data on cloud and edge platforms. To solve this intractable
problem for sparse big data analysis, we propose Locality Sensitive Hashing
(LSH) aggregated MF (LSH-MF), which can solve the following problems: 1) The
proposed probabilistic projection strategy of LSH-MF can avoid the construction
of the GSM. Furthermore, LSH-MF can satisfy the requirement for the accurate
projection of sparse big data. 2) To run LSH-MF for fine-grained
parallelization and online learning on GPUs, we also propose CULSH-MF, which
works on CUDA parallelization. Experimental results show that CULSH-MF can not
only reduce the computational time and memory overhead but also obtain higher
accuracy. Compared with deep learning models, CULSH-MF can not only save
training time but also achieve the same accuracy performance.

    

### [[2111.11836] NUMAscope: Capturing and Visualizing Hardware Metrics on Large ccNUMA Systems](http://arxiv.org/abs/2111.11836)


  Cache-coherent non-uniform memory access (ccNUMA) systems enable parallel
applications to scale-up to thousands of cores and many terabytes of main
memory. However, since remote accesses come at an increased cost, extra
measures are necessitated to scale the applications to high core-counts and
process far greater amounts of data than a typical server can hold. In a
similar manner to how applications are optimized to improve cache utilization,
applications also need to be optimized to improve data-locality on ccNUMA
systems to use larger topologies effectively. The first step to optimizing an
application is to understand what slows it down. Consequently, profiling tools,
or manual instrumentation, are necessary to achieve this. When optimizing
applications on large ccNUMA systems, however, there are limited mechanisms to
capture and present actionable telemetry. This is partially driven by the
proprietary nature of such interconnects, but also by the lack of development
of a common and accessible (read open-source) framework that developers or
vendors can leverage.
In this paper, we present an open-source, extensible framework that captures
high-rate on-chip events with low overhead (<10% single-core utilization). The
presented framework can operate in live or record mode, allowing both real-time
monitoring or capture for later post-workload or offline analysis.
High-resolution visualization is available either through a standards-based
(web) interactive graphical interface or through a convenient textual interface
for quick-look analysis.

    

### [[2111.11872] Real-time intelligent big data processing: technology, platform, and applications](http://arxiv.org/abs/2111.11872)


  Human beings keep exploring the physical space using information means. Only
recently, with the rapid development of information technologies and the
increasing accumulation of data, human beings can learn more about the unknown
world with data-driven methods. Given data timeliness, there is a growing
awareness of the importance of real-time data. There are two categories of
technologies accounting for data processing: batching big data and streaming
processing, which have not been integrated well. Thus, we propose an innovative
incremental processing technology named after Stream Cube to process both big
data and stream data. Also, we implement a real-time intelligent data
processing system, which is based on real-time acquisition, real-time
processing, real-time analysis, and real-time decision-making. The real-time
intelligent data processing technology system is equipped with a batching big
data platform, data analysis tools, and machine learning models. Based on our
applications and analysis, the real-time intelligent data processing system is
a crucial solution to the problems of the national society and economy.

    

### [[2111.11924] A Variant RSA Acceleration with Parallelization](http://arxiv.org/abs/2111.11924)


  The standard RSA relies on multiple big-number modular exponentiation
operations and longer key-length is required for better protection. This
imposes a hefty time penalty for encryption and decryption. In this study, we
analyzed and developed an improved parallel algorithm (PMKRSA) based on the
idea of splitting the plaintext into multiple chunks and encrypt the chunks
using multiple key-pairs. The algorithm in our new scheme is so natural for
parallelized implementation that we also investigated its parallelization in a
GPU environment. In the following, the structure of our new scheme is outlined,
and its correctness is proved mathematically. Then, with the algorithm
implemented and optimized on both CPU and CPU+GPU platforms, we showed that our
algorithm shortens the computational time considerably, and it has a security
advantage over the standard RSA as it is invulnerable to the common attacks.
Finally, we also proved the feasibility of using our algorithm to encrypt large
files through simulation. The results show that over the set of file size: 1
MB, 10 MB, 25 MB, 50 MB, 100 MB, the average encryption and decryption time of
the CPU version is 0.2476s and 9.4476s, and for the CPU+GPU version, it is
0.0009s and 0.0618s, respectively.

    

### [[2111.11983] Modular population protocols](http://arxiv.org/abs/2111.11983)


  Population protocols are a model of distributed computation intended for the
study of networks of independent computing agents with dynamic communication
structure. Each agent has a finite number of states, and communication
opportunities occur nondeterministically, allowing the agents involved to
change their states based on each other's states. Population protocols are
often studied in terms of reaching a consensus on whether the input
configuration satisfied some predicate.
A desirable property of a computation model is modularity, the ability to
combine existing simpler computations in a straightforward way. In the present
paper we present a more general notion of functionality implemented by a
population protocol. This notion allows to design multiphase protocols as
combinations of independently defined phases. The additional generality also
increases the range of behaviours that can be captured in applications.

    

### [[2111.12002] Armada: A Robust Latency-Sensitive Edge Cloud in Heterogeneous Edge-Dense Environments](http://arxiv.org/abs/2111.12002)


  Edge computing has enabled a large set of emerging edge applications by
exploiting data proximity and offloading latency-sensitive and
computation-intensive workloads to nearby edge servers. However, supporting
edge application users at scale in wide-area environments poses challenges due
to limited point-of-presence edge sites and constrained elasticity. In this
paper, we introduce Armada: a densely-distributed edge cloud infrastructure
that explores the use of dedicated and volunteer resources to serve
geo-distributed users in heterogeneous environments. We describe the
lightweight Armada architecture and optimization techniques including
performance-aware edge selection, auto-scaling and load balancing on the edge,
fault tolerance, and in-situ data access. We evaluate Armada in both real-world
volunteer environments and emulated platforms to show how common edge
applications, namely real-time object detection and face recognition, can be
easily deployed on Armada serving distributed users at scale with low latency.

    

### [[2111.12009] LEGOStore: A Linearizable Geo-Distributed Store Combining Replication and Erasure Coding](http://arxiv.org/abs/2111.12009)


  We design and implement LEGOStore, an erasure coding (EC) based linearizable
data store over geo-distributed public cloud data centers (DCs). For such a
data store, the confluence of the following factors opens up opportunities for
EC to be latency-competitive with replication: (a) the necessity of
communicating with remote DCs to tolerate entire DC failures and implement
linearizability; and (b) the emergence of DCs near most large population
centers. LEGOStore employs an optimization framework that, for a given object,
carefully chooses among replication and EC, as well as among various DC
placements to minimize overall costs. To handle workload dynamism, LEGOStore
employs a novel agile reconfiguration protocol. Our evaluation using a
LEGOStore prototype spanning 9 Google Cloud Platform DCs demonstrates the
efficacy of our ideas. We observe cost savings ranging from moderate (5-20\%)
to significant (60\%) over baselines representing the state of the art while
meeting tail latency SLOs. Our reconfiguration protocol is able to transition
key placements in 3 to 4 inter-DC RTTs ($<$ 1s in our experiments), allowing
for agile adaptation to dynamic conditions.

    

### [[2103.05288] DISC: A Dynamic Shape Compiler for Machine Learning Workloads](http://arxiv.org/abs/2103.05288)


  Many recent machine learning models show dynamic shape characteristics.
However, existing AI compiler optimization systems suffer a lot from problems
brought by dynamic shape models, including compilation overhead, memory usage,
optimization pipeline and deployment complexity. This paper provides a compiler
system to natively support optimization for dynamic shape workloads, named
DISC. DISC enriches a set of IR to form a fully dynamic shape representation.
It generates the runtime flow at compile time to support processing dynamic
shape based logic, which avoids the interpretation overhead at runtime and
enlarges the opportunity of host-device co-optimization. It addresses the
kernel fusion problem of dynamic shapes with shape propagation and constraints
collecting methods. This is the first work to demonstrate how to build an
end-to-end dynamic shape compiler based on MLIR infrastructure. Experiments
show that DISC achieves up to 3.3x speedup than TensorFlow/PyTorch, and 1.8x
than Nimble.

    

### [[2103.09425] Bolt-Dumbo Transformer: Asynchronous Consensus As Fast As Pipelined BFT](http://arxiv.org/abs/2103.09425)


  Optimistic asynchronous atomic broadcast was proposed to improve the
performance of asynchronous protocols while maintaining their liveness in
unstable networks (Kursawe-Shoup, 2002; Ramasamy-Cachin, 2005). They used a
faster deterministic protocol in the optimistic case when the network condition
remains good, and can safely fallback to a pessimistic path running
asynchronous atomic broadcast once the fast path fails to proceed.
Unfortunately, besides that the pessimistic path is slow, existing fallback
mechanisms directly use a heavy tool of asynchronous multi-valued validated
Byzantine agreement (MVBA). When deployed on the open Internet, which could be
fluctuating, the inefficient fallback may happen frequently thus the benefits
of adding the optimistic path are eliminated.
We give a generic framework for practical optimistic asynchronous atomic
broadcast. A new abstraction of the optimistic case protocols, which can be
instantiated easily, is presented. More importantly, it enables us to design a
highly efficient fallback mechanism to handle the fast path failures. The
resulting fallback replaces the cumbersome MVBA by a variant of simple binary
agreement only. Besides a detailed security analysis, we also give concrete
instantiations of our framework and implement them. Extensive experiments show
that our new fallback mechanism adds minimal overhead, demonstrating that our
framework can enjoy both the low latency of deterministic protocols and robust
liveness of randomized asynchronous protocols in practice.

    

### [[2108.13222] AuctionWhisk: Using an Auction-Inspired Approach for Function Placement in Serverless Fog Platforms](http://arxiv.org/abs/2108.13222)


  The Function-as-a-Service (FaaS) paradigm has a lot of potential as a
computing model for fog environments comprising both cloud and edge nodes, as
compute requests can be scheduled across the entire fog continuum in a
fine-grained manner. When the request rate exceeds capacity limits at the
resource-constrained edge, some functions need to be offloaded towards the
cloud.
In this paper, we present an auction-inspired approach in which application
developers bid on resources while fog nodes decide locally which functions to
execute and which to offload in order to maximize revenue. Unlike many current
approaches to function placement in the fog, our approach can work in an online
and decentralized manner. We also present our proof-of-concept prototype
AuctionWhisk that illustrates how such an approach can be implemented in a real
FaaS platform. Through a number of simulation runs and system experiments, we
show that revenue for overloaded nodes can be maximized without dropping
function requests.

    

### [[2111.11588] A Logical Semantics for PDDL+](http://arxiv.org/abs/2111.11588)


  PDDL+ is an extension of PDDL2.1 which incorporates fully-featured autonomous
processes and allows for better modelling of mixed discrete-continuous domains.
Unlike PDDL2.1, PDDL+ lacks a logical semantics, relying instead on
state-transitional semantics enriched with hybrid automata semantics for the
continuous states. This complex semantics makes analysis and comparisons to
other action formalisms difficult. In this paper, we propose a natural
extension of Reiter's situation calculus theories inspired by hybrid automata.
The kinship between PDDL+ and hybrid automata allows us to develop a direct
mapping between PDDL+ and situation calculus, thereby supplying PDDL+ with a
logical semantics and the situation calculus with a modern way of representing
autonomous processes. We outline the potential benefits of the mapping by
suggesting a new approach to effective planning in PDDL+.

    

### [[2111.11646] CytoImageNet: A large-scale pretraining dataset for bioimage transfer learning](http://arxiv.org/abs/2111.11646)


  Motivation: In recent years, image-based biological assays have steadily
become high-throughput, sparking a need for fast automated methods to extract
biologically-meaningful information from hundreds of thousands of images.
Taking inspiration from the success of ImageNet, we curate CytoImageNet, a
large-scale dataset of openly-sourced and weakly-labeled microscopy images
(890K images, 894 classes). Pretraining on CytoImageNet yields features that
are competitive to ImageNet features on downstream microscopy classification
tasks. We show evidence that CytoImageNet features capture information not
available in ImageNet-trained features. The dataset is made available at
\url{this https URL}.

    

### [[2111.11720] Gait Identification under Surveillance Environment based on Human Skeleton](http://arxiv.org/abs/2111.11720)


  As an emerging biological identification technology, vision-based gait
identification is an important research content in biometrics. Most existing
gait identification methods extract features from gait videos and identify a
probe sample by a query in the gallery. However, video data contains redundant
information and can be easily influenced by bagging (BG) and clothing (CL).
Since human body skeletons convey essential information about human gaits, a
skeleton-based gait identification network is proposed in our project. First,
extract skeleton sequences from the video and map them into a gait graph. Then
a feature extraction network based on Spatio-Temporal Graph Convolutional
Network (ST-GCN) is constructed to learn gait representations. Finally, the
probe sample is identified by matching with the most similar piece in the
gallery. We tested our method on the CASIA-B dataset. The result shows that our
approach is highly adaptive and gets the advanced result in BG, CL conditions,
and average.

    

### [[2111.11755] Guided-TTS:Text-to-Speech with Untranscribed Speech](http://arxiv.org/abs/2111.11755)


  Most neural text-to-speech (TTS) models require <speech, transcript> paired
data from the desired speaker for high-quality speech synthesis, which limits
the usage of large amounts of untranscribed data for training. In this work, we
present Guided-TTS, a high-quality TTS model that learns to generate speech
from untranscribed speech data. Guided-TTS combines an unconditional diffusion
probabilistic model with a separately trained phoneme classifier for
text-to-speech. By modeling the unconditional distribution for speech, our
model can utilize the untranscribed data for training. For text-to-speech
synthesis, we guide the generative process of the unconditional DDPM via
phoneme classification to produce mel-spectrograms from the conditional
distribution given transcript. We show that Guided-TTS achieves comparable
performance with the existing methods without any transcript for LJSpeech. Our
results further show that a single speaker-dependent phoneme classifier trained
on multispeaker large-scale data can guide unconditional DDPMs for various
speakers to perform TTS.

    

### [[2111.11773] Upsampling layers for music source separation](http://arxiv.org/abs/2111.11773)


  Upsampling artifacts are caused by problematic upsampling layers and due to
spectral replicas that emerge while upsampling. Also, depending on the used
upsampling layer, such artifacts can either be tonal artifacts (additive
high-frequency noise) or filtering artifacts (substractive, attenuating some
bands). In this work we investigate the practical implications of having
upsampling artifacts in the resulting audio, by studying how different
artifacts interact and assessing their impact on the models' performance. To
that end, we benchmark a large set of upsampling layers for music source
separation: different transposed and subpixel convolution setups, different
interpolation upsamplers (including two novel layers based on stretch and sinc
interpolation), and different wavelet-based upsamplers (including a novel
learnable wavelet layer). Our results show that filtering artifacts, associated
with interpolation upsamplers, are perceptually preferrable, even if they tend
to achieve worse objective scores.

    

### [[2111.11779] Answering Fuzzy Queries over Fuzzy DL-Lite Ontologies](http://arxiv.org/abs/2111.11779)


  A prominent problem in knowledge representation is how to answer queries
taking into account also the implicit consequences of an ontology representing
domain knowledge. While this problem has been widely studied within the realm
of description logic ontologies, it has been surprisingly neglected within the
context of vague or imprecise knowledge, particularly from the point of view of
mathematical fuzzy logic. In this paper we study the problem of answering
conjunctive queries and threshold queries w.r.t. ontologies in fuzzy DL-Lite.
Specifically, we show through a rewriting approach that threshold query
answering w.r.t. consistent ontologies remains in $AC_0$ in data complexity,
but that conjunctive query answering is highly dependent on the selected
triangular norm, which has an impact on the underlying semantics. For the
idempodent Gödel t-norm, we provide an effective method based on a reduction
to the classical case. This paper is under consideration in Theory and Practice
of Logic Programming (TPLP).

    

### [[2111.11785] Realistic simulation of users for IT systems in cyber ranges](http://arxiv.org/abs/2111.11785)


  Generating user activity is a key capability for both evaluating security
monitoring tools as well as improving the credibility of attacker analysis
platforms (e.g., honeynets). In this paper, to generate this activity, we
instrument each machine by means of an external agent. This agent combines both
deterministic and deep learning based methods to adapt to different environment
(e.g., multiple OS, software versions, etc.), while maintaining high
performances. We also propose conditional text generation models to facilitate
the creation of conversations and documents to accelerate the definition of
coherent, system-wide, life scenarios.

    

### [[2111.11871] Solve Optimization Problems with Unknown Constraint Networks](http://arxiv.org/abs/2111.11871)


  In most optimization problems, users have a clear understanding of the
function to optimize (e.g., minimize the makespan for scheduling problems).
However, the constraints may be difficult to state and their modelling often
requires expertise in Constraint Programming. Active constraint acquisition has
been successfully used to support non-experienced users in learning constraint
networks through the generation of a sequence of queries. In this paper, we
propose Learn&Optimize, a method to solve optimization problems with known
objective function and unknown constraint network. It uses an active constraint
acquisition algorithm which learns the unknown constraints and computes
boundaries for the optimal solution during the learning process. As a result,
our method allows users to solve optimization problems without learning the
overall constraint network.

    

### [[2111.11965] Object Recognition by a Minimally Pre-Trained System in the Process of Environment Exploration](http://arxiv.org/abs/2111.11965)


  We update the method of describing and assessing the process of the study of
an abstract environment by a system, proposed earlier. We do not model any
biological cognition mechanisms and consider the system as an agent equipped
with an information processor (or a group of such agents), which makes a move
in the environment, consumes information supplied by the environment, and gives
out the next move (hence, the process is considered as a game). The system
moves in an unknown environment and should recognize new objects located in it.
In this case, the system should build comprehensive images of visible things
and memorize them if necessary (and it should also choose the current goal
set). The main problems here are object recognition, and the informational
reward rating in the game. Thus, the main novelty of the paper is a new method
of evaluating the amount of visual information about the object as the reward.
In such a system, we suggest using a minimally pre-trained neural network to be
responsible for the recognition: at first, we train the network only for
Biederman geons (geometrical primitives). The geons are generated
programmatically and we demonstrate that such a trained network recognizes
geons in real objects quite well. We also offer to generate, procedurally, new
objects from geon schemes (geon combinations in images) obtained from the
environment and to store them in a database. In this case, we do not obtain new
information about an object (i.e., our reward is maximal, thus the game and the
object cognition process stop) when we stop getting new schemes of this kind.
These schemes are generated from geons connected with the object. In the case
of a possibly known item, the informational reward is maximal when we have no
more detection uncertainty for any of the objects.

    

### [[2111.11982] Is Dynamic Rumor Detection on social media Viable? An Unsupervised Perspective](http://arxiv.org/abs/2111.11982)


  With the growing popularity and ease of access to the internet, the problem
of online rumors is escalating. People are relying on social media to gain
information readily but fall prey to false information. There is a lack of
credibility assessment techniques for online posts to identify rumors as soon
as they arrive. Existing studies have formulated several mechanisms to combat
online rumors by developing machine learning and deep learning algorithms. The
literature so far provides supervised frameworks for rumor classification that
rely on huge training datasets. However, in the online scenario where
supervised learning is exigent, dynamic rumor identification becomes difficult.
Early detection of online rumors is a challenging task, and studies relating to
them are relatively few. It is the need of the hour to identify rumors as soon
as they appear online. This work proposes a novel framework for unsupervised
rumor detection that relies on an online post's content and social features
using state-of-the-art clustering techniques. The proposed architecture
outperforms several existing baselines and performs better than several
supervised techniques. The proposed method, being lightweight, simple, and
robust, offers the suitability of being adopted as a tool for online rumor
identification.

    

### [[2102.11023] Predicting Material Properties Using a 3D Graph Neural Network with Invariant Local Descriptors](http://arxiv.org/abs/2102.11023)


  Accurate prediction of physical properties is critical for discovering and
designing novel materials. Machine learning technologies have attracted
significant attention in the materials science community for their potential
for large-scale screening. Graph Convolution Neural Network (GCNN) is one of
the most successful machine learning methods because of its flexibility and
effectiveness in describing 3D structural data. Most existing GCNN models focus
on the topological structure but overly simplify the three-dimensional
geometric structure. However, in materials science, the 3D-spatial distribution
of atoms is crucial for determining the atomic states and interatomic forces.
This paper proposes an adaptive GCNN with a novel convolution mechanism that
simultaneously models atomic interactions among all neighbor atoms in
three-dimensional space. We apply the proposed model to two distinctly
challenging problems on predicting material properties. The first is Henry's
constant for gas adsorption in Metal-Organic Frameworks (MOFs), which is
notoriously difficult because of its high sensitivity to atomic configurations.
The second is the ion conductivity in solid-state crystal materials, which is
difficult because of few labeled data available for training. The new model
outperforms existing graph-based models on both data sets, suggesting that the
critical three-dimensional geometric information is indeed captured.

    

### [[2111.11819] Satisfiability of Constrained Horn Clauses on Algebraic Data Types: A Transformation-based Approach](http://arxiv.org/abs/2111.11819)


  We address the problem of checking the satisfiability of Constrained Horn
Clauses (CHCs) defined on Algebraic Data Types (ADTs), such as lists and trees.
We propose a new technique for transforming CHCs defined on ADTs into CHCs
where the arguments of the predicates have only basic types, such as integers
and booleans. Thus, our technique avoids, during satisfiability checking, the
explicit use of proof rules based on induction over the ADTs. The main
extension over previous techniques for ADT removal is a new transformation
rule, called differential replacement, which allows us to introduce auxiliary
predicates, whose definitions correspond to lemmas that are used when making
inductive proofs. We present an algorithm that performs the automatic removal
of ADTs by applying the new rule, together with the traditional
folding/unfolding rules. We prove that, under suitable hypotheses, the set of
the transformed clauses is satisfiable if and only if so is the set of the
original clauses. By an experimental evaluation, we show that the use of the
new rule significantly improves the effectiveness of ADT removal. We also show
that our approach is competitive with respect to tools that extend CHC solvers
with the use of inductive rules.

    

### [[2111.11984] Deconfined Global Types for Asynchronous Sessions](http://arxiv.org/abs/2111.11984)


  Multiparty sessions with asynchronous communications and global types play an
important role for the modelling of interaction protocols in distributed
systems. In designing such calculi the aim is to enforce, by typing, good
properties for all participants, maximising, at the same time, the behaviours
accepted. The global types presented in this paper improve the state-of-the-art
by extending the set of typable asynchronous sessions and preserving
decidability of type checking together with the key properties of Subject
Reduction, Session Fidelity and Progress. Our type system is equipped with a
type inference algorithm returning global types to be checked against
well-formedness conditions.

    

### [[2111.12063] Quantum Advantage for All](http://arxiv.org/abs/2111.12063)


  We show how to translate a subset of RISC-V machine code compiled from a
subset of C to quadratic unconstrained binary optimization (QUBO) models that
can be solved by a quantum annealing machine: given a bound $n$, there is input
$I$ to a program $P$ such that $P$ runs into a given program state $E$
executing no more than $n$ machine instructions if and only if the QUBO model
of $P$ for $n$ evaluates to 0 on $I$. Thus, with more qubits on the machine
than variables in the QUBO model, quantum annealing the model reaches 0
(ground) energy in constant time with high probability on some input $I$ that
is part of the ground state if and only if $P$ runs into $E$ on $I$ executing
no more than $n$ instructions. Translation takes $\mathcal{O}(n^2)$ time
effectively turning a quantum annealer into a polynomial-time symbolic
execution engine and bounded model checker, eliminating their path and state
explosion problems. Here, we take advantage of the fact that any machine
instruction may only increase the size of the program state by a constant
amount of bits. Translation time comes down from $\mathcal{O}(n^2)$ to
$\mathcal{O}(n\cdot|P|)$ if memory consumption of $P$ is bounded by a constant,
establishing a linear (quadratic) upper bound on quantum space, in number of
qubits on a quantum annealer, in terms of algorithmic time (space) in classical
computing. Our prototypical open-source toolchain translates machine code that
runs on real RISC-V hardware to models that can be solved by real quantum
annealing hardware, as shown in our experiments.

    