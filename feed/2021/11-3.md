
## 2021-11-3

### [[2111.01136] ASMDD: Arabic Speech Mispronunciation Detection Dataset](http://arxiv.org/abs/2111.01136)


  The largest dataset of Arabic speech mispronunciation detections in Egyptian
dialogues is introduced. The dataset is composed of annotated audio files
representing the top 100 words that are most frequently used in the Arabic
language, pronounced by 100 Egyptian children (aged between 2 and 8 years old).
The dataset is collected and annotated on segmental pronunciation error
detections by expert listeners.

    

### [[2111.01263] Optimized Passive Optical Networks with Cascaded-AWGRs for Data Centers](http://arxiv.org/abs/2111.01263)


  The use of Passive Optical Networks (PONs) in modern and future data centers
can provide energy efficiency, high capacity, low cost, scalability, and
elasticity. This paper introduces a passive optical network design with 2-tier
cascaded Arrayed Waveguide Grating Routers (AWGRs) to connect groups of racks
(i.e. cells) within a data center. This design employs a Software-Defined
Networking (SDN) controller to manage the routing and assignment of the
networking resource while introducing multiple paths between any two cells to
improve routing, load balancing and resilience. We provide benchmarking results
for the power consumption to compare the energy efficiency of this design to
state-of-the-art data centers. The results indicate that the cascaded AWGRs
architecture can achieve up to 43% saving in the networking power consumption
compared to Fat-Tree data center architecture.

    

### [[2111.01570] Privacy-Preserving Communication-Efficient Federated Multi-Armed Bandits](http://arxiv.org/abs/2111.01570)


  Communication bottleneck and data privacy are two critical concerns in
federated multi-armed bandit (MAB) problems, such as situations in
decision-making and recommendations of connected vehicles via wireless. In this
paper, we design the privacy-preserving communication-efficient algorithm in
such problems and study the interactions among privacy, communication and
learning performance in terms of the regret. To be specific, we design
privacy-preserving learning algorithms and communication protocols and derive
the learning regret when networked private agents are performing online bandit
learning in a master-worker, a decentralized and a hybrid structure. Our bandit
learning algorithms are based on epoch-wise sub-optimal arm eliminations at
each agent and agents exchange learning knowledge with the server/each other at
the end of each epoch. Furthermore, we adopt the differential privacy (DP)
approach to protect the data privacy at each agent when exchanging information;
and we curtail communication costs by making less frequent communications with
fewer agents participation. By analyzing the regret of our proposed algorithmic
framework in the master-worker, decentralized and hybrid structures, we
theoretically show tradeoffs between regret and communication costs/privacy.
Finally, we empirically show these trade-offs which are consistent with our
theoretical analysis.

    

### [[2111.01605] Profit Sharing Contracts between Content and Service Providers for Enhanced Network Quality](http://arxiv.org/abs/2111.01605)


  It has been a long demand of Internet Service Providers (ISPs) that the
Content Providers (CPs) share their profits for investments in network
infrastructure. In this paper, we study profit sharing contracts between a CP
with multiple ISPs. Each ISP commits to improving the Quality of Service (QoS)
for the end-users through higher investments efforts. The CP agrees to share
the profits due to the resulting higher demand for its content. We first model
non-cooperative interaction between the CP and the ISPs as a two-stage
Stackelberg game. CP is the leader that decides what fraction of its profits
will be shared with the ISPs. Each ISP then simultaneously decides the amount
of effort (investment) to enhance network quality. Here, CP cannot observe
individual effort by the ISPs, which poses a challenge for the CP to decide how
to share the profits with each ISP. Therefore, we also investigate a
cooperative scenario, where the CP only decides the total share it gives to the
ISPs, and each ISP then cooperatively shares the profit among themselves. We
study the effect of such cooperation between the ISPs by building a Nash
Bargaining based model. We show that the collaboration improves total effort by
the ISPs and the payoff of the CP.

    

### [[2111.01616] OnSlicing: Online End-to-End Network Slicing with Reinforcement Learning](http://arxiv.org/abs/2111.01616)


  Network slicing allows mobile network operators to virtualize infrastructures
and provide customized slices for supporting various use cases with
heterogeneous requirements. Online deep reinforcement learning (DRL) has shown
promising potential in solving network problems and eliminating the
simulation-to-reality discrepancy. Optimizing cross-domain resources with
online DRL is, however, challenging, as the random exploration of DRL violates
the service level agreement (SLA) of slices and resource constraints of
infrastructures. In this paper, we propose OnSlicing, an online end-to-end
network slicing system, to achieve minimal resource usage while satisfying
slices' SLA. OnSlicing allows individualized learning for each slice and
maintains its SLA by using a novel constraint-aware policy update method and
proactive baseline switching mechanism. OnSlicing complies with resource
constraints of infrastructures by using a unique design of action modification
in slices and parameter coordination in infrastructures. OnSlicing further
mitigates the poor performance of online learning during the early learning
stage by offline imitating a rule-based solution. Besides, we design four new
domain managers to enable dynamic resource configuration in radio access,
transport, core, and edge networks, respectively, at a timescale of subseconds.
We implement OnSlicing on an end-to-end slicing testbed designed based on
OpenAirInterface with both 4G LTE and 5G NR, OpenDayLight SDN platform, and
OpenAir-CN core network. The experimental results show that OnSlicing achieves
61.3% usage reduction as compared to the rule-based solution and maintains
nearly zero violation (0.06%) throughout the online learning phase. As online
learning is converged, OnSlicing reduces 12.5% usage without any violations as
compared to the state-of-the-art online DRL solution.

    

### [[2111.01634] Towards Enabling High-Five Over WiFi](http://arxiv.org/abs/2111.01634)


  The next frontier for immersive applications is enabling sentience over the
Internet. Tactile Internet (TI) envisages transporting skills by providing
Ultra-Low Latency (ULL) communications for transporting touch senses. In this
work, we focus our study on the first/last mile communication, where the future
generation WiFi-7 is pitched as the front-runner for ULL applications. We
discuss a few candidate features of WiFi-7 and highlight its major pitfalls
with respect to ULL communication. Further, through a specific implementation
of WiFi-7 (vanilla WiFi-7) in our custom simulator, we demonstrate the impact
of one of the pitfalls - standard practice of using jitter buffer in
conjunction with frame aggregation - on TI communication. To circumvent this,
we propose Non-Buffered Scheme (NoBuS) - a simple MAC layer enhancement for
enabling TI applications on WiFi-7. NoBuS trades off packet loss for latency
enabling swift synchronization between the master and controlled domains. Our
findings reveal that employing NoBuS yields a significant improvement in RMSE
of TI signals. Further, we show that the worst-case WiFi latency with NoBuS is
3.72 ms - an order of magnitude lower than vanilla WiFi-7 even under highly
congested network conditions.

    

### [[2111.01131] Hierarchical Decision Ensembles- An inferential framework for uncertain Human-AI collaboration in forensic examinations](http://arxiv.org/abs/2111.01131)


  Forensic examination of evidence like firearms and toolmarks, traditionally
involves a visual and therefore subjective assessment of similarity of two
questioned items. Statistical models are used to overcome this subjectivity and
allow specification of error rates. These models are generally quite complex
and produce abstract results at different levels of the analysis. Presenting
such metrics and complicated results to examiners is challenging, as examiners
generally do not have substantial statistical training to accurately interpret
results. This creates distrust in statistical modelling and lowers the rate of
acceptance of more objective measures that the discipline at large is striving
for. We present an inferential framework for assessing the model and its
output. The framework is designed to calibrate trust in forensic experts by
bridging the gap between domain specific knowledge and predictive model
results, allowing forensic examiners to validate the claims of the predictive
model while critically assessing results.

    

### [[2111.01134] Comparing Bayesian Models for Organ Contouring in Headand Neck Radiotherapy](http://arxiv.org/abs/2111.01134)


  Deep learning models for organ contouring in radiotherapy are poised for
clinical usage, but currently, there exist few tools for automated quality
assessment (QA) of the predicted contours. Using Bayesian models and their
associated uncertainty, one can potentially automate the process of detecting
inaccurate predictions. We investigate two Bayesian models for auto-contouring,
DropOut and FlipOut, using a quantitative measure - expected calibration error
(ECE) and a qualitative measure - region-based accuracy-vs-uncertainty (R-AvU)
graphs. It is well understood that a model should have low ECE to be considered
trustworthy. However, in a QA context, a model should also have high
uncertainty in inaccurate regions and low uncertainty in accurate regions. Such
behaviour could direct visual attention of expert users to potentially
inaccurate regions, leading to a speed up in the QA process. Using R-AvU
graphs, we qualitatively compare the behaviour of different models in accurate
and inaccurate regions. Experiments are conducted on the MICCAI2015 Head and
Neck Segmentation Challenge and on the DeepMindTCIA CT dataset using three
models: DropOut-DICE, Dropout-CE (Cross Entropy) and FlipOut-CE. Quantitative
results show that DropOut-DICE has the highest ECE, while Dropout-CE and
FlipOut-CE have the lowest ECE. To better understand the difference between
DropOut-CE and FlipOut-CE, we use the R-AvU graph which shows that FlipOut-CE
has better uncertainty coverage in inaccurate regions than DropOut-CE. Such a
combination of quantitative and qualitative metrics explores a new approach
that helps to select which model can be deployed as a QA tool in clinical
settings.

    

### [[2111.01135] Arch-Net: Model Distillation for Architecture Agnostic Model Deployment](http://arxiv.org/abs/2111.01135)


  Vast requirement of computation power of Deep Neural Networks is a major
hurdle to their real world applications. Many recent Application Specific
Integrated Circuit (ASIC) chips feature dedicated hardware support for Neural
Network Acceleration. However, as ASICs take multiple years to develop, they
are inevitably out-paced by the latest development in Neural Architecture
Research. For example, Transformer Networks do not have native support on many
popular chips, and hence are difficult to deploy. In this paper, we propose
Arch-Net, a family of Neural Networks made up of only operators efficiently
supported across most architectures of ASICs. When a Arch-Net is produced, less
common network constructs, like Layer Normalization and Embedding Layers, are
eliminated in a progressive manner through label-free Blockwise Model
Distillation, while performing sub-eight bit quantization at the same time to
maximize performance. Empirical results on machine translation and image
classification tasks confirm that we can transform latest developed Neural
Architectures into fast running and as-accurate Arch-Net, ready for deployment
on multiple mass-produced ASIC chips. The code will be available at
this https URL.

    

### [[2111.01137] Stock Price Prediction Using Time Series, Econometric, Machine Learning, and Deep Learning Models](http://arxiv.org/abs/2111.01137)


  For a long-time, researchers have been developing a reliable and accurate
predictive model for stock price prediction. According to the literature, if
predictive models are correctly designed and refined, they can painstakingly
and faithfully estimate future stock values. This paper demonstrates a set of
time series, econometric, and various learning-based models for stock price
prediction. The data of Infosys, ICICI, and SUN PHARMA from the period of
January 2004 to December 2019 was used here for training and testing the models
to know which model performs best in which sector. One time series model
(Holt-Winters Exponential Smoothing), one econometric model (ARIMA), two
machine Learning models (Random Forest and MARS), and two deep learning-based
models (simple RNN and LSTM) have been included in this paper. MARS has been
proved to be the best performing machine learning model, while LSTM has proved
to be the best performing deep learning model. But overall, for all three
sectors - IT (on Infosys data), Banking (on ICICI data), and Health (on SUN
PHARMA data), MARS has proved to be the best performing model in sales
forecasting.

    

### [[2111.01166] Investigating the locality of neural network training dynamics](http://arxiv.org/abs/2111.01166)


  A fundamental quest in the theory of deep-learning is to understand the
properties of the trajectories in the weight space that a learning algorithm
takes. One such property that had very recently been isolated is that of "local
elasticity" ($S_{\rm rel}$), which quantifies the propagation of influence of a
sampled data point on the prediction at another data point. In this work, we
perform a comprehensive study of local elasticity by providing new theoretical
insights and more careful empirical evidence of this property in a variety of
settings. Firstly, specific to the classification setting, we suggest a new
definition of the original idea of $S_{\rm rel}$. Via experiments on
state-of-the-art neural networks training on SVHN, CIFAR-10 and CIFAR-100 we
demonstrate how our new $S_{\rm rel}$ detects the property of the weight
updates preferring to make changes in predictions within the same class of the
sampled data. Next, we demonstrate via examples of neural nets doing regression
that the original $S_{\rm rel}$ reveals a $2-$phase behaviour: that their
training proceeds via an initial elastic phase when $S_{\rm rel}$ changes
rapidly and an eventual inelastic phase when $S_{\rm rel}$ remains large.
Lastly, we give multiple examples of learning via gradient flows for which one
can get a closed-form expression of the original $S_{\rm rel}$ function. By
studying the plots of these derived formulas we given a theoretical
demonstration of some of the experimentally detected properties of $S_{\rm
rel}$ in the regression setting.

    

### [[2111.01177] Don't Generate Me: Training Differentially Private Generative Models with Sinkhorn Divergence](http://arxiv.org/abs/2111.01177)


  Although machine learning models trained on massive data have led to
break-throughs in several areas, their deployment in privacy-sensitive domains
remains limited due to restricted access to data. Generative models trained
with privacy constraints on private data can sidestep this challenge, providing
indirect access to private data instead. We propose DP-Sinkhorn, a novel
optimal transport-based generative method for learning data distributions from
private data with differential privacy. DP-Sinkhorn minimizes the Sinkhorn
divergence, a computationally efficient approximation to the exact optimal
transport distance, between the model and data in a differentially private
manner and uses a novel technique for control-ling the bias-variance trade-off
of gradient estimates. Unlike existing approaches for training differentially
private generative models, which are mostly based on generative adversarial
networks, we do not rely on adversarial objectives, which are notoriously
difficult to optimize, especially in the presence of noise imposed by privacy
constraints. Hence, DP-Sinkhorn is easy to train and deploy. Experimentally, we
improve upon the state-of-the-art on multiple image modeling benchmarks and
show differentially private synthesis of informative RGB images. Project
page:this https URL.

    

### [[2111.01186] Combining Latent Space and Structured Kernels for Bayesian Optimization over Combinatorial Spaces](http://arxiv.org/abs/2111.01186)


  We consider the problem of optimizing combinatorial spaces (e.g., sequences,
trees, and graphs) using expensive black-box function evaluations. For example,
optimizing molecules for drug design using physical lab experiments. Bayesian
optimization (BO) is an efficient framework for solving such problems by
intelligently selecting the inputs with high utility guided by a learned
surrogate model. A recent BO approach for combinatorial spaces is through a
reduction to BO over continuous spaces by learning a latent representation of
structures using deep generative models (DGMs). The selected input from the
continuous space is decoded into a discrete structure for performing function
evaluation. However, the surrogate model over the latent space only uses the
information learned by the DGM, which may not have the desired inductive bias
to approximate the target black-box function. To overcome this drawback, this
paper proposes a principled approach referred as LADDER. The key idea is to
define a novel structure-coupled kernel that explicitly integrates the
structural information from decoded structures with the learned latent space
representation for better surrogate modeling. Our experiments on real-world
benchmarks show that LADDER significantly improves over the BO over latent
space method, and performs better or similar to state-of-the-art methods.

    

### [[2111.01193] Transformers for prompt-level EMA non-response prediction](http://arxiv.org/abs/2111.01193)


  Ecological Momentary Assessments (EMAs) are an important psychological data
source for measuring current cognitive states, affect, behavior, and
environmental factors from participants in mobile health (mHealth) studies and
treatment programs. Non-response, in which participants fail to respond to EMA
prompts, is an endemic problem. The ability to accurately predict non-response
could be utilized to improve EMA delivery and develop compliance interventions.
Prior work has explored classical machine learning models for predicting
non-response. However, as increasingly large EMA datasets become available,
there is the potential to leverage deep learning models that have been
effective in other fields. Recently, transformer models have shown
state-of-the-art performance in NLP and other domains. This work is the first
to explore the use of transformers for EMA data analysis. We address three key
questions in applying transformers to EMA data: 1. Input representation, 2.
encoding temporal information, 3. utility of pre-training on improving
downstream prediction task performance. The transformer model achieves a
non-response prediction AUC of 0.77 and is significantly better than classical
ML and LSTM-based deep learning models. We will make our a predictive model
trained on a corpus of 40K EMA samples freely-available to the research
community, in order to facilitate the development of future transformer-based
EMA analysis works.

    

### [[2111.01201] Unintended Selection: Persistent Qualification Rate Disparities and Interventions](http://arxiv.org/abs/2111.01201)


  Realistically -- and equitably -- modeling the dynamics of group-level
disparities in machine learning remains an open problem. In particular, we
desire models that do not suppose inherent differences between artificial
groups of people -- but rather endogenize disparities by appeal to unequal
initial conditions of insular subpopulations. In this paper, agents each have a
real-valued feature $X$ (e.g., credit score) informed by a "true" binary label
$Y$ representing qualification (e.g., for a loan). Each agent alternately (1)
receives a binary classification label $\hat{Y}$ (e.g., loan approval) from a
Bayes-optimal machine learning classifier observing $X$ and (2) may update
their qualification $Y$ by imitating successful strategies (e.g., seek a raise)
within an isolated group $G$ of agents to which they belong. We consider the
disparity of qualification rates $\Pr(Y=1)$ between different groups and how
this disparity changes subject to a sequence of Bayes-optimal classifiers
repeatedly retrained on the global population. We model the evolving
qualification rates of each subpopulation (group) using the replicator
equation, which derives from a class of imitation processes. We show that
differences in qualification rates between subpopulations can persist
indefinitely for a set of non-trivial equilibrium states due to uniformed
classifier deployments, even when groups are identical in all aspects except
initial qualification densities. We next simulate the effects of commonly
proposed fairness interventions on this dynamical system along with a new
feedback control mechanism capable of permanently eliminating group-level
qualification rate disparities. We conclude by discussing the limitations of
our model and findings and by outlining potential future work.

    

### [[2111.01203] One Proxy Device Is Enough for Hardware-Aware Neural Architecture Search](http://arxiv.org/abs/2111.01203)


  Convolutional neural networks (CNNs) are used in numerous real-world
applications such as vision-based autonomous driving and video content
analysis. To run CNN inference on various target devices, hardware-aware neural
architecture search (NAS) is crucial. A key requirement of efficient
hardware-aware NAS is the fast evaluation of inference latencies in order to
rank different architectures. While building a latency predictor for each
target device has been commonly used in state of the art, this is a very
time-consuming process, lacking scalability in the presence of extremely
diverse devices. In this work, we address the scalability challenge by
exploiting latency monotonicity -- the architecture latency rankings on
different devices are often correlated. When strong latency monotonicity
exists, we can re-use architectures searched for one proxy device on new target
devices, without losing optimality. In the absence of strong latency
monotonicity, we propose an efficient proxy adaptation technique to
significantly boost the latency monotonicity. Finally, we validate our approach
and conduct experiments with devices of different platforms on multiple
mainstream search spaces, including MobileNet-V2, MobileNet-V3, NAS-Bench-201,
ProxylessNAS and FBNet. Our results highlight that, by using just one proxy
device, we can find almost the same Pareto-optimal architectures as the
existing per-device NAS, while avoiding the prohibitive cost of building a
latency predictor for each device.

    

### [[2111.01207] Sig-Wasserstein GANs for Time Series Generation](http://arxiv.org/abs/2111.01207)


  Synthetic data is an emerging technology that can significantly accelerate
the development and deployment of AI machine learning pipelines. In this work,
we develop high-fidelity time-series generators, the SigWGAN, by combining
continuous-time stochastic models with the newly proposed signature $W_1$
metric. The former are the Logsig-RNN models based on the stochastic
differential equations, whereas the latter originates from the universal and
principled mathematical features to characterize the measure induced by time
series. SigWGAN allows turning computationally challenging GAN min-max problem
into supervised learning while generating high fidelity samples. We validate
the proposed model on both synthetic data generated by popular quantitative
risk models and empirical financial data. Codes are available at
this https URL.

    

### [[2111.01216] Learning To Generate Piano Music With Sustain Pedals](http://arxiv.org/abs/2111.01216)


  Recent years have witnessed a growing interest in research related to the
detection of piano pedals from audio signals in the music information retrieval
community. However, to our best knowledge, recent generative models for
symbolic music have rarely taken piano pedals into account. In this work, we
employ the transcription model proposed by Kong et al. to get pedal information
from the audio recordings of piano performance in the AILabs1k7 dataset, and
then modify the Compound Word Transformer proposed by Hsiao et al. to build a
Transformer decoder that generates pedal-related tokens along with other
musical tokens. While the work is done by using inferred sustain pedal
information as training data, the result shows hope for further improvement and
the importance of the involvement of sustain pedal in tasks of piano
performance generations.

    

### [[2111.01221] Robust Federated Learning via Over-The-Air Computation](http://arxiv.org/abs/2111.01221)


  This paper investigates the robustness of over-the-air federated learning to
Byzantine attacks. The simple averaging of the model updates via over-the-air
computation makes the learning task vulnerable to random or intended
modifications of the local model updates of some malicious clients. We propose
a robust transmission and aggregation framework to such attacks while
preserving the benefits of over-the-air computation for federated learning. For
the proposed robust federated learning, the participating clients are randomly
divided into groups and a transmission time slot is allocated to each group.
The parameter server aggregates the results of the different groups using a
robust aggregation technique and conveys the result to the clients for another
training round. We also analyze the convergence of the proposed algorithm.
Numerical simulations confirm the robustness of the proposed approach to
Byzantine attacks.

    

### [[2111.01222] Kernel Deformed Exponential Families for Sparse Continuous Attention](http://arxiv.org/abs/2111.01222)


  Attention mechanisms take an expectation of a data representation with
respect to probability weights. This creates summary statistics that focus on
important features. Recently, (Martins et al. 2020, 2021) proposed continuous
attention mechanisms, focusing on unimodal attention densities from the
exponential and deformed exponential families: the latter has sparse support.
(Farinhas et al. 2021) extended this to use Gaussian mixture attention
densities, which are a flexible class with dense support. In this paper, we
extend this to two general flexible classes: kernel exponential families and
our new sparse counterpart kernel deformed exponential families. Theoretically,
we show new existence results for both kernel exponential and deformed
exponential families, and that the deformed case has similar approximation
capabilities to kernel exponential families. Experiments show that kernel
deformed exponential families can attend to multiple compact regions of the
data domain.

    

### [[2111.01223] A framework for causal segmentation analysis with machine learning in large-scale digital experiments](http://arxiv.org/abs/2111.01223)


  We present an end-to-end methodological framework for causal segment
discovery that aims to uncover differential impacts of treatments across
subgroups of users in large-scale digital experiments. Building on recent
developments in causal inference and non/semi-parametric statistics, our
approach unifies two objectives: (1) the discovery of user segments that stand
to benefit from a candidate treatment based on subgroup-specific treatment
effects, and (2) the evaluation of causal impacts of dynamically assigning
units to a study's treatment arm based on their predicted segment-specific
benefit or harm. Our proposal is model-agnostic, capable of incorporating
state-of-the-art machine learning algorithms into the estimation procedure, and
is applicable in randomized A/B tests and quasi-experiments. An open source R
package implementation, sherlock, is introduced.

    

### [[2111.01225] Identifying causal associations in tweets using deep learning: Use case on diabetes-related tweets from 2017-2021](http://arxiv.org/abs/2111.01225)


  Objective: Leveraging machine learning methods, we aim to extract both
explicit and implicit cause-effect associations in patient-reported,
diabetes-related tweets and provide a tool to better understand opinion,
feelings and observations shared within the diabetes online community from a
causality perspective. Materials and Methods: More than 30 million
diabetes-related tweets in English were collected between April 2017 and
January 2021. Deep learning and natural language processing methods were
applied to focus on tweets with personal and emotional content. A
cause-effect-tweet dataset was manually labeled and used to train 1) a
fine-tuned Bertweet model to detect causal sentences containing a causal
association 2) a CRF model with BERT based features to extract possible
cause-effect associations. Causes and effects were clustered in a
semi-supervised approach and visualised in an interactive cause-effect-network.
Results: Causal sentences were detected with a recall of 68% in an imbalanced
dataset. A CRF model with BERT based features outperformed a fine-tuned BERT
model for cause-effect detection with a macro recall of 68%. This led to 96,676
sentences with cause-effect associations. "Diabetes" was identified as the
central cluster followed by "Death" and "Insulin". Insulin pricing related
causes were frequently associated with "Death". Conclusions: A novel
methodology was developed to detect causal sentences and identify both explicit
and implicit, single and multi-word cause and corresponding effect as expressed
in diabetes-related tweets leveraging BERT-based architectures and visualised
as cause-effect-network. Extracting causal associations on real-life, patient
reported outcomes in social media data provides a useful complementary source
of information in diabetes research.

    

### [[2111.01228] OPF-Learn: An Open-Source Framework for Creating Representative AC Optimal Power Flow Datasets](http://arxiv.org/abs/2111.01228)


  Increasing levels of renewable generation motivate a growing interest in
data-driven approaches for AC optimal power flow (AC OPF) to manage
uncertainty; however, a lack of disciplined dataset creation and benchmarking
prohibits useful comparison among approaches in the literature. To instill
confidence, models must be able to reliably predict solutions across a wide
range of operating conditions. This paper develops the OPF-Learn package for
Julia and Python, which uses a computationally efficient approach to create
representative datasets that span a wide spectrum of the AC OPF feasible
region. Load profiles are uniformly sampled from a convex set that contains the
AC OPF feasible set. For each infeasible point found, the convex set is reduced
using infeasibility certificates, found by using properties of a relaxed
formulation. The framework is shown to generate datasets that are more
representative of the entire feasible space versus traditional techniques seen
in the literature, improving machine learning model performance.

    

### [[2111.01235] Low-Cost Algorithmic Recourse for Users With Uncertain Cost Functions](http://arxiv.org/abs/2111.01235)


  The problem of identifying algorithmic recourse for people affected by
machine learning model decisions has received much attention recently. Some
recent works model user-incurred cost, which is directly linked to user
satisfaction. But they assume a single global cost function that is shared
across all users. This is an unrealistic assumption when users have dissimilar
preferences about their willingness to act upon a feature and different costs
associated with changing that feature. In this work, we formalize the notion of
user-specific cost functions and introduce a new method for identifying
actionable recourses for users. By default, we assume that users' cost
functions are hidden from the recourse method, though our framework allows
users to partially or completely specify their preferences or cost function. We
propose an objective function, Expected Minimum Cost (EMC), based on two key
ideas: (1) when presenting a set of options to a user, it is vital that there
is at least one low-cost solution the user could adopt; (2) when we do not know
the user's true cost function, we can approximately optimize for user
satisfaction by first sampling plausible cost functions, then finding a set
that achieves a good cost for the user in expectation. We optimize EMC with a
novel discrete optimization algorithm, Cost-Optimized Local Search (COLS),
which is guaranteed to improve the recourse set quality over iterations.
Experimental evaluation on popular real-world datasets with simulated user
costs demonstrates that our method satisfies up to 25.89 percentage points more
users compared to strong baseline methods. Using standard fairness metrics, we
also show that our method can provide more fair solutions across demographic
groups than comparable methods, and we verify that our method is robust to
misspecification of the cost function distribution.

    

### [[2111.01236] HRViT: Multi-Scale High-Resolution Vision Transformer](http://arxiv.org/abs/2111.01236)


  Vision transformers (ViTs) have attracted much attention for their superior
performance on computer vision tasks. To address their limitations of
single-scale low-resolution representations, prior work adapts ViTs to
high-resolution dense prediction tasks with hierarchical architectures to
generate pyramid features. However, multi-scale representation learning is
still under-explored on ViTs, given their classification-like sequential
topology. To enhance ViTs with more capability to learn semantically-rich and
spatially-precise multi-scale representations, in this work, we present an
efficient integration of high-resolution multi-branch architectures with vision
transformers, dubbed HRViT, pushing the Pareto front of dense prediction tasks
to a new level. We explore heterogeneous branch design, reduce the redundancy
in linear layers, and augment the model nonlinearity to balance the model
performance and hardware efficiency. The proposed HRViT achieves 50.20% mIoU on
ADE20K and 83.16% mIoU on Cityscapes for semantic segmentation tasks,
surpassing state-of-the-art MiT and CSWin with an average of +1.78 mIoU
improvement, 28% parameter reduction, and 21% FLOPs reduction, demonstrating
the potential of HRViT as strong vision backbones.

    

### [[2111.01243] Recent Advances in Natural Language Processing via Large Pre-Trained Language Models: A Survey](http://arxiv.org/abs/2111.01243)


  Large, pre-trained transformer-based language models such as BERT have
drastically changed the Natural Language Processing (NLP) field. We present a
survey of recent work that uses these large language models to solve NLP tasks
via pre-training then fine-tuning, prompting, or text generation approaches. We
also present approaches that use pre-trained language models to generate data
for training augmentation or other purposes. We conclude with discussions on
limitations and suggested directions for future research.

    

### [[2111.01245] Learning Eye-in-Hand Camera Calibration from a Single Image](http://arxiv.org/abs/2111.01245)


  Eye-in-hand camera calibration is a fundamental and long-studied problem in
robotics. We present a study on using learning-based methods for solving this
problem online from a single RGB image, whilst training our models with
entirely synthetic data. We study three main approaches: one direct regression
model that directly predicts the extrinsic matrix from an image, one sparse
correspondence model that regresses 2D keypoints and then uses PnP, and one
dense correspondence model that uses regressed depth and segmentation maps to
enable ICP pose estimation. In our experiments, we benchmark these methods
against each other and against well-established classical methods, to find the
surprising result that direct regression outperforms other approaches, and we
perform noise-sensitivity analysis to gain further insights into these results.

    

### [[2111.01256] Reverse engineering recurrent neural networks with Jacobian switching linear dynamical systems](http://arxiv.org/abs/2111.01256)


  Recurrent neural networks (RNNs) are powerful models for processing
time-series data, but it remains challenging to understand how they function.
Improving this understanding is of substantial interest to both the machine
learning and neuroscience communities. The framework of reverse engineering a
trained RNN by linearizing around its fixed points has provided insight, but
the approach has significant challenges. These include difficulty choosing
which fixed point to expand around when studying RNN dynamics and error
accumulation when reconstructing the nonlinear dynamics with the linearized
dynamics. We present a new model that overcomes these limitations by
co-training an RNN with a novel switching linear dynamical system (SLDS)
formulation. A first-order Taylor series expansion of the co-trained RNN and an
auxiliary function trained to pick out the RNN's fixed points govern the SLDS
dynamics. The results are a trained SLDS variant that closely approximates the
RNN, an auxiliary function that can produce a fixed point for each point in
state-space, and a trained nonlinear RNN whose dynamics have been regularized
such that its first-order terms perform the computation, if possible. This
model removes the post-training fixed point optimization and allows us to
unambiguously study the learned dynamics of the SLDS at any point in
state-space. It also generalizes SLDS models to continuous manifolds of
switching points while sharing parameters across switches. We validate the
utility of the model on two synthetic tasks relevant to previous work reverse
engineering RNNs. We then show that our model can be used as a drop-in in more
complex architectures, such as LFADS, and apply this LFADS hybrid to analyze
single-trial spiking activity from the motor system of a non-human primate.

    

### [[2111.01257] Implicit Model Specialization through DAG-based Decentralized Federated Learning](http://arxiv.org/abs/2111.01257)


  Federated learning allows a group of distributed clients to train a common
machine learning model on private data. The exchange of model updates is
managed either by a central entity or in a decentralized way, e.g. by a
blockchain. However, the strong generalization across all clients makes these
approaches unsuited for non-independent and identically distributed (non-IID)
data.
We propose a unified approach to decentralization and personalization in
federated learning that is based on a directed acyclic graph (DAG) of model
updates. Instead of training a single global model, clients specialize on their
local data while using the model updates from other clients dependent on the
similarity of their respective data. This specialization implicitly emerges
from the DAG-based communication and selection of model updates. Thus, we
enable the evolution of specialized models, which focus on a subset of the data
and therefore cover non-IID data better than federated learning in a
centralized or blockchain-based setup.
To the best of our knowledge, the proposed solution is the first to unite
personalization and poisoning robustness in fully decentralized federated
learning. Our evaluation shows that the specialization of models emerges
directly from the DAG-based communication of model updates on three different
datasets. Furthermore, we show stable model accuracy and less variance across
clients when compared to federated averaging.

    

### [[2111.01262] Minimax Optimization: The Case of Convex-Submodular](http://arxiv.org/abs/2111.01262)


  Minimax optimization has been central in addressing various applications in
machine learning, game theory, and control theory. Prior literature has thus
far mainly focused on studying such problems in the continuous domain, e.g.,
convex-concave minimax optimization is now understood to a significant extent.
Nevertheless, minimax problems extend far beyond the continuous domain to mixed
continuous-discrete domains or even fully discrete domains. In this paper, we
study mixed continuous-discrete minimax problems where the minimization is over
a continuous variable belonging to Euclidean space and the maximization is over
subsets of a given ground set. We introduce the class of convex-submodular
minimax problems, where the objective is convex with respect to the continuous
variable and submodular with respect to the discrete variable. Even though such
problems appear frequently in machine learning applications, little is known
about how to address them from algorithmic and theoretical perspectives. For
such problems, we first show that obtaining saddle points are hard up to any
approximation, and thus introduce new notions of (near-) optimality. We then
provide several algorithmic procedures for solving convex and
monotone-submodular minimax problems and characterize their convergence rates,
computational complexity, and quality of the final solution according to our
notions of optimally. Our proposed algorithms are iterative and combine tools
from both discrete and continuous optimization. Finally, we provide numerical
experiments to showcase the effectiveness of our purposed methods.

    

### [[2111.01264] Human-Level Control without Server-Grade Hardware](http://arxiv.org/abs/2111.01264)


  Deep Q-Network (DQN) marked a major milestone for reinforcement learning,
demonstrating for the first time that human-level control policies could be
learned directly from raw visual inputs via reward maximization. Even years
after its introduction, DQN remains highly relevant to the research community
since many of its innovations have been adopted by successor methods.
Nevertheless, despite significant hardware advances in the interim, DQN's
original Atari 2600 experiments remain costly to replicate in full. This poses
an immense barrier to researchers who cannot afford state-of-the-art hardware
or lack access to large-scale cloud computing resources. To facilitate improved
access to deep reinforcement learning research, we introduce a DQN
implementation that leverages a novel concurrent and synchronized execution
framework designed to maximally utilize a heterogeneous CPU-GPU desktop system.
With just one NVIDIA GeForce GTX 1080 GPU, our implementation reduces the
training time of a 200-million-frame Atari experiment from 25 hours to just 9
hours. The ideas introduced in our paper should be generalizable to a large
number of off-policy deep reinforcement learning methods.

    

### [[2111.01270] Deep learning of multi-resolution X-Ray micro-CT images for multi-scale modelling](http://arxiv.org/abs/2111.01270)


  There are inherent field-of-view and resolution trade-offs in X-Ray
micro-computed tomography imaging, which limit the characterization, analysis
and model development of multi-scale porous systems. In this paper, we overcome
these tradeoffs by developing a 3D Enhanced Deep Super Resolution (EDSR)
convolutional neural network to create enhanced, high-resolution data over
large spatial scales from low-resolution data. Paired high-resolution (HR,
2$\mu$m) and low resolution (LR, 6$\mu$m) image data from a Bentheimer rock
sample are used to train the network. Unseen LR and HR data from the training
sample, and another sample with a distinct micro-structure, are used to
validate the network with various metrics: textual analysis, segmentation
behaviour and pore-network model (PNM) multiphase flow simulations. The
validated EDSR network is used to generate ~1000 high-resolution REV subvolume
images for each full core sample of length 6-7cm (total image sizes are
~6000x6000x32000 voxels). Each subvolume has distinct petrophysical properties
predicted from PNMs, which are combined to create a 3D continuum-scale model of
each sample. Drainage immiscible flow at low capillary number is simulated
across a range of fractional flows and compared directly to experimental
pressures and 3D saturations on a 1:1 basis. The EDSR generated model is more
accurate than the base LR model at predicting experimental behaviour in the
presence of heterogeneities, especially in flow regimes where a wide
distribution of pore-sizes are encountered. The models are generally accurate
at predicting saturations to within the experimental repeatability and relative
permeability across three orders of magnitude. The demonstrated workflow is a
fully predictive, without calibration, and opens up the possibility to image,
simulate and analyse flow in truly multi-scale heterogeneous systems that are
otherwise intractable.

    

### [[2111.01271] Brain dynamics via Cumulative Auto-Regressive Self-Attention](http://arxiv.org/abs/2111.01271)


  Multivariate dynamical processes can often be intuitively described by a
weighted connectivity graph between components representing each individual
time-series. Even a simple representation of this graph as a Pearson
correlation matrix may be informative and predictive as demonstrated in the
brain imaging literature. However, there is a consensus expectation that
powerful graph neural networks (GNNs) should perform better in similar
settings. In this work, we present a model that is considerably shallow than
deep GNNs, yet outperforms them in predictive accuracy in a brain imaging
application. Our model learns the autoregressive structure of individual time
series and estimates directed connectivity graphs between the learned
representations via a self-attention mechanism in an end-to-end fashion. The
supervised training of the model as a classifier between patients and controls
results in a model that generates directed connectivity graphs and highlights
the components of the time-series that are predictive for each subject. We
demonstrate our results on a functional neuroimaging dataset classifying
schizophrenia patients and controls.

    

### [[2111.01272] Sequence Transduction with Graph-based Supervision](http://arxiv.org/abs/2111.01272)


  The recurrent neural network transducer (RNN-T) objective plays a major role
in building today's best automatic speech recognition (ASR) systems for
production. Similarly to the connectionist temporal classification (CTC)
objective, the RNN-T loss uses specific rules that define how a set of
alignments is generated to form a lattice for the full-sum training. However,
it is yet largely unknown if these rules are optimal and do lead to the best
possible ASR results. In this work, we present a new transducer objective
function that generalizes the RNN-T loss to accept a graph representation of
the labels, thus providing a flexible and efficient framework to manipulate
training lattices, for example for restricting alignments or studying different
transition rules. We demonstrate that transducer-based ASR with CTC-like
lattice achieves better results compared to standard RNN-T, while also ensuring
a strictly monotonic alignment, which will allow better optimization of the
decoding procedure. For example, the proposed CTC-like transducer system
achieves a word error rate of 5.9% for the test-other condition of LibriSpeech,
corresponding to an improvement of 4.8% relative to an equivalent RNN-T based
system.

    

### [[2111.01273] Network Clustering for Latent State and Changepoint Detection](http://arxiv.org/abs/2111.01273)


  Network models provide a powerful and flexible framework for analyzing a wide
range of structured data sources. In many situations of interest, however,
multiple networks can be constructed to capture different aspects of an
underlying phenomenon or to capture changing behavior over time. In such
settings, it is often useful to cluster together related networks in attempt to
identify patterns of common structure. In this paper, we propose a convex
approach for the task of network clustering. Our approach uses a convex fusion
penalty to induce a smoothly-varying tree-like cluster structure, eliminating
the need to select the number of clusters a priori. We provide an efficient
algorithm for convex network clustering and demonstrate its effectiveness on
synthetic examples.

    

### [[2111.01276] Multi network InfoMax: A pre-training method involving graph convolutional networks](http://arxiv.org/abs/2111.01276)


  Discovering distinct features and their relations from data can help us
uncover valuable knowledge crucial for various tasks, e.g., classification. In
neuroimaging, these features could help to understand, classify, and possibly
prevent brain disorders. Model introspection of highly performant
overparameterized deep learning (DL) models could help find these features and
relations. However, to achieve high-performance level DL models require
numerous labeled training samples ($n$) rarely available in many fields. This
paper presents a pre-training method involving graph convolutional/neural
networks (GCNs/GNNs), based on maximizing mutual information between two
high-level embeddings of an input sample. Many of the recently proposed
pre-training methods pre-train one of many possible networks of an
architecture. Since almost every DL model is an ensemble of multiple networks,
we take our high-level embeddings from two different networks of a model --a
convolutional and a graph network--. The learned high-level graph latent
representations help increase performance for downstream graph classification
tasks and bypass the need for a high number of labeled data samples. We apply
our method to a neuroimaging dataset for classifying subjects into healthy
control (HC) and schizophrenia (SZ) groups. Our experiments show that the
pre-trained model significantly outperforms the non-pre-trained model and
requires $50\%$ less data for similar performance.

    

### [[2111.01294] Learning to Operate an Electric Vehicle Charging Station Considering Vehicle-grid Integration](http://arxiv.org/abs/2111.01294)


  The rapid adoption of electric vehicles (EVs) calls for the widespread
installation of EV charging stations. To maximize the profitability of charging
stations, intelligent controllers that provide both charging and electric grid
services are in great need. However, it is challenging to determine the optimal
charging schedule due to the uncertain arrival time and charging demands of
EVs. In this paper, we propose a novel centralized allocation and decentralized
execution (CADE) reinforcement learning (RL) framework to maximize the charging
station's profit. In the centralized allocation process, EVs are allocated to
either the waiting or charging spots. In the decentralized execution process,
each charger makes its own charging/discharging decision while learning the
action-value functions from a shared replay memory. This CADE framework
significantly improves the scalability and sample efficiency of the RL
algorithm. Numerical results show that the proposed CADE framework is both
computationally efficient and scalable, and significantly outperforms the
baseline model predictive control (MPC). We also provide an in-depth analysis
of the learned action-value function to explain the inner working of the
reinforcement learning agent.

    

### [[2111.01297] Deep neural networks as nested dynamical systems](http://arxiv.org/abs/2111.01297)


  There is an analogy that is often made between deep neural networks and
actual brains, suggested by the nomenclature itself: the "neurons" in deep
neural networks should correspond to neurons (or nerve cells, to avoid
confusion) in the brain. We claim, however, that this analogy doesn't even type
check: it is structurally flawed. In agreement with the slightly glib summary
of Hebbian learning as "cells that fire together wire together", this article
makes the case that the analogy should be different. Since the "neurons" in
deep neural networks are managing the changing weights, they are more akin to
the synapses in the brain; instead, it is the wires in deep neural networks
that are more like nerve cells, in that they are what cause the information to
flow. An intuition that nerve cells seem like more than mere wires is exactly
right, and is justified by a precise category-theoretic analogy which we will
explore in this article. Throughout, we will continue to highlight the error in
equating artificial neurons with nerve cells by leaving "neuron" in quotes or
by calling them artificial neurons.
We will first explain how to view deep neural networks as nested dynamical
systems with a very restricted sort of interaction pattern, and then explain a
more general sort of interaction for dynamical systems that is useful
throughout engineering, but which fails to adapt to changing circumstances. As
mentioned, an analogy is then forced upon us by the mathematical formalism in
which they are both embedded. We call the resulting encompassing generalization
deeply interacting learning systems: they have complex interaction as in
control theory, but adaptation to circumstances as in deep neural networks.

    

### [[2111.01322] Diverse Distributions of Self-Supervised Tasks for Meta-Learning in NLP](http://arxiv.org/abs/2111.01322)


  Meta-learning considers the problem of learning an efficient learning process
that can leverage its past experience to accurately solve new tasks. However,
the efficacy of meta-learning crucially depends on the distribution of tasks
available for training, and this is often assumed to be known a priori or
constructed from limited supervised datasets. In this work, we aim to provide
task distributions for meta-learning by considering self-supervised tasks
automatically proposed from unlabeled text, to enable large-scale meta-learning
in NLP. We design multiple distributions of self-supervised tasks by
considering important aspects of task diversity, difficulty, type, domain, and
curriculum, and investigate how they affect meta-learning performance. Our
analysis shows that all these factors meaningfully alter the task distribution,
some inducing significant improvements in downstream few-shot accuracy of the
meta-learned models. Empirically, results on 20 downstream tasks show
significant improvements in few-shot learning -- adding up to +4.2% absolute
accuracy (on average) to the previous unsupervised meta-learning method, and
perform comparably to supervised methods on the FewRel 2.0 benchmark.

    

### [[2111.01348] Faster Convex Lipschitz Regression via 2-block ADMM](http://arxiv.org/abs/2111.01348)


  The task of approximating an arbitrary convex function arises in several
learning problems such as convex regression, learning with a difference of
convex (DC) functions, and approximating Bregman divergences. In this paper, we
show how a broad class of convex function learning problems can be solved via a
2-block ADMM approach, where updates for each block can be computed in closed
form. For the task of convex Lipschitz regression, we establish that our
proposed algorithm converges at the rate of $O(n^3 d^{1.5}+n^2 d^{2.5}+n d^3)$
for a dataset $X \in R^{n\times d}$. This new rate improves the state of the
art $O(n^5d^2$) available by interior point methods if $d = o( n^4)$. Further
we provide similar solvers for DC regression and Bregman divergence learning.
Unlike previous approaches, our method is amenable to the use of GPUs. We
demonstrate on regression and metric learning experiments that our approach is
up to 20 times faster than the existing method, and produces results that are
comparable to state-of-the-art.

    

### [[2111.01351] Major Depressive Disorder Recognition and Cognitive Analysis Based on Multi-layer Brain Functional Connectivity Networks](http://arxiv.org/abs/2111.01351)


  On the increase of major depressive disorders (MDD), many researchers paid
attention to their recognition and treatment. Existing MDD recognition
algorithms always use a single time-frequency domain method method, but the
single time-frequency domain method is too simple and is not conducive to
simulating the complex link relationship between brain functions. To solve this
problem, this paper proposes a recognition method based on multi-layer brain
functional connectivity networks (MBFCN) for major depressive disorder and
conducts cognitive analysis. Cognitive analysis based on the proposed MBFCN
finds that the Alpha-Beta1 frequency band is the key sub-band for recognizing
MDD. The connections between the right prefrontal lobe and the temporal lobe of
the extremely depressed disorders (EDD) are deficient in the brain functional
connectivity networks (BFCN) based on phase lag index (PLI). Furthermore,
potential biomarkers by the significance analysis of depression features and
PHQ-9 can be found.

    

### [[2111.01353] Can Vision Transformers Perform Convolution?](http://arxiv.org/abs/2111.01353)


  Several recent studies have demonstrated that attention-based networks, such
as Vision Transformer (ViT), can outperform Convolutional Neural Networks
(CNNs) on several computer vision tasks without using convolutional layers.
This naturally leads to the following questions: Can a self-attention layer of
ViT express any convolution operation? In this work, we prove that a single ViT
layer with image patches as the input can perform any convolution operation
constructively, where the multi-head attention mechanism and the relative
positional encoding play essential roles. We further provide a lower bound on
the number of heads for Vision Transformers to express CNNs. Corresponding with
our analysis, experimental results show that the construction in our proof can
help inject convolutional bias into Transformers and significantly improve the
performance of ViT in low data regimes.

    

### [[2111.01356] DeepParticle: learning invariant measure by a deep neural network minimizing Wasserstein distance on data generated from an interacting particle method](http://arxiv.org/abs/2111.01356)


  We introduce the so called DeepParticle method to learn and generate
invariant measures of stochastic dynamical systems with physical parameters
based on data computed from an interacting particle method (IPM). We utilize
the expressiveness of deep neural networks (DNNs) to represent the transform of
samples from a given input (source) distribution to an arbitrary target
distribution, neither assuming distribution functions in closed form nor a
finite state space for the samples. In training, we update the network weights
to minimize a discrete Wasserstein distance between the input and target
samples. To reduce computational cost, we propose an iterative
divide-and-conquer (a mini-batch interior point) algorithm, to find the optimal
transition matrix in the Wasserstein distance. We present numerical results to
demonstrate the performance of our method for accelerating IPM computation of
invariant measures of stochastic dynamical systems arising in computing
reaction-diffusion front speeds through chaotic flows. The physical parameter
is a large Peclét number reflecting the advection dominated regime of our
interest.

    

### [[2111.01361] Outlier-Robust Optimal Transport: Duality, Structure, and Statistical Applications](http://arxiv.org/abs/2111.01361)


  The Wasserstein distance, rooted in optimal transport (OT) theory, is a
popular discrepancy measure between probability distributions with various
applications to statistics and machine learning. Despite their rich structure
and demonstrated utility, Wasserstein distances are sensitive to outliers in
the considered distributions, which hinders applicability in practice. Inspired
by the Huber contamination model, we propose a new outlier-robust Wasserstein
distance $\mathsf{W}_p^\varepsilon$ which allows for $\varepsilon$ outlier mass
to be removed from each contaminated distribution. Our formulation amounts to a
highly regular optimization problem that lends itself better for analysis
compared to previously considered frameworks. Leveraging this, we conduct a
thorough theoretical study of $\mathsf{W}_p^\varepsilon$, encompassing
characterization of optimal perturbations, regularity, duality, and statistical
estimation and robustness results. In particular, by decoupling the
optimization variables, we arrive at a simple dual form for
$\mathsf{W}_p^\varepsilon$ that can be implemented via an elementary
modification to standard, duality-based OT solvers. We illustrate the benefits
of our framework via applications to generative modeling with contaminated
datasets.

    

### [[2111.01363] Knowledge Cross-Distillation for Membership Privacy](http://arxiv.org/abs/2111.01363)


  A membership inference attack (MIA) poses privacy risks on the training data
of a machine learning model. With an MIA, an attacker guesses if the target
data are a member of the training dataset. The state-of-the-art defense against
MIAs, distillation for membership privacy (DMP), requires not only private data
to protect but a large amount of unlabeled public data. However, in certain
privacy-sensitive domains, such as medical and financial, the availability of
public data is not obvious. Moreover, a trivial method to generate the public
data by using generative adversarial networks significantly decreases the model
accuracy, as reported by the authors of DMP. To overcome this problem, we
propose a novel defense against MIAs using knowledge distillation without
requiring public data. Our experiments show that the privacy protection and
accuracy of our defense are comparable with those of DMP for the benchmark
tabular datasets used in MIA researches, Purchase100 and Texas100, and our
defense has much better privacy-utility trade-off than those of the existing
defenses without using public data for image dataset CIFAR10.

    

### [[2111.01365] Koopman Q-learning: Offline Reinforcement Learning via Symmetries of Dynamics](http://arxiv.org/abs/2111.01365)


  Offline reinforcement learning leverages large datasets to train policies
without interactions with the environment. The learned policies may then be
deployed in real-world settings where interactions are costly or dangerous.
Current algorithms over-fit to the training dataset and as a consequence
perform poorly when deployed to out-of-distribution generalizations of the
environment. We aim to address these limitations by learning a Koopman latent
representation which allows us to infer symmetries of the system's underlying
dynamic. The latter is then utilized to extend the otherwise static offline
dataset during training; this constitutes a novel data augmentation framework
which reflects the system's dynamic and is thus to be interpreted as an
exploration of the environments phase space. To obtain the symmetries we employ
Koopman theory in which nonlinear dynamics are represented in terms of a linear
operator acting on the space of measurement functions of the system and thus
symmetries of the dynamics may be inferred directly. We provide novel
theoretical results on the existence and nature of symmetries relevant for
control systems such as reinforcement learning settings. Moreover, we
empirically evaluate our method on several benchmark offline reinforcement
learning tasks and datasets including D4RL, Metaworld and Robosuite and find
that by using our framework we consistently improve the state-of-the-art for
Q-learning methods.

    

### [[2111.01370] FedGraph: Federated Graph Learning with Intelligent Sampling](http://arxiv.org/abs/2111.01370)


  Federated learning has attracted much research attention due to its privacy
protection in distributed machine learning. However, existing work of federated
learning mainly focuses on Convolutional Neural Network (CNN), which cannot
efficiently handle graph data that are popular in many applications. Graph
Convolutional Network (GCN) has been proposed as one of the most promising
techniques for graph learning, but its federated setting has been seldom
explored. In this paper, we propose FedGraph for federated graph learning among
multiple computing clients, each of which holds a subgraph. FedGraph provides
strong graph learning capability across clients by addressing two unique
challenges. First, traditional GCN training needs feature data sharing among
clients, leading to risk of privacy leakage. FedGraph solves this issue using a
novel cross-client convolution operation. The second challenge is high GCN
training overhead incurred by large graph size. We propose an intelligent graph
sampling algorithm based on deep reinforcement learning, which can
automatically converge to the optimal sampling policies that balance training
speed and accuracy. We implement FedGraph based on PyTorch and deploy it on a
testbed for performance evaluation. The experimental results of four popular
datasets demonstrate that FedGraph significantly outperforms existing work by
enabling faster convergence to higher accuracy.

    

### [[2111.01383] A Comparative Analysis of Machine Learning Algorithms for Intrusion Detection in Edge-Enabled IoT Networks](http://arxiv.org/abs/2111.01383)


  A significant increase in the number of interconnected devices and data
communication through wireless networks has given rise to various threats,
risks and security concerns. Internet of Things (IoT) applications is deployed
in almost every field of daily life, including sensitive environments. The edge
computing paradigm has complemented IoT applications by moving the
computational processing near the data sources. Among various security models,
Machine Learning (ML) based intrusion detection is the most conceivable defense
mechanism to combat the anomalous behavior in edge-enabled IoT networks. The ML
algorithms are used to classify the network traffic into normal and malicious
attacks. Intrusion detection is one of the challenging issues in the area of
network security. The research community has proposed many intrusion detection
systems. However, the challenges involved in selecting suitable algorithm(s) to
provide security in edge-enabled IoT networks exist. In this paper, a
comparative analysis of conventional machine learning classification algorithms
has been performed to categorize the network traffic on NSL-KDD dataset using
Jupyter on Pycharm tool. It can be observed that Multi-Layer Perception (MLP)
has dependencies between input and output and relies more on network
configuration for intrusion detection. Therefore, MLP can be more appropriate
for edge-based IoT networks with a better training time of 1.2 seconds and
testing accuracy of 79%.

    

### [[2111.01387] Understanding Entropic Regularization in GANs](http://arxiv.org/abs/2111.01387)


  Generative Adversarial Networks are a popular method for learning
distributions from data by modeling the target distribution as a function of a
known distribution. The function, often referred to as the generator, is
optimized to minimize a chosen distance measure between the generated and
target distributions. One commonly used measure for this purpose is the
Wasserstein distance. However, Wasserstein distance is hard to compute and
optimize, and in practice entropic regularization techniques are used to
improve numerical convergence. The influence of regularization on the learned
solution, however, remains not well-understood. In this paper, we study how
several popular entropic regularizations of Wasserstein distance impact the
solution in a simple benchmark setting where the generator is linear and the
target distribution is high-dimensional Gaussian. We show that entropy
regularization promotes the solution sparsification, while replacing the
Wasserstein distance with the Sinkhorn divergence recovers the unregularized
solution. Both regularization techniques remove the curse of dimensionality
suffered by Wasserstein distance. We show that the optimal generator can be
learned to accuracy $\epsilon$ with $O(1/\epsilon^2)$ samples from the target
distribution. We thus conclude that these regularization techniques can improve
the quality of the generator learned from empirical data for a large class of
distributions.

    

### [[2111.01392] Overlapping and nonoverlapping models](http://arxiv.org/abs/2111.01392)


  Consider a directed network with $K_{r}$ row communities and $K_{c}$ column
communities. Previous works found that modeling directed networks in which all
nodes have overlapping property requires $K_{r}=K_{c}$ for identifiability. In
this paper, we propose an overlapping and nonoverlapping model to study
directed networks in which row nodes have overlapping property while column
nodes do not. The proposed model is identifiable when $K_{r}\leq K_{c}$.
Meanwhile, we provide one identifiable model as extension of ONM to model
directed networks with variation in node degree. Two spectral algorithms with
theoretical guarantee on consistent estimations are designed to fit the models.
A small scale of numerical studies are used to illustrate the algorithms.

    

### [[2111.01393] Time Series Comparisons in Deep Space Network](http://arxiv.org/abs/2111.01393)


  The Deep Space Network is NASA's international array of antennas that support
interplanetary spacecraft missions. A track is a block of multi-dimensional
time series from the beginning to end of DSN communication with the target
spacecraft, containing thousands of monitor data items lasting several hours at
a frequency of 0.2-1Hz. Monitor data on each track reports on the performance
of specific spacecraft operations and the DSN itself. DSN is receiving signals
from 32 spacecraft across the solar system. DSN has pressure to reduce costs
while maintaining the quality of support for DSN mission users. DSN Link
Control Operators need to simultaneously monitor multiple tracks and identify
anomalies in real time. DSN has seen that as the number of missions increases,
the data that needs to be processed increases over time. In this project, we
look at the last 8 years of data for analysis. Any anomaly in the track
indicates a problem with either the spacecraft, DSN equipment, or weather
conditions. DSN operators typically write Discrepancy Reports for further
analysis. It is recognized that it would be quite helpful to identify 10
similar historical tracks out of the huge database to quickly find and match
anomalies. This tool has three functions: (1) identification of the top 10
similar historical tracks, (2) detection of anomalies compared to the reference
normal track, and (3) comparison of statistical differences between two given
tracks. The requirements for these features were confirmed by survey responses
from 21 DSN operators and engineers. The preliminary machine learning model has
shown promising performance (AUC=0.92). We plan to increase the number of data
sets and perform additional testing to improve performance further before its
planned integration into the track visualizer interface to assist DSN field
operators and engineers.

    

### [[2111.01394] Solving Partial Differential Equations with Point Source Based on Physics-Informed Neural Networks](http://arxiv.org/abs/2111.01394)


  In recent years, deep learning technology has been used to solve partial
differential equations (PDEs), among which the physics-informed neural networks
(PINNs) emerges to be a promising method for solving both forward and inverse
PDE problems. PDEs with a point source that is expressed as a Dirac delta
function in the governing equations are mathematical models of many physical
processes. However, they cannot be solved directly by conventional PINNs method
due to the singularity brought by the Dirac delta function. We propose a
universal solution to tackle this problem with three novel techniques. Firstly
the Dirac delta function is modeled as a continuous probability density
function to eliminate the singularity; secondly a lower bound constrained
uncertainty weighting algorithm is proposed to balance the PINNs losses between
point source area and other areas; and thirdly a multi-scale deep neural
network with periodic activation function is used to improve the accuracy and
convergence speed of the PINNs method. We evaluate the proposed method with
three representative PDEs, and the experimental results show that our method
outperforms existing deep learning-based methods with respect to the accuracy,
the efficiency and the versatility.

    

### [[2111.01395] Training Certifiably Robust Neural Networks with Efficient Local Lipschitz Bounds](http://arxiv.org/abs/2111.01395)


  Certified robustness is a desirable property for deep neural networks in
safety-critical applications, and popular training algorithms can certify
robustness of a neural network by computing a global bound on its Lipschitz
constant. However, such a bound is often loose: it tends to over-regularize the
neural network and degrade its natural accuracy. A tighter Lipschitz bound may
provide a better tradeoff between natural and certified accuracy, but is
generally hard to compute exactly due to non-convexity of the network. In this
work, we propose an efficient and trainable \emph{local} Lipschitz upper bound
by considering the interactions between activation functions (e.g. ReLU) and
weight matrices. Specifically, when computing the induced norm of a weight
matrix, we eliminate the corresponding rows and columns where the activation
function is guaranteed to be a constant in the neighborhood of each given data
point, which provides a provably tighter bound than the global Lipschitz
constant of the neural network. Our method can be used as a plug-in module to
tighten the Lipschitz bound in many certifiable training algorithms.
Furthermore, we propose to clip activation functions (e.g., ReLU and MaxMin)
with a learnable upper threshold and a sparsity loss to assist the network to
achieve an even tighter local Lipschitz bound. Experimentally, we show that our
method consistently outperforms state-of-the-art methods in both clean and
certified accuracy on MNIST, CIFAR-10 and TinyImageNet datasets with various
network architectures.

    

### [[2111.01409] Efficient Learning of the Parameters of Non-Linear Models using Differentiable Resampling in Particle Filters](http://arxiv.org/abs/2111.01409)


  It has been widely documented that the sampling and resampling steps in
particle filters cannot be differentiated. The {\itshape reparameterisation
trick} was introduced to allow the sampling step to be reformulated into a
differentiable function. We extend the {\itshape reparameterisation trick} to
include the stochastic input to resampling therefore limiting the
discontinuities in the gradient calculation after this step. Knowing the
gradients of the prior and likelihood allows us to run particle Markov Chain
Monte Carlo (p-MCMC) and use the No-U-Turn Sampler (NUTS) as the proposal when
estimating parameters.
We compare the Metropolis-adjusted Langevin algorithm (MALA), Hamiltonian
Monte Carlo with different number of steps and NUTS. We consider two
state-space models and show that NUTS improves the mixing of the Markov chain
and can produce more accurate results in less computational time.

    

### [[2111.01432] Practical and Light-weight Secure Aggregation for Federated Submodel Learning](http://arxiv.org/abs/2111.01432)


  Recently, Niu, et. al. introduced a new variant of Federated Learning (FL),
called Federated Submodel Learning (FSL). Different from traditional FL, each
client locally trains the submodel (e.g., retrieved from the servers) based on
its private data and uploads a submodel at its choice to the servers. Then all
clients aggregate all their submodels and finish the iteration. Inevitably, FSL
introduces two privacy-preserving computation tasks, i.e., Private Submodel
Retrieval (PSR) and Secure Submodel Aggregation (SSA). Existing work fails to
provide a loss-less scheme, or has impractical efficiency. In this work, we
leverage Distributed Point Function (DPF) and cuckoo hashing to construct a
practical and light-weight secure FSL scheme in the two-server setting. More
specifically, we propose two basic protocols with few optimisation techniques,
which ensures our protocol practicality on specific real-world FSL tasks. Our
experiments show that our proposed protocols can finish in less than 1 minute
when weight sizes $\leq 2^{15}$, we also demonstrate protocol efficiency by
comparing with existing work and by handling a real-world FSL task.

    

### [[2111.01436] Learning Size and Shape of Calabi-Yau Spaces](http://arxiv.org/abs/2111.01436)


  We present a new machine learning library for computing metrics of string
compactification spaces. We benchmark the performance on Monte-Carlo sampled
integrals against previous numerical approximations and find that our neural
networks are more sample- and computation-efficient. We are the first to
provide the possibility to compute these metrics for arbitrary, user-specified
shape and size parameters of the compact space and observe a linear relation
between optimization of the partial differential equation we are training
against and vanishing Ricci curvature.

    

### [[2111.01456] WaveSense: Efficient Temporal Convolutions with Spiking Neural Networks for Keyword Spotting](http://arxiv.org/abs/2111.01456)


  Ultra-low power local signal processing is a crucial aspect for edge
applications on always-on devices. Neuromorphic processors emulating spiking
neural networks show great computational power while fulfilling the limited
power budget as needed in this domain. In this work we propose spiking neural
dynamics as a natural alternative to dilated temporal convolutions. We extend
this idea to WaveSense, a spiking neural network inspired by the WaveNet
architecture. WaveSense uses simple neural dynamics, fixed time-constants and a
simple feed-forward architecture and hence is particularly well suited for a
neuromorphic implementation. We test the capabilities of this model on several
datasets for keyword-spotting. The results show that the proposed network beats
the state of the art of other spiking neural networks and reaches near
state-of-the-art performance of artificial neural networks such as CNNs and
LSTMs.

    

### [[2111.01457] Synthesizing Speech from Intracranial Depth Electrodes using an Encoder-Decoder Framework](http://arxiv.org/abs/2111.01457)


  Speech Neuroprostheses have the potential to enable communication for people
with dysarthria or anarthria. Recent advances have demonstrated high-quality
text decoding and speech synthesis from electrocorticographic grids placed on
the cortical surface. Here, we investigate a less invasive measurement
modality, namely stereotactic EEG (sEEG) that provides sparse sampling from
multiple brain regions, including subcortical regions. To evaluate whether sEEG
can also be used to synthesize high-quality audio from neural recordings, we
employ a recurrent encoder-decoder framework based on modern deep learning
methods. We demonstrate that high-quality speech can be reconstructed from
these minimally invasive recordings, despite a limited amount of training data.
Finally, we utilize variational feature dropout to successfully identify the
most informative electrode contacts.

    

### [[2111.01460] Geometry-aware Bayesian Optimization in Robotics using Riemannian Matérn Kernels](http://arxiv.org/abs/2111.01460)


  Bayesian optimization is a data-efficient technique which can be used for
control parameter tuning, parametric policy adaptation, and structure design in
robotics. Many of these problems require optimization of functions defined on
non-Euclidean domains like spheres, rotation groups, or spaces of
positive-definite matrices. To do so, one must place a Gaussian process prior,
or equivalently define a kernel, on the space of interest. Effective kernels
typically reflect the geometry of the spaces they are defined on, but designing
them is generally non-trivial. Recent work on the Riemannian Matérn kernels,
based on stochastic partial differential equations and spectral theory of the
Laplace-Beltrami operator, offers promising avenues towards constructing such
geometry-aware kernels. In this paper, we study techniques for implementing
these kernels on manifolds of interest in robotics, demonstrate their
performance on a set of artificial benchmark functions, and illustrate
geometry-aware Bayesian optimization for a variety of robotic applications,
covering orientation control, manipulability optimization, and motion planning,
while showing its improved performance.

    

### [[2111.01471] Zero-Shot Translation using Diffusion Models](http://arxiv.org/abs/2111.01471)


  In this work, we show a novel method for neural machine translation (NMT),
using a denoising diffusion probabilistic model (DDPM), adjusted for textual
data, following recent advances in the field. We show that it's possible to
translate sentences non-autoregressively using a diffusion model conditioned on
the source sentence. We also show that our model is able to translate between
pairs of languages unseen during training (zero-shot learning).

    

### [[2111.01479] Dealing With Misspecification In Fixed-Confidence Linear Top-m Identification](http://arxiv.org/abs/2111.01479)


  We study the problem of the identification of m arms with largest means under
a fixed error rate $\delta$ (fixed-confidence Top-m identification), for
misspecified linear bandit models. This problem is motivated by practical
applications, especially in medicine and recommendation systems, where linear
models are popular due to their simplicity and the existence of efficient
algorithms, but in which data inevitably deviates from linearity. In this work,
we first derive a tractable lower bound on the sample complexity of any
$\delta$-correct algorithm for the general Top-m identification problem. We
show that knowing the scale of the deviation from linearity is necessary to
exploit the structure of the problem. We then describe the first algorithm for
this setting, which is both practical and adapts to the amount of
misspecification. We derive an upper bound to its sample complexity which
confirms this adaptivity and that matches the lower bound when $\delta$
$\rightarrow$ 0. Finally, we evaluate our algorithm on both synthetic and
real-world data, showing competitive performance with respect to existing
baselines.

    

### [[2111.01480] Variational message passing (VMP) applied to LDA](http://arxiv.org/abs/2111.01480)


  Variational Bayes (VB) applied to latent Dirichlet allocation (LDA) is the
original inference mechanism for LDA. Many variants of VB for LDA, as well as
for VB in general, have been developed since LDA's inception in 2013, but
standard VB is still widely applied to LDA. Variational message passing (VMP)
is the message passing equivalent of VB and is a useful tool for constructing a
variational inference solution for a large variety of conjugate exponential
graphical models (there is also a non conjugate variant available for other
models). In this article we present the VMP equations for LDA and also provide
a brief discussion of the equations. We hope that this will assist others when
deriving variational inference solutions to other similar graphical models.

    

### [[2111.01482] DAGSurv: Directed Acyclic Graph Based Survival Analysis Using Deep Neural Networks](http://arxiv.org/abs/2111.01482)


  Causal structures for observational survival data provide crucial information
regarding the relationships between covariates and time-to-event. We derive
motivation from the information theoretic source coding argument, and show that
incorporating the knowledge of the directed acyclic graph (DAG) can be
beneficial if suitable source encoders are employed. As a possible source
encoder in this context, we derive a variational inference based conditional
variational autoencoder for causal structured survival prediction, which we
refer to as DAGSurv. We illustrate the performance of DAGSurv on low and
high-dimensional synthetic datasets, and real-world datasets such as METABRIC
and GBSG. We demonstrate that the proposed method outperforms other survival
analysis baselines such as Cox Proportional Hazards, DeepSurv and Deephit,
which are oblivious to the underlying causal relationship between data
entities.

    

### [[2111.01495] Constructing Neural Network-Based Models for Simulating Dynamical Systems](http://arxiv.org/abs/2111.01495)


  Dynamical systems see widespread use in natural sciences like physics,
biology, chemistry, as well as engineering disciplines such as circuit
analysis, computational fluid dynamics, and control. For simple systems, the
differential equations governing the dynamics can be derived by applying
fundamental physical laws. However, for more complex systems, this approach
becomes exceedingly difficult. Data-driven modeling is an alternative paradigm
that seeks to learn an approximation of the dynamics of a system using
observations of the true system. In recent years, there has been an increased
interest in data-driven modeling techniques, in particular neural networks have
proven to provide an effective framework for solving a wide range of tasks.
This paper provides a survey of the different ways to construct models of
dynamical systems using neural networks. In addition to the basic overview, we
review the related literature and outline the most significant challenges from
numerical simulations that this modeling paradigm must overcome. Based on the
reviewed literature and identified challenges, we provide a discussion on
promising research areas.

    

### [[2111.01516] FedFly: Towards Migration in Edge-based Distributed Federated Learning](http://arxiv.org/abs/2111.01516)


  Federated learning (FL) is a privacy-preserving distributed machine learning
technique that trains models without having direct access to the original data
generated on devices. Since devices may be resource constrained, offloading can
be used to improve FL performance by transferring computational workload from
devices to edge servers. However, due to mobility, devices participating in FL
may leave the network during training and need to connect to a different edge
server. This is challenging because the offloaded computations from edge server
need to be migrated. In line with this assertion, we present FedFly, which is,
to the best of our knowledge, the first work to migrate a deep neural network
(DNN) when devices move between edge servers during FL training. Our empirical
results on the CIFAR-10 dataset, with both balanced and imbalanced data
distribution support our claims that FedFly can reduce training time by up to
33% when a device moves after 50% of the training is completed, and by up to
45% when 90% of the training is completed when compared to state-of-the-art
offloading approach in FL. FedFly has negligible overhead of 2 seconds and does
not compromise accuracy. Finally, we highlight a number of open research issues
for further investigation. FedFly can be downloaded from
this https URL


### [[2111.01531] Generating synthetic transactional profiles](http://arxiv.org/abs/2111.01531)


  Financial institutions use clients' payment transactions in numerous banking
applications. Transactions are very personal and rich in behavioural patterns,
often unique to individuals, which make them equivalent to personally
identifiable information in some cases. In this paper, we generate synthetic
transactional profiles using machine learning techniques with the goal to
preserve both data utility and privacy. A challenge we faced was to deal with
sparse vectors due to the few spending categories a client uses compared to all
the ones available. We measured data utility by calculating common insights
used by the banking industry on both the original and the synthetic data-set.
Our approach shows that neural network models can generate valuable synthetic
data in such context. Finally, we tried privacy-preserving techniques and
observed its effect on models' performances.

    

### [[2111.01533] A comparison of mixed-variables Bayesian optimization approaches](http://arxiv.org/abs/2111.01533)


  Most real optimization problems are defined over a mixed search space where
the variables are both discrete and continuous. In engineering applications,
the objective function is typically calculated with a numerically costly
black-box simulation.General mixed and costly optimization problems are
therefore of a great practical interest, yet their resolution remains in a
large part an open scientific question. In this article, costly mixed problems
are approached through Gaussian processes where the discrete variables are
relaxed into continuous latent variables. The continuous space is more easily
harvested by classical Bayesian optimization techniques than a mixed space
would. Discrete variables are recovered either subsequently to the continuous
optimization, or simultaneously with an additional continuous-discrete
compatibility constraint that is handled with augmented Lagrangians. Several
possible implementations of such Bayesian mixed optimizers are compared. In
particular, the reformulation of the problem with continuous latent variables
is put in competition with searches working directly in the mixed space. Among
the algorithms involving latent variables and an augmented Lagrangian, a
particular attention is devoted to the Lagrange multipliers for which a local
and a global estimation techniques are studied. The comparisons are based on
the repeated optimization of three analytical functions and a beam design
problem.

    

### [[2111.01536] Learning Circular Hidden Quantum Markov Models: A Tensor Network Approach](http://arxiv.org/abs/2111.01536)


  In this paper, we propose circular Hidden Quantum Markov Models (c-HQMMs),
which can be applied for modeling temporal data in quantum datasets (with
classical datasets as a special case). We show that c-HQMMs are equivalent to a
constrained tensor network (more precisely, circular Local Purified State with
positive-semidefinite decomposition) model. This equivalence enables us to
provide an efficient learning model for c-HQMMs. The proposed learning approach
is evaluated on six real datasets and demonstrates the advantage of c-HQMMs on
multiple datasets as compared to HQMMs, circular HMMs, and HMMs.

    

### [[2111.01549] Overcoming Catastrophic Forgetting in Incremental Few-Shot Learning by Finding Flat Minima](http://arxiv.org/abs/2111.01549)


  This paper considers incremental few-shot learning, which requires a model to
continually recognize new categories with only a few examples provided. Our
study shows that existing methods severely suffer from catastrophic forgetting,
a well-known problem in incremental learning, which is aggravated due to data
scarcity and imbalance in the few-shot setting. Our analysis further suggests
that to prevent catastrophic forgetting, actions need to be taken in the
primitive stage -- the training of base classes instead of later few-shot
learning sessions. Therefore, we propose to search for flat local minima of the
base training objective function and then fine-tune the model parameters within
the flat region on new tasks. In this way, the model can efficiently learn new
classes while preserving the old ones. Comprehensive experimental results
demonstrate that our approach outperforms all prior state-of-the-art methods
and is very close to the approximate upper bound. The source code is available
at this https URL.

    

### [[2111.01555] Likelihood-Free Inference in State-Space Models with Unknown Dynamics](http://arxiv.org/abs/2111.01555)


  We introduce a method for inferring and predicting latent states in the
important and difficult case of state-space models where observations can only
be simulated, and transition dynamics are unknown. In this setting, the
likelihood of observations is not available and only synthetic observations can
be generated from a black-box simulator. We propose a way of doing
likelihood-free inference (LFI) of states and state prediction with a limited
number of simulations. Our approach uses a multi-output Gaussian process for
state inference, and a Bayesian Neural Network as a model of the transition
dynamics for state prediction. We improve upon existing LFI methods for the
inference task, while also accurately learning transition dynamics. The
proposed method is necessary for modelling inverse problems in dynamical
systems with computationally expensive simulations, as demonstrated in
experiments with non-stationary user models.

    

### [[2111.01560] Efficient Learning of Quadratic Variance Function Directed Acyclic Graphs via Topological Layers](http://arxiv.org/abs/2111.01560)


  Directed acyclic graph (DAG) models are widely used to represent causal
relationships among random variables in many application domains. This paper
studies a special class of non-Gaussian DAG models, where the conditional
variance of each node given its parents is a quadratic function of its
conditional mean. Such a class of non-Gaussian DAG models are fairly flexible
and admit many popular distributions as special cases, including Poisson,
Binomial, Geometric, Exponential, and Gamma. To facilitate learning, we
introduce a novel concept of topological layers, and develop an efficient DAG
learning algorithm. It first reconstructs the topological layers in a
hierarchical fashion and then recoveries the directed edges between nodes in
different layers, which requires much less computational cost than most
existing algorithms in literature. Its advantage is also demonstrated in a
number of simulated examples, as well as its applications to two real-life
datasets, including an NBA player statistics data and a cosmetic sales data
collected by Alibaba.

    

### [[2111.01562] Evaluating deep transfer learning for whole-brain cognitive decoding](http://arxiv.org/abs/2111.01562)


  Research in many fields has shown that transfer learning (TL) is well-suited
to improve the performance of deep learning (DL) models in datasets with small
numbers of samples. This empirical success has triggered interest in the
application of TL to cognitive decoding analyses with functional neuroimaging
data. Here, we systematically evaluate TL for the application of DL models to
the decoding of cognitive states (e.g., viewing images of faces or houses) from
whole-brain functional Magnetic Resonance Imaging (fMRI) data. We first
pre-train two DL architectures on a large, public fMRI dataset and subsequently
evaluate their performance in an independent experimental task and a fully
independent dataset. The pre-trained models consistently achieve higher
decoding accuracies and generally require less training time and data than
model variants that were not pre-trained, clearly underlining the benefits of
pre-training. We demonstrate that these benefits arise from the ability of the
pre-trained models to reuse many of their learned features when training with
new data, providing deeper insights into the mechanisms giving rise to the
benefits of pre-training. Yet, we also surface nuanced challenges for
whole-brain cognitive decoding with DL models when interpreting the decoding
decisions of the pre-trained models, as these have learned to utilize the fMRI
data in unforeseen and counterintuitive ways to identify individual cognitive
states.

    

### [[2111.01564] MultiplexNet: Towards Fully Satisfied Logical Constraints in Neural Networks](http://arxiv.org/abs/2111.01564)


  We propose a novel way to incorporate expert knowledge into the training of
deep neural networks. Many approaches encode domain constraints directly into
the network architecture, requiring non-trivial or domain-specific engineering.
In contrast, our approach, called MultiplexNet, represents domain knowledge as
a logical formula in disjunctive normal form (DNF) which is easy to encode and
to elicit from human experts. It introduces a Categorical latent variable that
learns to choose which constraint term optimizes the error function of the
network and it compiles the constraints directly into the output of existing
learning algorithms. We demonstrate the efficacy of this approach empirically
on several classical deep learning tasks, such as density estimation and
classification in both supervised and unsupervised settings where prior
knowledge about the domains was expressed as logical constraints. Our results
show that the MultiplexNet approach learned to approximate unknown
distributions well, often requiring fewer data samples than the alternative
approaches. In some cases, MultiplexNet finds better solutions than the
baselines; or solutions that could not be achieved with the alternative
approaches. Our contribution is in encoding domain knowledge in a way that
facilitates inference that is shown to be both efficient and general; and
critically, our approach guarantees 100% constraint satisfaction in a network's
output.

    

### [[2111.01576] Provably efficient, succinct, and precise explanations](http://arxiv.org/abs/2111.01576)


  We consider the problem of explaining the predictions of an arbitrary
blackbox model $f$: given query access to $f$ and an instance $x$, output a
small set of $x$'s features that in conjunction essentially determines $f(x)$.
We design an efficient algorithm with provable guarantees on the succinctness
and precision of the explanations that it returns. Prior algorithms were either
efficient but lacked such guarantees, or achieved such guarantees but were
inefficient.
We obtain our algorithm via a connection to the problem of {\sl implicitly}
learning decision trees. The implicit nature of this learning task allows for
efficient algorithms even when the complexity of $f$ necessitates an
intractably large surrogate decision tree. We solve the implicit learning
problem by bringing together techniques from learning theory, local computation
algorithms, and complexity theory.
Our approach of "explaining by implicit learning" shares elements of two
previously disparate methods for post-hoc explanations, global and local
explanations, and we make the case that it enjoys advantages of both.

    

### [[2111.01584] Fitness Landscape Footprint: A Framework to Compare Neural Architecture Search Problems](http://arxiv.org/abs/2111.01584)


  Neural architecture search is a promising area of research dedicated to
automating the design of neural network models. This field is rapidly growing,
with a surge of methodologies ranging from Bayesian optimization,neuroevoltion,
to differentiable search, and applications in various contexts. However,
despite all great advances, few studies have presented insights on the
difficulty of the problem itself, thus the success (or fail) of these
methodologies remains unexplained. In this sense, the field of optimization has
developed methods that highlight key aspects to describe optimization problems.
The fitness landscape analysis stands out when it comes to characterize
reliably and quantitatively search algorithms. In this paper, we propose to use
fitness landscape analysis to study a neural architecture search problem.
Particularly, we introduce the fitness landscape footprint, an aggregation of
eight (8)general-purpose metrics to synthesize the landscape of an architecture
search problem. We studied two problems, the classical image classification
benchmark CIFAR-10, and the Remote-Sensing problem So2Sat LCZ42. The results
present a quantitative appraisal of the problems, allowing to characterize the
relative difficulty and other characteristics, such as the ruggedness or the
persistence, that helps to tailor a search strategy to the problem. Also, the
footprint is a tool that enables the comparison of multiple problems.

    

### [[2111.01587] Procedural Generalization by Planning with Self-Supervised World Models](http://arxiv.org/abs/2111.01587)


  One of the key promises of model-based reinforcement learning is the ability
to generalize using an internal model of the world to make predictions in novel
environments and tasks. However, the generalization ability of model-based
agents is not well understood because existing work has focused on model-free
agents when benchmarking generalization. Here, we explicitly measure the
generalization ability of model-based agents in comparison to their model-free
counterparts. We focus our analysis on MuZero (Schrittwieser et al., 2020), a
powerful model-based agent, and evaluate its performance on both procedural and
task generalization. We identify three factors of procedural generalization --
planning, self-supervised representation learning, and procedural data
diversity -- and show that by combining these techniques, we achieve
state-of-the art generalization performance and data efficiency on Procgen
(Cobbe et al., 2019). However, we find that these factors do not always provide
the same benefits for the task generalization benchmarks in Meta-World (Yu et
al., 2019), indicating that transfer remains a challenge and may require
different approaches than procedural generalization. Overall, we suggest that
building generalizable agents requires moving beyond the single-task,
model-free paradigm and towards self-supervised model-based agents that are
trained in rich, procedural, multi-task environments.

    

### [[2111.01589] Nonstochastic Bandits and Experts with Arm-Dependent Delays](http://arxiv.org/abs/2111.01589)


  We study nonstochastic bandits and experts in a delayed setting where delays
depend on both time and arms. While the setting in which delays only depend on
time has been extensively studied, the arm-dependent delay setting better
captures real-world applications at the cost of introducing new technical
challenges. In the full information (experts) setting, we design an algorithm
with a first-order regret bound that reveals an interesting trade-off between
delays and losses. We prove a similar first-order regret bound also for the
bandit setting, when the learner is allowed to observe how many losses are
missing. These are the first bounds in the delayed setting that depend on the
losses and delays of the best arm only. When in the bandit setting no
information other than the losses is observed, we still manage to prove a
regret bound through a modification to the algorithm of Zimmert and Seldin
(2020). Our analyses hinge on a novel bound on the drift, measuring how much
better an algorithm can perform when given a look-ahead of one round.

    

### [[2111.01602] Stochastic Online Linear Regression: the Forward Algorithm to Replace Ridge](http://arxiv.org/abs/2111.01602)


  We consider the problem of online linear regression in the stochastic
setting. We derive high probability regret bounds for online ridge regression
and the forward algorithm. This enables us to compare online regression
algorithms more accurately and eliminate assumptions of bounded observations
and predictions. Our study advocates for the use of the forward algorithm in
lieu of ridge due to its enhanced bounds and robustness to the regularization
parameter. Moreover, we explain how to integrate it in algorithms involving
linear function approximation to remove a boundedness assumption without
deteriorating theoretical bounds. We showcase this modification in linear
bandit settings where it yields improved regret bounds. Last, we provide
numerical experiments to illustrate our results and endorse our intuitions.

    

### [[2111.01619] StyleGAN of All Trades: Image Manipulation with Only Pretrained StyleGAN](http://arxiv.org/abs/2111.01619)


  Recently, StyleGAN has enabled various image manipulation and editing tasks
thanks to the high-quality generation and the disentangled latent space.
However, additional architectures or task-specific training paradigms are
usually required for different tasks. In this work, we take a deeper look at
the spatial properties of StyleGAN. We show that with a pretrained StyleGAN
along with some operations, without any additional architecture, we can perform
comparably to the state-of-the-art methods on various tasks, including image
blending, panorama generation, generation from a single image, controllable and
local multimodal image to image translation, and attributes transfer. The
proposed method is simple, effective, efficient, and applicable to any existing
pretrained StyleGAN model.

    

### [[2111.01632] Elucidating Noisy Data via Uncertainty-Aware Robust Learning](http://arxiv.org/abs/2111.01632)


  Robust learning methods aim to learn a clean target distribution from noisy
and corrupted training data where a specific corruption pattern is often
assumed a priori. Our proposed method can not only successfully learn the clean
target distribution from a dirty dataset but also can estimate the underlying
noise pattern. To this end, we leverage a mixture-of-experts model that can
distinguish two different types of predictive uncertainty, aleatoric and
epistemic uncertainty. We show that the ability to estimate the uncertainty
plays a significant role in elucidating the corruption patterns as these two
objectives are tightly intertwined. We also present a novel validation scheme
for evaluating the performance of the corruption pattern estimation. Our
proposed method is extensively assessed in terms of both robustness and
corruption pattern estimation through a number of domains, including computer
vision and natural language processing.

    

### [[2111.01635] Characterizing and Understanding the Generalization Error of Transfer Learning with Gibbs Algorithm](http://arxiv.org/abs/2111.01635)


  We provide an information-theoretic analysis of the generalization ability of
Gibbs-based transfer learning algorithms by focusing on two popular transfer
learning approaches, $\alpha$-weighted-ERM and two-stage-ERM. Our key result is
an exact characterization of the generalization behaviour using the conditional
symmetrized KL information between the output hypothesis and the target
training samples given the source samples. Our results can also be applied to
provide novel distribution-free generalization error upper bounds on these two
aforementioned Gibbs algorithms. Our approach is versatile, as it also
characterizes the generalization errors and excess risks of these two Gibbs
algorithms in the asymptotic regime, where they converge to the
$\alpha$-weighted-ERM and two-stage-ERM, respectively. Based on our theoretical
results, we show that the benefits of transfer learning can be viewed as a
bias-variance trade-off, with the bias induced by the source distribution and
the variance induced by the lack of target samples. We believe this viewpoint
can guide the choice of transfer learning algorithms in practice.

    

### [[2111.01657] LogLAB: Attention-Based Labeling of Log Data Anomalies via Weak Supervision](http://arxiv.org/abs/2111.01657)


  With increasing scale and complexity of cloud operations, automated detection
of anomalies in monitoring data such as logs will be an essential part of
managing future IT infrastructures. However, many methods based on artificial
intelligence, such as supervised deep learning models, require large amounts of
labeled training data to perform well. In practice, this data is rarely
available because labeling log data is expensive, time-consuming, and requires
a deep understanding of the underlying system. We present LogLAB, a novel
modeling approach for automated labeling of log messages without requiring
manual work by experts. Our method relies on estimated failure time windows
provided by monitoring systems to produce precise labeled datasets in
retrospect. It is based on the attention mechanism and uses a custom objective
function for weak supervision deep learning techniques that accounts for
imbalanced data. Our evaluation shows that LogLAB consistently outperforms nine
benchmark approaches across three different datasets and maintains an F1-score
of more than 0.98 even at large failure time windows.

    

### [[2111.01662] OSOA: One-Shot Online Adaptation of Deep Generative Models for Lossless Compression](http://arxiv.org/abs/2111.01662)


  Explicit deep generative models (DGMs), e.g., VAEs and Normalizing Flows,
have shown to offer an effective data modelling alternative for lossless
compression. However, DGMs themselves normally require large storage space and
thus contaminate the advantage brought by accurate data density estimation. To
eliminate the requirement of saving separate models for different target
datasets, we propose a novel setting that starts from a pretrained deep
generative model and compresses the data batches while adapting the model with
a dynamical system for only one epoch. We formalise this setting as that of
One-Shot Online Adaptation (OSOA) of DGMs for lossless compression and propose
a vanilla algorithm under this setting. Experimental results show that vanilla
OSOA can save significant time versus training bespoke models and space versus
using one model for all targets. With the same adaptation step number or
adaptation time, it is shown vanilla OSOA can exhibit better space efficiency,
e.g., $47\%$ less space, than fine-tuning the pretrained model and saving the
fine-tuned model. Moreover, we showcase the potential of OSOA and motivate more
sophisticated OSOA algorithms by showing further space or time efficiency with
multiple updates per batch and early stopping.

    

### [[2111.01665] Explainable Medical Image Segmentation via Generative Adversarial Networks and Layer-wise Relevance Propagation](http://arxiv.org/abs/2111.01665)


  This paper contributes to automating medical image segmentation by proposing
generative adversarial network-based models to segment both polyps and
instruments in endoscopy images. A major contribution of this work is to
provide explanations for the predictions using a layer-wise relevance
propagation approach designating which input image pixels are relevant to the
predictions and to what extent. On the polyp segmentation task, the models
achieved 0.84 of accuracy and 0.46 on Jaccard index. On the instrument
segmentation task, the models achieved 0.96 of accuracy and 0.70 on Jaccard
index. The code is available at this https URL.

    

### [[2111.01674] Minimizing Energy Consumption Leads to the Emergence of Gaits in Legged Robots](http://arxiv.org/abs/2111.01674)


  Legged locomotion is commonly studied and expressed as a discrete set of gait
patterns, like walk, trot, gallop, which are usually treated as given and
pre-programmed in legged robots for efficient locomotion at different speeds.
However, fixing a set of pre-programmed gaits limits the generality of
locomotion. Recent animal motor studies show that these conventional gaits are
only prevalent in ideal flat terrain conditions while real-world locomotion is
unstructured and more like bouts of intermittent steps. What principles could
lead to both structured and unstructured patterns across mammals and how to
synthesize them in robots? In this work, we take an analysis-by-synthesis
approach and learn to move by minimizing mechanical energy. We demonstrate that
learning to minimize energy consumption plays a key role in the emergence of
natural locomotion gaits at different speeds in real quadruped robots. The
emergent gaits are structured in ideal terrains and look similar to that of
horses and sheep. The same approach leads to unstructured gaits in rough
terrains which is consistent with the findings in animal motor control. We
validate our hypothesis in both simulation and real hardware across natural
terrains. Videos at this https URL


### [[2111.01677] Top1 Solution of QQ Browser 2021 Ai Algorithm Competition Track 1 : Multimodal Video Similarity](http://arxiv.org/abs/2111.01677)


  In this paper, we describe the solution to the QQ Browser 2021 Ai Algorithm
Competition (AIAC) Track 1. We use the multi-modal transformer model for the
video embedding extraction. In the pretrain phase, we train the model with
three tasks, (1) Video Tag Classification (VTC), (2) Mask Language Modeling
(MLM) and (3) Mask Frame Modeling (MFM). In the finetune phase, we train the
model with video similarity based on rank normalized human labels. Our full
pipeline, after ensembling several models, scores 0.852 on the leaderboard,
which we achieved the 1st place in the competition. The source codes have been
released at Github.

    

### [[2111.01682] Progressive observation of Covid-19 vaccination effects on skin-cellular structures by use of Intelligent Laser Speckle Classification (ILSC)](http://arxiv.org/abs/2111.01682)


  We have made a progressive observation of Covid-19 Astra Zeneca Vaccination
effect on Skin cellular network and properties by use of well established
Intelligent Laser Speckle Classification (ILSC) image based technique and
managed to distinguish between three different subjects groups via their laser
speckle skin image samplings such as early-vaccinated, late-vaccinated and
non-vaccinated individuals. The results have proven that the ILSC technique in
association with the optimised Bayesian network is capable of classifying skin
changes of vaccinated and non-vaccinated individuals and also of detecting
progressive development made on skin cellular properties for a month period.

    

### [[2111.01692] Efficient hierarchical Bayesian inference for spatio-temporal regression models in neuroimaging](http://arxiv.org/abs/2111.01692)


  Several problems in neuroimaging and beyond require inference on the
parameters of multi-task sparse hierarchical regression models. Examples
include M/EEG inverse problems, neural encoding models for task-based fMRI
analyses, and temperature monitoring of climate or CPU and GPU. In these
domains, both the model parameters to be inferred and the measurement noise may
exhibit a complex spatio-temporal structure. Existing work either neglects the
temporal structure or leads to computationally demanding inference schemes.
Overcoming these limitations, we devise a novel flexible hierarchical Bayesian
framework within which the spatio-temporal dynamics of model parameters and
noise are modeled to have Kronecker product covariance structure. Inference in
our framework is based on majorization-minimization optimization and has
guaranteed convergence properties. Our highly efficient algorithms exploit the
intrinsic Riemannian geometry of temporal autocovariance matrices. For
stationary dynamics described by Toeplitz matrices, the theory of circulant
embeddings is employed. We prove convex bounding properties and derive update
rules of the resulting algorithms. On both synthetic and real neural data from
M/EEG, we demonstrate that our methods lead to improved performance.

    

### [[2111.01697] Low-Rank+Sparse Tensor Compression for Neural Networks](http://arxiv.org/abs/2111.01697)


  Low-rank tensor compression has been proposed as a promising approach to
reduce the memory and compute requirements of neural networks for their
deployment on edge devices. Tensor compression reduces the number of parameters
required to represent a neural network weight by assuming network weights
possess a coarse higher-order structure. This coarse structure assumption has
been applied to compress large neural networks such as VGG and ResNet. However
modern state-of-the-art neural networks for computer vision tasks (i.e.
MobileNet, EfficientNet) already assume a coarse factorized structure through
depthwise separable convolutions, making pure tensor decomposition a less
attractive approach. We propose to combine low-rank tensor decomposition with
sparse pruning in order to take advantage of both coarse and fine structure for
compression. We compress weights in SOTA architectures (MobileNetv3,
EfficientNet, Vision Transformer) and compare this approach to sparse pruning
and tensor decomposition alone.

    

### [[2111.01705] AI Ethics Statements -- Analysis and lessons learnt from NeurIPS Broader Impact Statements](http://arxiv.org/abs/2111.01705)


  Ethics statements have been proposed as a mechanism to increase transparency
and promote reflection on the societal impacts of published research. In 2020,
the machine learning (ML) conference NeurIPS broke new ground by requiring that
all papers include a broader impact statement. This requirement was removed in
2021, in favour of a checklist approach. The 2020 statements therefore provide
a unique opportunity to learn from the broader impact experiment: to
investigate the benefits and challenges of this and similar governance
mechanisms, as well as providing an insight into how ML researchers think about
the societal impacts of their own work. Such learning is needed as NeurIPS and
other venues continue to question and adapt their policies. To enable this, we
have created a dataset containing the impact statements from all NeurIPS 2020
papers, along with additional information such as affiliation type, location
and subject area, and a simple visualisation tool for exploration. We also
provide an initial quantitative analysis of the dataset, covering
representation, engagement, common themes, and willingness to discuss potential
harms alongside benefits. We investigate how these vary by geography,
affiliation type and subject area. Drawing on these findings, we discuss the
potential benefits and negative outcomes of ethics statement requirements, and
their possible causes and associated challenges. These lead us to several
lessons to be learnt from the 2020 requirement: (i) the importance of creating
the right incentives, (ii) the need for clear expectations and guidance, and
(iii) the importance of transparency and constructive deliberation. We
encourage other researchers to use our dataset to provide additional analysis,
to further our understanding of how researchers responded to this requirement,
and to investigate the benefits and challenges of this and related mechanisms.

    

### [[2111.01713] Realistic galaxy image simulation via score-based generative models](http://arxiv.org/abs/2111.01713)


  We show that a Denoising Diffusion Probabalistic Model (DDPM), a class of
score-based generative model, can be used to produce realistic yet fake images
that mimic observations of galaxies. Our method is tested with Dark Energy
Spectroscopic Instrument grz imaging of galaxies from the Photometry and
Rotation curve OBservations from Extragalactic Surveys (PROBES) sample and
galaxies selected from the Sloan Digital Sky Survey. Subjectively, the
generated galaxies are highly realistic when compared with samples from the
real dataset. We quantify the similarity by borrowing from the deep generative
learning literature, using the `Fréchet Inception Distance' to test for
subjective and morphological similarity. We also introduce the `Synthetic
Galaxy Distance' metric to compare the emergent physical properties (such as
total magnitude, colour and half light radius) of a ground truth parent and
synthesised child dataset. We argue that the DDPM approach produces sharper and
more realistic images than other generative methods such as Adversarial
Networks (with the downside of more costly inference), and could be used to
produce large samples of synthetic observations tailored to a specific imaging
survey. We demonstrate two potential uses of the DDPM: (1) accurate in-painting
of occluded data, such as satellite trails, and (2) domain transfer, where new
input images can be processed to mimic the properties of the DDPM training set.
Here we `DESI-fy' cartoon images as a proof of concept for domain transfer.
Finally, we suggest potential applications for score-based approaches that
could motivate further research on this topic within the astronomical
community.

    

### [[2111.01714] Meta-Learning the Search Distribution of Black-Box Random Search Based Adversarial Attacks](http://arxiv.org/abs/2111.01714)


  Adversarial attacks based on randomized search schemes have obtained
state-of-the-art results in black-box robustness evaluation recently. However,
as we demonstrate in this work, their efficiency in different query budget
regimes depends on manual design and heuristic tuning of the underlying
proposal distributions. We study how this issue can be addressed by adapting
the proposal distribution online based on the information obtained during the
attack. We consider Square Attack, which is a state-of-the-art score-based
black-box attack, and demonstrate how its performance can be improved by a
learned controller that adjusts the parameters of the proposal distribution
online during the attack. We train the controller using gradient-based
end-to-end training on a CIFAR10 model with white box access. We demonstrate
that plugging the learned controller into the attack consistently improves its
black-box robustness estimate in different query regimes by up to 20% for a
wide range of different models with black-box access. We further show that the
learned adaptation principle transfers well to the other data distributions
such as CIFAR100 or ImageNet and to the targeted attack setting.

    

### [[2111.01721] Bayes-Newton Methods for Approximate Bayesian Inference with PSD Guarantees](http://arxiv.org/abs/2111.01721)


  We formulate natural gradient variational inference (VI), expectation
propagation (EP), and posterior linearisation (PL) as extensions of Newton's
method for optimising the parameters of a Bayesian posterior distribution. This
viewpoint explicitly casts inference algorithms under the framework of
numerical optimisation. We show that common approximations to Newton's method
from the optimisation literature, namely Gauss-Newton and quasi-Newton methods
(e.g., the BFGS algorithm), are still valid under this `Bayes-Newton'
framework. This leads to a suite of novel algorithms which are guaranteed to
result in positive semi-definite covariance matrices, unlike standard VI and
EP. Our unifying viewpoint provides new insights into the connections between
various inference schemes. All the presented methods apply to any model with a
Gaussian prior and non-conjugate likelihood, which we demonstrate with (sparse)
Gaussian processes and state space models.

    

### [[2111.01722] Predicting the Location of Bicycle-sharing Stations using OpenStreetMap Data](http://arxiv.org/abs/2111.01722)


  Planning the layout of bicycle-sharing stations is a complex process,
especially in cities where bicycle sharing systems are just being implemented.
Urban planners often have to make a lot of estimates based on both publicly
available data and privately provided data from the administration and then use
the Location-Allocation model popular in the field. Many municipalities in
smaller cities may have difficulty hiring specialists to carry out such
planning. This thesis proposes a new solution to streamline and facilitate the
process of such planning by using spatial embedding methods. Based only on
publicly available data from OpenStreetMap, and station layouts from 34 cities
in Europe, a method has been developed to divide cities into micro-regions
using the Uber H3 discrete global grid system and to indicate regions where it
is worth placing a station based on existing systems in different cities using
transfer learning. The result of the work is a mechanism to support planners in
their decision making when planning a station layout with a choice of reference
cities.

    

### [[2111.01732] Spatio-Temporal Variational Gaussian Processes](http://arxiv.org/abs/2111.01732)


  We introduce a scalable approach to Gaussian process inference that combines
spatio-temporal filtering with natural gradient variational inference,
resulting in a non-conjugate GP method for multivariate data that scales
linearly with respect to time. Our natural gradient approach enables
application of parallel filtering and smoothing, further reducing the temporal
span complexity to be logarithmic in the number of time steps. We derive a
sparse approximation that constructs a state-space model over a reduced set of
spatial inducing points, and show that for separable Markov kernels the full
and sparse cases exactly recover the standard variational GP, whilst exhibiting
favourable computational properties. To further improve the spatial scaling we
propose a mean-field assumption of independence between spatial locations
which, when coupled with sparsity and parallelisation, leads to an efficient
and accurate method for large spatio-temporal problems.

    

### [[2111.01742] LogAvgExp Provides a Principled and Performant Global Pooling Operator](http://arxiv.org/abs/2111.01742)


  We seek to improve the pooling operation in neural networks, by applying a
more theoretically justified operator. We demonstrate that LogSumExp provides a
natural OR operator for logits. When one corrects for the number of elements
inside the pooling operator, this becomes $\text{LogAvgExp} :=
\log(\text{mean}(\exp(x)))$. By introducing a single temperature parameter,
LogAvgExp smoothly transitions from the max of its operands to the mean (found
at the limiting cases $t \to 0^+$ and $t \to +\infty$). We experimentally
tested LogAvgExp, both with and without a learnable temperature parameter, in a
variety of deep neural network architectures for computer vision.

    

### [[2111.01743] Designing Inherently Interpretable Machine Learning Models](http://arxiv.org/abs/2111.01743)


  Interpretable machine learning (IML) becomes increasingly important in highly
regulated industry sectors related to the health and safety or fundamental
rights of human beings. In general, the inherently IML models should be adopted
because of their transparency and explainability, while black-box models with
model-agnostic explainability can be more difficult to defend under regulatory
scrutiny. For assessing inherent interpretability of a machine learning model,
we propose a qualitative template based on feature effects and model
architecture constraints. It provides the design principles for
high-performance IML model development, with examples given by reviewing our
recent works on ExNN, GAMI-Net, SIMTree, and the Aletheia toolkit for local
linear interpretability of deep ReLU networks. We further demonstrate how to
design an interpretable ReLU DNN model with evaluation of conceptual soundness
for a real case study of predicting credit default in home lending. We hope
that this work will provide a practical guide of developing inherently IML
models in high risk applications in banking industry, as well as other sectors.

    

### [[2111.01744] UnProjection: Leveraging Inverse-Projections for Visual Analytics of High-Dimensional Data](http://arxiv.org/abs/2111.01744)


  Projection techniques are often used to visualize high-dimensional data,
allowing users to better understand the overall structure of multi-dimensional
spaces on a 2D screen. Although many such methods exist, comparably little work
has been done on generalizable methods of inverse-projection -- the process of
mapping the projected points, or more generally, the projection space back to
the original high-dimensional space. In this paper we present NNInv, a deep
learning technique with the ability to approximate the inverse of any
projection or mapping. NNInv learns to reconstruct high-dimensional data from
any arbitrary point on a 2D projection space, giving users the ability to
interact with the learned high-dimensional representation in a visual analytics
system. We provide an analysis of the parameter space of NNInv, and offer
guidance in selecting these parameters. We extend validation of the
effectiveness of NNInv through a series of quantitative and qualitative
analyses. We then demonstrate the method's utility by applying it to three
visualization tasks: interactive instance interpolation, classifier agreement,
and gradient visualization.

    

### [[2111.01750] Spiking Generative Adversarial Networks With a Neural Network Discriminator: Local Training, Bayesian Models, and Continual Meta-Learning](http://arxiv.org/abs/2111.01750)


  Neuromorphic data carries information in spatio-temporal patterns encoded by
spikes. Accordingly, a central problem in neuromorphic computing is training
spiking neural networks (SNNs) to reproduce spatio-temporal spiking patterns in
response to given spiking stimuli. Most existing approaches model the
input-output behavior of an SNN in a deterministic fashion by assigning each
input to a specific desired output spiking sequence. In contrast, in order to
fully leverage the time-encoding capacity of spikes, this work proposes to
train SNNs so as to match distributions of spiking signals rather than
individual spiking signals. To this end, the paper introduces a novel hybrid
architecture comprising a conditional generator, implemented via an SNN, and a
discriminator, implemented by a conventional artificial neural network (ANN).
The role of the ANN is to provide feedback during training to the SNN within an
adversarial iterative learning strategy that follows the principle of
generative adversarial network (GANs). In order to better capture multi-modal
spatio-temporal distribution, the proposed approach -- termed SpikeGAN -- is
further extended to support Bayesian learning of the generator's weight.
Finally, settings with time-varying statistics are addressed by proposing an
online meta-learning variant of SpikeGAN. Experiments bring insights into the
merits of the proposed approach as compared to existing solutions based on
(static) belief networks and maximum likelihood (or empirical risk
minimization).

    

### [[2111.01754] Meta-Learning to Improve Pre-Training](http://arxiv.org/abs/2111.01754)


  Pre-training (PT) followed by fine-tuning (FT) is an effective method for
training neural networks, and has led to significant performance improvements
in many domains. PT can incorporate various design choices such as task and
data reweighting strategies, augmentation policies, and noise models, all of
which can significantly impact the quality of representations learned. The
hyperparameters introduced by these strategies therefore must be tuned
appropriately. However, setting the values of these hyperparameters is
challenging. Most existing methods either struggle to scale to high dimensions,
are too slow and memory-intensive, or cannot be directly applied to the
two-stage PT and FT learning process. In this work, we propose an efficient,
gradient-based algorithm to meta-learn PT hyperparameters. We formalize the PT
hyperparameter optimization problem and propose a novel method to obtain PT
hyperparameter gradients by combining implicit differentiation and
backpropagation through unrolled optimization. We demonstrate that our method
improves predictive performance on two real-world domains. First, we optimize
high-dimensional task weighting hyperparameters for multitask pre-training on
protein-protein interaction graphs and improve AUROC by up to 3.9%. Second, we
optimize a data augmentation neural network for self-supervised PT with SimCLR
on electrocardiography data and improve AUROC by up to 1.9%.

    

### [[2111.01760] Increasing Liquid State Machine Performance with Edge-of-Chaos Dynamics Organized by Astrocyte-modulated Plasticity](http://arxiv.org/abs/2111.01760)


  The liquid state machine (LSM) combines low training complexity and
biological plausibility, which has made it an attractive machine learning
framework for edge and neuromorphic computing paradigms. Originally proposed as
a model of brain computation, the LSM tunes its internal weights without
backpropagation of gradients, which results in lower performance compared to
multi-layer neural networks. Recent findings in neuroscience suggest that
astrocytes, a long-neglected non-neuronal brain cell, modulate synaptic
plasticity and brain dynamics, tuning brain networks to the vicinity of the
computationally optimal critical phase transition between order and chaos.
Inspired by this disruptive understanding of how brain networks self-tune, we
propose the neuron-astrocyte liquid state machine (NALSM) that addresses
under-performance through self-organized near-critical dynamics. Similar to its
biological counterpart, the astrocyte model integrates neuronal activity and
provides global feedback to spike-timing-dependent plasticity (STDP), which
self-organizes NALSM dynamics around a critical branching factor that is
associated with the edge-of-chaos. We demonstrate that NALSM achieves
state-of-the-art accuracy versus comparable LSM methods, without the need for
data-specific hand-tuning. With a top accuracy of 97.61% on MNIST, 97.51% on
N-MNIST, and 85.84% on Fashion-MNIST, NALSM achieved comparable performance to
current fully-connected multi-layer spiking neural networks trained via
backpropagation. Our findings suggest that the further development of
brain-inspired machine learning methods has the potential to reach the
performance of deep learning, with the added benefits of supporting robust and
energy-efficient neuromorphic computing on the edge.

    

### [[2111.01763] Modelling COVID-19 Pandemic Dynamics Using Transparent, Interpretable, Parsimonious and Simulatable (TIPS) Machine Learning Models: A Case Study from Systems Thinking and System Identification Perspectives](http://arxiv.org/abs/2111.01763)


  Since the outbreak of COVID-19, an astronomical number of publications on the
pandemic dynamics appeared in the literature, of which many use the susceptible
infected removed (SIR) and susceptible exposed infected removed (SEIR) models,
or their variants, to simulate and study the spread of the coronavirus. SIR and
SEIR are continuous-time models which are a class of initial value problems
(IVPs) of ordinary differential equations (ODEs). Discrete-time models such as
regression and machine learning have also been applied to analyze COVID-19
pandemic data (e.g. predicting infection cases), but most of these methods use
simplified models involving a small number of input variables pre-selected
based on a priori knowledge, or use very complicated models (e.g. deep
learning), purely focusing on certain prediction purposes and paying little
attention to the model interpretability. There have been relatively fewer
studies focusing on the investigations of the inherent time-lagged or
time-delayed relationships e.g. between the reproduction number (R number),
infection cases, and deaths, analyzing the pandemic spread from a systems
thinking and dynamic perspective. The present study, for the first time,
proposes using systems engineering and system identification approach to build
transparent, interpretable, parsimonious and simulatable (TIPS) dynamic machine
learning models, establishing links between the R number, the infection cases
and deaths caused by COVID-19. The TIPS models are developed based on the
well-known NARMAX (Nonlinear AutoRegressive Moving Average with eXogenous
inputs) model, which can help better understand the COVID-19 pandemic dynamics.
A case study on the UK COVID-19 data is carried out, and new findings are
detailed. The proposed method and the associated new findings are useful for
better understanding the spread dynamics of the COVID-19 pandemic.

    

### [[2111.01767] Regularization for Shuffled Data Problems via Exponential Family Priors on the Permutation Group](http://arxiv.org/abs/2111.01767)


  In the analysis of data sets consisting of (X, Y)-pairs, a tacit assumption
is that each pair corresponds to the same observation unit. If, however, such
pairs are obtained via record linkage of two files, this assumption can be
violated as a result of mismatch error rooting, for example, in the lack of
reliable identifiers in the two files. Recently, there has been a surge of
interest in this setting under the term "Shuffled data" in which the underlying
correct pairing of (X, Y)-pairs is represented via an unknown index
permutation. Explicit modeling of the permutation tends to be associated with
substantial overfitting, prompting the need for suitable methods of
regularization. In this paper, we propose a flexible exponential family prior
on the permutation group for this purpose that can be used to integrate various
structures such as sparse and locally constrained shuffling. This prior turns
out to be conjugate for canonical shuffled data problems in which the
likelihood conditional on a fixed permutation can be expressed as product over
the corresponding (X,Y)-pairs. Inference is based on the EM algorithm in which
the intractable E-step is approximated by the Fisher-Yates algorithm. The
M-step is shown to admit a significant reduction from $n^2$ to $n$ terms if the
likelihood of (X,Y)-pairs has exponential family form as in the case of
generalized linear models. Comparisons on synthetic and real data show that the
proposed approach compares favorably to competing methods.

    

### [[2111.01768] Nearly Optimal Algorithms for Level Set Estimation](http://arxiv.org/abs/2111.01768)


  The level set estimation problem seeks to find all points in a domain ${\cal
X}$ where the value of an unknown function $f:{\cal X}\rightarrow \mathbb{R}$
exceeds a threshold $\alpha$. The estimation is based on noisy function
evaluations that may be acquired at sequentially and adaptively chosen
locations in ${\cal X}$. The threshold value $\alpha$ can either be
\emph{explicit} and provided a priori, or \emph{implicit} and defined relative
to the optimal function value, i.e. $\alpha = (1-\epsilon)f(x_\ast)$ for a
given $\epsilon > 0$ where $f(x_\ast)$ is the maximal function value and is
unknown. In this work we provide a new approach to the level set estimation
problem by relating it to recent adaptive experimental design methods for
linear bandits in the Reproducing Kernel Hilbert Space (RKHS) setting. We
assume that $f$ can be approximated by a function in the RKHS up to an unknown
misspecification and provide novel algorithms for both the implicit and
explicit cases in this setting with strong theoretical guarantees. Moreover, in
the linear (kernel) setting, we show that our bounds are nearly optimal,
namely, our upper bounds match existing lower bounds for threshold linear
bandits. To our knowledge this work provides the first instance-dependent,
non-asymptotic upper bounds on sample complexity of level-set estimation that
match information theoretic lower bounds.

    

### [[2111.01773] Data-Driven System Identification of 6-DoF Ship Motion in Waves with Neural Networks](http://arxiv.org/abs/2111.01773)


  Critical evaluation and understanding of ship responses in the ocean is
important for not only the design and engineering of future platforms but also
the operation and safety of those that are currently deployed. Simulations or
experiments are typically performed in nominal sea conditions during ship
design or prior to deployment and the results may not be reflective of the
instantaneous state of the vessel and the ocean environment while deployed.
Short-term temporal predictions of ship responses given the current wave
environment and ship state would enable enhanced decision-making onboard for
both manned and unmanned vessels. However, the current state-of-the-art in
numerical hydrodynamic simulation tools are too computationally expensive to be
employed for real-time ship motion forecasting and the computationally
efficient tools are too low fidelity to provide accurate responses. A
methodology is developed with long short-term memory (LSTM) neural networks to
represent the motions of a free running David Taylor Model Basin (DTMB) 5415
destroyer operating at 20 knots in Sea State 7 stern-quartering irregular seas.
Case studies are performed for both course-keeping and turning circle
scenarios. An estimate of the vessel's encounter frame is made with the
trajectories observed in the training dataset. Wave elevation time histories
are given by artificial wave probes that travel with the estimated encounter
frame and serve as input into the neural network, while the output is the 6-DOF
temporal ship motion response. Overall, the neural network is able to predict
the temporal response of the ship due to unseen waves accurately, which makes
this methodology suitable for system identification and real-time ship motion
forecasting. The methodology, the dependence of model accuracy on wave probe
and training data quantity and the estimated encounter frame are all detailed.

    

### [[2111.01777] A Framework for Real-World Multi-Robot Systems Running Decentralized GNN-Based Policies](http://arxiv.org/abs/2111.01777)


  Graph Neural Networks (GNNs) are a paradigm-shifting neural architecture to
facilitate the learning of complex multi-agent behaviors. Recent work has
demonstrated remarkable performance in tasks such as flocking, multi-agent path
planning and cooperative coverage. However, the policies derived through
GNN-based learning schemes have not yet been deployed to the real-world on
physical multi-robot systems. In this work, we present the design of a system
that allows for fully decentralized execution of GNN-based policies. We create
a framework based on ROS2 and elaborate its details in this paper. We
demonstrate our framework on a case-study that requires tight coordination
between robots, and present first-of-a-kind results that show successful
real-world deployment of GNN-based policies on a decentralized multi-robot
system relying on Adhoc communication. A video demonstration of this case-study
can be found online. this https URL


### [[2111.01785] PatchGame: Learning to Signal Mid-level Patches in Referential Games](http://arxiv.org/abs/2111.01785)


  We study a referential game (a type of signaling game) where two agents
communicate with each other via a discrete bottleneck to achieve a common goal.
In our referential game, the goal of the speaker is to compose a message or a
symbolic representation of "important" image patches, while the task for the
listener is to match the speaker's message to a different view of the same
image. We show that it is indeed possible for the two agents to develop a
communication protocol without explicit or implicit supervision. We further
investigate the developed protocol and show the applications in speeding up
recent Vision Transformers by using only important patches, and as pre-training
for downstream recognition tasks (e.g., classification). Code available at
this https URL.

    

### [[2111.01786] A Recommendation System to Enhance Midwives' Capacities in Low-Income Countries](http://arxiv.org/abs/2111.01786)


  Maternal and child mortality is a public health problem that
disproportionately affects low- and middle-income countries. Every day, 800
women and 6,700 newborns die from complications related to pregnancy or
childbirth. And for every maternal death, about 20 women suffer serious birth
injuries. However, nearly all of these deaths and negative health outcomes are
preventable. Midwives are key to revert this situation, and thus it is
essential to strengthen their capacities and the quality of their education.
This is the aim of the Safe Delivery App, a digital job aid and learning tool
to enhance the knowledge, confidence and skills of health practitioners. Here,
we use the behavioral logs of the App to implement a recommendation system that
presents each midwife with suitable contents to continue gaining expertise. We
focus on predicting the click-through rate, the probability that a given user
will click on a recommended content. We evaluate four deep learning models and
show that all of them produce highly accurate predictions.

    

### [[1805.09719] Learning convex polyhedra with margin](http://arxiv.org/abs/1805.09719)


  We present an improved algorithm for {\em quasi-properly} learning convex
polyhedra in the realizable PAC setting from data with a margin. Our learning
algorithm constructs a consistent polyhedron as an intersection of about $t
\log t$ halfspaces with constant-size margins in time polynomial in $t$ (where
$t$ is the number of halfspaces forming an optimal polyhedron). We also
identify distinct generalizations of the notion of margin from hyperplanes to
polyhedra and investigate how they relate geometrically; this result may have
ramifications beyond the learning setting.

    

### [[1903.00715] Efficient Reinforcement Learning for StarCraft by Abstract Forward Models and Transfer Learning](http://arxiv.org/abs/1903.00715)


  Injecting human knowledge is an effective way to accelerate reinforcement
learning (RL). However, these methods are underexplored. This paper presents
our discovery that an abstract forward model (thought-game (TG)) combined with
transfer learning (TL) is an effective way. We take StarCraft II as our study
environment. With the help of a designed TG, the agent can learn a 99% win-rate
on a 64x64 map against the Level-7 built-in AI, using only 1.08 hours in a
single commercial machine. We also show that the TG method is not as
restrictive as it was thought to be. It can work with roughly designed TGs, and
can also be useful when the environment changes. Comparing with previous
model-based RL, we show TG is more effective. We also present a TG hypothesis
that gives the influence of different fidelity levels of TG. For real games
that have unequal state and action spaces, we proposed a novel XfrNet of which
usefulness is validated while achieving a 90% win-rate against the cheating
Level-10 AI. We argue that the TG method might shed light on further studies of
efficient RL with human knowledge.

    

### [[1906.05799] Deep Reinforcement Learning for Cyber Security](http://arxiv.org/abs/1906.05799)


  The scale of Internet-connected systems has increased considerably, and these
systems are being exposed to cyber attacks more than ever. The complexity and
dynamics of cyber attacks require protecting mechanisms to be responsive,
adaptive, and scalable. Machine learning, or more specifically deep
reinforcement learning (DRL), methods have been proposed widely to address
these issues. By incorporating deep learning into traditional RL, DRL is highly
capable of solving complex, dynamic, and especially high-dimensional cyber
defense problems. This paper presents a survey of DRL approaches developed for
cyber security. We touch on different vital aspects, including DRL-based
security methods for cyber-physical systems, autonomous intrusion detection
techniques, and multiagent DRL-based game theory simulations for defense
strategies against cyber attacks. Extensive discussions and future research
directions on DRL-based cyber security are also given. We expect that this
comprehensive review provides the foundations for and facilitates future
studies on exploring the potential of emerging DRL to cope with increasingly
complex cyber security problems.

    

### [[1910.05384] ORCCA: Optimal Randomized Canonical Correlation Analysis](http://arxiv.org/abs/1910.05384)


  Random features approach has been widely used for kernel approximation in
large-scale machine learning. A number of recent studies have explored
data-dependent sampling of features, modifying the stochastic oracle from which
random features are sampled. While proposed techniques in this realm improve
the approximation, their suitability is often verified on a single learning
task. In this paper, we propose a task-specific scoring rule for selecting
random features, which can be employed for different applications with some
adjustments. We restrict our attention to Canonical Correlation Analysis (CCA),
and we provide a novel, principled guide for finding the score function
maximizing the canonical correlations. We prove that this method, called ORCCA,
can outperform (in expectation) the corresponding Kernel CCA with a default
kernel. Numerical experiments verify that ORCCA is significantly superior than
other approximation techniques in the CCA task.

    

### [[2001.04385] Universal Differential Equations for Scientific Machine Learning](http://arxiv.org/abs/2001.04385)


  In the context of science, the well-known adage "a picture is worth a
thousand words" might well be "a model is worth a thousand datasets." In this
manuscript we introduce the SciML software ecosystem as a tool for mixing the
information of physical laws and scientific models with data-driven machine
learning approaches. We describe a mathematical object, which we denote
universal differential equations (UDEs), as the unifying framework connecting
the ecosystem. We show how a wide variety of applications, from automatically
discovering biological mechanisms to solving high-dimensional
Hamilton-Jacobi-Bellman equations, can be phrased and efficiently handled
through the UDE formalism and its tooling. We demonstrate the generality of the
software tooling to handle stochasticity, delays, and implicit constraints.
This funnels the wide variety of SciML applications into a core set of training
mechanisms which are highly optimized, stabilized for stiff equations, and
compatible with distributed parallelism and GPU accelerators.

    

### [[2003.08904] RAB: Provable Robustness Against Backdoor Attacks](http://arxiv.org/abs/2003.08904)


  Recent studies have shown that deep neural networks (DNNs) are vulnerable to
adversarial attacks, including evasion and backdoor (poisoning) attacks. On the
defense side, there have been intensive efforts on improving both empirical and
provable robustness against evasion attacks; however, provable robustness
against backdoor attacks still remains largely unexplored. In this paper, we
focus on certifying the machine learning model robustness against general
threat models, especially backdoor attacks. We first provide a unified
framework via randomized smoothing techniques and show how it can be
instantiated to certify the robustness against both evasion and backdoor
attacks. We then propose the first robust training process, RAB, to smooth the
trained model and certify its robustness against backdoor attacks. We derive
the robustness bound for machine learning models trained with RAB, and prove
that our robustness bound is tight. In addition, we show that it is possible to
train the robust smoothed models efficiently for simple models such as
K-nearest neighbor classifiers, and we propose an exact smooth-training
algorithm which eliminates the need to sample from a noise distribution for
such models. Empirically, we conduct comprehensive experiments for different
machine learning (ML) models such as DNNs, differentially private DNNs, and
K-NN models on MNIST, CIFAR-10 and ImageNet datasets, and provide the first
benchmark for certified robustness against backdoor attacks. In addition, we
evaluate K-NN models on a spambase tabular dataset to demonstrate the
advantages of the proposed exact algorithm. Both the theoretic analysis and the
comprehensive evaluation on diverse ML models and datasets shed lights on
further robust learning strategies against general training time attacks.

    

### [[2006.06555] Multi-Agent Reinforcement Learning in Stochastic Networked Systems](http://arxiv.org/abs/2006.06555)


  We study multi-agent reinforcement learning (MARL) in a stochastic network of
agents. The objective is to find localized policies that maximize the
(discounted) global reward. In general, scalability is a challenge in this
setting because the size of the global state/action space can be exponential in
the number of agents. Scalable algorithms are only known in cases where
dependencies are static, fixed and local, e.g., between neighbors in a fixed,
time-invariant underlying graph. In this work, we propose a Scalable Actor
Critic framework that applies in settings where the dependencies can be
non-local and stochastic, and provide a finite-time error bound that shows how
the convergence rate depends on the speed of information spread in the network.
Additionally, as a byproduct of our analysis, we obtain novel finite-time
convergence results for a general stochastic approximation scheme and for
temporal difference learning with state aggregation, which apply beyond the
setting of MARL in networked systems.

    

### [[2006.06721] Backdoor Smoothing: Demystifying Backdoor Attacks on Deep Neural Networks](http://arxiv.org/abs/2006.06721)


  Backdoor attacks mislead machine-learning models to output an
attacker-specified class when presented a specific trigger at test time. These
attacks require poisoning the training data to compromise the learning
algorithm, e.g., by injecting poisoning samples containing the trigger into the
training set, along with the desired class label. Despite the increasing number
of studies on backdoor attacks and defenses, the underlying factors affecting
the success of backdoor attacks, along with their impact on the learning
algorithm, are not yet well understood. In this work, we aim to shed light on
this issue by unveiling that backdoor attacks induce a smoother decision
function around the triggered samples -- a phenomenon which we refer to as
\textit{backdoor smoothing}. To quantify backdoor smoothing, we define a
measure that evaluates the uncertainty associated to the predictions of a
classifier around the input samples.
Our experiments show that smoothness increases when the trigger is added to
the input samples, and that this phenomenon is more pronounced for more
successful attacks.
We also provide preliminary evidence that backdoor triggers are not the only
smoothing-inducing patterns, but that also other artificial patterns can be
detected by our approach, paving the way towards understanding the limitations
of current defenses and designing novel ones.

    

### [[2006.14841] Not all Failure Modes are Created Equal: Training Deep Neural Networks for Explicable (Mis)Classification](http://arxiv.org/abs/2006.14841)


  Deep Neural Networks are often brittle on image classification tasks and
known to misclassify inputs. While these misclassifications may be inevitable,
all failure modes cannot be considered equal. Certain misclassifications (eg.
classifying the image of a dog to an airplane) can perplex humans and result in
the loss of human trust in the system. Even worse, these errors (eg. a person
misclassified as a primate) can have odious societal impacts. Thus, in this
work, we aim to reduce inexplicable errors. To address this challenge, we first
discuss methods to obtain the class-level semantics that capture the human's
expectation ($M^h$) regarding which classes are semantically close {\em vs.}
ones that are far away. We show that for popular image benchmarks (like
CIFAR-10, CIFAR-100, ImageNet), class-level semantics can be readily obtained
by leveraging either human subject studies or publicly available human-curated
knowledge bases. Second, we propose the use of Weighted Loss Functions (WLFs)
to penalize misclassifications by the weight of their inexplicability. Finally,
we show that training (or fine-tuning) existing classifiers with the proposed
methods lead to Deep Neural Networks that have (1) comparable top-1 accuracy,
(2) more explicable failure modes on both in-distribution and
out-of-distribution (OOD) test data, and (3) incur significantly less cost in
the gathering of additional human labels compared to existing works.

    

### [[2010.00300] OutbreakFlow: Model-based Bayesian inference of disease outbreak dynamics with invertible neural networks and its application to the COVID-19 pandemics in Germany](http://arxiv.org/abs/2010.00300)


  Mathematical models in epidemiology are an indispensable tool to determine
the dynamics and important characteristics of infectious diseases. Apart from
their scientific merit, these models are often used to inform political
decisions and intervention measures during an ongoing outbreak. However,
reliably inferring the dynamics of ongoing outbreaks by connecting complex
models to real data is still hard and requires either laborious manual
parameter fitting or expensive optimization methods which have to be repeated
from scratch for every application of a given model. In this work, we address
this problem with a novel combination of epidemiological modeling with
specialized neural networks. Our approach entails two computational phases: In
an initial training phase, a mathematical model describing the epidemic is used
as a coach for a neural network, which acquires global knowledge about the full
range of possible disease dynamics. In the subsequent inference phase, the
trained neural network processes the observed data of an actual outbreak and
infers the parameters of the model in order to realistically reproduce the
observed dynamics and reliably predict future progression. With its flexible
framework, our simulation-based approach is applicable to a variety of
epidemiological models. Moreover, since our method is fully Bayesian, it is
designed to incorporate all available prior knowledge about plausible parameter
values and returns complete joint posterior distributions over these
parameters. Application of our method to the early Covid-19 outbreak phase in
Germany demonstrates that we are able to obtain reliable probabilistic
estimates for important disease characteristics, such as generation time,
fraction of undetected infections, likelihood of transmission before symptom
onset, and reporting delays using a very moderate amount of real-world
observations.

    

### [[2010.01748] Policy Learning Using Weak Supervision](http://arxiv.org/abs/2010.01748)


  Most existing policy learning solutions require the learning agents to
receive high-quality supervision signals such as well-designed rewards in
reinforcement learning (RL) or high-quality expert demonstrations in behavioral
cloning (BC). These quality supervisions are usually infeasible or
prohibitively expensive to obtain in practice. We aim for a unified framework
that leverages the available cheap weak supervisions to perform policy learning
efficiently. To handle this problem, we treat the "weak supervision" as
imperfect information coming from a peer agent, and evaluate the learning
agent's policy based on a "correlated agreement" with the peer agent's policy
(instead of simple agreements). Our approach explicitly punishes a policy for
overfitting to the weak supervision. In addition to theoretical guarantees,
extensive evaluations on tasks including RL with noisy rewards, BC with weak
demonstrations, and standard policy co-training show that our method leads to
substantial performance improvements, especially when the complexity or the
noise of the learning environments is high.

    

### [[2010.13363] Provable Memorization via Deep Neural Networks using Sub-linear Parameters](http://arxiv.org/abs/2010.13363)


  It is known that $O(N)$ parameters are sufficient for neural networks to
memorize arbitrary $N$ input-label pairs. By exploiting depth, we show that
$O(N^{2/3})$ parameters suffice to memorize $N$ pairs, under a mild condition
on the separation of input points. In particular, deeper networks (even with
width $3$) are shown to memorize more pairs than shallow networks, which also
agrees with the recent line of works on the benefits of depth for function
approximation. We also provide empirical results that support our theoretical
findings.

    

### [[2011.11201] Modular Action Concept Grounding in Semantic Video Prediction](http://arxiv.org/abs/2011.11201)


  Recent works in video prediction have mainly focused on passive forecasting
and low-level action-conditional prediction, which sidesteps the learning of
interaction between agents and objects. We introduce the task of semantic
action-conditional video prediction, which uses semantic action labels to
describe those interactions and can be regarded as an inverse problem of action
recognition. The challenge of this new task primarily lies in how to
effectively inform the model of semantic action information. Inspired by the
idea of Mixture of Experts, we embody each abstract label by a structured
combination of various visual concept learners and propose a novel video
prediction model, Modular Action Concept Network (MAC). Our method is evaluated
on two newly designed synthetic datasets, CLEVR-Building-Blocks and
Sapien-Kitchen, and one real-world dataset called Tower-Creation. Extensive
experiments demonstrate that MAC can correctly condition on given instructions
and generate corresponding future frames without need of bounding boxes. We
further show that the trained model can make out-of-distribution
generalization, be quickly adapted to new object categories and exploit its
learnt features for object detection, showing the progression towards
higher-level cognitive abilities.

    

### [[2102.09759] Applications of deep learning in traffic congestion detection, prediction and alleviation: A survey](http://arxiv.org/abs/2102.09759)


  Detecting, predicting, and alleviating traffic congestion are targeted at
improving the level of service of the transportation network. With increasing
access to larger datasets of higher resolution, the relevance of deep learning
for such tasks is increasing. Several comprehensive survey papers in recent
years have summarised the deep learning applications in the transportation
domain. However, the system dynamics of the transportation network vary greatly
between the non-congested state and the congested state -- thereby
necessitating the need for a clear understanding of the challenges specific to
congestion prediction. In this survey, we present the current state of deep
learning applications in the tasks related to detection, prediction, and
alleviation of congestion. Recurring and non-recurring congestion are discussed
separately. Our survey leads us to uncover inherent challenges and gaps in the
current state of research. Finally, we present some suggestions for future
research directions as answers to the identified challenges.

    

### [[2102.09808] Improving Anytime Prediction with Parallel Cascaded Networks and a Temporal-Difference Loss](http://arxiv.org/abs/2102.09808)


  Although deep feedforward neural networks share some characteristics with the
primate visual system, a key distinction is their dynamics. Deep nets typically
operate in serial stages wherein each layer completes its computation before
processing begins in subsequent layers. In contrast, biological systems have
cascaded dynamics: information propagates from neurons at all layers in
parallel but transmission occurs gradually over time, leading to speed-accuracy
trade offs even in feedforward architectures. We explore the consequences of
biologically inspired parallel hardware by constructing cascaded ResNets in
which each residual block has propagation delays but all blocks update in
parallel in a stateful manner. Because information transmitted through skip
connections avoids delays, the functional depth of the architecture increases
over time, yielding anytime predictions that improve with internal-processing
time. We introduce a temporal-difference training loss that achieves a strictly
superior speed-accuracy profile over standard losses and enables the cascaded
architecture to outperform state-of-the-art anytime-prediction methods. The
cascaded architecture has intriguing properties, including: it classifies
typical instances more rapidly than atypical instances; it is more robust to
both persistent and transient noise than is a conventional ResNet; and its
time-varying output trace provides a signal that can be exploited to improve
information processing and inference.

    

### [[2102.11903] Neural ranking models for document retrieval](http://arxiv.org/abs/2102.11903)


  Ranking models are the main components of information retrieval systems.
Several approaches to ranking are based on traditional machine learning
algorithms using a set of hand-crafted features. Recently, researchers have
leveraged deep learning models in information retrieval. These models are
trained end-to-end to extract features from the raw data for ranking tasks, so
that they overcome the limitations of hand-crafted features. A variety of deep
learning models have been proposed, and each model presents a set of neural
network components to extract features that are used for ranking. In this
paper, we compare the proposed models in the literature along different
dimensions in order to understand the major contributions and limitations of
each model. In our discussion of the literature, we analyze the promising
neural components, and propose future research directions. We also show the
analogy between document retrieval and other retrieval tasks where the items to
be ranked are structured documents, answers, images and videos.

    

### [[2103.01242] Cryptonite: A Cryptic Crossword Benchmark for Extreme Ambiguity in Language](http://arxiv.org/abs/2103.01242)


  Current NLP datasets targeting ambiguity can be solved by a native speaker
with relative ease. We present Cryptonite, a large-scale dataset based on
cryptic crosswords, which is both linguistically complex and naturally sourced.
Each example in Cryptonite is a cryptic clue, a short phrase or sentence with a
misleading surface reading, whose solving requires disambiguating semantic,
syntactic, and phonetic wordplays, as well as world knowledge. Cryptic clues
pose a challenge even for experienced solvers, though top-tier experts can
solve them with almost 100% accuracy. Cryptonite is a challenging task for
current models; fine-tuning T5-Large on 470k cryptic clues achieves only 7.6%
accuracy, on par with the accuracy of a rule-based clue solver (8.6%).

    

### [[2103.04804] Detecting quantum entanglement with unsupervised learning](http://arxiv.org/abs/2103.04804)


  Quantum properties, such as entanglement and coherence, are indispensable
resources in various quantum information processing tasks. However, there still
lacks an efficient and scalable way to detecting these useful features,
especially for high-dimensional and multipartite quantum systems. In this work,
we exploit the convexity of samples without the desired quantum features and
design an unsupervised machine learning method to detect the presence of such
features as anomalies. Particularly, in the context of entanglement detection,
we propose a complex-valued neural network composed of pseudo-siamese network
and generative adversarial net, and then train it with only separable states to
construct non-linear witnesses for entanglement. It is shown via numerical
examples, ranging from two-qubit to ten-qubit systems, that our network is able
to achieve high detection accuracy which is above 97.5% on average.Moreover, it
is capable of revealing rich structures of entanglement, such as partial
entanglement among subsystems. Our results are readily applicable to the
detection of other quantum resources such as Bell nonlocality and steerability,
and thus our work could provide a powerful tool to extract quantum features
hidden in multipartite quantum data.

    

### [[2104.02040] Segmentation of EM showers for neutrino experiments with deep graph neural networks](http://arxiv.org/abs/2104.02040)


  We introduce a first-ever algorithm for the reconstruction of multiple
showers from the data collected with electromagnetic (EM) sampling
calorimeters. Such detectors are widely used in High Energy Physics to measure
the energy and kinematics of in-going particles. In this work, we consider the
case when many electrons pass through an Emulsion Cloud Chamber (ECC) brick,
initiating electron-induced electromagnetic showers, which can be the case with
long exposure times or large input particle flux. For example, SHiP experiment
is planning to use emulsion detectors for dark matter search and neutrino
physics investigation. The expected full flux of SHiP experiment is about 10^20
particles over five years. To reduce the cost of the experiment associated with
the replacement of the ECC brick and off-line data taking (emulsion scanning),
it is decided to increase exposure time. Thus, we expect to observe a lot of
overlapping showers, which turn EM showers reconstruction into a challenging
point cloud segmentation problem. Our reconstruction pipeline consists of a
Graph Neural Network that predicts an adjacency matrix and a clustering
algorithm. We propose a new layer type (EmulsionConv) that takes into account
geometrical properties of shower development in ECC brick. For the clustering
of overlapping showers, we use a modified hierarchical density-based clustering
algorithm. Our method does not use any prior information about the incoming
particles and identifies up to 87% of electromagnetic showers in emulsion
detectors. The main test bench for the algorithm for reconstructing
electromagnetic showers is going to be SND@LHC.

    

### [[2104.12820] Universal Off-Policy Evaluation](http://arxiv.org/abs/2104.12820)


  When faced with sequential decision-making problems, it is often useful to be
able to predict what would happen if decisions were made using a new policy.
Those predictions must often be based on data collected under some previously
used decision-making rule. Many previous methods enable such off-policy (or
counterfactual) estimation of the expected value of a performance measure
called the return. In this paper, we take the first steps towards a universal
off-policy estimator (UnO) -- one that provides off-policy estimates and
high-confidence bounds for any parameter of the return distribution. We use UnO
for estimating and simultaneously bounding the mean, variance,
quantiles/median, inter-quantile range, CVaR, and the entire cumulative
distribution of returns. Finally, we also discuss Uno's applicability in
various settings, including fully observable, partially observable (i.e., with
unobserved confounders), Markovian, non-Markovian, stationary, smoothly
non-stationary, and discrete distribution shifts.

    

### [[2105.01924] Novelty Detection and Analysis of Traffic Scenario Infrastructures in the Latent Space of a Vision Transformer-Based Triplet Autoencoder](http://arxiv.org/abs/2105.01924)


  Detecting unknown and untested scenarios is crucial for scenario-based
testing. Scenario-based testing is considered to be a possible approach to
validate autonomous vehicles. A traffic scenario consists of multiple
components, with infrastructure being one of it. In this work, a method to
detect novel traffic scenarios based on their infrastructure images is
presented. An autoencoder triplet network provides latent representations for
infrastructure images which are used for outlier detection. The triplet
training of the network is based on the connectivity graphs of the
infrastructure. By using the proposed architecture, expert-knowledge is used to
shape the latent space such that it incorporates a pre-defined similarity in
the neighborhood relationships of an autoencoder. An ablation study on the
architecture is highlighting the importance of the triplet autoencoder
combination. The best performing architecture is based on vision transformers,
a convolution-free attention-based network. The presented method outperforms
other state-of-the-art outlier detection approaches.

    

### [[2105.02716] Noether's Learning Dynamics: Role of Symmetry Breaking in Neural Networks](http://arxiv.org/abs/2105.02716)


  In nature, symmetry governs regularities, while symmetry breaking brings
texture. In artificial neural networks, symmetry has been a central design
principle to efficiently capture regularities in the world, but the role of
symmetry breaking is not well understood. Here, we develop a theoretical
framework to study the "geometry of learning dynamics" in neural networks, and
reveal a key mechanism of explicit symmetry breaking behind the efficiency and
stability of modern neural networks. To build this understanding, we model the
discrete learning dynamics of gradient descent using a continuous-time
Lagrangian formulation, in which the learning rule corresponds to the kinetic
energy and the loss function corresponds to the potential energy. Then, we
identify "kinetic symmetry breaking" (KSB), the condition when the kinetic
energy explicitly breaks the symmetry of the potential function. We generalize
Noether's theorem known in physics to take into account KSB and derive the
resulting motion of the Noether charge: "Noether's Learning Dynamics" (NLD).
Finally, we apply NLD to neural networks with normalization layers and reveal
how KSB introduces a mechanism of "implicit adaptive optimization",
establishing an analogy between learning dynamics induced by normalization
layers and RMSProp. Overall, through the lens of Lagrangian mechanics, we have
established a theoretical foundation to discover geometric design principles
for the learning dynamics of neural networks.

    

### [[2105.11066] Policy Mirror Descent for Regularized Reinforcement Learning: A Generalized Framework with Linear Convergence](http://arxiv.org/abs/2105.11066)


  Policy optimization, which learns the policy of interest by maximizing the
value function via large-scale optimization techniques, lies at the heart of
modern reinforcement learning (RL). In addition to value maximization, other
practical considerations arise commonly as well, including the need of
encouraging exploration, and that of ensuring certain structural properties of
the learned policy due to safety, resource and operational constraints. These
considerations can often be accounted for by resorting to regularized RL, which
augments the target value function with a structure-promoting regularization
term. Focusing on an infinite-horizon discounted Markov decision process, this
paper proposes a generalized policy mirror descent (GPMD) algorithm for solving
regularized RL. As a generalization of policy mirror descent Lan (2021), the
proposed algorithm accommodates a general class of convex regularizers as well
as a broad family of Bregman divergence in cognizant of the regularizer in use.
We demonstrate that our algorithm converges linearly over an entire range of
learning rates, in a dimension-free fashion, to the global solution, even when
the regularizer lacks strong convexity and smoothness. In addition, this linear
convergence feature is provably stable in the face of inexact policy evaluation
and imperfect policy updates. Numerical experiments are provided to corroborate
the applicability and appealing performance of GPMD.

    

### [[2105.11367] FedScale: Benchmarking Model and System Performance of Federated Learning at Scale](http://arxiv.org/abs/2105.11367)


  We present FedScale, a diverse set of challenging and realistic benchmark
datasets to facilitate scalable, comprehensive, and reproducible federated
learning (FL) research. FedScale datasets are large-scale, encompassing a
diverse range of important FL tasks, such as image classification, object
detection, word prediction, and speech recognition. For each dataset, we
provide a unified evaluation protocol using realistic data splits and
evaluation metrics. To meet the pressing need for reproducing realistic FL at
scale, we have also built an efficient evaluation platform to simplify and
standardize the process of FL experimental setup and model evaluation. Our
evaluation platform provides flexible APIs to implement new FL algorithms and
includes new execution backends with minimal developer efforts. Finally, we
perform in-depth benchmark experiments on these datasets. Our experiments
suggest fruitful opportunities in heterogeneity-aware co-optimizations of the
system and statistical efficiency under realistic FL characteristics. FedScale
is open-source with permissive licenses and actively maintained, and we welcome
feedback and contributions from the community.

    

### [[2106.03632] Quantifying and Improving Transferability in Domain Generalization](http://arxiv.org/abs/2106.03632)


  Out-of-distribution generalization is one of the key challenges when
transferring a model from the lab to the real world. Existing efforts mostly
focus on building invariant features among source and target domains. Based on
invariant features, a high-performing classifier on source domains could
hopefully behave equally well on a target domain. In other words, the invariant
features are \emph{transferable}. However, in practice, there are no perfectly
transferable features, and some algorithms seem to learn "more transferable"
features than others. How can we understand and quantify such
\emph{transferability}? In this paper, we formally define transferability that
one can quantify and compute in domain generalization. We point out the
difference and connection with common discrepancy measures between domains,
such as total variation and Wasserstein distance. We then prove that our
transferability can be estimated with enough samples and give a new upper bound
for the target error based on our transferability. Empirically, we evaluate the
transferability of the feature embeddings learned by existing algorithms for
domain generalization. Surprisingly, we find that many algorithms are not quite
learning transferable features, although few could still survive. In light of
this, we propose a new algorithm for learning transferable features and test it
over various benchmark datasets, including RotatedMNIST, PACS, Office-Home and
WILDS-FMoW. Experimental results show that the proposed algorithm achieves
consistent improvement over many state-of-the-art algorithms, corroborating our
theoretical findings.

    

### [[2106.03898] SPANet: Generalized Permutationless Set Assignment for Particle Physics using Symmetry Preserving Attention](http://arxiv.org/abs/2106.03898)


  The creation of unstable heavy particles at the Large Hadron Collider is the
most direct way to address some of the deepest open questions in physics.
Collisions typically produce variable-size sets of observed particles which
have inherent ambiguities complicating the assignment of observed particles to
the decay products of the heavy particles. Current strategies for tackling
these challenges in the physics community ignore the physical symmetries of the
decay products and consider all possible assignment permutations and do not
scale to complex configurations. Attention based deep learning methods for
sequence modelling have achieved state-of-the-art performance in natural
language processing, but they lack built-in mechanisms to deal with the unique
symmetries found in physical set-assignment problems. We introduce a novel
method for constructing symmetry-preserving attention networks which reflect
the problem's natural invariances to efficiently find assignments without
evaluating all permutations. This general approach is applicable to arbitrarily
complex configurations and significantly outperforms current methods, improving
reconstruction efficiency between 19\% - 35\% on typical benchmark problems
while decreasing inference time by two to five orders of magnitude on the most
complex events, making many important and previously intractable cases
tractable.
A full code repository containing a general library, the specific
configuration used, and a complete dataset release, are avaiable at
this https URL


### [[2106.04169] On Improving Adversarial Transferability of Vision Transformers](http://arxiv.org/abs/2106.04169)


  Vision transformers (ViTs) process input images as sequences of patches via
self-attention; a radically different architecture than convolutional neural
networks (CNNs). This makes it interesting to study the adversarial feature
space of ViT models and their transferability. In particular, we observe that
adversarial patterns found via conventional adversarial attacks show very low
black-box transferability even for large ViT models. However, we show that this
phenomenon is only due to the sub-optimal attack procedures that do not
leverage the true representation potential of ViTs. A deep ViT is composed of
multiple blocks, with a consistent architecture comprising of self-attention
and feed-forward layers, where each block is capable of independently producing
a class token. Formulating an attack using only the last class token
(conventional approach) does not directly leverage the discriminative
information stored in the earlier tokens, leading to poor adversarial
transferability of ViTs. Using the compositional nature of ViT models, we
enhance the transferability of existing attacks by introducing two novel
strategies specific to the architecture of ViT models. (i) Self-Ensemble: We
propose a method to find multiple discriminative pathways by dissecting a
single ViT model into an ensemble of networks. This allows explicitly utilizing
class-specific information at each ViT block. (ii) Token Refinement: We then
propose to refine the tokens to further enhance the discriminative capacity at
each block of ViT. Our token refinement systematically combines the class
tokens with structural information preserved within the patch tokens. An
adversarial attack, when applied to such refined tokens within the ensemble of
classifiers found in a single vision transformer, has significantly higher
transferability.

    

### [[2106.04537] Can You Learn an Algorithm? Generalizing from Easy to Hard Problems with Recurrent Networks](http://arxiv.org/abs/2106.04537)


  Deep neural networks are powerful machines for visual pattern recognition,
but reasoning tasks that are easy for humans may still be difficult for neural
models. Humans possess the ability to extrapolate reasoning strategies learned
on simple problems to solve harder examples, often by thinking for longer. For
example, a person who has learned to solve small mazes can easily extend the
very same search techniques to solve much larger mazes by spending more time.
In computers, this behavior is often achieved through the use of algorithms,
which scale to arbitrarily hard problem instances at the cost of more
computation. In contrast, the sequential computing budget of feed-forward
neural networks is limited by their depth, and networks trained on simple
problems have no way of extending their reasoning to accommodate harder
problems. In this work, we show that recurrent networks trained to solve simple
problems with few recurrent steps can indeed solve much more complex problems
simply by performing additional recurrences during inference. We demonstrate
this algorithmic behavior of recurrent networks on prefix sum computation,
mazes, and chess. In all three domains, networks trained on simple problem
instances are able to extend their reasoning abilities at test time simply by
"thinking for longer."

    

### [[2106.05378] Parameter and Feature Selection in Stochastic Linear Bandits](http://arxiv.org/abs/2106.05378)


  We study two model selection settings in stochastic linear bandits (LB). In
the first setting, which we refer to as feature selection, the expected reward
of the LB problem is in the linear span of at least one of $M$ feature maps
(models). In the second setting, the reward parameter of the LB problem is
arbitrarily selected from $M$ models represented as (possibly) overlapping
balls in $\mathbb R^d$. However, the agent only has access to misspecified
models, i.e., estimates of the centers and radii of the balls. We refer to this
setting as parameter selection. For each setting, we develop and analyze an
algorithm that is based on a reduction from bandits to full-information
problems. This allows us to obtain regret bounds that are not worse (up to a
$\sqrt{\log M}$ factor) than the case where the true model is known. The regret
of our parameter selection algorithm also scales logarithmically with model
uncertainty. Finally, we empirically show the effectiveness of our algorithms
using synthetic and real-world experiments.

    

### [[2106.06308] The Complexity of Sparse Tensor PCA](http://arxiv.org/abs/2106.06308)


  We study the problem of sparse tensor principal component analysis: given a
tensor $\pmb Y = \pmb W + \lambda x^{\otimes p}$ with $\pmb W \in
\otimes^p\mathbb{R}^n$ having i.i.d. Gaussian entries, the goal is to recover
the $k$-sparse unit vector $x \in \mathbb{R}^n$. The model captures both sparse
PCA (in its Wigner form) and tensor PCA.
For the highly sparse regime of $k \leq \sqrt{n}$, we present a family of
algorithms that smoothly interpolates between a simple polynomial-time
algorithm and the exponential-time exhaustive search algorithm. For any $1 \leq
t \leq k$, our algorithms recovers the sparse vector for signal-to-noise ratio
$\lambda \geq \tilde{\mathcal{O}} (\sqrt{t} \cdot (k/t)^{p/2})$ in time
$\tilde{\mathcal{O}}(n^{p+t})$, capturing the state-of-the-art guarantees for
the matrix settings (in both the polynomial-time and sub-exponential time
regimes).
Our results naturally extend to the case of $r$ distinct $k$-sparse signals
with disjoint supports, with guarantees that are independent of the number of
spikes. Even in the restricted case of sparse PCA, known algorithms only
recover the sparse vectors for $\lambda \geq \tilde{\mathcal{O}}(k \cdot r)$
while our algorithms require $\lambda \geq \tilde{\mathcal{O}}(k)$.
Finally, by analyzing the low-degree likelihood ratio, we complement these
algorithmic results with rigorous evidence illustrating the trade-offs between
signal-to-noise ratio and running time. This lower bound captures the known
lower bounds for both sparse PCA and tensor PCA. In this general model, we
observe a more intricate three-way trade-off between the number of samples $n$,
the sparsity $k$, and the tensor power $p$.

    

### [[2106.07504] Characterizing the risk of fairwashing](http://arxiv.org/abs/2106.07504)


  Fairwashing refers to the risk that an unfair black-box model can be
explained by a fairer model through post-hoc explanation manipulation. In this
paper, we investigate the capability of fairwashing attacks by analyzing their
fidelity-unfairness trade-offs. In particular, we show that fairwashed
explanation models can generalize beyond the suing group (i.e., data points
that are being explained), meaning that a fairwashed explainer can be used to
rationalize subsequent unfair decisions of a black-box model. We also
demonstrate that fairwashing attacks can transfer across black-box models,
meaning that other black-box models can perform fairwashing without explicitly
using their predictions. This generalization and transferability of fairwashing
attacks imply that their detection will be difficult in practice. Finally, we
propose an approach to quantify the risk of fairwashing, which is based on the
computation of the range of the unfairness of high-fidelity explainers.

    

### [[2106.16187] Reinforcement Learning based Disease Progression Model for Alzheimer's Disease](http://arxiv.org/abs/2106.16187)


  We model Alzheimer's disease (AD) progression by combining differential
equations (DEs) and reinforcement learning (RL) with domain knowledge. DEs
provide relationships between some, but not all, factors relevant to AD. We
assume that the missing relationships must satisfy general criteria about the
working of the brain, for e.g., maximizing cognition while minimizing the cost
of supporting cognition. This allows us to extract the missing relationships by
using RL to optimize an objective (reward) function that captures the above
criteria. We use our model consisting of DEs (as a simulator) and the trained
RL agent to predict individualized 10-year AD progression using baseline (year
0) features on synthetic and real data. The model was comparable or better at
predicting 10-year cognition trajectories than state-of-the-art learning-based
models. Our interpretable model demonstrated, and provided insights into,
"recovery/compensatory" processes that mitigate the effect of AD, even though
those processes were not explicitly encoded in the model. Our framework
combines DEs with RL for modelling AD progression and has broad applicability
for understanding other neurological disorders.

    

### [[2110.01445] Robust and Decomposable Average Precision for Image Retrieval](http://arxiv.org/abs/2110.01445)


  In image retrieval, standard evaluation metrics rely on score ranking, e.g.
average precision (AP). In this paper, we introduce a method for robust and
decomposable average precision (ROADMAP) addressing two major challenges for
end-to-end training of deep neural networks with AP: non-differentiability and
non-decomposability. Firstly, we propose a new differentiable approximation of
the rank function, which provides an upper bound of the AP loss and ensures
robust training. Secondly, we design a simple yet effective loss function to
reduce the decomposability gap between the AP in the whole training set and its
averaged batch approximation, for which we provide theoretical guarantees.
Extensive experiments conducted on three image retrieval datasets show that
ROADMAP outperforms several recent AP approximation methods and highlight the
importance of our two contributions. Finally, using ROADMAP for training deep
models yields very good performances, outperforming state-of-the-art results on
the three datasets.

    

### [[2110.13067] Evolutionary Optimization of High-Coverage Budgeted Classifiers](http://arxiv.org/abs/2110.13067)


  Classifiers are often utilized in time-constrained settings where labels must
be assigned to inputs quickly. To address these scenarios, budgeted multi-stage
classifiers (MSC) process inputs through a sequence of partial feature
acquisition and evaluation steps with early-exit options until a confident
prediction can be made. This allows for fast evaluation that can prevent
expensive, unnecessary feature acquisition in time-critical instances. However,
performance of MSCs is highly sensitive to several design aspects -- making
optimization of these systems an important but difficult problem.
To approximate an initially intractable combinatorial problem, current
approaches to MSC configuration rely on well-behaved surrogate loss functions
accounting for two primary objectives (processing cost, error). These
approaches have proven useful in many scenarios but are limited by analytic
constraints (convexity, smoothness, etc.) and do not manage additional
performance objectives. Notably, such methods do not explicitly account for an
important aspect of real-time detection systems -- the ratio of "accepted"
predictions satisfying some confidence criterion imposed by a risk-averse
monitor.
This paper proposes a problem-specific genetic algorithm, EMSCO, that
incorporates a terminal reject option for indecisive predictions and treats MSC
design as an evolutionary optimization problem with distinct objectives
(accuracy, cost, coverage). The algorithm's design emphasizes Pareto efficiency
while respecting a notion of aggregated performance via a unique scalarization.
Experiments are conducted to demonstrate EMSCO's ability to find global optima
in a variety of Theta(k^n) solution spaces, and multiple experiments show EMSCO
is competitive with alternative budgeted approaches.

    

### [[2111.00856] Large-Scale Deep Learning Optimizations: A Comprehensive Survey](http://arxiv.org/abs/2111.00856)


  Deep learning have achieved promising results on a wide spectrum of AI
applications. Larger datasets and models consistently yield better performance.
However, we generally spend longer training time on more computation and
communication. In this survey, we aim to provide a clear sketch about the
optimizations for large-scale deep learning with regard to the model accuracy
and model efficiency. We investigate algorithms that are most commonly used for
optimizing, elaborate the debatable topic of generalization gap arises in
large-batch training, and review the SOTA strategies in addressing the
communication overhead and reducing the memory footprints.

    

### [[2111.00905] Smart Fashion: A Review of AI Applications in the Fashion & Apparel Industry](http://arxiv.org/abs/2111.00905)


  The fashion industry is on the verge of an unprecedented change. The
implementation of machine learning, computer vision, and artificial
intelligence (AI) in fashion applications is opening lots of new opportunities
for this industry. This paper provides a comprehensive survey on this matter,
categorizing more than 580 related articles into 22 well-defined
fashion-related tasks. Such structured task-based multi-label classification of
fashion research articles provides researchers with explicit research
directions and facilitates their access to the related studies, improving the
visibility of studies simultaneously. For each task, a time chart is provided
to analyze the progress through the years. Furthermore, we provide a list of 86
public fashion datasets accompanied by a list of suggested applications and
additional information for each.

    

### [[2111.00929] Bounds all around: training energy-based models with bidirectional bounds](http://arxiv.org/abs/2111.00929)


  Energy-based models (EBMs) provide an elegant framework for density
estimation, but they are notoriously difficult to train. Recent work has
established links to generative adversarial networks, where the EBM is trained
through a minimax game with a variational value function. We propose a
bidirectional bound on the EBM log-likelihood, such that we maximize a lower
bound and minimize an upper bound when solving the minimax game. We link one
bound to a gradient penalty that stabilizes training, thereby providing
grounding for best engineering practice. To evaluate the bounds we develop a
new and efficient estimator of the Jacobi-determinant of the EBM generator. We
demonstrate that these developments significantly stabilize training and yield
high-quality density estimation and sample generation.

    

### [[2111.00947] Nested Multiple Instance Learning with Attention Mechanisms](http://arxiv.org/abs/2111.00947)


  Multiple instance learning (MIL) is a type of weakly supervised learning
where multiple instances of data with unknown labels are sorted into bags.
Since knowledge about the individual instances is incomplete, labels are
assigned to the bags containing the instances. While this method fits diverse
applications were labelled data is scarce, it lacks depth for solving more
complex scenarios where associations between sets of instances have to be made,
like finding relevant regions of interest in an image or detecting events in a
set of time-series signals. Nested MIL considers labelled bags within bags,
where only the outermost bag is labelled and inner-bags and instances are
represented as latent labels. In addition, we propose using an attention
mechanism to add interpretability, providing awareness into the impact of each
instance to the weak bag label. Experiments in classical image datasets show
that our proposed model provides high accuracy performance as well as spotting
relevant instances on image regions.

    

### [[2111.00961] Robustness of deep learning algorithms in astronomy -- galaxy morphology studies](http://arxiv.org/abs/2111.00961)


  Deep learning models are being increasingly adopted in wide array of
scientific domains, especially to handle high-dimensionality and volume of the
scientific data. However, these models tend to be brittle due to their
complexity and overparametrization, especially to the inadvertent adversarial
perturbations that can appear due to common image processing such as
compression or blurring that are often seen with real scientific data. It is
crucial to understand this brittleness and develop models robust to these
adversarial perturbations. To this end, we study the effect of observational
noise from the exposure time, as well as the worst case scenario of a one-pixel
attack as a proxy for compression or telescope errors on performance of
ResNet18 trained to distinguish between galaxies of different morphologies in
LSST mock data. We also explore how domain adaptation techniques can help
improve model robustness in case of this type of naturally occurring attacks
and help scientists build more trustworthy and stable models.

    

### [[2111.01369] Wafer-level Variation Modeling for Multi-site RF IC Testing via Hierarchical Gaussian Process](http://arxiv.org/abs/2111.01369)


  Wafer-level performance prediction has been attracting attention to reduce
measurement costs without compromising test quality in production tests.
Although several efficient methods have been proposed, the site-to-site
variation, which is often observed in multi-site testing for radio frequency
circuits, has not yet been sufficiently addressed. In this paper, we propose a
wafer-level performance prediction method for multi-site testing that can
consider the site-to-site variation. The proposed method is based on the
Gaussian process, which is widely used for wafer-level spatial correlation
modeling, improving the prediction accuracy by extending hierarchical modeling
to exploit the test site information provided by test engineers. In addition,
we propose an active test-site sampling method to maximize measurement cost
reduction. Through experiments using industrial production test data, we
demonstrate that the proposed method can reduce the estimation error to 1/19 of
that obtained using a conventional method. Moreover, we demonstrate that the
proposed sampling method can reduce the number of the measurements by 97% while
achieving sufficient estimation accuracy.

    

### [[2009.07773] SideLine: How Delay-Lines (May) Leak Secrets from your SoC](http://arxiv.org/abs/2009.07773)


  To meet the ever-growing need for performance in silicon devices, SoC
providers have been increasingly relying on software-hardware cooperation. By
controlling hardware resources such as power or clock management from the
software, developers earn the possibility to build more flexible and power
efficient applications. Despite the benefits, these hardware components are now
exposed to software code and can potentially be misused as open-doors to
jeopardize trusted environments, perform privilege escalation or steal
cryptographic secrets. In this work, we introduce SideLine, a novel
side-channel vector based on delay-line components widely implemented in
high-end SoCs. After providing a detailed method on how to access and convert
delay-line data into power consumption information, we demonstrate that these
entities can be used to perform remote power side-channel attacks. We report
experiments carried out on two SoCs from distinct vendors and we recount
several core-vs-core attack scenarios in which an adversary process located in
one processor core aims at eavesdropping the activity of a victim process
located in another core. For each scenario, we demonstrate the adversary
ability to fully recover the secret key of an OpenSSL AES running in the victim
core. Even more detrimental, we show that these attacks are still practicable
if the victim or the attacker program runs over an operating system.

    

### [[2111.01425] Rational Agreement in the Presence of Crash Faults](http://arxiv.org/abs/2111.01425)


  Blockchain systems need to solve consensus despite the presence of rational
users and failures. The notion of $(k,t)$-robustness has shown instrumental to
list problems that cannot be solved if $k$ players are rational and $t$ players
are Byzantine or act arbitrarily. What is less clear is whether one can solve
such problems if the faults are benign.
In this paper, we bridge the gap between games that are robust against
Byzantine players and games that are robust against crash players. Our first
result is an impossibility result:
We show that no $(k,t)$-robust consensus protocol can solve consensus in the
crash model if $k+2t\geq n$ unless there is a particular punishment strategy,
called the $(k,t)$-baiting strategy. This reveals the need to introduce baiting
as the act of rewarding a colluding node when betraying its coalition, to make
blockchains more secure.
Our second result is an equivalence relation between crash fault tolerant
games and Byzantine fault tolerant games, which raises an interesting research
question on the power of baiting to solve consensus. To this end, we show, on
the one hand, that a $(k,t)$-robust consensus protocol becomes $(k+t,t)$-robust
in the crash model. We show, on the other hand, that the existence of a
$(k,t)$-robust consensus protocol in the crash model that does not make use of
a baiting strategy implies the existence of a $(k-t,t)$-robust consensus
protocol in the Byzantine model, with the help of cryptography.

    

### [[2111.01504] Towards Enabling I/O Awareness in Task-based Programming Models](http://arxiv.org/abs/2111.01504)


  Storage systems have not kept the same technology improvement rate as
computing systems. As applications produce more and more data, I/O becomes the
limiting factor for increasing application performance. I/O congestion caused
by concurrent access to storage devices is one of the main obstacles that cause
I/O performance degradation and, consequently, total performance degradation.
Although task-based programming models made it possible to achieve higher
levels of parallelism by enabling the execution of tasks in large-scale
distributed platforms, this parallelism only benefited the compute workload of
the application. Previous efforts addressing I/O performance bottlenecks either
focused on optimizing fine-grained I/O access patterns using I/O libraries or
avoiding system-wide I/O congestion by minimizing interference between multiple
applications.
In this paper, we propose enabling I/O Awareness in task-based programming
models for improving the total performance of applications. An I/O aware
programming model is able to create more parallelism and mitigate the causes of
I/O performance degradation. On the one hand, more parallelism can be created
by supporting special tasks for executing I/O workloads, called I/O tasks, that
can overlap with the execution of compute tasks. On the other hand, I/O
congestion can be mitigated by constraining I/O tasks scheduling. We propose
two approaches for specifying such constraints: explicitly set by the users or
automatically inferred and tuned during application's execution to optimize the
execution of variable I/O workloads on a certain storage infrastructure.
Our experiments on the MareNostrum 4 Supercomputer demonstrate that using I/O
aware programming model can achieve up to 43% total performance improvement as
compared to the I/O non-aware implementation.

    

### [[2103.00091] Design and Performance Characterization of RADICAL-Pilot on Leadership-class Platforms](http://arxiv.org/abs/2103.00091)


  Many extreme scale scientific applications have workloads comprised of a
large number of individual high-performance tasks. The Pilot abstraction
decouples workload specification, resource management, and task execution via
job placeholders and late-binding. As such, suitable implementations of the
Pilot abstraction can support the collective execution of large number of tasks
on supercomputers. We introduce RADICAL-Pilot (RP) as a portable, modular and
extensible pilot-enabled runtime system. We describe RP's design, architecture
and implementation. We characterize its performance and show its ability to
scalably execute workloads comprised of tens of thousands heterogeneous tasks
on DOE and NSF leadership-class HPC platforms. Specifically, we investigate
RP's weak/strong scaling with CPU/GPU, single/multi core, (non)MPI tasks and
Python functions when using most of ORNL Summit and TACC Frontera.
RADICAL-Pilot can be used stand-alone, as well as the runtime for third-party
workflow systems.

    

### [[2111.01306] On the Current and Emerging Challenges of Developing Fair and Ethical AI Solutions in Financial Services](http://arxiv.org/abs/2111.01306)


  Artificial intelligence (AI) continues to find more numerous and more
critical applications in the financial services industry, giving rise to fair
and ethical AI as an industry-wide objective. While many ethical principles and
guidelines have been published in recent years, they fall short of addressing
the serious challenges that model developers face when building ethical AI
solutions. We survey the practical and overarching issues surrounding model
development, from design and implementation complexities, to the shortage of
tools, and the lack of organizational constructs. We show how practical
considerations reveal the gaps between high-level principles and concrete,
deployed AI applications, with the aim of starting industry-wide conversations
toward solution approaches.

    

### [[2111.01338] Federated Split Vision Transformer for COVID-19CXR Diagnosis using Task-Agnostic Training](http://arxiv.org/abs/2111.01338)


  Federated learning, which shares the weights of the neural network across
clients, is gaining attention in the healthcare sector as it enables training
on a large corpus of decentralized data while maintaining data privacy. For
example, this enables neural network training for COVID-19 diagnosis on chest
X-ray (CXR) images without collecting patient CXR data across multiple
hospitals. Unfortunately, the exchange of the weights quickly consumes the
network bandwidth if highly expressive network architecture is employed.
So-called split learning partially solves this problem by dividing a neural
network into a client and a server part, so that the client part of the network
takes up less extensive computation resources and bandwidth. However, it is not
clear how to find the optimal split without sacrificing the overall network
performance. To amalgamate these methods and thereby maximize their distinct
strengths, here we show that the Vision Transformer, a recently developed deep
learning architecture with straightforward decomposable configuration, is
ideally suitable for split learning without sacrificing performance. Even under
the non-independent and identically distributed data distribution which
emulates a real collaboration between hospitals using CXR datasets from
multiple sources, the proposed framework was able to attain performance
comparable to data-centralized training. In addition, the proposed framework
along with heterogeneous multi-task clients also improves individual task
performances including the diagnosis of COVID-19, eliminating the need for
sharing large weights with innumerable parameters. Our results affirm the
suitability of Transformer for collaborative learning in medical imaging and
pave the way forward for future real-world implementations.

    

### [[2111.01364] Learning to Explore by Reinforcement over High-Level Options](http://arxiv.org/abs/2111.01364)


  Autonomous 3D environment exploration is a fundamental task for various
applications such as navigation. The goal of exploration is to investigate a
new environment and build its occupancy map efficiently. In this paper, we
propose a new method which grants an agent two intertwined options of
behaviors: "look-around" and "frontier navigation". This is implemented by an
option-critic architecture and trained by reinforcement learning algorithms. In
each timestep, an agent produces an option and a corresponding action according
to the policy. We also take advantage of macro-actions by incorporating classic
path-planning techniques to increase training efficiency. We demonstrate the
effectiveness of the proposed method on two publicly available 3D environment
datasets and the results show our method achieves higher coverage than
competing techniques with better efficiency.

    

### [[2111.01366] Improved Loss Function-Based Prediction Method of Extreme Temperatures in Greenhouses](http://arxiv.org/abs/2111.01366)


  The prediction of extreme greenhouse temperatures to which crops are
susceptible is essential in the field of greenhouse planting. It can help avoid
heat or freezing damage and economic losses. Therefore, it's important to
develop models that can predict them accurately. Due to the lack of extreme
temperature data in datasets, it is challenging for models to accurately
predict it. In this paper, we propose an improved loss function, which is
suitable for a variety of machine learning models. By increasing the weight of
extreme temperature samples and reducing the possibility of misjudging extreme
temperature as normal, the proposed loss function can enhance the prediction
results in extreme situations. To verify the effectiveness of the proposed
method, we implement the improved loss function in LightGBM, long short-term
memory, and artificial neural network and conduct experiments on a real-world
greenhouse dataset. The results show that the performance of models with the
improved loss function is enhanced compared to the original models in extreme
cases. The improved models can be used to guarantee the timely judgment of
extreme temperatures in agricultural greenhouses, thereby preventing
unnecessary losses caused by incorrect predictions.

    

### [[2111.01371] Envelope Imbalance Learning Algorithm based on Multilayer Fuzzy C-means Clustering and Minimum Interlayer discrepancy](http://arxiv.org/abs/2111.01371)


  Imbalanced learning is important and challenging since the problem of the
classification of imbalanced datasets is prevalent in machine learning and data
mining fields. Sampling approaches are proposed to address this issue, and
cluster-based oversampling methods have shown great potential as they aim to
simultaneously tackle between-class and within-class imbalance issues. However,
all existing clustering methods are based on a one-time approach. Due to the
lack of a priori knowledge, improper setting of the number of clusters often
exists, which leads to poor clustering performance. Besides, the existing
methods are likely to generate noisy instances. To solve these problems, this
paper proposes a deep instance envelope network-based imbalanced learning
algorithm with the multilayer fuzzy c-means (MlFCM) and a minimum interlayer
discrepancy mechanism based on the maximum mean discrepancy (MIDMD). This
algorithm can guarantee high quality balanced instances using a deep instance
envelope network in the absence of prior knowledge. In the experimental
section, thirty-three popular public datasets are used for verification, and
over ten representative algorithms are used for comparison. The experimental
results show that the proposed approach significantly outperforms other popular
methods.

    

### [[2111.01398] Integrating Pretrained Language Model for Dialogue Policy Learning](http://arxiv.org/abs/2111.01398)


  Reinforcement Learning (RL) has been witnessed its potential for training a
dialogue policy agent towards maximizing the accumulated rewards given from
users. However, the reward can be very sparse for it is usually only provided
at the end of a dialog session, which causes unaffordable interaction
requirements for an acceptable dialog agent. Distinguished from many efforts
dedicated to optimizing the policy and recovering the reward alternatively
which suffers from easily getting stuck in local optima and model collapse, we
decompose the adversarial training into two steps: 1) we integrate a
pre-trained language model as a discriminator to judge whether the current
system action is good enough for the last user action (i.e., \textit{next
action prediction}); 2) the discriminator gives and extra local dense reward to
guide the agent's exploration. The experimental result demonstrates that our
method significantly improves the complete rate (~4.4\%) and success rate
(~8.0\%) of the dialogue system.

    

### [[2111.01414] A Review of Dialogue Systems: From Trained Monkeys to Stochastic Parrots](http://arxiv.org/abs/2111.01414)


  In spoken dialogue systems, we aim to deploy artificial intelligence to build
automated dialogue agents that can converse with humans. Dialogue systems are
increasingly being designed to move beyond just imitating conversation and also
improve from such interactions over time. In this survey, we present a broad
overview of methods developed to build dialogue systems over the years.
Different use cases for dialogue systems ranging from task-based systems to
open domain chatbots motivate and necessitate specific systems. Starting from
simple rule-based systems, research has progressed towards increasingly complex
architectures trained on a massive corpus of datasets, like deep learning
systems. Motivated with the intuition of resembling human dialogues, progress
has been made towards incorporating emotions into the natural language
generator, using reinforcement learning. While we see a trend of highly
marginal improvement on some metrics, we find that limited justification exists
for the metrics, and evaluation practices are not uniform. To conclude, we flag
these concerns and highlight possible research directions.

    

### [[2111.01415] iCallee: Recovering Call Graphs for Binaries](http://arxiv.org/abs/2111.01415)


  Recovering programs' call graphs is crucial for inter-procedural analysis
tasks and applications based on them. The core challenge is recognizing targets
of indirect calls (i.e., indirect callees). It becomes more challenging if
target programs are in binary forms, due to information loss in binaries.
Existing indirect callee recognition solutions for binaries all have high false
positives and negatives, making call graphs inaccurate.
In this paper, we propose a new solution iCallee based on the Siamese Neural
Network, inspired by the advances in question-answering applications. The key
insight is that, neural networks can learn to answer whether a callee function
is a potential target of an indirect callsite by comprehending their contexts,
i.e., instructions nearby callsites and of callees. Following this insight, we
first preprocess target binaries to extract contexts of callsites and callees.
Then, we build a customized Natural Language Processing (NLP) model applicable
to assembly language. Further, we collect abundant pairs of callsites and
callees, and embed their contexts with the NLP model, then train a Siamese
network and a classifier to answer the callsite-callee question. We have
implemented a prototype of iCallee and evaluated it on several groups of
targets. Evaluation results showed that, our solution could match callsites to
callees with an F1-Measure of 93.7%, recall of 93.8%, and precision of 93.5%,
much better than state-of-the-art solutions. To show its usefulness, we apply
iCallee to two specific applications - binary code similarity detection and
binary program hardening, and found that it could greatly improve
state-of-the-art solutions.

    

### [[2111.01431] Graph Tree Deductive Networks](http://arxiv.org/abs/2111.01431)


  In this paper, we introduce Graph Tree Deductive Networks, a network that
performs deductive reasoning. To have high-dimensional thinking, combining
various axioms and putting the results back into another axiom is necessary to
produce new relationships and results. For example, it would be given two
propositions: "Socrates is a man." and "All men are mortals." and two
propositions could be used to infer the new proposition, "Therefore Socrates is
mortal.". To evaluate, we used MNIST Dataset, a handwritten numerical image
dataset, to apply it to the group theory and show the results of performing
deductive learning.

    

### [[2111.01484] ArchABM: an agent-based simulator of human interaction with the built environment. $CO_2$ and viral load analysis for indoor air quality](http://arxiv.org/abs/2111.01484)


  Recent evidence suggests that SARS-CoV-2, which is the virus causing a global
pandemic in 2020, is predominantly transmitted via airborne aerosols in indoor
environments. This calls for novel strategies when assessing and controlling a
building's indoor air quality (IAQ). IAQ can generally be controlled by
ventilation and/or policies to regulate human-building-interaction. However, in
a building, occupants use rooms in different ways, and it may not be obvious
which measure or combination of measures leads to a cost- and energy-effective
solution ensuring good IAQ across the entire building. Therefore, in this
article, we introduce a novel agent-based simulator, ArchABM, designed to
assist in creating new or adapt existing buildings by estimating adequate room
sizes, ventilation parameters and testing the effect of policies while taking
into account IAQ as a result of complex human-building interaction patterns. A
recently published aerosol model was adapted to calculate time-dependent carbon
dioxide ($CO_2$) and virus quanta concentrations in each room and inhaled
$CO_2$ and virus quanta for each occupant over a day as a measure of
physiological response. ArchABM is flexible regarding the aerosol model and the
building layout due to its modular architecture, which allows implementing
further models, any number and size of rooms, agents, and actions reflecting
human-building interaction patterns. We present a use case based on a real
floor plan and working schedules adopted in our research center. This study
demonstrates how advanced simulation tools can contribute to improving IAQ
across a building, thereby ensuring a healthy indoor environment.

    

### [[2111.01510] A Hybrid Approach for Learning to Shift and Grasp with Elaborate Motion Primitives](http://arxiv.org/abs/2111.01510)


  Many possible fields of application of robots in real world settings hinge on
the ability of robots to grasp objects. As a result, robot grasping has been an
active field of research for many years. With our publication we contribute to
the endeavor of enabling robots to grasp, with a particular focus on bin
picking applications. Bin picking is especially challenging due to the often
cluttered and unstructured arrangement of objects and the often limited
graspability of objects by simple top down grasps. To tackle these challenges,
we propose a fully self-supervised reinforcement learning approach based on a
hybrid discrete-continuous adaptation of soft actor-critic (SAC). We employ
parametrized motion primitives for pushing and grasping movements in order to
enable a flexibly adaptable behavior to the difficult setups we consider.
Furthermore, we use data augmentation to increase sample efficiency. We
demonnstrate our proposed method on challenging picking scenarios in which
planar grasp learning or action discretization methods would face a lot of
difficulties

    

### [[2111.01526] A modified gravity model based on network efficiency for vital nodes identification in complex networks](http://arxiv.org/abs/2111.01526)


  Vital nodes identification is an essential problem in network science.
Various methods have been proposed to solve this problem. In particular, based
on the gravity model, a series of improved gravity models are proposed to find
vital nodes better in complex networks. However, they still have the room to be
improved. In this paper, a novel and improved gravity model, which is named
network efficiency gravity centrality model (NEG), integrates gravity model and
network efficiency is proposed. Compared to other methods based on different
gravity models, the proposed method considers the effect of the nodes on
structure robustness of the network better. To solidate the superiority of the
proposed method, experiments on varieties of real-world networks are carried
out.

    

### [[2111.01543] UQuAD1.0: Development of an Urdu Question Answering Training Data for Machine Reading Comprehension](http://arxiv.org/abs/2111.01543)


  In recent years, low-resource Machine Reading Comprehension (MRC) has made
significant progress, with models getting remarkable performance on various
language datasets. However, none of these models have been customized for the
Urdu language. This work explores the semi-automated creation of the Urdu
Question Answering Dataset (UQuAD1.0) by combining machine-translated SQuAD
with human-generated samples derived from Wikipedia articles and Urdu RC
worksheets from Cambridge O-level books. UQuAD1.0 is a large-scale Urdu dataset
intended for extractive machine reading comprehension tasks consisting of 49k
question Answers pairs in question, passage, and answer format. In UQuAD1.0,
45000 pairs of QA were generated by machine translation of the original
SQuAD1.0 and approximately 4000 pairs via crowdsourcing. In this study, we used
two types of MRC models: rule-based baseline and advanced Transformer-based
models. However, we have discovered that the latter outperforms the others;
thus, we have decided to concentrate solely on Transformer-based architectures.
Using XLMRoBERTa and multi-lingual BERT, we acquire an F1 score of 0.66 and
0.63, respectively.

    

### [[2111.01566] Strategyproof and Proportionally Fair Facility Location](http://arxiv.org/abs/2111.01566)


  We focus on a simple, one-dimensional collective decision problem (often
referred to as the facility location problem) and explore issues of
strategyproofness and proportional fairness. We present several
characterization results for mechanisms that satisfy strategyproofness and
varying levels of proportional fairness. We also characterize one of the
mechanisms as the unique equilibrium outcome for any mechanism that satisfies
natural fairness and monotonicity properties. Finally, we identify
strategyproof and proportionally fair mechanisms that provide the best
welfare-optimal approximation among all mechanisms that satisfy the
corresponding fairness axiom.

    

### [[2111.01625] Learning Robotic Ultrasound Scanning Skills via Human Demonstrations and Guided Explorations](http://arxiv.org/abs/2111.01625)


  Medical ultrasound has become a routine examination approach nowadays and is
widely adopted for different medical applications, so it is desired to have a
robotic ultrasound system to perform the ultrasound scanning autonomously.
However, the ultrasound scanning skill is considerably complex, which highly
depends on the experience of the ultrasound physician. In this paper, we
propose a learning-based approach to learn the robotic ultrasound scanning
skills from human demonstrations. First, the robotic ultrasound scanning skill
is encapsulated into a high-dimensional multi-modal model, which takes the
ultrasound images, the pose/position of the probe and the contact force into
account. Second, we leverage the power of imitation learning to train the
multi-modal model with the training data collected from the demonstrations of
experienced ultrasound physicians. Finally, a post-optimization procedure with
guided explorations is proposed to further improve the performance of the
learned model. Robotic experiments are conducted to validate the advantages of
our proposed framework and the learned models.

    

### [[2111.01633] Neural Program Generation Modulo Static Analysis](http://arxiv.org/abs/2111.01633)


  State-of-the-art neural models of source code tend to be evaluated on the
generation of individual expressions and lines of code, and commonly fail on
long-horizon tasks such as the generation of entire method bodies. We propose
to address this deficiency using weak supervision from a static program
analyzer. Our neurosymbolic method allows a deep generative model to
symbolically compute, using calls to a static-analysis tool, long-distance
semantic relationships in the code that it has already generated. During
training, the model observes these relationships and learns to generate
programs conditioned on them. We apply our approach to the problem of
generating entire Java methods given the remainder of the class that contains
the method. Our experiments show that the approach substantially outperforms
state-of-the-art transformers and a model that explicitly tries to learn
program semantics on this task, both in terms of producing programs free of
basic semantic errors and in terms of syntactically matching the ground truth.

    

### [[2111.01654] Modeling and Automating Public Announcement Logic with Rela\-tivized Common Knowledge as a Fragment of HOL in LogiKEy](http://arxiv.org/abs/2111.01654)


  A shallow semantical embedding for public announcement logic with relativized
common knowledge is presented. This embedding enables the first-time automation
of this logic with off-the-shelf theorem provers for classical higher-order
logic. It is demonstrated (i) how meta-theoretical studies can be automated
this way, and (ii) how non-trivial reasoning in the target logic (public
announcement logic), required e.g. to obtain a convincing encoding and
automation of the wise men puzzle, can be realized.
Key to the presented semantical embedding is that evaluation domains are
modeled explicitly and treated as an additional parameter in the encodings of
the constituents of the embedded target logic; in previous related works, e.g.
on the embedding of normal modal logics, evaluation domains were implicitly
shared between meta-logic and target logic.
The work presented in this article constitutes an important addition to the
pluralist \logikey\ knowledge engineering methodology, which enables
experimentation with logics and their combinations, with general and domain
knowledge, and with concrete use cases -- all at the same time.

    

### [[2111.01663] Classification of Goods Using Text Descriptions With Sentences Retrieval](http://arxiv.org/abs/2111.01663)


  The task of assigning and validating internationally accepted commodity code
(HS code) to traded goods is one of the critical functions at the customs
office. This decision is crucial to importers and exporters, as it determines
the tariff rate. However, similar to court decisions made by judges, the task
can be non-trivial even for experienced customs officers. The current paper
proposes a deep learning model to assist this seemingly challenging HS code
classification. Together with Korea Customs Service, we built a decision model
based on KoELECTRA that suggests the most likely heading and subheadings (i.e.,
the first four and six digits) of the HS code. Evaluation on 129,084 past cases
shows that the top-3 suggestions made by our model have an accuracy of 95.5% in
classifying 265 subheadings. This promising result implies algorithms may
reduce the time and effort taken by customs officers substantially by assisting
the HS code classification task.

    

### [[2111.01683] Using Synthetic Images To Uncover Population Biases In Facial Landmarks Detection](http://arxiv.org/abs/2111.01683)


  In order to analyze a trained model performance and identify its weak spots,
one has to set aside a portion of the data for testing. The test set has to be
large enough to detect statistically significant biases with respect to all the
relevant sub-groups in the target population. This requirement may be difficult
to satisfy, especially in data-hungry applications. We propose to overcome this
difficulty by generating synthetic test set. We use the face landmarks
detection task to validate our proposal by showing that all the biases observed
on real datasets are also seen on a carefully designed synthetic dataset. This
shows that synthetic test sets can efficiently detect a model's weak spots and
overcome limitations of real test set in terms of quantity and/or diversity.

    

### [[2111.01689] Improving Classifier Training Efficiency for Automatic Cyberbullying Detection with Feature Density](http://arxiv.org/abs/2111.01689)


  We study the effectiveness of Feature Density (FD) using different
linguistically-backed feature preprocessing methods in order to estimate
dataset complexity, which in turn is used to comparatively estimate the
potential performance of machine learning (ML) classifiers prior to any
training. We hypothesise that estimating dataset complexity allows for the
reduction of the number of required experiments iterations. This way we can
optimize the resource-intensive training of ML models which is becoming a
serious issue due to the increases in available dataset sizes and the ever
rising popularity of models based on Deep Neural Networks (DNN). The problem of
constantly increasing needs for more powerful computational resources is also
affecting the environment due to alarmingly-growing amount of CO2 emissions
caused by training of large-scale ML models. The research was conducted on
multiple datasets, including popular datasets, such as Yelp business review
dataset used for training typical sentiment analysis models, as well as more
recent datasets trying to tackle the problem of cyberbullying, which, being a
serious social problem, is also a much more sophisticated problem form the
point of view of linguistic representation. We use cyberbullying datasets
collected for multiple languages, namely English, Japanese and Polish. The
difference in linguistic complexity of datasets allows us to additionally
discuss the efficacy of linguistically-backed word preprocessing.

    

### [[2111.01690] Recent Advances in End-to-End Automatic Speech Recognition](http://arxiv.org/abs/2111.01690)


  Recently, the speech community is seeing a significant trend of moving from
deep neural network based hybrid modeling to end-to-end (E2E) modeling for
automatic speech recognition (ASR). While E2E models achieve the
state-of-the-art results in most benchmarks in terms of ASR accuracy, hybrid
models are still used in a large proportion of commercial ASR systems at the
current time. There are lots of practical factors that affect the production
model deployment decision. Traditional hybrid models, being optimized for
production for decades, are usually good at these factors. Without providing
excellent solutions to all these factors, it is hard for E2E models to be
widely commercialized. In this paper, we will overview the recent advances in
E2E models, focusing on technologies addressing those challenges from the
industry's perspective.

    

### [[2111.01706] Assessing Effectiveness of Using Internal Signals for Check-Worthy Claim Identification in Unlabeled Data for Automated Fact-Checking](http://arxiv.org/abs/2111.01706)


  While recent work on automated fact-checking has focused mainly on verifying
and explaining claims, for which the list of claims is readily available,
identifying check-worthy claim sentences from a text remains challenging.
Current claim identification models rely on manual annotations for each
sentence in the text, which is an expensive task and challenging to conduct on
a frequent basis across multiple domains. This paper explores methodology to
identify check-worthy claim sentences from fake news articles, irrespective of
domain, without explicit sentence-level annotations. We leverage two internal
supervisory signals - headline and the abstractive summary - to rank the
sentences based on semantic similarity. We hypothesize that this ranking
directly correlates to the check-worthiness of the sentences. To assess the
effectiveness of this hypothesis, we build pipelines that leverage the ranking
of sentences based on either the headline or the abstractive summary. The
top-ranked sentences are used for the downstream fact-checking tasks of
evidence retrieval and the article's veracity prediction by the pipeline. Our
findings suggest that the top 3 ranked sentences contain enough information for
evidence-based fact-checking of a fake news article. We also show that while
the headline has more gisting similarity with how a fact-checking website
writes a claim, the summary-based pipeline is the most promising for an
end-to-end fact-checking system.

    

### [[2111.01726] Instructive artificial intelligence (AI) for human training, assistance, and explainability](http://arxiv.org/abs/2111.01726)


  We propose a novel approach to explainable AI (XAI) based on the concept of
"instruction" from neural networks. In this case study, we demonstrate how a
superhuman neural network might instruct human trainees as an alternative to
traditional approaches to XAI. Specifically, an AI examines human actions and
calculates variations on the human strategy that lead to better performance.
Experiments with a JHU/APL-developed AI player for the cooperative card game
Hanabi suggest this technique makes unique contributions to explainability
while improving human performance. One area of focus for Instructive AI is in
the significant discrepancies that can arise between a human's actual strategy
and the strategy they profess to use. This inaccurate self-assessment presents
a barrier for XAI, since explanations of an AI's strategy may not be properly
understood or implemented by human recipients. We have developed and are
testing a novel, Instructive AI approach that estimates human strategy by
observing human actions. With neural networks, this allows a direct calculation
of the changes in weights needed to improve the human strategy to better
emulate a more successful AI. Subjected to constraints (e.g. sparsity) these
weight changes can be interpreted as recommended changes to human strategy
(e.g. "value A more, and value B less"). Instruction from AI such as this
functions both to help humans perform better at tasks, but also to better
understand, anticipate, and correct the actions of an AI. Results will be
presented on AI instruction's ability to improve human decision-making and
human-AI teaming in Hanabi.

    

### [[2102.11137] Program Synthesis Guided Reinforcement Learning for Partially Observed Environments](http://arxiv.org/abs/2102.11137)


  A key challenge for reinforcement learning is solving long-horizon planning
problems. Recent work has leveraged programs to guide reinforcement learning in
these settings. However, these approaches impose a high manual burden on the
user since they must provide a guiding program for every new task. Partially
observed environments further complicate the programming task because the
program must implement a strategy that correctly, and ideally optimally,
handles every possible configuration of the hidden regions of the environment.
We propose a new approach, model predictive program synthesis (MPPS), that uses
program synthesis to automatically generate the guiding programs. It trains a
generative model to predict the unobserved portions of the world, and then
synthesizes a program based on samples from this model in a way that is robust
to its uncertainty. In our experiments, we show that our approach significantly
outperforms non-program-guided approaches on a set of challenging benchmarks,
including a 2D Minecraft-inspired environment where the agent must complete a
complex sequence of subtasks to achieve its goal, and achieves a similar
performance as using handcrafted programs to guide the agent. Our results
demonstrate that our approach can obtain the benefits of program-guided
reinforcement learning without requiring the user to provide a new guiding
program for every new task.

    

### [[2103.03991] Passing Through Narrow Gaps with Deep Reinforcement Learning](http://arxiv.org/abs/2103.03991)


  The U.S. Defense Advanced Research Projects Agency (DARPA) Subterranean
Challenge requires teams of robots to traverse difficult and diverse
underground environments. Traversing small gaps is one of the challenging
scenarios that robots encounter. Imperfect sensor information makes it
difficult for classical navigation methods, where behaviours require
significant manual fine tuning. In this paper we present a deep reinforcement
learning method for autonomously navigating through small gaps, where contact
between the robot and the gap may be required. We first learn a gap behaviour
policy to get through small gaps (only centimeters wider than the robot). We
then learn a goal-conditioned behaviour selection policy that determines when
to activate the gap behaviour policy. We train our policies in simulation and
demonstrate their effectiveness with a large tracked robot in simulation and on
the real platform. In simulation experiments, our approach achieves 93\%
success rate when the gap behaviour is activated manually by an operator, and
63\% with autonomous activation using the behaviour selection policy. In real
robot experiments, our approach achieves a success rate of 73\% with manual
activation, and 40\% with autonomous behaviour selection. While we show the
feasibility of our approach in simulation, the difference in performance
between simulated and real world scenarios highlight the difficulty of direct
sim-to-real transfer for deep reinforcement learning policies. In both the
simulated and real world environments alternative methods were unable to
traverse the gap.

    

### [[2104.07149] On the Robustness of Intent Classification and Slot Labeling in Goal-oriented Dialog Systems to Real-world Noise](http://arxiv.org/abs/2104.07149)


  Intent Classification (IC) and Slot Labeling (SL) models, which form the
basis of dialogue systems, often encounter noisy data in real-word
environments. In this work, we investigate how robust IC/SL models are to noisy
data. We collect and publicly release a test-suite for seven common noise types
found in production human-to-bot conversations (abbreviations, casing,
misspellings, morphological variants, paraphrases, punctuation and synonyms).
On this test-suite, we show that common noise types substantially degrade the
IC accuracy and SL F1 performance of state-of-the-art BERT-based IC/SL models.
By leveraging cross-noise robustness transfer -- training on one noise type to
improve robustness on another noise type -- we design aggregate
data-augmentation approaches that increase the model performance across all
seven noise types by +10.8% for IC accuracy and +15 points for SL F1 on
average. To the best of our knowledge, this is the first work to present a
single IC/SL model that is robust to a wide range of noise phenomena.

    

### [[2105.06742] Cybersecurity Anomaly Detection in Adversarial Environments](http://arxiv.org/abs/2105.06742)


  The proliferation of interconnected battlefield information-sharing devices,
known as the Internet of Battlefield Things (IoBT), introduced several security
challenges. Inherent to the IoBT operating environment is the practice of
adversarial machine learning, which attempts to circumvent machine learning
models. This work examines the feasibility of cost-effective unsupervised
learning and graph-based methods for anomaly detection in the network intrusion
detection system setting, and also leverages an ensemble approach to supervised
learning of the anomaly detection problem. We incorporate a realistic
adversarial training mechanism when training supervised models to enable strong
classification performance in adversarial environments. The results indicate
that the unsupervised and graph-based methods were outperformed in detecting
anomalies (malicious activity) by the supervised stacking ensemble method with
two levels. This model consists of three different classifiers in the first
level, followed by either a Naive Bayes or Decision Tree classifier for the
second level. The model maintains an F1-score above 0.97 for malicious samples
across all tested level two classifiers. Notably, Naive Bayes is the fastest
level two classifier averaging 1.12 seconds while Decision Tree maintains the
highest AUC score of 0.98.

    

### [[2111.00801] Livestock Monitoring with Transformer](http://arxiv.org/abs/2111.00801)


  Tracking the behaviour of livestock enables early detection and thus
prevention of contagious diseases in modern animal farms. Apart from economic
gains, this would reduce the amount of antibiotics used in livestock farming
which otherwise enters the human diet exasperating the epidemic of antibiotic
resistance - a leading cause of death. We could use standard video cameras,
available in most modern farms, to monitor livestock. However, most computer
vision algorithms perform poorly on this task, primarily because, (i) animals
bred in farms look identical, lacking any obvious spatial signature, (ii) none
of the existing trackers are robust for long duration, and (iii) real-world
conditions such as changing illumination, frequent occlusion, varying camera
angles, and sizes of the animals make it hard for models to generalize. Given
these challenges, we develop an end-to-end behaviour monitoring system for
group-housed pigs to perform simultaneous instance level segmentation,
tracking, action recognition and re-identification (STAR) tasks. We present
starformer, the first end-to-end multiple-object livestock monitoring framework
that learns instance-level embeddings for grouped pigs through the use of
transformer architecture. For benchmarking, we present Pigtrace, a carefully
curated dataset comprising video sequences with instance level bounding box,
segmentation, tracking and activity classification of pigs in real indoor
farming environment. Using simultaneous optimization on STAR tasks we show that
starformer outperforms popular baseline models trained for individual tasks.

    

### [[2111.00826] Teaching Fairness, Accountability, Confidentiality, and Transparency in Artificial Intelligence through the Lens of Reproducibility](http://arxiv.org/abs/2111.00826)


  In this work we explain the setup for a technical, graduate-level course on
Fairness, Accountability, Confidentiality and Transparency in Artificial
Intelligence (FACT-AI) at the University of Amsterdam, which teaches FACT-AI
concepts through the lens of reproducibility. The focal point of the course is
a group project based on reproducing existing FACT-AI algorithms from top AI
conferences, and writing a report about their experiences. In the first
iteration of the course, we created an open source repository with the code
implementations from the group projects. In the second iteration, we encouraged
students to submit their group projects to the Machine Learning Reproducibility
Challenge, which resulted in 9 reports from our course being accepted to the
challenge. We reflect on our experience teaching the course over two academic
years, where one year coincided with a global pandemic, and propose guidelines
for teaching FACT-AI through reproducibility in graduate-level AI programs. We
hope this can be a useful resource for instructors to set up similar courses at
their universities in the future.

    

### [[2111.00962] RefineGAN: Universally Generating Waveform Better than Ground Truth with Highly Accurate Pitch and Intensity Responses](http://arxiv.org/abs/2111.00962)


  Most GAN(Generative Adversarial Network)-based approaches towards
high-fidelity waveform generation heavily rely on discriminators to improve
their performance. However, the over-use of this GAN method introduces much
uncertainty into the generation process and often result in mismatches of pitch
and intensity, which is fatal when it comes to sensitive using cases such as
singing voice synthesis(SVS). To address this problem, we propose RefineGAN, a
high-fidelity neural vocoder with faster-than-real-time generation capability,
and focused on the robustness, pitch and intensity accuracy, and full-band
audio generation. We employed a pitch-guided refine architecture with a
multi-scale spectrogram-based loss function to help stabilize the training
process and maintain the robustness of the neural vocoder while using the
GAN-based training method. Audio generated using this method shows a better
performance in subjective tests when compared with the ground-truth audio. This
result shows that the fidelity is even improved during the waveform
reconstruction by eliminating defects produced by the speaker and the recording
procedure. Moreover, a further study shows that models trained on a specified
type of data can perform on totally unseen language and unseen speaker
identically well. Generated sample pairs are provided on
this https URL.

    

### [[2111.01413] A Minmax Utilization Algorithm for Network Traffic Scheduling of Industrial Robots](http://arxiv.org/abs/2111.01413)


  Emerging 5G and beyond wireless industrial virtualized networks are expected
to support a significant number of robotic manipulators. Depending on the
processes involved, these industrial robots might result in significant volume
of multi-modal traffic that will need to traverse the network all the way to
the (public/private) edge cloud, where advanced processing, control and service
orchestration will be taking place. In this paper, we perform the traffic
engineering by capitalizing on the underlying pseudo-deterministic nature of
the repetitive processes of robotic manipulators in an industrial environment
and propose an integer linear programming (ILP) model to minimize the maximum
aggregate traffic in the network. The task sequence and time gap requirements
are also considered in the proposed model. To tackle the curse of
dimensionality in ILP, we provide a random search algorithm with quadratic time
complexity. Numerical investigations reveal that the proposed scheme can reduce
the peak data rate up to 53.4% compared with the nominal case where robotic
manipulators operate in an uncoordinated fashion, resulting in significant
improvement in the utilization of the underlying network resources.

    

### [[2111.01594] Mean Field and Refined Mean Field Approximations for Heterogeneous Systems: It Works!](http://arxiv.org/abs/2111.01594)


  Mean field approximation is a powerful technique to study the performance of
large stochastic systems represented as $n$ interacting objects. Applications
include load balancing models, epidemic spreading, cache replacement policies,
or large-scale data centers. Mean field approximation is asymptotically exact
for systems composed of $n$ homogeneous objects under mild conditions. In this
paper, we study what happens when objects are heterogeneous. This can represent
servers with different speeds or contents with different popularities. We
define an interaction model that allows obtaining asymptotic convergence
results for stochastic systems with heterogeneous object behavior, and show
that the error of the mean field approximation is of order $O(1/n)$. More
importantly, we show how to adapt the refined mean field approximation,
developed by Gast et al. 2019, and show that the error of this approximation is
reduced to $O(1/n^2)$. To illustrate the applicability of our result, we
present two examples. The first addresses a list-based cache replacement model
RANDOM($m$), which is an extension of the RANDOM policy. The second is a
heterogeneous supermarket model. These examples show that the proposed
approximations are computationally tractable and very accurate. They also show
that for moderate system sizes ($n\approx30$) the refined mean field
approximation tends to be more accurate than simulations for any reasonable
simulation time.

    

### [[2111.01421] The Security Risk of Lacking Compiler Protection in WebAssembly](http://arxiv.org/abs/2111.01421)


  WebAssembly is increasingly used as the compilation target for cross-platform
applications. In this paper, we investigate whether one can rely on the
security measures enforced by existing C compilers when compiling C programs to
WebAssembly. We compiled 4,469 C programs with known buffer overflow
vulnerabilities to x86 code and to WebAssembly, and observed the outcome of the
execution of the generated code to differ for 1,088 programs. Through manual
inspection, we identified that the root cause for these is the lack of security
measures such as stack canaries in the generated WebAssembly: while x86 code
crashes upon a stack-based buffer overflow, the corresponding WebAssembly
continues to be executed. We conclude that compiling an existing C program to
WebAssembly without additional precautions may hamper its security, and we
encourage more research in this direction.

    

### [[2111.01577] Do Names Echo Semantics? A Large-Scale Study of Identifiers Used in C++'s Named Casts](http://arxiv.org/abs/2111.01577)


  Developers relax restrictions on a type to reuse methods with other types.
While type casts are prevalent, in weakly typed languages such as C++, they are
also extremely permissive. If type conversions are performed without care, they
can lead to software bugs. Therefore, there is a clear need to check whether a
type conversion is essential and used adequately according to the developer's
intent. In this paper, we propose a technique to judge the fidelity of type
conversions from an explicit cast operation, using the identifiers in an
assignment. We measure accord in the identifiers using entropy and use it to
check if the semantics of the source expression in the cast match the semantics
of the variable it is being assigned. We present the results of running our
tool on 34 components of the Chromium project, which collectively account for
27MLOC. Our tool identified 1,368 cases of discord indicating potential
anti-patterns in the usage of explicit casts. We performed a manual evaluation
of a random-uniform sample of these cases. Our evaluation shows that our tool
identified 25.6% cases representing incorrect implementations of named casts
and 28.04% cases representing imprecise names of identifiers.

    

### [[2104.01065] Fairness and Communication-Based Semantics for Session-Typed Languages](http://arxiv.org/abs/2104.01065)


  We give communication-based semantics and reasoning techniques for Polarized
SILL, a rich session-typed programming language with general recursion. Its
features include channel and code transmission, synchronous and asynchronous
communication, and functional programming. Our contributions are distinguished
by their faithfulness to the process abstraction, i.e., to the premise that
communication is the only observable phenomenon of processes. We give the first
observed communication semantics that supports general recursion and code
transmission. Observed communication semantics define the meaning of processes
in terms of their observed communications. We use this observational semantics
to define experiments on processes, and we give a communication-based testing
equivalences framework for defining observational simulations and equivalences
on processes. This framework captures several natural equivalences, and we show
that one of these coincides with barbed congruence, the canonical notion of
process equivalence.
Polarized SILL is defined using a substructural operational semantics based
on multiset rewriting. To ensure that our contributions are well-defined in the
presence of non-termination, we introduce fairness for multiset rewriting
systems. We construct a fair scheduler, we give sufficient conditions for
traces to be fair, and we study the effects of permutation on fair traces.

    