
## 2021-10-1

### [[2109.13922] New Hybrid Techniques for Business Recommender Systems](http://arxiv.org/abs/2109.13922)


  Besides the typical applications of recommender systems in B2C scenarios such
as movie or shopping platforms, there is a rising interest in transforming the
human-driven advice provided e.g. in consultancy via the use of recommender
systems. We explore the special characteristics of such knowledge-based B2B
services and propose a process that allows to incorporate recommender systems
into them. We suggest and compare several recommender techniques that allow to
incorporate the necessary contextual knowledge (e.g. company demographics).
These techniques are evaluated in isolation on a test set of business
intelligence consultancy cases. We then identify the respective strengths of
the different techniques and propose a new hybridisation strategy to combine
these strengths. Our results show that the hybridisation leads to a substantial
performance improvement over the individual methods.

    

### [[2109.14696] Time-Distributed Feature Learning in Network Traffic Classification for Internet of Things](http://arxiv.org/abs/2109.14696)


  The plethora of Internet of Things (IoT) devices leads to explosive network
traffic. The network traffic classification (NTC) is an essential tool to
explore behaviours of network flows, and NTC is required for Internet service
providers (ISPs) to manage the performance of the IoT network. We propose a
novel network data representation, treating the traffic data as a series of
images. Thus, the network data is realized as a video stream to employ
time-distributed (TD) feature learning. The intra-temporal information within
the network statistical data is learned using convolutional neural networks
(CNN) and long short-term memory (LSTM), and the inter pseudo-temporal feature
among the flows is learned by TD multi-layer perceptron (MLP). We conduct
experiments using a large data-set with more number of classes. The
experimental result shows that the TD feature learning elevates the network
classification performance by 10%.

    

### [[2109.14738] A Novel Initialization Method for HybridUnderwater Optical Acoustic Networks](http://arxiv.org/abs/2109.14738)


  To satisfy the high data rate requirement andreliable transmission demands in
underwater scenarios, it isdesirable to construct an efficient hybrid
underwater opticalacoustic network (UWOAN) architecture by considering the
keyfeatures and critical needs of underwater terminals. In UWOANs,optical
uplinks and acoustic downlinks are configured betweenunderwater nodes (UWNs)
and the base station (BS), wherethe optical beam transmits the high data rate
traffic to theBS, while the acoustic waves carry the control information
torealize the network management. In this paper, we focus onsolving the network
initializing problem in UWOANs, which isa challenging task due to the lack of
GPS service and limiteddevice payload in underwater environments. To this end,
weleverage acoustic waves for node localization and propose anovel network
initialization method, which consists of UWNidentification, discovery,
localization, as well as decomposition.Numerical simulations are also conducted
to verify the proposedinitialization method.

    

### [[2109.14794] TopoShot: Uncovering Ethereum's Network Topology Leveraging Replacement Transactions](http://arxiv.org/abs/2109.14794)


  Ethereum relies on a peer-to-peer overlay network to propagate information.
The knowledge of Ethereum network topology holds the key to understanding
Ethereum's security, availability, and user anonymity. From a measurement
perspective, an Ethereum network's topology is routing-table information hidden
inside individual Ethereum nodes, measuring which poses challenges and remains
an open research problem in the existing literature.
This paper presents TopoShot, a new method uniquely repurposing Ethereum's
transaction replacement/eviction policies for topology measurement. TopoShot
can be configured to support Geth, Parity, and other major Ethereum clients. As
validated on local nodes, TopoShot achieves 100% measurement precision and high
recall 88% - 97%. To efficiently measure the large Ethereum networks in the
wild, we propose a non-trivial schedule to run pair-wise measurements in
parallel. To enable ethical measurement on Ethereum mainnet, we propose
workload-adaptive configurations of TopoShot to minimize the service
interruption to target nodes/network.
We systematically measure a variety of Ethereum networks and obtain new
knowledge including the full-network topology in major testnets (Ropsten,
Rinkeby and Goerli) and critical sub-network topology in the mainnet. The
results on testnets show interesting graph-theoretic properties, such as all
testnets exhibit graph modularity significantly lower than random graphs,
implying resilience to network partitions. The mainnet results show biased
neighbor selection strategies adopted by critical Ethereum services such as
mining pools and transaction relays, implying a degree of centralization in
real Ethereum networks.

    

### [[2109.14868] From Zero-Shot Machine Learning to Zero-Day Attack Detection](http://arxiv.org/abs/2109.14868)


  The standard ML methodology assumes that the test samples are derived from a
set of pre-observed classes used in the training phase. Where the model
extracts and learns useful patterns to detect new data samples belonging to the
same data classes. However, in certain applications such as Network Intrusion
Detection Systems, it is challenging to obtain data samples for all attack
classes that the model will most likely observe in production. ML-based NIDSs
face new attack traffic known as zero-day attacks, that are not used in the
training of the learning models due to their non-existence at the time. In this
paper, a zero-shot learning methodology has been proposed to evaluate the ML
model performance in the detection of zero-day attack scenarios. In the
attribute learning stage, the ML models map the network data features to
distinguish semantic attributes from known attack (seen) classes. In the
inference stage, the models are evaluated in the detection of zero-day attack
(unseen) classes by constructing the relationships between known attacks and
zero-day attacks. A new metric is defined as Zero-day Detection Rate, which
measures the effectiveness of the learning model in the inference stage. The
results demonstrate that while the majority of the attack classes do not
represent significant risks to organisations adopting an ML-based NIDS in a
zero-day attack scenario. However, for certain attack groups identified in this
paper, such systems are not effective in applying the learnt attributes of
attack behaviour to detect them as malicious. Further Analysis was conducted
using the Wasserstein Distance technique to measure how different such attacks
are from other attack types used in the training of the ML model. The results
demonstrate that sophisticated attacks with a low zero-day detection rate have
a significantly distinct feature distribution compared to the other attack
classes.

    

### [[2109.14904] Opportunistic Federation of CubeSat Constellations: a Game-Changing Paradigm Enabling Enhanced IoT Services in the Sky](http://arxiv.org/abs/2109.14904)


  Internet of Space Things (IoST) is a challenging paradigm, which is currently
attracting great interest from the scientific and industrial communities. IoST
is based on the integration of the space segment into the global Internet of
Things (IoT) infrastructure. In the relevant literature, reference is generally
made to multiple constellations of nanosatellite platforms, used to enable IoT
services on a global scale, including also disadvantaged and poorly
infrastructured areas. In this paper, we focus on multi-tenant IoT scenarios,
wherein multiple CubeSats constellations are enabled to offer services by
exploiting a dynamic federation model. The objective is to efficiently provide
services in an IoST scenario by leveraging an effective cooperation strategy
originally designed for terrestrial IoT networks, the
Mobile-IoT-Federation-as-a-Service (MIFaaS) paradigm. We extend this vision to
IoT satellite networks in order to allow a constellation of satellites to
effectively execute tasks through a tight cooperative behaviour with other
CubeSats constellations. The reported performance evaluation studies show that
better performance, in terms of percentage of tasks successfully completed, can
be achieved through the implementation of the proposed cooperation paradigm.

    

### [[2109.14946] Crowdsourcing through Cognitive Opportunistic Networks](http://arxiv.org/abs/2109.14946)


  Untile recently crowdsourcing has been primarily conceived as an online
activity to harness resources for problem solving. However the emergence of
opportunistic networking (ON) has opened up crowdsourcing to the spatial
domain. In this paper we bring the ON model for potential crowdsourcing in the
smart city environment. We introduce cognitive features to the ON that allow
users' mobile devices to become aware of the surrounding physical environment.
Specifically, we exploit cognitive psychology studies on dynamic memory
structures and cognitive heuristics, i.e. mental models that describe how the
human brain handle decision-making amongst complex and real-time stimuli.
Combined with ON, these cognitive features allow devices to act as proxies in
the cyber-world of their users and exchange knowledge to deliver awareness of
places in an urban environment. This is done through tags associated with
locations. They represent features that are perceived by humans about a place.
We consider the extent to which this knowledge becomes available to
participants, using interactions with locations and other nodes. This is
assessed taking into account a wide range of cognitive parameters. Outcomes are
important because this functionality could support a new type of recommendation
system that is independent of the traditional forms of networking.

    

### [[2109.14958] A Social Cognitive Heuristic for Adaptive Data Dissemination in Mobile Opportunistic Networks](http://arxiv.org/abs/2109.14958)


  It is commonly agreed that data will be one of the cornerstones of Future
Internet systems. In this context, mobile Opportunistic Networks (ONs) are one
of the key paradigms to support, in a self-organising and decentralised manner,
the growth of data generated by localized interactions between users mobile
devices, and between them and nearby devices such as IoT nodes. In ONs, the
spontaneous collaboration among mobile devices is exploited to disseminate data
toward interested users. However, the limited resources and knowledge available
at each node, and the vast amount of data available, make it difficult to
devise efficient schemes to accomplish this task. Recent solutions propose to
equip each device with data filtering methods derived from human data
processing schemes, known as Cognitive Heuristics, i.e. very effective methods
used by the brain to quickly drop useless information, while keeping the most
relevant one. These solutions can become less effective when facing dynamic
scenarios or situations where nodes cannot fully collaborate. One of the
reasons is that the solutions proposed so far do not take take into account the
social structure of the environment where the nodes move in. To be more
effective, the selection of information performed by each node should take into
consideration this dimension of the environment. In this paper we propose a
social-based data dissemination scheme, based on the cognitive Social Circle
Heuristic. This evaluation method exploits the structure of the social
environment to make inferences about the relevance of discovered information.
We show how the Social Circle Heuristic, coupled with a cognitive-based
community detection scheme, can be exploited to design an effective data
dissemination algorithm for ONs. We provide a detailed analysis of the
performance of the proposed solution via simulation.

    

### [[2109.15017] Can 5G NR-Light Operate at Millimeter Waves? Design Guidelines for Mid-Market IoT Use Cases](http://arxiv.org/abs/2109.15017)


  5th generation (5G) systems have been designed with three main objectives in
mind: increasing throughput, reducing latency, and enabling reliable
communications. To meet these (often conflicting) constraints, in 2019 the 3GPP
released a set of specifications for 5G NR, one of the main innovations being
the support for communications in the millimeter wave (mmWave) bands. However,
how to implement lower complexity, energy efficient, mid-market Internet of
Things (IoT) applications is still an on-going investigation, currently led by
the 3GPP which is extending the NR standard with NR-Light specifications to
support devices with reduced capabilities (REDCAP). In this paper we
investigate the feasibility of operating such devices at mmWaves, in view of
the requirements and expectations for NR- Light applications in terms of cost
and complexity, throughput, and latency. Contributions of this paper are
threefold. First, we il- lustrate the potential of mmWave communication for
mid-market IoT use cases. Then, we highlight and motivate the design of an
NR-Light candidate interface derived from NR by a selection of features.
Finally, we demonstrate the technical soundness of this interface in an
industrial IoT setup via simulations.

    

### [[2109.15040] Stateful Function-as-a-Service at the Edge](http://arxiv.org/abs/2109.15040)


  In FaaS, users invoke remote functions, which encapsulate service(s). These
functions typically need to remotely access a persistent state via external
services: this makes the paradigm less attractive in edge systems, especially
for IoT applications, due to the increased delay and outbound traffic. We
propose to generalize the FaaS paradigm by allowing functions to alternate
between remote-state and local-state phases, depending on internal and external
conditions, and dedicating a container with persistent memory to functions when
in a local-state phase. We present initial results showing that this simple yet
powerful pattern allows to better utilize the available resources, which are
scarce on edge nodes, while significantly reducing tail latencies, which is key
to enable many new applications based on real-time ML, e.g., in smart vehicles
and smart factory scenarios

    

### [[2109.15088] An Efficient Probe-based Routing for Content-Centric Networking](http://arxiv.org/abs/2109.15088)


  With the development of new technologies and applications, such as Internet
of Things, smart cities, 5G and edge computing, traditional IP-based networks
have exposed many problems. Information-Centric Networking (ICN), Named Data
Networking (NDN) and Content-Centric Networking (CCN) are therefore proposed as
an alternative for future network. However, unlike IP-based networks, the
routing of CCN is not deterministic and hard to be optimized due to the
frequent replacement of in-network caching. This paper presents a novel
probe-based routing algorithm that real-time explores in-network caching to
ensure the routing table storing the optimal paths to the nearest content
provider up to date. Effective probe-selections, PIT probe and FIB probe are
discussed and analyzed by simulation with different performance measurements.
Compared with the basic CCN, in the aspect of qualitative analysis, the
additional computational overhead of our approach are O(NCS + Nrt + NFIB) and
O(NFIB) on processing interest packets and data packets, respectively. However,
in the aspect of quantitative analysis, our approach reduces the number of
timeout interest by 6% and reduces the average response time by 0.6 seconds. In
addition, in the aspect of QoS, although basic CCN and our approach are in the
same QoS category, our approach performs better in terms of real values. In
summary, the results demonstrate that compared to basic CCN, our probe-based
routing approach raises FIB accuracy and reduces network congestion and
response time that achieves efficient routing.

    

### [[2109.15095] Third Time's Not a Charm: Exploiting SNMPv3 for Router Fingerprinting](http://arxiv.org/abs/2109.15095)


  In this paper, we show that adoption of the SNMPv3 network management
protocol standard offers a unique -- but likely unintended -- opportunity for
remotely fingerprinting network infrastructure in the wild. Specifically, by
sending unsolicited and unauthenticated SNMPv3 requests, we obtain detailed
information about the configuration and status of network devices including
vendor, uptime, and the number of restarts. More importantly, the reply
contains a persistent and strong identifier that allows for lightweight
Internet-scale alias resolution and dual-stack association. By launching active
Internet-wide SNMPv3 scan campaigns, we show that our technique can fingerprint
more than 4.6 million devices of which around 350k are network routers. Not
only is our technique lightweight and accurate, it is complementary to existing
alias resolution, dual-stack inference, and device fingerprinting approaches.
Our analysis not only provides fresh insights into the router deployment
strategies of network operators worldwide, but also highlights potential
vulnerabilities of SNMPv3 as currently deployed.

    

### [[2109.15139] High-Availability Clusters A Taxonomy, Review, and Future Directions](http://arxiv.org/abs/2109.15139)


  The delivery of key services in domains ranging from finance and
manufacturing to healthcare and transportation is underpinned by a rapidly
growing number of mission-critical enterprise applications. Ensuring the
continuity of these complex applications requires the use of software-managed
infrastructures called high-availability clusters (HACs). HACs employ
sophisticated techniques to monitor the health of key enterprise application
layers and of the resources they use, and to seamlessly restart or relocate
application components after failures. In this paper, we first describe the
manifold uses of HACs to protect essential layers of a critical application and
present the architecture of high availability clusters. We then propose a
taxonomy that covers all key aspects of HACs -- deployment patterns,
application areas, types of cluster, topology, cluster management, failure
detection and recovery, consistency and integrity, and data synchronisation;
and we use this taxonomy to provide a comprehensive survey of the end-to-end
software solutions available for the HAC deployment of enterprise applications.
Finally, we discuss the limitations and challenges of existing HAC solutions,
and we identify opportunities for future research in the area.

    

### [[2109.15143] MCS Adaptation within the Cellular V2X Sidelink](http://arxiv.org/abs/2109.15143)


  Adaptation of the Modulation and Coding Scheme (MCS) within the Cellular
Vehicle-To-Everything (C-V2X) sidelink has the potential for a wide range of
applications including congestion control, support of variable packet sizes,
density aware rate adaptation and improved support of unicast transmissions.
However, the practical implementation of MCS adaptation presents a wide range
of implications for the C-V2X radio resources, bandwidth, computation of power
levels and operation of the scheduling mechanism. This paper presents the first
study that provides a detailed analysis and an implemented model highlighting
the implications of MCS adaptation on the operation of the Sensing-Based
Semi-Persistent Scheduling (SB-SPS) mechanism within the C-V2X sidelink. To
showcase the use of MCS adaptation for a particular purpose, a detailed
analysis of its performance for distributed congestion control is undertaken,
while considering different vehicular densities. The results indicate that MCS
adaptation can be useful to reduce channel congestion by decreasing resource
occupation but may not improve the overall packet delivery rate unless
subchannel occupation is reduced. Finally, this study provides the foundation
for other applications of MCS adaptation within the C-V2X sidelink.

    

### [[2109.15175] Coordinated Reinforcement Learning for Optimizing Mobile Networks](http://arxiv.org/abs/2109.15175)


  Mobile networks are composed of many base stations and for each of them many
parameters must be optimized to provide good services. Automatically and
dynamically optimizing all these entities is challenging as they are sensitive
to variations in the environment and can affect each other through
interferences. Reinforcement learning (RL) algorithms are good candidates to
automatically learn base station configuration strategies from incoming data
but they are often hard to scale to many agents. In this work, we demonstrate
how to use coordination graphs and reinforcement learning in a complex
application involving hundreds of cooperating agents. We show how mobile
networks can be modeled using coordination graphs and how network optimization
problems can be solved efficiently using multi- agent reinforcement learning.
The graph structure occurs naturally from expert knowledge about the network
and allows to explicitly learn coordinating behaviors between the antennas
through edge value functions represented by neural networks. We show
empirically that coordinated reinforcement learning outperforms other methods.
The use of local RL updates and parameter sharing can handle a large number of
agents without sacrificing coordination which makes it well suited to optimize
the ever denser networks brought by 5G and beyond.

    

### [[2109.15190] inbaverSim: An OMNeT++ Model Framework for Content Centric Networking](http://arxiv.org/abs/2109.15190)


  Today's networks are used primarily to move content. To cater to this
requirement Information Centric Networks (ICN) were introduced. One of the main
architectures of ICN is Content Centric Networking (CCN) and its derivative,
Named Data Networking (NDN). CCN is standardized at the Internet Engineering
Task Force (IETF) and is envisaged to replace the current Internet over time.
To evaluate large scale deployments of CCN, a model framework called the
inbaverSim is developed in OMNeT++. This work presents the architecture of this
model framework together with an example evaluation using the model framework.
The code is open source and is available at GitHub.

    

### [[2105.10535] Ising Machines' Dynamics and Regularization for Near-Optimal Large and Massive MIMO Detection](http://arxiv.org/abs/2105.10535)


  Optimal MIMO detection has been one of the most challenging and
computationally inefficient tasks in wireless systems. We show that the new
analog computing techniques like Coherent Ising Machines (CIM) are promising
candidates for performing near-optimal MIMO detection. We propose a novel
regularized Ising formulation for MIMO detection that mitigates a common error
floor problem and further evolves it into an algorithm that achieves
near-optimal MIMO detection. Massive MIMO systems, that have a large number of
antennas at the Access point (AP), allow linear detectors to be near-optimal.
However, the simplified detection in these systems comes at the cost of overall
throughput, which could be improved by supporting more users. By means of
numerical simulations, we show that in principle a MIMO detector based on a
hybrid use of a CIM would allow us to add more transmitter antennas/users and
increase the overall throughput of the cell by a significant factor. This would
open up the opportunity to operate using more aggressive modulation and coding
schemes and hence achieve high throughput: for a $16\times16$ large MIMO
system, we estimate around 2.5$\times$ more throughput in mid-SNR regime
($\approx 12 dB$) and 2$\times$ more throughput in high-SNR regime( $>$ 20dB)
than the industry standard, Minimum-Mean Square Error decoding (MMSE).

    

### [[2109.14611] Federated Self-Supervised Contrastive Learning via Ensemble Similarity Distillation](http://arxiv.org/abs/2109.14611)


  This paper investigates the feasibility of learning good representation space
with unlabeled client data in the federated scenario. Existing works trivially
inherit the supervised federated learning methods, which does not apply to the
model heterogeneity and has the potential risk of privacy exposure. To tackle
the problems above, we first identify that self-supervised contrastive local
training is more robust against the non-i.i.d.-ness than the traditional
supervised learning paradigm. Then we propose a novel federated self-supervised
contrastive learning framework FLESD that supports architecture-agnostic local
training and communication-efficient global aggregation. At each round of
communication, the server first gathers a fraction of the clients' inferred
similarity matrices on a public dataset. Then FLESD ensembles the similarity
matrices and trains the global model via similarity distillation. We verify the
effectiveness of our proposed framework by a series of empirical experiments
and show that FLESD has three main advantages over the existing methods: it
handles the model heterogeneity, is less prone to privacy leak, and is more
communication-efficient. We will release the code of this paper in the future.

    

### [[2109.14646] FathomNet: A global underwater image training set for enabling artificial intelligence in the ocean](http://arxiv.org/abs/2109.14646)


  Ocean-going platforms are integrating high-resolution camera feeds for
observation and navigation, producing a deluge of visual data. The volume and
rate of this data collection can rapidly outpace researchers' abilities to
process and analyze them. Recent advances in machine learning enable fast,
sophisticated analysis of visual data, but have had limited success in the
oceanographic world due to lack of dataset standardization, sparse annotation
tools, and insufficient formatting and aggregation of existing, expertly
curated imagery for use by data scientists. To address this need, we have built
FathomNet, a public platform that makes use of existing (and future), expertly
curated data. Initial efforts have leveraged MBARI's Video Annotation and
Reference System and annotated deep sea video database, which has more than 7M
annotations, 1M framegrabs, and 5k terms in the knowledgebase, with additional
contributions by National Geographic Society (NGS) and NOAA's Office of Ocean
Exploration and Research. FathomNet has over 100k localizations of 1k midwater
and benthic classes, and contains iconic and non-iconic views of marine
animals, underwater equipment, debris, etc. We will demonstrate how machine
learning models trained on FathomNet data can be applied across different
institutional video data, (e.g., NGS' Deep Sea Camera System and NOAA's ROV
Deep Discoverer), and enable automated acquisition and tracking of midwater
animals using MBARI's ROV MiniROV. As FathomNet continues to develop and
incorporate more image data from other oceanographic community members, this
effort will enable scientists, explorers, policymakers, storytellers, and the
public to understand and care for our ocean.

    

### [[2109.14648] A Study of Feature Selection and Extraction Algorithms for Cancer Subtype Prediction](http://arxiv.org/abs/2109.14648)


  In this work, we study and analyze different feature selection algorithms
that can be used to classify cancer subtypes in case of highly varying
high-dimensional data. We apply three different feature selection methods on
five different types of cancers having two separate omics each. We show that
the existing feature selection methods are computationally expensive when
applied individually. Instead, we apply these algorithms sequentially which
helps in lowering the computational cost and improving the predictive
performance. We further show that reducing the number of features using some
dimension reduction techniques can improve the performance of machine learning
models in some cases. We support our findings through comprehensive data
analysis and visualization.

    

### [[2109.14653] An Empirical Study of Accuracy, Fairness, Explainability, Distributional Robustness, and Adversarial Robustness](http://arxiv.org/abs/2109.14653)


  To ensure trust in AI models, it is becoming increasingly apparent that
evaluation of models must be extended beyond traditional performance metrics,
like accuracy, to other dimensions, such as fairness, explainability,
adversarial robustness, and distribution shift. We describe an empirical study
to evaluate multiple model types on various metrics along these dimensions on
several datasets. Our results show that no particular model type performs well
on all dimensions, and demonstrate the kinds of trade-offs involved in
selecting models evaluated along multiple dimensions.

    

### [[2109.14659] Stroke recovery phenotyping through network trajectory approaches and graph neural networks](http://arxiv.org/abs/2109.14659)


  Stroke is a leading cause of neurological injury characterized by impairments
in multiple neurological domains including cognition, language, sensory and
motor functions. Clinical recovery in these domains is tracked using a wide
range of measures that may be continuous, ordinal, interval or categorical in
nature, which presents challenges for standard multivariate regression
approaches. This has hindered stroke researchers' ability to achieve an
integrated picture of the complex time-evolving interactions amongst symptoms.
Here we use tools from network science and machine learning that are
particularly well-suited to extracting underlying patterns in such data, and
may assist in prediction of recovery patterns. To demonstrate the utility of
this approach, we analyzed data from the NINDS tPA trial using the Trajectory
Profile Clustering (TPC) method to identify distinct stroke recovery patterns
for 11 different neurological domains at 5 discrete time points. Our analysis
identified 3 distinct stroke trajectory profiles that align with clinically
relevant stroke syndromes, characterized both by distinct clusters of symptoms,
as well as differing degrees of symptom severity. We then validated our
approach using graph neural networks to determine how well our model performed
predictively for stratifying patients into these trajectory profiles at early
vs. later time points post-stroke. We demonstrate that trajectory profile
clustering is an effective method for identifying clinically relevant recovery
subtypes in multidimensional longitudinal datasets, and for early prediction of
symptom progression subtypes in individual patients. This paper is the first
work introducing network trajectory approaches for stroke recovery phenotyping,
and is aimed at enhancing the translation of such novel computational
approaches for practical clinical application.

    

### [[2109.14671] Segmentation of Roads in Satellite Images using specially modified U-Net CNNs](http://arxiv.org/abs/2109.14671)


  The image classification problem has been deeply investigated by the research
community, with computer vision algorithms and with the help of Neural
Networks. The aim of this paper is to build an image classifier for satellite
images of urban scenes that identifies the portions of the images in which a
road is located, separating these portions from the rest. Unlike conventional
computer vision algorithms, convolutional neural networks (CNNs) provide
accurate and reliable results on this task. Our novel approach uses a sliding
window to extract patches out of the whole image, data augmentation for
generating more training/testing data and lastly a series of specially modified
U-Net CNNs. This proposed technique outperforms all other baselines tested in
terms of mean F-score metric.

    

### [[2109.14675] Data Sharing and Compression for Cooperative Networked Control](http://arxiv.org/abs/2109.14675)


  Sharing forecasts of network timeseries data, such as cellular or electricity
load patterns, can improve independent control applications ranging from
traffic scheduling to power generation. Typically, forecasts are designed
without knowledge of a downstream controller's task objective, and thus simply
optimize for mean prediction error. However, such task-agnostic representations
are often too large to stream over a communication network and do not emphasize
salient temporal features for cooperative control. This paper presents a
solution to learn succinct, highly-compressed forecasts that are co-designed
with a modular controller's task objective. Our simulations with real cellular,
Internet-of-Things (IoT), and electricity load data show we can improve a model
predictive controller's performance by at least $25\%$ while transmitting
$80\%$ less data than the competing method. Further, we present theoretical
compression results for a networked variant of the classical linear quadratic
regulator (LQR) control problem.

    

### [[2109.14676] Active Refinement for Multi-Label Learning: A Pseudo-Label Approach](http://arxiv.org/abs/2109.14676)


  The goal of multi-label learning (MLL) is to associate a given instance with
its relevant labels from a set of concepts. Previous works of MLL mainly
focused on the setting where the concept set is assumed to be fixed, while many
real-world applications require introducing new concepts into the set to meet
new demands. One common need is to refine the original coarse concepts and
split them into finer-grained ones, where the refinement process typically
begins with limited labeled data for the finer-grained concepts. To address the
need, we formalize the problem into a special weakly supervised MLL problem to
not only learn the fine-grained concepts efficiently but also allow interactive
queries to strategically collect more informative annotations to further
improve the classifier. The key idea within our approach is to learn to assign
pseudo-labels to the unlabeled entries, and in turn leverage the pseudo-labels
to train the underlying classifier and to inform a better query strategy.
Experimental results demonstrate that our pseudo-label approach is able to
accurately recover the missing ground truth, boosting the prediction
performance significantly over the baseline methods and facilitating a
competitive active learning strategy.

    

### [[2109.14678] Mitigation of Adversarial Policy Imitation via Constrained Randomization of Policy (CRoP)](http://arxiv.org/abs/2109.14678)


  Deep reinforcement learning (DRL) policies are vulnerable to unauthorized
replication attacks, where an adversary exploits imitation learning to
reproduce target policies from observed behavior. In this paper, we propose
Constrained Randomization of Policy (CRoP) as a mitigation technique against
such attacks. CRoP induces the execution of sub-optimal actions at random under
performance loss constraints. We present a parametric analysis of CRoP, address
the optimality of CRoP, and establish theoretical bounds on the adversarial
budget and the expectation of loss. Furthermore, we report the experimental
evaluation of CRoP in Atari environments under adversarial imitation, which
demonstrate the efficacy and feasibility of our proposed method against policy
replication attacks.

    

### [[2109.14685] Automatic Estimation of Ulcerative Colitis Severity from Endoscopy Videos using Ordinal Multi-Instance Learning](http://arxiv.org/abs/2109.14685)


  Ulcerative colitis (UC) is a chronic inflammatory bowel disease characterized
by relapsing inflammation of the large intestine. The severity of UC is often
represented by the Mayo Endoscopic Subscore (MES) which quantifies mucosal
disease activity from endoscopy videos. In clinical trials, an endoscopy video
is assigned an MES based upon the most severe disease activity observed in the
video. For this reason, severe inflammation spread throughout the colon will
receive the same MES as an otherwise healthy colon with severe inflammation
restricted to a small, localized segment. Therefore, the extent of disease
activity throughout the large intestine, and overall response to treatment, may
not be completely captured by the MES. In this work, we aim to automatically
estimate UC severity for each frame in an endoscopy video to provide a higher
resolution assessment of disease activity throughout the colon. Because
annotating severity at the frame-level is expensive, labor-intensive, and
highly subjective, we propose a novel weakly supervised, ordinal classification
method to estimate frame severity from video MES labels alone. Using clinical
trial data, we first achieved 0.92 and 0.90 AUC for predicting mucosal healing
and remission of UC, respectively. Then, for severity estimation, we
demonstrate that our models achieve substantial Cohen's Kappa agreement with
ground truth MES labels, comparable to the inter-rater agreement of expert
clinicians. These findings indicate that our framework could serve as a
foundation for novel clinical endpoints, based on a more localized scoring
system, to better evaluate UC drug efficacy in clinical trials.

    

### [[2109.14686] Vision-Aided Beam Tracking: Explore the Proper Use of Camera Images with Deep Learning](http://arxiv.org/abs/2109.14686)


  We investigate the problem of wireless beam tracking on mmWave bands with the
assistance of camera images. In particular, based on the user's beam indices
used and camera images taken in the trajectory, we predict the optimal beam
indices in the next few time spots. To resolve this problem, we first
reformulate the "ViWi" dataset in [1] to get rid of the image repetition
problem. Then we develop a deep learning approach and investigate various model
components to achieve the best performance. Finally, we explore whether, when,
and how to use the image for better beam prediction. To answer this question,
we split the dataset into three clusters -- (LOS, light NLOS, serious
NLOS)-like -- based on the standard deviation of the beam sequence. With
experiments we demonstrate that using the image indeed helps beam tracking
especially when the user is in serious NLOS, and the solution relies on
carefully-designed dataset for training a model. Generally speaking, including
NLOS-like data for training a model does not benefit beam tracking of the user
in LOS, but including light NLOS-like data for training a model benefits beam
tracking of the user in serious NLOS.

    

### [[2109.14688] Reliable Estimation of KL Divergence using a Discriminator in Reproducing Kernel Hilbert Space](http://arxiv.org/abs/2109.14688)


  Estimating Kullback Leibler (KL) divergence from samples of two distributions
is essential in many machine learning problems. Variational methods using
neural network discriminator have been proposed to achieve this task in a
scalable manner. However, we noted that most of these methods using neural
network discriminators suffer from high fluctuations (variance) in estimates
and instability in training. In this paper, we look at this issue from
statistical learning theory and function space complexity perspective to
understand why this happens and how to solve it. We argue that the cause of
these pathologies is lack of control over the complexity of the neural network
discriminator function and could be mitigated by controlling it. To achieve
this objective, we 1) present a novel construction of the discriminator in the
Reproducing Kernel Hilbert Space (RKHS), 2) theoretically relate the error
probability bound of the KL estimates to the complexity of the discriminator in
the RKHS space, 3) present a scalable way to control the complexity (RKHS norm)
of the discriminator for a reliable estimation of KL divergence, and 4) prove
the consistency of the proposed estimator. In three different applications of
KL divergence : estimation of KL, estimation of mutual information and
Variational Bayes, we show that by controlling the complexity as developed in
the theory, we are able to reduce the variance of KL estimates and stabilize
the training

    

### [[2109.14703] Sequential Estimation under Multiple Resources: a Bandit Point of View](http://arxiv.org/abs/2109.14703)


  The problem of Sequential Estimation under Multiple Resources (SEMR) is
defined in a federated setting. SEMR could be considered as the intersection of
statistical estimation and bandit theory. In this problem, an agent is
confronting with k resources to estimate a parameter $\theta$. The agent should
continuously learn the quality of the resources by wisely choosing them and at
the end, proposes an estimator based on the collected data. In this paper, we
assume that the resources' distributions are Gaussian. The quality of the final
estimator is evaluated by its mean squared error. Also, we restrict our class
of estimators to unbiased estimators in order to define a meaningful notion of
regret. The regret measures the performance of the agent by the variance of the
final estimator in comparison to the optimal variance. We propose a lower bound
to determine the fundamental limit of the setting even in the case that the
distributions are not Gaussian. Also, we offer an order-optimal algorithm to
achieve this lower bound.

    

### [[2109.14707] BulletTrain: Accelerating Robust Neural Network Training via Boundary Example Mining](http://arxiv.org/abs/2109.14707)


  Neural network robustness has become a central topic in machine learning in
recent years. Most training algorithms that improve the model's robustness to
adversarial and common corruptions also introduce a large computational
overhead, requiring as many as ten times the number of forward and backward
passes in order to converge. To combat this inefficiency, we propose
BulletTrain $-$ a boundary example mining technique to drastically reduce the
computational cost of robust training. Our key observation is that only a small
fraction of examples are beneficial for improving robustness. BulletTrain
dynamically predicts these important examples and optimizes robust training
algorithms to focus on the important examples. We apply our technique to
several existing robust training algorithms and achieve a 2.1$\times$ speed-up
for TRADES and MART on CIFAR-10 and a 1.7$\times$ speed-up for AugMix on
CIFAR-10-C and CIFAR-100-C without any reduction in clean and robust accuracy.

    

### [[2109.14710] Convolutional Neural Network Compression through Generalized Kronecker Product Decomposition](http://arxiv.org/abs/2109.14710)


  Modern Convolutional Neural Network (CNN) architectures, despite their
superiority in solving various problems, are generally too large to be deployed
on resource constrained edge devices. In this paper, we reduce memory usage and
floating-point operations required by convolutional layers in CNNs. We compress
these layers by generalizing the Kronecker Product Decomposition to apply to
multidimensional tensors, leading to the Generalized Kronecker Product
Decomposition(GKPD). Our approach yields a plug-and-play module that can be
used as a drop-in replacement for any convolutional layer. Experimental results
for image classification on CIFAR-10 and ImageNet datasets using ResNet,
MobileNetv2 and SeNet architectures substantiate the effectiveness of our
proposed approach. We find that GKPD outperforms state-of-the-art decomposition
methods including Tensor-Train and Tensor-Ring as well as other relevant
compression methods such as pruning and knowledge distillation.

    

### [[2109.14711] Explanation-Aware Experience Replay in Rule-Dense Environments](http://arxiv.org/abs/2109.14711)


  Human environments are often regulated by explicit and complex rulesets.
Integrating Reinforcement Learning (RL) agents into such environments motivates
the development of learning mechanisms that perform well in rule-dense and
exception-ridden environments such as autonomous driving on regulated roads. In
this paper, we propose a method for organising experience by means of
partitioning the experience buffer into clusters labelled on a per-explanation
basis. We present discrete and continuous navigation environments compatible
with modular rulesets and 9 learning tasks. For environments with explainable
rulesets, we convert rule-based explanations into case-based explanations by
allocating state-transitions into clusters labelled with explanations. This
allows us to sample experiences in a curricular and task-oriented manner,
focusing on the rarity, importance, and meaning of events. We label this
concept Explanation-Awareness (XA). We perform XA experience replay (XAER) with
intra and inter-cluster prioritisation, and introduce XA-compatible versions of
DQN, TD3, and SAC. Performance is consistently superior with XA versions of
those algorithms, compared to traditional Prioritised Experience Replay
baselines, indicating that explanation engineering can be used in lieu of
reward engineering for environments with explainable features.

    

### [[2109.14719] Deep neural networks with controlled variable selection for the identification of putative causal genetic variants](http://arxiv.org/abs/2109.14719)


  Deep neural networks (DNN) have been used successfully in many scientific
problems for their high prediction accuracy, but their application to genetic
studies remains challenging due to their poor interpretability. In this paper,
we consider the problem of scalable, robust variable selection in DNN for the
identification of putative causal genetic variants in genome sequencing
studies. We identified a pronounced randomness in feature selection in DNN due
to its stochastic nature, which may hinder interpretability and give rise to
misleading results. We propose an interpretable neural network model,
stabilized using ensembling, with controlled variable selection for genetic
studies. The merit of the proposed method includes: (1) flexible modelling of
the non-linear effect of genetic variants to improve statistical power; (2)
multiple knockoffs in the input layer to rigorously control false discovery
rate; (3) hierarchical layers to substantially reduce the number of weight
parameters and activations to improve computational efficiency; (4)
de-randomized feature selection to stabilize identified signals. We evaluated
the proposed method in extensive simulation studies and applied it to the
analysis of Alzheimer disease genetics. We showed that the proposed method,
when compared to conventional linear and nonlinear methods, can lead to
substantially more discoveries.

    

### [[2109.14725] Tiny-CRNN: Streaming Wakeword Detection In A Low Footprint Setting](http://arxiv.org/abs/2109.14725)


  In this work, we propose Tiny-CRNN (Tiny Convolutional Recurrent Neural
Network) models applied to the problem of wakeword detection, and augment them
with scaled dot product attention. We find that, compared to Convolutional
Neural Network models, False Accepts in a 250k parameter budget can be reduced
by 25% with a 10% reduction in parameter size by using models based on the
Tiny-CRNN architecture, and we can get up to 32% reduction in False Accepts at
a 50k parameter budget with 75% reduction in parameter size compared to
word-level Dense Neural Network models. We discuss solutions to the challenging
problem of performing inference on streaming audio with this architecture, as
well as differences in start-end index errors and latency in comparison to CNN,
DNN, and DNN-HMM models.

    

### [[2109.14727] Dr Jekyll and Mr Hyde: the Strange Case of Off-Policy Policy Updates](http://arxiv.org/abs/2109.14727)


  The policy gradient theorem states that the policy should only be updated in
states that are visited by the current policy, which leads to insufficient
planning in the off-policy states, and thus to convergence to suboptimal
policies. We tackle this planning issue by extending the policy gradient theory
to policy updates with respect to any state density. Under these generalized
policy updates, we show convergence to optimality under a necessary and
sufficient condition on the updates' state densities, and thereby solve the
aforementioned planning issue. We also prove asymptotic convergence rates that
significantly improve those in the policy gradient literature.
To implement the principles prescribed by our theory, we propose an agent, Dr
Jekyll & Mr Hyde (JH), with a double personality: Dr Jekyll purely exploits
while Mr Hyde purely explores. JH's independent policies allow to record two
separate replay buffers: one on-policy (Dr Jekyll's) and one off-policy (Mr
Hyde's), and therefore to update JH's models with a mixture of on-policy and
off-policy updates. More than an algorithm, JH defines principles for
actor-critic algorithms to satisfy the requirements we identify in our
analysis. We extensively test on finite MDPs where JH demonstrates a superior
ability to recover from converging to a suboptimal policy without impairing its
speed of convergence. We also implement a deep version of the algorithm and
test it on a simple problem where it shows promising results.

    

### [[2109.14733] Batched Bandits with Crowd Externalities](http://arxiv.org/abs/2109.14733)


  In Batched Multi-Armed Bandits (BMAB), the policy is not allowed to be
updated at each time step. Usually, the setting asserts a maximum number of
allowed policy updates and the algorithm schedules them so that to minimize the
expected regret. In this paper, we describe a novel setting for BMAB, with the
following twist: the timing of the policy update is not controlled by the BMAB
algorithm, but instead the amount of data received during each batch, called
\textit{crowd}, is influenced by the past selection of arms. We first design a
near-optimal policy with approximate knowledge of the parameters that we prove
to have a regret in $\mathcal{O}(\sqrt{\frac{\ln x}{x}}+\epsilon)$ where $x$ is
the size of the crowd and $\epsilon$ is the parameter error. Next, we implement
a UCB-inspired algorithm that guarantees an additional regret in
$\mathcal{O}\left(\max(K\ln T,\sqrt{T\ln T})\right)$, where $K$ is the number
of arms and $T$ is the horizon.

    

### [[2109.14737] Unlocking the potential of deep learning for marine ecology: overview, applications, and outlook](http://arxiv.org/abs/2109.14737)


  The deep learning revolution is touching all scientific disciplines and
corners of our lives as a means of harnessing the power of big data. Marine
ecology is no exception. These new methods provide analysis of data from
sensors, cameras, and acoustic recorders, even in real time, in ways that are
reproducible and rapid. Off-the-shelf algorithms can find, count, and classify
species from digital images or video and detect cryptic patterns in noisy data.
Using these opportunities requires collaboration across ecological and data
science disciplines, which can be challenging to initiate. To facilitate these
collaborations and promote the use of deep learning towards ecosystem-based
management of the sea, this paper aims to bridge the gap between marine
ecologists and computer scientists. We provide insight into popular deep
learning approaches for ecological data analysis in plain language, focusing on
the techniques of supervised learning with deep neural networks, and illustrate
challenges and opportunities through established and emerging applications of
deep learning to marine ecology. We use established and future-looking case
studies on plankton, fishes, marine mammals, pollution, and nutrient cycling
that involve object detection, classification, tracking, and segmentation of
visualized data. We conclude with a broad outlook of the field's opportunities
and challenges, including potential technological advances and issues with
managing complex data sets.

    

### [[2109.14743] Posttraumatic Stress Disorder Hyperarousal Event Detection Using Smartwatch Physiological and Activity Data](http://arxiv.org/abs/2109.14743)


  Posttraumatic Stress Disorder (PTSD) is a psychiatric condition affecting
nearly a quarter of the United States war veterans who return from war zones.
Treatment for PTSD typically consists of a combination of in-session therapy
and medication. However; patients often experience their most severe PTSD
symptoms outside of therapy sessions. Mobile health applications may address
this gap, but their effectiveness is limited by the current gap in continuous
monitoring and detection capabilities enabling timely intervention. The goal of
this article is to develop a novel method to detect hyperarousal events using
physiological and activity-based machine learning algorithms. Physiological
data including heart rate and body acceleration as well as self-reported
hyperarousal events were collected using a tool developed for commercial
off-the-shelf wearable devices from 99 United States veterans diagnosed with
PTSD over several days. The data were used to develop four machine learning
algorithms: Random Forest, Support Vector Machine, Logistic Regression and
XGBoost. The XGBoost model had the best performance in detecting onset of PTSD
symptoms with over 83% accuracy and an AUC of 0.70. Post-hoc SHapley Additive
exPlanations (SHAP) additive explanation analysis showed that algorithm
predictions were correlated with average heart rate, minimum heart rate and
average body acceleration. Findings show promise in detecting onset of PTSD
symptoms which could be the basis for developing remote and continuous
monitoring systems for PTSD. Such systems may address a vital gap in
just-in-time interventions for PTSD self-management outside of scheduled
clinical appointments.

    

### [[2109.14752] Kernel distance measures for time series, random fields and other structured data](http://arxiv.org/abs/2109.14752)


  This paper introduces kdiff, a novel kernel-based measure for estimating
distances between instances of time series, random fields and other forms of
structured data. This measure is based on the idea of matching distributions
that only overlap over a portion of their region of support. Our proposed
measure is inspired by MPdist which has been previously proposed for such
datasets and is constructed using Euclidean metrics, whereas kdiff is
constructed using non-linear kernel distances. Also, kdiff accounts for both
self and cross similarities across the instances and is defined using a lower
quantile of the distance distribution. Comparing the cross similarity to self
similarity allows for measures of similarity that are more robust to noise and
partial occlusions of the relevant signals. Our proposed measure kdiff is a
more general form of the well known kernel-based Maximum Mean Discrepancy (MMD)
distance estimated over the embeddings. Some theoretical results are provided
for separability conditions using kdiff as a distance measure for clustering
and classification problems where the embedding distributions can be modeled as
two component mixtures. Applications are demonstrated for clustering of
synthetic and real-life time series and image data, and the performance of
kdiff is compared to competing distance measures for clustering.

    

### [[2109.14756] A Two-Time-Scale Stochastic Optimization Framework with Applications in Control and Reinforcement Learning](http://arxiv.org/abs/2109.14756)


  We study a novel two-time-scale stochastic gradient method for solving
optimization problems where the gradient samples are generated from a
time-varying Markov random process parameterized by the underlying optimization
variable. These time-varying samples make the stochastic gradient biased and
dependent, which can potentially lead to the divergence of the iterates. To
address this issue, we consider a two-time-scale update scheme, where one scale
is used to estimate the true gradient from the Markovian samples and the other
scale is used to update the decision variable with the estimated gradient.
While these two iterates are implemented simultaneously, the former is updated
"faster" (using bigger step sizes) than the latter (using smaller step sizes).
Our first contribution is to characterize the finite-time complexity of the
proposed two-time-scale stochastic gradient method. In particular, we provide
explicit formulas for the convergence rates of this method under different
objective functions, namely, strong convexity, convexity, non-convexity under
the PL condition, and general non-convexity.
Our second contribution is to apply our framework to study the performance of
the popular actor-critic methods in solving stochastic control and
reinforcement learning problems. First, we study an online natural actor-critic
algorithm for the linear-quadratic regulator and show that a convergence rate
of $\mathcal{O}(k^{-2/3})$ is achieved. This is the first time such a result is
known in the literature. Second, we look at the standard online actor-critic
algorithm over finite state and action spaces and derive a convergence rate of
$\mathcal{O}(k^{-2/5})$, which recovers the best known rate derived
specifically for this problem. Finally, we support our theoretical analysis
with numerical simulations where the convergence rate is visualized.

    

### [[2109.14760] Chest X-Rays Image Classification from beta-Variational Autoencoders Latent Features](http://arxiv.org/abs/2109.14760)


  Chest X-Ray (CXR) is one of the most common diagnostic techniques used in
everyday clinical practice all around the world. We hereby present a work which
intends to investigate and analyse the use of Deep Learning (DL) techniques to
extract information from such images and allow to classify them, trying to keep
our methodology as general as possible and possibly also usable in a real world
scenario without much effort, in the future. To move in this direction, we
trained several beta-Variational Autoencoder (beta-VAE) models on the CheXpert
dataset, one of the largest publicly available collection of labeled CXR
images; from these models, latent features have been extracted and used to
train other Machine Learning models, able to classify the original images from
the features extracted by the beta-VAE. Lastly, tree-based models have been
combined together in ensemblings to improve the results without the necessity
of further training or models engineering. Expecting some drop in pure
performance with the respect to state of the art classification specific
models, we obtained encouraging results, which show the viability of our
approach and the usability of the high level features extracted by the
autoencoders for classification tasks.

    

### [[2109.14766] Boost-RS: Boosted Embeddings for Recommender Systems and its Application to Enzyme-Substrate Interaction Prediction](http://arxiv.org/abs/2109.14766)


  Despite experimental and curation efforts, the extent of enzyme promiscuity
on substrates continues to be largely unexplored and under documented.
Recommender systems (RS), which are currently unexplored for the
enzyme-substrate interaction prediction problem, can be utilized to provide
enzyme recommendations for substrates, and vice versa. The performance of
Collaborative-Filtering (CF) recommender systems however hinges on the quality
of embedding vectors of users and items (enzymes and substrates in our case).
Importantly, enhancing CF embeddings with heterogeneous auxiliary data,
specially relational data (e.g., hierarchical, pairwise, or groupings), remains
a challenge. We propose an innovative general RS framework, termed Boost-RS,
that enhances RS performance by "boosting" embedding vectors through auxiliary
data. Specifically, Boost-RS is trained and dynamically tuned on multiple
relevant auxiliary learning tasks Boost-RS utilizes contrastive learning tasks
to exploit relational data. To show the efficacy of Boost-RS for the
enzyme-substrate prediction interaction problem, we apply the Boost-RS
framework to several baseline CF models. We show that each of our auxiliary
tasks boosts learning of the embedding vectors, and that contrastive learning
using Boost-RS outperforms attribute concatenation and multi-label learning. We
also show that Boost-RS outperforms similarity-based models. Ablation studies
and visualization of learned representations highlight the importance of using
contrastive learning on some of the auxiliary data in boosting the embedding
vectors.

    

### [[2109.14770] Feature Selection on a Flare Forecasting Testbed: A Comparative Study of 24 Methods](http://arxiv.org/abs/2109.14770)


  The Space-Weather ANalytics for Solar Flares (SWAN-SF) is a multivariate time
series benchmark dataset recently created to serve the heliophysics community
as a testbed for solar flare forecasting models. SWAN-SF contains 54 unique
features, with 24 quantitative features computed from the photospheric magnetic
field maps of active regions, describing their precedent flare activity. In
this study, for the first time, we systematically attacked the problem of
quantifying the relevance of these features to the ambitious task of flare
forecasting. We implemented an end-to-end pipeline for preprocessing, feature
selection, and evaluation phases. We incorporated 24 Feature Subset Selection
(FSS) algorithms, including multivariate and univariate, supervised and
unsupervised, wrappers and filters. We methodologically compared the results of
different FSS algorithms, both on the multivariate time series and vectorized
formats, and tested their correlation and reliability, to the extent possible,
by using the selected features for flare forecasting on unseen data, in
univariate and multivariate fashions. We concluded our investigation with a
report of the best FSS methods in terms of their top-k features, and the
analysis of the findings. We wish the reproducibility of our study and the
availability of the data allow the future attempts be comparable with our
findings and themselves.

    

### [[2109.14772] An Automated Scanning Transmission Electron Microscope Guided by Sparse Data Analytics](http://arxiv.org/abs/2109.14772)


  Artificial intelligence (AI) promises to reshape scientific inquiry and
enable breakthrough discoveries in areas such as energy storage, quantum
computing, and biomedicine. Scanning transmission electron microscopy (STEM), a
cornerstone of the study of chemical and materials systems, stands to benefit
greatly from AI-driven automation. However, present barriers to low-level
instrument control, as well as generalizable and interpretable feature
detection, make truly automated microscopy impractical. Here, we discuss the
design of a closed-loop instrument control platform guided by emerging sparse
data analytics. We demonstrate how a centralized controller, informed by
machine learning combining limited $a$ $priori$ knowledge and task-based
discrimination, can drive on-the-fly experimental decision-making. This
platform unlocks practical, automated analysis of a variety of material
features, enabling new high-throughput and statistical studies.

    

### [[2109.14775] A Prior Knowledge Based Tumor and Tumoral Subregion Segmentation Tool for Pediatric Brain Tumors](http://arxiv.org/abs/2109.14775)


  In the past few years, deep learning (DL) models have drawn great attention
and shown superior performance on brain tumor and subregion segmentation tasks.
However, the success is limited to segmentation of adult gliomas, where
sufficient data have been collected, manually labeled, and published for
training DL models. It is still challenging to segment pediatric tumors,
because the appearances are different from adult gliomas. Hence, directly
applying a pretained DL model on pediatric data usually generates unacceptable
results. Because pediatric data is very limited, both labeled and unlabeled, we
present a brain tumor segmentation model that is based on knowledge rather than
learning from data. We also provide segmentation of more subregions for super
heterogeneous tumor like atypical teratoid rhabdoid tumor (ATRT). Our proposed
approach showed superior performance on both whole tumor and subregion
segmentation tasks to DL based models on our pediatric data when training data
is not available for transfer learning.

    

### [[2109.14778] CALDA: Improving Multi-Source Time Series Domain Adaptation with Contrastive Adversarial Learning](http://arxiv.org/abs/2109.14778)


  Unsupervised domain adaptation (UDA) provides a strategy for improving
machine learning performance in data-rich (target) domains where ground truth
labels are inaccessible but can be found in related (source) domains. In cases
where meta-domain information such as label distributions is available, weak
supervision can further boost performance. We propose a novel framework, CALDA,
to tackle these two problems. CALDA synergistically combines the principles of
contrastive learning and adversarial learning to robustly support multi-source
UDA (MS-UDA) for time series data. Similar to prior methods, CALDA utilizes
adversarial learning to align source and target feature representations. Unlike
prior approaches, CALDA additionally leverages cross-source label information
across domains. CALDA pulls examples with the same label close to each other,
while pushing apart examples with different labels, reshaping the space through
contrastive learning. Unlike prior contrastive adaptation methods, CALDA
requires neither data augmentation nor pseudo labeling, which may be more
challenging for time series. We empirically validate our proposed approach.
Based on results from human activity recognition, electromyography, and
synthetic datasets, we find utilizing cross-source information improves
performance over prior time series and contrastive methods. Weak supervision
further improves performance, even in the presence of noise, allowing CALDA to
offer generalizable strategies for MS-UDA. Code is available at:
this https URL


### [[2109.14789] Bitcoin Transaction Strategy Construction Based on Deep Reinforcement Learning](http://arxiv.org/abs/2109.14789)


  The emerging cryptocurrency market has lately received great attention for
asset allocation due to its decentralization uniqueness. However, its
volatility and brand new trading mode have made it challenging to devising an
acceptable automatically-generating strategy. This study proposes a framework
for automatic high-frequency bitcoin transactions based on a deep reinforcement
learning algorithm-proximal policy optimization (PPO). The framework creatively
regards the transaction process as actions, returns as awards and prices as
states to align with the idea of reinforcement learning. It compares advanced
machine learning-based models for static price predictions including support
vector machine (SVM), multi-layer perceptron (MLP), long short-term memory
(LSTM), temporal convolutional network (TCN), and Transformer by applying them
to the real-time bitcoin price and the experimental results demonstrate that
LSTM outperforms. Then an automatically-generating transaction strategy is
constructed building on PPO with LSTM as the basis to construct the policy.
Extensive empirical studies validate that the proposed method performs
superiorly to various common trading strategy benchmarks for a single financial
product. The approach is able to trade bitcoins in a simulated environment with
synchronous data and obtains a 31.67% more return than that of the best
benchmark, improving the benchmark by 12.75%. The proposed framework can earn
excess returns through both the period of volatility and surge, which opens the
door to research on building a single cryptocurrency trading strategy based on
deep learning. Visualizations of trading the process show how the model handles
high-frequency transactions to provide inspiration and demonstrate that it can
be expanded to other financial products.

    

### [[2109.14792] Automated airway segmentation by learning graphical structure](http://arxiv.org/abs/2109.14792)


  In this research project, we put forward an advanced method for airway
segmentation based on the existent convolutional neural network (CNN) and graph
neural network (GNN). The method is originated from the vessel segmentation,
but we ameliorate it and enable the novel model to perform better for datasets
from computed tomography (CT) scans. Current methods for airway segmentation
are considering the regular grid only. No matter what the detailed model is,
including the 3-dimensional CNN or 2-dimensional CNN in three directions, the
overall graph structures are not taken into consideration. In our model, with
the neighbourhoods of airway taken into account, the graph structure is
incorporated and the segmentation of airways are improved compared with the
traditional CNN methods. We perform experiments on the chest CT scans, where
the ground truth segmentation labels are produced manually. The proposed model
shows that compared with the CNN-only method, the combination of CNN and GNN
has a better performance in that the bronchi in the chest CT scans can be
detected in most cases. In addition, the model we propose has a wide extension
since the architecture is also utilitarian in fulfilling similar aims in other
datasets. Hence, the state-of-the-art model is of great significance and highly
applicable in our daily lives.
Keywords: Airway segmentation, Convolutional neural network, Graph neural
network

    

### [[2109.14795] Towards Better Data Augmentation using Wasserstein Distance in Variational Auto-encoder](http://arxiv.org/abs/2109.14795)


  VAE, or variational auto-encoder, compresses data into latent attributes, and
generates new data of different varieties. VAE based on KL divergence has been
considered as an effective technique for data augmentation. In this paper, we
propose the use of Wasserstein distance as a measure of distributional
similarity for the latent attributes, and show its superior theoretical lower
bound (ELBO) compared with that of KL divergence under mild conditions. Using
multiple experiments, we demonstrate that the new loss function exhibits better
convergence property and generates artificial images that could better aid the
image classification tasks.

    

### [[2109.14798] Introducing the DOME Activation Functions](http://arxiv.org/abs/2109.14798)


  In this paper, we introduce a novel non-linear activation function that
spontaneously induces class-compactness and regularization in the embedding
space of neural networks. The function is dubbed DOME for Difference Of
Mirrored Exponential terms. The basic form of the function can replace the
sigmoid or the hyperbolic tangent functions as an output activation function
for binary classification problems. The function can also be extended to the
case of multi-class classification, and used as an alternative to the standard
softmax function. It can also be further generalized to take more flexible
shapes suitable for intermediate layers of a network. In this version of the
paper, we only introduce the concept. In a subsequent version, experimental
evaluation will be added.

    

### [[2109.14811] Surveillance Evasion Through Bayesian Reinforcement Learning](http://arxiv.org/abs/2109.14811)


  We consider a 2D continuous path planning problem with a completely unknown
intensity of random termination: an Evader is trying to escape a domain while
minimizing the cumulative risk of detection (termination) by adversarial
Observers. Those Observers' surveillance intensity is a priori unknown and has
to be learned through repetitive path planning. We propose a new algorithm that
utilizes Gaussian process regression to model the unknown surveillance
intensity and relies on a confidence bound technique to promote strategic
exploration. We illustrate our method through several examples and confirm the
convergence of averaged regret experimentally.

    

### [[2109.14820] A Generalized Hierarchical Nonnegative Tensor Decomposition](http://arxiv.org/abs/2109.14820)


  Nonnegative matrix factorization (NMF) has found many applications including
topic modeling and document analysis. Hierarchical NMF (HNMF) variants are able
to learn topics at various levels of granularity and illustrate their
hierarchical relationship. Recently, nonnegative tensor factorization (NTF)
methods have been applied in a similar fashion in order to handle data sets
with complex, multi-modal structure. Hierarchical NTF (HNTF) methods have been
proposed, however these methods do not naturally generalize their matrix-based
counterparts. Here, we propose a new HNTF model which directly generalizes a
HNMF model special case, and provide a supervised extension. We also provide a
multiplicative updates training method for this model. Our experimental results
show that this model more naturally illuminates the topic hierarchy than
previous HNMF and HNTF methods.

    

### [[2109.14830] Reinforcement Learning for Classical Planning: Viewing Heuristics as Dense Reward Generators](http://arxiv.org/abs/2109.14830)


  Recent advances in reinforcement learning (RL) have led to a growing interest
in applying RL to classical planning domains or applying classical planning
methods to some complex RL domains. However, the long-horizon goal-based
problems found in classical planning lead to sparse rewards for RL, making
direct application inefficient. In this paper, we propose to leverage
domain-independent heuristic functions commonly used in the classical planning
literature to improve the sample efficiency of RL. These classical heuristics
act as dense reward generators to alleviate the sparse-rewards issue and enable
our RL agent to learn domain-specific value functions as residuals on these
heuristics, making learning easier. Correct application of this technique
requires consolidating the discounted metric used in RL and the non-discounted
metric used in heuristics. We implement the value functions using Neural Logic
Machines, a neural network architecture designed for grounded first-order logic
inputs. We demonstrate on several classical planning domains that using
classical heuristics for RL allows for good sample efficiency compared to
sparse-reward RL. We further show that our learned value functions generalize
to novel problem instances in the same domain.

    

### [[2109.14840] A system on chip for melanoma detection using FPGA-based SVM classifier](http://arxiv.org/abs/2109.14840)


  Support Vector Machine (SVM) is a robust machine learning model that shows
high accuracy with different classification problems, and is widely used for
various embedded applications. However , implementation of embedded SVM
classifiers is challenging, due to the inherent complicated computations
required. This motivates implementing the SVM on hardware platforms for
achieving high performance computing at low cost and power consumption.
Melanoma is the most aggressive form of skin cancer that increases the
mortality rate. We aim to develop an optimized embedded SVM classifier
dedicated for a low-cost handheld device for early detection of melanoma at the
primary healthcare. In this paper, we propose a hardware/software co-design for
implementing the SVM classifier onto FPGA to realize melanoma detection on a
chip. The implemented SVM on a recent hybrid FPGA (Zynq) platform utilizing the
modern UltraFast High-Level Synthesis design methodology achieves efficient
melanoma classification on chip. The hardware implementation results
demonstrate classification accuracy of 97.9%, and a significant hardware
acceleration rate of 21 with only 3% resources utilization and 1.69W for power
consumption. These results show that the implemented system on chip meets
crucial embedded system constraints of high performance and low resources
utilization, power consumption, and cost, while achieving efficient
classification with high classification accuracy.

    

### [[2109.14844] LIFE: Learning Individual Features for Multivariate Time Series Prediction with Missing Values](http://arxiv.org/abs/2109.14844)


  Multivariate time series (MTS) prediction is ubiquitous in real-world fields,
but MTS data often contains missing values. In recent years, there has been an
increasing interest in using end-to-end models to handle MTS with missing
values. To generate features for prediction, existing methods either merge all
input dimensions of MTS or tackle each input dimension independently. However,
both approaches are hard to perform well because the former usually produce
many unreliable features and the latter lacks correlated information. In this
paper, we propose a Learning Individual Features (LIFE) framework, which
provides a new paradigm for MTS prediction with missing values. LIFE generates
reliable features for prediction by using the correlated dimensions as
auxiliary information and suppressing the interference from uncorrelated
dimensions with missing values. Experiments on three real-world data sets
verify the superiority of LIFE to existing state-of-the-art models.

    

### [[2109.14845] AffectGAN: Affect-Based Generative Art Driven by Semantics](http://arxiv.org/abs/2109.14845)


  This paper introduces a novel method for generating artistic images that
express particular affective states. Leveraging state-of-the-art deep learning
methods for visual generation (through generative adversarial networks),
semantic models from OpenAI, and the annotated dataset of the visual art
encyclopedia WikiArt, our AffectGAN model is able to generate images based on
specific or broad semantic prompts and intended affective outcomes. A small
dataset of 32 images generated by AffectGAN is annotated by 50 participants in
terms of the particular emotion they elicit, as well as their quality and
novelty. Results show that for most instances the intended emotion used as a
prompt for image generation matches the participants' responses. This
small-scale study brings forth a new vision towards blending affective
computing with computational creativity, enabling generative systems with
intentionality in terms of the emotions they wish their output to elicit.

    

### [[2109.14855] Learning Material Parameters and Hydrodynamics of Soft Robotic Fish via Differentiable Simulation](http://arxiv.org/abs/2109.14855)


  The high dimensionality of soft mechanisms and the complex physics of
fluid-structure interactions render the sim2real gap for soft robots
particularly challenging. Our framework allows high fidelity prediction of
dynamic behavior for composite bi-morph bending structures in real hardware to
accuracy near measurement uncertainty. We address this gap with our
differentiable simulation tool by learning the material parameters and
hydrodynamics of our robots. We demonstrate an experimentally-verified, fast
optimization pipeline for learning the material parameters and hydrodynamics
from quasi-static and dynamic data via differentiable simulation. Our method
identifies physically plausible Young's moduli for various soft silicone
elastomers and stiff acetal copolymers used in creation of our three different
fish robot designs. For these robots we provide a differentiable and more
robust estimate of the thrust force than analytical models and we successfully
predict deformation to millimeter accuracy in dynamic experiments under various
actuation signals. Although we focus on a specific application for underwater
soft robots, our framework is applicable to any pneumatically actuated soft
mechanism. This work presents a prototypical hardware and simulation problem
solved using our framework that can be extended straightforwardly to higher
dimensional parameter inference, learning control policies, and computational
design enabled by its differentiability.

    

### [[2109.14873] Early Bearing Fault Diagnosis of Rotating Machinery by 1D Self-Organized Operational Neural Networks](http://arxiv.org/abs/2109.14873)


  Preventive maintenance of modern electric rotating machinery (RM) is critical
for ensuring reliable operation, preventing unpredicted breakdowns and avoiding
costly repairs. Recently many studies investigated machine learning monitoring
methods especially based on Deep Learning networks focusing mostly on detecting
bearing faults; however, none of them addressed bearing fault severity
classification for early fault diagnosis with high enough accuracy. 1D
Convolutional Neural Networks (CNNs) have indeed achieved good performance for
detecting RM bearing faults from raw vibration and current signals but did not
classify fault severity. Furthermore, recent studies have demonstrated the
limitation in terms of learning capability of conventional CNNs attributed to
the basic underlying linear neuron model. Recently, Operational Neural Networks
(ONNs) were proposed to enhance the learning capability of CNN by introducing
non-linear neuron models and further heterogeneity in the network
configuration. In this study, we propose 1D Self-organized ONNs (Self-ONNs)
with generative neurons for bearing fault severity classification and providing
continuous condition monitoring. Experimental results over the benchmark
NSF/IMS bearing vibration dataset using both x- and y-axis vibration signals
for inner race and rolling element faults demonstrate that the proposed 1D
Self-ONNs achieve significant performance gap against the state-of-the-art (1D
CNNs) with similar computational complexity.

    

### [[2109.14875] Adversarial Regression with Doubly Non-negative Weighting Matrices](http://arxiv.org/abs/2109.14875)


  Many machine learning tasks that involve predicting an output response can be
solved by training a weighted regression model. Unfortunately, the predictive
power of this type of models may severely deteriorate under low sample sizes or
under covariate perturbations. Reweighting the training samples has aroused as
an effective mitigation strategy to these problems. In this paper, we propose a
novel and coherent scheme for kernel-reweighted regression by reparametrizing
the sample weights using a doubly non-negative matrix. When the weighting
matrix is confined in an uncertainty set using either the log-determinant
divergence or the Bures-Wasserstein distance, we show that the adversarially
reweighted estimate can be solved efficiently using first-order methods.
Numerical experiments show that our reweighting strategy delivers promising
results on numerous datasets.

    

### [[2109.14879] Robust Segmentation Models using an Uncertainty Slice Sampling Based Annotation Workflow](http://arxiv.org/abs/2109.14879)


  Semantic segmentation neural networks require pixel-level annotations in
large quantities to achieve a good performance. In the medical domain, such
annotations are expensive, because they are time-consuming and require expert
knowledge. Active learning optimizes the annotation effort by devising
strategies to select cases for labeling that are most informative to the model.
In this work, we propose an uncertainty slice sampling (USS) strategy for
semantic segmentation of 3D medical volumes that selects 2D image slices for
annotation and compare it with various other strategies. We demonstrate the
efficiency of USS on a CT liver segmentation task using multi-site data. After
five iterations, the training data resulting from USS consisted of 2410 slices
(4% of all slices in the data pool) compared to 8121 (13%), 8641 (14%), and
3730 (6%) for uncertainty volume (UVS), random volume (RVS), and random slice
(RSS) sampling, respectively. Despite being trained on the smallest amount of
data, the model based on the USS strategy evaluated on 234 test volumes
significantly outperformed models trained according to other strategies and
achieved a mean Dice index of 0.964, a relative volume error of 4.2%, a mean
surface distance of 1.35 mm, and a Hausdorff distance of 23.4 mm. This was only
slightly inferior to 0.967, 3.8%, 1.18 mm, and 22.9 mm achieved by a model
trained on all available data, but the robustness analysis using the 5th
percentile of Dice and the 95th percentile of the remaining metrics
demonstrated that USS resulted not only in the most robust model compared to
other sampling schemes, but also outperformed the model trained on all data
according to Dice (0.946 vs. 0.945) and mean surface distance (1.92 mm vs. 2.03
mm).

    

### [[2109.14881] Extracting stochastic dynamical systems with $α$-stable Lévy noise from data](http://arxiv.org/abs/2109.14881)


  With the rapid increase of valuable observational, experimental and simulated
data for complex systems, much efforts have been devoted to identifying
governing laws underlying the evolution of these systems. Despite the wide
applications of non-Gaussian fluctuations in numerous physical phenomena, the
data-driven approaches to extract stochastic dynamical systems with
(non-Gaussian) Lévy noise are relatively few so far. In this work, we propose
a data-driven method to extract stochastic dynamical systems with
$\alpha$-stable Lévy noise from short burst data based on the properties of
$\alpha$-stable distributions. More specifically, we first estimate the Lévy
jump measure and noise intensity via computing mean and variance of the
amplitude of the increment of the sample paths. Then we approximate the drift
coefficient by combining nonlocal Kramers-Moyal formulas with normalizing
flows. Numerical experiments on one- and two-dimensional prototypical examples
illustrate the accuracy and effectiveness of our method. This approach will
become an effective scientific tool in discovering stochastic governing laws of
complex phenomena and understanding dynamical behaviors under non-Gaussian
fluctuations.

    

### [[2109.14885] Out-of-Distribution Detection for Medical Applications: Guidelines for Practical Evaluation](http://arxiv.org/abs/2109.14885)


  Detection of Out-of-Distribution (OOD) samples in real time is a crucial
safety check for deployment of machine learning models in the medical field.
Despite a growing number of uncertainty quantification techniques, there is a
lack of evaluation guidelines on how to select OOD detection methods in
practice. This gap impedes implementation of OOD detection methods for
real-world applications. Here, we propose a series of practical considerations
and tests to choose the best OOD detector for a specific medical dataset. These
guidelines are illustrated on a real-life use case of Electronic Health Records
(EHR). Our results can serve as a guide for implementation of OOD detection
methods in clinical practice, mitigating risks associated with the use of
machine learning models in healthcare.

    

### [[2109.14894] How Neural Processes Improve Graph Link Prediction](http://arxiv.org/abs/2109.14894)


  Link prediction is a fundamental problem in graph data analysis. While most
of the literature focuses on transductive link prediction that requires all the
graph nodes and majority of links in training, inductive link prediction, which
only uses a proportion of the nodes and their links in training, is a more
challenging problem in various real-world applications. In this paper, we
propose a meta-learning approach with graph neural networks for link
prediction: Neural Processes for Graph Neural Networks (NPGNN), which can
perform both transductive and inductive learning tasks and adapt to patterns in
a large new graph after training with a small subgraph. Experiments on
real-world graphs are conducted to validate our model, where the results
suggest that the proposed method achieves stronger performance compared to
other state-of-the-art models, and meanwhile generalizes well when training on
a small subgraph.

    

### [[2109.14900] Impact of Channel Variation on One-Class Learning for Spoof Detection](http://arxiv.org/abs/2109.14900)


  The value of Spoofing detection in increasing the reliability of the ASV
system is unparalleled. In reality, however, the performance of countermeasure
systems (CMs) degrades significantly due to channel variation.
Multi-conditional training(MCT) is a well-established technique to handle such
scenarios. However, "which data-feeding strategy is optimal for MCT?" is not
known in the case of spoof detection. In this paper, various codec simulations
were used to modify ASVspoof 2019 dataset, and assessments were done using
data-feeding and mini-batching strategies to help address this question. Our
experiments aim to test the efficacy of the various margin-based losses for
training Resnet based models with LFCC front-end feature extractor to correctly
classify the spoofed and bonafide samples degraded using codec simulations.
Contrastingly to most of the works that focus mainly on architectures, this
study highlights the relevance of the deemed-of-low-importance process of
data-feeding and mini-batching to raise awareness of the need to refine it for
better performance.

    

### [[2109.14910] CrossCLR: Cross-modal Contrastive Learning For Multi-modal Video Representations](http://arxiv.org/abs/2109.14910)


  Contrastive learning allows us to flexibly define powerful losses by
contrasting positive pairs from sets of negative samples. Recently, the
principle has also been used to learn cross-modal embeddings for video and
text, yet without exploiting its full potential. In particular, previous losses
do not take the intra-modality similarities into account, which leads to
inefficient embeddings, as the same content is mapped to multiple points in the
embedding space. With CrossCLR, we present a contrastive loss that fixes this
issue. Moreover, we define sets of highly related samples in terms of their
input embeddings and exclude them from the negative samples to avoid issues
with false negatives. We show that these principles consistently improve the
quality of the learned embeddings. The joint embeddings learned with CrossCLR
extend the state of the art in video-text retrieval on Youcook2 and LSMDC
datasets and in video captioning on Youcook2 dataset by a large margin. We also
demonstrate the generality of the concept by learning improved joint embeddings
for other pairs of modalities.

    

### [[2109.14925] Genealogical Population-Based Training for Hyperparameter Optimization](http://arxiv.org/abs/2109.14925)


  Hyperparameter optimization aims at finding more rapidly and efficiently the
best hyperparameters (HPs) of learning models such as neural networks. In this
work, we present a new approach called GPBT (Genealogical Population-Based
Training), which shares many points with Population-Based Training: our
approach outputs a schedule of HPs and updates both weights and HPs in a single
run, but brings several novel contributions: the choice of new HPs is made by a
modular search algorithm, the search algorithm can search HPs independently for
models with different weights and can exploit separately the maximum amount of
meaningful information (genealogically-related) from previous HPs evaluations
instead of exploiting together all previous HPs evaluations, a variation of
early stopping allows a 2-3 fold acceleration at small performance cost. GPBT
significantly outperforms all other approaches of HP Optimization, on all
supervised learning experiments tested in terms of speed and performances. HPs
tuning will become less computationally expensive using our approach, not only
in the deep learning field, but potentially for all processes based on
iterative optimization.

    

### [[2109.14929] Learning the Markov Decision Process in the Sparse Gaussian Elimination](http://arxiv.org/abs/2109.14929)


  We propose a learning-based approach for the sparse Gaussian Elimination.
There are many hard combinatorial optimization problems in modern sparse
solver. These NP-hard problems could be handled in the framework of Markov
Decision Process, especially the Q-Learning technique. We proposed some
Q-Learning algorithms for the main modules of sparse solver: minimum degree
ordering, task scheduling and adaptive pivoting. Finally, we recast the sparse
solver into the framework of Q-Learning.
Our study is the first step to connect these two classical mathematical
models: Gaussian Elimination and Markov Decision Process. Our learning-based
algorithm could help improve the performance of sparse solver, which has been
verified in some numerical experiments.

    

### [[2109.14950] A useful criterion on studying consistent estimation in community detection](http://arxiv.org/abs/2109.14950)


  In network analysis, developing a unified theoretical framework that can
compare methods under different models is an interesting problem. This paper
proposes a partial solution to this problem. We summarize the idea of using
separation condition for a standard network and sharp threshold of
Erdös-Rényi random graph to study consistent estimation, compare
theoretical error rates and requirements on network sparsity of spectral
methods under models that can degenerate to stochastic block model as a
four-step criterion SCSTC. Using SCSTC, we find some inconsistent phenomena on
separation condition and sharp threshold in community detection. Especially, we
find original theoretical results of the SPACL algorithm introduced to estimate
network memberships under the mixed membership stochastic blockmodel were
sub-optimal. To find the formation mechanism of inconsistencies, we
re-establish theoretical convergence rates of this algorithm by applying recent
techniques on row-wise eigenvector deviation. The results are further extended
to the degree corrected mixed membership model. By comparison, our results
enjoy smaller error rates, lesser dependence on the number of communities,
weaker requirements on network sparsity, and so forth. Furthermore, separation
condition and sharp threshold obtained from our theoretical results match
classical results, which shows the usefulness of this criterion on studying
consistent estimation.

    

### [[2109.14956] Comparative Validation of Machine Learning Algorithms for Surgical Workflow and Skill Analysis with the HeiChole Benchmark](http://arxiv.org/abs/2109.14956)


  PURPOSE: Surgical workflow and skill analysis are key technologies for the
next generation of cognitive surgical assistance systems. These systems could
increase the safety of the operation through context-sensitive warnings and
semi-autonomous robotic assistance or improve training of surgeons via
data-driven feedback. In surgical workflow analysis up to 91% average precision
has been reported for phase recognition on an open data single-center dataset.
In this work we investigated the generalizability of phase recognition
algorithms in a multi-center setting including more difficult recognition tasks
such as surgical action and surgical skill. METHODS: To achieve this goal, a
dataset with 33 laparoscopic cholecystectomy videos from three surgical centers
with a total operation time of 22 hours was created. Labels included annotation
of seven surgical phases with 250 phase transitions, 5514 occurences of four
surgical actions, 6980 occurences of 21 surgical instruments from seven
instrument categories and 495 skill classifications in five skill dimensions.
The dataset was used in the 2019 Endoscopic Vision challenge, sub-challenge for
surgical workflow and skill analysis. Here, 12 teams submitted their machine
learning algorithms for recognition of phase, action, instrument and/or skill
assessment. RESULTS: F1-scores were achieved for phase recognition between
23.9% and 67.7% (n=9 teams), for instrument presence detection between 38.5%
and 63.8% (n=8 teams), but for action recognition only between 21.8% and 23.3%
(n=5 teams). The average absolute error for skill assessment was 0.78 (n=1
team). CONCLUSION: Surgical workflow and skill analysis are promising
technologies to support the surgical team, but are not solved yet, as shown by
our comparison of algorithms. This novel benchmark can be used for comparable
evaluation and validation of future work.

    

### [[2109.14960] Prune Your Model Before Distill It](http://arxiv.org/abs/2109.14960)


  Unstructured pruning reduces a significant amount of weights of neural
networks. However, unstructured pruning provides a sparse network with the same
network architecture as the original network. On the other hand, structured
pruning provides an efficient network architecture by removing channels, but
the parameter reduction is not significant. In this paper, we consider
transferring knowledge from unstructured pruning to a network with efficient
architecture (with fewer channels). In particular, we apply the knowledge
distillation (KD), where the teacher network is a sparse network (obtained from
unstructured pruning), and the student network has an efficient architecture.
We observe that learning from the pruned teacher is more effective than
learning from the unpruned teacher. We further achieve the promising
experimental results that unstructured pruning can improve the performance of
knowledge distillation in general.

    

### [[2109.14970] A Friend Recommendation System using Semantic Based KNN Algorithm](http://arxiv.org/abs/2109.14970)


  Social networking has become a major part of all our lives and we depend on
it for day to day purposes. It is a medium that is used by people all around
the world even in the smallest of towns. Its main purpose is to promote and aid
communication between people. Social networks, such as Facebook, Twitter etc.
were created for the sole purpose of helping individuals communicate about
anything with each other. These networks are becoming an important and also
contemporary method to make friends from any part of this world. These new
friends can communicate through any form of social media. Recommendation
systems exist in all the social networks which aid users to find new friends
and unite to more people and form associations and alliances with people.

    

### [[2109.14974] Unified Data Collection for Visual-Inertial Calibration via Deep Reinforcement Learning](http://arxiv.org/abs/2109.14974)


  Visual-inertial sensors have a wide range of applications in robotics.
However, good performance often requires different sophisticated motion
routines to accurately calibrate camera intrinsics and inter-sensor extrinsics.
This work presents a novel formulation to learn a motion policy to be executed
on a robot arm for automatic data collection for calibrating intrinsics and
extrinsics jointly. Our approach models the calibration process compactly using
model-free deep reinforcement learning to derive a policy that guides the
motions of a robotic arm holding the sensor to efficiently collect measurements
that can be used for both camera intrinsic calibration and camera-IMU extrinsic
calibration. Given the current pose and collected measurements, the learned
policy generates the subsequent transformation that optimizes sensor
calibration accuracy. The evaluations in simulation and on a real robotic
system show that our learned policy generates favorable motion trajectories and
collects enough measurements efficiently that yield the desired intrinsics and
extrinsics with short path lengths. In simulation we are able to perform
calibrations 10 times faster than hand-crafted policies, which transfers to a
real-world speed up of 3 times over a human expert.

    

### [[2109.14998] A Privacy-preserving Distributed Training Framework for Cooperative Multi-agent Deep Reinforcement Learning](http://arxiv.org/abs/2109.14998)


  Deep Reinforcement Learning (DRL) sometimes needs a large amount of data to
converge in the training procedure and in some cases, each action of the agent
may produce regret. This barrier naturally motivates different data sets or
environment owners to cooperate to share their knowledge and train their agents
more efficiently. However, it raises privacy concerns if we directly merge the
raw data from different owners. To solve this problem, we proposed a new Deep
Neural Network (DNN) architecture with both global NN and local NN, and a
distributed training framework. We allow the global weights to be updated by
all the collaborator agents while the local weights are only updated by the
agent they belong to. In this way, we hope the global weighs can share the
common knowledge among these collaborators while the local NN can keep the
specialized properties and ensure the agent to be compatible with its specific
environment. Experiments show that the framework can efficiently help agents in
the same or similar environments to collaborate in their training process and
gain a higher convergence rate and better performance.

    

### [[2109.15004] XPROAX-Local explanations for text classification with progressive neighborhood approximation](http://arxiv.org/abs/2109.15004)


  The importance of the neighborhood for training a local surrogate model to
approximate the local decision boundary of a black box classifier has been
already highlighted in the literature. Several attempts have been made to
construct a better neighborhood for high dimensional data, like texts, by using
generative autoencoders. However, existing approaches mainly generate neighbors
by selecting purely at random from the latent space and struggle under the
curse of dimensionality to learn a good local decision boundary. To overcome
this problem, we propose a progressive approximation of the neighborhood using
counterfactual instances as initial landmarks and a careful 2-stage sampling
approach to refine counterfactuals and generate factuals in the neighborhood of
the input instance to be explained. Our work focuses on textual data and our
explanations consist of both word-level explanations from the original instance
(intrinsic) and the neighborhood (extrinsic) and factual- and
counterfactual-instances discovered during the neighborhood generation process
that further reveal the effect of altering certain parts in the input text. Our
experiments on real-world datasets demonstrate that our method outperforms the
competitors in terms of usefulness and stability (for the qualitative part) and
completeness, compactness and correctness (for the quantitative part).

    

### [[2109.15005] SCIMAT: Science and Mathematics Dataset](http://arxiv.org/abs/2109.15005)


  In this work, we announce a comprehensive well curated and opensource dataset
with millions of samples for pre-college and college level problems in
mathematicsand science. A preliminary set of results using transformer
architecture with character to character encoding is shown. The dataset
identifies some challenging problem and invites research on better architecture
search

    

### [[2109.15014] Deep Neural Compression Via Concurrent Pruning and Self-Distillation](http://arxiv.org/abs/2109.15014)


  Pruning aims to reduce the number of parameters while maintaining performance
close to the original network. This work proposes a novel
\emph{self-distillation} based pruning strategy, whereby the representational
similarity between the pruned and unpruned versions of the same network is
maximized. Unlike previous approaches that treat distillation and pruning
separately, we use distillation to inform the pruning criteria, without
requiring a separate student network as in knowledge distillation. We show that
the proposed {\em cross-correlation objective for self-distilled pruning}
implicitly encourages sparse solutions, naturally complementing magnitude-based
pruning criteria. Experiments on the GLUE and XGLUE benchmarks show that
self-distilled pruning increases mono- and cross-lingual language model
performance. Self-distilled pruned models also outperform smaller Transformers
with an equal number of parameters and are competitive against (6 times) larger
distilled networks. We also observe that self-distillation (1) maximizes class
separability, (2) increases the signal-to-noise ratio, and (3) converges faster
after pruning steps, providing further insights into why self-distilled pruning
improves generalization.

    

### [[2109.15015] Robust Allocations with Diversity Constraints](http://arxiv.org/abs/2109.15015)


  We consider the problem of allocating divisible items among multiple agents,
and consider the setting where any agent is allowed to introduce diversity
constraints on the items they are allocated. We motivate this via settings
where the items themselves correspond to user ad slots or task workers with
attributes such as race and gender on which the principal seeks to achieve
demographic parity. We consider the following question: When an agent expresses
diversity constraints into an allocation rule, is the allocation of other
agents hurt significantly? If this happens, the cost of introducing such
constraints is disproportionately borne by agents who do not benefit from
diversity. We codify this via two desiderata capturing robustness. These are no
negative externality -- other agents are not hurt -- and monotonicity -- the
agent enforcing the constraint does not see a large increase in value. We show
in a formal sense that the Nash Welfare rule that maximizes product of agent
values is uniquely positioned to be robust when diversity constraints are
introduced, while almost all other natural allocation rules fail this
criterion. We also show that the guarantees achieved by Nash Welfare are nearly
optimal within a widely studied class of allocation rules. We finally perform
an empirical simulation on real-world data that models ad allocations to show
that this gap between Nash Welfare and other rules persists in the wild.

    

### [[2109.15021] On Riemannian Approach for Constrained Optimization Model in Extreme Classification Problems](http://arxiv.org/abs/2109.15021)


  We propose a novel Riemannian method for solving the Extreme multi-label
classification problem that exploits the geometric structure of the sparse
low-dimensional local embedding models. A constrained optimization problem is
formulated as an optimization problem on matrix manifold and solved using a
Riemannian optimization method. The proposed approach is tested on several real
world large scale multi-label datasets and its usefulness is demonstrated
through numerical experiments. The numerical experiments suggest that the
proposed method is fastest to train and has least model size among the
embedding-based methods. An outline of the proof of convergence for the
proposed Riemannian optimization method is also stated.

    

### [[2109.15030] On the Convergence of the Projected Alternating Maximization Algorithm for Equitable and Optimal Transport](http://arxiv.org/abs/2109.15030)


  This paper studies the equitable and optimal transport (EOT) problem, which
has many applications such as fair division problems and optimal transport with
multiple agents etc. In the discrete distributions case, the EOT problem can be
formulated as a linear program (LP). Since this LP is prohibitively large for
general LP solvers, Scetbon \etal \cite{scetbon2021equitable} suggests to
perturb the problem by adding an entropy regularization. They proposed a
projected alternating maximization algorithm (PAM) to solve the dual of the
entropy regularized EOT. In this paper, we provide the first convergence
analysis of PAM. A novel rounding procedure is proposed to help construct the
primal solution for the original EOT problem. We also propose a variant of PAM
by incorporating the extrapolation technique that can numerically improve the
performance of PAM. Results in this paper may shed lights on block coordinate
(gradient) descent methods for general optimization problems.

    

### [[2109.15031] Back in Black: A Comparative Evaluation of Recent State-Of-The-Art Black-Box Attacks](http://arxiv.org/abs/2109.15031)


  The field of adversarial machine learning has experienced a near exponential
growth in the amount of papers being produced since 2018. This massive
information output has yet to be properly processed and categorized. In this
paper, we seek to help alleviate this problem by systematizing the recent
advances in adversarial machine learning black-box attacks since 2019. Our
survey summarizes and categorizes 20 recent black-box attacks. We also present
a new analysis for understanding the attack success rate with respect to the
adversarial model used in each paper. Overall, our paper surveys a wide body of
literature to highlight recent attack developments and organizes them into four
attack categories: score based attacks, decision based attacks, transfer
attacks and non-traditional attacks. Further, we provide a new mathematical
framework to show exactly how attack results can fairly be compared.

    

### [[2109.15035] Who Explains the Explanation? Quantitatively Assessing Feature Attribution Methods](http://arxiv.org/abs/2109.15035)


  AI explainability seeks to increase the transparency of models, making them
more trustworthy in the process. The need for transparency has been recently
motivated by the emergence of deep learning models, which are particularly
obscure by nature. Even in the domain of images, where deep learning has
succeeded the most, explainability is still poorly assessed. Multiple feature
attribution methods have been proposed in the literature with the purpose of
explaining a DL model's behavior using visual queues, but no standardized
metrics to assess or select these methods exist. In this paper we propose a
novel evaluation metric -- the Focus -- designed to quantify the faithfulness
of explanations provided by feature attribution methods, such as LRP or
GradCAM. First, we show the robustness of the metric through randomization
experiments, and then use Focus to evaluate and compare three popular
explainability techniques using multiple architectures and datasets. Our
results find LRP and GradCAM to be consistent and reliable, the former being
more accurate for high performing models, while the latter remains most
competitive even when applied to poorly performing models. Finally, we identify
a strong relation between Focus and factors like model architecture and task,
unveiling a new unsupervised approach for the assessment of models.

    

### [[2109.15036] Automated Workers Ergonomic Risk Assessment in Manual Material Handling using sEMG Wearable Sensors and Machine Learning](http://arxiv.org/abs/2109.15036)


  Manual material handling tasks have the potential to be highly unsafe from an
ergonomic viewpoint. Safety inspections to monitor body postures can help
mitigate ergonomic risks of material handling. However, the real effect of
awkward muscle movements, strains, and excessive forces that may result in an
injury may not be identified by external cues. This paper evaluates the ability
of surface electromyogram (EMG)-based systems together with machine learning
algorithms to automatically detect body movements that may harm muscles in
material handling. The analysis utilized a lifting equation developed by the
U.S. National Institute for Occupational Safety and Health (NIOSH). This
equation determines a Recommended Weight Limit, which suggests the maximum
acceptable weight that a healthy worker can lift and carry as well as a Lifting
Index value to assess the risk extent. Four different machine learning models,
namely Decision Tree, Support Vector Machine, K-Nearest Neighbor, and Random
Forest are developed to classify the risk assessments calculated based on the
NIOSH lifting equation. The sensitivity of the models to various parameters is
also evaluated to find the best performance using each algorithm. Results
indicate that Decision Tree models have the potential to predict the risk level
with close to 99.35% accuracy.

    

### [[2109.15042] A Priori Calibration of Transient Kinetics Data via Machine Learning](http://arxiv.org/abs/2109.15042)


  The temporal analysis of products reactor provides a vast amount of transient
kinetic information that may be used to describe a variety of chemical features
including the residence time distribution, kinetic coefficients, number of
active sites, and the reaction mechanism. However, as with any measurement
device, the TAP reactor signal is convoluted with noise. To reduce the
uncertainty of the kinetic measurement and any derived parameters or
mechanisms, proper preprocessing must be performed prior to any advanced
analysis. This preprocessing consists of baseline correction, i.e., a shift in
the voltage response, and calibration, i.e., a scaling of the flux response
based on prior experiments. The current methodology of preprocessing requires
significant user discretion and reliance on previous experiments that may drift
over time. Herein we use machine learning techniques combined with physical
constraints to convert the raw instrument signal to chemical information. As
such, the proposed methodology demonstrates clear benefits over the traditional
preprocessing in the calibration of the inert and feed mixture products without
need of prior calibration experiments or heuristic input from the user.

    

### [[2109.15044] SPATE-GAN: Improved Generative Modeling of Dynamic Spatio-Temporal Patterns with an Autoregressive Embedding Loss](http://arxiv.org/abs/2109.15044)


  From ecology to atmospheric sciences, many academic disciplines deal with
data characterized by intricate spatio-temporal complexities, the modeling of
which often requires specialized approaches. Generative models of these data
are of particular interest, as they enable a range of impactful downstream
applications like simulation or creating synthetic training data. Recent work
has highlighted the potential of generative adversarial nets (GANs) for
generating spatio-temporal data. A new GAN algorithm COT-GAN, inspired by the
theory of causal optimal transport (COT), was proposed in an attempt to better
tackle this challenge. However, the task of learning more complex
spatio-temporal patterns requires additional knowledge of their specific data
structures. In this study, we propose a novel loss objective combined with
COT-GAN based on an autoregressive embedding to reinforce the learning of
spatio-temporal dynamics. We devise SPATE (spatio-temporal association), a new
metric measuring spatio-temporal autocorrelation by using the deviance of
observations from their expected values. We compute SPATE for real and
synthetic data samples and use it to compute an embedding loss that considers
space-time interactions, nudging the GAN to learn outputs that are faithful to
the observed dynamics. We test this new objective on a diverse set of complex
spatio-temporal patterns: turbulent flows, log-Gaussian Cox processes and
global weather data. We show that our novel embedding loss improves performance
without any changes to the architecture of the COT-GAN backbone, highlighting
our model's increased capacity for capturing autoregressive structures. We also
contextualize our work with respect to recent advances in physics-informed deep
learning and interdisciplinary work connecting neural networks with geographic
and geophysical sciences.

    

### [[2109.15045] Stock Index Prediction using Cointegration test and Quantile Loss](http://arxiv.org/abs/2109.15045)


  Recent researches on stock prediction using deep learning methods has been
actively studied. This is the task to predict the movement of stock prices in
the future based on historical trends. The approach to predicting the movement
based solely on the pattern of the historical movement of it on charts, not on
fundamental values, is called the Technical Analysis, which can be divided into
univariate and multivariate methods in the regression task. According to the
latter approach, it is important to select different factors well as inputs to
enhance the performance of the model. Moreover, its performance can depend on
which loss is used to train the model. However, most studies tend to focus on
building the structures of models, not on how to select informative factors as
inputs to train them. In this paper, we propose a method that can get better
performance in terms of returns when selecting informative factors using the
cointegration test and learning the model using quantile loss. We compare the
two RNN variants with quantile loss with only five factors obtained through the
cointegration test among the entire 15 stock index factors collected in the
experiment. The Cumulative return and Sharpe ratio were used to evaluate the
performance of trained models. Our experimental results show that our proposed
method outperforms the other conventional approaches.

    

### [[2109.15048] Physical Gradients for Deep Learning](http://arxiv.org/abs/2109.15048)


  Solving inverse problems, such as parameter estimation and optimal control,
is a vital part of science. Many experiments repeatedly collect data and employ
machine learning algorithms to quickly infer solutions to the associated
inverse problems. We find that state-of-the-art training techniques are not
well-suited to many problems that involve physical processes since the
magnitude and direction of the gradients can vary strongly. We propose a novel
hybrid training approach that combines higher-order optimization methods with
machine learning techniques. We replace the gradient of the physical process by
a new construct, referred to as the physical gradient. This also allows us to
introduce domain knowledge into training by incorporating priors about the
solution space into the gradients. We demonstrate the capabilities of our
method on a variety of canonical physical systems, showing that physical
gradients yield significant improvements on a wide range of optimization and
learning problems.

    

### [[2109.15050] An Offline Deep Reinforcement Learning for Maintenance Decision-Making](http://arxiv.org/abs/2109.15050)


  Several machine learning and deep learning frameworks have been proposed to
solve remaining useful life estimation and failure prediction problems in
recent years. Having access to the remaining useful life estimation or
likelihood of failure in near future helps operators to assess the operating
conditions and, therefore, provides better opportunities for sound repair and
maintenance decisions. However, many operators believe remaining useful life
estimation and failure prediction solutions are incomplete answers to the
maintenance challenge. They argue that knowing the likelihood of failure in the
future is not enough to make maintenance decisions that minimize costs and keep
the operators safe. In this paper, we present a maintenance framework based on
offline supervised deep reinforcement learning that instead of providing
information such as likelihood of failure, suggests actions such as
"continuation of the operation" or "the visitation of the repair shop" to the
operators in order to maximize the overall profit. Using offline reinforcement
learning makes it possible to learn the optimum maintenance policy from
historical data without relying on expensive simulators. We demonstrate the
application of our solution in a case study using the NASA C-MAPSS dataset.

    

### [[2109.15053] Fine-tuning wav2vec2 for speaker recognition](http://arxiv.org/abs/2109.15053)


  This paper explores applying the wav2vec2 framework to speaker recognition
instead of speech recognition. We study the effectiveness of the pre-trained
weights on the speaker recognition task, and how to pool the wav2vec2 output
sequence into a fixed-length speaker embedding. To adapt the framework to
speaker recognition, we propose a single-utterance classification variant with
CE or AAM softmax loss, and an utterance-pair classification variant with BCE
loss. Our best performing variant, w2v2-aam, achieves a 1.88% EER on the
extended voxceleb1 test set compared to 1.69% EER with an ECAPA-TDNN baseline.
Code is available at this https URL.

    

### [[2109.15057] Dynamical symmetry breaking through AI: The dimer self-trapping transition](http://arxiv.org/abs/2109.15057)


  The nonlinear dimer obtained through the nonlinear Schr{ö}dinger equation
has been a workhorse for the discovery the role nonlinearity plays in strongly
interacting systems. While the analysis of the stationary states demonstrates
the onset of a symmetry broken state for some degree of nonlinearity, the full
dynamics maps the system into an effective $\phi^4$ model. In this latter
context, the self-trapping transition is an initial condition dependent
transfer of a classical particle over a barrier set by the nonlinear term. This
transition has been investigated analytically and mathematically it is
expressed through the hyperbolic limit of Jacobian elliptic functions. The aim
of the present work is to recapture this transition through the use of methods
of Artificial Intelligence (AI). Specifically, we used a physics motivated
machine learning model that is shown to be able to capture the original dynamic
self-trapping transition and its dependence on initial conditions. Exploitation
of this result in the case of the non-degenerate nonlinear dimer gives
additional information on the more general dynamics and helps delineate linear
from nonlinear localization. This work shows how AI methods may be embedded in
physics and provide useful tools for discovery.

    

### [[2109.15059] Stock Price Prediction Under Anomalous Circumstances](http://arxiv.org/abs/2109.15059)


  The stock market is volatile and complicated, especially in 2020. Because of
a series of global and regional "black swans," such as the COVID-19 pandemic,
the U.S. stock market triggered the circuit breaker three times within one week
of March 9 to 16, which is unprecedented throughout history. Affected by the
whole circumstance, the stock prices of individual corporations also plummeted
by rates that were never predicted by any pre-developed forecasting models. It
reveals that there was a lack of satisfactory models that could predict the
changes in stocks prices when catastrophic, highly unlikely events occur. To
fill the void of such models and to help prevent investors from heavy losses
during uncertain times, this paper aims to capture the movement pattern of
stock prices under anomalous circumstances. First, we detect outliers in
sequential stock prices by fitting a standard ARIMA model and identifying the
points where predictions deviate significantly from actual values. With the
selected data points, we train ARIMA and LSTM models at the single-stock level,
industry level, and general market level, respectively. Since the public moods
affect the stock market tremendously, a sentiment analysis is also incorporated
into the models in the form of sentiment scores, which are converted from
comments about specific stocks on Reddit. Based on 100 companies' stock prices
in the period of 2016 to 2020, the models achieve an average prediction
accuracy of 98% which can be used to optimize existing prediction
methodologies.

    

### [[2109.15062] Towards Principled Causal Effect Estimation by Deep Identifiable Models](http://arxiv.org/abs/2109.15062)


  As an important problem of causal inference, we discuss the estimation of
treatment effects (TEs) under unobserved confounding. Representing the
confounder as a latent variable, we propose Intact-VAE, a new variant of
variational autoencoder (VAE), motivated by the prognostic score that is
sufficient for identifying TEs. Our VAE also naturally gives representation
balanced for treatment groups, using its prior. Experiments on (semi-)synthetic
datasets show state-of-the-art performance under diverse settings. Based on the
identifiability of our model, further theoretical developments on
identification and consistent estimation are also discussed. This paves the way
towards principled causal effect estimation by deep neural networks.

    

### [[2109.15063] Workflow Augmentation of Video Data for Event Recognition with Time-Sensitive Neural Networks](http://arxiv.org/abs/2109.15063)


  Supervised training of neural networks requires large, diverse and well
annotated data sets. In the medical field, this is often difficult to achieve
due to constraints in time, expert knowledge and prevalence of an event.
Artificial data augmentation can help to prevent overfitting and improve the
detection of rare events as well as overall performance. However, most
augmentation techniques use purely spatial transformations, which are not
sufficient for video data with temporal correlations. In this paper, we present
a novel methodology for workflow augmentation and demonstrate its benefit for
event recognition in cataract surgery. The proposed approach increases the
frequency of event alternation by creating artificial videos. The original
video is split into event segments and a workflow graph is extracted from the
original annotations. Finally, the segments are assembled into new videos based
on the workflow graph. Compared to the original videos, the frequency of event
alternation in the augmented cataract surgery videos increased by 26%. Further,
a 3% higher classification accuracy and a 7.8% higher precision was achieved
compared to a state-of-the-art approach. Our approach is particularly helpful
to increase the occurrence of rare but important events and can be applied to a
large variety of use cases.

    

### [[2109.15068] iShape: A First Step Towards Irregular Shape Instance Segmentation](http://arxiv.org/abs/2109.15068)


  In this paper, we introduce a brand new dataset to promote the study of
instance segmentation for objects with irregular shapes. Our key observation is
that though irregularly shaped objects widely exist in daily life and
industrial scenarios, they received little attention in the instance
segmentation field due to the lack of corresponding datasets. To fill this gap,
we propose iShape, an irregular shape dataset for instance segmentation. iShape
contains six sub-datasets with one real and five synthetics, each represents a
scene of a typical irregular shape. Unlike most existing instance segmentation
datasets of regular objects, iShape has many characteristics that challenge
existing instance segmentation algorithms, such as large overlaps between
bounding boxes of instances, extreme aspect ratios, and large numbers of
connected components per instance. We benchmark popular instance segmentation
methods on iShape and find their performance drop dramatically. Hence, we
propose an affinity-based instance segmentation algorithm, called ASIS, as a
stronger baseline. ASIS explicitly combines perception and reasoning to solve
Arbitrary Shape Instance Segmentation including irregular objects. Experimental
results show that ASIS outperforms the state-of-the-art on iShape. Dataset and
code are available at this https URL


### [[2109.15089] Biologically Plausible Training Mechanisms for Self-Supervised Learning in Deep Networks](http://arxiv.org/abs/2109.15089)


  We develop biologically plausible training mechanisms for self-supervised
learning (SSL) in deep networks. SSL, with a contrastive loss, is more natural
as it does not require labelled data and its robustness to perturbations yields
more adaptable embeddings. Moreover the perturbation of data required to create
positive pairs for SSL is easily produced in a natural environment by observing
objects in motion and with variable lighting over time. We propose a
contrastive hinge based loss whose error involves simple local computations as
opposed to the standard contrastive losses employed in the literature, which do
not lend themselves easily to implementation in a network architecture due to
complex computations involving ratios and inner products. Furthermore we show
that learning can be performed with one of two more plausible alternatives to
backpropagation. The first is difference target propagation (DTP), which trains
network parameters using target-based local losses and employs a Hebbian
learning rule, thus overcoming the biologically implausible symmetric weight
problem in backpropagation. The second is simply layer-wise learning, where
each layer is directly connected to a layer computing the loss error. The
layers are either updated sequentially in a greedy fashion (GLL) or in random
order (RLL), and each training stage involves a single hidden layer network.
The one step backpropagation needed for each such network can either be altered
with fixed random feedback weights as proposed in Lillicrap et al. (2016), or
using updated random feedback as in Amit (2019). Both methods represent
alternatives to the symmetric weight issue of backpropagation. By training
convolutional neural networks (CNNs) with SSL and DTP, GLL or RLL, we find that
our proposed framework achieves comparable performance to its implausible
counterparts in both linear evaluation and transfer learning tasks.

    

### [[2109.15097] The Role of Bio-Inspired Modularity in General Learning](http://arxiv.org/abs/2109.15097)


  One goal of general intelligence is to learn novel information without
overwriting prior learning. The utility of learning without forgetting (CF) is
twofold: first, the system can return to previously learned tasks after
learning something new. In addition, bootstrapping previous knowledge may allow
for faster learning of a novel task. Previous approaches to CF and
bootstrapping are primarily based on modifying learning in the form of changing
weights to tune the model to the current task, overwriting previously tuned
weights from previous tasks.However, another critical factor that has been
largely overlooked is the initial network topology, or architecture. Here, we
argue that the topology of biological brains likely evolved certain features
that are designed to achieve this kind of informational conservation. In
particular, we consider that the highly conserved property of modularity may
offer a solution to weight-update learning methods that adheres to the learning
without catastrophic forgetting and bootstrapping constraints. Final
considerations are then made on how to combine these two learning objectives in
a dynamical, general learning system.

    

### [[2109.15101] Compositional generalization in semantic parsing with pretrained transformers](http://arxiv.org/abs/2109.15101)


  Large-scale pretraining instills large amounts of knowledge in deep neural
networks. This, in turn, improves the generalization behavior of these models
in downstream tasks. What exactly are the limits to the generalization benefits
of large-scale pretraining? Here, we report observations from some simple
experiments aimed at addressing this question in the context of two semantic
parsing tasks involving natural language, SCAN and COGS. We show that language
models pretrained exclusively with non-English corpora, or even with
programming language corpora, significantly improve out-of-distribution
generalization in these benchmarks, compared with models trained from scratch,
even though both benchmarks are English-based. This demonstrates the
surprisingly broad transferability of pretrained representations and knowledge.
Pretraining with a large-scale protein sequence prediction task, on the other
hand, mostly deteriorates the generalization performance in SCAN and COGS,
suggesting that pretrained representations do not transfer universally and that
there are constraints on the similarity between the pretraining and downstream
domains for successful transfer. Finally, we show that larger models are harder
to train from scratch and their generalization accuracy is lower when trained
up to convergence on the relatively small SCAN and COGS datasets, but the
benefits of large-scale pretraining become much clearer with larger models.

    

### [[2109.15103] Scalable Rule-Based Representation Learning for Interpretable Classification](http://arxiv.org/abs/2109.15103)


  Rule-based models, e.g., decision trees, are widely used in scenarios
demanding high model interpretability for their transparent inner structures
and good model expressivity. However, rule-based models are hard to optimize,
especially on large data sets, due to their discrete parameters and structures.
Ensemble methods and fuzzy/soft rules are commonly used to improve performance,
but they sacrifice the model interpretability. To obtain both good scalability
and interpretability, we propose a new classifier, named Rule-based
Representation Learner (RRL), that automatically learns interpretable non-fuzzy
rules for data representation and classification. To train the
non-differentiable RRL effectively, we project it to a continuous space and
propose a novel training method, called Gradient Grafting, that can directly
optimize the discrete model using gradient descent. An improved design of
logical activation functions is also devised to increase the scalability of RRL
and enable it to discretize the continuous features end-to-end. Exhaustive
experiments on nine small and four large data sets show that RRL outperforms
the competitive interpretable approaches and can be easily adjusted to obtain a
trade-off between classification accuracy and model complexity for different
scenarios. Our code is available at: this https URL.

    

### [[2109.15112] Interpretability in Safety-Critical FinancialTrading Systems](http://arxiv.org/abs/2109.15112)


  Sophisticated machine learning (ML) models to inform trading in the financial
sector create problems of interpretability and risk management. Seemingly
robust forecasting models may behave erroneously in out of distribution
settings. In 2020, some of the world's most sophisticated quant hedge funds
suffered losses as their ML models were first underhedged, and then
overcompensated. We implement a gradient-based approach for precisely
stress-testing how a trading model's forecasts can be manipulated, and their
effects on downstream tasks at the trading execution level. We construct inputs
-- whether in changes to sentiment or market variables -- that efficiently
affect changes in the return distribution. In an industry-standard trading
pipeline, we perturb model inputs for eight S&P 500 stocks. We find our
approach discovers seemingly in-sample input settings that result in large
negative shifts in return distributions. We provide the financial community
with mechanisms to interpret ML forecasts in trading systems. For the security
community, we provide a compelling application where studying ML robustness
necessitates that one capture an end-to-end system's performance rather than
study a ML model in isolation. Indeed, we show in our evaluation that errors in
the forecasting model's predictions alone are not sufficient for trading
decisions made based on these forecasts to yield a negative return.

    

### [[2109.15118] Overview of the CLEF-2019 CheckThat!: Automatic Identification and Verification of Claims](http://arxiv.org/abs/2109.15118)


  We present an overview of the second edition of the CheckThat! Lab at CLEF
2019. The lab featured two tasks in two different languages: English and
Arabic. Task 1 (English) challenged the participating systems to predict which
claims in a political debate or speech should be prioritized for fact-checking.
Task 2 (Arabic) asked to (A) rank a given set of Web pages with respect to a
check-worthy claim based on their usefulness for fact-checking that claim, (B)
classify these same Web pages according to their degree of usefulness for
fact-checking the target claim, (C) identify useful passages from these pages,
and (D) use the useful pages to predict the claim's factuality. CheckThat!
provided a full evaluation framework, consisting of data in English (derived
from fact-checking sources) and Arabic (gathered and annotated from scratch)
and evaluation based on mean average precision (MAP) and normalized discounted
cumulative gain (nDCG) for ranking, and F1 for classification. A total of 47
teams registered to participate in this lab, and fourteen of them actually
submitted runs (compared to nine last year). The evaluation results show that
the most successful approaches to Task 1 used various neural networks and
logistic regression. As for Task 2, learning-to-rank was used by the highest
scoring runs for subtask A, while different classifiers were used in the other
subtasks. We release to the research community all datasets from the lab as
well as the evaluation scripts, which should enable further research in the
important tasks of check-worthiness estimation and automatic claim
verification.

    

### [[2109.15120] SUper Team at SemEval-2016 Task 3: Building a feature-rich system for community question answering](http://arxiv.org/abs/2109.15120)


  We present the system we built for participating in SemEval-2016 Task 3 on
Community Question Answering. We achieved the best results on subtask C, and
strong results on subtasks A and B, by combining a rich set of various types of
features: semantic, lexical, metadata, and user-related. The most important
group turned out to be the metadata for the question and for the comment,
semantic vectors trained on QatarLiving data and similarities between the
question and the comment for subtasks A and C, and between the original and the
related question for Subtask B.

    

### [[2109.15121] Feature-Rich Named Entity Recognition for Bulgarian Using Conditional Random Fields](http://arxiv.org/abs/2109.15121)


  The paper presents a feature-rich approach to the automatic recognition and
categorization of named entities (persons, organizations, locations, and
miscellaneous) in news text for Bulgarian. We combine well-established features
used for other languages with language-specific lexical, syntactic and
morphological information. In particular, we make use of the rich tagset
annotation of the BulTreeBank (680 morpho-syntactic tags), from which we derive
suitable task-specific tagsets (local and nonlocal). We further add
domain-specific gazetteers and additional unlabeled data, achieving F1=89.4%,
which is comparable to the state-of-the-art results for English.

    

### [[2109.15127] Real-Time Multi-Level Neonatal Heart and Lung Sound Quality Assessment for Telehealth Applications](http://arxiv.org/abs/2109.15127)


  Digital stethoscopes in combination with telehealth allow chest sounds to be
easily collected and transmitted for remote monitoring and diagnosis. Chest
sounds contain important information about a newborn's cardio-respiratory
health. However, low-quality recordings complicate the remote monitoring and
diagnosis. In this study, a new method is proposed to objectively and
automatically assess heart and lung signal quality on a 5-level scale in
real-time and to assess the effect of signal quality on vital sign estimation.
For the evaluation, a total of 207 10s long chest sounds were taken from 119
preterm and full-term babies. Thirty of the recordings from ten subjects were
obtained with synchronous vital signs from the Neonatal Intensive Care Unit
(NICU) based on electrocardiogram recordings. As reference, seven annotators
independently assessed the signal quality. For automatic quality
classification, 400 features were extracted from the chest sounds. After
feature selection using minimum redundancy and maximum relevancy algorithm,
class balancing, and hyper-parameter optimization, a variety of multi-class and
ordinal classification and regression algorithms were trained. Then, heart rate
and breathing rate were automatically estimated from the chest sounds using
adapted pre-existing methods. The results of subject-wise leave-one-out
cross-validation show that the best-performing models had a mean squared error
(MSE) of 0.49 and 0.61, and balanced accuracy of 57% and 51% for heart and lung
qualities, respectively. The best-performing models for real-time analysis
(<200ms) had MSE of 0.459 and 0.67, and balanced accuracy of 57% and 46%,
respectively. Our experimental results underscore that increasing the signal
quality leads to a reduction in vital sign error, with only high-quality
recordings having a mean absolute error of less than 5 beats per minute, as
required for clinical usage.

    

### [[2109.15129] Convolution-Free Waveform Transformers for Multi-Lead ECG Classification](http://arxiv.org/abs/2109.15129)


  We present our entry to the 2021 PhysioNet/CinC challenge - a waveform
transformer model to detect cardiac abnormalities from ECG recordings. We
compare the performance of the waveform transformer model on different ECG-lead
subsets using approximately 88,000 ECG recordings from six datasets. In the
official rankings, team prna ranked between 9 and 15 on 12, 6, 4, 3 and 2-lead
sets respectively. Our waveform transformer model achieved an average challenge
metric of 0.47 on the held-out test set across all ECG-lead subsets. Our
combined performance across all leads placed us at rank 11 out of 39 officially
ranking teams.

    

### [[2109.15134] Variational Marginal Particle Filters](http://arxiv.org/abs/2109.15134)


  Variational inference for state space models (SSMs) is known to be hard in
general. Recent works focus on deriving variational objectives for SSMs from
unbiased sequential Monte Carlo estimators. We reveal that the marginal
particle filter is obtained from sequential Monte Carlo by applying
Rao-Blackwellization operations, which sacrifices the trajectory information
for reduced variance and differentiability. We propose the variational marginal
particle filter (VMPF), which is a differentiable and reparameterizable
variational filtering objective for SSMs based on an unbiased estimator. We
find that VMPF with biased gradients gives tighter bounds than previous
objectives, and the unbiased reparameterization gradients are sometimes
beneficial.

    

### [[2109.15136] Transfer Learning Based Multi-Objective Evolutionary Algorithm for Community Detection of Dynamic Complex Networks](http://arxiv.org/abs/2109.15136)


  Dynamic community detection is the hotspot and basic problem of complex
network and artificial intelligence research in recent years. It is necessary
to maximize the accuracy of clustering as the network structure changes, but
also to minimize the two consecutive clustering differences between the two
results. There is a trade-off relationship between these two objectives. In
this paper, we propose a Feature Transfer Based Multi-Objective Optimization
Genetic Algorithm (TMOGA) based on transfer learning and traditional
multi-objective evolutionary algorithm framework. The main idea is to extract
stable features from past community structures, retain valuable feature
information, and integrate this feature information into current optimization
processes to improve the evolutionary algorithms. Additionally, a new
theoretical framework is proposed in this paper to analyze community detection
problem based on information theory. Then, we exploit this framework to prove
the rationality of TMOGA. Finally, the experimental results show that our
algorithm can achieve better clustering effects compared with the
state-of-the-art dynamic network community detection algorithms in diverse test
problems.

    

### [[2109.15141] Predicting Code Review Completion Time in Modern Code Review](http://arxiv.org/abs/2109.15141)


  Context. Modern Code Review (MCR) is being adopted in both open source and
commercial projects as a common practice. MCR is a widely acknowledged quality
assurance practice that allows early detection of defects as well as poor
coding practices. It also brings several other benefits such as knowledge
sharing, team awareness, and collaboration.
Problem. In practice, code reviews can experience significant delays to be
completed due to various socio-technical factors which can affect the project
quality and cost. For a successful review process, peer reviewers should
perform their review tasks in a timely manner while providing relevant feedback
about the code change being reviewed. However, there is a lack of tool support
to help developers estimating the time required to complete a code review prior
to accepting or declining a review request.
Objective. Our objective is to build and validate an effective approach to
predict the code review completion time in the context of MCR and help
developers better manage and prioritize their code review tasks.
Method. We formulate the prediction of the code review completion time as a
learning problem. In particular, we propose a framework based on regression
models to (i) effectively estimate the code review completion time, and (ii)
understand the main factors influencing code review completion time.

    

### [[2109.15142] Redesigning the Transformer Architecture with Insights from Multi-particle Dynamical Systems](http://arxiv.org/abs/2109.15142)


  The Transformer and its variants have been proven to be efficient sequence
learners in many different domains. Despite their staggering success, a
critical issue has been the enormous number of parameters that must be trained
(ranging from $10^7$ to $10^{11}$) along with the quadratic complexity of
dot-product attention. In this work, we investigate the problem of
approximating the two central components of the Transformer -- multi-head
self-attention and point-wise feed-forward transformation, with reduced
parameter space and computational complexity. We build upon recent developments
in analyzing deep neural networks as numerical solvers of ordinary differential
equations. Taking advantage of an analogy between Transformer stages and the
evolution of a dynamical system of multiple interacting particles, we formulate
a temporal evolution scheme, TransEvolve, to bypass costly dot-product
attention over multiple stacked layers. We perform exhaustive experiments with
TransEvolve on well-known encoder-decoder as well as encoder-only tasks. We
observe that the degree of approximation (or inversely, the degree of parameter
reduction) has different effects on the performance, depending on the task.
While in the encoder-decoder regime, TransEvolve delivers performances
comparable to the original Transformer, in encoder-only tasks it consistently
outperforms Transformer along with several subsequent variants.

    

### [[2109.15147] Reinforcement Learning with Information-Theoretic Actuation](http://arxiv.org/abs/2109.15147)


  Reinforcement Learning formalises an embodied agent's interaction with the
environment through observations, rewards and actions. But where do the actions
come from? Actions are often considered to represent something external, such
as the movement of a limb, a chess piece, or more generally, the output of an
actuator. In this work we explore and formalize a contrasting view, namely that
actions are best thought of as the output of a sequence of internal choices
with respect to an action model. This view is particularly well-suited for
leveraging the recent advances in large sequence models as prior knowledge for
multi-task reinforcement learning problems. Our main contribution in this work
is to show how to augment the standard MDP formalism with a sequential notion
of internal action using information-theoretic techniques, and that this leads
to self-consistent definitions of both internal and external action value
functions.

    

### [[2109.15149] Deep Embedded K-Means Clustering](http://arxiv.org/abs/2109.15149)


  Recently, deep clustering methods have gained momentum because of the high
representational power of deep neural networks (DNNs) such as autoencoder. The
key idea is that representation learning and clustering can reinforce each
other: Good representations lead to good clustering while good clustering
provides good supervisory signals to representation learning. Critical
questions include: 1) How to optimize representation learning and clustering?
2) Should the reconstruction loss of autoencoder be considered always? In this
paper, we propose DEKM (for Deep Embedded K-Means) to answer these two
questions. Since the embedding space generated by autoencoder may have no
obvious cluster structures, we propose to further transform the embedding space
to a new space that reveals the cluster-structure information. This is achieved
by an orthonormal transformation matrix, which contains the eigenvectors of the
within-class scatter matrix of K-means. The eigenvalues indicate the importance
of the eigenvectors' contributions to the cluster-structure information in the
new space. Our goal is to increase the cluster-structure information. To this
end, we discard the decoder and propose a greedy method to optimize the
representation. Representation learning and clustering are alternately
optimized by DEKM. Experimental results on the real-world datasets demonstrate
that DEKM achieves state-of-the-art performance.

    

### [[2109.15154] Causal Matrix Completion](http://arxiv.org/abs/2109.15154)


  Matrix completion is the study of recovering an underlying matrix from a
sparse subset of noisy observations. Traditionally, it is assumed that the
entries of the matrix are "missing completely at random" (MCAR), i.e., each
entry is revealed at random, independent of everything else, with uniform
probability. This is likely unrealistic due to the presence of "latent
confounders", i.e., unobserved factors that determine both the entries of the
underlying matrix and the missingness pattern in the observed matrix. For
example, in the context of movie recommender systems -- a canonical application
for matrix completion -- a user who vehemently dislikes horror films is
unlikely to ever watch horror films. In general, these confounders yield
"missing not at random" (MNAR) data, which can severely impact any inference
procedure that does not correct for this bias. We develop a formal causal model
for matrix completion through the language of potential outcomes, and provide
novel identification arguments for a variety of causal estimands of interest.
We design a procedure, which we call "synthetic nearest neighbors" (SNN), to
estimate these causal estimands. We prove finite-sample consistency and
asymptotic normality of our estimator. Our analysis also leads to new
theoretical results for the matrix completion literature. In particular, we
establish entry-wise, i.e., max-norm, finite-sample consistency and asymptotic
normality results for matrix completion with MNAR data. As a special case, this
also provides entry-wise bounds for matrix completion with MCAR data. Across
simulated and real data, we demonstrate the efficacy of our proposed estimator.

    

### [[2109.15159] Focused Contrastive Training for Test-based Constituency Analysis](http://arxiv.org/abs/2109.15159)


  We propose a scheme for self-training of grammaticality models for
constituency analysis based on linguistic tests. A pre-trained language model
is fine-tuned by contrastive estimation of grammatical sentences from a corpus,
and ungrammatical sentences that were perturbed by a syntactic test, a
transformation that is motivated by constituency theory. We show that
consistent gains can be achieved if only certain positive instances are chosen
for training, depending on whether they could be the result of a test
transformation. This way, the positives, and negatives exhibit similar
characteristics, which makes the objective more challenging for the language
model, and also allows for additional markup that indicates the position of the
test application within the sentence.

    

### [[2109.15160] Mitigating Black-Box Adversarial Attacks via Output Noise Perturbation](http://arxiv.org/abs/2109.15160)


  In black-box adversarial attacks, adversaries query the deep neural network
(DNN), use the output to reconstruct gradients, and then optimize the
adversarial inputs iteratively. In this paper, we study the method of adding
white noise to the DNN output to mitigate such attacks, with a unique focus on
the trade-off analysis of noise level and query cost. The attacker's query
count (QC) is derived mathematically as a function of noise standard deviation.
With this result, the defender can conveniently find the noise level needed to
mitigate attacks for the desired security level specified by QC and limited DNN
performance loss. Our analysis shows that the added noise is drastically
magnified by the small variation of DNN outputs, which makes the reconstructed
gradient have an extremely low signal-to-noise ratio (SNR). Adding slight white
noise with a standard deviation less than 0.01 is enough to increase QC by many
orders of magnitude without introducing any noticeable classification accuracy
reduction. Our experiments demonstrate that this method can effectively
mitigate both soft-label and hard-label black-box attacks under realistic QC
constraints. We also show that this method outperforms many other defense
methods and is robust to the attacker's countermeasures.

    

### [[2109.15163] HSVA: Hierarchical Semantic-Visual Adaptation for Zero-Shot Learning](http://arxiv.org/abs/2109.15163)


  Zero-shot learning (ZSL) tackles the unseen class recognition problem,
transferring semantic knowledge from seen classes to unseen ones. Typically, to
guarantee desirable knowledge transfer, a common (latent) space is adopted for
associating the visual and semantic domains in ZSL. However, existing common
space learning methods align the semantic and visual domains by merely
mitigating distribution disagreement through one-step adaptation. This strategy
is usually ineffective due to the heterogeneous nature of the feature
representations in the two domains, which intrinsically contain both
distribution and structure variations. To address this and advance ZSL, we
propose a novel hierarchical semantic-visual adaptation (HSVA) framework.
Specifically, HSVA aligns the semantic and visual domains by adopting a
hierarchical two-step adaptation, i.e., structure adaptation and distribution
adaptation. In the structure adaptation step, we take two task-specific
encoders to encode the source data (visual domain) and the target data
(semantic domain) into a structure-aligned common space. To this end, a
supervised adversarial discrepancy (SAD) module is proposed to adversarially
minimize the discrepancy between the predictions of two task-specific
classifiers, thus making the visual and semantic feature manifolds more closely
aligned. In the distribution adaptation step, we directly minimize the
Wasserstein distance between the latent multivariate Gaussian distributions to
align the visual and semantic distributions using a common encoder. Finally,
the structure and distribution adaptation are derived in a unified framework
under two partially-aligned variational autoencoders. Extensive experiments on
four benchmark datasets demonstrate that HSVA achieves superior performance on
both conventional and generalized ZSL. The code is available at
\url{this https URL} .

    

### [[2109.15200] Semi-tensor Product-based TensorDecomposition for Neural Network Compression](http://arxiv.org/abs/2109.15200)


  The existing tensor networks adopt conventional matrix product for
connection. The classical matrix product requires strict dimensionality
consistency between factors, which can result in redundancy in data
representation. In this paper, the semi-tensor product is used to generalize
classical matrix product-based mode product to semi-tensor mode product. As it
permits the connection of two factors with different dimensionality, more
flexible and compact tensor decompositions can be obtained with smaller sizes
of factors. Tucker decomposition, Tensor Train (TT) and Tensor Ring (TR) are
common decomposition for low rank compression of deep neural networks. The
semi-tensor product is applied to these tensor decompositions to obtained their
generalized versions, i.e., semi-tensor Tucker decomposition (STTu),
semi-tensor train(STT) and semi-tensor ring (STR). Experimental results show
the STTu, STT and STR achieve higher compression factors than the conventional
tensor decompositions with the same accuracy but less training times in ResNet
and WideResNetcompression. With 2% accuracy degradation, the TT-RN (rank = 14)
and the TR-WRN (rank = 16) only obtain 3 times and99t times compression factors
while the STT-RN (rank = 14) and the STR-WRN (rank = 16) achieve 9 times and
179 times compression factors, respectively.

    

### [[2109.15226] Coding for Straggler Mitigation in Federated Learning](http://arxiv.org/abs/2109.15226)


  We present a novel coded federated learning (FL) scheme for linear regression
that mitigates the effect of straggling devices while retaining the privacy
level of conventional FL. The proposed scheme combines one-time padding to
preserve privacy and gradient codes to yield resiliency against stragglers and
consists of two phases. In the first phase, the devices share a one-time padded
version of their local data with a subset of other devices. In the second
phase, the devices and the central server collaboratively and iteratively train
a global linear model using gradient codes on the one-time padded local data.
To apply one-time padding to real data, our scheme exploits a fixed-point
arithmetic representation of the data. Unlike the coded FL scheme recently
introduced by Prakash et al., the proposed scheme maintains the same level of
privacy as conventional FL while achieving a similar training time. Compared to
conventional FL, we show that the proposed scheme achieves a training speed-up
factor of $6.6$ and $9.2$ on the MNIST and Fashion-MNIST datasets for an
accuracy of $95\%$ and $85\%$, respectively.

    

### [[2109.15228] Adapting Bandit Algorithms for Settings with Sequentially Available Arms](http://arxiv.org/abs/2109.15228)


  Although the classical version of the Multi-Armed Bandits (MAB) framework has
been applied successfully to several practical problems, in many real-world
applications, the possible actions are not presented to the learner
simultaneously, such as in the Internet campaign management and environmental
monitoring settings. Instead, in such applications, a set of options is
presented sequentially to the learner within a time span, and this process is
repeated throughout a time horizon. At each time, the learner is asked whether
to select the proposed option or not. We define this scenario as the Sequential
Pull/No-pull Bandit setting, and we propose a meta-algorithm, namely Sequential
Pull/No-pull for MAB (Seq), to adapt any classical MAB policy to better suit
this setting for both the regret minimization and best-arm identification
problems. By allowing the selection of multiple arms within a round, the
proposed meta-algorithm gathers more information, especially in the first
rounds, characterized by a high uncertainty in the arms estimate value. At the
same time, the adapted algorithms provide the same theoretical guarantees as
the classical policy employed. The Seq meta-algorithm was extensively tested
and compared with classical MAB policies on synthetic and real-world datasets
from advertising and environmental monitoring applications, highlighting its
good empirical performances.

    

### [[2109.15233] Real Robot Challenge using Deep Reinforcement Learning](http://arxiv.org/abs/2109.15233)


  This paper details our winning submission to Phase 1 of the 2021 Real Robot
Challenge, a challenge in which a three fingered robot must carry a cube along
specified goal trajectories. To solve Phase 1, we use a pure reinforcement
learning approach which requires minimal expert knowledge of the robotic system
or of robotic grasping in general. A sparse goal-based reward is employed in
conjunction with Hindsight Experience Replay to teach the control policy to
move the cube to the desired x and y coordinates. Simultaneously, a dense
distance-based reward is employed to teach the policy to lift the cube to the
desired z coordinate. The policy is trained in simulation with domain
randomization before being transferred to the real robot for evaluation.
Although performance tends to worsen after this transfer, our best trained
policy can successfully lift the real cube along goal trajectories via the use
of an effective pinching grasp. Our approach outperforms all other submissions,
including those leveraging more traditional robotic control techniques, and is
the first learning-based approach to solve this challenge.

    

### [[2109.15239] Multi Scale Graph Wavenet for Spatio-temporal Wind Speed Forecasting](http://arxiv.org/abs/2109.15239)


  Geometric deep learning has gained tremendous attention in both academia and
industry due to its inherent capability of representing arbitrary structures.
Due to exponential increase in interest towards renewable sources of energy,
especially wind energy, accurate wind speed forecasting has become very
important. . In this paper, we propose a novel deep learning architecture,
Multi Scale Graph Wavenet for wind speed forecasting. It is based on a graph
convolutional neural network and captures both spatial and temporal
relationships in multivariate time series weather data for wind speed
forecasting. We especially took inspiration from dilated convolutions, skip
connections and the inception network to capture temporal relationships and
graph convolutional networks for capturing spatial relationships in the data.
We conducted experiments on real wind speed data measured at different cities
in Denmark and compared our results with the state-of-the-art baseline models.
Our novel architecture outperformed the state-of-the-art methods on wind speed
forecasting for multiple forecast horizons by 4-5%.

    

### [[2109.15256] Inducing Transformer's Compositional Generalization Ability via Auxiliary Sequence Prediction Tasks](http://arxiv.org/abs/2109.15256)


  Systematic compositionality is an essential mechanism in human language,
allowing the recombination of known parts to create novel expressions. However,
existing neural models have been shown to lack this basic ability in learning
symbolic structures. Motivated by the failure of a Transformer model on the
SCAN compositionality challenge (Lake and Baroni, 2018), which requires parsing
a command into actions, we propose two auxiliary sequence prediction tasks that
track the progress of function and argument semantics, as additional training
supervision. These automatically-generated sequences are more representative of
the underlying compositional symbolic structures of the input data. During
inference, the model jointly predicts the next action and the next tokens in
the auxiliary sequences at each step. Experiments on the SCAN dataset show that
our method encourages the Transformer to understand compositional structures of
the command, improving its accuracy on multiple challenging splits from <= 10%
to 100%. With only 418 (5%) training instances, our approach still achieves
97.8% accuracy on the MCD1 split. Therefore, we argue that compositionality can
be induced in Transformers given minimal but proper guidance. We also show that
a better result is achieved using less contextualized vectors as the
attention's query, providing insights into architecture choices in achieving
systematic compositionality. Finally, we show positive generalization results
on the groundedSCAN task (Ruis et al., 2020). Our code is publicly available
at: this https URL


### [[2109.15257] Latent Network Embedding via Adversarial Auto-encoders](http://arxiv.org/abs/2109.15257)


  Graph auto-encoders have proved to be useful in network embedding task.
However, current models only consider explicit structures and fail to explore
the informative latent structures cohered in networks. To address this issue,
we propose a latent network embedding model based on adversarial graph
auto-encoders. Under this framework, the problem of discovering latent
structures is formulated as inferring the latent ties from partial
observations. A latent transmission matrix that describes the strengths of
existing edges and latent ties is derived based on influence cascades sampled
by simulating diffusion processes over networks. Besides, since the inference
process may bring extra noises, we introduce an adversarial training that works
as regularization to dislodge noises and improve the model robustness.
Extensive experiments on link prediction and node classification tasks show
that the proposed model achieves superior results compared with baseline
models.

    

### [[2109.15258] Federated Dropout -- A Simple Approach for Enabling Federated Learning on Resource Constrained Devices](http://arxiv.org/abs/2109.15258)


  Federated learning (FL) is a popular framework for training an AI model using
distributed mobile data in a wireless network. It features data parallelism by
distributing the learning task to multiple edge devices while attempting to
preserve their local-data privacy. One main challenge confronting practical FL
is that resource constrained devices struggle with the computation intensive
task of updating of a deep-neural network model. To tackle the challenge, in
this paper, a federated dropout (FedDrop) scheme is proposed building on the
classic dropout scheme for random model pruning. Specifically, in each
iteration of the FL algorithm, several subnets are independently generated from
the global model at the server using dropout but with heterogeneous dropout
rates (i.e., parameter-pruning probabilities), each of which is adapted to the
state of an assigned channel. The subsets are downloaded to associated devices
for updating. Thereby, FdeDrop reduces both the communication overhead and
devices' computation loads compared with the conventional FL while
outperforming the latter in the case of overfitting and also the FL scheme with
uniform dropout (i.e., identical subsets).

    

### [[2109.15266] Modeling Interactions of Autonomous Vehicles and Pedestrians with Deep Multi-Agent Reinforcement Learning for Collision Avoidance](http://arxiv.org/abs/2109.15266)


  Reliable pedestrian crash avoidance mitigation (PCAM) systems are crucial
components of safe autonomous vehicles (AVs). The sequential nature of the
vehicle-pedestrian interaction, i.e., where immediate decisions of one agent
directly influence the following decisions of the other agent, is an often
neglected but important aspect. In this work, we model the corresponding
interaction sequence as a Markov decision process (MDP) that is solved by deep
reinforcement learning (DRL) algorithms to define the PCAM system's policy. The
simulated driving scenario is based on an AV acting as a DRL agent driving
along an urban street, facing a pedestrian at an unmarked crosswalk who tries
to cross. Since modeling realistic crossing behavior of the pedestrian is
challenging, we introduce two levels of intelligent pedestrian behavior: While
the baseline model follows a predefined strategy, our advanced model captures
continuous learning and the inherent uncertainty in human behavior by defining
the pedestrian as a second DRL agent, i.e., we introduce a deep multi-agent
reinforcement learning (DMARL) problem. The presented PCAM system with
different levels of intelligent pedestrian behavior is benchmarked according to
the agents' collision rate and the resulting traffic flow efficiency. In this
analysis, our focus lies on evaluating the influence of observation noise on
the decision making of the agents. The results show that the AV is able to
completely mitigate collisions under the majority of the investigated
conditions and that the DRL-based pedestrian model indeed learns a more
human-like crossing behavior.

    

### [[2109.15273] DAAS: Differentiable Architecture and Augmentation Policy Search](http://arxiv.org/abs/2109.15273)


  Neural architecture search (NAS) has been an active direction of automatic
machine learning (Auto-ML), aiming to explore efficient network structures. The
searched architecture is evaluated by training on datasets with fixed data
augmentation policies. However, recent works on auto-augmentation show that the
suited augmentation policies can vary over different structures. Therefore,
this work considers the possible coupling between neural architectures and data
augmentation and proposes an effective algorithm jointly searching for them.
Specifically, 1) for the NAS task, we adopt a single-path based differentiable
method with Gumbel-softmax reparameterization strategy due to its memory
efficiency; 2) for the auto-augmentation task, we introduce a novel search
method based on policy gradient algorithm, which can significantly reduce the
computation complexity. Our approach achieves 97.91% accuracy on CIFAR-10 and
76.6% Top-1 accuracy on ImageNet dataset, showing the outstanding performance
of our search algorithm.

    

### [[2109.15283] Bend-Net: Bending Loss Regularized Multitask Learning Network for Nuclei Segmentation in Histopathology Images](http://arxiv.org/abs/2109.15283)


  Separating overlapped nuclei is a major challenge in histopathology image
analysis. Recently published approaches have achieved promising overall
performance on nuclei segmentation; however, their performance on separating
overlapped nuclei is quite limited. To address the issue, we propose a novel
multitask learning network with a bending loss regularizer to separate
overlapped nuclei accurately. The newly proposed multitask learning
architecture enhances the generalization by learning shared representation from
three tasks: instance segmentation, nuclei distance map prediction, and
overlapped nuclei distance map prediction. The proposed bending loss defines
high penalties to concave contour points with large curvatures, and applies
small penalties to convex contour points with small curvatures. Minimizing the
bending loss avoids generating contours that encompass multiple nuclei. In
addition, two new quantitative metrics, Aggregated Jaccard Index of overlapped
nuclei (AJIO) and Accuracy of overlapped nuclei (ACCO), are designed for the
evaluation of overlapped nuclei segmentation. We validate the proposed approach
on the CoNSeP and MoNuSegv1 datasets using seven quantitative metrics:
Aggregate Jaccard Index, Dice, Segmentation Quality, Recognition Quality,
Panoptic Quality, AJIO, and ACCO. Extensive experiments demonstrate that the
proposed Bend-Net outperforms eight state-of-the-art approaches.

    

### [[2109.15284] Which Design Decisions in AI-enabled Mobile Applications Contribute to Greener AI?](http://arxiv.org/abs/2109.15284)


  Background: The construction, evolution and usage of complex artificial
intelligence (AI) models demand expensive computational resources. While
currently available high-performance computing environments support well this
complexity, the deployment of AI models in mobile devices, which is an
increasing trend, is challenging. Mobile applications consist of environments
with low computational resources and hence imply limitations in the design
decisions during the AI-enabled software engineering lifecycle that balance the
trade-off between the accuracy and the complexity of the mobile applications.
Objective: Our objective is to systematically assess the trade-off between
accuracy and complexity when deploying complex AI models (e.g. neural networks)
to mobile devices, which have an implicit resource limitation. We aim to cover
(i) the impact of the design decisions on the achievement of high-accuracy and
low resource-consumption implementations; and (ii) the validation of profiling
tools for systematically promoting greener AI.
Method: This confirmatory registered report consists of a plan to conduct an
empirical study to quantify the implications of the design decisions on
AI-enabled applications performance and to report experiences of the end-to-end
AI-enabled software engineering lifecycle. Concretely, we will implement both
image-based and language-based neural networks in mobile applications to solve
multiple image classification and text classification problems on different
benchmark datasets. Overall, we plan to model the accuracy and complexity of
AI-enabled applications in operation with respect to their design decisions and
will provide tools for allowing practitioners to gain consciousness of the
quantitative relationship between the design decisions and the green
characteristics of study.

    

### [[2109.15292] Accelerating Perturbed Stochastic Iterates in Asynchronous Lock-Free Optimization](http://arxiv.org/abs/2109.15292)


  We show that stochastic acceleration can be achieved under the perturbed
iterate framework (Mania et al., 2017) in asynchronous lock-free optimization,
which leads to the optimal incremental gradient complexity for finite-sum
objectives. We prove that our new accelerated method requires the same linear
speed-up condition as the existing non-accelerated methods. Our core
algorithmic discovery is a new accelerated SVRG variant with sparse updates.
Empirical results are presented to verify our theoretical findings.

    

### [[2109.15310] Width-Based Planning and Active Learning for Atari](http://arxiv.org/abs/2109.15310)


  Width-based planning has shown promising results on Atari 2600 games using
pixel input, while using substantially fewer environment interactions than
reinforcement learning. Recent width-based approaches have computed feature
vectors for each screen using a hand designed feature set or a variational
autoencoder (VAE) trained on game screens, and prune screens that do not have
novel features during the search. In this paper, we explore consideration of
uncertainty in features generated by a VAE during width-based planning. Our
primary contribution is the introduction of active learning to maximize the
utility of screens observed during planning. Experimental results demonstrate
that use of active learning strategies increases gameplay scores compared to
alternative width-based approaches with equal numbers of environment
interactions.

    

### [[2109.15317] Unsupervised Few-Shot Action Recognition via Action-Appearance Aligned Meta-Adaptation](http://arxiv.org/abs/2109.15317)


  We present MetaUVFS as the first Unsupervised Meta-learning algorithm for
Video Few-Shot action recognition. MetaUVFS leverages over 550K unlabeled
videos to train a two-stream 2D and 3D CNN architecture via contrastive
learning to capture the appearance-specific spatial and action-specific
spatio-temporal video features respectively. MetaUVFS comprises a novel
Action-Appearance Aligned Meta-adaptation (A3M) module that learns to focus on
the action-oriented video features in relation to the appearance features via
explicit few-shot episodic meta-learning over unsupervised hard-mined episodes.
Our action-appearance alignment and explicit few-shot learner conditions the
unsupervised training to mimic the downstream few-shot task, enabling MetaUVFS
to significantly outperform all unsupervised methods on few-shot benchmarks.
Moreover, unlike previous few-shot action recognition methods that are
supervised, MetaUVFS needs neither base-class labels nor a supervised
pretrained backbone. Thus, we need to train MetaUVFS just once to perform
competitively or sometimes even outperform state-of-the-art supervised methods
on popular HMDB51, UCF101, and Kinetics100 few-shot datasets.

    

### [[1905.10409] Greedy Shallow Networks: An Approach for Constructing and Training Neural Networks](http://arxiv.org/abs/1905.10409)


  We present a greedy-based approach to construct an efficient single hidden
layer neural network with the ReLU activation that approximates a target
function. In our approach we obtain a shallow network by utilizing a greedy
algorithm with the prescribed dictionary provided by the available training
data and a set of possible inner weights. To facilitate the greedy selection
process we employ an integral representation of the network, based on the
ridgelet transform, that significantly reduces the cardinality of the
dictionary and hence promotes feasibility of the greedy selection. Our approach
allows for the construction of efficient architectures which can be treated
either as improved initializations to be used in place of random-based
alternatives, or as fully-trained networks in certain cases, thus potentially
nullifying the need for backpropagation training. Numerical experiments
demonstrate the tenability of the proposed concept and its advantages compared
to the conventional techniques for selecting architectures and initializations
for neural networks.

    

### [[1910.12799] Deep learning is adaptive to intrinsic dimensionality of model smoothness in anisotropic Besov space](http://arxiv.org/abs/1910.12799)


  Deep learning has exhibited superior performance for various tasks,
especially for high-dimensional datasets, such as images. To understand this
property, we investigate the approximation and estimation ability of deep
learning on anisotropic Besov spaces. The anisotropic Besov space is
characterized by direction-dependent smoothness and includes several function
classes that have been investigated thus far. We demonstrate that the
approximation error and estimation error of deep learning only depend on the
average value of the smoothness parameters in all directions. Consequently, the
curse of dimensionality can be avoided if the smoothness of the target function
is highly anisotropic. Unlike existing studies, our analysis does not require a
low-dimensional structure of the input data. We also investigate the minimax
optimality of deep learning and compare its performance with that of the kernel
method (more generally, linear estimators). The results show that deep learning
has better dependence on the input dimensionality if the target function
possesses anisotropic smoothness, and it achieves an adaptive rate for
functions with spatially inhomogeneous smoothness.

    

### [[2003.12043] From unbiased MDI Feature Importance to Explainable AI for Trees](http://arxiv.org/abs/2003.12043)


  We attempt to give a unifying view of the various recent attempts to (i)
improve the interpretability of tree-based models and (ii) debias the the
default variable-importance measure in random Forests, Gini importance. In
particular, we demonstrate a common thread among the out-of-bag based bias
correction methods and their connection to local explanation for trees. In
addition, we point out a bias caused by the inclusion of inbag data in the
newly developed explainable AI for trees algorithms.

    

### [[2004.01190] Predicting the outputs of finite deep neural networks trained with noisy gradients](http://arxiv.org/abs/2004.01190)


  A recent line of works studied wide deep neural networks (DNNs) by
approximating them as Gaussian Processes (GPs). A DNN trained with gradient
flow was shown to map to a GP governed by the Neural Tangent Kernel (NTK),
whereas earlier works showed that a DNN with an i.i.d. prior over its weights
maps to the so-called Neural Network Gaussian Process (NNGP). Here we consider
a DNN training protocol, involving noise, weight decay and finite width, whose
outcome corresponds to a certain non-Gaussian stochastic process. An analytical
framework is then introduced to analyze this non-Gaussian process, whose
deviation from a GP is controlled by the finite width. Our contribution is
three-fold: (i) In the infinite width limit, we establish a correspondence
between DNNs trained with noisy gradients and the NNGP, not the NTK. (ii) We
provide a general analytical form for the finite width correction (FWC) for
DNNs with arbitrary activation functions and depth and use it to predict the
outputs of empirical finite networks with high accuracy. Analyzing the FWC
behavior as a function of $n$, the training set size, we find that it is
negligible for both the very small $n$ regime, and, surprisingly, for the large
$n$ regime (where the GP error scales as $O(1/n)$). (iii) We flesh out
algebraically how these FWCs can improve the performance of finite
convolutional neural networks (CNNs) relative to their GP counterparts on image
classification tasks.

    

### [[2006.06102] Multi-index Antithetic Stochastic Gradient Algorithm](http://arxiv.org/abs/2006.06102)


  Stochastic Gradient Algorithms (SGAs) are ubiquitous in computational
statistics, machine learning and optimisation. Recent years have brought an
influx of interest in SGAs, and the non-asymptotic analysis of their bias is by
now well-developed. However, relatively little is known about the optimal
choice of the random approximation (e.g mini-batching) of the gradient in SGAs
as this relies on the analysis of the variance and is problem specific. While
there have been numerous attempts to reduce the variance of SGAs, these
typically exploit a particular structure of the sampled distribution by
requiring a priori knowledge of its density's mode. It is thus unclear how to
adapt such algorithms to non-log-concave settings. In this paper, we construct
a Multi-index Antithetic Stochastic Gradient Algorithm (MASGA) whose
implementation is independent of the structure of the target measure and which
achieves performance on par with Monte Carlo estimators that have access to
unbiased samples from the distribution of interest. In other words, MASGA is an
optimal estimator from the mean square error-computational cost perspective
within the class of Monte Carlo estimators. We prove this fact rigorously for
log-concave settings and verify it numerically for some examples where the
log-concavity assumption is not satisfied.

    

### [[2006.14615] LayoutTransformer: Layout Generation and Completion with Self-attention](http://arxiv.org/abs/2006.14615)


  We address the problem of scene layout generation for diverse domains such as
images, mobile applications, documents, and 3D objects. Most complex scenes,
natural or human-designed, can be expressed as a meaningful arrangement of
simpler compositional graphical primitives. Generating a new layout or
extending an existing layout requires understanding the relationships between
these primitives. To do this, we propose LayoutTransformer, a novel framework
that leverages self-attention to learn contextual relationships between layout
elements and generate novel layouts in a given domain. Our framework allows us
to generate a new layout either from an empty set or from an initial seed set
of primitives, and can easily scale to support an arbitrary of primitives per
layout. Furthermore, our analyses show that the model is able to automatically
capture the semantic properties of the primitives. We propose simple
improvements in both representation of layout primitives, as well as training
methods to demonstrate competitive performance in very diverse data domains
such as object bounding boxes in natural images(COCO bounding box), documents
(PubLayNet), mobile applications (RICO dataset) as well as 3D shapes
(Part-Net). Code and other materials will be made available at
this https URL.

    

### [[2007.06823] Hands-on Bayesian Neural Networks -- a Tutorial for Deep Learning Users](http://arxiv.org/abs/2007.06823)


  Modern deep learning methods constitute incredibly powerful tools to tackle a
myriad of challenging problems. However, since deep learning methods operate as
black boxes, the uncertainty associated with their predictions is often
challenging to quantify. Bayesian statistics offer a formalism to understand
and quantify the uncertainty associated with deep neural network predictions.
This tutorial provides an overview of the relevant literature and a complete
toolset to design, implement, train, use and evaluate Bayesian Neural Networks,
i.e. Stochastic Artificial Neural Networks trained using Bayesian methods.

    

### [[2007.07302] Optimal Learning for Structured Bandits](http://arxiv.org/abs/2007.07302)


  We study structured multi-armed bandits, which is the problem of online
decision-making under uncertainty in the presence of structural information. In
this problem, the decision-maker needs to discover the best course of action
despite observing only uncertain rewards over time. The decision-maker is aware
of certain structural information regarding the reward distributions and would
like to minimize their regret by exploiting this information, where the regret
is its performance difference against a benchmark policy that knows the best
action ahead of time. In the absence of structural information, the classical
upper confidence bound (UCB) and Thomson sampling algorithms are well known to
suffer only minimal regret. As recently pointed out, neither algorithms are,
however, capable of exploiting structural information that is commonly
available in practice. We propose a novel learning algorithm that we call DUSA
whose worst-case regret matches the information-theoretic regret lower bound up
to a constant factor and can handle a wide range of structural information. Our
algorithm DUSA solves a dual counterpart of the regret lower bound at the
empirical reward distribution and follows its suggested play. Our proposed
algorithm is the first computationally viable learning policy for structured
bandit problems that has asymptotic minimal regret.

    

### [[2007.07869] Gradient-based Hyperparameter Optimization Over Long Horizons](http://arxiv.org/abs/2007.07869)


  Gradient-based hyperparameter optimization has earned a widespread popularity
in the context of few-shot meta-learning, but remains broadly impractical for
tasks with long horizons (many gradient steps), due to memory scaling and
gradient degradation issues. A common workaround is to learn hyperparameters
online, but this introduces greediness which comes with a significant
performance drop. We propose forward-mode differentiation with sharing (FDS), a
simple and efficient algorithm which tackles memory scaling issues with
forward-mode differentiation, and gradient degradation issues by sharing
hyperparameters that are contiguous in time. We provide theoretical guarantees
about the noise reduction properties of our algorithm, and demonstrate its
efficiency empirically by differentiating through $\sim 10^4$ gradient steps of
unrolled optimization. We consider large hyperparameter search ranges on
CIFAR-10 where we significantly outperform greedy gradient-based alternatives,
while achieving $\times 20$ speedups compared to the state-of-the-art black-box
methods. Code is available at: \url{this https URL}

    

### [[2009.01367] An End-to-End Approach for Training Neural Network Binary Classifiers on Metrics Based on the Confusion Matrix](http://arxiv.org/abs/2009.01367)


  While neural network binary classifiers are often evaluated on metrics such
as Accuracy and $F_1$-Score, they are commonly trained with a cross-entropy
objective. How can this training-testing gap be addressed? While specific
techniques have been adopted to optimize certain confusion matrix based
metrics, it is challenging or impossible in some cases to generalize the
techniques to other metrics. Adversarial learning approaches have also been
proposed to optimize networks via confusion matrix based metrics, but they tend
to be much slower than common training methods. In this work, we propose to
approximate the Heaviside step function, typically used to compute confusion
matrix based metrics, to render these metrics amenable to gradient descent. Our
extensive experiments show the effectiveness of our end-to-end approach for
binary classification in several domains.

    

### [[2009.06125] A Qualitative Study of the Dynamic Behavior for Adaptive Gradient Algorithms](http://arxiv.org/abs/2009.06125)


  The dynamic behavior of RMSprop and Adam algorithms is studied through a
combination of careful numerical experiments and theoretical explanations.
Three types of qualitative features are observed in the training loss curve:
fast initial convergence, oscillations, and large spikes in the late phase. The
sign gradient descent (signGD) flow, which is the limit of Adam when taking the
learning rate to 0 while keeping the momentum parameters fixed, is used to
explain the fast initial convergence. For the late phase of Adam, three
different types of qualitative patterns are observed depending on the choice of
the hyper-parameters: oscillations, spikes, and divergence. In particular, Adam
converges much smoother and even faster when the values of the two momentum
factors are close to each other. This observation is particularly important for
scientific computing tasks, for which the training process usually proceeds
into the high precision regime.

    

### [[2009.06576] Disease control as an optimization problem](http://arxiv.org/abs/2009.06576)


  In the context of epidemiology, policies for disease control are often
devised through a mixture of intuition and brute-force, whereby the set of
logically conceivable policies is narrowed down to a small family described by
a few parameters, following which linearization or grid search is used to
identify the optimal policy within the set. This scheme runs the risk of
leaving out more complex (and perhaps counter-intuitive) policies for disease
control that could tackle the disease more efficiently. In this article, we use
techniques from convex optimization theory and machine learning to conduct
optimizations over disease policies described by hundreds of parameters. In
contrast to past approaches for policy optimization based on control theory,
our framework can deal with arbitrary uncertainties on the initial conditions
and model parameters controlling the spread of the disease, and stochastic
models. In addition, our methods allow for optimization over policies which
remain constant over weekly periods, specified by either continuous or discrete
(e.g.: lockdown on/off) government measures. We illustrate our approach by
minimizing the total time required to eradicate COVID-19 within the
Susceptible-Exposed-Infected-Recovered (SEIR) model proposed by Kissler
\emph{et al.} (March, 2020).

    

### [[2010.11821] Scalable Hierarchical Agglomerative Clustering](http://arxiv.org/abs/2010.11821)


  The applicability of agglomerative clustering, for inferring both
hierarchical and flat clustering, is limited by its scalability. Existing
scalable hierarchical clustering methods sacrifice quality for speed and often
lead to over-merging of clusters. In this paper, we present a scalable,
agglomerative method for hierarchical clustering that does not sacrifice
quality and scales to billions of data points. We perform a detailed
theoretical analysis, showing that under mild separability conditions our
algorithm can not only recover the optimal flat partition, but also provide a
two-approximation to non-parametric DP-Means objective. This introduces a novel
application of hierarchical clustering as an approximation algorithm for the
non-parametric clustering objective. We additionally relate our algorithm to
the classic hierarchical agglomerative clustering method. We perform extensive
empirical experiments in both hierarchical and flat clustering settings and
show that our proposed approach achieves state-of-the-art results on publicly
available clustering benchmarks. Finally, we demonstrate our method's
scalability by applying it to a dataset of 30 billion queries. Human evaluation
of the discovered clusters show that our method finds better quality of
clusters than the current state-of-the-art.

    

### [[2011.03384] Suppression of Independent and Correlated Noise with Similarity-based Unsupervised Deep Learning](http://arxiv.org/abs/2011.03384)


  Denoising is one of the most important data processing tasks and is generally
a prerequisite for downstream image analysis in many fields. Despite their
superior denoising performance, supervised deep denoising methods require
paired noise-clean or noise-noise samples often unavailable in practice. On the
other hand, unsupervised deep denoising methods such as Noise2Void and its
variants predict masked pixels from their neighboring pixels in single noisy
images. However, these unsupervised algorithms only work under the independent
noise assumption while real noise distributions are usually correlated with
complex structural patterns. Here we propose the first-of-its-kind feature
similarity-based unsupervised denoising approach that works in a nonlocal and
nonlinear fashion to suppress not only independent but also correlated noise.
Our approach is referred to as Noise2Sim since different noisy sub-images with
similar signals are extracted to form as many as possible training pairs so
that the parameters of a deep denoising network can be optimized in a
self-learning fashion. Theoretically, the theorem is established that Noise2Sim
is equivalent to the supervised learning methods under mild conditions.
Experimentally, Noise2Sim achieves excellent results on natural, microscopic,
low-dose CT and photon-counting micro-CT images, removing image noise
independent or not and being superior to the competitive denoising methods.
Potentially, Noise2Sim would open a new direction of research and lead to the
development of adaptive denoising tools in diverse applications.

    

### [[2012.00628] Convergence of Gradient Algorithms for Nonconvex C^{1+alpha} Cost Functions](http://arxiv.org/abs/2012.00628)


  This paper is concerned with convergence of stochastic gradient algorithms
with momentum terms in the nonconvex setting. A class of stochastic momentum
methods, including stochastic gradient descent, heavy ball, and Nesterov's
accelerated gradient, is analyzed in a general framework under mild
assumptions. Based on the convergence result of expected gradients, we prove
the almost sure convergence by a detailed discussion of the effects of momentum
and the number of upcrossings. It is worth noting that there are not additional
restrictions imposed on the objective function and stepsize. Another
improvement over previous results is that the existing Lipschitz condition of
the gradient is relaxed into the condition of Holder continuity. As a
byproduct, we apply a localization procedure to extend our results to
stochastic stepsizes.

    

### [[2012.01345] Cross-Modal Retrieval and Synthesis (X-MRS): Closing the Modality Gap in Shared Representation Learning](http://arxiv.org/abs/2012.01345)


  Computational food analysis (CFA) naturally requires multi-modal evidence of
a particular food, e.g., images, recipe text, etc. A key to making CFA possible
is multi-modal shared representation learning, which aims to create a joint
representation of the multiple views (text and image) of the data. In this work
we propose a method for food domain cross-modal shared representation learning
that preserves the vast semantic richness present in the food data. Our
proposed method employs an effective transformer-based multilingual recipe
encoder coupled with a traditional image embedding architecture. Here, we
propose the use of imperfect multilingual translations to effectively
regularize the model while at the same time adding functionality across
multiple languages and alphabets. Experimental analysis on the public Recipe1M
dataset shows that the representation learned via the proposed method
significantly outperforms the current state-of-the-arts (SOTA) on retrieval
tasks. Furthermore, the representational power of the learned representation is
demonstrated through a generative food image synthesis model conditioned on
recipe embeddings. Synthesized images can effectively reproduce the visual
appearance of paired samples, indicating that the learned representation
captures the joint semantics of both the textual recipe and its visual content,
thus narrowing the modality gap.

    

### [[2012.15006] Dynamic Graph-Based Anomaly Detection in the Electrical Grid](http://arxiv.org/abs/2012.15006)


  Given sensor readings over time from a power grid, how can we accurately
detect when an anomaly occurs? A key part of achieving this goal is to use the
network of power grid sensors to quickly detect, in real-time, when any unusual
events, whether natural faults or malicious, occur on the power grid. Existing
bad-data detectors in the industry lack the sophistication to robustly detect
broad types of anomalies, especially those due to emerging cyber-attacks, since
they operate on a single measurement snapshot of the grid at a time. New ML
methods are more widely applicable, but generally do not consider the impact of
topology change on sensor measurements and thus cannot accommodate regular
topology adjustments in historical data. Hence, we propose DYNWATCH, a domain
knowledge based and topology-aware algorithm for anomaly detection using
sensors placed on a dynamic grid. Our approach is accurate, outperforming
existing approaches by 20% or more (F-measure) in experiments; and fast,
running in less than 1.7ms on average per time tick per sensor on a 60K+ branch
case using a laptop computer, and scaling linearly in the size of the graph.

    

### [[2102.04353] Unlocking Pixels for Reinforcement Learning via Implicit Attention](http://arxiv.org/abs/2102.04353)


  There has recently been significant interest in training reinforcement
learning (RL) agents in vision-based environments. This poses many challenges,
such as high dimensionality and the potential for observational overfitting
through spurious correlations. A promising approach to solve both of these
problems is an attention bottleneck, which provides a simple and effective
framework for learning high performing policies, even in the presence of
distractions. However, due to poor scalability of attention architectures,
these methods cannot be applied beyond low resolution visual inputs, using
large patches (thus small attention matrices). In this paper we make use of new
efficient attention algorithms, recently shown to be highly effective for
Transformers, and demonstrate that these techniques can be successfully adopted
for the RL setting. This allows our attention-based controllers to scale to
larger visual inputs, and facilitate the use of smaller patches, even
individual pixels, improving generalization. We show this on a range of tasks
from the Distracting Control Suite to vision-based quadruped robots locomotion.
We provide rigorous theoretical analysis of the proposed algorithm.

    

### [[2102.06024] Feature Selection for Multivariate Time Series via Network Pruning](http://arxiv.org/abs/2102.06024)


  In recent years, there has been an ever increasing amount of multivariate
time series (MTS) data in various domains, typically generated by a large
family of sensors such as wearable devices. This has led to the development of
novel learning methods on MTS data, with deep learning models dominating the
most recent advancements. Prior literature has primarily focused on designing
new network architectures for modeling temporal dependencies within MTS.
However, a less studied challenge is associated with high dimensionality of MTS
data. In this paper, we propose a novel neural component, namely Neural Feature
Se-lector (NFS), as an end-2-end solution for feature selection in MTS data.
Specifically, NFS is based on decomposed convolution design and includes two
modules: firstly each feature stream within MTS is processed by a temporal CNN
independently; then an aggregating CNN combines the processed streams to
produce input for other downstream networks. We evaluated the proposed NFS
model on four real-world MTS datasets and found that it achieves comparable
results with state-of-the-art methods while providing the benefit of feature
selection. Our paper also highlights the robustness and effectiveness of
feature selection with NFS compared to using recent autoencoder-based methods.

    

### [[2102.07631] Accelerating COVID-19 research with graph mining and transformer-based learning](http://arxiv.org/abs/2102.07631)


  In 2020, the White House released the, "Call to Action to the Tech Community
on New Machine Readable COVID-19 Dataset," wherein artificial intelligence
experts are asked to collect data and develop text mining techniques that can
help the science community answer high-priority scientific questions related to
COVID-19. The Allen Institute for AI and collaborators announced the
availability of a rapidly growing open dataset of publications, the COVID-19
Open Research Dataset (CORD-19). As the pace of research accelerates,
biomedical scientists struggle to stay current. To expedite their
investigations, scientists leverage hypothesis generation systems, which can
automatically inspect published papers to discover novel implicit connections.
We present an automated general purpose hypothesis generation systems AGATHA-C
and AGATHA-GP for COVID-19 research. The systems are based on graph-mining and
the transformer model. The systems are massively validated using retrospective
information rediscovery and proactive analysis involving human-in-the-loop
expert analysis. Both systems achieve high-quality predictions across domains
(in some domains up to 0.97% ROC AUC) in fast computational time and are
released to the broad scientific community to accelerate biomedical research.
In addition, by performing the domain expert curated study, we show that the
systems are able to discover on-going research findings such as the
relationship between COVID-19 and oxytocin hormone.

    

### [[2103.04918] A Survey of Embodied AI: From Simulators to Research Tasks](http://arxiv.org/abs/2103.04918)


  There has been an emerging paradigm shift from the era of "internet AI" to
"embodied AI", where AI algorithms and agents no longer learn from datasets of
images, videos or text curated primarily from the internet. Instead, they learn
through interactions with their environments from an egocentric perception
similar to humans. Consequently, there has been substantial growth in the
demand for embodied AI simulators to support various embodied AI research
tasks. This growing interest in embodied AI is beneficial to the greater
pursuit of Artificial General Intelligence (AGI), but there has not been a
contemporary and comprehensive survey of this field. This paper aims to provide
an encyclopedic survey for the field of embodied AI, from its simulators to its
research. By evaluating nine current embodied AI simulators with our proposed
seven features, this paper aims to understand the simulators in their provision
for use in embodied AI research and their limitations. Lastly, this paper
surveys the three main research tasks in embodied AI -- visual exploration,
visual navigation and embodied question answering (QA), covering the
state-of-the-art approaches, evaluation metrics and datasets. Finally, with the
new insights revealed through surveying the field, the paper will provide
suggestions for simulator-for-task selections and recommendations for the
future directions of the field.

    

### [[2103.08312] Trainless Model Performance Estimation for Neural Architecture Search](http://arxiv.org/abs/2103.08312)


  Neural architecture search has become an indispensable part of the deep
learning field. Modern methods allow to find one of the best performing
architectures, or to build one from scratch, but they typically make decisions
based on the trained accuracy information. In the present article we explore
instead how the architectural component of a neural network affects its
prediction power. We focus on relationships between the trained accuracy of an
architecture and its accuracy prior to training, by considering statistics over
multiple initialisations. We observe that minimising the coefficient of
variation of the untrained accuracy, $CV_{U}$, consistently leads to better
performing architectures. We test the $CV_{U}$ as a neural architecture search
scoring metric using the NAS-Bench-201 database of trained neural
architectures. The architectures with the lowest $CV_{U}$ value have on average
an accuracy of $91.90 \pm 2.27$, $64.08 \pm 5.63$ and $38.76 \pm 6.62$ for
CIFAR-10, CIFAR-100 and a downscaled version of ImageNet, respectively. Since
these values are statistically above the random baseline, we make a conclusion
that a good architecture should be stable against weights initialisations. It
takes about $190$ s for CIFAR-10 and CIFAR-100 and $133.9$ s for ImageNet16-120
to process $100$ architectures, on a batch of $256$ images, with $100$
initialisations.

    

### [[2103.11395] ScanMix: Learning from Severe Label Noise via Semantic Clustering and Semi-Supervised Learning](http://arxiv.org/abs/2103.11395)


  In this paper, we address the problem of training deep neural networks in the
presence of severe label noise. Our proposed training algorithm ScanMix,
combines semantic clustering with semi-supervised learning (SSL) to improve the
feature representations and enable an accurate identification of noisy samples,
even in severe label noise scenarios. To be specific, ScanMix is designed based
on the expectation maximisation (EM) framework, where the E-step estimates the
value of a latent variable to cluster the training images based on their
appearance representations and classification results, and the M-step optimises
the SSL classification and learns effective feature representations via
semantic clustering. In our evaluations, we show state-of-the-art results on
standard benchmarks for symmetric, asymmetric and semantic label noise on
CIFAR-10 and CIFAR-100, as well as large scale real label noise on WebVision.
Most notably, for the benchmarks contaminated with large noise rates (80% and
above), our results are up to 27% better than the related work. The code is
available at this https URL.

    

### [[2103.14718] Increasing the Efficiency of Policy Learning for Autonomous Vehicles by Multi-Task Representation Learning](http://arxiv.org/abs/2103.14718)


  Driving in a dynamic, multi-agent, and complex urban environment is a
difficult task requiring a complex decision-making policy. The learning of such
a policy requires a state representation that can encode the entire
environment. Mid-level representations that encode a vehicle's environment as
images have become a popular choice. Still, they are quite high-dimensional,
limiting their use in data-hungry approaches such as reinforcement learning. In
this article, we propose to learn a low-dimensional and rich latent
representation of the environment by leveraging the knowledge of relevant
semantic factors. To do this, we train an encoder-decoder deep neural network
to predict multiple application-relevant factors such as the trajectories of
other agents and the ego car. Furthermore, we propose a hazard signal based on
other vehicles' future trajectories and the planned route which is used in
conjunction with the learned latent representation as input to a down-stream
policy. We demonstrate that using the multi-head encoder-decoder neural network
results in a more informative representation than a standard single-head model.
In particular, the proposed representation learning and the hazard signal help
reinforcement learning to learn faster, with increased performance and less
data than baseline methods.

    

### [[2103.17106] Linear systems with neural network nonlinearities: Improved stability analysis via acausal Zames-Falb multipliers](http://arxiv.org/abs/2103.17106)


  In this paper, we analyze the stability of feedback interconnections of a
linear time-invariant system with a neural network nonlinearity in discrete
time. Our analysis is based on abstracting neural networks using integral
quadratic constraints (IQCs), exploiting the sector-bounded and
slope-restricted structure of the underlying activation functions. In contrast
to existing approaches, we leverage the full potential of dynamic IQCs to
describe the nonlinear activation functions in a less conservative fashion. To
be precise, we consider multipliers based on the full-block Yakubovich / circle
criterion in combination with acausal Zames-Falb multipliers, leading to linear
matrix inequality based stability certificates. Our approach provides a
flexible and versatile framework for stability analysis of feedback
interconnections with neural network nonlinearities, allowing to trade off
computational efficiency and conservatism. Finally, we provide numerical
examples that demonstrate the applicability of the proposed framework and the
achievable improvements over previous approaches.

    

### [[2104.02334] Robust Adversarial Classification via Abstaining](http://arxiv.org/abs/2104.02334)


  In this work, we consider a binary classification problem and cast it into a
binary hypothesis testing framework, where the observations can be perturbed by
an adversary. To improve the adversarial robustness of a classifier, we include
an abstain option, where the classifier abstains from making a decision when it
has low confidence about the prediction. We propose metrics to quantify the
nominal performance of a classifier with an abstain option and its robustness
against adversarial perturbations. We show that there exist a tradeoff between
the two metrics regardless of what method is used to choose the abstain region.
Our results imply that the robustness of a classifier with an abstain option
can only be improved at the expense of its nominal performance. Further, we
provide necessary conditions to design the abstain region for a 1- dimensional
binary classification problem. We validate our theoretical results on the MNIST
dataset, where we numerically show that the tradeoff between performance and
robustness also exist for the general multi-class classification problems.

    

### [[2104.05807] Does My Representation Capture X? Probe-Ably](http://arxiv.org/abs/2104.05807)


  Probing (or diagnostic classification) has become a popular strategy for
investigating whether a given set of intermediate features is present in the
representations of neural models. Probing studies may have misleading results,
but various recent works have suggested more reliable methodologies that
compensate for the possible pitfalls of probing. However, these best practices
are numerous and fast-evolving. To simplify the process of running a set of
probing experiments in line with suggested methodologies, we introduce
Probe-Ably: an extendable probing framework which supports and automates the
application of probing methods to the user's inputs.

    

### [[2104.09452] Epsilon Consistent Mixup: Structural Regularization with an Adaptive Consistency-Interpolation Tradeoff](http://arxiv.org/abs/2104.09452)


  In this paper we propose $\epsilon$-Consistent Mixup ($\epsilon$mu).
$\epsilon$mu is a data-based structural regularization technique that combines
Mixup's linear interpolation with consistency regularization in the Mixup
direction, by compelling a simple adaptive tradeoff between the two. This
learnable combination of consistency and interpolation induces a more flexible
structure on the evolution of the response across the feature space and is
shown to improve semi-supervised classification accuracy on the SVHN and
CIFAR10 benchmark datasets, yielding the largest gains in the most challenging
low label-availability scenarios. Empirical studies comparing $\epsilon$mu and
Mixup are presented and provide insight into the mechanisms behind
$\epsilon$mu's effectiveness. In particular, $\epsilon$mu is found to produce
more accurate synthetic labels and more confident predictions than Mixup.

    

### [[2104.13840] Twins: Revisiting the Design of Spatial Attention in Vision Transformers](http://arxiv.org/abs/2104.13840)


  Very recently, a variety of vision transformer architectures for dense
prediction tasks have been proposed and they show that the design of spatial
attention is critical to their success in these tasks. In this work, we revisit
the design of the spatial attention and demonstrate that a carefully-devised
yet simple spatial attention mechanism performs favourably against the
state-of-the-art schemes. As a result, we propose two vision transformer
architectures, namely, Twins-PCPVT and Twins-SVT. Our proposed architectures
are highly-efficient and easy to implement, only involving matrix
multiplications that are highly optimized in modern deep learning frameworks.
More importantly, the proposed architectures achieve excellent performance on a
wide range of visual tasks, including image level classification as well as
dense detection and segmentation. The simplicity and strong performance suggest
that our proposed architectures may serve as stronger backbones for many vision
tasks. Our code is released at this https URL .

    

### [[2104.15109] Memory-Efficient Deep Learning Inference in Trusted Execution Environments](http://arxiv.org/abs/2104.15109)


  This study identifies and proposes techniques to alleviate two key
bottlenecks to executing deep neural networks in trusted execution environments
(TEEs): page thrashing during the execution of convolutional layers and the
decryption of large weight matrices in fully-connected layers. For the former,
we propose a novel partitioning scheme, y-plane partitioning, designed to (i)
provide consistent execution time when the layer output is large compared to
the TEE secure memory; and (ii) significantly reduce the memory footprint of
convolutional layers. For the latter, we leverage quantization and compression.
In our evaluation, the proposed optimizations incurred latency overheads
ranging from 1.09X to 2X baseline for a wide range of TEE sizes; in contrast,
an unmodified implementation incurred latencies of up to 26X when running
inside of the TEE.

    

### [[2106.00489] Extended Tactile Perception: Vibration Sensing through Tools and Grasped Objects](http://arxiv.org/abs/2106.00489)


  Humans display the remarkable ability to sense the world through tools and
other held objects. For example, we are able to pinpoint impact locations on a
held rod and tell apart different textures using a rigid probe. In this work,
we consider how we can enable robots to have a similar capacity, i.e., to
embody tools and extend perception using standard grasped objects. We propose
that vibro-tactile sensing using dynamic tactile sensors on the robot fingers,
along with machine learning models, enables robots to decipher contact
information that is transmitted as vibrations along rigid objects. This paper
reports on extensive experiments using the BioTac micro-vibration sensor and a
new event dynamic sensor, the NUSkin, capable of multi-taxel sensing at 4~kHz.
We demonstrate that fine localization on a held rod is possible using our
approach (with errors less than 1 cm on a 20 cm rod). Next, we show that
vibro-tactile perception can lead to reasonable grasp stability prediction
during object handover, and accurate food identification using a standard fork.
We find that multi-taxel vibro-tactile sensing at sufficiently high sampling
rate led to the best performance across the various tasks and objects. Taken
together, our results provides both evidence and guidelines for using
vibro-tactile perception to extend tactile perception, which we believe will
lead to enhanced competency with tools and better physical
human-robot-interaction.

    

### [[2106.02808] A Variational Perspective on Diffusion-Based Generative Models and Score Matching](http://arxiv.org/abs/2106.02808)


  Discrete-time diffusion-based generative models and score matching methods
have shown promising results in modeling high-dimensional image data. Recently,
Song et al. (2021) show that diffusion processes that transform data into noise
can be reversed via learning the score function, i.e. the gradient of the
log-density of the perturbed data. They propose to plug the learned score
function into an inverse formula to define a generative diffusion process.
Despite the empirical success, a theoretical underpinning of this procedure is
still lacking. In this work, we approach the (continuous-time) generative
diffusion directly and derive a variational framework for likelihood
estimation, which includes continuous-time normalizing flows as a special case,
and can be seen as an infinitely deep variational autoencoder. Under this
framework, we show that minimizing the score-matching loss is equivalent to
maximizing a lower bound of the likelihood of the plug-in reverse SDE proposed
by Song et al. (2021), bridging the theoretical gap.

    

### [[2109.14652] Seeds of SEED: A Side-Channel Resilient Cache Skewed by a Linear Function over a Galois Field](http://arxiv.org/abs/2109.14652)


  Consider a set-associative cache with $p^n$ sets and $p^n$ ways where $p$ is
prime and $n>0$. Furthermore, assume that the cache may be shared among $p^n$
mutually distrusting principals that may use the Prime+Probe side-channel
attack against one another; architecturally, these principals occupy separate
security domains (for example, separate processes, virtual machines, sandboxes,
etc.). This paper shows that there exists a linear skewing of cache sets over
the Galois field $G_{p^n}$ that exhibits the following property: each cache set
of each security domain intersects every cache set of every other security
domain exactly once. Therefore, a random eviction from a single cache set in
security domain $A$ may be observed via Prime+Probe in any of security domain
$B$'s cache sets. This paper characterizes this linear skewing and describes
how it can be implemented efficiently in hardware.

    

### [[2109.14704] Accelerating Encrypted Computing on Intel GPUs](http://arxiv.org/abs/2109.14704)


  Homomorphic Encryption (HE) is an emerging encryption scheme that allows
computations to be performed directly on encrypted messages. This property
provides promising applications such as privacy-preserving deep learning and
cloud computing. Prior works have been proposed to enable practical
privacy-preserving applications with architectural-aware optimizations on CPUs,
GPUs and FPGAs. However, there is no systematic optimization for the whole HE
pipeline on Intel GPUs. In this paper, we present the first-ever SYCL-based GPU
backend for Microsoft SEAL APIs. We perform optimizations from instruction
level, algorithmic level and application level to accelerate our HE library
based on the Cheon, Kim, Kimand Song (CKKS) scheme on Intel GPUs. The
performance is validated on two latest Intel GPUs. Experimental results show
that our staged optimizations together with optimizations including low-level
optimizations and kernel fusion accelerate the Number Theoretic Transform
(NTT), a key algorithm for HE, by up to 9.93X compared with the naïve GPU
baseline. The roofline analysis confirms that our optimized NTT reaches 79.8%
and85.7% of the peak performance on two GPU devices. Through the highly
optimized NTT and the assembly-level optimization, we obtain 2.32X - 3.05X
acceleration for HE evaluation routines. In addition, our all-together
systematic optimizations improve the performance of encrypted element-wise
polynomial matrix multiplication application by up to 3.10X.

    

### [[2109.14878] Accelerating Fully Connected Neural Network on Optical Network-on-Chip (ONoC)](http://arxiv.org/abs/2109.14878)


  Fully Connected Neural Network (FCNN) is a class of Artificial Neural
Networks widely used in computer science and engineering, whereas the training
process can take a long time with large datasets in existing many-core systems.
Optical Network-on-Chip (ONoC), an emerging chip-scale optical interconnection
technology, has great potential to accelerate the training of FCNN with low
transmission delay, low power consumption, and high throughput. However,
existing methods based on Electrical Network-on-Chip (ENoC) cannot fit in ONoC
because of the unique properties of ONoC. In this paper, we propose a
fine-grained parallel computing model for accelerating FCNN training on ONoC
and derive the optimal number of cores for each execution stage with the
objective of minimizing the total amount of time to complete one epoch of FCNN
training. To allocate the optimal number of cores for each execution stage, we
present three mapping strategies and compare their advantages and disadvantages
in terms of hotspot level, memory requirement, and state transitions.
Simulation results show that the average prediction error for the optimal
number of cores in NN benchmarks is within 2.3%. We further carry out extensive
simulations which demonstrate that FCNN training time can be reduced by 22.28%
and 4.91% on average using our proposed scheme, compared with traditional
parallel computing methods that either allocate a fixed number of cores or
allocate as many cores as possible, respectively. Compared with ENoC,
simulation results show that under batch sizes of 64 and 128, on average ONoC
can achieve 21.02% and 12.95% on reducing training time with 47.85% and 39.27%
on saving energy, respectively.

    

### [[1512.09314] Asynchronous Exclusive Selection](http://arxiv.org/abs/1512.09314)


  We consider the task of assigning unique integers to a group of processes in
an asynchronous distributed system of a total of $n$ processes prone to crashes
that communicate through shared read-write registers. In the Renaming problem,
an arbitrary group of $k\le n$ processes that hold the original names from a
range $[N]=\{1,\ldots,N\}$, contend to acquire unique integers in a smaller
range $[M]$ as new names using some $r$ auxiliary shared registers. We give
number of wait-free renaming algorithms, in particular an adaptive one having
$M=8k-\lg k-1$ as a bound on the range of new names that operates in $O(k)$
local steps and uses $r=O(n^2)$ registers. As a lower bound, we show that a
wait-free solution to Renaming requires $1+\min\{k-2,\lfloor\log_{2r}
\frac{N}{M+k-1}\rfloor\}$ steps in the worst case. We apply renaming algorithms
to obtain solutions to Store&Collect problem, which is about a group of $k\le
n$ processes with the original names in a range $[N]$ proposing individual
values (operation Store) and returning a view of all proposed values (operation
Collect), while using some $r$ auxiliary shared read-write registers. We
consider a problem Mining-Names, in which processes may repeatedly request
positive integers as new names subject to the constraints that no integer can
be assigned to different processes and the number of integers never acquired as
names is finite in an infinite execution. We give two solutions to Mining-Names
in a distributed system in which there are infinitely many shared read-write
registers available. A non-blocking solution leaves at most $2n-2$ nonnegative
integers never assigned as names, and a wait-free algorithm leaves at most
$(n+2)(n-1)$ nonnegative integers never assigned as names.

    

### [[2011.00715] Toward Performance-Portable PETSc for GPU-based Exascale Systems](http://arxiv.org/abs/2011.00715)


  The Portable Extensible Toolkit for Scientific computation (PETSc) library
delivers scalable solvers for nonlinear time-dependent differential and
algebraic equations and for numerical optimization.The PETSc design for
performance portability addresses fundamental GPU accelerator challenges and
stresses flexibility and extensibility by separating the programming model used
by the application from that used by the library, and it enables application
developers to use their preferred programming model, such as Kokkos, RAJA,
SYCL, HIP, CUDA, or OpenCL, on upcoming exascale systems. A blueprint for using
GPUs from PETSc-based codes is provided, and case studies emphasize the
flexibility and high performance achieved on current GPU-based systems.

    

### [[2109.14695] Subdimensional Expansion Using Attention-Based Learning For Multi-Agent Path Finding](http://arxiv.org/abs/2109.14695)


  Multi-Agent Path Finding (MAPF) finds conflict-free paths for multiple agents
from their respective start to goal locations. MAPF is challenging as the joint
configuration space grows exponentially with respect to the number of agents.
Among MAPF planners, search-based methods, such as CBS and M*, effectively
bypass the curse of dimensionality by employing a dynamically-coupled strategy:
agents are planned in a fully decoupled manner at first, where potential
conflicts between agents are ignored; and then agents either follow their
individual plans or are coupled together for planning to resolve the conflicts
between them. In general, the number of conflicts to be resolved decides the
run time of these planners and most of the existing work focuses on how to
efficiently resolve these conflicts. In this work, we take a different view and
aim to reduce the number of conflicts (and thus improve the overall search
efficiency) by improving each agent's individual plan. By leveraging a Visual
Transformer, we develop a learning-based single-agent planner, which plans for
a single agent while paying attention to both the structure of the map and
other agents with whom conflicts may happen. We then develop a novel
multi-agent planner called LM* by integrating this learning-based single-agent
planner with M*. Our results show that for both "seen" and "unseen" maps, in
comparison with M*, LM* has fewer conflicts to be resolved and thus, runs
faster and enjoys higher success rates. We empirically show that MAPF solutions
computed by LM* are near-optimal. Our code is available at
this https URL .

    

### [[2109.14728] Collaborative Storytelling with Human Actors and AI Narrators](http://arxiv.org/abs/2109.14728)


  Large language models can be used for collaborative storytelling. In this
work we report on using GPT-3 \cite{brown2020language} to co-narrate stories.
The AI system must track plot progression and character arcs while the human
actors perform scenes. This event report details how a novel conversational
agent was employed as creative partner with a team of professional improvisers
to explore long-form spontaneous story narration in front of a live public
audience. We introduced novel constraints on our language model to produce
longer narrative text and tested the model in rehearsals with a team of
professional improvisers. We then field tested the model with two live
performances for public audiences as part of a live theatre festival in Europe.
We surveyed audience members after each performance as well as performers to
evaluate how well the AI performed in its role as narrator. Audiences and
performers responded positively to AI narration and indicated preference for AI
narration over AI characters within a scene. Performers also responded
positively to AI narration and expressed enthusiasm for the creative and
meaningful novel narrative directions introduced to the scenes. Our findings
support improvisational theatre as a useful test-bed to explore how different
language models can collaborate with humans in a variety of social contexts.

    

### [[2109.14732] The MatrixX Solver For Argumentation Frameworks](http://arxiv.org/abs/2109.14732)


  MatrixX is a solver for Abstract Argumentation Frameworks. Offensive and
defensive properties of an Argumentation Framework are notated in a matrix
style. Rows and columns of this matrix are systematically reduced by the
solver. This procedure is implemented through the use of hash maps in order to
accelerate calculation time. MatrixX works for stable and complete semantics
and was designed for the ICCMA 2021 competition.

    

### [[2109.14746] Improvising the Learning of Neural Networks on Hyperspherical Manifold](http://arxiv.org/abs/2109.14746)


  The impact of convolution neural networks (CNNs) in the supervised settings
provided tremendous increment in performance. The representations learned from
CNN's operated on hyperspherical manifold led to insightful outcomes in face
recognition, face identification and other supervised tasks. A broad range of
activation functions is developed with hypersphere intuition which performs
superior to softmax in euclidean space. The main motive of this research is to
provide insights. First, the stereographic projection is implied to transform
data from Euclidean space ($\mathbb{R}^{n}$) to hyperspherical manifold
($\mathbb{S}^{n}$) to analyze the performance of angular margin losses.
Secondly, proving both theoretically and practically that decision boundaries
constructed on hypersphere using stereographic projection obliges the learning
of neural networks. Experiments have proved that applying stereographic
projection on existing state-of-the-art angular margin objective functions led
to improve performance for standard image classification data sets
(CIFAR-10,100). The code is publicly available at:
this https URL.

    

### [[2109.14797] Emergency Vehicles Audio Detection and Localization in AutonomousDriving](http://arxiv.org/abs/2109.14797)


  Emergency vehicles in service have right-of-way over all other vehicles.
Hence, all other vehicles are supposed to take proper actions to yield
emergency vehicles with active sirens. As this task requires the cooperation
between ears and eyes for human drivers, it also needs audio detection as a
supplement to vision-based algorithms for fully autonomous driving vehicles. In
urban driving scenarios, we need to know both the existence of emergency
vehicles and their relative positions to us to decide the proper actions. We
present a novel system from collecting the real-world siren data to the
deployment of models using only two cost-efficient microphones. We are able to
achieve promising performance for each task separately, especially within the
crucial 10m to 50m distance range to react (the size of our ego vehicle is
around 5m in length and 2m in width). The recall rate to determine the
existence of sirens is 99.16% , the median and mean angle absolute error is
9.64° and 19.18° respectively, and the median and mean distance
absolute error of 9.30m and 10.58m respectively within that range. We also
benchmark various machine learning approaches that can determine the siren
existence and sound source localization which includes direction and distance
simultaneously within 50ms of latency.

    

### [[2109.14857] First to Possess His Statistics: Data-Free Model Extraction Attack on Tabular Data](http://arxiv.org/abs/2109.14857)


  Model extraction attacks are a kind of attacks where an adversary obtains a
machine learning model whose performance is comparable with one of the victim
model through queries and their results. This paper presents a novel model
extraction attack, named TEMPEST, applicable on tabular data under a practical
data-free setting. Whereas model extraction is more challenging on tabular data
due to normalization, TEMPEST no longer needs initial samples that previous
attacks require; instead, it makes use of publicly available statistics to
generate query samples. Experiments show that our attack can achieve the same
level of performance as the previous attacks. Moreover, we identify that the
use of mean and variance as statistics for query generation and the use of the
same normalization process as the victim model can improve the performance of
our attack. We also discuss a possibility whereby TEMPEST is executed in the
real world through an experiment with a medical diagnosis dataset. We plan to
release the source code for reproducibility and a reference to subsequent
works.

    

### [[2109.14979] Moving Object Detection for Event-based vision using Graph Spectral Clustering](http://arxiv.org/abs/2109.14979)


  Moving object detection has been a central topic of discussion in computer
vision for its wide range of applications like in self-driving cars, video
surveillance, security, and enforcement. Neuromorphic Vision Sensors (NVS) are
bio-inspired sensors that mimic the working of the human eye. Unlike
conventional frame-based cameras, these sensors capture a stream of
asynchronous 'events' that pose multiple advantages over the former, like high
dynamic range, low latency, low power consumption, and reduced motion blur.
However, these advantages come at a high cost, as the event camera data
typically contains more noise and has low resolution. Moreover, as event-based
cameras can only capture the relative changes in brightness of a scene, event
data do not contain usual visual information (like texture and color) as
available in video data from normal cameras. So, moving object detection in
event-based cameras becomes an extremely challenging task. In this paper, we
present an unsupervised Graph Spectral Clustering technique for Moving Object
Detection in Event-based data (GSCEventMOD). We additionally show how the
optimum number of moving objects can be automatically determined. Experimental
comparisons on publicly available datasets show that the proposed GSCEventMOD
algorithm outperforms a number of state-of-the-art techniques by a maximum
margin of 30%.

    

### [[2109.14985] The Artificial Intelligence behind the winning entry to the 2019 AI Robotic Racing Competition](http://arxiv.org/abs/2109.14985)


  Robotics is the next frontier in the progress of Artificial Intelligence
(AI), as the real world in which robots operate represents an enormous,
complex, continuous state space with inherent real-time requirements. One
extreme challenge in robotics is currently formed by autonomous drone racing.
Human drone racers can fly through complex tracks at speeds of up to 190 km/h.
Achieving similar speeds with autonomous drones signifies tackling fundamental
problems in AI under extreme restrictions in terms of resources. In this
article, we present the winning solution of the first AI Robotic Racing (AIRR)
Circuit, a competition consisting of four races in which all participating
teams used the same drone, to which they had limited access. The core of our
approach is inspired by how human pilots combine noisy observations of the race
gates with their mental model of the drone's dynamics to achieve fast control.
Our approach has a large focus on gate detection with an efficient deep neural
segmentation network and active vision. Further, we make contributions to
robust state estimation and risk-based control. This allowed us to reach speeds
of ~9.2m/s in the last race, unrivaled by previous autonomous drone race
competitions. Although our solution was the fastest and most robust, it still
lost against one of the best human pilots, Gab707. The presented approach
indicates a promising direction to close the gap with human drone pilots,
forming an important step in bringing AI to the real world.

    

### [[2109.15012] USER: A Unified Information Search and Recommendation Model based on Integrated Behavior Sequence](http://arxiv.org/abs/2109.15012)


  Search and recommendation are the two most common approaches used by people
to obtain information. They share the same goal -- satisfying the user's
information need at the right time. There are already a lot of Internet
platforms and Apps providing both search and recommendation services, showing
us the demand and opportunity to simultaneously handle both tasks. However,
most platforms consider these two tasks independently -- they tend to train
separate search model and recommendation model, without exploiting the
relatedness and dependency between them. In this paper, we argue that jointly
modeling these two tasks will benefit both of them and finally improve overall
user satisfaction. We investigate the interactions between these two tasks in
the specific information content service domain. We propose first integrating
the user's behaviors in search and recommendation into a heterogeneous behavior
sequence, then utilizing a joint model for handling both tasks based on the
unified sequence. More specifically, we design the Unified Information Search
and Recommendation model (USER), which mines user interests from the integrated
sequence and accomplish the two tasks in a unified way.

    

### [[2109.15107] CrossAug: A Contrastive Data Augmentation Method for Debiasing Fact Verification Models](http://arxiv.org/abs/2109.15107)


  Fact verification datasets are typically constructed using crowdsourcing
techniques due to the lack of text sources with veracity labels. However, the
crowdsourcing process often produces undesired biases in data that cause models
to learn spurious patterns. In this paper, we propose CrossAug, a contrastive
data augmentation method for debiasing fact verification models. Specifically,
we employ a two-stage augmentation pipeline to generate new claims and
evidences from existing samples. The generated samples are then paired
cross-wise with the original pair, forming contrastive samples that facilitate
the model to rely less on spurious patterns and learn more robust
representations. Experimental results show that our method outperforms the
previous state-of-the-art debiasing technique by 3.6% on the debiased extension
of the FEVER dataset, with a total performance boost of 10.13% from the
baseline. Furthermore, we evaluate our approach in data-scarce settings, where
models can be more susceptible to biases due to the lack of training data.
Experimental results demonstrate that our approach is also effective at
debiasing in these low-resource conditions, exceeding the baseline performance
on the Symmetric dataset with just 1% of the original data.

    

### [[2109.15119] Improved statistical machine translation using monolingual paraphrases](http://arxiv.org/abs/2109.15119)


  We propose a novel monolingual sentence paraphrasing method for augmenting
the training data for statistical machine translation systems "for free" -- by
creating it from data that is already available rather than having to create
more aligned data. Starting with a syntactic tree, we recursively generate new
sentence variants where noun compounds are paraphrased using suitable
prepositions, and vice-versa -- preposition-containing noun phrases are turned
into noun compounds. The evaluation shows an improvement equivalent to 33%-50%
of that of doubling the amount of training data.

    

### [[2109.15144] A Review of Text Style Transfer using Deep Learning](http://arxiv.org/abs/2109.15144)


  Style is an integral component of a sentence indicated by the choice of words
a person makes. Different people have different ways of expressing themselves,
however, they adjust their speaking and writing style to a social context, an
audience, an interlocutor or the formality of an occasion. Text style transfer
is defined as a task of adapting and/or changing the stylistic manner in which
a sentence is written, while preserving the meaning of the original sentence.
A systematic review of text style transfer methodologies using deep learning
is presented in this paper. We point out the technological advances in deep
neural networks that have been the driving force behind current successes in
the fields of natural language understanding and generation. The review is
structured around two key stages in the text style transfer process, namely,
representation learning and sentence generation in a new style. The discussion
highlights the commonalities and differences between proposed solutions as well
as challenges and opportunities that are expected to direct and foster further
research in the field.

    

### [[2109.15177] You Cannot Easily Catch Me: A Low-Detectable Adversarial Patch for Object Detectors](http://arxiv.org/abs/2109.15177)


  Blind spots or outright deceit can bedevil and deceive machine learning
models. Unidentified objects such as digital "stickers," also known as
adversarial patches, can fool facial recognition systems, surveillance systems
and self-driving cars. Fortunately, most existing adversarial patches can be
outwitted, disabled and rejected by a simple classification network called an
adversarial patch detector, which distinguishes adversarial patches from
original images. An object detector classifies and predicts the types of
objects within an image, such as by distinguishing a motorcyclist from the
motorcycle, while also localizing each object's placement within the image by
"drawing" so-called bounding boxes around each object, once again separating
the motorcyclist from the motorcycle. To train detectors even better, however,
we need to keep subjecting them to confusing or deceitful adversarial patches
as we probe for the models' blind spots. For such probes, we came up with a
novel approach, a Low-Detectable Adversarial Patch, which attacks an object
detector with small and texture-consistent adversarial patches, making these
adversaries less likely to be recognized. Concretely, we use several geometric
primitives to model the shapes and positions of the patches. To enhance our
attack performance, we also assign different weights to the bounding boxes in
terms of loss function. Our experiments on the common detection dataset COCO as
well as the driving-video dataset D2-City show that LDAP is an effective attack
method, and can resist the adversarial patch detector.

    

### [[2109.15180] Submodular Optimization Beyond Nonnegativity: Adaptive Seed Selection in Incentivized Social Advertising](http://arxiv.org/abs/2109.15180)


  The idea of social advertising (or social promotion) is to select a group of
influential individuals (a.k.a \emph{seeds}) to help promote some products or
ideas through an online social networks. There are two major players in the
social advertising ecosystem: advertiser and platform. The platform sells viral
engagements such as "like"s to advertisers by inserting their ads into the feed
of seeds. These seeds receive monetary incentives from the platform in exchange
for their participation in the social advertising campaign. Once an ad is
engaged by a follower of some seed, the platform receives a fixed amount of
payment, called cost per engagement, from the advertiser. The ad could
potentially attract more engagements from followers' followers and trigger a
viral contagion. At the beginning of a campaign, the advertiser submits a
budget to the platform and this budget can be used for two purposes: recruiting
seeds and paying for the viral engagements generated by the seeds. Note that
the first part of payment goes to the seeds and the latter one is the actual
revenue collected by the platform. In this setting, the problem for the
platform is to recruit a group of seeds such that she can collect the largest
possible amount of revenue subject to the budget constraint. We formulate this
problem as a seed selection problem whose objective function is non-monotone
and it might take on negative values, making existing results on submodular
optimization and influence maximization not applicable to our setting. We study
this problem under both non-adaptive and adaptive settings. Although we focus
on social advertising in this paper, our results apply to any optimization
problems whose objective function is the expectation of the minimum of a
stochastic submodular function and a linear function.

    

### [[2109.15196] Multilingual AMR Parsing with Noisy Knowledge Distillation](http://arxiv.org/abs/2109.15196)


  We study multilingual AMR parsing from the perspective of knowledge
distillation, where the aim is to learn and improve a multilingual AMR parser
by using an existing English parser as its teacher. We constrain our
exploration in a strict multilingual setting: there is but one model to parse
all different languages including English. We identify that noisy input and
precise output are the key to successful distillation. Together with extensive
pre-training, we obtain an AMR parser whose performances surpass all previously
published results on four different foreign languages, including German,
Spanish, Italian, and Chinese, by large margins (up to 18.8 \textsc{Smatch}
points on Chinese and on average 11.3 \textsc{Smatch} points). Our parser also
achieves comparable performance on English to the latest state-of-the-art
English-only parser.

    

### [[2109.15316] Scalable Online Planning via Reinforcement Learning Fine-Tuning](http://arxiv.org/abs/2109.15316)


  Lookahead search has been a critical component of recent AI successes, such
as in the games of chess, go, and poker. However, the search methods used in
these games, and in many other settings, are tabular. Tabular search methods do
not scale well with the size of the search space, and this problem is
exacerbated by stochasticity and partial observability. In this work we replace
tabular search with online model-based fine-tuning of a policy neural network
via reinforcement learning, and show that this approach outperforms
state-of-the-art search algorithms in benchmark settings. In particular, we use
our search algorithm to achieve a new state-of-the-art result in self-play
Hanabi, and show the generality of our algorithm by also showing that it
outperforms tabular search in the Atari game Ms. Pacman.

    

### [[1912.00147] Integrating Graph Contextualized Knowledge into Pre-trained Language Models](http://arxiv.org/abs/1912.00147)


  Complex node interactions are common in knowledge graphs, and these
interactions also contain rich knowledge information. However, traditional
methods usually treat a triple as a training unit during the knowledge
representation learning (KRL) procedure, neglecting contextualized information
of the nodes in knowledge graphs (KGs). We generalize the modeling object to a
very general form, which theoretically supports any subgraph extracted from the
knowledge graph, and these subgraphs are fed into a novel transformer-based
model to learn the knowledge embeddings. To broaden usage scenarios of
knowledge, pre-trained language models are utilized to build a model that
incorporates the learned knowledge representations. Experimental results
demonstrate that our model achieves the state-of-the-art performance on several
medical NLP tasks, and improvement above TransE indicates that our KRL method
captures the graph contextualized information effectively.

    

### [[2006.14804] Widening the Pipeline in Human-Guided Reinforcement Learning with Explanation and Context-Aware Data Augmentation](http://arxiv.org/abs/2006.14804)


  Human explanation (e.g., in terms of feature importance) has been recently
used to extend the communication channel between human and agent in interactive
machine learning. Under this setting, human trainers provide not only the
ground truth but also some form of explanation. However, this kind of human
guidance was only investigated in supervised learning tasks, and it remains
unclear how to best incorporate this type of human knowledge into deep
reinforcement learning. In this paper, we present the first study of using
human visual explanations in human-in-the-loop reinforcement learning (HRL). We
focus on the task of learning from feedback, in which the human trainer not
only gives binary evaluative "good" or "bad" feedback for queried state-action
pairs, but also provides a visual explanation by annotating relevant features
in images. We propose EXPAND (EXPlanation AugmeNted feeDback) to encourage the
model to encode task-relevant features through a context-aware data
augmentation that only perturbs irrelevant features in human salient
information. We choose five tasks, namely Pixel-Taxi and four Atari games, to
evaluate the performance and sample efficiency of this approach. We show that
our method significantly outperforms methods leveraging human explanation that
are adapted from supervised learning, and Human-in-the-loop RL baselines that
only utilize evaluative feedback.

    

### [[2101.10953] Predicting the future with a scale-invariant temporal memory for the past](http://arxiv.org/abs/2101.10953)


  In recent years it has become clear that the brain maintains a temporal
memory of recent events stretching far into the past. This paper presents a
neurally-inspired algorithm to use a scale-invariant temporal representation of
the past to predict a scale-invariant future. The result is a scale-invariant
estimate of future events as a function of the time at which they are expected
to occur. The algorithm is time-local, with credit assigned to the present
event by observing how it affects the prediction of the future. To illustrate
the potential utility of this approach, we test the model on simultaneous
renewal processes with different time scales. The algorithm scales well on
these problems despite the fact that the number of states needed to describe
them as a Markov process grows exponentially.

    

### [[2102.10558] Inconsistency thresholds for incomplete pairwise comparison matrices](http://arxiv.org/abs/2102.10558)


  Pairwise comparison matrices are increasingly used in settings where some
pairs are missing. However, there exist few inconsistency indices for similar
incomplete data sets and no reasonable measure has an associated threshold.
This paper generalises the famous rule of thumb for the acceptable level of
inconsistency, proposed by Saaty, to incomplete pairwise comparison matrices.
The extension is based on choosing the missing elements such that the maximal
eigenvalue of the incomplete matrix is minimised. Consequently, the
well-established values of the random index cannot be adopted: the
inconsistency of random matrices is found to be the function of matrix size and
the number of missing elements, with a nearly linear dependence in the case of
the latter variable. Our results can be directly built into decision-making
software and used by practitioners as a statistical criterion for accepting or
rejecting an incomplete pairwise comparison matrix.

    

### [[2103.05746] Analyzing Human Models that Adapt Online](http://arxiv.org/abs/2103.05746)


  Predictive human models often need to adapt their parameters online from
human data. This raises previously ignored safety-related questions for robots
relying on these models such as what the model could learn online and how
quickly could it learn it. For instance, when will the robot have a confident
estimate in a nearby human's goal? Or, what parameter initializations guarantee
that the robot can learn the human's preferences in a finite number of
observations? To answer such analysis questions, our key idea is to model the
robot's learning algorithm as a dynamical system where the state is the current
model parameter estimate and the control is the human data the robot observes.
This enables us to leverage tools from reachability analysis and optimal
control to compute the set of hypotheses the robot could learn in finite time,
as well as the worst and best-case time it takes to learn them. We demonstrate
the utility of our analysis tool in four human-robot domains, including
autonomous driving and indoor navigation.

    

### [[2104.08758] Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus](http://arxiv.org/abs/2104.08758)


  Large language models have led to remarkable progress on many NLP tasks, and
researchers are turning to ever-larger text corpora to train them. Some of the
largest corpora available are made by scraping significant portions of the
internet, and are frequently introduced with only minimal documentation. In
this work we provide some of the first documentation for the Colossal Clean
Crawled Corpus (C4; Raffel et al., 2020), a dataset created by applying a set
of filters to a single snapshot of Common Crawl. We begin by investigating
where the data came from, and find a significant amount of text from unexpected
sources like patents and US military websites. Then we explore the content of
the text itself, and find machine-generated text (e.g., from machine
translation systems) and evaluation examples from other benchmark NLP datasets.
To understand the impact of the filters applied to create this dataset, we
evaluate the text that was removed, and show that blocklist filtering
disproportionately removes text from and about minority individuals. Finally,
we conclude with some recommendations for how to created and document web-scale
datasets from a scrape of the internet.

    

### [[2105.02605] GraphFormers: GNN-nested Transformers for Representation Learning on Textual Graph](http://arxiv.org/abs/2105.02605)


  The representation learning on textual graph is to generate low-dimensional
embeddings for the nodes based on the individual textual features and the
neighbourhood information. Recent breakthroughs on pretrained language models
and graph neural networks push forward the development of corresponding
techniques. The existing works mainly rely on the cascaded model architecture:
the textual features of nodes are independently encoded by language models at
first; the textual embeddings are aggregated by graph neural networks
afterwards. However, the above architecture is limited due to the independent
modeling of textual features. In this work, we propose GraphFormers, where
layerwise GNN components are nested alongside the transformer blocks of
language models. With the proposed architecture, the text encoding and the
graph aggregation are fused into an iterative workflow, making each node's
semantic accurately comprehended from the global perspective. In addition, a
progressive learning strategy is introduced, where the model is successively
trained on manipulated data and original data to reinforce its capability of
integrating information on graph. Extensive evaluations are conducted on three
large-scale benchmark datasets, where GraphFormers outperform the SOTA
baselines with comparable running efficiency.

    

### [[2109.11946] Evaluating X-vector-based Speaker Anonymization under White-box Assessment](http://arxiv.org/abs/2109.11946)


  In the scenario of the Voice Privacy challenge, anonymization is achieved by
converting all utterances from a source speaker to match the same target
identity; this identity being randomly selected. In this context, an attacker
with maximum knowledge about the anonymization system can not infer the target
identity. This article proposed to constrain the target selection to a specific
identity, i.e., removing the random selection of identity, to evaluate the
extreme threat under a whitebox assessment (the attacker has complete knowledge
about the system). Targeting a unique identity also allows us to investigate
whether some target's identities are better than others to anonymize a given
speaker.

    

### [[2109.14682] Unified Shader Programming in C++](http://arxiv.org/abs/2109.14682)


  In real-time graphics, the strict separation of programming languages and
environments for host (CPU) code and GPU code results in code duplication,
subtle compatibility bugs, and additional development and maintenance costs. In
contrast, popular general-purpose GPU (GPGPU) programming models like CUDA and
C++ AMP avoid many of these issues by presenting unified programming
environments where both host and GPU code are written in the same language, can
be in the same file, and share lexical scopes. To bring the benefits of unified
programming to real-time graphics, this paper examines graphics-specific
challenges that complicate the development of such a unified model and explores
how to overcome them in a widely used programming language.
We observe that GPU code specialization, a key optimization in real-time
graphics, requires coordination between parameters that are
compile-time-constant in GPU code but are assigned values at runtime in host
code based on dynamic data. Current methods used to implement specialization do
not translate to a unified environment where host and GPU code share
declarations of these parameters. Furthermore, this compile-time vs. runtime
coordination is not innately expressible in the popular languages used in this
domain.
In this paper, we create a unified environment for real-time graphics
programming in C++ by co-opting existing features of the language and
implementing them with alternate semantics to express the services required.
Specifically, we co-opt C++ attributes and virtual functions, which enables us
to provide first-class support for specialization in our unified system. By
co-opting existing features, we enable programmers to use familiar C++
programming techniques to write host and GPU code together, while still
achieving efficient generated C++ and HLSL code via our source-to-source
translator.

    

### [[2109.14908] Proceedings 14th Interaction and Concurrency Experience](http://arxiv.org/abs/2109.14908)


  This volume contains the proceedings of ICE'21, the 14th Interaction and
Concurrency Experience, which was held online on the 18th of June 2021, as a
satellite event of DisCoTec'21. The ICE workshop series features a
distinguishing review and selection procedure, allowing PC members to interact
anonymously with authors. As in the past 13 editions, this interaction
considerably improved the accuracy of the feedback from the reviewers and the
quality of accepted papers, and offered the basis for lively discussion during
the workshop. The 2021 edition of ICE included double blind reviewing of
original research papers, in order to increase fairness and avoid bias in
reviewing. Each paper was reviewed by three or four PC members, and altogether
5 papers were accepted for publication - plus 4 oral presentations which are
not part of this volume. We were proud to host 2 invited talks, by Laura Bocchi
and Helene Coullon. The abstracts of these talks are included in this volume
together with the regular papers. The final versions of the contributions,
taking into account the discussion at the workshop, are included.

    

### [[2102.11605] A tier-based typed programming language characterizing Feasible Functionals](http://arxiv.org/abs/2102.11605)


  The class of Basic Feasible Functionals BFF$_2$ is the type-2 counterpart of
the class FP of type-1 functions computable in polynomial time. Several
characterizations have been suggested in the literature, but none of these
present a programming language with a type system guaranteeing this complexity
bound. We give a characterization of BFF$_2$ based on an imperative language
with oracle calls using a tier-based type system whose inference is decidable.
Such a characterization should make it possible to link higher-order complexity
with programming theory. The low complexity (cubic in the size of the program)
of the type inference algorithm contrasts with the intractability of the
aforementioned methods and does not overly constrain the expressive power of
the language.

    

### [<title>XGBoost GPU Support for Macbook - XGBoost</title>](https://discuss.xgboost.ai/t/xgboost-gpu-support-for-macbook/2479/1)

### [[2109.14883] Process discovery on deviant traces and other stranger things](http://arxiv.org/abs/2109.14883)


  As the need to understand and formalise business processes into a model has
grown over the last years, the process discovery research field has gained more
and more importance, developing two different classes of approaches to model
representation: procedural and declarative. Orthogonally to this
classification, the vast majority of works envisage the discovery task as a
one-class supervised learning process guided by the traces that are recorded
into an input log. In this work instead, we focus on declarative processes and
embrace the less-popular view of process discovery as a binary supervised
learning task, where the input log reports both examples of the normal system
execution, and traces representing "stranger" behaviours according to the
domain semantics. We therefore deepen how the valuable information brought by
both these two sets can be extracted and formalised into a model that is
"optimal" according to user-defined goals. Our approach, namely NegDis, is
evaluated w.r.t. other relevant works in this field, and shows promising
results as regards both the performance and the quality of the obtained
solution.

    