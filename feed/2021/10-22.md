
## 2021-10-22

### [[2110.10534] FairNet: A Measurement Framework for Traffic Discrimination Detection on the Internet](http://arxiv.org/abs/2110.10534)


  Network neutrality is related to the non-discriminatory treatment of packets
on the Internet. Any deliberate discrimination of traffic of one application
while favoring others violates the principle of neutrality. Many countries have
enforced laws against such discrimination. To enforce such laws, one requires
tools to detect any net neutrality violations. However, detecting such
violations is challenging as it is hard to separate any degradation in quality
due to natural network effects and selective degradation. Also, legitimate
traffic management and deliberate discrimination methods can be technically the
same, making it further challenging to distinguish them.
We developed an end-to-end measurement framework named FairNet to detect
discrimination of traffic. It compares the performance of similar services. Our
focus is on HTTPS streaming services which constitute a predominant portion of
the Internet traffic. The effect of confounding factors (congestion, traffic
management policy, dynamic rate adaptation) is made `similar' on the test
services to ensure a fair comparison. FairNet framework uses a ``replay
server'' and user-client that exchanges correctly identifiable traffic streams
over the Internet. The Server Name Indication (SNI) field in the TLS handshake,
which goes in plaintext, ensures that the traffic from the replay server
appears to network middle-boxes as that coming from its actual server. We
validated that appropriate SNIs results in the correct classification of
services using a commercial traffic shaper. FairNet uses two novel algorithms
based on application-level throughput and connection status to detect traffic
discrimination. We also validated the methodology's effectiveness by collecting
network logs through mobile apps over the live Internet and analyzing them.

    

### [[2110.10617] Colosseum: Large-Scale Wireless Experimentation Through Hardware-in-the-Loop Network Emulation](http://arxiv.org/abs/2110.10617)


  Colosseum is an open-access and publicly-available large-scale wireless
testbed for experimental research via virtualized and softwarized waveforms and
protocol stacks on a fully programmable, "white-box" platform. Through 256
state-of-the-art Software-defined Radios and a Massive Channel Emulator core,
Colosseum can model virtually any scenario, enabling the design, development
and testing of solutions at scale in a variety of deployments and channel
conditions. These Colosseum radio-frequency scenarios are reproduced through
high-fidelity FPGA-based emulation with finite-impulse response filters.
Filters model the taps of desired wireless channels and apply them to the
signals generated by the radio nodes, faithfully mimicking the conditions of
real-world wireless environments. In this paper we describe the architecture of
Colosseum and its experimentation and emulation capabilities. We then
demonstrate the effectiveness of Colosseum for experimental research at scale
through exemplary use cases including prevailing wireless technologies (e.g.,
cellular and Wi-Fi) in spectrum sharing and unmanned aerial vehicle scenarios.
A roadmap for Colosseum future updates concludes the paper.

    

### [[2006.07559] Enabling Joint Communication and Radar Sensing in Mobile Networks -- A Survey](http://arxiv.org/abs/2006.07559)


  Mobile network is evolving from a communication-only network towards one with
joint communication and radar/radio sensing (JCAS) capabilities, that we call
perceptive mobile network (PMN). In PMNs, JCAS integrates sensing into
communications, sharing a majority of system modules and the same transmitted
signals. The PMN is expected to provide a ubiquitous radio sensing platform and
enable a vast number of novel smart applications, whilst providing
non-compromised communications. In this paper, we present a broad picture of
the motivation, methodologies, challenges, and research opportunities of
realizing PMN, by providing a comprehensive survey for systems and technologies
developed mainly in the last ten years. Beginning by reviewing the work on
coexisting communication and radar systems, we highlight their limits on
addressing the interference problem, and then introduce the JCAS technology. We
then set up JCAS in the mobile network context and envisage its potential
applications. We continue to provide a brief review of three types of JCAS
systems, with particular attention to their differences in design philosophy.
We then introduce a framework of PMN, including the system platform and
infrastructure, three types of sensing operations, and signals usable for
sensing. Subsequently, we discuss required system modifications to enable
sensing on current communication-only infrastructure. Within the context of
PMN, we review stimulating research problems and potential solutions, organized
under nine topics: performance bounds, waveform optimization, antenna array
design, clutter suppression, sensing parameter estimation, resolution of
sensing ambiguity, pattern analysis, networked sensing under cellular topology,
and sensing-assisted communications. We conclude the paper by listing key open
research problems for the aforementioned topics and sharing some lessons that
we have learned.

    

### [[2008.02695] New Directions in Automated Traffic Analysis](http://arxiv.org/abs/2008.02695)


  Despite the use of machine learning for many network traffic analysis tasks
in security, from application identification to intrusion detection, the
aspects of the machine learning pipeline that ultimately determine the
performance of the model -- feature selection and representation, model
selection, and parameter tuning -- remain manual and painstaking. This paper
presents a method to automate many aspects of traffic analysis, making it
easier to apply machine learning techniques to a wider variety of traffic
analysis tasks. We introduce nPrint, a tool that generates a unified packet
representation that is amenable for representation learning and model training.
We integrate nPrint with automated machine learning (AutoML), resulting in
nPrintML, a public system that largely eliminates feature extraction and model
tuning for a wide variety of traffic analysis tasks. We have evaluated nPrintML
on eight separate traffic analysis tasks and released nPrint and nPrintML to
enable future work to extend these methods.

    

### [[2012.06774] A network analysis on cloud gaming: Stadia, GeForce Now and PSNow](http://arxiv.org/abs/2012.06774)


  Cloud gaming is a new class of services that promises to revolutionize the
videogame market. It allows the user to play a videogame with basic equipment
while using a remote server for the actual execution. The multimedia content is
streamed through the network from the server to the user. This service requires
low latency and a large bandwidth to work properly with low response time and
high-definition video. Three of the leading tech companies, (Google, Sony and
NVIDIA) entered this market with their own products, and others, like Microsoft
and Amazon, are planning to launch their own platforms in the near future.
However, these companies released so far little information about their cloud
gaming operation and how they utilize the network. In this work, we study these
new cloud gaming services from the network point of view. We collect more than
200 packet traces under different application settings and network conditions
for 3 cloud gaming services, namely Stadia from Google, GeForce Now from NVIDIA
and PS Now from Sony. We analyze the employed protocols and the workload they
impose on the network. We find that GeForce Now and Stadia use the RTP protocol
to stream the multimedia content, with the latter relying on the standard
WebRTC APIs. They result in bandwidth-hungry and consume up to 45 Mbit/s,
depending on the network and video quality. PS Now instead uses only
undocumented protocols and never exceeds 13 Mbit/s.

    

### [[2102.08866] IoTDevID: A Behavior-Based Device Identification Method for the IoT](http://arxiv.org/abs/2102.08866)


  Device identification is one way to secure a network of IoT devices, whereby
devices identified as suspicious can subsequently be isolated from a network.
In this study, we present a machine learning-based method, IoTDevID, that
recognizes devices through characteristics of their network packets. As a
result of using a rigorous feature analysis and selection process, our study
offers a generalizable and realistic approach to modelling device behavior,
achieving high predictive accuracy across two public datasets. The model's
underlying feature set is shown to be more predictive than existing feature
sets used for device identification, and is shown to generalize to data unseen
during the feature selection process. Unlike most existing approaches to IoT
device identification, IoTDevID is able to detect devices using non-IP and
low-energy protocols.

    

### [[2104.06926] Optimizing Response Time in SDN-Edge Environments for Time-Strict IoT Applications](http://arxiv.org/abs/2104.06926)


  The rise of the Internet of Things (IoT) has opened new research lines that
focus on applying IoT applications to domains further beyond basic user-grade
applications, such as Industry or Healthcare. These domains demand a very high
Quality of Service (QoS), mainly a very short response time. In order to meet
these demands, some works are evaluating how to modularize and deploy IoT
applications in different nodes of the infrastructure (edge, fog, cloud), as
well as how to place the network controllers, since these decisions affect the
response time of the application. Some works in the literature have approached
this problem by providing separate plans for deployment and placing of
controllers. However, this approach makes sub-optimal decisions, that
complicate guaranteeing the demanded response time. To guarantee an optimal
response time, it is crucial to solve the problem in a single effort that
considers both, the networking and computing dimensions. In this work, we
analyze the influences between the response time of computing and networking in
edge computing environments with SDN networks, merging both optimization
efforts into a single one and proposing a solution to the joint problem. Our
evaluation shows that our proposal can shorten response time by up to 28.97%

    

### [[2103.06221] Privacy-Preserving and Sustainable Contact Tracing Using Batteryless BLE Beacons](http://arxiv.org/abs/2103.06221)


  Contact tracing with mobile applications is an attractive approach for many
governments and industry initiatives to address the Covid-19 pandemic. However,
many approaches today have severe privacy and security issues, and many of them
also fail to offer a sustainable contact tracing infrastructure due to the
demanding energy consumption. This work makes several contributions towards
overcoming these limitations. First, we propose a privacy-preserving
architecture for contact tracing that leverages a fixed infrastructure of BLE
beacon transmitters. Second, we evaluate the feasibility of adopting
batteryless or energy-harvesting BLE beacons to make this architecture more
sustainable and green. Finally, we identify practical research challenges and
opportunities for academia and industry to advance and realize the proposed
privacy-preserving and sustainable contact tracing architecture.

    

### [[2110.10152] Identifying Stroke Indicators Using Rough Sets](http://arxiv.org/abs/2110.10152)


  Stroke is widely considered as the second most common cause of mortality. The
adverse consequences of stroke have led to global interest and work for
improving the management and diagnosis of stroke. Various techniques for data
mining have been used globally for accurate prediction of occurrence of stroke
based on the risk factors that are associated with the electronic health care
records (EHRs) of the patients. In particular, EHRs routinely contain several
thousands of features and most of them are redundant and irrelevant that need
to be discarded to enhance the prediction accuracy. The choice of
feature-selection methods can help in improving the prediction accuracy of the
model and efficient data management of the archived input features. In this
paper, we systematically analyze the various features in EHR records for the
detection of stroke. We propose a novel rough-set based technique for ranking
the importance of the various EHR records in detecting stroke. Unlike the
conventional rough-set techniques, our proposed technique can be applied on any
dataset that comprises binary feature sets. We evaluated our proposed method in
a publicly available dataset of EHR, and concluded that age, average glucose
level, heart disease, and hypertension were the most essential attributes for
detecting stroke in patients. Furthermore, we benchmarked the proposed
technique with other popular feature-selection techniques. We obtained the best
performance in ranking the importance of individual features in detecting
stroke.

    

### [[2110.10165] NAS-HPO-Bench-II: A Benchmark Dataset on Joint Optimization of Convolutional Neural Network Architecture and Training Hyperparameters](http://arxiv.org/abs/2110.10165)


  The benchmark datasets for neural architecture search (NAS) have been
developed to alleviate the computationally expensive evaluation process and
ensure a fair comparison. Recent NAS benchmarks only focus on architecture
optimization, although the training hyperparameters affect the obtained model
performances. Building the benchmark dataset for joint optimization of
architecture and training hyperparameters is essential to further NAS research.
The existing NAS-HPO-Bench is a benchmark for joint optimization, but it does
not consider the network connectivity design as done in modern NAS algorithms.
This paper introduces the first benchmark dataset for joint optimization of
network connections and training hyperparameters, which we call
NAS-HPO-Bench-II. We collect the performance data of 4K cell-based
convolutional neural network architectures trained on the CIFAR-10 dataset with
different learning rate and batch size settings, resulting in the data of 192K
configurations. The dataset includes the exact data for 12 epoch training. We
further build the surrogate model predicting the accuracies after 200 epoch
training to provide the performance data of longer training epoch. By analyzing
NAS-HPO-Bench-II, we confirm the dependency between architecture and training
hyperparameters and the necessity of joint optimization. Finally, we
demonstrate the benchmarking of the baseline optimization algorithms using
NAS-HPO-Bench-II.

    

### [[2110.10189] StructFormer: Learning Spatial Structure for Language-Guided Semantic Rearrangement of Novel Objects](http://arxiv.org/abs/2110.10189)


  Geometric organization of objects into semantically meaningful arrangements
pervades the built world. As such, assistive robots operating in warehouses,
offices, and homes would greatly benefit from the ability to recognize and
rearrange objects into these semantically meaningful structures. To be useful,
these robots must contend with previously unseen objects and receive
instructions without significant programming. While previous works have
examined recognizing pairwise semantic relations and sequential manipulation to
change these simple relations none have shown the ability to arrange objects
into complex structures such as circles or table settings. To address this
problem we propose a novel transformer-based neural network, StructFormer,
which takes as input a partial-view point cloud of the current object
arrangement and a structured language command encoding the desired object
configuration. We show through rigorous experiments that StructFormer enables a
physical robot to rearrange novel objects into semantically meaningful
structures with multi-object relational constraints inferred from the language
command.

    

### [[2110.10200] fairadapt: Causal Reasoning for Fair Data Pre-processing](http://arxiv.org/abs/2110.10200)


  Machine learning algorithms are useful for various predictions tasks, but
they can also learn how to discriminate, based on gender, race or other
sensitive attributes. This realization gave rise to the field of fair machine
learning, which aims to measure and mitigate such algorithmic bias. This
manuscript describes the R-package fairadapt, which implements a causal
inference pre-processing method. By making use of a causal graphical model and
the observed data, the method can be used to address hypothetical questions of
the form "What would my salary have been, had I been of a different
gender/race?". Such individual level counterfactual reasoning can help
eliminate discrimination and help justify fair decisions. We also discuss
appropriate relaxations which assume certain causal pathways from the sensitive
attribute to the outcome are not discriminatory.

    

### [[2110.10210] Long Random Matrices and Tensor Unfolding](http://arxiv.org/abs/2110.10210)


  In this paper, we consider the singular values and singular vectors of low
rank perturbations of large rectangular random matrices, in the regime the
matrix is "long": we allow the number of rows (columns) to grow polynomially in
the number of columns (rows). We prove there exists a critical signal-to-noise
ratio (depending on the dimensions of the matrix), and the extreme singular
values and singular vectors exhibit a BBP type phase transition. As a main
application, we investigate the tensor unfolding algorithm for the asymmetric
rank-one spiked tensor model, and obtain an exact threshold, which is
independent of the procedure of tensor unfolding. If the signal-to-noise ratio
is above the threshold, tensor unfolding detects the signals; otherwise, it
fails to capture the signals.

    

### [[2110.10211] Learning Equivariances and Partial Equivariances from Data](http://arxiv.org/abs/2110.10211)


  Group equivariant Convolutional Neural Networks (G-CNNs) constrain features
to respect the chosen symmetries, and lead to better generalization when these
symmetries appear in the data. However, if the chosen symmetries are not
present, group equivariant architectures lead to overly constrained models and
worse performance. Frequently, the distribution of the data can be better
represented by a subset of a group than by the group as a whole, e.g.,
rotations in $[-90^{\circ}, 90^{\circ}]$. In such cases, a model that respects
equivariance partially is better suited to represent the data. Moreover,
relevant symmetries may differ for low and high-level features, e.g., edge
orientations in a face, and face poses relative to the camera. As a result, the
optimal level of equivariance may differ per layer. In this work, we introduce
Partial G-CNNs: a family of equivariant networks able to learn partial and full
equivariances from data at every layer end-to-end. Partial G-CNNs retain full
equivariance whenever beneficial, e.g., for rotated MNIST, but are able to
restrict it whenever it becomes harmful, e.g., for 6~/~9 or natural image
classification. Partial G-CNNs perform on par with G-CNNs when full
equivariance is necessary, and outperform them otherwise. Our method is
applicable to discrete groups, continuous groups and combinations thereof.

    

### [[2110.10220] Patch Based Transformation for Minimum Variance Beamformer Image Approximation Using Delay and Sum Pipeline](http://arxiv.org/abs/2110.10220)


  In the recent past, there have been several efforts in accelerating
computationally heavy beamforming algorithms such as minimum variance
distortionless response (MVDR) beamforming to achieve real-time performance
comparable to the popular delay and sum (DAS) beamforming. This has been
achieved using a variety of neural network architectures ranging from fully
connected neural networks (FCNNs), convolutional neural networks (CNNs) and
general adversarial networks (GANs). However most of these approaches are
working with optimizations considering image level losses and hence require a
significant amount of dataset to ensure that the process of beamforming is
learned. In this work, a patch level U-Net based neural network is proposed,
where the delay compensated radio frequency (RF) patch for a fixed region in
space (e.g. 32x32) is transformed through a U-Net architecture and multiplied
with DAS apodization weights and optimized for similarity with MVDR image of
the patch. Instead of framing the beamforming problem as a regression problem
to estimate the apodization weights, the proposed approach treats the
non-linear transformation of the RF data space that can account for the data
driven weight adaptation done by the MVDR approach in the parameters of the
network. In this way, it is also observed that by restricting the input to a
patch the model will learn the beamforming pipeline as an image non-linear
transformation problem.

    

### [[2110.10221] The CoRa Tensor Compiler: Compilation for Ragged Tensors with Minimal Padding](http://arxiv.org/abs/2110.10221)


  There is often variation in the shape and size of input data used for deep
learning. In many cases, such data can be represented using tensors with
non-uniform shapes, or ragged tensors. Due to limited and non-portable support
for efficient execution on ragged tensors, current deep learning frameworks
generally use techniques such as padding and masking to make the data shapes
uniform and then offload the computations to optimized kernels for dense tensor
algebra. Such techniques can, however, lead to a lot of wasted computation and
therefore, a loss in performance. This paper presents CoRa, a tensor compiler
that allows users to easily generate efficient code for ragged tensor operators
targeting a wide range of CPUs and GPUs. Evaluating CoRa on a variety of
operators on ragged tensors as well as on an encoder layer of the transformer
model, we find that CoRa (i)performs competitively with hand-optimized
implementations of the operators and the transformer encoder and (ii) achieves,
over PyTorch, a 1.6X geomean speedup for the encoder on an Nvidia GPU and a
1.86X geomean speedup for the multi-head attention module used in transformers
on an ARM CPU.

    

### [[2110.10223] A Federated Learning Aggregation Algorithm for Pervasive Computing: Evaluation and Comparison](http://arxiv.org/abs/2110.10223)


  Pervasive computing promotes the installation of connected devices in our
living spaces in order to provide services. Two major developments have gained
significant momentum recently: an advanced use of edge resources and the
integration of machine learning techniques for engineering applications. This
evolution raises major challenges, in particular related to the appropriate
distribution of computing elements along an edge-to-cloud continuum. About
this, Federated Learning has been recently proposed for distributed model
training in the edge. The principle of this approach is to aggregate models
learned on distributed clients in order to obtain a new, more general model.
The resulting model is then redistributed to clients for further training. To
date, the most popular federated learning algorithm uses coordinate-wise
averaging of the model parameters for aggregation. However, it has been shown
that this method is not adapted in heterogeneous environments where data is not
identically and independently distributed (non-iid). This corresponds directly
to some pervasive computing scenarios where heterogeneity of devices and users
challenges machine learning with the double objective of generalization and
personalization. In this paper, we propose a novel aggregation algorithm,
termed FedDist, which is able to modify its model architecture (here, deep
neural network) by identifying dissimilarities between specific neurons amongst
the clients. This permits to account for clients' specificity without impairing
generalization. Furthermore, we define a complete method to evaluate federated
learning in a realistic way taking generalization and personalization into
account.
Using this method, FedDist is extensively tested and compared with three
state-of-the-art federated learning algorithms on the pervasive domain of Human
Activity Recognition with smartphones.

    

### [[2110.10225] What Averages Do Not Tell -- Predicting Real Life Processes with Sequential Deep Learning](http://arxiv.org/abs/2110.10225)


  Deep Learning is proven to be an effective tool for modeling sequential data
as shown by the success in Natural Language, Computer Vision and Signal
Processing. Process Mining concerns discovering insights on business processes
from their execution data that are logged by supporting information systems.
The logged data (event log) is formed of event sequences (traces) that
correspond to executions of a process. Many Deep Learning techniques have been
successfully adapted for predictive Process Mining that aims to predict process
outcomes, remaining time, the next event, or even the suffix of running traces.
Traces in Process Mining are multimodal sequences and very differently
structured than natural language sentences or images. This may require a
different approach to processing. So far, there has been little focus on these
differences and the challenges introduced. Looking at suffix prediction as the
most challenging of these tasks, the performance of Deep Learning models was
evaluated only on average measures and for a small number of real-life event
logs. Comparing the results between papers is difficult due to different
pre-processing and evaluation strategies. Challenges that may be relevant are
the skewness of trace-length distribution and the skewness of the activity
distribution in real-life event logs. We provide an end-to-end framework which
enables to compare the performance of seven state-of-the-art sequential
architectures in common settings. Results show that sequence modeling still has
a lot of room for improvement for majority of the more complex datasets.
Further research and insights are required to get consistent performance not
just in average measures but additionally over all the prefixes.

    

### [[2110.10232] Test time Adaptation through Perturbation Robustness](http://arxiv.org/abs/2110.10232)


  Data samples generated by several real world processes are dynamic in nature
\textit{i.e.}, their characteristics vary with time. Thus it is not possible to
train and tackle all possible distributional shifts between training and
inference, using the host of transfer learning methods in literature. In this
paper, we tackle this problem of adapting to domain shift at inference time
\textit{i.e.}, we do not change the training process, but quickly adapt the
model at test-time to handle any domain shift. For this, we propose to enforce
consistency of predictions of data sampled in the vicinity of test sample on
the image manifold. On a host of test scenarios like dealing with corruptions
(CIFAR-10-C and CIFAR-100-C), and domain adaptation (VisDA-C), our method is at
par or significantly outperforms previous methods.

    

### [[2110.10233] Forecasting Market Prices using DL with Data Augmentation and Meta-learning: ARIMA still wins!](http://arxiv.org/abs/2110.10233)


  Deep-learning techniques have been successfully used for time-series
forecasting and have often shown superior performance on many standard
benchmark datasets as compared to traditional techniques. Here we present a
comprehensive and comparative study of performance of deep-learning techniques
for forecasting prices in financial markets. We benchmark state-of-the-art
deep-learning baselines, such as NBeats, etc., on data from currency as well as
stock markets. We also generate synthetic data using a fuzzy-logic based model
of demand driven by technical rules such as moving averages, which are often
used by traders. We benchmark the baseline techniques on this synthetic data as
well as use it for data augmentation. We also apply gradient-based
meta-learning to account for non-stationarity of financial time-series. Our
extensive experiments notwithstanding, the surprising result is that the
standard ARIMA models outperforms deep-learning even using data augmentation or
meta-learning. We conclude by speculating as to why this might be the case.

    

### [[2110.10234] More Engineering, No Silos: Rethinking Processes and Interfaces in Collaboration between Interdisciplinary Teams for Machine Learning Projects](http://arxiv.org/abs/2110.10234)


  The introduction of machine learning (ML) components in software projects has
created the need for software engineers to collaborate with data scientists and
other specialists. While collaboration can always be challenging, ML introduces
additional challenges with its exploratory model development process,
additional skills and knowledge needed, difficulties testing ML systems, need
for continuous evolution and monitoring, and non-traditional quality
requirements such as fairness and explainability. Through interviews with 45
practitioners from 28 organizations, we identified key collaboration challenges
that teams face when building and deploying ML systems into production. We
report on common collaboration points in the development of production ML
systems for requirements, data, and integration, as well as corresponding team
patterns and challenges. We find that most of these challenges center around
communication, documentation, engineering, and process and collect
recommendations to address these challenges.

    

### [[2110.10242] A New Automatic Change Detection Frame-work Based on Region Growing and Weighted Local Mutual Information: Analysis of Breast Tumor Response to Chemotherapy in Serial MR Images](http://arxiv.org/abs/2110.10242)


  The automatic analysis of subtle changes between longitudinal MR images is an
important task as it is still a challenging issue in scope of the breast
medical image processing. In this paper we propose an effective automatic
change detection framework composed of two phases since previously used methods
have features with low distinctive power. First, in the preprocessing phase an
intensity normalization method is suggested based on Hierarchical Histogram
Matching (HHM) that is more robust to noise than previous methods. To eliminate
undesirable changes and extract the regions containing significant changes the
proposed Extraction Region of Changes (EROC) method is applied based on
intensity distribution and Hill-Climbing algorithm. Second, in the detection
phase a region growing-based approach is suggested to differentiate significant
changes from unreal ones. Due to using proposed Weighted Local Mutual
Information (WLMI) method to extract high level features and also utilizing the
principle of the local consistency of changes, the proposed approach enjoys
reasonable performance. The experimental results on both simulated and real
longitudinal Breast MR Images confirm the effectiveness of the proposed
framework. Also, this framework outperforms the human expert in some cases
which can detect many lesion evolutions that are missed by expert.

    

### [[2110.10249] Neural Stochastic Partial Differential Equations](http://arxiv.org/abs/2110.10249)


  Stochastic partial differential equations (SPDEs) are the mathematical tool
of choice to model complex spatio-temporal dynamics of systems subject to the
influence of randomness. We introduce the Neural SPDE model providing an
extension to two important classes of physics-inspired neural architectures. On
the one hand, it extends all the popular neural -- ordinary, controlled,
stochastic, rough -- differential equation models in that it is capable of
processing incoming information even when the latter evolves in an infinite
dimensional state space. On the other hand, it extends Neural Operators --
recent generalizations of neural networks modelling mappings between functional
spaces -- in that it can be used to learn complex SPDE solution operators
$(u_0,\xi) \mapsto u$ depending simultaneously on an initial condition $u_0$
and on a stochastic forcing term $\xi$, while remaining resolution-invariant
and equation-agnostic. A Neural SPDE is constrained to respect real physical
dynamics and consequently requires only a modest amount of data to train,
depends on a significantly smaller amount of parameters and has better
generalization properties compared to Neural Operators. Through various
experiments on semilinear SPDEs with additive and multiplicative noise
(including the stochastic Navier-Stokes equations) we demonstrate how Neural
SPDEs can flexibly be used in a supervised learning setting as well as
conditional generative models to sample solutions of SPDEs conditioned on prior
knowledge, systematically achieving in both cases better performance than all
alternative models.

    

### [[2110.10255] A Simple Approach to Continual Learning by Transferring Skill Parameters](http://arxiv.org/abs/2110.10255)


  In order to be effective general purpose machines in real world environments,
robots not only will need to adapt their existing manipulation skills to new
circumstances, they will need to acquire entirely new skills on-the-fly. A
great promise of continual learning is to endow robots with this ability, by
using their accumulated knowledge and experience from prior skills. We take a
fresh look at this problem, by considering a setting in which the robot is
limited to storing that knowledge and experience only in the form of learned
skill policies. We show that storing skill policies, careful pre-training, and
appropriately choosing when to transfer those skill policies is sufficient to
build a continual learner in the context of robotic manipulation. We analyze
which conditions are needed to transfer skills in the challenging Meta-World
simulation benchmark. Using this analysis, we introduce a pair-wise metric
relating skills that allows us to predict the effectiveness of skill transfer
between tasks, and use it to reduce the problem of continual learning to
curriculum selection. Given an appropriate curriculum, we show how to
continually acquire robotic manipulation skills without forgetting, and using
far fewer samples than needed to train them from scratch.

    

### [[2110.10275] Early- and in-season crop type mapping without current-year ground truth: generating labels from historical information via a topology-based approach](http://arxiv.org/abs/2110.10275)


  Land cover classification in remote sensing is often faced with the challenge
of limited ground truth. Incorporating historical information has the potential
to significantly lower the expensive cost associated with collecting ground
truth and, more importantly, enable early- and in-season mapping that is
helpful to many pre-harvest decisions. In this study, we propose a new approach
that can effectively transfer knowledge about the topology (i.e. relative
position) of different crop types in the spectral feature space (e.g. the
histogram of SWIR1 vs RDEG1 bands) to generate labels, thereby support crop
classification in a different year. Importantly, our approach does not attempt
to transfer classification decision boundaries that are susceptible to
inter-annual variations of weather and management, but relies on the more
robust and shift-invariant topology information. We tested this approach for
mapping corn/soybeans in the US Midwest and paddy rice/corn/soybeans in
Northeast China using Landsat-8 and Sentinel-2 data. Results show that our
approach automatically generates high-quality labels for crops in the target
year immediately after each image becomes available. Based on these generated
labels from our approach, the subsequent crop type mapping using a random
forest classifier reach the F1 score as high as 0.887 for corn as early as the
silking stage and 0.851 for soybean as early as the flowering stage and the
overall accuracy of 0.873 in Iowa. In Northeast China, F1 scores of paddy rice,
corn and soybeans and the overall accuracy can exceed 0.85 two and half months
ahead of harvest. Overall, these results highlight unique advantages of our
approach in transferring historical knowledge and maximizing the timeliness of
crop maps. Our approach supports a general paradigm shift towards learning
transferrable and generalizable knowledge to facilitate land cover
classification.

    

### [[2110.10279] Factorization Approach for Low-complexity Matrix Completion Problems: Exponential Number of Spurious Solutions and Failure of Gradient Methods](http://arxiv.org/abs/2110.10279)


  It is well-known that the Burer-Monteiro (B-M) factorization approach can
efficiently solve low-rank matrix optimization problems under the RIP
condition. It is natural to ask whether B-M factorization-based methods can
succeed on any low-rank matrix optimization problems with a low
information-theoretic complexity, i.e., polynomial-time solvable problems that
have a unique solution. In this work, we provide a negative answer to the above
question. We investigate the landscape of B-M factorized polynomial-time
solvable matrix completion (MC) problems, which are the most popular subclass
of low-rank matrix optimization problems without the RIP condition. We
construct an instance of polynomial-time solvable MC problems with
exponentially many spurious local minima, which leads to the failure of most
gradient-based methods. Based on those results, we define a new complexity
metric that potentially measures the solvability of low-rank matrix
optimization problems based on the B-M factorization approach. In addition, we
show that more measurements of the ground truth matrix can deteriorate the
landscape, which further reveals the unfavorable behavior of the B-M
factorization on general low-rank matrix optimization problems.

    

### [[2110.10281] Joint Gaussian Graphical Model Estimation: A Survey](http://arxiv.org/abs/2110.10281)


  Graphs from complex systems often share a partial underlying structure across
domains while retaining individual features. Thus, identifying common
structures can shed light on the underlying signal, for instance, when applied
to scientific discoveries or clinical diagnoses. Furthermore, growing evidence
shows that the shared structure across domains boosts the estimation power of
graphs, particularly for high-dimensional data. However, building a joint
estimator to extract the common structure may be more complicated than it
seems, most often due to data heterogeneity across sources. This manuscript
surveys recent work on statistical inference of joint Gaussian graphical
models, identifying model structures that fit various data generation
processes. Simulations under different data generation processes are
implemented with detailed discussions on the choice of models.

    

### [[2110.10286] Robust Semi-Supervised Classification using GANs with Self-Organizing Maps](http://arxiv.org/abs/2110.10286)


  Generative adversarial networks (GANs) have shown tremendous promise in
learning to generate data and effective at aiding semi-supervised
classification. However, to this point, semi-supervised GAN methods make the
assumption that the unlabeled data set contains only samples of the joint
distribution of the classes of interest, referred to as inliers. Consequently,
when presented with a sample from other distributions, referred to as outliers,
GANs perform poorly at determining that it is not qualified to make a decision
on the sample. The problem of discriminating outliers from inliers while
maintaining classification accuracy is referred to here as the DOIC problem. In
this work, we describe an architecture that combines self-organizing maps
(SOMs) with SS-GANS with the goal of mitigating the DOIC problem and
experimental results indicating that the architecture achieves the goal.
Multiple experiments were conducted on hyperspectral image data sets. The
SS-GANS performed slightly better than supervised GANS on classification
problems with and without the SOM. Incorporating the SOMs into the SS-GANs and
the supervised GANS led to substantially mitigation of the DOIC problem when
compared to SS-GANS and GANs without the SOMs. Furthermore, the SS-GANS
performed much better than GANS on the DOIC problem, even without the SOMs.

    

### [[2110.10287] Multi-concept adversarial attacks](http://arxiv.org/abs/2110.10287)


  As machine learning (ML) techniques are being increasingly used in many
applications, their vulnerability to adversarial attacks becomes well-known.
Test time attacks, usually launched by adding adversarial noise to test
instances, have been shown effective against the deployed ML models. In
practice, one test input may be leveraged by different ML models. Test time
attacks targeting a single ML model often neglect their impact on other ML
models. In this work, we empirically demonstrate that naively attacking the
classifier learning one concept may negatively impact classifiers trained to
learn other concepts. For example, for the online image classification
scenario, when the Gender classifier is under attack, the (wearing) Glasses
classifier is simultaneously attacked with the accuracy dropped from 98.69 to
88.42. This raises an interesting question: is it possible to attack one set of
classifiers without impacting the other set that uses the same test instance?
Answers to the above research question have interesting implications for
protecting privacy against ML model misuse. Attacking ML models that pose
unnecessary risks of privacy invasion can be an important tool for protecting
individuals from harmful privacy exploitation. In this paper, we address the
above research question by developing novel attack techniques that can
simultaneously attack one set of ML models while preserving the accuracy of the
other. In the case of linear classifiers, we provide a theoretical framework
for finding an optimal solution to generate such adversarial examples. Using
this theoretical framework, we develop a multi-concept attack strategy in the
context of deep learning. Our results demonstrate that our techniques can
successfully attack the target classes while protecting the protected classes
in many different settings, which is not possible with the existing test-time
attack-single strategies.

    

### [[2110.10289] On Coordinate Decoding for Keypoint Estimation Tasks](http://arxiv.org/abs/2110.10289)


  A series of 2D (and 3D) keypoint estimation tasks are built upon heatmap
coordinate representation, i.e. a probability map that allows for learnable and
spatially aware encoding and decoding of keypoint coordinates on grids, even
allowing for sub-pixel coordinate accuracy. In this report, we aim to reproduce
the findings of DARK that investigated the 2D heatmap representation by
highlighting the importance of the encoding of the ground truth heatmap and the
decoding of the predicted heatmap to keypoint coordinates. The authors claim
that a) a more principled distribution-aware coordinate decoding method
overcomes the limitations of the standard techniques widely used in the
literature, and b), that the reconstruction of heatmaps from ground-truth
coordinates by generating accurate and continuous heatmap distributions lead to
unbiased model training, contrary to the standard coordinate encoding process
that quantizes the keypoint coordinates on the resolution of the input image
grid.

    

### [[2110.10293] Learning Rich Nearest Neighbor Representations from Self-supervised Ensembles](http://arxiv.org/abs/2110.10293)


  Pretraining convolutional neural networks via self-supervision, and applying
them in transfer learning, is an incredibly fast-growing field that is rapidly
and iteratively improving performance across practically all image domains.
Meanwhile, model ensembling is one of the most universally applicable
techniques in supervised learning literature and practice, offering a simple
solution to reliably improve performance. But how to optimally combine
self-supervised models to maximize representation quality has largely remained
unaddressed. In this work, we provide a framework to perform self-supervised
model ensembling via a novel method of learning representations directly
through gradient descent at inference time. This technique improves
representation quality, as measured by k-nearest neighbors, both on the
in-domain dataset and in the transfer setting, with models transferable from
the former setting to the latter. Additionally, this direct learning of feature
through backpropagation improves representations from even a single model,
echoing the improvements found in self-distillation.

    

### [[2110.10295] Expressivity of Neural Networks via Chaotic Itineraries beyond Sharkovsky's Theorem](http://arxiv.org/abs/2110.10295)


  Given a target function $f$, how large must a neural network be in order to
approximate $f$? Recent works examine this basic question on neural network
\textit{expressivity} from the lens of dynamical systems and provide novel
``depth-vs-width'' tradeoffs for a large family of functions $f$. They suggest
that such tradeoffs are governed by the existence of \textit{periodic} points
or \emph{cycles} in $f$. Our work, by further deploying dynamical systems
concepts, illuminates a more subtle connection between periodicity and
expressivity: we prove that periodic points alone lead to suboptimal
depth-width tradeoffs and we improve upon them by demonstrating that certain
``chaotic itineraries'' give stronger exponential tradeoffs, even in regimes
where previous analyses only imply polynomial gaps. Contrary to prior works,
our bounds are nearly-optimal, tighten as the period increases, and handle
strong notions of inapproximability (e.g., constant $L_1$ error). More broadly,
we identify a phase transition to the \textit{chaotic regime} that exactly
coincides with an abrupt shift in other notions of function complexity,
including VC-dimension and topological entropy.

    

### [[2110.10302] Layer-wise Adaptive Model Aggregation for Scalable Federated Learning](http://arxiv.org/abs/2110.10302)


  In Federated Learning, a common approach for aggregating local models across
clients is periodic averaging of the full model parameters. It is, however,
known that different layers of neural networks can have a different degree of
model discrepancy across the clients. The conventional full aggregation scheme
does not consider such a difference and synchronizes the whole model parameters
at once, resulting in inefficient network bandwidth consumption. Aggregating
the parameters that are similar across the clients does not make meaningful
training progress while increasing the communication cost. We propose FedLAMA,
a layer-wise model aggregation scheme for scalable Federated Learning. FedLAMA
adaptively adjusts the aggregation interval in a layer-wise manner, jointly
considering the model discrepancy and the communication cost. The layer-wise
aggregation method enables to finely control the aggregation interval to relax
the aggregation frequency without a significant impact on the model accuracy.
Our empirical study shows that FedLAMA reduces the communication cost by up to
60% for IID data and 70% for non-IID data while achieving a comparable accuracy
to FedAvg.

    

### [[2110.10303] Momentum Contrastive Autoencoder: Using Contrastive Learning for Latent Space Distribution Matching in WAE](http://arxiv.org/abs/2110.10303)


  Wasserstein autoencoder (WAE) shows that matching two distributions is
equivalent to minimizing a simple autoencoder (AE) loss under the constraint
that the latent space of this AE matches a pre-specified prior distribution.
This latent space distribution matching is a core component of WAE, and a
challenging task. In this paper, we propose to use the contrastive learning
framework that has been shown to be effective for self-supervised
representation learning, as a means to resolve this problem. We do so by
exploiting the fact that contrastive learning objectives optimize the latent
space distribution to be uniform over the unit hyper-sphere, which can be
easily sampled from. We show that using the contrastive learning framework to
optimize the WAE loss achieves faster convergence and more stable optimization
compared with existing popular algorithms for WAE. This is also reflected in
the FID scores on CelebA and CIFAR-10 datasets, and the realistic generated
image quality on the CelebA-HQ dataset.

    

### [[2110.10305] When in Doubt, Summon the Titans: Efficient Inference with Large Models](http://arxiv.org/abs/2110.10305)


  Scaling neural networks to "large" sizes, with billions of parameters, has
been shown to yield impressive results on many challenging problems. However,
the inference cost incurred by such large models often prevents their
application in most real-world settings. In this paper, we propose a two-stage
framework based on distillation that realizes the modelling benefits of the
large models, while largely preserving the computational benefits of inference
with more lightweight models. In a nutshell, we use the large teacher models to
guide the lightweight student models to only make correct predictions on a
subset of "easy" examples; for the "hard" examples, we fall-back to the
teacher. Such an approach allows us to efficiently employ large models in
practical scenarios where easy examples are much more frequent than rare hard
examples. Our proposed use of distillation to only handle easy instances allows
for a more aggressive trade-off in the student size, thereby reducing the
amortized cost of inference and achieving better accuracy than standard
distillation. Empirically, we demonstrate the benefits of our approach on both
image classification and natural language processing benchmarks.

    

### [[2110.10318] Improved Multilingual Language Model Pretraining for Social Media Text via Translation Pair Prediction](http://arxiv.org/abs/2110.10318)


  We evaluate a simple approach to improving zero-shot multilingual transfer of
mBERT on social media corpus by adding a pretraining task called translation
pair prediction (TPP), which predicts whether a pair of cross-lingual texts are
a valid translation. Our approach assumes access to translations (exact or
approximate) between source-target language pairs, where we fine-tune a model
on source language task data and evaluate the model in the target language. In
particular, we focus on language pairs where transfer learning is difficult for
mBERT: those where source and target languages are different in script,
vocabulary, and linguistic typology. We show improvements from TPP pretraining
over mBERT alone in zero-shot transfer from English to Hindi, Arabic, and
Japanese on two social media tasks: NER (a 37% average relative improvement in
F1 across target languages) and sentiment classification (12% relative
improvement in F1) on social media text, while also benchmarking on a
non-social media task of Universal Dependency POS tagging (6.7% relative
improvement in accuracy). Our results are promising given the lack of social
media bitext corpus. Our code can be found at:
this https URL.

    

### [[2110.10319] LMSOC: An Approach for Socially Sensitive Pretraining](http://arxiv.org/abs/2110.10319)


  While large-scale pretrained language models have been shown to learn
effective linguistic representations for many NLP tasks, there remain many
real-world contextual aspects of language that current approaches do not
capture. For instance, consider a cloze-test "I enjoyed the ____ game this
weekend": the correct answer depends heavily on where the speaker is from, when
the utterance occurred, and the speaker's broader social milieu and
preferences. Although language depends heavily on the geographical, temporal,
and other social contexts of the speaker, these elements have not been
incorporated into modern transformer-based language models. We propose a simple
but effective approach to incorporate speaker social context into the learned
representations of large-scale language models. Our method first learns dense
representations of social contexts using graph representation learning
algorithms and then primes language model pretraining with these social context
representations. We evaluate our approach on geographically-sensitive
language-modeling tasks and show a substantial improvement (more than 100%
relative lift on MRR) compared to baselines.

    

### [[2110.10320] Frontiers in Evolutionary Computation: A Workshop Report](http://arxiv.org/abs/2110.10320)


  In July of 2021, the Santa Fe Institute hosted a workshop on evolutionary
computation as part of its Foundations of Intelligence in Natural and
Artificial Systems project. This project seeks to advance the field of
artificial intelligence by promoting interdisciplinary research on the nature
of intelligence. The workshop brought together computer scientists and
biologists to share their insights about the nature of evolution and the future
of evolutionary computation. In this report, we summarize each of the talks and
the subsequent discussions. We also draw out a number of key themes and
identify important frontiers for future research.

    

### [[2110.10323] Computational Graph Completion](http://arxiv.org/abs/2110.10323)


  We introduce a framework for generating, organizing, and reasoning with
computational knowledge. It is motivated by the observation that most problems
in Computational Sciences and Engineering (CSE) can be described as that of
completing (from data) a computational graph representing dependencies between
functions and variables. Functions and variables may be known, unknown, or
random. Data comes in the form of observations of distinct values of a finite
number of subsets of the variables of the graph. The underlying problem
combines a regression problem (approximating unknown functions) with a matrix
completion problem (recovering unobserved variables in the data). Replacing
unknown functions by Gaussian Processes (GPs) and conditioning on observed data
provides a simple but efficient approach to completing such graphs. Since the
proposed framework is highly expressive, it has a vast potential application
scope. Since the completion process can be automatized, as one solves
$\sqrt{\sqrt{2}+\sqrt{3}}$ on a pocket calculator without thinking about it,
one could, with the proposed framework, solve a complex CSE problem by drawing
a diagram. Compared to traditional kriging, the proposed framework can be used
to recover unknown functions with much scarcer data by exploiting
interdependencies between multiple functions and variables. The Computational
Graph Completion (CGC) problem addressed by the proposed framework could
therefore also be interpreted as a generalization of that of solving linear
systems of equations to that of approximating unknown variables and functions
with noisy, incomplete, and nonlinear dependencies. Numerous examples
illustrate the flexibility, scope, efficacy, and robustness of the CGC
framework and show how it can be used as a pathway to identifying simple
solutions to classical CSE problems (digital twin modeling, dimension
reduction, mode decomposition, etc.).

    

### [[2110.10325] One-Step Abductive Multi-Target Learning with Diverse Noisy Samples](http://arxiv.org/abs/2110.10325)


  One-step abductive multi-target learning (OSAMTL) was proposed to handle
complex noisy labels. In this paper, giving definition of diverse noisy samples
(DNS), we propose one-step abductive multi-target learning with DNS
(OSAMTL-DNS) to expand the original OSAMTL to a wider range of tasks that
handle complex noisy labels.

    

### [[2110.10329] SLAM: A Unified Encoder for Speech and Language Modeling via Speech-Text Joint Pre-Training](http://arxiv.org/abs/2110.10329)


  Unsupervised pre-training is now the predominant approach for both text and
speech understanding. Self-attention models pre-trained on large amounts of
unannotated data have been hugely successful when fine-tuned on downstream
tasks from a variety of domains and languages. This paper takes the
universality of unsupervised language pre-training one step further, by
unifying speech and text pre-training within a single model. We build a single
encoder with the BERT objective on unlabeled text together with the w2v-BERT
objective on unlabeled speech. To further align our model representations
across modalities, we leverage alignment losses, specifically Translation
Language Modeling (TLM) and Speech Text Matching (STM) that make use of
supervised speech-text recognition data. We demonstrate that incorporating both
speech and text data during pre-training can significantly improve downstream
quality on CoVoST~2 speech translation, by around 1 BLEU compared to
single-modality pre-trained models, while retaining close to SotA performance
on LibriSpeech and SpeechStew ASR tasks. On four GLUE tasks and
text-normalization, we observe evidence of capacity limitations and
interference between the two modalities, leading to degraded performance
compared to an equivalent text-only model, while still being competitive with
BERT. Through extensive empirical analysis we also demonstrate the importance
of the choice of objective function for speech pre-training, and the beneficial
effect of adding additional supervised signals on the quality of the learned
representations.

    

### [[2110.10342] Minibatch vs Local SGD with Shuffling: Tight Convergence Bounds and Beyond](http://arxiv.org/abs/2110.10342)


  In distributed learning, local SGD (also known as federated averaging) and
its simple baseline minibatch SGD are widely studied optimization methods. Most
existing analyses of these methods assume independent and unbiased gradient
estimates obtained via with-replacement sampling. In contrast, we study
shuffling-based variants: minibatch and local Random Reshuffling, which draw
stochastic gradients without replacement and are thus closer to practice. For
smooth functions satisfying the Polyak-ojasiewicz condition, we obtain
convergence bounds (in the large epoch regime) which show that these
shuffling-based variants converge faster than their with-replacement
counterparts. Moreover, we prove matching lower bounds showing that our
convergence analysis is tight. Finally, we propose an algorithmic modification
called synchronized shuffling that leads to convergence rates faster than our
lower bounds in near-homogeneous settings.

    

### [[2110.10343] EBJR: Energy-Based Joint Reasoning for Adaptive Inference](http://arxiv.org/abs/2110.10343)


  State-of-the-art deep learning models have achieved significant performance
levels on various benchmarks. However, the excellent performance comes at a
cost of inefficient computational cost. Light-weight architectures, on the
other hand, achieve moderate accuracies, but at a much more desirable latency.
This paper presents a new method of jointly using the large accurate models
together with the small fast ones. To this end, we propose an Energy-Based
Joint Reasoning (EBJR) framework that adaptively distributes the samples
between shallow and deep models to achieve an accuracy close to the deep model,
but latency close to the shallow one. Our method is applicable to
out-of-the-box pre-trained models as it does not require an architecture change
nor re-training. Moreover, it is easy to use and deploy, especially for cloud
services. Through a comprehensive set of experiments on different down-stream
tasks, we show that our method outperforms strong state-of-the-art approaches
with a considerable margin. In addition, we propose specialized EBJR, an
extension of our method where we create a smaller specialized side model that
performs the target task only partially, but yields an even higher accuracy and
faster inference. We verify the strengths of our methods with both theoretical
and experimental evaluations.

    

### [[2110.10349] Distributed Reinforcement Learning for Privacy-Preserving Dynamic Edge Caching](http://arxiv.org/abs/2110.10349)


  Mobile edge computing (MEC) is a prominent computing paradigm which expands
the application fields of wireless communication. Due to the limitation of the
capacities of user equipments and MEC servers, edge caching (EC) optimization
is crucial to the effective utilization of the caching resources in MEC-enabled
wireless networks. However, the dynamics and complexities of content
popularities over space and time as well as the privacy preservation of users
pose significant challenges to EC optimization. In this paper, a
privacy-preserving distributed deep deterministic policy gradient (P2D3PG)
algorithm is proposed to maximize the cache hit rates of devices in the MEC
networks. Specifically, we consider the fact that content popularities are
dynamic, complicated and unobservable, and formulate the maximization of cache
hit rates on devices as distributed problems under the constraints of privacy
preservation. In particular, we convert the distributed optimizations into
distributed model-free Markov decision process problems and then introduce a
privacy-preserving federated learning method for popularity prediction.
Subsequently, a P2D3PG algorithm is developed based on distributed
reinforcement learning to solve the distributed problems. Simulation results
demonstrate the superiority of the proposed approach in improving EC hit rate
over the baseline methods while preserving user privacy.

    

### [[2110.10351] Faster Algorithm and Sharper Analysis for Constrained Markov Decision Process](http://arxiv.org/abs/2110.10351)


  The problem of constrained Markov decision process (CMDP) is investigated,
where an agent aims to maximize the expected accumulated discounted reward
subject to multiple constraints on its utilities/costs. A new primal-dual
approach is proposed with a novel integration of three ingredients: entropy
regularized policy optimizer, dual variable regularizer, and Nesterov's
accelerated gradient descent dual optimizer, all of which are critical to
achieve a faster convergence. The finite-time error bound of the proposed
approach is characterized. Despite the challenge of the nonconcave objective
subject to nonconcave constraints, the proposed approach is shown to converge
to the global optimum with a complexity of $\tilde{\mathcal O}(1/\epsilon)$ in
terms of the optimality gap and the constraint violation, which improves the
complexity of the existing primal-dual approach by a factor of $\mathcal
O(1/\epsilon)$ \citep{ding2020natural,paternain2019constrained}. This is the
first demonstration that nonconcave CMDP problems can attain the complexity
lower bound of $\mathcal O(1/\epsilon)$ for convex optimization subject to
convex constraints. Our primal-dual approach and non-asymptotic analysis are
agnostic to the RL optimizer used, and thus are more flexible for practical
applications. More generally, our approach also serves as the first algorithm
that provably accelerates constrained nonconvex optimization with zero duality
gap by exploiting the geometries such as the gradient dominance condition, for
which the existing acceleration methods for constrained convex optimization are
not applicable.

    

### [[2110.10354] Detecting Backdoor Attacks Against Point Cloud Classifiers](http://arxiv.org/abs/2110.10354)


  Backdoor attacks (BA) are an emerging threat to deep neural network
classifiers. A classifier being attacked will predict to the attacker's target
class when a test sample from a source class is embedded with the backdoor
pattern (BP). Recently, the first BA against point cloud (PC) classifiers was
proposed, creating new threats to many important applications including
autonomous driving. Such PC BAs are not detectable by existing BA defenses due
to their special BP embedding mechanism. In this paper, we propose a
reverse-engineering defense that infers whether a PC classifier is backdoor
attacked, without access to its training set or to any clean classifiers for
reference. The effectiveness of our defense is demonstrated on the benchmark
ModeNet40 dataset for PCs.

    

### [[2110.10366] Repaint: Improving the Generalization of Down-Stream Visual Tasks by Generating Multiple Instances of Training Examples](http://arxiv.org/abs/2110.10366)


  Convolutional Neural Networks (CNNs) for visual tasks are believed to learn
both the low-level textures and high-level object attributes, throughout the
network depth. This paper further investigates the `texture bias' in CNNs. To
this end, we regenerate multiple instances of training examples from each
original image, through a process we call `repainting'. The repainted examples
preserve the shape and structure of the regions and objects within the scenes,
but diversify their texture and color. Our method can regenerate a same image
at different daylight, season, or weather conditions, can have colorization or
de-colorization effects, or even bring back some texture information from
blacked-out areas. The in-place repaint allows us to further use these
repainted examples for improving the generalization of CNNs. Through an
extensive set of experiments, we demonstrate the usefulness of the repainted
examples in training, for the tasks of image classification (ImageNet) and
object detection (COCO), over several state-of-the-art network architectures at
different capacities, and across different data availability regimes.

    

### [[2110.10368] ABC: Auxiliary Balanced Classifier for Class-imbalanced Semi-supervised Learning](http://arxiv.org/abs/2110.10368)


  Existing semi-supervised learning (SSL) algorithms typically assume
class-balanced datasets, although the class distributions of many real-world
datasets are imbalanced. In general, classifiers trained on a class-imbalanced
dataset are biased toward the majority classes. This issue becomes more
problematic for SSL algorithms because they utilize the biased prediction of
unlabeled data for training. However, traditional class-imbalanced learning
techniques, which are designed for labeled data, cannot be readily combined
with SSL algorithms. We propose a scalable class-imbalanced SSL algorithm that
can effectively use unlabeled data, while mitigating class imbalance by
introducing an auxiliary balanced classifier (ABC) of a single layer, which is
attached to a representation layer of an existing SSL algorithm. The ABC is
trained with a class-balanced loss of a minibatch, while using high-quality
representations learned from all data points in the minibatch using the
backbone SSL algorithm to avoid overfitting and information loss.Moreover, we
use consistency regularization, a recent SSL technique for utilizing unlabeled
data in a modified way, to train the ABC to be balanced among the classes by
selecting unlabeled data with the same probability for each class. The proposed
algorithm achieves state-of-the-art performance in various class-imbalanced SSL
experiments using four benchmark datasets.

    

### [[2110.10369] Model Composition: Can Multiple Neural Networks Be Combined into a Single Network Using Only Unlabeled Data?](http://arxiv.org/abs/2110.10369)


  The diversity of deep learning applications, datasets, and neural network
architectures necessitates a careful selection of the architecture and data
that match best to a target application. As an attempt to mitigate this
dilemma, this paper investigates the idea of combining multiple trained neural
networks using unlabeled data. In addition, combining multiple models into one
can speed up the inference, result in stronger, more capable models, and allows
us to select efficient device-friendly target network architectures. To this
end, the proposed method makes use of generation, filtering, and aggregation of
reliable pseudo-labels collected from unlabeled data. Our method supports using
an arbitrary number of input models with arbitrary architectures and
categories. Extensive performance evaluations demonstrated that our method is
very effective. For example, for the task of object detection and without using
any ground-truth labels, an EfficientDet-D0 trained on Pascal-VOC and an
EfficientDet-D1 trained on COCO, can be combined to a RetinaNet-ResNet50 model,
with a similar mAP as the supervised training. If fine-tuned in a
semi-supervised setting, the combined model achieves +18.6%, +12.6%, and +8.1%
mAP improvements over supervised training with 1%, 5%, and 10% of labels.

    

### [[2110.10379] Cascaded Compressed Sensing Networks: A Reversible Architecture for Layerwise Learning](http://arxiv.org/abs/2110.10379)


  Recently, the method that learns networks layer by layer has attracted
increasing interest for its ease of analysis. For the method, the main
challenge lies in deriving an optimization target for each layer by inversely
propagating the global target of the network. The propagation problem is ill
posed, due to involving the inversion of nonlinear activations from
lowdimensional to high-dimensional spaces. To address the problem, the existing
solution is to learn an auxiliary network to specially propagate the target.
However, the network lacks stability, and moreover, it results in higher
complexity for network learning. In the letter, we show that target propagation
could be achieved by modeling the network s each layer with compressed sensing,
without the need of auxiliary networks. Experiments show that the proposed
method could achieve better performance than the auxiliary network-based
method.

    

### [[2110.10380] Learning to Remember Patterns: Pattern Matching Memory Networks for Traffic Forecasting](http://arxiv.org/abs/2110.10380)


  Traffic forecasting is a challenging problem due to complex road networks and
sudden speed changes caused by various events on roads. A number of models have
been proposed to solve this challenging problem with a focus on learning
spatio-temporal dependencies of roads. In this work, we propose a new
perspective of converting the forecasting problem into a pattern matching task,
assuming that large data can be represented by a set of patterns. To evaluate
the validness of the new perspective, we design a novel traffic forecasting
model, called Pattern-Matching Memory Networks (PM-MemNet), which learns to
match input data to the representative patterns with a key-value memory
structure. We first extract and cluster representative traffic patterns, which
serve as keys in the memory. Then via matching the extracted keys and inputs,
PM-MemNet acquires necessary information of existing traffic patterns from the
memory and uses it for forecasting. To model spatio-temporal correlation of
traffic, we proposed novel memory architecture GCMem, which integrates
attention and graph convolution for memory enhancement. The experiment results
indicate that PM-MemNet is more accurate than state-of-the-art models, such as
Graph WaveNet with higher responsiveness. We also present a qualitative
analysis result, describing how PM-MemNet works and achieves its higher
accuracy when road speed rapidly changes.

    

### [[2110.10381] Medical Knowledge-Guided Deep Curriculum Learning for Elbow Fracture Diagnosis from X-Ray Images](http://arxiv.org/abs/2110.10381)


  Elbow fractures are one of the most common fracture types. Diagnoses on elbow
fractures often need the help of radiographic imaging to be read and analyzed
by a specialized radiologist with years of training. Thanks to the recent
advances of deep learning, a model that can classify and detect different types
of bone fractures needs only hours of training and has shown promising results.
However, most existing deep learning models are purely data-driven, lacking
incorporation of known domain knowledge from human experts. In this work, we
propose a novel deep learning method to diagnose elbow fracture from elbow
X-ray images by integrating domain-specific medical knowledge into a curriculum
learning framework. In our method, the training data are permutated by sampling
without replacement at the beginning of each training epoch. The sampling
probability of each training sample is guided by a scoring criterion
constructed based on clinically known knowledge from human experts, where the
scoring indicates the diagnosis difficultness of different elbow fracture
subtypes. We also propose an algorithm that updates the sampling probabilities
at each epoch, which is applicable to other sampling-based curriculum learning
frameworks. We design an experiment with 1865 elbow X-ray images for a
fracture/normal binary classification task and compare our proposed method to a
baseline method and a previous method using multiple metrics. Our results show
that the proposed method achieves the highest classification performance. Also,
our proposed probability update algorithm boosts the performance of the
previous method.

    

### [[2110.10391] Robust lEarned Shrinkage-Thresholding (REST): Robust unrolling for sparse recover](http://arxiv.org/abs/2110.10391)


  In this paper, we consider deep neural networks for solving inverse problems
that are robust to forward model mis-specifications. Specifically, we treat
sensing problems with model mismatch where one wishes to recover a sparse
high-dimensional vector from low-dimensional observations subject to
uncertainty in the measurement operator. We then design a new robust deep
neural network architecture by applying algorithm unfolding techniques to a
robust version of the underlying recovery problem. Our proposed network - named
Robust lEarned Shrinkage-Thresholding (REST) - exhibits an additional
normalization processing compared to Learned Iterative Shrinkage-Thresholding
Algorithm (LISTA), leading to reliable recovery of the signal under sample-wise
varying model mismatch. The proposed REST network is shown to outperform
state-of-the-art model-based and data-driven algorithms in both compressive
sensing and radar imaging problems wherein model mismatch is taken into
consideration.

    

### [[2110.10394] Deep Learning for HDR Imaging: State-of-the-Art and Future Trends](http://arxiv.org/abs/2110.10394)


  High dynamic range (HDR) imaging is a technique that allows an extensive
dynamic range of exposures, which is important in image processing, computer
graphics, and computer vision. In recent years, there has been a significant
advancement in HDR imaging using deep learning (DL). This study conducts a
comprehensive and insightful survey and analysis of recent developments in deep
HDR imaging methodologies. We hierarchically and structurally group existing
deep HDR imaging methods into five categories based on (1) number/domain of
input exposures, (2) number of learning tasks, (3) novel sensor data, (4) novel
learning strategies, and (5) applications. Importantly, we provide a
constructive discussion on each category regarding its potential and
challenges. Moreover, we review some crucial aspects of deep HDR imaging, such
as datasets and evaluation metrics. Finally, we highlight some open problems
and point out future research directions.

    

### [[2110.10402] An Investigation of Enhancing CTC Model for Triggered Attention-based Streaming ASR](http://arxiv.org/abs/2110.10402)


  In the present paper, an attempt is made to combine Mask-CTC and the
triggered attention mechanism to construct a streaming end-to-end automatic
speech recognition (ASR) system that provides high performance with low
latency. The triggered attention mechanism, which performs autoregressive
decoding triggered by the CTC spike, has shown to be effective in streaming
ASR. However, in order to maintain high accuracy of alignment estimation based
on CTC outputs, which is the key to its performance, it is inevitable that
decoding should be performed with some future information input (i.e., with
higher latency). It should be noted that in streaming ASR, it is desirable to
be able to achieve high recognition accuracy while keeping the latency low.
Therefore, the present study aims to achieve highly accurate streaming ASR with
low latency by introducing Mask-CTC, which is capable of learning feature
representations that anticipate future information (i.e., that can consider
long-term contexts), to the encoder pre-training. Experimental comparisons
conducted using WSJ data demonstrate that the proposed method achieves higher
accuracy with lower latency than the conventional triggered attention-based
streaming ASR system.

    

### [[2110.10403] AFTer-UNet: Axial Fusion Transformer UNet for Medical Image Segmentation](http://arxiv.org/abs/2110.10403)


  Recent advances in transformer-based models have drawn attention to exploring
these techniques in medical image segmentation, especially in conjunction with
the U-Net model (or its variants), which has shown great success in medical
image segmentation, under both 2D and 3D settings. Current 2D based methods
either directly replace convolutional layers with pure transformers or consider
a transformer as an additional intermediate encoder between the encoder and
decoder of U-Net. However, these approaches only consider the attention
encoding within one single slice and do not utilize the axial-axis information
naturally provided by a 3D volume. In the 3D setting, convolution on volumetric
data and transformers both consume large GPU memory. One has to either
downsample the image or use cropped local patches to reduce GPU memory usage,
which limits its performance. In this paper, we propose Axial Fusion
Transformer UNet (AFTer-UNet), which takes both advantages of convolutional
layers' capability of extracting detailed features and transformers' strength
on long sequence modeling. It considers both intra-slice and inter-slice
long-range cues to guide the segmentation. Meanwhile, it has fewer parameters
and takes less GPU memory to train than the previous transformer-based models.
Extensive experiments on three multi-organ segmentation datasets demonstrate
that our method outperforms current state-of-the-art methods.

    

### [[2110.10404] JavaBERT: Training a transformer-based model for the Java programming language](http://arxiv.org/abs/2110.10404)


  Code quality is and will be a crucial factor while developing new software
code, requiring appropriate tools to ensure functional and reliable code.
Machine learning techniques are still rarely used for software engineering
tools, missing out the potential benefits of its application. Natural language
processing has shown the potential to process text data regarding a variety of
tasks. We argue, that such models can also show similar benefits for software
code processing. In this paper, we investigate how models used for natural
language processing can be trained upon software code. We introduce a data
retrieval pipeline for software code and train a model upon Java software code.
The resulting model, JavaBERT, shows a high accuracy on the masked language
modeling task showing its potential for software engineering tools.

    

### [[2110.10422] Encoding spatiotemporal priors with VAEs for small-area estimation](http://arxiv.org/abs/2110.10422)


  Gaussian processes (GPs), implemented through multivariate Gaussian
distributions for a finite collection of data, are the most popular approach in
small-area spatiotemporal statistical modelling. In this context they are used
to encode correlation structures over space and time and can generalise well in
interpolation tasks. Despite their flexibility, off-the-shelf GPs present
serious computational challenges which limit their scalability and practical
usefulness in applied settings. Here, we propose a novel, deep generative
modelling approach to tackle this challenge: for a particular spatiotemporal
setting, we approximate a class of GP priors through prior sampling and
subsequent fitting of a variational autoencoder (VAE). Given a trained VAE, the
resultant decoder allows spatiotemporal inference to become incredibly
efficient due to the low dimensional, independently distributed latent Gaussian
space representation of the VAE. Once trained, inference using the VAE decoder
replaces the GP within a Bayesian sampling framework. This approach provides
tractable and easy-to-implement means of approximately encoding spatiotemporal
priors and facilitates efficient statistical inference. We demonstrate the
utility of our VAE two stage approach on Bayesian, small-area estimation tasks.

    

### [[2110.10423] ProxyBO: Accelerating Neural Architecture Search via Bayesian Optimization with Zero-cost Proxies](http://arxiv.org/abs/2110.10423)


  Designing neural architectures requires immense manual efforts. This has
promoted the development of neural architecture search (NAS) to automate this
design. While previous NAS methods achieve promising results but run slowly and
zero-cost proxies run extremely fast but are less promising, recent work
considers utilizing zero-cost proxies via a simple warm-up. The existing method
has two limitations, which are unforeseeable reliability and one-shot usage. To
address the limitations, we present ProxyBO, an efficient Bayesian optimization
framework that utilizes the zero-cost proxies to accelerate neural architecture
search. We propose the generalization ability measurement to estimate the
fitness of proxies on the task during each iteration and then combine BO with
zero-cost proxies via dynamic influence combination. Extensive empirical
studies show that ProxyBO consistently outperforms competitive baselines on
five tasks from three public benchmarks. Concretely, ProxyBO achieves up to
5.41x and 3.83x speedups over the state-of-the-art approach REA and BRP-NAS,
respectively.

    

### [[2110.10428] Reconstruction of Fragmented Trajectories of Collective Motion using Hadamard Deep Autoencoders](http://arxiv.org/abs/2110.10428)


  Learning dynamics of collectively moving agents such as fish or humans is an
active field in research. Due to natural phenomena such as occlusion and change
of illumination, the multi-object methods tracking such dynamics might lose
track of the agents where that might result fragmentation in the constructed
trajectories. Here, we present an extended deep autoencoder (DA) that we train
only on fully observed segments of the trajectories by defining its loss
function as the Hadamard product of a binary indicator matrix with the absolute
difference between the outputs and the labels. The trajectories of the agents
practicing collective motion is low-rank due to mutual interactions and
dependencies between the agents that we utilize as the underlying pattern that
our Hadamard deep autoencoder (HDA) codes during its training. The performance
of our HDA is compared with that of a low-rank matrix completion scheme in the
context of fragmented trajectory reconstruction.

    

### [[2110.10429] Knowledge distillation from language model to acoustic model: a hierarchical multi-task learning approach](http://arxiv.org/abs/2110.10429)


  The remarkable performance of the pre-trained language model (LM) using
self-supervised learning has led to a major paradigm shift in the study of
natural language processing. In line with these changes, leveraging the
performance of speech recognition systems with massive deep learning-based LMs
is a major topic of speech recognition research. Among the various methods of
applying LMs to speech recognition systems, in this paper, we focus on a
cross-modal knowledge distillation method that transfers knowledge between two
types of deep neural networks with different modalities. We propose an acoustic
model structure with multiple auxiliary output layers for cross-modal
distillation and demonstrate that the proposed method effectively compensates
for the shortcomings of the existing label-interpolation-based distillation
method. In addition, we extend the proposed method to a hierarchical
distillation method using LMs trained in different units (senones, monophones,
and subwords) and reveal the effectiveness of the hierarchical distillation
method through an ablation study.

    

### [[2110.10441] Feedback Linearization of Car Dynamics for Racing via Reinforcement Learning](http://arxiv.org/abs/2110.10441)


  Through the method of Learning Feedback Linearization, we seek to learn a
linearizing controller to simplify the process of controlling a car to race
autonomously. A soft actor-critic approach is used to learn a decoupling matrix
and drift vector that effectively correct for errors in a hand-designed
linearizing controller. The result is an exactly linearizing controller that
can be used to enable the well-developed theory of linear systems to design
path planning and tracking schemes that are easy to implement and significantly
less computationally demanding. To demonstrate the method of feedback
linearization, it is first used to learn a simulated model whose exact
structure is known, but varied from the initial controller, so as to introduce
error. We further seek to apply this method to a system that introduces even
more error in the form of a gym environment specifically designed for modeling
the dynamics of car racing. To do so, we posit an extension to the method of
learning feedback linearization; a neural network that is trained using
supervised learning to convert the output of our linearizing controller to the
required input for the racing environment. Our progress towards these goals is
reported and the next steps in their accomplishment are discussed.

    

### [[2110.10461] Scalable One-Pass Optimisation of High-Dimensional Weight-Update Hyperparameters by Implicit Differentiation](http://arxiv.org/abs/2110.10461)


  Machine learning training methods depend plentifully and intricately on
hyperparameters, motivating automated strategies for their optimisation. Many
existing algorithms restart training for each new hyperparameter choice, at
considerable computational cost. Some hypergradient-based one-pass methods
exist, but these either cannot be applied to arbitrary optimiser
hyperparameters (such as learning rates and momenta) or take several times
longer to train than their base models. We extend these existing methods to
develop an approximate hypergradient-based hyperparameter optimiser which is
applicable to any continuous hyperparameter appearing in a differentiable model
weight update, yet requires only one training episode, with no restarts. We
also provide a motivating argument for convergence to the true hypergradient,
and perform tractable gradient-based optimisation of independent learning rates
for each model parameter. Our method performs competitively from varied random
hyperparameter initialisations on several UCI datasets and Fashion-MNIST (using
a one-layer MLP), Penn Treebank (using an LSTM) and CIFAR-10 (using a
ResNet-18), in time only 2-3x greater than vanilla training.

    

### [[2110.10486] A TinyML Platform for On-Device Continual Learning with Quantized Latent Replays](http://arxiv.org/abs/2110.10486)


  In the last few years, research and development on Deep Learning models and
techniques for ultra-low-power devices in a word, TinyML has mainly focused on
a train-then-deploy assumption, with static models that cannot be adapted to
newly collected data without cloud-based data collection and fine-tuning.
Latent Replay-based Continual Learning (CL) techniques[1] enable online,
serverless adaptation in principle, but so farthey have still been too
computation and memory-hungry for ultra-low-power TinyML devices, which are
typically based on microcontrollers. In this work, we introduce a HW/SW
platform for end-to-end CL based on a 10-core FP32-enabled parallel
ultra-low-power (PULP) processor. We rethink the baseline Latent Replay CL
algorithm, leveraging quantization of the frozen stage of the model and Latent
Replays (LRs) to reduce their memory cost with minimal impact on accuracy. In
particular, 8-bit compression of the LR memory proves to be almost lossless
(-0.26% with 3000LR) compared to the full-precision baseline implementation,
but requires 4x less memory, while 7-bit can also be used with an additional
minimal accuracy degradation (up to 5%). We also introduce optimized primitives
for forward and backward propagation on the PULP processor. Our results show
that by combining these techniques, continual learning can be achieved in
practice using less than 64MB of memory an amount compatible with embedding in
TinyML devices. On an advanced 22nm prototype of our platform, called VEGA, the
proposed solution performs onaverage 65x faster than a low-power STM32 L4
microcontroller, being 37x more energy efficient enough for a lifetime of 535h
when learning a new mini-batch of data once every minute.

    

### [[2110.10490] Transferring Reinforcement Learning for DC-DC Buck Converter Control via Duty Ratio Mapping: From Simulation to Implementation](http://arxiv.org/abs/2110.10490)


  Reinforcement learning (RL) control approach with application into power
electronics systems has become an emerging topic whilst the sim-to-real issue
remains a challenging problem as very few results can be referred to in the
literature. Indeed, due to the inevitable mismatch between simulation models
and real-life systems, offline trained RL control strategies may sustain
unexpected hurdles in practical implementation during transferring procedure.
As the main contribution of this paper, a transferring methodology via a
delicately designed duty ratio mapping (DRM) is proposed for a DC-DC buck
converter. Then, a detailed sim-to-real process is presented to enable the
implementation of a model-free deep reinforcement learning (DRL) controller.
The feasibility and effectiveness of the proposed methodology are demonstrated
by comparative experimental studies.

    

### [[2110.10510] Periodic DMP formulation for Quaternion Trajectories](http://arxiv.org/abs/2110.10510)


  Imitation learning techniques have been used as a way to transfer skills to
robots. Among them, dynamic movement primitives (DMPs) have been widely
exploited as an effective and an efficient technique to learn and reproduce
complex discrete and periodic skills. While DMPs have been properly formulated
for learning point-to-point movements for both translation and orientation,
periodic ones are missing a formulation to learn the orientation. To address
this gap, we propose a novel DMP formulation that enables encoding of periodic
orientation trajectories. Within this formulation we develop two approaches:
Riemannian metric-based projection approach and unit quaternion based periodic
DMP. Both formulations exploit unit quaternions to represent the orientation.
However, the first exploits the properties of Riemannian manifolds to work in
the tangent space of the unit sphere. The second encodes directly the unit
quaternion trajectory while guaranteeing the unitary norm of the generated
quaternions. We validated the technical aspects of the proposed methods in
simulation. Then we performed experiments on a real robot to execute daily
tasks that involve periodic orientation changes (i.e., surface polishing/wiping
and liquid mixing by shaking).

    

### [[2110.10518] Online non-parametric change-point detection for heterogeneous data streams observed over graph nodes](http://arxiv.org/abs/2110.10518)


  Consider a heterogeneous data stream being generated by the nodes of a graph.
The data stream is in essence composed by multiple streams, possibly of
different nature that depends on each node. At a given moment $\tau$, a
change-point occurs for a subset of nodes $C$, signifying the change in the
probability distribution of their associated streams. In this paper we propose
an online non-parametric method to infer $\tau$ based on the direct estimation
of the likelihood-ratio between the post-change and the pre-change distribution
associated with the data stream of each node. We propose a kernel-based method,
under the hypothesis that connected nodes of the graph are expected to have
similar likelihood-ratio estimates when there is no change-point. We
demonstrate the quality of our method on synthetic experiments and real-world
applications.

    

### [[2110.10522] CIM-PPO:Proximal Policy Optimization with Liu-Correntropy Induced Metric](http://arxiv.org/abs/2110.10522)


  As an algorithm based on deep reinforcement learning, Proximal Policy
Optimization (PPO) performs well in many complex tasks and has become one of
the most popular RL algorithms in recent years. According to the mechanism of
penalty in surrogate objective, PPO can be divided into PPO with KL Divergence
(KL-PPO) and PPO with Clip function(Clip-PPO). Clip-PPO is widely used in a
variety of practical scenarios and has attracted the attention of many
researchers. Therefore, many variations have also been created, making the
algorithm better and better. However, as a more theoretical algorithm, KL-PPO
was neglected because its performance was not as good as CliP-PPO. In this
article, we analyze the asymmetry effect of KL divergence on PPO's objective
function , and give the inequality that can indicate when the asymmetry will
affect the efficiency of KL-PPO. Proposed PPO with Correntropy Induced Metric
algorithm(CIM-PPO) that use the theory of correntropy(a symmetry metric method
that was widely used in M-estimation to evaluate two distributions'
difference)and applied it in PPO. Then, we designed experiments based on
OpenAIgym to test the effectiveness of the new algorithm and compare it with
KL-PPO and CliP-PPO.

    

### [[2110.10523] Detecting and Identifying Optical Signal Attacks on Autonomous Driving Systems](http://arxiv.org/abs/2110.10523)


  For autonomous driving, an essential task is to detect surrounding objects
accurately. To this end, most existing systems use optical devices, including
cameras and light detection and ranging (LiDAR) sensors, to collect environment
data in real time. In recent years, many researchers have developed advanced
machine learning models to detect surrounding objects. Nevertheless, the
aforementioned optical devices are vulnerable to optical signal attacks, which
could compromise the accuracy of object detection. To address this critical
issue, we propose a framework to detect and identify sensors that are under
attack. Specifically, we first develop a new technique to detect attacks on a
system that consists of three sensors. Our main idea is to: 1) use data from
three sensors to obtain two versions of depth maps (i.e., disparity) and 2)
detect attacks by analyzing the distribution of disparity errors. In our study,
we use real data sets and the state-of-the-art machine learning model to
evaluate our attack detection scheme and the results confirm the effectiveness
of our detection method. Based on the detection scheme, we further develop an
identification model that is capable of identifying up to n-2 attacked sensors
in a system with one LiDAR and n cameras. We prove the correctness of our
identification scheme and conduct experiments to show the accuracy of our
identification method. Finally, we investigate the overall sensitivity of our
framework.

    

### [[2110.10524] Statistical and Topological Properties of Gaussian Smoothed Sliced Probability Divergences](http://arxiv.org/abs/2110.10524)


  Gaussian smoothed sliced Wasserstein distance has been recently introduced
for comparing probability distributions, while preserving privacy on the data.
It has been shown, in applications such as domain adaptation, to provide
performances similar to its non-private (non-smoothed) counterpart. However,
the computational and statistical properties of such a metric is not yet been
well-established. In this paper, we analyze the theoretical properties of this
distance as well as those of generalized versions denoted as Gaussian smoothed
sliced divergences. We show that smoothing and slicing preserve the metric
property and the weak topology. We also provide results on the sample
complexity of such divergences. Since, the privacy level depends on the amount
of Gaussian smoothing, we analyze the impact of this parameter on the
divergence. We support our theoretical findings with empirical studies of
Gaussian smoothed and sliced version of Wassertein distance, Sinkhorn
divergence and maximum mean discrepancy (MMD). In the context of
privacy-preserving domain adaptation, we confirm that those Gaussian smoothed
sliced Wasserstein and MMD divergences perform very well while ensuring data
privacy.

    

### [[2110.10527] Sampling from Arbitrary Functions via PSD Models](http://arxiv.org/abs/2110.10527)


  In many areas of applied statistics and machine learning, generating an
arbitrary number of independent and identically distributed (i.i.d.) samples
from a given distribution is a key task. When the distribution is known only
through evaluations of the density, current methods either scale badly with the
dimension or require very involved implementations. Instead, we take a two-step
approach by first modeling the probability distribution and then sampling from
that model. We use the recently introduced class of positive semi-definite
(PSD) models, which have been shown to be efficient for approximating
probability densities. We show that these models can approximate a large class
of densities concisely using few evaluations, and present a simple algorithm to
effectively sample from these models. We also present preliminary empirical
results to illustrate our assertions.

    

### [[2110.10538] Anisotropic Separable Set Abstraction for Efficient Point Cloud Representation Learning](http://arxiv.org/abs/2110.10538)


  Access to 3D point cloud representations has been widely facilitated by LiDAR
sensors embedded in various mobile devices. This has led to an emerging need
for fast and accurate point cloud processing techniques. In this paper, we
revisit and dive deeper into PointNet++, one of the most influential yet
under-explored networks, and develop faster and more accurate variants of the
model. We first present a novel Separable Set Abstraction (SA) module that
disentangles the vanilla SA module used in PointNet++ into two separate
learning stages: (1) learning channel correlation and (2) learning spatial
correlation. The Separable SA module is significantly faster than the vanilla
version, yet it achieves comparable performance. We then introduce a new
Anisotropic Reduction function into our Separable SA module and propose an
Anisotropic Separable SA (ASSA) module that substantially increases the
network's accuracy. We later replace the vanilla SA modules in PointNet++ with
the proposed ASSA module, and denote the modified network as ASSANet. Extensive
experiments on point cloud classification, semantic segmentation, and part
segmentation show that ASSANet outperforms PointNet++ and other methods,
achieving much higher accuracy and faster speeds. In particular, ASSANet
outperforms PointNet++ by $7.4$ mIoU on S3DIS Area 5, while maintaining $1.6
\times $ faster inference speed on a single NVIDIA 2080Ti GPU. Our scaled
ASSANet variant achieves $66.8$ mIoU and outperforms KPConv, while being more
than $54 \times$ faster.

    

### [[2110.10545] Ranking and Tuning Pre-trained Models: A New Paradigm of Exploiting Model Hubs](http://arxiv.org/abs/2110.10545)


  Pre-trained model hubs with many pre-trained models (PTMs) have been a
cornerstone in deep learning. Although built at a high cost, they are in fact
\emph{under-exploited}: practitioners usually pick one PTM from the provided
model hub by popularity, and then fine-tune the PTM to solve the target task.
This nave but common practice poses two obstacles to sufficiently exploiting
pre-trained model hubs: (1) the PTM selection procedure has no optimality
guarantee; (2) only one PTM is used while the rest PTMs are overlooked.
Ideally, to maximally exploit pre-trained model hubs, trying all combinations
of PTMs and extensively fine-tuning each combination of PTMs are required,
which incurs exponential combinations and unaffordable computational budget. In
this paper, we propose a new paradigm of exploiting model hubs by ranking and
tuning pre-trained models: (1) Our conference work~\citep{you_logme:_2021}
proposed LogME to estimate the maximum value of label evidence given features
extracted by pre-trained models, which can rank all the PTMs in a model hub for
various types of PTMs and tasks \emph{before fine-tuning}. (2) the best ranked
PTM can be fine-tuned and deployed if we have no preference for the model's
architecture, or the target PTM can be tuned by top-K ranked PTMs via the
proposed B-Tuning algorithm. The ranking part is based on the conference paper,
and we complete its theoretical analysis (convergence proof of the heuristic
evidence maximization procedure, and the influence of feature dimension) in
this paper. The tuning part introduces a novel Bayesian Tuning (B-Tuning)
method for multiple PTMs tuning, which surpasses dedicated methods designed for
homogeneous PTMs tuning and sets up new state of the art for heterogeneous PTMs
tuning. We believe the new paradigm of exploiting PTM hubs can interest a large
audience of the community.

    

### [[2110.10546] Trash or Treasure? An Interactive Dual-Stream Strategy for Single Image Reflection Separation](http://arxiv.org/abs/2110.10546)


  Single image reflection separation (SIRS), as a representative blind source
separation task, aims to recover two layers, $\textit{i.e.}$, transmission and
reflection, from one mixed observation, which is challenging due to the highly
ill-posed nature. Existing deep learning based solutions typically restore the
target layers individually, or with some concerns at the end of the output,
barely taking into account the interaction across the two streams/branches. In
order to utilize information more efficiently, this work presents a general yet
simple interactive strategy, namely $\textit{your trash is my treasure}$
(YTMT), for constructing dual-stream decomposition networks. To be specific, we
explicitly enforce the two streams to communicate with each other block-wisely.
Inspired by the additive property between the two components, the interactive
path can be easily built via transferring, instead of discarding, deactivated
information by the ReLU rectifier from one stream to the other. Both ablation
studies and experimental results on widely-used SIRS datasets are conducted to
demonstrate the efficacy of YTMT, and reveal its superiority over other
state-of-the-art alternatives. The implementation is quite simple and our code
is publicly available at
$\href{this https URL}{\textit{this https URL}}$.

    

### [[2110.10548] Synthesizing Optimal Parallelism Placement and Reduction Strategies on Hierarchical Systems for Deep Learning](http://arxiv.org/abs/2110.10548)


  We present a novel characterization of the mapping of multiple parallelism
forms (e.g. data and model parallelism) onto hierarchical accelerator systems
that is hierarchy-aware and greatly reduces the space of software-to-hardware
mapping. We experimentally verify the substantial effect of these mappings on
all-reduce performance (up to 448x). We offer a novel syntax-guided program
synthesis framework that is able to decompose reductions over one or more
parallelism axes to sequences of collectives in a hierarchy- and mapping-aware
way. For 69% of parallelism placements and user requested reductions, our
framework synthesizes programs that outperform the default all-reduce
implementation when evaluated on different GPU hierarchies (max 2.04x, average
1.27x). We complement our synthesis tool with a simulator exceeding 90% top-10
accuracy, which therefore reduces the need for massive evaluations of synthesis
results to determine a small set of optimal programs and mappings.

    

### [[2110.10552] Few-Shot Temporal Action Localization with Query Adaptive Transformer](http://arxiv.org/abs/2110.10552)


  Existing temporal action localization (TAL) works rely on a large number of
training videos with exhaustive segment-level annotation, preventing them from
scaling to new classes. As a solution to this problem, few-shot TAL (FS-TAL)
aims to adapt a model to a new class represented by as few as a single video.
Exiting FS-TAL methods assume trimmed training videos for new classes. However,
this setting is not only unnatural actions are typically captured in untrimmed
videos, but also ignores background video segments containing vital contextual
cues for foreground action segmentation. In this work, we first propose a new
FS-TAL setting by proposing to use untrimmed training videos. Further, a novel
FS-TAL model is proposed which maximizes the knowledge transfer from training
classes whilst enabling the model to be dynamically adapted to both the new
class and each video of that class simultaneously. This is achieved by
introducing a query adaptive Transformer in the model. Extensive experiments on
two action localization benchmarks demonstrate that our method can outperform
all the state of the art alternatives significantly in both single-domain and
cross-domain scenarios. The source code can be found in
this https URL


### [[2110.10555] Why Settle for Just One? Extending EL++ Ontology Embeddings with Many-to-Many Relationships](http://arxiv.org/abs/2110.10555)


  Knowledge Graph (KG) embeddings provide a low-dimensional representation of
entities and relations of a Knowledge Graph and are used successfully for
various applications such as question answering and search, reasoning,
inference, and missing link prediction. However, most of the existing KG
embeddings only consider the network structure of the graph and ignore the
semantics and the characteristics of the underlying ontology that provides
crucial information about relationships between entities in the KG. Recent
efforts in this direction involve learning embeddings for a Description Logic
(logical underpinning for ontologies) named EL++. However, such methods
consider all the relations defined in the ontology to be one-to-one which
severely limits their performance and applications. We provide a simple and
effective solution to overcome this shortcoming that allows such methods to
consider many-to-many relationships while learning embedding representations.
Experiments conducted using three different EL++ ontologies show substantial
performance improvement over five baselines. Our proposed solution also paves
the way for learning embedding representations for even more expressive
description logics such as SROIQ.

    

### [[2110.10563] Robust Monocular Localization in Sparse HD Maps Leveraging Multi-Task Uncertainty Estimation](http://arxiv.org/abs/2110.10563)


  Robust localization in dense urban scenarios using a low-cost sensor setup
and sparse HD maps is highly relevant for the current advances in autonomous
driving, but remains a challenging topic in research. We present a novel
monocular localization approach based on a sliding-window pose graph that
leverages predicted uncertainties for increased precision and robustness
against challenging scenarios and per frame failures. To this end, we propose
an efficient multi-task uncertainty-aware perception module, which covers
semantic segmentation, as well as bounding box detection, to enable the
localization of vehicles in sparse maps, containing only lane borders and
traffic lights. Further, we design differentiable cost maps that are directly
generated from the estimated uncertainties. This opens up the possibility to
minimize the reprojection loss of amorphous map elements in an association free
and uncertainty-aware manner. Extensive evaluation on the Lyft 5 dataset shows
that, despite the sparsity of the map, our approach enables robust and accurate
6D localization in challenging urban scenarios

    

### [[2110.10570] Behavioral Experiments for Understanding Catastrophic Forgetting](http://arxiv.org/abs/2110.10570)


  In this paper we explore whether the fundamental tool of experimental
psychology, the behavioral experiment, has the power to generate insight not
only into humans and animals, but artificial systems too. We apply the
techniques of experimental psychology to investigating catastrophic forgetting
in neural networks. We present a series of controlled experiments with
two-layer ReLU networks, and exploratory results revealing a new understanding
of the behavior of catastrophic forgetting. Alongside our empirical findings,
we demonstrate an alternative, behavior-first approach to investigating neural
network phenomena.

    

### [[2110.10582] Distributionally Robust Semi-Supervised Learning Over Graphs](http://arxiv.org/abs/2110.10582)


  Semi-supervised learning (SSL) over graph-structured data emerges in many
network science applications. To efficiently manage learning over graphs,
variants of graph neural networks (GNNs) have been developed recently. By
succinctly encoding local graph structures and features of nodes,
state-of-the-art GNNs can scale linearly with the size of graph. Despite their
success in practice, most of existing methods are unable to handle graphs with
uncertain nodal attributes. Specifically whenever mismatches between training
and testing data distribution exists, these models fail in practice. Challenges
also arise due to distributional uncertainties associated with data acquired by
noisy measurements. In this context, a distributionally robust learning
framework is developed, where the objective is to train models that exhibit
quantifiable robustness against perturbations. The data distribution is
considered unknown, but lies within a Wasserstein ball centered around
empirical data distribution. A robust model is obtained by minimizing the worst
expected loss over this ball. However, solving the emerging functional
optimization problem is challenging, if not impossible. Advocating a strong
duality condition, we develop a principled method that renders the problem
tractable and efficiently solvable. Experiments assess the performance of the
proposed method.

    

### [[2110.10593] Time-Domain Mapping Based Single-Channel Speech Separation With Hierarchical Constraint Training](http://arxiv.org/abs/2110.10593)


  Single-channel speech separation is required for multi-speaker speech
recognition. Recent deep learning-based approaches focused on time-domain audio
separation net (TasNet) because it has superior performance and lower latency
compared to the conventional time-frequency-based (T-F-based) approaches. Most
of these works rely on the masking-based method that estimates a linear mapping
function (mask) for each speaker. However, the other commonly used method, the
mapping-based method that is less sensitive to SNR variations, is inadequately
studied in the time domain. We explore the potential of the mapping-based
method by introducing attention augmented DPRNN (AttnAugDPRNN) which directly
approximates the clean sources from the mixture for speech separation.
Permutation Invariant Training (PIT) has been a paradigm to solve the label
ambiguity problem for speech separation but usually leads to suboptimal
performance. To solve this problem, we propose an efficient training strategy
called Hierarchical Constraint Training (HCT) to regularize the training, which
could effectively improve the model performance. When using PIT, our results
showed that mapping-based AttnAugDPRNN outperformed masking-based AttnAugDPRNN
when the training corpus is large. Mapping-based AttnAugDPRNN with HCT
significantly improved the SI-SDR by 10.1% compared to the masking-based
AttnAugDPRNN without HCT.

    

### [[2110.10596] Look at What I'm Doing: Self-Supervised Spatial Grounding of Narrations in Instructional Videos](http://arxiv.org/abs/2110.10596)


  We introduce the task of spatially localizing narrated interactions in
videos. Key to our approach is the ability to learn to spatially localize
interactions with self-supervision on a large corpus of videos with
accompanying transcribed narrations. To achieve this goal, we propose a
multilayer cross-modal attention network that enables effective optimization of
a contrastive loss during training. We introduce a divided strategy that
alternates between computing inter- and intra-modal attention across the visual
and natural language modalities, which allows effective training via directly
contrasting the two modalities' representations. We demonstrate the
effectiveness of our approach by self-training on the HowTo100M instructional
video dataset and evaluating on a newly collected dataset of localized
described interactions in the YouCook2 dataset. We show that our approach
outperforms alternative baselines, including shallow co-attention and full
cross-modal attention. We also apply our approach to grounding phrases in
images with weak supervision on Flickr30K and show that stacking multiple
attention layers is effective and, when combined with a word-to-region loss,
achieves state of the art on recall-at-one and pointing hand accuracies.

    

### [[2110.10601] Color Teams for Machine Learning Development](http://arxiv.org/abs/2110.10601)


  Machine learning and software development share processes and methodologies
for reliably delivering products to customers. This work proposes the use of a
new teaming construct for forming machine learning teams for better combatting
adversarial attackers. In cybersecurity, infrastructure uses these teams to
protect their systems by using system builders and programmers to also offer
more robustness to their platforms. Color teams provide clear responsibility to
the individuals on each team for which part of the baseline (Yellow), attack
(Red), and defense (Blue) breakout of the pipeline. Combining colors leads to
additional knowledge shared across the team and more robust models built during
development. The responsibilities of the new teams Orange, Green, and Purple
will be outlined during this paper along with an overview of the necessary
resources for these teams to be successful.

    

### [[2110.10602] Transductive Robust Learning Guarantees](http://arxiv.org/abs/2110.10602)


  We study the problem of adversarially robust learning in the transductive
setting. For classes $\mathcal{H}$ of bounded VC dimension, we propose a simple
transductive learner that when presented with a set of labeled training
examples and a set of unlabeled test examples (both sets possibly adversarially
perturbed), it correctly labels the test examples with a robust error rate that
is linear in the VC dimension and is adaptive to the complexity of the
perturbation set. This result provides an exponential improvement in dependence
on VC dimension over the best known upper bound on the robust error in the
inductive setting, at the expense of competing with a more restrictive notion
of optimal robust error.

    

### [[2110.10614] Independent Natural Policy Gradient Always Converges in Markov Potential Games](http://arxiv.org/abs/2110.10614)


  Multi-agent reinforcement learning has been successfully applied to
fully-cooperative and fully-competitive environments, but little is currently
known about mixed cooperative/competitive environments. In this paper, we focus
on a particular class of multi-agent mixed cooperative/competitive stochastic
games called Markov Potential Games (MPGs), which include cooperative games as
a special case. Recent results have shown that independent policy gradient
converges in MPGs but it was not known whether Independent Natural Policy
Gradient converges in MPGs as well. We prove that Independent Natural Policy
Gradient always converges in the last iterate using constant learning rates.
The proof deviates from the existing approaches and the main challenge lies in
the fact that Markov Potential Games do not have unique optimal values (as
single-agent settings exhibit) so different initializations can lead to
different limit point values. We complement our theoretical results with
experiments that indicate that Natural Policy Gradient outperforms Policy
Gradient in routing games and congestion games.

    

### [[2110.10632] More Efficient Exploration with Symbolic Priors on Action Sequence Equivalences](http://arxiv.org/abs/2110.10632)


  Incorporating prior knowledge in reinforcement learning algorithms is mainly
an open question. Even when insights about the environment dynamics are
available, reinforcement learning is traditionally used in a tabula rasa
setting and must explore and learn everything from scratch. In this paper, we
consider the problem of exploiting priors about action sequence equivalence:
that is, when different sequences of actions produce the same effect. We
propose a new local exploration strategy calibrated to minimize collisions and
maximize new state visitations. We show that this strategy can be computed at
little cost, by solving a convex optimization problem. By replacing the usual
epsilon-greedy strategy in a DQN, we demonstrate its potential in several
environments with various dynamic structures.

    

### [[2110.10640] OSS-Net: Memory Efficient High Resolution Semantic Segmentation of 3D Medical Data](http://arxiv.org/abs/2110.10640)


  Convolutional neural networks (CNNs) are the current state-of-the-art
meta-algorithm for volumetric segmentation of medical data, for example, to
localize COVID-19 infected tissue on computer tomography scans or the detection
of tumour volumes in magnetic resonance imaging. A key limitation of 3D CNNs on
voxelised data is that the memory consumption grows cubically with the training
data resolution. Occupancy networks (O-Nets) are an alternative for which the
data is represented continuously in a function space and 3D shapes are learned
as a continuous decision boundary. While O-Nets are significantly more memory
efficient than 3D CNNs, they are limited to simple shapes, are relatively slow
at inference, and have not yet been adapted for 3D semantic segmentation of
medical data. Here, we propose Occupancy Networks for Semantic Segmentation
(OSS-Nets) to accurately and memory-efficiently segment 3D medical data. We
build upon the original O-Net with modifications for increased expressiveness
leading to improved segmentation performance comparable to 3D CNNs, as well as
modifications for faster inference. We leverage local observations to represent
complex shapes and prior encoder predictions to expedite inference. We showcase
OSS-Net's performance on 3D brain tumour and liver segmentation against a
function space baseline (O-Net), a performance baseline (3D residual U-Net),
and an efficiency baseline (2D residual U-Net). OSS-Net yields segmentation
results similar to the performance baseline and superior to the function space
and efficiency baselines. In terms of memory efficiency, OSS-Net consumes
comparable amounts of memory as the function space baseline, somewhat more
memory than the efficiency baseline and significantly less than the performance
baseline. As such, OSS-Net enables memory-efficient and accurate 3D semantic
segmentation that can scale to high resolutions.

    

### [[2110.10655] Adversarial Socialbot Learning via Multi-Agent Deep Hierarchical Reinforcement Learning](http://arxiv.org/abs/2110.10655)


  Socialbots are software-driven user accounts on social platforms, acting
autonomously (mimicking human behavior), with the aims to influence the
opinions of other users or spread targeted misinformation for particular goals.
As socialbots undermine the ecosystem of social platforms, they are often
considered harmful. As such, there have been several computational efforts to
auto-detect the socialbots. However, to our best knowledge, the adversarial
nature of these socialbots has not yet been studied. This begs a question "can
adversaries, controlling socialbots, exploit AI techniques to their advantage?"
To this question, we successfully demonstrate that indeed it is possible for
adversaries to exploit computational learning mechanism such as reinforcement
learning (RL) to maximize the influence of socialbots while avoiding being
detected. We first formulate the adversarial socialbot learning as a
cooperative game between two functional hierarchical RL agents. While one agent
curates a sequence of activities that can avoid the detection, the other agent
aims to maximize network influence by selectively connecting with right users.
Our proposed policy networks train with a vast amount of synthetic graphs and
generalize better than baselines on unseen real-life graphs both in terms of
maximizing network influence (up to +18%) and sustainable stealthiness (up to
+40% undetectability) under a strong bot detector (with 90% detection
accuracy). During inference, the complexity of our approach scales linearly,
independent of a network's structure and the virality of news. This makes our
approach a practical adversarial attack when deployed in a real-life setting.

    

### [[2110.10659] OMB-Py: Python Micro-Benchmarks for Evaluating Performance of MPI Libraries on HPC Systems](http://arxiv.org/abs/2110.10659)


  Python has become a dominant programming language for emerging areas like
Machine Learning (ML), Deep Learning (DL), and Data Science (DS). An attractive
feature of Python is that it provides easy-to-use programming interface while
allowing library developers to enhance performance of their applications by
harnessing the computing power offered by High Performance Computing (HPC)
platforms. Efficient communication is key to scaling applications on parallel
systems, which is typically enabled by the Message Passing Interface (MPI)
standard and compliant libraries on HPC hardware. mpi4py is a Python-based
communication library that provides an MPI-like interface for Python
applications allowing application developers to utilize parallel processing
elements including GPUs. However, there is currently no benchmark suite to
evaluate communication performance of mpi4py -- and Python MPI codes in general
-- on modern HPC systems. In order to bridge this gap, we propose OMB-Py --
Python extensions to the open-source OSU Micro-Benchmark (OMB) suite -- aimed
to evaluate communication performance of MPI-based parallel applications in
Python. To the best of our knowledge, OMB-Py is the first communication
benchmark suite for parallel Python applications. OMB-Py consists of a variety
of point-to-point and collective communication benchmark tests that are
implemented for a range of popular Python libraries including NumPy, CuPy,
Numba, and PyCUDA. We also provide Python implementation for several
distributed ML algorithms as benchmarks to understand the potential gain in
performance for ML/DL workloads. Our evaluation reveals that mpi4py introduces
a small overhead when compared to native MPI libraries. We also evaluate the
ML/DL workloads and report up to 106x speedup on 224 CPU cores compared to
sequential execution. We plan to publicly release OMB-Py to benefit Python HPC
community.

    

### [[2110.10661] SILG: The Multi-environment Symbolic Interactive Language Grounding Benchmark](http://arxiv.org/abs/2110.10661)


  Existing work in language grounding typically study single environments. How
do we build unified models that apply across multiple environments? We propose
the multi-environment Symbolic Interactive Language Grounding benchmark (SILG),
which unifies a collection of diverse grounded language learning environments
under a common interface. SILG consists of grid-world environments that require
generalization to new dynamics, entities, and partially observed worlds (RTFM,
Messenger, NetHack), as well as symbolic counterparts of visual worlds that
require interpreting rich natural language with respect to complex scenes
(ALFWorld, Touchdown). Together, these environments provide diverse grounding
challenges in richness of observation space, action space, language
specification, and plan complexity. In addition, we propose the first shared
model architecture for RL on these environments, and evaluate recent advances
such as egocentric local convolution, recurrent state-tracking, entity-centric
attention, and pretrained LM using SILG. Our shared architecture achieves
comparable performance to environment-specific architectures. Moreover, we find
that many recent modelling advances do not result in significant gains on
environments other than the one they were designed for. This highlights the
need for a multi-environment benchmark. Finally, the best models significantly
underperform humans on SILG, which suggests ample room for future work. We hope
SILG enables the community to quickly identify new methodologies for language
grounding that generalize to a diverse set of environments and their associated
challenges.

    

### [[2110.10674] SEA: Graph Shell Attention in Graph Neural Networks](http://arxiv.org/abs/2110.10674)


  A common issue in Graph Neural Networks (GNNs) is known as over-smoothing. By
increasing the number of iterations within the message-passing of GNNs, the
nodes' representations of the input graph align with each other and become
indiscernible. Recently, it has been shown that increasing a model's complexity
by integrating an attention mechanism yields more expressive architectures.
This is majorly contributed to steering the nodes' representations only towards
nodes that are more informative than others. Transformer models in combination
with GNNs result in architectures including Graph Transformer Layers (GTL),
where layers are entirely based on the attention operation. However, the
calculation of a node's representation is still restricted to the computational
working flow of a GNN. In our work, we relax the GNN architecture by means of
implementing a routing heuristic. Specifically, the nodes' representations are
routed to dedicated experts. Each expert calculates the representations
according to their respective GNN workflow. The definitions of distinguishable
GNNs result from k-localized views starting from the central node. We call this
procedure Graph Shell Attention (SEA), where experts process different
subgraphs in a transformer-motivated fashion. Intuitively, by increasing the
number of experts, the models gain in expressiveness such that a node's
representation is solely based on nodes that are located within the receptive
field of an expert. We evaluate our architecture on various benchmark datasets
showing competitive results compared to state-of-the-art models.

    

### [[2110.10709] Predicting Tau Accumulation in Cerebral Cortex with Multivariate MRI Morphometry Measurements, Sparse Coding, and Correntropy](http://arxiv.org/abs/2110.10709)


  Biomarker-assisted diagnosis and intervention in Alzheimer's disease (AD) may
be the key to prevention breakthroughs. One of the hallmarks of AD is the
accumulation of tau plaques in the human brain. However, current methods to
detect tau pathology are either invasive (lumbar puncture) or quite costly and
not widely available (Tau PET). In our previous work, structural MRI-based
hippocampal multivariate morphometry statistics (MMS) showed superior
performance as an effective neurodegenerative biomarker for preclinical AD and
Patch Analysis-based Surface Correntropy-induced Sparse coding and max-pooling
(PASCS-MP) has excellent ability to generate low-dimensional representations
with strong statistical power for brain amyloid prediction. In this work, we
apply this framework together with ridge regression models to predict Tau
deposition in Braak12 and Braak34 brain regions separately. We evaluate our
framework on 925 subjects from the Alzheimer's Disease Neuroimaging Initiative
(ADNI). Each subject has one pair consisting of a PET image and MRI scan which
were collected at about the same times. Experimental results suggest that the
representations from our MMS and PASCS-MP have stronger predictive power and
their predicted Braak12 and Braak34 are closer to the real values compared to
the measures derived from other approaches such as hippocampal surface area and
volume, and shape morphometry features based on spherical harmonics (SPHARM).

    

### [[2110.10710] Stochastic Learning Rate Optimization in the Stochastic Approximation and Online Learning Settings](http://arxiv.org/abs/2110.10710)


  In this work, multiplicative stochasticity is applied to the learning rate of
stochastic optimization algorithms, giving rise to stochastic learning-rate
schemes. In-expectation theoretical convergence results of Stochastic Gradient
Descent equipped with this novel stochastic learning rate scheme under the
stochastic setting, as well as convergence results under the online
optimization settings are provided. Empirical results consider the case of an
adaptively uniformly distributed multiplicative stochasticity and include not
only Stochastic Gradient Descent, but also other popular algorithms equipped
with a stochastic learning rate. They demonstrate noticeable optimization
performance gains, with respect to their deterministic-learning-rate versions.

    

### [[2110.10713] PPFS: Predictive Permutation Feature Selection](http://arxiv.org/abs/2110.10713)


  We propose Predictive Permutation Feature Selection (PPFS), a novel
wrapper-based feature selection method based on the concept of Markov Blanket
(MB). Unlike previous MB methods, PPFS is a universal feature selection
technique as it can work for both classification as well as regression tasks on
datasets containing categorical and/or continuous features. We propose
Predictive Permutation Independence (PPI), a new Conditional Independence (CI)
test, which enables PPFS to be categorised as a wrapper feature selection
method. This is in contrast to current filter based MB feature selection
techniques that are unable to harness the advancements in supervised algorithms
such as Gradient Boosting Machines (GBM). The PPI test is based on the knockoff
framework and utilizes supervised algorithms to measure the association between
an individual or a set of features and the target variable. We also propose a
novel MB aggregation step that addresses the issue of sample inefficiency.
Empirical evaluations and comparisons on a large number of datasets demonstrate
that PPFS outperforms state-of-the-art Markov blanket discovery algorithms as
well as, well-known wrapper methods. We also provide a sketch of the proof of
correctness of our method. Implementation of this work is available at
\url{this https URL}

    

### [[2110.10721] Learning quantum dynamics with latent neural ODEs](http://arxiv.org/abs/2110.10721)


  The core objective of machine-assisted scientific discovery is to learn
physical laws from experimental data without prior knowledge of the systems in
question. In the area of quantum physics, making progress towards these goals
is significantly more challenging due to the curse of dimensionality as well as
the counter-intuitive nature of quantum mechanics. Here, we present the QNODE,
a latent neural ODE trained on dynamics from closed and open quantum systems.
The QNODE can learn to generate quantum dynamics and extrapolate outside of its
training region that satisfy the von Neumann and time-local Lindblad master
equations for closed and open quantum systems. Furthermore the QNODE
rediscovers quantum mechanical laws such as Heisenberg's uncertainty principle
in a totally data-driven way, without constraints or guidance. Additionally, we
show that trajectories that are generated from the QNODE and are close in its
latent space have similar quantum dynamics while preserving the physics of the
training system.

    

### [[2110.10724] Semi-supervised physics guided DL framework for predicting the I-V characteristics of GAN HEMT](http://arxiv.org/abs/2110.10724)


  This letter proposes a novel deep learning framework (DLF) that addresses two
major hurdles in the adoption of deep learning techniques for solving
physics-based problems: 1) requirement of the large dataset for training the DL
model, 2) consistency of the DL model with the physics of the phenomenon. The
framework is generic in nature and can be applied to model a phenomenon from
other fields of research too as long as its behaviour is known. To demonstrate
the technique, a semi-supervised physics guided neural network (SPGNN) has been
developed that predicts I-V characteristics of a gallium nitride-based high
electron mobility transistor (GaN HEMT). A two-stage training method is
proposed, where in the first stage, the DL model is trained via the
unsupervised learning method using the I-V equations of a field-effect
transistor as a loss function of the model that incorporates physical behaviors
in the DL model and in the second stage, the DL model has been fine-tuned with
a very small set of experimental data. The SPGNN significantly reduces the
requirement of the training data by more than 80% for achieving similar or
better performance than a traditional neural network (TNN) even for unseen
conditions. The SPGNN predicts 32.4% of the unseen test data with less than 1%
of error and only 0.4% of the unseen test data with more than 10% of error.

    

### [[2110.10729] Part-X: A Family of Stochastic Algorithms for Search-Based Test Generation with Probabilistic Guarantees](http://arxiv.org/abs/2110.10729)


  Requirements driven search-based testing (also known as falsification) has
proven to be a practical and effective method for discovering erroneous
behaviors in Cyber-Physical Systems. Despite the constant improvements on the
performance and applicability of falsification methods, they all share a common
characteristic. Namely, they are best-effort methods which do not provide any
guarantees on the absence of erroneous behaviors (falsifiers) when the testing
budget is exhausted. The absence of finite time guarantees is a major
limitation which prevents falsification methods from being utilized in
certification procedures. In this paper, we address the finite-time guarantees
problem by developing a new stochastic algorithm. Our proposed algorithm not
only estimates (bounds) the probability that falsifying behaviors exist, but
also it identifies the regions where these falsifying behaviors may occur. We
demonstrate the applicability of our approach on standard benchmark functions
from the optimization literature and on the F16 benchmark problem.

    

### [[2110.10735] Dynamic Bottleneck for Robust Self-Supervised Exploration](http://arxiv.org/abs/2110.10735)


  Exploration methods based on pseudo-count of transitions or curiosity of
dynamics have achieved promising results in solving reinforcement learning with
sparse rewards. However, such methods are usually sensitive to environmental
dynamics-irrelevant information, e.g., white-noise. To handle such
dynamics-irrelevant information, we propose a Dynamic Bottleneck (DB) model,
which attains a dynamics-relevant representation based on the
information-bottleneck principle. Based on the DB model, we further propose
DB-bonus, which encourages the agent to explore state-action pairs with high
information gain. We establish theoretical connections between the proposed
DB-bonus, the upper confidence bound (UCB) for linear case, and the visiting
count for tabular case. We evaluate the proposed method on Atari suits with
dynamics-irrelevant noises. Our experiments show that exploration with DB bonus
outperforms several state-of-the-art exploration methods in noisy environments.

    

### [[2110.10741] Class Incremental Online Streaming Learning](http://arxiv.org/abs/2110.10741)


  A wide variety of methods have been developed to enable lifelong learning in
conventional deep neural networks. However, to succeed, these methods require a
`batch' of samples to be available and visited multiple times during training.
While this works well in a static setting, these methods continue to suffer in
a more realistic situation where data arrives in \emph{online streaming
manner}. We empirically demonstrate that the performance of current approaches
degrades if the input is obtained as a stream of data with the following
restrictions: $(i)$ each instance comes one at a time and can be seen only
once, and $(ii)$ the input data violates the i.i.d assumption, i.e., there can
be a class-based correlation. We propose a novel approach (CIOSL) for the
class-incremental learning in an \emph{online streaming setting} to address
these challenges. The proposed approach leverages implicit and explicit dual
weight regularization and experience replay. The implicit regularization is
leveraged via the knowledge distillation, while the explicit regularization
incorporates a novel approach for parameter regularization by learning the
joint distribution of the buffer replay and the current sample. Also, we
propose an efficient online memory replay and replacement buffer strategy that
significantly boosts the model's performance. Extensive experiments and
ablation on challenging datasets show the efficacy of the proposed method.

    

### [[2110.10745] Iterated Block Particle Filter for High-dimensional Parameter Learning: Beating the Curse of Dimensionality](http://arxiv.org/abs/2110.10745)


  Parameter learning for high-dimensional, partially observed, and nonlinear
stochastic processes is a methodological challenge. Spatiotemporal disease
transmission systems provide examples of such processes giving rise to open
inference problems. We propose the iterated block particle filter (IBPF)
algorithm for learning high-dimensional parameters over graphical state space
models with general state spaces, measures, transition densities and graph
structure. Theoretical performance guarantees are obtained on beating the curse
of dimensionality (COD), algorithm convergence, and likelihood maximization.
Experiments on a highly nonlinear and non-Gaussian spatiotemporal model for
measles transmission reveal that the iterated ensemble Kalman filter algorithm
(Li et al. (2020)) is ineffective and the iterated filtering algorithm (Ionides
et al. (2015)) suffers from the COD, while our IBPF algorithm beats COD
consistently across various experiments with different metrics.

    

### [[2110.10770] Pick-and-Mix Information Operators for Probabilistic ODE Solvers](http://arxiv.org/abs/2110.10770)


  Probabilistic numerical solvers for ordinary differential equations compute
posterior distributions over the solution of an initial value problem via
Bayesian inference. In this paper, we leverage their probabilistic formulation
to seamlessly include additional information as general likelihood terms. We
show that second-order differential equations should be directly provided to
the solver, instead of transforming the problem to first order. Additionally,
by including higher-order information or physical conservation laws in the
model, solutions become more accurate and more physically meaningful. Lastly,
we demonstrate the utility of flexible information operators by solving
differential-algebraic equations. In conclusion, the probabilistic formulation
of numerical solvers offers a flexible way to incorporate various types of
information, thus improving the resulting solutions.

    

### [[2110.10783] Adversarial attacks against Bayesian forecasting dynamic models](http://arxiv.org/abs/2110.10783)


  The last decade has seen the rise of Adversarial Machine Learning (AML). This
discipline studies how to manipulate data to fool inference engines, and how to
protect those systems against such manipulation attacks. Extensive work on
attacks against regression and classification systems is available, while
little attention has been paid to attacks against time series forecasting
systems. In this paper, we propose a decision analysis based attacking strategy
that could be utilized against Bayesian forecasting dynamic models.

    

### [[2110.10784] Style Agnostic 3D Reconstruction via Adversarial Style Transfer](http://arxiv.org/abs/2110.10784)


  Reconstructing the 3D geometry of an object from an image is a major
challenge in computer vision. Recently introduced differentiable renderers can
be leveraged to learn the 3D geometry of objects from 2D images, but those
approaches require additional supervision to enable the renderer to produce an
output that can be compared to the input image. This can be scene information
or constraints such as object silhouettes, uniform backgrounds, material,
texture, and lighting. In this paper, we propose an approach that enables a
differentiable rendering-based learning of 3D objects from images with
backgrounds without the need for silhouette supervision. Instead of trying to
render an image close to the input, we propose an adversarial style-transfer
and domain adaptation pipeline that allows to translate the input image domain
to the rendered image domain. This allows us to directly compare between a
translated image and the differentiable rendering of a 3D object reconstruction
in order to train the 3D object reconstruction network. We show that the
approach learns 3D geometry from images with backgrounds and provides a better
performance than constrained methods for single-view 3D object reconstruction
on this task.

    

### [[2110.10802] A Data-Centric Optimization Framework for Machine Learning](http://arxiv.org/abs/2110.10802)


  Rapid progress in deep learning is leading to a diverse set of quickly
changing models, with a dramatically growing demand for compute. However, as
frameworks specialize optimization to patterns in popular networks, they
implicitly constrain novel and diverse models that drive progress in research.
We empower deep learning researchers by defining a flexible and
user-customizable pipeline for optimizing training of arbitrary deep neural
networks, based on data movement minimization. The pipeline begins with
standard networks in PyTorch or ONNX and transforms computation through
progressive lowering. We define four levels of general-purpose transformations,
from local intra-operator optimizations to global data movement reduction.
These operate on a data-centric graph intermediate representation that
expresses computation and data movement at all levels of abstraction, including
expanding basic operators such as convolutions to their underlying
computations. Central to the design is the interactive and introspectable
nature of the pipeline. Every part is extensible through a Python API, and can
be tuned interactively using a GUI. We demonstrate competitive performance or
speedups on ten different networks, with interactive optimizations discovering
new opportunities in EfficientNet.

    

### [[2110.10803] Propensity-scored Probabilistic Label Trees](http://arxiv.org/abs/2110.10803)


  Extreme multi-label classification (XMLC) refers to the task of tagging
instances with small subsets of relevant labels coming from an extremely large
set of all possible labels. Recently, XMLC has been widely applied to diverse
web applications such as automatic content labeling, online advertising, or
recommendation systems. In such environments, label distribution is often
highly imbalanced, consisting mostly of very rare tail labels, and relevant
labels can be missing. As a remedy to these problems, the propensity model has
been introduced and applied within several XMLC algorithms. In this work, we
focus on the problem of optimal predictions under this model for probabilistic
label trees, a popular approach for XMLC problems. We introduce an inference
procedure, based on the $A^*$-search algorithm, that efficiently finds the
optimal solution, assuming that all probabilities and propensities are known.
We demonstrate the attractiveness of this approach in a wide empirical study on
popular XMLC benchmark datasets.

    

### [[2110.10804] Identifiable Variational Autoencoders via Sparse Decoding](http://arxiv.org/abs/2110.10804)


  We develop the Sparse VAE, a deep generative model for unsupervised
representation learning on high-dimensional data. Given a dataset of
observations, the Sparse VAE learns a set of latent factors that captures its
distribution. The model is sparse in the sense that each feature of the dataset
(i.e., each dimension) depends on a small subset of the latent factors. As
examples, in ratings data each movie is only described by a few genres; in text
data each word is only applicable to a few topics; in genomics, each gene is
active in only a few biological processes. We first show that the Sparse VAE is
identifiable: given data drawn from the model, there exists a uniquely optimal
set of factors. (In contrast, most VAE-based models are not identifiable.) The
key assumption behind Sparse-VAE identifiability is the existence of "anchor
features", where for each factor there exists a feature that depends only on
that factor. Importantly, the anchor features do not need to be known in
advance. We then show how to fit the Sparse VAE with variational EM. Finally,
we empirically study the Sparse VAE with both simulated and real data. We find
that it recovers meaningful latent factors and has smaller heldout
reconstruction error than related methods.

    

### [[2110.10809] Hierarchical Skills for Efficient Exploration](http://arxiv.org/abs/2110.10809)


  In reinforcement learning, pre-trained low-level skills have the potential to
greatly facilitate exploration. However, prior knowledge of the downstream task
is required to strike the right balance between generality (fine-grained
control) and specificity (faster learning) in skill design. In previous work on
continuous control, the sensitivity of methods to this trade-off has not been
addressed explicitly, as locomotion provides a suitable prior for navigation
tasks, which have been of foremost interest. In this work, we analyze this
trade-off for low-level policy pre-training with a new benchmark suite of
diverse, sparse-reward tasks for bipedal robots. We alleviate the need for
prior knowledge by proposing a hierarchical skill learning framework that
acquires skills of varying complexity in an unsupervised manner. For
utilization on downstream tasks, we present a three-layered hierarchical
learning algorithm to automatically trade off between general and specific
skills as required by the respective task. In our experiments, we show that our
approach performs this trade-off effectively and achieves better results than
current state-of-the-art methods for end- to-end hierarchical reinforcement
learning and unsupervised skill discovery. Code and videos are available at
this https URL .

    

### [[2110.10811] HALP: Hardware-Aware Latency Pruning](http://arxiv.org/abs/2110.10811)


  Structural pruning can simplify network architecture and improve inference
speed. We propose Hardware-Aware Latency Pruning (HALP) that formulates
structural pruning as a global resource allocation optimization problem, aiming
at maximizing the accuracy while constraining latency under a predefined
budget. For filter importance ranking, HALP leverages latency lookup table to
track latency reduction potential and global saliency score to gauge accuracy
drop. Both metrics can be evaluated very efficiently during pruning, allowing
us to reformulate global structural pruning under a reward maximization problem
given target constraint. This makes the problem solvable via our augmented
knapsack solver, enabling HALP to surpass prior work in pruning efficacy and
accuracy-efficiency trade-off. We examine HALP on both classification and
detection tasks, over varying networks, on ImageNet and VOC datasets. In
particular, for ResNet-50/-101 pruning on ImageNet, HALP improves network
throughput by $1.60\times$/$1.90\times$ with $+0.3\%$/$-0.2\%$ top-1 accuracy
changes, respectively. For SSD pruning on VOC, HALP improves throughput by
$1.94\times$ with only a $0.56$ mAP drop. HALP consistently outperforms prior
art, sometimes by large margins.

    

### [[2110.10812] REAL-M: Towards Speech Separation on Real Mixtures](http://arxiv.org/abs/2110.10812)


  In recent years, deep learning based source separation has achieved
impressive results. Most studies, however, still evaluate separation models on
synthetic datasets, while the performance of state-of-the-art techniques on
in-the-wild speech data remains an open question. This paper contributes to
fill this gap in two ways. First, we release the REAL-M dataset, a
crowd-sourced corpus of real-life mixtures. Secondly, we address the problem of
performance evaluation of real-life mixtures, where the ground truth is not
available. We bypass this issue by carefully designing a blind Scale-Invariant
Signal-to-Noise Ratio (SI-SNR) neural estimator. Through a user study, we show
that our estimator reliably evaluates the separation performance on real
mixtures. The performance predictions of the SI-SNR estimator indeed correlate
well with human opinions. Moreover, we observe that the performance trends
predicted by our estimator on the REAL-M dataset closely follow those achieved
on synthetic benchmarks when evaluating popular speech separation models.

    

### [[2110.10813] CXR-Net: An Encoder-Decoder-Encoder Multitask Deep Neural Network for Explainable and Accurate Diagnosis of COVID-19 pneumonia with Chest X-ray Images](http://arxiv.org/abs/2110.10813)


  Accurate and rapid detection of COVID-19 pneumonia is crucial for optimal
patient treatment. Chest X-Ray (CXR) is the first line imaging test for
COVID-19 pneumonia diagnosis as it is fast, cheap and easily accessible.
Inspired by the success of deep learning (DL) in computer vision, many
DL-models have been proposed to detect COVID-19 pneumonia using CXR images.
Unfortunately, these deep classifiers lack the transparency in interpreting
findings, which may limit their applications in clinical practice. The existing
commonly used visual explanation methods are either too noisy or imprecise,
with low resolution, and hence are unsuitable for diagnostic purposes. In this
work, we propose a novel explainable deep learning framework (CXRNet) for
accurate COVID-19 pneumonia detection with an enhanced pixel-level visual
explanation from CXR images. The proposed framework is based on a new
Encoder-Decoder-Encoder multitask architecture, allowing for both disease
classification and visual explanation. The method has been evaluated on real
world CXR datasets from both public and private data sources, including:
healthy, bacterial pneumonia, viral pneumonia and COVID-19 pneumonia cases The
experimental results demonstrate that the proposed method can achieve a
satisfactory level of accuracy and provide fine-resolution classification
activation maps for visual explanation in lung disease detection. The Average
Accuracy, the Precision, Recall and F1-score of COVID-19 pneumonia reached
0.879, 0.985, 0.992 and 0.989, respectively. We have also found that using lung
segmented (CXR) images can help improve the performance of the model. The
proposed method can provide more detailed high resolution visual explanation
for the classification decision, compared to current state-of-the-art visual
explanation methods and has a great potential to be used in clinical practice
for COVID-19 pneumonia diagnosis.

    

### [[2110.10815] Convergence Analysis and Implicit Regularization of Feedback Alignment for Deep Linear Networks](http://arxiv.org/abs/2110.10815)


  We theoretically analyze the Feedback Alignment (FA) algorithm, an efficient
alternative to backpropagation for training neural networks. We provide
convergence guarantees with rates for deep linear networks for both continuous
and discrete dynamics. Additionally, we study incremental learning phenomena
for shallow linear networks. Interestingly, certain specific initializations
imply that negligible components are learned before the principal ones, thus
potentially negatively affecting the effectiveness of such a learning
algorithm; a phenomenon we classify as implicit anti-regularization. We also
provide initialization schemes where the components of the problem are
approximately learned by decreasing order of importance, thus providing a form
of implicit regularization.

    

### [[2110.10817] The R package sentometrics to compute, aggregate and predict with textual sentiment](http://arxiv.org/abs/2110.10817)


  We provide a hands-on introduction to optimized textual sentiment indexation
using the R package sentometrics. Textual sentiment analysis is increasingly
used to unlock the potential information value of textual data. The
sentometrics package implements an intuitive framework to efficiently compute
sentiment scores of numerous texts, to aggregate the scores into multiple time
series, and to use these time series to predict other variables. The workflow
of the package is illustrated with a built-in corpus of news articles from two
major U.S. journals to forecast the CBOE Volatility Index.

    

### [[2110.10819] Shaking the foundations: delusions in sequence models for interaction and control](http://arxiv.org/abs/2110.10819)


  The recent phenomenal success of language models has reinvigorated machine
learning research, and large sequence models such as transformers are being
applied to a variety of domains. One important problem class that has remained
relatively elusive however is purposeful adaptive behavior. Currently there is
a common perception that sequence models "lack the understanding of the cause
and effect of their actions" leading them to draw incorrect inferences due to
auto-suggestive delusions. In this report we explain where this mismatch
originates, and show that it can be resolved by treating actions as causal
interventions. Finally, we show that in supervised learning, one can teach a
system to condition or intervene on data by training with factual and
counterfactual error signals respectively.

    

### [[2110.10828] AdamD: Improved bias-correction in Adam](http://arxiv.org/abs/2110.10828)


  Here I present a small update to the bias correction term in the Adam
optimizer that has the advantage of behaving well in the first several steps.
The default implementation of Adam may be as sensitive as it is to
hyperparameters partially due to the originally proposed bias correction
procedure, and its behavior in early steps of training.

    

### [[2110.10832] Ensemble of Averages: Improving Model Selection and Boosting Performance in Domain Generalization](http://arxiv.org/abs/2110.10832)


  In Domain Generalization (DG) settings, models trained on a given set of
training domains have notoriously chaotic performance on distribution shifted
test domains, and stochasticity in optimization (e.g. seed) plays a big role.
This makes deep learning models unreliable in real world settings. We first
show that a simple protocol for averaging model parameters along the
optimization path, starting early during training, both significantly boosts
domain generalization and diminishes the impact of stochasticity by improving
the rank correlation between the in-domain validation accuracy and out-domain
test accuracy, which is crucial for reliable model selection. Next, we show
that an ensemble of independently trained models also has a chaotic behavior in
the DG setting. Taking advantage of our observation, we show that instead of
ensembling unaveraged models, ensembling moving average models (EoA) from
different runs does increase stability and further boosts performance. On the
DomainBed benchmark, when using a ResNet-50 pre-trained on ImageNet, this
ensemble of averages achieves $88.6\%$ on PACS, $79.1\%$ on VLCS, $72.5\%$ on
OfficeHome, $52.3\%$ on TerraIncognita, and $47.4\%$ on DomainNet, an average
of $68.0\%$, beating ERM (w/o model averaging) by $\sim 4\%$. We also evaluate
a model that is pre-trained on a larger dataset, where we show EoA achieves an
average accuracy of $72.7\%$, beating its corresponding ERM baseline by $5\%$.

    

### [[2110.10833] High-resolution rainfall-runoff modeling using graph neural network](http://arxiv.org/abs/2110.10833)


  Time-series modeling has shown great promise in recent studies using the
latest deep learning algorithms such as LSTM (Long Short-Term Memory). These
studies primarily focused on watershed-scale rainfall-runoff modeling or
streamflow forecasting, but the majority of them only considered a single
watershed as a unit. Although this simplification is very effective, it does
not take into account spatial information, which could result in significant
errors in large watersheds. Several studies investigated the use of GNN (Graph
Neural Networks) for data integration by decomposing a large watershed into
multiple sub-watersheds, but each sub-watershed is still treated as a whole,
and the geoinformation contained within the watershed is not fully utilized. In
this paper, we propose the GNRRM (Graph Neural Rainfall-Runoff Model), a novel
deep learning model that makes full use of spatial information from
high-resolution precipitation data, including flow direction and geographic
information. When compared to baseline models, GNRRM has less over-fitting and
significantly improves model performance. Our findings support the importance
of hydrological data in deep learning-based rainfall-runoff modeling, and we
encourage researchers to include more domain knowledge in their models.

    

### [[2110.10834] Integrating Visuospatial, Linguistic and Commonsense Structure into Story Visualization](http://arxiv.org/abs/2110.10834)


  While much research has been done in text-to-image synthesis, little work has
been done to explore the usage of linguistic structure of the input text. Such
information is even more important for story visualization since its inputs
have an explicit narrative structure that needs to be translated into an image
sequence (or visual story). Prior work in this domain has shown that there is
ample room for improvement in the generated image sequence in terms of visual
quality, consistency and relevance. In this paper, we first explore the use of
constituency parse trees using a Transformer-based recurrent architecture for
encoding structured input. Second, we augment the structured input with
commonsense information and study the impact of this external knowledge on the
generation of visual story. Third, we also incorporate visual structure via
bounding boxes and dense captioning to provide feedback about the
characters/objects in generated images within a dual learning setup. We show
that off-the-shelf dense-captioning models trained on Visual Genome can improve
the spatial structure of images from a different target domain without needing
fine-tuning. We train the model end-to-end using intra-story contrastive loss
(between words and image sub-regions) and show significant improvements in
several metrics (and human evaluation) for multiple datasets. Finally, we
provide an analysis of the linguistic and visuo-spatial information. Code and
data: this https URL.

    

### [[1708.02511] Parametric Adversarial Divergences are Good Losses for Generative Modeling](http://arxiv.org/abs/1708.02511)


  Parametric adversarial divergences, which are a generalization of the losses
used to train generative adversarial networks (GANs), have often been described
as being approximations of their nonparametric counterparts, such as the
Jensen-Shannon divergence, which can be derived under the so-called optimal
discriminator assumption. In this position paper, we argue that despite being
"non-optimal", parametric divergences have distinct properties from their
nonparametric counterparts which can make them more suitable for learning
high-dimensional distributions. A key property is that parametric divergences
are only sensitive to certain aspects/moments of the distribution, which depend
on the architecture of the discriminator and the loss it was trained with. In
contrast, nonparametric divergences such as the Kullback-Leibler divergence are
sensitive to moments ignored by the discriminator, but they do not necessarily
correlate with sample quality (Theis et al., 2016). Similarly, we show that
mutual information can lead to unintuitive interpretations, and explore more
intuitive alternatives based on parametric divergences. We conclude that
parametric divergences are a flexible framework for defining statistical
quantities relevant to a specific modeling task.

    

### [[1901.07186] Towards Learning to Imitate from a Single Video Demonstration](http://arxiv.org/abs/1901.07186)


  Agents that can learn to imitate given video observation -- \emph{without
direct access to state or action information} are more applicable to learning
in the natural world. However, formulating a reinforcement learning (RL) agent
that facilitates this goal remains a significant challenge. We approach this
challenge using contrastive training to learn a reward function comparing an
agent's behaviour with a single demonstration. We use a Siamese recurrent
neural network architecture to learn rewards in space and time between motion
clips while training an RL policy to minimize this distance. Through
experimentation, we also find that the inclusion of multi-task data and
additional image encoding losses improve the temporal consistency of the
learned rewards and, as a result, significantly improves policy learning. We
demonstrate our approach on simulated humanoid, dog, and raptor agents in 2D
and a quadruped and a humanoid in 3D. We show that our method outperforms
current state-of-the-art techniques in these environments and can learn to
imitate from a single video demonstration.

    

### [[1901.10860] Learning Context-Dependent Choice Functions](http://arxiv.org/abs/1901.10860)


  Choice functions accept a set of alternatives as input and produce a
preferred subset of these alternatives as output. We study the problem of
learning such functions under conditions of context-dependence of preferences,
which means that the preference in favor of a certain choice alternative may
depend on what other options are also available. In spite of its practical
relevance, this kind of context-dependence has received little attention in
preference learning so far. We propose a suitable model based on
context-dependent (latent) utility functions, thereby reducing the problem to
the task of learning such utility functions. Practically, this comes with a
number of challenges. For example, the set of alternatives provided as input to
a choice function can be of any size, and the output of the function should not
depend on the order in which the alternatives are presented. To meet these
requirements, we propose two general approaches based on two representations of
context-dependent utility functions, as well as instantiations in the form of
appropriate end-to-end trainable neural network architectures. Moreover, to
demonstrate the performance of both networks, we present extensive empirical
evaluations on both synthetic and real-world datasets.

    

### [[1905.10201] Model Validation Using Mutated Training Labels: An Exploratory Study](http://arxiv.org/abs/1905.10201)


  We introduce an exploratory study on Mutation Validation (MV), a model
validation method using mutated training labels for supervised learning. MV
mutates training data labels, retrains the model against the mutated data, then
uses the metamorphic relation that captures the consequent training performance
changes to assess model fit. It does not use a validation set or test set. The
intuition underpinning MV is that overfitting models tend to fit noise in the
training data. We explore 8 different learning algorithms, 18 datasets, and 5
types of hyperparameter tuning tasks. Our results demonstrate that MV is
accurate in model selection: the model recommendation hit rate is 92\% for MV
and less than 60\% for out-of-sample-validation. MV also provides more stable
hyperparameter tuning results than out-of-sample-validation across different
runs.

    

### [[1907.08410] On Linear Convergence of Weighted Kernel Herding](http://arxiv.org/abs/1907.08410)


  The rate of convergence of weighted kernel herding (WKH) and sequential
Bayesian quadrature (SBQ), two kernel-based sampling algorithms for estimating
integrals with respect to some target probability measure, is investigated.
Under verifiable conditions on the chosen kernel and target measure, we
establish a near-geometric rate of convergence for target measures that are
nearly atomic. Furthermore, we show these algorithms perform comparably to the
theoretical best possible sampling algorithm under the maximum mean
discrepancy. An analysis is also conducted in a distributed setting. Our
theoretical developments are supported by empirical observations on simulated
data as well as a real world application.

    

### [[1912.01666] Insights into Ordinal Embedding Algorithms: A Systematic Evaluation](http://arxiv.org/abs/1912.01666)


  The objective of ordinal embedding is to find a Euclidean representation of a
set of abstract items, using only answers to triplet comparisons of the form
"Is item $i$ closer to the item $j$ or item $k$?". In recent years, numerous
algorithms have been proposed to solve this problem. However, there does not
exist a fair and thorough assessment of these embedding methods and therefore
several key questions remain unanswered: Which algorithms perform better when
the embedding dimension is constrained or few triplet comparisons are
available? Which ones scale better with increasing sample size or dimension? In
our paper, we address these questions and provide the first comprehensive and
systematic empirical evaluation of existing algorithms as well as a new neural
network approach. We find that simple, relatively unknown, non-convex methods
consistently outperform all other algorithms, including elaborate approaches
based on neural networks or landmark approaches. This finding can be explained
by our insight that many of the non-convex optimization approaches do not
suffer from local optima. Our comprehensive assessment is enabled by our
unified library of popular embedding algorithms that leverages GPU resources
and allows for fast and accurate embeddings of millions of data points.

    

### [[2002.08569] Byzantine-resilient Decentralized Stochastic Gradient Descent](http://arxiv.org/abs/2002.08569)


  Decentralized learning has gained great popularity to improve learning
efficiency and preserve data privacy. Each computing node makes equal
contribution to collaboratively learn a Deep Learning model. The elimination of
centralized Parameter Servers (PS) can effectively address many issues such as
privacy, performance bottleneck and single-point-failure. However, how to
achieve Byzantine Fault Tolerance in decentralized learning systems is rarely
explored, although this problem has been extensively studied in centralized
systems.
In this paper, we present an in-depth study towards the Byzantine resilience
of decentralized learning systems with two contributions. First, from the
adversarial perspective, we theoretically illustrate that Byzantine attacks are
more dangerous and feasible in decentralized learning systems: even one
malicious participant can arbitrarily alter the models of other participants by
sending carefully crafted updates to its neighbors. Second, from the defense
perspective, we propose UBAR, a novel algorithm to enhance decentralized
learning with Byzantine Fault Tolerance. Specifically, UBAR provides a Uniform
Byzantine-resilient Aggregation Rule for benign nodes to select the useful
parameter updates and filter out the malicious ones in each training iteration.
It guarantees that each benign node in a decentralized system can train a
correct model under very strong Byzantine attacks with an arbitrary number of
faulty nodes. We conduct extensive experiments on standard image classification
tasks and the results indicate that UBAR can effectively defeat both simple and
sophisticated Byzantine attacks with higher performance efficiency than
existing solutions.

    

### [[2005.09147] Increasing-Margin Adversarial (IMA) Training to Improve Adversarial Robustness of Neural Networks](http://arxiv.org/abs/2005.09147)


  Convolutional neural network (CNN) has surpassed traditional methods for
medical image classification. However, CNN is vulnerable to adversarial attacks
which may lead to disastrous consequences in medical applications. Although
adversarial noises are usually generated by attack algorithms,
white-noise-induced adversarial samples can exist, and therefore the threats
are real. In this study, we propose a novel training method, named IMA, to
improve the robust-ness of CNN against adversarial noises. During training, the
IMA method increases the margins of training samples in the input space, i.e.,
moving CNN decision boundaries far away from the training samples to improve
robustness. The IMA method is evaluated on publicly available datasets under
strong 100-PGD white-box adversarial attacks, and the results show that the
proposed method significantly improved CNN classification and segmentation
accuracy on noisy data while keeping a high accuracy on clean data. We hope our
approach may facilitate the development of robust applications in medical
field.

    

### [[2006.03696] High-Dimensional Non-Parametric Density Estimation in Mixed Smooth Sobolev Spaces](http://arxiv.org/abs/2006.03696)


  Density estimation plays a key role in many tasks in machine learning,
statistical inference, and visualization. The main bottleneck in
high-dimensional density estimation is the prohibitive computational cost and
the slow convergence rate. In this paper, we propose novel estimators for
high-dimensional non-parametric density estimation called the adaptive
hyperbolic cross density estimators, which enjoys nice convergence properties
in the mixed smooth Sobolev spaces. As modifications of the usual Sobolev
spaces, the mixed smooth Sobolev spaces are more suitable for describing
high-dimensional density functions in some applications. We prove that, unlike
other existing approaches, the proposed estimator does not suffer the curse of
dimensionality under Integral Probability Metric, including Hlder Integral
Probability Metric, where Total Variation Metric and Wasserstein Distance are
special cases. Applications of the proposed estimators to generative
adversarial networks (GANs) and goodness of fit test for high-dimensional data
are discussed to illustrate the proposed estimator's good performance in
high-dimensional problems. Numerical experiments are conducted and illustrate
the efficiency of our proposed method.

    

### [[2006.10875] Provably adaptive reinforcement learning in metric spaces](http://arxiv.org/abs/2006.10875)


  We study reinforcement learning in continuous state and action spaces endowed
with a metric. We provide a refined analysis of a variant of the algorithm of
Sinclair, Banerjee, and Yu (2019) and show that its regret scales with the
\emph{zooming dimension} of the instance. This parameter, which originates in
the bandit literature, captures the size of the subsets of near optimal actions
and is always smaller than the covering dimension used in previous analyses. As
such, our results are the first provably adaptive guarantees for reinforcement
learning in metric spaces.

    

### [[2006.13533] Provably Convergent Working Set Algorithm for Non-Convex Regularized Regression](http://arxiv.org/abs/2006.13533)


  Owing to their statistical properties, non-convex sparse regularizers have
attracted much interest for estimating a sparse linear model from high
dimensional data. Given that the solution is sparse, for accelerating
convergence, a working set strategy addresses the optimization problem through
an iterative algorithm by incre-menting the number of variables to optimize
until the identification of the solution support. While those methods have been
well-studied and theoretically supported for convex regularizers, this paper
proposes a working set algorithm for non-convex sparse regularizers with
convergence guarantees. The algorithm, named FireWorks, is based on a
non-convex reformulation of a recent primal-dual approach and leverages on the
geometry of the residuals. Our theoretical guarantees derive from a lower bound
of the objective function decrease between two inner solver iterations and
shows the convergence to a stationary point of the full problem. More
importantly, we also show that convergence is preserved even when the inner
solver is inexact, under sufficient decay of the error across iterations. Our
experimental results demonstrate high computational gain when using our working
set strategy compared to the full problem solver for both block-coordinate
descent or a proximal gradient solver.

    

### [[2006.15590] VPNet: Variable Projection Networks](http://arxiv.org/abs/2006.15590)


  We introduce VPNet, a novel model-driven neural network architecture based on
variable projection (VP). Applying VP operators to neural networks results in
learnable features, interpretable parameters, and compact network structures.
This paper discusses the motivation and mathematical background of VPNet and
presents experiments. The VPNet approach was evaluated in the context of signal
processing, where we classified a synthetic dataset and real electrocardiogram
(ECG) signals. Compared to fully connected and one-dimensional convolutional
networks, VPNet offers fast learning ability and good accuracy at a low
computational cost of both training and inference. Based on these advantages
and the promising results obtained, we anticipate a profound impact on the
broader field of signal processing, in particular on classification, regression
and clustering problems.

    

### [[2007.06679] Lipschitz regularity of graph Laplacians on random data clouds](http://arxiv.org/abs/2007.06679)


  In this paper we study Lipschitz regularity of elliptic PDEs on geometric
graphs, constructed from random data points. The data points are sampled from a
distribution supported on a smooth manifold. The family of equations that we
study arises in data analysis in the context of graph-based learning and
contains, as important examples, the equations satisfied by graph Laplacian
eigenvectors. In particular, we prove high probability interior and global
Lipschitz estimates for solutions of graph Poisson equations. Our results can
be used to show that graph Laplacian eigenvectors are, with high probability,
essentially Lipschitz regular with constants depending explicitly on their
corresponding eigenvalues. Our analysis relies on a probabilistic coupling
argument of suitable random walks at the continuum level, and an interpolation
method for extending functions on random point clouds to the continuum
manifold. As a byproduct of our general regularity results, we obtain high
probability $L^\infty$ and approximate $\mathcal{C}^{0,1}$ convergence rates
for the convergence of graph Laplacian eigenvectors towards eigenfunctions of
the corresponding weighted Laplace-Beltrami operators. The convergence rates we
obtain scale like the $L^2$-convergence rates established by two of the authors
in previous work.

    

### [[2007.10626] Sparse Nonnegative Tensor Factorization and Completion with Noisy Observations](http://arxiv.org/abs/2007.10626)


  In this paper, we study the sparse nonnegative tensor factorization and
completion problem from partial and noisy observations for third-order tensors.
Because of sparsity and nonnegativity, the underlying tensor is decomposed into
the tensor-tensor product of one sparse nonnegative tensor and one nonnegative
tensor. We propose to minimize the sum of the maximum likelihood estimation for
the observations with nonnegativity constraints and the tensor $\ell_0$ norm
for the sparse factor. We show that the error bounds of the estimator of the
proposed model can be established under general noise observations. The
detailed error bounds under specific noise distributions including additive
Gaussian noise, additive Laplace noise, and Poisson observations can be
derived. Moreover, the minimax lower bounds are shown to be matched with the
established upper bounds up to a logarithmic factor of the sizes of the
underlying tensor. These theoretical results for tensors are better than those
obtained for matrices, and this illustrates the advantage of the use of
nonnegative sparse tensor models for completion and denoising. Numerical
experiments are provided to validate the superiority of the proposed
tensor-based method compared with the matrix-based approach.

    

### [[2008.03229] Towards Sample Efficient Agents through Algorithmic Alignment](http://arxiv.org/abs/2008.03229)


  In this work, we propose and explore Deep Graph Value Network (DeepGV) as a
promising method to work around sample complexity in deep
reinforcement-learning agents using a message-passing mechanism. The main idea
is that the agent should be guided by structured non-neural-network algorithms
like dynamic programming. According to recent advances in algorithmic
alignment, neural networks with structured computation procedures can be
trained efficiently. We demonstrate the potential of graph neural network in
supporting sample efficient learning by showing that Deep Graph Value Network
can outperform unstructured baselines by a large margin in solving the Markov
Decision Process (MDP). We believe this would open up a new avenue for
structured agent design. See
this https URL for the code.

    

### [[2008.12050] Hybrid quantum-classical optimization for financial index tracking](http://arxiv.org/abs/2008.12050)


  Tracking a financial index boils down to replicating its trajectory of
returns for a well-defined time span by investing in a weighted subset of the
securities included in the benchmark. Picking the optimal combination of assets
becomes a challenging NP-hard problem even for moderately large indices
consisting of dozens or hundreds of assets, thereby requiring heuristic methods
to find approximate solutions. Hybrid quantum-classical optimization with
variational gate-based quantum circuits arises as a plausible method to improve
performance of current schemes. In this work we introduce a heuristic pruning
algorithm to find weighted combinations of assets subject to cardinality
constraints. We further consider different strategies to respect such
constraints and compare the performance of relevant quantum anstze and
classical optimizers through numerical simulations.

    

### [[2009.04003] Bayesian Inverse Reinforcement Learning for Collective Animal Movement](http://arxiv.org/abs/2009.04003)


  Agent-based methods allow for defining simple rules that generate complex
group behaviors. The governing rules of such models are typically set a priori
and parameters are tuned from observed behavior trajectories. Instead of making
simplifying assumptions across all anticipated scenarios, inverse reinforcement
learning provides inference on the short-term (local) rules governing long term
behavior policies by using properties of a Markov decision process. We use the
computationally efficient linearly-solvable Markov decision process to learn
the local rules governing collective movement for a simulation of the self
propelled-particle (SPP) model and a data application for a captive guppy
population. The estimation of the behavioral decision costs is done in a
Bayesian framework with basis function smoothing. We recover the true costs in
the SPP simulation and find the guppies value collective movement more than
targeted movement toward shelter.

    

### [[2010.02006] Interpretable Machine Learning for COVID-19: An Empirical Study on Severity Prediction Task](http://arxiv.org/abs/2010.02006)


  The black-box nature of machine learning models hinders the deployment of
some high-accuracy models in medical diagnosis. It is risky to put one's life
in the hands of models that medical researchers do not fully understand.
However, through model interpretation, black-box models can promptly reveal
significant biomarkers that medical practitioners may have overlooked due to
the surge of infected patients in the COVID-19 pandemic.
This research leverages a database of 92 patients with confirmed SARS-CoV-2
laboratory tests between 18th Jan. 2020 and 5th Mar. 2020, in Zhuhai, China, to
identify biomarkers indicative of severity prediction. Through the
interpretation of four machine learning models, decision tree, random forests,
gradient boosted trees, and neural networks using permutation feature
importance, Partial Dependence Plot (PDP), Individual Conditional Expectation
(ICE), Accumulated Local Effects (ALE), Local Interpretable Model-agnostic
Explanations (LIME), and Shapley Additive Explanation (SHAP), we identify an
increase in N-Terminal pro-Brain Natriuretic Peptide (NTproBNP), C-Reaction
Protein (CRP), and lactic dehydrogenase (LDH), a decrease in lymphocyte (LYM)
is associated with severe infection and an increased risk of death, which is
consistent with recent medical research on COVID-19 and other research using
dedicated models. We further validate our methods on a large open dataset with
5644 confirmed patients from the Hospital Israelita Albert Einstein, at So
Paulo, Brazil from Kaggle, and unveil leukocytes, eosinophils, and platelets as
three indicative biomarkers for COVID-19.

    

### [[2010.06917] UAV Path Planning using Global and Local Map Information with Deep Reinforcement Learning](http://arxiv.org/abs/2010.06917)


  Path planning methods for autonomous unmanned aerial vehicles (UAVs) are
typically designed for one specific type of mission. This work presents a
method for autonomous UAV path planning based on deep reinforcement learning
(DRL) that can be applied to a wide range of mission scenarios. Specifically,
we compare coverage path planning (CPP), where the UAV's goal is to survey an
area of interest to data harvesting (DH), where the UAV collects data from
distributed Internet of Things (IoT) sensor devices. By exploiting structured
map information of the environment, we train double deep Q-networks (DDQNs)
with identical architectures on both distinctly different mission scenarios to
make movement decisions that balance the respective mission goal with
navigation constraints. By introducing a novel approach exploiting a compressed
global map of the environment combined with a cropped but uncompressed local
map showing the vicinity of the UAV agent, we demonstrate that the proposed
method can efficiently scale to large environments. We also extend previous
results for generalizing control policies that require no retraining when
scenario parameters change and offer a detailed analysis of crucial map
processing parameters' effects on path planning performance.

    

### [[2010.08993] Planning with Learned Dynamics: Probabilistic Guarantees on Safety and Reachability via Lipschitz Constants](http://arxiv.org/abs/2010.08993)


  We present a method for feedback motion planning of systems with unknown
dynamics which provides probabilistic guarantees on safety, reachability, and
goal stability. To find a domain in which a learned control-affine
approximation of the true dynamics can be trusted, we estimate the Lipschitz
constant of the difference between the true and learned dynamics, and ensure
the estimate is valid with a given probability. Provided the system has at
least as many controls as states, we also derive existence conditions for a
one-step feedback law which can keep the real system within a small bound of a
nominal trajectory planned with the learned dynamics. Our method imposes the
feedback law existence as a constraint in a sampling-based planner, which
returns a feedback policy around a nominal plan ensuring that, if the Lipschitz
constant estimate is valid, the true system is safe during plan execution,
reaches the goal, and is ultimately invariant in a small set about the goal. We
demonstrate our approach by planning using learned models of a 6D quadrotor and
a 7DOF Kuka arm. We show that a baseline which plans using the same learned
dynamics without considering the error bound or the existence of the feedback
law can fail to stabilize around the plan and become unsafe.

    

### [[2010.09170] Belief-Grounded Networks for Accelerated Robot Learning under Partial Observability](http://arxiv.org/abs/2010.09170)


  Many important robotics problems are partially observable in the sense that a
single visual or force-feedback measurement is insufficient to reconstruct the
state. Standard approaches involve learning a policy over beliefs or
observation-action histories. However, both of these have drawbacks; it is
expensive to track the belief online, and it is hard to learn policies directly
over histories. We propose a method for policy learning under partial
observability called the Belief-Grounded Network (BGN) in which an auxiliary
belief-reconstruction loss incentivizes a neural network to concisely summarize
its input history. Since the resulting policy is a function of the history
rather than the belief, it can be executed easily at runtime. We compare BGN
against several baselines on classic benchmark tasks as well as three novel
robotic touch-sensing tasks. BGN outperforms all other tested methods and its
learned policies work well when transferred onto a physical robot.

    

### [[2010.10797] Tensor Train Random Projection](http://arxiv.org/abs/2010.10797)


  This work proposes a novel tensor train random projection (TTRP) method for
dimension reduction, where pairwise distances can be approximately preserved.
Our TTRP is systematically constructed through a tensor train (TT)
representation with TT-ranks equal to one. Based on the tensor train format,
this new random projection method can speed up the dimension reduction
procedure for high-dimensional datasets and requires less storage costs with
little loss in accuracy, compared with existing methods. We provide a
theoretical analysis of the bias and the variance of TTRP, which shows that
this approach is an expected isometric projection with bounded variance, and we
show that the Rademacher distribution is an optimal choice for generating the
corresponding TT-cores. Detailed numerical experiments with synthetic datasets
and the MNIST dataset are conducted to demonstrate the efficiency of TTRP.

    

### [[2010.14649] Learning Contextualised Cross-lingual Word Embeddings and Alignments for Extremely Low-Resource Languages Using Parallel Corpora](http://arxiv.org/abs/2010.14649)


  We propose a new approach for learning contextualised cross-lingual word
embeddings based on a small parallel corpus (e.g. a few hundred sentence
pairs). Our method obtains word embeddings via an LSTM encoder-decoder model
that simultaneously translates and reconstructs an input sentence. Through
sharing model parameters among different languages, our model jointly trains
the word embeddings in a common cross-lingual space. We also propose to combine
word and subword embeddings to make use of orthographic similarities across
different languages. We base our experiments on real-world data from endangered
languages, namely Yongning Na, Shipibo-Konibo, and Griko. Our experiments on
bilingual lexicon induction and word alignment tasks show that our model
outperforms existing methods by a large margin for most language pairs. These
results demonstrate that, contrary to common belief, an encoder-decoder
translation model is beneficial for learning cross-lingual representations even
in extremely low-resource conditions. Furthermore, our model also works well on
high-resource conditions, achieving state-of-the-art performance on a
German-English word-alignment task.

    

### [[2011.01445] Towards Fundamental Limits of Multi-armed Bandits with Random Walk Feedback](http://arxiv.org/abs/2011.01445)


  In this paper, we consider a new Multi-Armed Bandit (MAB) problem where arms
are nodes in an unknown and possibly changing graph, and the agent (i)
initiates random walks over the graph by pulling arms, (ii) observes the random
walk trajectories, and (iii) receives rewards equal to the lengths of the
walks. We provide a comprehensive understanding of this problem by studying
both the stochastic and the adversarial setting. In the stochastic setting, we
show that this problem is not easier than a standard MAB, although additional
information is available through random walk trajectories. In the adversarial
setting, we show that an extension of the exponential weight algorithm can
achieve a regret bound of order $\widetilde{\mathcal{O}} \left( \sqrt{ \kappa
T}\right) $, where $\kappa$ is a constant that depends on the structure of the
graph, instead of the number of arms.

    

### [[2011.06023] Discovering alignment relations with Graph Convolutional Networks: a biomedical case study](http://arxiv.org/abs/2011.06023)


  Knowledge graphs are freely aggregated, published, and edited in the Web of
data, and thus may overlap. Hence, a key task resides in aligning (or matching)
their content. This task encompasses the identification, within an aggregated
knowledge graph, of nodes that are equivalent, more specific, or weakly
related. In this article, we propose to match nodes within a knowledge graph by
(i) learning node embeddings with Graph Convolutional Networks such that
similar nodes have low distances in the embedding space, and (ii) clustering
nodes based on their embeddings, in order to suggest alignment relations
between nodes of a same cluster. We conducted experiments with this approach on
the real world application of aligning knowledge in the field of
pharmacogenomics, which motivated our study. We particularly investigated the
interplay between domain knowledge and GCN models with the two following
focuses. First, we applied inference rules associated with domain knowledge,
independently or combined, before learning node embeddings, and we measured the
improvements in matching results. Second, while our GCN model is agnostic to
the exact alignment relations (e.g., equivalence, weak similarity), we observed
that distances in the embedding space are coherent with the ``strength'' of
these different relations (e.g., smaller distances for equivalences), letting
us considering clustering and distances in the embedding space as a means to
suggest alignment relations in our case study.

    

### [[2011.10480] On the coercivity condition in the learning of interacting particle systems](http://arxiv.org/abs/2011.10480)


  In the learning of systems of interacting particles or agents, coercivity
condition ensures identifiability of the interaction functions, providing the
foundation of learning by nonparametric regression. The coercivity condition is
equivalent to the strictly positive definiteness of an integral kernel arising
in the learning. We show that for a class of interaction functions such that
the system is ergodic, the integral kernel is strictly positive definite, and
hence the coercivity condition holds true.

    

### [[2011.10725] Impact of signal-to-noise ratio and bandwidth on graph Laplacian spectrum from high-dimensional noisy point cloud](http://arxiv.org/abs/2011.10725)


  We systematically {study the spectrum} of kernel-based graph Laplacian (GL)
constructed from high-dimensional and noisy random point cloud in the nonnull
setup, where the point cloud is sampled from a low-dimensional geometric
object, like a manifold, and corrupted by high-dimensional noise. We quantify
how the signal and noise interact over different regimes of signal-to-noise
ratio (SNR), and report {the resulting peculiar spectral behavior} of GL. In
addition, we explore the choice of kernel bandwidth on the spectrum of GL over
different regimes of SNR, which leads to an adaptive choice of bandwidth that
coincides with the common practice in real data. This result provides a
theoretical support for what practitioner do when the dataset is noisy.

    

### [[2011.11477] Dimensionality reduction, regularization, and generalization in overparameterized regressions](http://arxiv.org/abs/2011.11477)


  Overparameterization in deep learning is powerful: Very large models fit the
training data perfectly and yet often generalize well. This realization brought
back the study of linear models for regression, including ordinary least
squares (OLS), which, like deep learning, shows a "double-descent" behavior:
(1) The risk (expected out-of-sample prediction error) can grow arbitrarily
when the number of parameters $p$ approaches the number of samples $n$, and (2)
the risk decreases with $p$ for $p>n$, sometimes achieving a lower value than
the lowest risk for $p<n$. The divergence of the risk for OLS can be avoided
with regularization. In this work, we show that for some data models it can
also be avoided with a PCA-based dimensionality reduction (PCA-OLS, also known
as principal component regression). We provide non-asymptotic bounds for the
risk of PCA-OLS by considering the alignments of the population and empirical
principal components. We show that dimensionality reduction improves robustness
while OLS is arbitrarily susceptible to adversarial attacks, particularly in
the overparameterized regime. We compare PCA-OLS theoretically and empirically
with a wide range of projection-based methods, including random projections,
partial least squares (PLS), and certain classes of linear two-layer neural
networks. These comparisons are made for different data generation models to
assess the sensitivity to signal-to-noise and the alignment of regression
coefficients with the features. We find that methods in which the projection
depends on the training data can outperform methods where the projections are
chosen independently of the training data, even those with oracle knowledge of
population quantities, another seemingly paradoxical phenomenon that has been
identified previously. This suggests that overparameterization may not be
necessary for good generalization.

    

### [[2012.01296] A Safe Reinforcement Learning Architecture for Antenna Tilt Optimisation](http://arxiv.org/abs/2012.01296)


  Safe interaction with the environment is one of the most challenging aspects
of Reinforcement Learning (RL) when applied to real-world problems. This is
particularly important when unsafe actions have a high or irreversible negative
impact on the environment. In the context of network management operations,
Remote Electrical Tilt (RET) optimisation is a safety-critical application in
which exploratory modifications of antenna tilt angles of base stations can
cause significant performance degradation in the network. In this paper, we
propose a modular Safe Reinforcement Learning (SRL) architecture which is then
used to address the RET optimisation in cellular networks. In this approach, a
safety shield continuously benchmarks the performance of RL agents against safe
baselines, and determines safe antenna tilt updates to be performed on the
network. Our results demonstrate improved performance of the SRL agent over the
baseline while ensuring the safety of the performed actions.

    

### [[2012.04859] Enhanced Recurrent Neural Tangent Kernels for Non-Time-Series Data](http://arxiv.org/abs/2012.04859)


  Kernels derived from deep neural networks (DNNs) in the infinite-width regime
provide not only high performance in a range of machine learning tasks but also
new theoretical insights into DNN training dynamics and generalization. In this
paper, we extend the family of kernels associated with recurrent neural
networks (RNNs), which were previously derived only for simple RNNs, to more
complex architectures including bidirectional RNNs and RNNs with average
pooling. We also develop a fast GPU implementation to exploit the full
practical potential of the kernels. Though RNNs are typically only applied to
time-series data, we demonstrate that classifiers using RNN-based kernels
outperform a range of baseline methods on 90 non-time-series datasets from the
UCI data repository.

    

### [[2012.04905] ESAD: End-to-end Deep Semi-supervised Anomaly Detection](http://arxiv.org/abs/2012.04905)


  This paper explores semi-supervised anomaly detection, a more practical
setting for anomaly detection where a small additional set of labeled samples
are provided. We propose a new KL-divergence based objective function for
semi-supervised anomaly detection, and show that two factors: the mutual
information between the data and latent representations, and the entropy of
latent representations, constitute an integral objective function for anomaly
detection. To resolve the contradiction in simultaneously optimizing the two
factors, we propose a novel encoder-decoder-encoder structure, with the first
encoder focusing on optimizing the mutual information and the second encoder
focusing on optimizing the entropy. The two encoders are enforced to share
similar encoding with a consistent constraint on their latent representations.
Extensive experiments have revealed that the proposed method significantly
outperforms several state-of-the-arts on multiple benchmark datasets, including
medical diagnosis and several classic anomaly detection benchmarks.

    

### [[2101.02118] Do We Really Need Deep Learning Models for Time Series Forecasting?](http://arxiv.org/abs/2101.02118)


  Time series forecasting is a crucial task in machine learning, as it has a
wide range of applications including but not limited to forecasting electricity
consumption, traffic, and air quality. Traditional forecasting models rely on
rolling averages, vector auto-regression and auto-regressive integrated moving
averages. On the other hand, deep learning and matrix factorization models have
been recently proposed to tackle the same problem with more competitive
performance. However, one major drawback of such models is that they tend to be
overly complex in comparison to traditional techniques. In this paper, we
report the results of prominent deep learning models with respect to a
well-known machine learning baseline, a Gradient Boosting Regression Tree
(GBRT) model. Similar to the deep neural network (DNN) models, we transform the
time series forecasting task into a window-based regression problem.
Furthermore, we feature-engineered the input and output structure of the GBRT
model, such that, for each training window, the target values are concatenated
with external features, and then flattened to form one input instance for a
multi-output GBRT model. We conducted a comparative study on nine datasets for
eight state-of-the-art deep-learning models that were presented at top-level
conferences in the last years. The results demonstrate that the window-based
input transformation boosts the performance of a simple GBRT model to levels
that outperform all state-of-the-art DNN models evaluated in this paper.

    

### [[2101.04516] Machine learning based automated identification of thunderstorms from anemometric records using shapelet transform](http://arxiv.org/abs/2101.04516)


  Detection of thunderstorms is important to the wind hazard community to
better understand extreme winds field characteristics and associated wind
induced load effects on structures. This paper contributes to this effort by
proposing a new course of research that uses machine learning techniques,
independent of wind statistics based parameters, to autonomously identify and
separate thunderstorms from large databases containing high frequency sampled
continuous wind speed measurements. In this context, the use of Shapelet
transform is proposed to identify key individual attributes distinctive to
extreme wind events based on similarity of shape of their time series. This
novel shape based representation when combined with machine learning algorithms
yields a practical event detection procedure with minimal domain expertise. In
this paper, the shapelet transform along with Random Forest classifier is
employed for the identification of thunderstorms from 1 year of data from 14
ultrasonic anemometers that are a part of an extensive in situ wind monitoring
network in the Northern Mediterranean ports. A collective total of 235
non-stationary records associated with thunderstorms were identified using this
method. The results lead to enhancing the pool of thunderstorm data for more
comprehensive understanding of a wide variety of thunderstorms that have not
been previously detected using conventional gust factor-based methods.

    

### [[2101.09258] Maximum Likelihood Training of Score-Based Diffusion Models](http://arxiv.org/abs/2101.09258)


  Score-based diffusion models synthesize samples by reversing a stochastic
process that diffuses data to noise, and are trained by minimizing a weighted
combination of score matching losses. The log-likelihood of score-based
diffusion models can be tractably computed through a connection to continuous
normalizing flows, but log-likelihood is not directly optimized by the weighted
combination of score matching losses. We show that for a specific weighting
scheme, the objective upper bounds the negative log-likelihood, thus enabling
approximate maximum likelihood training of score-based diffusion models. We
empirically observe that maximum likelihood training consistently improves the
likelihood of score-based diffusion models across multiple datasets, stochastic
processes, and model architectures. Our best models achieve negative
log-likelihoods of 2.83 and 3.76 bits/dim on CIFAR-10 and ImageNet 32x32
without any data augmentation, on a par with state-of-the-art autoregressive
models on these tasks.

    

### [[2102.02400] Provably End-to-end Label-Noise Learning without Anchor Points](http://arxiv.org/abs/2102.02400)


  In label-noise learning, the transition matrix plays a key role in building
statistically consistent classifiers. Existing consistent estimators for the
transition matrix have been developed by exploiting anchor points. However, the
anchor-point assumption is not always satisfied in real scenarios. In this
paper, we propose an end-to-end framework for solving label-noise learning
without anchor points, in which we simultaneously optimize two objectives: the
cross entropy loss between the noisy label and the predicted probability by the
neural network, and the volume of the simplex formed by the columns of the
transition matrix. Our proposed framework can identify the transition matrix if
the clean class-posterior probabilities are sufficiently scattered. This is by
far the mildest assumption under which the transition matrix is provably
identifiable and the learned classifier is statistically consistent.
Experimental results on benchmark datasets demonstrate the effectiveness and
robustness of the proposed method.

    

### [[2102.04373] Partition-based formulations for mixed-integer optimization of trained ReLU neural networks](http://arxiv.org/abs/2102.04373)


  This paper introduces a class of mixed-integer formulations for trained ReLU
neural networks. The approach balances model size and tightness by partitioning
node inputs into a number of groups and forming the convex hull over the
partitions via disjunctive programming. At one extreme, one partition per input
recovers the convex hull of a node, i.e., the tightest possible formulation for
each node. For fewer partitions, we develop smaller relaxations that
approximate the convex hull, and show that they outperform existing
formulations. Specifically, we propose strategies for partitioning variables
based on theoretical motivations and validate these strategies using extensive
computational experiments. Furthermore, the proposed scheme complements known
algorithmic approaches, e.g., optimization-based bound tightening captures
dependencies within a partition.

    

### [[2102.05502] On the Suboptimality of Thompson Sampling in High Dimensions](http://arxiv.org/abs/2102.05502)


  In this paper we consider Thompson Sampling (TS) for combinatorial
semi-bandits. We demonstrate that, perhaps surprisingly, TS is sub-optimal for
this problem in the sense that its regret scales exponentially in the ambient
dimension, and its minimax regret scales almost linearly. This phenomenon
occurs under a wide variety of assumptions including both non-linear and linear
reward functions, with Bernoulli distributed rewards and uniform priors. We
also show that including a fixed amount of forced exploration to TS does not
alleviate the problem. We complement our theoretical results with numerical
results and show that in practice TS indeed can perform very poorly in some
high dimensional situations.

    

### [[2102.06406] A Too-Good-to-be-True Prior to Reduce Shortcut Reliance](http://arxiv.org/abs/2102.06406)


  Despite their impressive performance in object recognition and other tasks
under standard testing conditions, deep networks often fail to generalize to
out-of-distribution (o.o.d.) samples. One cause for this shortcoming is that
modern architectures tend to rely on "shortcuts" - superficial features that
correlate with categories without capturing deeper invariants that hold across
contexts. Real-world concepts often possess a complex structure that can vary
superficially across contexts, which can make the most intuitive and promising
solutions in one context not generalize to others. One potential way to improve
o.o.d. generalization is to assume simple solutions are unlikely to be valid
across contexts and avoid them, which we refer to as the too-good-to-be-true
prior. A low-capacity network (LCN) with a shallow architecture should only be
able to learn surface relationships, including shortcuts. We find that LCNs can
serve as shortcut detectors. Furthermore, an LCN's predictions can be used in a
two-stage approach to encourage a high-capacity network (HCN) to rely on deeper
invariant features that should generalize broadly. In particular, items that
the LCN can master are downweighted when training the HCN. Using a modified
version of the CIFAR-10 dataset in which we introduced shortcuts, we found that
the two-stage LCN-HCN approach reduced reliance on shortcuts and facilitated
o.o.d. generalization.

    

### [[2102.07937] Inverse Reinforcement Learning in a Continuous State Space with Formal Guarantees](http://arxiv.org/abs/2102.07937)


  Inverse Reinforcement Learning (IRL) is the problem of finding a reward
function which describes observed/known expert behavior. The IRL setting is
remarkably useful for automated control, in situations where the reward
function is difficult to specify manually or as a means to extract agent
preference. In this work, we provide a new IRL algorithm for the continuous
state space setting with unknown transition dynamics by modeling the system
using a basis of orthonormal functions. Moreover, we provide a proof of
correctness and formal guarantees on the sample and time complexity of our
algorithm. Finally, we present synthetic experiments to corroborate our
theoretical guarantees.

    

### [[2102.09030] Bringing Differential Private SGD to Practice: On the Independence of Gaussian Noise and the Number of Training Rounds](http://arxiv.org/abs/2102.09030)


  In the context of DP-SGD each round communicates a local SGD update which
leaks some new information about the underlying local data set to the outside
world. In order to provide privacy, Gaussian noise is added to local SGD
updates. However, privacy leakage still aggregates over multiple training
rounds. Therefore, in order to control privacy leakage over an increasing
number of training rounds, we need to increase the added Gaussian noise per
local SGD update. This dependence of the amount of Gaussian noise $\sigma$ on
the number of training rounds $T$ may impose an impractical upper bound on $T$
(because $\sigma$ cannot be too large) leading to a low accuracy global model
(because the global model receives too few local SGD updates). This makes
DP-SGD much less competitive compared to other existing privacy techniques.
We show for the first time that for $(\epsilon,\delta)$-differential privacy
$\sigma$ can be chosen equal to $\sqrt{2(\epsilon +\ln(1/\delta))/\epsilon}$
for $\epsilon=\Omega(T/N^2)$. In many existing machine learning problems, $N$
is always large and $T=O(N)$. Hence, $\sigma$ becomes ``independent'' of any
$T=O(N)$ choice with $\epsilon=\Omega(1/N)$ (aggregation of privacy leakage
increases to a limit). This means that our $\sigma$ only depends on $N$ rather
than $T$. This important discovery brings DP-SGD to practice -- as also
demonstrated by experiments -- because $\sigma$ can remain small to make the
trained model have high accuracy even for large $T$ as usually happens in
practice.

    

### [[2102.09671] When Are Solutions Connected in Deep Networks?](http://arxiv.org/abs/2102.09671)


  The question of how and why the phenomenon of mode connectivity occurs in
training deep neural networks has gained remarkable attention in the research
community. From a theoretical perspective, two possible explanations have been
proposed: (i) the loss function has connected sublevel sets, and (ii) the
solutions found by stochastic gradient descent are dropout stable. While these
explanations provide insights into the phenomenon, their assumptions are not
always satisfied in practice. In particular, the first approach requires the
network to have one layer with order of $N$ neurons ($N$ being the number of
training samples), while the second one requires the loss to be almost
invariant after removing half of the neurons at each layer (up to some
rescaling of the remaining ones). In this work, we improve both conditions by
exploiting the quality of the features at every intermediate layer together
with a milder over-parameterization condition. More specifically, we show that:
(i) under generic assumptions on the features of intermediate layers, it
suffices that the last two hidden layers have order of $\sqrt{N}$ neurons, and
(ii) if subsets of features at each layer are linearly separable, then no
over-parameterization is needed to show the connectivity. Our experiments
confirm that the proposed condition ensures the connectivity of solutions found
by stochastic gradient descent, even in settings where the previous
requirements do not hold.

    

### [[2102.10131] Scaling up DNA digital data storage by efficiently predicting DNA hybridisation using deep learning](http://arxiv.org/abs/2102.10131)


  Deoxyribonucleic acid (DNA) has shown great promise in enabling computational
applications, most notably in the fields of DNA digital data storage and DNA
computing. Information is encoded as DNA strands, which will naturally bind in
solution, thus enabling search and pattern-matching capabilities. Being able to
control and predict the process of DNA hybridisation is crucial for the
ambitious future of Hybrid Molecular-Electronic Computing. Current tools are,
however, limited in terms of throughput and applicability to large-scale
problems. We present the first comprehensive study of machine learning methods
applied to the task of predicting DNA hybridisation. For this purpose, we
introduce an in silico-generated hybridisation dataset of over 2.5 million data
points, enabling the use of deep learning. Depending on hardware, we achieve a
reduction in inference time ranging from one to over two orders of magnitude
compared to the state-of-the-art, while retaining high fidelity. We then
discuss the integration of our methods in modern, scalable workflows.

    

### [[2102.10873] Non-linear, Sparse Dimensionality Reduction via Path Lasso Penalized Autoencoders](http://arxiv.org/abs/2102.10873)


  High-dimensional data sets are often analyzed and explored via the
construction of a latent low-dimensional space which enables convenient
visualization and efficient predictive modeling or clustering. For complex data
structures, linear dimensionality reduction techniques like PCA may not be
sufficiently flexible to enable low-dimensional representation. Non-linear
dimension reduction techniques, like kernel PCA and autoencoders, suffer from
loss of interpretability since each latent variable is dependent of all input
dimensions. To address this limitation, we here present path lasso penalized
autoencoders. This structured regularization enhances interpretability by
penalizing each path through the encoder from an input to a latent variable,
thus restricting how many input variables are represented in each latent
dimension. Our algorithm uses a group lasso penalty and non-negative matrix
factorization to construct a sparse, non-linear latent representation. We
compare the path lasso regularized autoencoder to PCA, sparse PCA, autoencoders
and sparse autoencoders on real and simulated data sets. We show that the
algorithm exhibits much lower reconstruction errors than sparse PCA and
parameter-wise lasso regularized autoencoders for low-dimensional
representations. Moreover, path lasso representations provide a more accurate
reconstruction match, i.e. preserved relative distance between objects in the
original and reconstructed spaces.

    

### [[2102.12919] Distribution-Free Robust Linear Regression](http://arxiv.org/abs/2102.12919)


  We study random design linear regression with no assumptions on the
distribution of the covariates and with a heavy-tailed response variable. In
this distribution-free regression setting, we show that boundedness of the
conditional second moment of the response given the covariates is a necessary
and sufficient condition for achieving nontrivial guarantees. As a starting
point, we prove an optimal version of the classical in-expectation bound for
the truncated least squares estimator due to Gyrfi, Kohler, Krzyak,
and Walk. However, we show that this procedure fails with constant probability
for some distributions despite its optimal in-expectation performance. Then,
combining the ideas of truncated least squares, median-of-means procedures, and
aggregation theory, we construct a non-linear estimator achieving excess risk
of order $d/n$ with an optimal sub-exponential tail. While existing approaches
to linear regression for heavy-tailed distributions focus on proper estimators
that return linear functions, we highlight that the improperness of our
procedure is necessary for attaining nontrivial guarantees in the
distribution-free setting.

    

### [[2102.13519] PredDiff: Explanations and Interactions from Conditional Expectations](http://arxiv.org/abs/2102.13519)


  PredDiff is a model-agnostic, local attribution method that is firmly rooted
in probability theory. Its simple intuition is to measure prediction changes
while marginalizing features. In this work, we clarify properties of PredDiff
and its connection to Shapley values. We stress important differences between
classification and regression, which require a specific treatment within both
formalisms. We extend PredDiff by introducing a new, well-founded measure for
interaction effects between arbitrary feature subsets. The study of interaction
effects represents an inevitable step towards a comprehensive understanding of
black-box models and is particularly important for science applications. As
opposed to Shapley values, our novel measure maintains the original linear
scaling and is thus generally applicable to real-world problems.

    

### [[2103.01133] Posterior Meta-Replay for Continual Learning](http://arxiv.org/abs/2103.01133)


  Learning a sequence of tasks without access to i.i.d. observations is a
widely studied form of continual learning (CL) that remains challenging. In
principle, Bayesian learning directly applies to this setting, since recursive
and one-off Bayesian updates yield the same result. In practice, however,
recursive updating often leads to poor trade-off solutions across tasks because
approximate inference is necessary for most models of interest. Here, we
describe an alternative Bayesian approach where task-conditioned parameter
distributions are continually inferred from data. We offer a practical deep
learning implementation of our framework based on probabilistic
task-conditioned hypernetworks, an approach we term posterior meta-replay.
Experiments on standard benchmarks show that our probabilistic hypernetworks
compress sequences of posterior parameter distributions with virtually no
forgetting. We obtain considerable performance gains compared to existing
Bayesian CL methods, and identify task inference as our major limiting factor.
This limitation has several causes that are independent of the considered
sequential setting, opening up new avenues for progress in CL.

    

### [[2103.01148] Class Means as an Early Exit Decision Mechanism](http://arxiv.org/abs/2103.01148)


  State-of-the-art neural networks with early exit mechanisms often need
considerable amount of training and fine-tuning to achieve good performance
with low computational cost. We propose a novel early exit technique based on
the class means of samples. Unlike most existing schemes, our method does not
require gradient-based training of internal classifiers. This makes our method
particularly useful for neural network training in low-power devices, as in
wireless edge networks. In particular, given a fixed training time budget, our
scheme achieves higher accuracy as compared to existing early exit mechanisms.
Moreover, if there are no limitations on the training time budget, our method
can be combined with an existing early exit scheme to boost its performance,
achieving a better trade-off between computational cost and network accuracy.

    

### [[2103.01203] Generating Probabilistic Safety Guarantees for Neural Network Controllers](http://arxiv.org/abs/2103.01203)


  Neural networks serve as effective controllers in a variety of complex
settings due to their ability to represent expressive policies. The complex
nature of neural networks, however, makes their output difficult to verify and
predict, which limits their use in safety-critical applications. While
simulations provide insight into the performance of neural network controllers,
they are not enough to guarantee that the controller will perform safely in all
scenarios. To address this problem, recent work has focused on formal methods
to verify properties of neural network outputs. For neural network controllers,
we can use a dynamics model to determine the output properties that must hold
for the controller to operate safely. In this work, we develop a method to use
the results from neural network verification tools to provide probabilistic
safety guarantees on a neural network controller. We develop an adaptive
verification approach to efficiently generate an overapproximation of the
neural network policy. Next, we modify the traditional formulation of Markov
decision process (MDP) model checking to provide guarantees on the
overapproximated policy given a stochastic dynamics model. Finally, we
incorporate techniques in state abstraction to reduce overapproximation error
during the model checking process. We show that our method is able to generate
meaningful probabilistic safety guarantees for aircraft collision avoidance
neural networks that are loosely inspired by Airborne Collision Avoidance
System X (ACAS X), a family of collision avoidance systems that formulates the
problem as a partially observable Markov decision process (POMDP).

    

### [[2103.03104] Learning to run a Power Network Challenge: a Retrospective Analysis](http://arxiv.org/abs/2103.03104)


  Power networks, responsible for transporting electricity across large
geographical regions, are complex infrastructures on which modern life
critically depend. Variations in demand and production profiles, with
increasing renewable energy integration, as well as the high voltage network
technology, constitute a real challenge for human operators when optimizing
electricity transportation while avoiding blackouts. Motivated to investigate
the potential of AI methods in enabling adaptability in power network
operation, we have designed a L2RPN challenge to encourage the development of
reinforcement learning solutions to key problems present in the next-generation
power networks. The NeurIPS 2020 competition was well received by the
international community attracting over 300 participants worldwide.
The main contribution of this challenge is our proposed comprehensive
'Grid2Op' framework, and associated benchmark, which plays realistic sequential
network operations scenarios. The Grid2Op framework, which is open-source and
easily re-usable, allows users to define new environments with its companion
GridAlive ecosystem. Grid2Op relies on existing non-linear physical power
network simulators and let users create a series of perturbations and
challenges that are representative of two important problems: a) the
uncertainty resulting from the increased use of unpredictable renewable energy
sources, and b) the robustness required with contingent line disconnections. In
this paper, we give the competition highlights. We present the benchmark suite
and analyse the winning solutions, including one super-human performance
demonstration. We propose our organizational insights for a successful
competition and conclude on open research avenues. Given the challenge success,
we expect our work will foster research to create more sustainable solutions
for power network operations.

    

### [[2103.09430] OGB-LSC: A Large-Scale Challenge for Machine Learning on Graphs](http://arxiv.org/abs/2103.09430)


  Enabling effective and efficient machine learning (ML) over large-scale graph
data (e.g., graphs with billions of edges) can have a great impact on both
industrial and scientific applications. However, existing efforts to advance
large-scale graph ML have been largely limited by the lack of a suitable public
benchmark. Here we present OGB Large-Scale Challenge (OGB-LSC), a collection of
three real-world datasets for facilitating the advancements in large-scale
graph ML. The OGB-LSC datasets are orders of magnitude larger than existing
ones, covering three core graph learning tasks -- link prediction, graph
regression, and node classification. Furthermore, we provide dedicated baseline
experiments, scaling up expressive graph ML models to the massive datasets. We
show that expressive models significantly outperform simple scalable baselines,
indicating an opportunity for dedicated efforts to further improve graph ML at
scale. Moreover, OGB-LSC datasets were deployed at ACM KDD Cup 2021 and
attracted more than 500 team registrations globally, during which significant
performance improvements were made by a variety of innovative techniques. We
summarize the common techniques used by the winning solutions and highlight the
current best practices in large-scale graph ML. Finally, we describe how we
have updated the datasets after the KDD Cup to further facilitate research
advances. The OGB-LSC datasets, baseline code, and all the information about
the KDD Cup are available at this https URL .

    

### [[2103.12690] An Exponential Lower Bound for Linearly-Realizable MDPs with Constant Suboptimality Gap](http://arxiv.org/abs/2103.12690)


  A fundamental question in the theory of reinforcement learning is: suppose
the optimal $Q$-function lies in the linear span of a given $d$ dimensional
feature mapping, is sample-efficient reinforcement learning (RL) possible? The
recent and remarkable result of Weisz et al. (2020) resolved this question in
the negative, providing an exponential (in $d$) sample size lower bound, which
holds even if the agent has access to a generative model of the environment.
One may hope that this information theoretic barrier for RL can be circumvented
by further supposing an even more favorable assumption: there exists a
\emph{constant suboptimality gap} between the optimal $Q$-value of the best
action and that of the second-best action (for all states). The hope is that
having a large suboptimality gap would permit easier identification of optimal
actions themselves, thus making the problem tractable; indeed, provided the
agent has access to a generative model, sample-efficient RL is in fact possible
with the addition of this more favorable assumption.
This work focuses on this question in the standard online reinforcement
learning setting, where our main result resolves this question in the negative:
our hardness result shows that an exponential sample complexity lower bound
still holds even if a constant suboptimality gap is assumed in addition to
having a linearly realizable optimal $Q$-function. Perhaps surprisingly, this
implies an exponential separation between the online RL setting and the
generative model setting. Complementing our negative hardness result, we give
two positive results showing that provably sample-efficient RL is possible
either under an additional low-variance assumption or under a novel
hypercontractivity assumption (both implicitly place stronger conditions on the
underlying dynamics model).

    

### [[2103.13646] Contrast to Divide: Self-Supervised Pre-Training for Learning with Noisy Labels](http://arxiv.org/abs/2103.13646)


  The success of learning with noisy labels (LNL) methods relies heavily on the
success of a warm-up stage where standard supervised training is performed
using the full (noisy) training set. In this paper, we identify a "warm-up
obstacle": the inability of standard warm-up stages to train high quality
feature extractors and avert memorization of noisy labels. We propose "Contrast
to Divide" (C2D), a simple framework that solves this problem by pre-training
the feature extractor in a self-supervised fashion. Using self-supervised
pre-training boosts the performance of existing LNL approaches by drastically
reducing the warm-up stage's susceptibility to noise level, shortening its
duration, and improving extracted feature quality. C2D works out of the box
with existing methods and demonstrates markedly improved performance,
especially in the high noise regime, where we get a boost of more than 27% for
CIFAR-100 with 90% noise over the previous state of the art. In real-life noise
settings, C2D trained on mini-WebVision outperforms previous works both in
WebVision and ImageNet validation sets by 3% top-1 accuracy. We perform an
in-depth analysis of the framework, including investigating the performance of
different pre-training approaches and estimating the effective upper bound of
the LNL performance with semi-supervised learning. Code for reproducing our
experiments is available at this https URL


### [[2104.04580] Predicting the Reproducibility of Social and Behavioral Science Papers Using Supervised Learning Models](http://arxiv.org/abs/2104.04580)


  In recent years, significant effort has been invested verifying the
reproducibility and robustness of research claims in social and behavioral
sciences (SBS), much of which has involved resource-intensive replication
projects. In this paper, we investigate prediction of the reproducibility of
SBS papers using machine learning methods based on a set of features. We
propose a framework that extracts five types of features from scholarly work
that can be used to support assessments of reproducibility of published
research claims. Bibliometric features, venue features, and author features are
collected from public APIs or extracted using open source machine learning
libraries with customized parsers. Statistical features, such as p-values, are
extracted by recognizing patterns in the body text. Semantic features, such as
funding information, are obtained from public APIs or are extracted using
natural language processing models. We analyze pairwise correlations between
individual features and their importance for predicting a set of human-assessed
ground truth labels. In doing so, we identify a subset of 9 top features that
play relatively more important roles in predicting the reproducibility of SBS
papers in our corpus. Results are verified by comparing performances of 10
supervised predictive classifiers trained on different sets of features.

    

### [[2104.08690] Rethinking Image-Scaling Attacks: The Interplay Between Vulnerabilities in Machine Learning Systems](http://arxiv.org/abs/2104.08690)


  As real-world images come in varying sizes, the machine learning model is
part of a larger system that includes an upstream image scaling algorithm. In
this system, the model and the scaling algorithm have become attractive targets
for numerous attacks, such as adversarial examples and the recent image-scaling
attack. In response to these attacks, researchers have developed defense
approaches that are tailored to attacks at each processing stage. As these
defenses are developed in isolation, their underlying assumptions may not hold
when viewing them from the perspective of an end-to-end machine learning
system. Thus, it is necessary to study these attacks and defenses in the
context of machine learning systems. In this paper, we investigate the
interplay between vulnerabilities of the image scaling procedure and machine
learning models in the challenging hard-label black-box setting. We propose a
series of novel techniques to make a black-box attack exploit vulnerabilities
in scaling algorithms, scaling defenses, and the final machine learning model
in an end-to-end manner. Based on this scaling-aware attack, we reveal that
most existing scaling defenses are ineffective under threat from downstream
models. Moreover, we empirically observe that standard black-box attacks can
significantly improve their performance by exploiting the vulnerable scaling
procedure. We further demonstrate this problem on a commercial Image Analysis
API with transfer-based black-box attacks.

    

### [[2104.08760] Solving Inefficiency of Self-supervised Representation Learning](http://arxiv.org/abs/2104.08760)


  Self-supervised learning (especially contrastive learning) has attracted
great interest due to its huge potential in learning discriminative
representations in an unsupervised manner. Despite the acknowledged successes,
existing contrastive learning methods suffer from very low learning efficiency,
e.g., taking about ten times more training epochs than supervised learning for
comparable recognition accuracy. In this paper, we reveal two contradictory
phenomena in contrastive learning that we call under-clustering and
over-clustering problems, which are major obstacles to learning efficiency.
Under-clustering means that the model cannot efficiently learn to discover the
dissimilarity between inter-class samples when the negative sample pairs for
contrastive learning are insufficient to differentiate all the actual object
classes. Over-clustering implies that the model cannot efficiently learn
features from excessive negative sample pairs, forcing the model to
over-cluster samples of the same actual classes into different clusters. To
simultaneously overcome these two problems, we propose a novel self-supervised
learning framework using a truncated triplet loss. Precisely, we employ a
triplet loss tending to maximize the relative distance between the positive
pair and negative pairs to address the under-clustering problem; and we
construct the negative pair by selecting a negative sample deputy from all
negative samples to avoid the over-clustering problem, guaranteed by the
Bernoulli Distribution model. We extensively evaluate our framework in several
large-scale benchmarks (e.g., ImageNet, SYSU-30k, and COCO). The results
demonstrate our model's superiority (e.g., the learning efficiency) over the
latest state-of-the-art methods by a clear margin. Codes available at:
this https URL .

    

### [[2105.00278] A Perceptual Distortion Reduction Framework: Towards Generating Adversarial Examples with High Perceptual Quality and Attack Success Rate](http://arxiv.org/abs/2105.00278)


  Most of the adversarial attack methods suffer from large perceptual
distortions such as visible artifacts, when the attack strength is relatively
high. These perceptual distortions contain a certain portion which contributes
less to the attack success rate. This portion of distortions, which is induced
by unnecessary modifications and lack of proper perceptual distortion
constraint, is the target of the proposed framework. In this paper, we propose
a perceptual distortion reduction framework to tackle this problem from two
perspectives. Firstly, we propose a perceptual distortion constraint and add it
into the objective function to jointly optimize the perceptual distortions and
attack success rate. Secondly, we propose an adaptive penalty factor $\lambda$
to balance the discrepancies between different samples. Since SGD and
Momentum-SGD cannot optimize our complex non-convex problem, we exploit Adam in
optimization. Extensive experiments have verified the superiority of our
proposed framework.

    

### [[2105.06347] Identity testing of reversible Markov chains](http://arxiv.org/abs/2105.06347)


  We consider the problem of identity testing of Markov chains based on a
single trajectory of observations under the distance notion introduced by
Daskalakis et al. [2018a] and further analyzed by Cherapanamjeri and Bartlett
[2019]. Both works made the restrictive assumption that the Markov chains under
consideration are symmetric. In this work we relax the symmetry assumption to
the more natural assumption of reversibility, still assuming that both the
reference and the unknown Markov chains share the same stationary distribution.

    

### [[2105.09513] Deep Kronecker neural networks: A general framework for neural networks with adaptive activation functions](http://arxiv.org/abs/2105.09513)


  We propose a new type of neural networks, Kronecker neural networks (KNNs),
that form a general framework for neural networks with adaptive activation
functions. KNNs employ the Kronecker product, which provides an efficient way
of constructing a very wide network while keeping the number of parameters low.
Our theoretical analysis reveals that under suitable conditions, KNNs induce a
faster decay of the loss than that by the feed-forward networks. This is also
empirically verified through a set of computational examples. Furthermore,
under certain technical assumptions, we establish global convergence of
gradient descent for KNNs. As a specific case, we propose the Rowdy activation
function that is designed to get rid of any saturation region by injecting
sinusoidal fluctuations, which include trainable parameters. The proposed Rowdy
activation function can be employed in any neural network architecture like
feed-forward neural networks, Recurrent neural networks, Convolutional neural
networks etc. The effectiveness of KNNs with Rowdy activation is demonstrated
through various computational experiments including function approximation
using feed-forward neural networks, solution inference of partial differential
equations using the physics-informed neural networks, and standard deep
learning benchmark problems using convolutional and fully-connected neural
networks.

    

### [[2105.09821] DEHB: Evolutionary Hyperband for Scalable, Robust and Efficient Hyperparameter Optimization](http://arxiv.org/abs/2105.09821)


  Modern machine learning algorithms crucially rely on several design decisions
to achieve strong performance, making the problem of Hyperparameter
Optimization (HPO) more important than ever. Here, we combine the advantages of
the popular bandit-based HPO method Hyperband (HB) and the evolutionary search
approach of Differential Evolution (DE) to yield a new HPO method which we call
DEHB. Comprehensive results on a very broad range of HPO problems, as well as a
wide range of tabular benchmarks from neural architecture search, demonstrate
that DEHB achieves strong performance far more robustly than all previous HPO
methods we are aware of, especially for high-dimensional problems with discrete
input dimensions. For example, DEHB is up to 1000x faster than random search.
It is also efficient in computational time, conceptually simple and easy to
implement, positioning it well to become a new default HPO method.

    

### [[2105.10347] Removing the mini-batching error in Bayesian inference using Adaptive Langevin dynamics](http://arxiv.org/abs/2105.10347)


  The computational cost of usual Monte Carlo methods for sampling a posteriori
laws in Bayesian inference scales linearly with the number of data points. One
option to reduce it to a fraction of this cost is to resort to mini-batching in
conjunction with unadjusted discretizations of Langevin dynamics, in which case
only a random fraction of the data is used to estimate the gradient. However,
this leads to an additional noise in the dynamics and hence a bias on the
invariant measure which is sampled by the Markov chain. We advocate using the
so-called Adaptive Langevin dynamics, which is a modification of standard
inertial Langevin dynamics with a dynamical friction which automatically
corrects for the increased noise arising from mini-batching. We investigate the
practical relevance of the assumptions underpinning Adaptive Langevin (constant
covariance for the estimation of the gradient), which are not satisfied in
typical models of Bayesian inference, and quantify the bias induced by
minibatching in this case. We also show how to extend AdL in order to
systematically reduce the bias on the posterior distribution by considering a
dynamical friction depending on the current value of the parameter to sample.

    

### [[2105.12894] MAGI-X: Manifold-Constrained Gaussian Process Inference for Unknown System Dynamics](http://arxiv.org/abs/2105.12894)


  Ordinary differential equations (ODEs), commonly used to characterize the
dynamic systems, are difficult to propose in closed-form for many complicated
scientific applications, even with the help of domain expert. We propose a fast
and accurate data-driven method, MAGI-X, to learn the unknown dynamic from the
observation data in a non-parametric fashion, without the need of any domain
knowledge. Unlike the existing methods that mainly rely on the costly numerical
integration, MAGI-X utilizes the powerful functional approximator of neural
network to learn the unknown nonlinear dynamic within the MAnifold-constrained
Gaussian process Inference (MAGI) framework that completely circumvents the
numerical integration. Comparing against the state-of-the-art methods on three
realistic examples, MAGI-X achieves competitive accuracy in both fitting and
forecasting while only taking a fraction of computational time. Moreover,
MAGI-X provides practical solution for the inference of partial observed
systems, which no previous method is able to handle.

    

### [[2105.13495] Learning Dynamic Graph Representation of Brain Connectome with Spatio-Temporal Attention](http://arxiv.org/abs/2105.13495)


  Functional connectivity (FC) between regions of the brain can be assessed by
the degree of temporal correlation measured with functional neuroimaging
modalities. Based on the fact that these connectivities build a network,
graph-based approaches for analyzing the brain connectome have provided
insights into the functions of the human brain. The development of graph neural
networks (GNNs) capable of learning representation from graph structured data
has led to increased interest in learning the graph representation of the brain
connectome. Although recent attempts to apply GNN to the FC network have shown
promising results, there is still a common limitation that they usually do not
incorporate the dynamic characteristics of the FC network which fluctuates over
time. In addition, a few studies that have attempted to use dynamic FC as an
input for the GNN reported a reduction in performance compared to static FC
methods, and did not provide temporal explainability. Here, we propose STAGIN,
a method for learning dynamic graph representation of the brain connectome with
spatio-temporal attention. Specifically, a temporal sequence of brain graphs is
input to the STAGIN to obtain the dynamic graph representation, while novel
READOUT functions and the Transformer encoder provide spatial and temporal
explainability with attention, respectively. Experiments on the HCP-Rest and
the HCP-Task datasets demonstrate exceptional performance of our proposed
method. Analysis of the spatio-temporal attention also provide concurrent
interpretation with the neuroscientific knowledge, which further validates our
method. Code is available at this https URL


### [[2105.14203] Understanding Instance-based Interpretability of Variational Auto-Encoders](http://arxiv.org/abs/2105.14203)


  Instance-based interpretation methods have been widely studied for supervised
learning methods as they help explain how black box neural networks predict.
However, instance-based interpretations remain ill-understood in the context of
unsupervised learning. In this paper, we investigate influence functions [20],
a popular instance-based interpretation method, for a class of deep generative
models called variational auto-encoders (VAE). We formally frame the
counter-factual question answered by influence functions in this setting, and
through theoretical analysis, examine what they reveal about the impact of
training samples on classical unsupervised learning methods. We then introduce
VAE-TracIn, a computationally efficient and theoretically sound solution based
on Pruthi et al., for VAEs. Finally, we evaluate VAE-TracIn on several real
world datasets with extensive quantitative and qualitative analysis.

    

### [[2105.14586] Kolmogorov-Smirnov Test-Based Actively-Adaptive Thompson Sampling for Non-Stationary Bandits](http://arxiv.org/abs/2105.14586)


  We consider the non-stationary multi-armed bandit (MAB) framework and propose
a Kolmogorov-Smirnov (KS) test based Thompson Sampling (TS) algorithm named
TS-KS, that actively detects change points and resets the TS parameters once a
change is detected. In particular, for the two-armed bandit case, we derive
bounds on the number of samples of the reward distribution to detect the change
once it occurs. Consequently, we show that the proposed algorithm has
sub-linear regret. Contrary to existing works, our algorithm is able to detect
a change when the underlying reward distribution changes even though the mean
reward remains the same. Finally, to test the efficacy of the proposed
algorithm, we employ it in two case-studies: i) task-offloading scenario in
wireless edge-computing, and ii) portfolio optimization. Our results show that
the proposed TS-KS algorithm outperforms not only the static TS algorithm but
also it performs better than other bandit algorithms designed for
non-stationary environments. Moreover, the performance of TS-KS is at par with
the state-of-the-art forecasting algorithms such as Facebook-PROPHET and ARIMA.

    

### [[2106.00252] Information-Theoretic Analysis of Epistemic Uncertainty in Bayesian Meta-learning](http://arxiv.org/abs/2106.00252)


  The overall predictive uncertainty of a trained predictor can be decomposed
into separate contributions due to epistemic and aleatoric uncertainty. Under a
Bayesian formulation, assuming a well-specified model, the two contributions
can be exactly expressed (for the log-loss) or bounded (for more general
losses) in terms of information-theoretic quantities (Xu and Raginsky, 2020).
This paper addresses the study of epistemic uncertainty within an
information-theoretic framework in the broader setting of Bayesian
meta-learning. A general hierarchical Bayesian model is assumed in which
hyperparameters determine the per-task priors of the model parameters. Exact
characterizations (for the log-loss) and bounds (for more general losses) are
derived for the epistemic uncertainty -quantified by the minimum excess
meta-risk (MEMR)- of optimal meta-learning rules. This characterization is
leveraged to bring insights into the dependence of the epistemic uncertainty on
the number of tasks and on the amount of per-task training data. Experiments
are presented that use the proposed information-theoretic bounds, evaluated via
neural mutual information estimators, to compare the performance of
conventional learning and meta-learning as the number of meta-learning tasks
increases.

    

### [[2106.01528] Normalizing Flows for Knockoff-free Controlled Feature Selection](http://arxiv.org/abs/2106.01528)


  Controlled feature selection aims to discover the features a response depends
on while limiting the false discovery rate (FDR) to a predefined level.
Recently, multiple deep-learning-based methods have been proposed to perform
controlled feature selection through the Model-X knockoff framework. We
demonstrate, however, that these methods often fail to control the FDR for two
reasons. First, these methods often learn inaccurate models of features.
Second, the "swap" property, which is required for knockoffs to be valid, is
often not well enforced. We propose a new procedure called FlowSelect that
remedies both of these problems. To more accurately model the features,
FlowSelect uses normalizing flows, the state-of-the-art method for density
estimation. To circumvent the need to enforce the swap property, FlowSelect
uses a novel MCMC-based procedure to calculate p-values for each feature
directly. Asymptotically, FlowSelect computes valid p-values. Empirically,
FlowSelect consistently controls the FDR on both synthetic and semi-synthetic
benchmarks, whereas competing knockoff-based approaches do not. FlowSelect also
demonstrates greater power on these benchmarks. Additionally, FlowSelect
correctly infers the genetic variants associated with specific soybean traits
from GWAS data.

    

### [[2106.02496] Quantum Perceptron Revisited: Computational-Statistical Tradeoffs](http://arxiv.org/abs/2106.02496)


  Quantum machine learning algorithms could provide significant speed-ups over
their classical counterparts; however, whether they could also achieve good
generalization remains unclear. Recently, two quantum perceptron models which
give a quadratic improvement over the classical perceptron algorithm using
Grover's search have been proposed by Wiebe et al. arXiv:1602.04799 . While the
first model reduces the complexity with respect to the size of the training
set, the second one improves the bound on the number of mistakes made by the
perceptron. In this paper, we introduce a hybrid quantum-classical perceptron
algorithm with lower complexity and better generalization ability than the
classical perceptron. We show a quadratic improvement over the classical
perceptron in both the number of samples and the margin of the data. We derive
a bound on the expected error of the hypothesis returned by our algorithm,
which compares favorably to the one obtained with the classical online
perceptron. We use numerical experiments to illustrate the trade-off between
computational complexity and statistical accuracy in quantum perceptron
learning and discuss some of the key practical issues surrounding the
implementation of quantum perceptron models into near-term quantum devices,
whose practical implementation represents a serious challenge due to inherent
noise. However, the potential benefits make correcting this worthwhile.

    

### [[2106.04193] Targeted Active Learning for Bayesian Decision-Making](http://arxiv.org/abs/2106.04193)


  Active learning is usually applied to acquire labels of informative data
points in supervised learning, to maximize accuracy in a sample-efficient way.
However, maximizing the accuracy is not the end goal when the results are used
for decision-making, for example in personalized medicine or economics. We
argue that when acquiring samples sequentially, separating learning and
decision-making is sub-optimal, and we introduce an active learning strategy
which takes the down-the-line decision problem into account. Specifically, we
introduce a novel active learning criterion which maximizes the expected
information gain on the posterior distribution of the optimal decision. We
compare our targeted active learning strategy to existing alternatives on both
simulated and real data, and show improved performance in decision-making
accuracy.

    

### [[2106.04911] Memory-based Optimization Methods for Model-Agnostic Meta-Learning](http://arxiv.org/abs/2106.04911)


  Recently, model-agnostic meta-learning (MAML) has garnered tremendous
attention. However, stochastic optimization of MAML is still immature. Existing
algorithms for MAML are based on the "episode" idea by sampling a number of
tasks and a number of data points for each sampled task at each iteration for
updating the meta-model. However, they either do not necessarily guarantee
convergence with a constant mini-batch size or require processing a larger
number of tasks at every iteration, which is not viable for continual learning
or cross-device federated learning where only a small number of tasks are
available per-iteration or per-round. This paper addresses these issues by (i)
proposing efficient memory-based stochastic algorithms for MAML with a
diminishing convergence error, which only requires sampling a constant number
of tasks and a constant number of examples per-task per-iteration; (ii)
proposing communication-efficient distributed memory-based MAML algorithms for
personalized federated learning in both the cross-device (w/ client sampling)
and the cross-silo (w/o client sampling) settings. The key novelty of the
proposed algorithms is to maintain an individual personalized model (aka
memory) for each task besides the meta-model and only update them for the
sampled tasks by a momentum method that incorporates historical updates at each
iteration. The theoretical results significantly improve the optimization
theory for MAML and the empirical results also corroborate the theory.

    

### [[2106.04927] A Bi-Level Framework for Learning to Solve Combinatorial Optimization on Graphs](http://arxiv.org/abs/2106.04927)


  Combinatorial Optimization (CO) has been a long-standing challenging research
topic featured by its NP-hard nature. Traditionally such problems are
approximately solved with heuristic algorithms which are usually fast but may
sacrifice the solution quality. Currently, machine learning for combinatorial
optimization (MLCO) has become a trending research topic, but most existing
MLCO methods treat CO as a single-level optimization by directly learning the
end-to-end solutions, which are hard to scale up and mostly limited by the
capacity of ML models given the high complexity of CO. In this paper, we
propose a hybrid approach to combine the best of the two worlds, in which a
bi-level framework is developed with an upper-level learning method to optimize
the graph (e.g. add, delete or modify edges in a graph), fused with a
lower-level heuristic algorithm solving on the optimized graph. Such a bi-level
approach simplifies the learning on the original hard CO and can effectively
mitigate the demand for model capacity. The experiments and results on several
popular CO problems like Directed Acyclic Graph scheduling, Graph Edit Distance
and Hamiltonian Cycle Problem show its effectiveness over manually designed
heuristics and single-level learning methods.

    

### [[2106.05967] Revisiting Contrastive Methods for Unsupervised Learning of Visual Representations](http://arxiv.org/abs/2106.05967)


  Contrastive self-supervised learning has outperformed supervised pretraining
on many downstream tasks like segmentation and object detection. However,
current methods are still primarily applied to curated datasets like ImageNet.
In this paper, we first study how biases in the dataset affect existing
methods. Our results show that current contrastive approaches work surprisingly
well across: (i) object- versus scene-centric, (ii) uniform versus long-tailed
and (iii) general versus domain-specific datasets. Second, given the generality
of the approach, we try to realize further gains with minor modifications. We
show that learning additional invariances -- through the use of multi-scale
cropping, stronger augmentations and nearest neighbors -- improves the
representations. Finally, we observe that MoCo learns spatially structured
representations when trained with a multi-crop strategy. The representations
can be used for semantic segment retrieval and video instance segmentation
without finetuning. Moreover, the results are on par with specialized models.
We hope this work will serve as a useful study for other researchers. The code
and models are available at
this https URL.

    

### [[2106.07767] On the Relationship between Heterophily and Robustness of Graph Neural Networks](http://arxiv.org/abs/2106.07767)


  Empirical studies on the robustness of graph neural networks (GNNs) have
suggested a relation between the vulnerabilities of GNNs to adversarial attacks
and the increased presence of heterophily in perturbed graphs (where edges tend
to connect nodes with dissimilar features and labels). In this work, we
formalize the relation between heterophily and robustness, bridging two topics
previously investigated by separate lines of research. We theoretically and
empirically show that for graphs exhibiting homophily (low heterophily),
impactful structural attacks always lead to increased levels of heterophily,
while for graph with heterophily the change in the homophily level depends on
the node degrees. By leveraging these insights, we deduce that a design
principle identified to significantly improve predictive performance under
heterophily -- separate aggregators for ego- and neighbor-embeddings -- can
also inherently offer increased robustness to GNNs. Our extensive empirical
analysis shows that GNNs adopting this design alone can achieve significantly
improved empirical and certifiable robustness compared to the best-performing
unvaccinated model. Furthermore, models with this design can be readily
combined with explicit defense mechanisms to yield improved robustness with up
to 18.33% increase in performance under attacks compared to the best-performing
vaccinated model.

    

### [[2106.08229] MICo: Improved representations via sampling-based state similarity for Markov decision processes](http://arxiv.org/abs/2106.08229)


  We present a new behavioural distance over the state space of a Markov
decision process, and demonstrate the use of this distance as an effective
means of shaping the learnt representations of deep reinforcement learning
agents. While existing notions of state similarity are typically difficult to
learn at scale due to high computational cost and lack of sample-based
algorithms, our newly-proposed distance addresses both of these issues. In
addition to providing detailed theoretical analysis, we provide empirical
evidence that learning this distance alongside the value function yields
structured and informative representations, including strong results on the
Arcade Learning Environment benchmark.

    

### [[2106.08601] Self-Supervised GANs with Label Augmentation](http://arxiv.org/abs/2106.08601)


  Recently, transformation-based self-supervised learning has been applied to
generative adversarial networks (GANs) to mitigate catastrophic forgetting in
the discriminator by introducing stationary learning environments. However, the
separate self-supervised tasks in existing self-supervised GANs cause a goal
inconsistent with generative modeling due to the fact that their
self-supervised classifiers are agnostic to the generator distribution. To
address this problem, we propose a novel self-supervised GAN that unifies the
GAN task with the self-supervised task by augmenting the GAN labels (real or
fake) via self-supervision of data transformation. Specifically, the original
discriminator and self-supervised classifier are unified into a label-augmented
discriminator that predicts the augmented labels to be aware of the generator
distribution and the data distribution under every transformation, and then
provide the discrepancy between them to optimize the generator. Theoretically,
we prove that the optimal generator converges to replicate the real data
distribution under mild assumptions. Empirically, we show that the proposed
method significantly outperforms previous self-supervised and data augmentation
GANs on both generative modeling and representation learning across various
benchmark datasets.

    

### [[2106.09608] Learning Knowledge Graph-based World Models of Textual Environments](http://arxiv.org/abs/2106.09608)


  World models improve a learning agent's ability to efficiently operate in
interactive and situated environments. This work focuses on the task of
building world models of text-based game environments. Text-based games, or
interactive narratives, are reinforcement learning environments in which agents
perceive and interact with the world using textual natural language. These
environments contain long, multi-step puzzles or quests woven through a world
that is filled with hundreds of characters, locations, and objects. Our world
model learns to simultaneously: (1) predict changes in the world caused by an
agent's actions when representing the world as a knowledge graph; and (2)
generate the set of contextually relevant natural language actions required to
operate in the world. We frame this task as a Set of Sequences generation
problem by exploiting the inherent structure of knowledge graphs and actions
and introduce both a transformer-based multi-task architecture and a loss
function to train it. A zero-shot ablation study on never-before-seen textual
worlds shows that our methodology significantly outperforms existing textual
world modeling techniques as well as the importance of each of our
contributions.

    

### [[2106.09992] Exploring Counterfactual Explanations Through the Lens of Adversarial Examples: A Theoretical and Empirical Analysis](http://arxiv.org/abs/2106.09992)


  As machine learning (ML) models become more widely deployed in high-stakes
applications, counterfactual explanations have emerged as key tools for
providing actionable model explanations in practice. Despite the growing
popularity of counterfactual explanations, a deeper understanding of these
explanations is still lacking. In this work, we systematically analyze
counterfactual explanations through the lens of adversarial examples. We do so
by formalizing the similarities between popular counterfactual explanation and
adversarial example generation methods identifying conditions when they are
equivalent. We then derive the upper bounds on the distances between the
solutions output by counterfactual explanation and adversarial example
generation methods, which we validate on several real-world data sets. By
establishing these theoretical and empirical similarities between
counterfactual explanations and adversarial examples, our work raises
fundamental questions about the design and development of existing
counterfactual explanation algorithms.

    

### [[2106.11215] Machine Learning based optimization for interval uncertainty propagation](http://arxiv.org/abs/2106.11215)


  Two non-intrusive uncertainty propagation approaches are proposed for the
performance analysis of engineering systems described by expensive-to-evaluate
deterministic computer models with parameters defined as interval variables.
These approaches employ a machine learning based optimization strategy, the
so-called Bayesian optimization, for evaluating the upper and lower bounds of a
generic response variable over the set of possible responses obtained when each
interval variable varies independently over its range. The lack of knowledge
caused by not evaluating the response function for all the possible
combinations of the interval variables is accounted for by developing a
probabilistic description of the response variable itself by using a Gaussian
Process regression model. An iterative procedure is developed for selecting a
small number of simulations to be evaluated for updating this statistical model
by using well-established acquisition functions and to assess the response
bounds. In both approaches, an initial training dataset is defined. While one
approach builds iteratively two distinct training datasets for evaluating
separately the upper and lower bounds of the response variable, the other
builds iteratively a single training dataset. Consequently, the two approaches
will produce different bound estimates at each iteration. The upper and lower
bound responses are expressed as point estimates obtained from the mean
function of the posterior distribution. Moreover, a confidence interval on each
estimate is provided for effectively communicating to engineers when these
estimates are obtained for a combination of the interval variables for which no
deterministic simulation has been run. Finally, two metrics are proposed to
define conditions for assessing if the predicted bound estimates can be
considered satisfactory.

    

### [[2106.14771] HALF: Holistic Auto Machine Learning for FPGAs](http://arxiv.org/abs/2106.14771)


  Deep Neural Networks (DNNs) are capable of solving complex problems in
domains related to embedded systems, such as image and natural language
processing. To efficiently implement DNNs on a specific FPGA platform for a
given cost criterion, e.g. energy efficiency, an enormous amount of design
parameters has to be considered from the topology down to the final hardware
implementation. Interdependencies between the different design layers have to
be taken into account and explored efficiently, making it hardly possible to
find optimized solutions manually. An automatic, holistic design approach can
improve the quality of DNN implementations on FPGA significantly. To this end,
we present a cross-layer design space exploration methodology. It comprises
optimizations starting from a hardware-aware topology search for DNNs down to
the final optimized implementation for a given FPGA platform. The methodology
is implemented in our Holistic Auto machine Learning for FPGAs (HALF)
framework, which combines an evolutionary search algorithm, various
optimization steps and a library of parametrizable hardware DNN modules. HALF
automates both the exploration process and the implementation of optimized
solutions on a target FPGA platform for various applications. We demonstrate
the performance of HALF on a medical use case for arrhythmia detection for
three different design goals, i.e. low-energy, low-power and high-throughput
respectively. Our FPGA implementation outperforms a TensorRT optimized model on
an Nvidia Jetson platform in both throughput and energy consumption.

    

### [[2107.00090] Mesh-based graph convolutional neural networks for modeling materials with microstructure](http://arxiv.org/abs/2107.00090)


  Predicting the evolution of a representative sample of a material with
microstructure is a fundamental problem in homogenization. In this work we
propose a graph convolutional neural network that utilizes the discretized
representation of the initial microstructure directly, without segmentation or
clustering. Compared to feature-based and pixel-based convolutional neural
network models, the proposed method has a number of advantages: (a) it is deep
in that it does not require featurization but can benefit from it, (b) it has a
simple implementation with standard convolutional filters and layers, (c) it
works natively on unstructured and structured grid data without interpolation
(unlike pixel-based convolutional neural networks), and (d) it preserves
rotational invariance like other graph-based convolutional neural networks. We
demonstrate the performance of the proposed network and compare it to
traditional pixel-based convolution neural network models and feature-based
graph convolutional neural networks on multiple large datasets.

    

### [[2107.00436] Overhead-MNIST: Machine Learning Baselines for Image Classification](http://arxiv.org/abs/2107.00436)


  Twenty-three machine learning algorithms were trained then scored to
establish baseline comparison metrics and to select an image classification
algorithm worthy of embedding into mission-critical satellite imaging systems.
The Overhead-MNIST dataset is a collection of satellite images similar in style
to the ubiquitous MNIST hand-written digits found in the machine learning
literature. The CatBoost classifier, Light Gradient Boosting Machine, and
Extreme Gradient Boosting models produced the highest accuracies, Areas Under
the Curve (AUC), and F1 scores in a PyCaret general comparison. Separate
evaluations showed that a deep convolutional architecture was the most
promising. We present results for the overall best performing algorithm as a
baseline for edge deployability and future performance improvement: a
convolutional neural network (CNN) scoring 0.965 categorical accuracy on unseen
test data.

    

### [[2107.01091] CrowdSpeech and VoxDIY: Benchmark Datasets for Crowdsourced Audio Transcription](http://arxiv.org/abs/2107.01091)


  Domain-specific data is the crux of the successful transfer of machine
learning systems from benchmarks to real life. In simple problems such as image
classification, crowdsourcing has become one of the standard tools for cheap
and time-efficient data collection: thanks in large part to advances in
research on aggregation methods. However, the applicability of crowdsourcing to
more complex tasks (e.g., speech recognition) remains limited due to the lack
of principled aggregation methods for these modalities. The main obstacle
towards designing aggregation methods for more advanced applications is the
absence of training data, and in this work, we focus on bridging this gap in
speech recognition. For this, we collect and release CrowdSpeech -- the first
publicly available large-scale dataset of crowdsourced audio transcriptions.
Evaluation of existing and novel aggregation methods on our data shows room for
improvement, suggesting that our work may entail the design of better
algorithms. At a higher level, we also contribute to the more general challenge
of developing the methodology for reliable data collection via crowdsourcing.
In that, we design a principled pipeline for constructing datasets of
crowdsourced audio transcriptions in any novel domain. We show its
applicability on an under-resourced language by constructing VoxDIY -- a
counterpart of CrowdSpeech for the Russian language. We also release the code
that allows a full replication of our data collection pipeline and share
various insights on best practices of data collection via crowdsourcing.

    

### [[2108.10132] TRAPDOOR: Repurposing backdoors to detect dataset bias in machine learning-based genomic analysis](http://arxiv.org/abs/2108.10132)


  Machine Learning (ML) has achieved unprecedented performance in several
applications including image, speech, text, and data analysis. Use of ML to
understand underlying patterns in gene mutations (genomics) has far-reaching
results, not only in overcoming diagnostic pitfalls, but also in designing
treatments for life-threatening diseases like cancer. Success and
sustainability of ML algorithms depends on the quality and diversity of data
collected and used for training. Under-representation of groups (ethnic groups,
gender groups, etc.) in such a dataset can lead to inaccurate predictions for
certain groups, which can further exacerbate systemic discrimination issues.
In this work, we propose TRAPDOOR, a methodology for identification of biased
datasets by repurposing a technique that has been mostly proposed for nefarious
purposes: Neural network backdoors. We consider a typical collaborative
learning setting of the genomics supply chain, where data may come from
hospitals, collaborative projects, or research institutes to a central cloud
without awareness of bias against a sensitive group. In this context, we
develop a methodology to leak potential bias information of the collective data
without hampering the genuine performance using ML backdooring catered for
genomic applications. Using a real-world cancer dataset, we analyze the dataset
with the bias that already existed towards white individuals and also
introduced biases in datasets artificially, and our experimental result show
that TRAPDOOR can detect the presence of dataset bias with 100% accuracy, and
furthermore can also extract the extent of bias by recovering the percentage
with a small error.

    

### [[2109.09692] Modeling Regime Shifts in Multiple Time Series](http://arxiv.org/abs/2109.09692)


  We investigate the problem of discovering and modeling regime shifts in an
ecosystem comprising multiple time series known as co-evolving time series.
Regime shifts refer to the changing behaviors exhibited by series at different
time intervals. Learning these changing behaviors is a key step toward time
series forecasting. While advances have been made, existing methods suffer from
one or more of the following shortcomings: (1) failure to take relationships
between time series into consideration for discovering regimes in multiple time
series; (2) lack of an effective approach that models time-dependent behaviors
exhibited by series; (3) difficulties in handling data discontinuities which
may be informative. Most of the existing methods are unable to handle all of
these three issues in a unified framework. This, therefore, motivates our
effort to devise a principled approach for modeling interactions and
time-dependency in co-evolving time series. Specifically, we model an ecosystem
of multiple time series by summarizing the heavy ensemble of time series into a
lighter and more meaningful structure called a \textit{mapping grid}. By using
the mapping grid, our model first learns time series behavioral dependencies
through a dynamic network representation, then learns the regime transition
mechanism via a full time-dependent Cox regression model. The originality of
our approach lies in modeling interactions between time series in regime
identification and in modeling time-dependent regime transition probabilities,
usually assumed to be static in existing work.

    

### [[2110.08826] Exploring Deep Neural Networks on Edge TPU](http://arxiv.org/abs/2110.08826)


  This paper explores the performance of Google's Edge TPU on feed forward
neural networks. We consider Edge TPU as a hardware platform and explore
different architectures of deep neural network classifiers, which traditionally
has been a challenge to run on resource constrained edge devices. Based on the
use of a joint-time-frequency data representation, also known as spectrogram,
we explore the trade-off between classification performance and the energy
consumed for inference. The energy efficiency of Edge TPU is compared with that
of widely-used embedded CPU ARM Cortex-A53. Our results quantify the impact of
neural network architectural specifications on the Edge TPU's performance,
guiding decisions on the TPU's optimal operating point, where it can provide
high classification accuracy with minimal energy consumption. Also, our
evaluations highlight the crossover in performance between the Edge TPU and
Cortex-A53, depending on the neural network specifications. Based on our
analysis, we provide a decision chart to guide decisions on platform selection
based on the model parameters and context.

    

### [[2110.08871] Noise-robust Clustering](http://arxiv.org/abs/2110.08871)


  This paper presents noise-robust clustering techniques in unsupervised
machine learning. The uncertainty about the noise, consistency, and other
ambiguities can become severe obstacles in data analytics. As a result, data
quality, cleansing, management, and governance remain critical disciplines when
working with Big Data. With this complexity, it is no longer sufficient to
treat data deterministically as in a classical setting, and it becomes
meaningful to account for noise distribution and its impact on data sample
values. Classical clustering methods group data into "similarity classes"
depending on their relative distances or similarities in the underlying space.
This paper addressed this problem via the extension of classical $K$-means and
$K$-medoids clustering over data distributions (rather than the raw data). This
involves measuring distances among distributions using two types of measures:
the optimal mass transport (also called Wasserstein distance, denoted $W_2$)
and a novel distance measure proposed in this paper, the expected value of
random variable distance (denoted ED). The presented distribution-based
$K$-means and $K$-medoids algorithms cluster the data distributions first and
then assign each raw data to the cluster of data's distribution.

    

### [[2110.08896] Damped Anderson Mixing for Deep Reinforcement Learning: Acceleration, Convergence, and Stabilization](http://arxiv.org/abs/2110.08896)


  Anderson mixing has been heuristically applied to reinforcement learning (RL)
algorithms for accelerating convergence and improving the sampling efficiency
of deep RL. Despite its heuristic improvement of convergence, a rigorous
mathematical justification for the benefits of Anderson mixing in RL has not
yet been put forward. In this paper, we provide deeper insights into a class of
acceleration schemes built on Anderson mixing that improve the convergence of
deep RL algorithms. Our main results establish a connection between Anderson
mixing and quasi-Newton methods and prove that Anderson mixing increases the
convergence radius of policy iteration schemes by an extra contraction factor.
The key focus of the analysis roots in the fixed-point iteration nature of RL.
We further propose a stabilization strategy by introducing a stable
regularization term in Anderson mixing and a differentiable, non-expansive
MellowMax operator that can allow both faster convergence and more stable
behavior. Extensive experiments demonstrate that our proposed method enhances
the convergence, stability, and performance of RL algorithms.

    

### [[2110.09401] Mesh Convolutional Autoencoder for Semi-Regular Meshes of Different Sizes](http://arxiv.org/abs/2110.09401)


  The analysis of deforming 3D surface meshes is accelerated by autoencoders
since the low-dimensional embeddings can be used to visualize underlying
dynamics. But, state-of-the-art mesh convolutional autoencoders require a fixed
connectivity of all input meshes handled by the autoencoder. This is due to
either the use of spectral convolutional layers or mesh dependent pooling
operations. Therefore, the types of datasets that one can study are limited and
the learned knowledge cannot be transferred to other datasets that exhibit
similar behavior. To address this, we transform the discretization of the
surfaces to semi-regular meshes that have a locally regular connectivity and
whose meshing is hierarchical. This allows us to apply the same spatial
convolutional filters to the local neighborhoods and to define a pooling
operator that can be applied to every semi-regular mesh. We apply the same mesh
autoencoder to different datasets and our reconstruction error is more than 50%
lower than the error from state-of-the-art models, which have to be trained for
every mesh separately. Additionally, we visualize the underlying dynamics of
unseen mesh sequences with an autoencoder trained on different classes of
meshes.

    

### [[2105.09142] Laughing Heads: Can Transformers Detect What Makes a Sentence Funny?](http://arxiv.org/abs/2105.09142)


  The automatic detection of humor poses a grand challenge for natural language
processing. Transformer-based systems have recently achieved remarkable results
on this task, but they usually (1)~were evaluated in setups where serious vs
humorous texts came from entirely different sources, and (2)~focused on
benchmarking performance without providing insights into how the models work.
We make progress in both respects by training and analyzing transformer-based
humor recognition models on a recently introduced dataset consisting of minimal
pairs of aligned sentences, one serious, the other humorous. We find that,
although our aligned dataset is much harder than previous datasets,
transformer-based models recognize the humorous sentence in an aligned pair
with high accuracy (78%). In a careful error analysis, we characterize easy vs
hard instances. Finally, by analyzing attention weights, we obtain important
insights into the mechanisms by which transformers recognize humor. Most
remarkably, we find clear evidence that one single attention head learns to
recognize the words that make a test sentence humorous, even without access to
this information at training time.

    

### [[2107.06013] Barriers and Dynamical Paths in Alternating Gibbs Sampling of Restricted Boltzmann Machines](http://arxiv.org/abs/2107.06013)


  Restricted Boltzmann Machines (RBM) are bi-layer neural networks used for the
unsupervised learning of model distributions from data. The bipartite
architecture of RBM naturally defines an elegant sampling procedure, called
Alternating Gibbs Sampling (AGS), where the configurations of the
latent-variable layer are sampled conditional to the data-variable layer, and
vice versa. We study here the performance of AGS on several analytically
tractable models borrowed from statistical mechanics. We show that standard AGS
is not more efficient than classical Metropolis-Hastings (MH) sampling of the
effective energy landscape defined on the data layer. However, RBM can identify
meaningful representations of training data in their latent space. Furthermore,
using these representations and combining Gibbs sampling with the MH algorithm
in the latent space can enhance the sampling performance of the RBM when the
hidden units encode weakly dependent features of the data. We illustrate our
findings on three datasets: Bars and Stripes and MNIST, well known in machine
learning, and the so-called Lattice Proteins, introduced in theoretical biology
to study the sequence-to-structure mapping in proteins.

    

### [[2110.10291] A Deeper Look into RowHammer`s Sensitivities: Experimental Analysis of Real DRAM Chips and Implications on Future Attacks and Defenses](http://arxiv.org/abs/2110.10291)


  RowHammer is a circuit-level DRAM vulnerability where repeatedly accessing
(i.e., hammering) a DRAM row can cause bit flips in physically nearby rows. The
RowHammer vulnerability worsens as DRAM cell size and cell-to-cell spacing
shrink. Recent studies demonstrate that modern DRAM chips, including chips
previously marketed as RowHammer-safe, are even more vulnerable to RowHammer
than older chips such that the required hammer count to cause a bit flip has
reduced by more than 10X in the last decade. Therefore, it is essential to
develop a better understanding and in-depth insights into the RowHammer
vulnerability of modern DRAM chips to more effectively secure current and
future systems.
Our goal in this paper is to provide insights into fundamental properties of
the RowHammer vulnerability that are not yet rigorously studied by prior works,
but can potentially be $i$) exploited to develop more effective RowHammer
attacks or $ii$) leveraged to design more effective and efficient defense
mechanisms. To this end, we present an experimental characterization using
248~DDR4 and 24~DDR3 modern DRAM chips from four major DRAM manufacturers
demonstrating how the RowHammer effects vary with three fundamental properties:
1)~DRAM chip temperature, 2)~aggressor row active time, and 3)~victim DRAM
cell's physical location. Among our 16 new observations, we highlight that a
RowHammer bit flip 1)~is very likely to occur in a bounded range, specific to
each DRAM cell (e.g., 5.4% of the vulnerable DRAM cells exhibit errors in the
range 70C to 90C), 2)~is more likely to occur if the aggressor row is active
for longer time (e.g., RowHammer vulnerability increases by 36% if we keep a
DRAM row active for 15 column accesses), and 3)~is more likely to occur in
certain physical regions of the DRAM module under attack (e.g., 5% of the rows
are 2x more vulnerable than the remaining 95% of the rows).

    

### [[2110.10603] Uncovering In-DRAM RowHammer Protection Mechanisms: A New Methodology, Custom RowHammer Patterns, and Implications](http://arxiv.org/abs/2110.10603)


  The RowHammer vulnerability in DRAM is a critical threat to system security.
To protect against RowHammer, vendors commit to security-through-obscurity:
modern DRAM chips rely on undocumented, proprietary, on-die mitigations,
commonly known as Target Row Refresh (TRR). At a high level, TRR detects and
refreshes potential RowHammer-victim rows, but its exact implementations are
not openly disclosed. Security guarantees of TRR mechanisms cannot be easily
studied due to their proprietary nature.
To assess the security guarantees of recent DRAM chips, we present Uncovering
TRR (U-TRR), an experimental methodology to analyze in-DRAM TRR
implementations. U-TRR is based on the new observation that data retention
failures in DRAM enable a side channel that leaks information on how TRR
refreshes potential victim rows. U-TRR allows us to (i) understand how logical
DRAM rows are laid out physically in silicon; (ii) study undocumented on-die
TRR mechanisms; and (iii) combine (i) and (ii) to evaluate the RowHammer
security guarantees of modern DRAM chips. We show how U-TRR allows us to craft
RowHammer access patterns that successfully circumvent the TRR mechanisms
employed in 45 DRAM modules of the three major DRAM vendors. We find that the
DRAM modules we analyze are vulnerable to RowHammer, having bit flips in up to
99.9% of all DRAM rows.

    

### [[2110.10401] Monitoring Collective Communication Among GPUs](http://arxiv.org/abs/2110.10401)


  Communication among devices in multi-GPU systems plays an important role in
terms of performance and scalability. In order to optimize an application,
programmers need to know the type and amount of the communication happening
among GPUs. Although there are prior works to gather this information in MPI
applications on distributed systems and multi-threaded applications on shared
memory systems, there is no tool that identifies communication among GPUs. Our
prior work, ComScribe, presents a point-to-point (P2P) communication detection
tool for GPUs sharing a common host. In this work, we extend ComScribe to
identify communication among GPUs for collective and P2P communication
primitives in NVIDIA's NCCL library. In addition to P2P communications,
collective communications are commonly used in HPC and AI workloads thus it is
important to monitor the induced data movement due to collectives. Our tool
extracts the size and the frequency of data transfers in an application and
visualizes them as a communication matrix. To demonstrate the tool in action,
we present communication matrices and some statistics for two applications
coming from machine translation and image classification domains.

    

### [[2110.10666] Efficient Consensus-Free Weight Reassignment for Atomic Storage](http://arxiv.org/abs/2110.10666)


  Weighted voting is a conventional approach to improving the performance of
replicated systems based on commonly-used majority quorum systems in
heterogeneous environments. In long-lived systems, a weight reassignment
protocol is required to reassign weights over time in order to accommodate
performance variations accordingly. The weight reassignment protocol should be
consensus-free in asynchronous failure-prone systems because of the
impossibility of solving consensus in such systems. This paper presents an
efficient consensus-free weight reassignment protocol for atomic storage
systems in heterogeneous, dynamic, and asynchronous message-passing systems. An
experimental evaluation shows that the proposed protocol improves the
performance of atomic read/write storage implemented by majority quorum systems
compared with previous solutions.

    

### [[2110.10762] Asynchronous parareal time discretization for partial differential equations](http://arxiv.org/abs/2110.10762)


  Asynchronous iterations are more and more investigated for both scaling and
fault-resilience purpose on high performance computing platforms. While so far,
they have been exclusively applied within space domain decomposition
frameworks, this paper advocates a novel application direction targeting
time-decomposed time-parallel approaches. Specifically, an asynchronous
iterative model is derived from the Parareal scheme, for which convergence and
speedup analysis are then conducted. It turned out that Parareal and
async-Parareal feature very close convergence conditions, asymptotically
equivalent, including the finite-time termination property. Based on a
computational cost model aware of unsteady communication delays, our speedup
analysis shows the potential performance gain from asynchronous iterations,
which is confirmed by some experimental case of heat evolution on a homogeneous
supercomputer. This primary work clearly suggests possible further benefits
from asynchronous iterations.

    

### [[2110.10765] Accelerating quantum many-body configuration interaction with directives](http://arxiv.org/abs/2110.10765)


  Many-Fermion Dynamics-nuclear, or MFDn, is a configuration interaction (CI)
code for nuclear structure calculations. It is a platform-independent Fortran
90 code using a hybrid MPI+X programming model. For CPU platforms the
application has a robust and optimized OpenMP implementation for shared memory
parallelism. As part of the NESAP application readiness program for NERSC's
latest Perlmutter system, MFDn has been updated to take advantage of
accelerators. The current mainline GPU port is based on OpenACC. In this work
we describe some of the key challenges of creating an efficient GPU
implementation. Additionally, we compare the support of OpenMP and OpenACC on
AMD and NVIDIA GPUs.

    

### [[2005.13499] Asynchronous Reconfiguration with Byzantine Failures](http://arxiv.org/abs/2005.13499)


  Replicated services are inherently vulnerable to failures and security
breaches. In a long-running system, it is, therefore, indispensable to maintain
a reconfiguration mechanism that would replace faulty replicas with correct
ones. An important challenge is to enable reconfiguration without affecting the
availability and consistency of the replicated data: the clients should be able
to get correct service even when the set of service replicas is being updated.
In this paper, we address the problem of reconfiguration in the presence of
Byzantine failures: faulty replicas or clients may arbitrarily deviate from
their expected behavior. We describe a generic technique for building
asynchronous and Byzantine fault-tolerant reconfigurable objects: clients can
manipulate the object data and issue reconfiguration calls without reaching
consensus on the current configuration. With the help of forward-secure digital
signatures, our solution makes sure that superseded and possibly compromised
configurations are harmless, that slow clients cannot be fooled into reading
stale data, and that Byzantine clients cannot cause a denial of service by
flooding the system with reconfiguration requests. Our approach is modular and
based on dynamic Byzantine lattice agreement abstraction, and we discuss how to
extend it to enable Byzantine fault-tolerant implementations of a large class
of reconfigurable replicated services.

    

### [[2107.00164] MIND: In-Network Memory Management for Disaggregated Data Centers](http://arxiv.org/abs/2107.00164)


  Memory-compute disaggregation promises transparent elasticity, high
utilization and balanced usage for resources in data centers by physically
separating memory and compute into network-attached resource "blades". However,
existing designs achieve performance at the cost of resource elasticity,
restricting memory sharing to a single compute blade to avoid costly memory
coherence traffic over the network.
In this work, we show that emerging programmable network switches can enable
an efficient shared memory abstraction for disaggregated architectures by
placing memory management logic in the network fabric. We find that
centralizing memory management in the network permits bandwidth and
latency-efficient realization of in-network cache coherence protocols, while
programmable switch ASICs support other memory management logic at line-rate.
We realize these insights into MIND, an in-network memory management unit for
rack-scale memory disaggregation. MIND enables transparent resource elasticity
while matching the performance of prior memory disaggregation proposals for
real-world workloads.

    

### [[2110.10185] GenNI: Human-AI Collaboration for Data-Backed Text Generation](http://arxiv.org/abs/2110.10185)


  Table2Text systems generate textual output based on structured data utilizing
machine learning. These systems are essential for fluent natural language
interfaces in tools such as virtual assistants; however, left to generate
freely these ML systems often produce misleading or unexpected outputs. GenNI
(Generation Negotiation Interface) is an interactive visual system for
high-level human-AI collaboration in producing descriptive text. The tool
utilizes a deep learning model designed with explicit control states. These
controls allow users to globally constrain model generations, without
sacrificing the representation power of the deep learning models. The visual
interface makes it possible for users to interact with AI systems following a
Refine-Forecast paradigm to ensure that the generation system acts in a manner
human users find suitable. We report multiple use cases on two experiments that
improve over uncontrolled generation approaches, while at the same time
providing fine-grained control. A demo and source code are available at
this https URL .

    

### [[2110.10205] MultiHead MultiModal Deep Interest Recommendation Network](http://arxiv.org/abs/2110.10205)


  With the development of information technology, human beings are constantly
producing a large amount of information at all times. How to obtain the
information that users are interested in from the large amount of information
has become an issue of great concern to users and even business managers. In
order to solve this problem, from traditional machine learning to deep learning
recommendation systems, researchers continue to improve optimization models and
explore solutions. Because researchers have optimized more on the
recommendation model network structure, they have less research on enriching
recommendation model features, and there is still room for in-depth
recommendation model optimization. Based on the DIN\cite{Authors01} model, this
paper adds multi-head and multi-modal modules, which enriches the feature sets
that the model can use, and at the same time strengthens the cross-combination
and fitting capabilities of the model. Experiments show that the multi-head
multi-modal DIN improves the recommendation prediction effect, and outperforms
current state-of-the-art methods on various comprehensive indicators.

    

### [[2110.10284] flip-hoisting: Exploiting Repeated Parameters in Discrete Probabilistic Programs](http://arxiv.org/abs/2110.10284)


  Probabilistic programming is emerging as a popular and effective means of
probabilistic modeling and an alternative to probabilistic graphical models.
Probabilistic programs provide greater expressivity and flexibility in modeling
probabilistic systems than graphical models, but this flexibility comes at a
cost: there remains a significant disparity in performance between specialized
Bayesian network solvers and probabilistic program inference algorithms. In
this work we present a program analysis and associated optimization,
flip-hoisting, that collapses repetitious parameters in discrete probabilistic
programs to improve inference performance. flip-hoisting generalizes parameter
sharing - a well-known important optimization from discrete graphical models -
to probabilistic programs. We implement flip-hoisting in an existing
probabilistic programming language and show empirically that it significantly
improves inference performance, narrowing the gap between the performances of
probabilistic programs and probabilistic graphical models.

    

### [[2110.10324] Semantic Sensing and Planning for Human-Robot Collaboration in Uncertain Environments](http://arxiv.org/abs/2110.10324)


  Autonomous robots can benefit greatly from human-provided semantic
characterizations of uncertain task environments and states. However, the
development of integrated strategies which let robots model, communicate, and
act on such soft data remains challenging. Here, a framework is presented for
active semantic sensing and planning in human-robot teams which addresses these
gaps by formally combining the benefits of online sampling-based POMDP
policies, multi-modal semantic interaction, and Bayesian data fusion. This
approach lets humans opportunistically impose model structure and extend the
range of semantic soft data in uncertain environments by sketching and labeling
arbitrary landmarks across the environment. Dynamic updating of the environment
while searching for a mobile target allows robotic agents to actively query
humans for novel and relevant semantic data, thereby improving beliefs of
unknown environments and target states for improved online planning. Target
search simulations show significant improvements in time and belief state
estimates required for interception versus conventional planning based solely
on robotic sensing. Human subject studies demonstrate a average doubling in
dynamic target capture rate compared to the lone robot case, employing
reasoning over a range of user characteristics and interaction modalities.
Video of interaction can be found at this https URL.

    

### [[2110.10372] Distributionally Robust Classifiers in Sentiment Analysis](http://arxiv.org/abs/2110.10372)


  In this paper, we propose sentiment classification models based on BERT
integrated with DRO (Distributionally Robust Classifiers) to improve model
performance on datasets with distributional shifts. We added 2-Layer Bi-LSTM,
projection layer (onto simplex or Lp ball), and linear layer on top of BERT to
achieve distributionally robustness. We considered one form of distributional
shift (from IMDb dataset to Rotten Tomatoes dataset). We have confirmed through
experiments that our DRO model does improve performance on our test set with
distributional shift from the training set.

    

### [[2110.10374] Playing 2048 With Reinforcement Learning](http://arxiv.org/abs/2110.10374)


  The game of 2048 is a highly addictive game. It is easy to learn the game,
but hard to master as the created game revealed that only about 1% games out of
hundreds million ever played have been won. In this paper, we would like to
explore reinforcement learning techniques to win 2048. The approaches we have
took include deep Q-learning and beam search, with beam search reaching 2048
28.5 of time.

    

### [[2110.10457] Knowledge Graph informed Fake News Classification via Heterogeneous Representation Ensembles](http://arxiv.org/abs/2110.10457)


  Increasing amounts of freely available data both in textual and relational
form offers exploration of richer document representations, potentially
improving the model performance and robustness. An emerging problem in the
modern era is fake news detection -- many easily available pieces of
information are not necessarily factually correct, and can lead to wrong
conclusions or are used for manipulation. In this work we explore how different
document representations, ranging from simple symbolic bag-of-words, to
contextual, neural language model-based ones can be used for efficient fake
news identification. One of the key contributions is a set of novel document
representation learning methods based solely on knowledge graphs, i.e.
extensive collections of (grounded) subject-predicate-object triplets. We
demonstrate that knowledge graph-based representations already achieve
competitive performance to conventionally accepted representation learners.
Furthermore, when combined with existing, contextual representations, knowledge
graph-based document representations can achieve state-of-the-art performance.
To our knowledge this is the first larger-scale evaluation of how knowledge
graph-based representations can be systematically incorporated into the process
of fake news classification.

    

### [[2110.10474] R4: A Framework for Route Representation and Route Recommendation](http://arxiv.org/abs/2110.10474)


  Route recommendation is significant in navigation service. Two major
challenges for route recommendation are route representation and user
representation. Different from items that can be identified by unique IDs in
traditional recommendation, routes are combinations of links (i.e., a road
segment and its following action like turning left) and the number of
combinations could be close to infinite. Besides, the representation of a route
changes under different scenarios. These facts result in severe sparsity of
routes, which increases the difficulty of route representation. Moreover, link
attribute deficiencies and errors affect preciseness of route representation.
Because of the sparsity of routes, the interaction data between users and
routes are also sparse. This makes it not easy to acquire user representation
from historical user-item interactions as traditional recommendations do. To
address these issues, we propose a novel learning framework R4. In R4, we
design a sparse & dense network to obtain representations of routes. The sparse
unit learns link ID embeddings and aggregates them to represent a route, which
captures implicit route characteristics and subsequently alleviates problems
caused by link attribute deficiencies and errors. The dense unit extracts
implicit local features of routes from link attributes. For user
representation, we utilize a series of historical navigation to extract user
preference. R4 achieves remarkable performance in both offline and online
experiments.

    

### [[2110.10482] Surrogate Representation Learning with Isometric Mapping for Gray-box Graph Adversarial Attacks](http://arxiv.org/abs/2110.10482)


  Gray-box graph attacks aim at disrupting the performance of the victim model
by using inconspicuous attacks with limited knowledge of the victim model. The
parameters of the victim model and the labels of the test nodes are invisible
to the attacker. To obtain the gradient on the node attributes or graph
structure, the attacker constructs an imaginary surrogate model trained under
supervision. However, there is a lack of discussion on the training of
surrogate models and the robustness of provided gradient information. The
general node classification model loses the topology of the nodes on the graph,
which is, in fact, an exploitable prior for the attacker. This paper
investigates the effect of representation learning of surrogate models on the
transferability of gray-box graph adversarial attacks. To reserve the topology
in the surrogate embedding, we propose Surrogate Representation Learning with
Isometric Mapping (SRLIM). By using Isometric mapping method, our proposed
SRLIM can constrain the topological structure of nodes from the input layer to
the embedding space, that is, to maintain the similarity of nodes in the
propagation process. Experiments prove the effectiveness of our approach
through the improvement in the performance of the adversarial attacks generated
by the gradient-based attacker in untargeted poisoning gray-box setups.

    

### [[2110.10718] Bootstrapping confidence in future safety based on past safe operation](http://arxiv.org/abs/2110.10718)


  With autonomous vehicles (AVs), a major concern is the inability to give
meaningful quantitative assurance of safety, to the extent required by society
- e.g. that an AV must be at least as safe as a good human driver - before that
AV is in extensive use. We demonstrate an approach to achieving more moderate,
but useful, confidence, e.g., confidence of low enough probability of causing
accidents in the early phases of operation. This formalises mathematically the
common approach of operating a system on a limited basis in the hope that
mishap-free operation will confirm one's confidence in its safety and allow
progressively more extensive operation: a process of "bootstrapping" of
confidence. Translating that intuitive approach into theorems shows: (1) that
it is substantially sound in the right circumstances, and could be a good
method for deciding about the early deployment phase for an AV; (2) how much
confidence can be rightly derived from such a "cautious deployment" approach,
so that we can avoid over-optimism; (3) under which conditions our sound
formulas for future confidence are applicable; (4) thus, which analyses of the
concrete situations, and/or constraints on practice, are needed in order to
enjoy the advantages of provably correct confidence in adequate future safety.

    

### [[2110.10720] Privacy in Open Search: A Review of Challenges and Solutions](http://arxiv.org/abs/2110.10720)


  Privacy is of worldwide concern regarding activities and processes that
include sensitive data. For this reason, many countries and territories have
been recently approving regulations controlling the extent to which
organizations may exploit data provided by people. Artificial intelligence
areas, such as machine learning and natural language processing, have already
successfully employed privacy-preserving mechanisms in order to safeguard data
privacy in a vast number of applications. Information retrieval (IR) is
likewise prone to privacy threats, such as attacks and unintended disclosures
of documents and search history, which may cripple the security of users and be
penalized by data protection laws. This work aims at highlighting and
discussing open challenges for privacy in the recent literature of IR, focusing
on tasks featuring user-generated text data. Our contribution is threefold:
firstly, we present an overview of privacy threats to IR tasks; secondly, we
discuss applicable privacy-preserving mechanisms which may be employed in
solutions to restrain privacy hazards; finally, we bring insights on the
tradeoffs between privacy preservation and utility performance for IR tasks.

    

### [[2110.10746] Better than Average: Paired Evaluation of NLP Systems](http://arxiv.org/abs/2110.10746)


  Evaluation in NLP is usually done by comparing the scores of competing
systems independently averaged over a common set of test instances. In this
work, we question the use of averages for aggregating evaluation scores into a
final number used to decide which system is best, since the average, as well as
alternatives such as the median, ignores the pairing arising from the fact that
systems are evaluated on the same test instances. We illustrate the importance
of taking the instance-level pairing of evaluation scores into account and
demonstrate, both theoretically and empirically, the advantages of aggregation
methods based on pairwise comparisons, such as the Bradley-Terry (BT) model, a
mechanism based on the estimated probability that a given system scores better
than another on the test set. By re-evaluating 296 real NLP evaluation setups
across four tasks and 18 evaluation metrics, we show that the choice of
aggregation mechanism matters and yields different conclusions as to which
systems are state of the art in about 30% of the setups. To facilitate the
adoption of pairwise evaluation, we release a practical tool for performing the
full analysis of evaluation scores with the mean, median, BT, and two variants
of BT (Elo and TrueSkill), alongside functionality for appropriate statistical
testing.

    

### [[2110.10790] Human-Centered Explainable AI (XAI): From Algorithms to User Experiences](http://arxiv.org/abs/2110.10790)


  As a technical sub-field of artificial intelligence (AI), explainable AI
(XAI) has produced a vast collection of algorithms in recent years. However,
explainability is an inherently human-centric property and the field is
starting to embrace inter-disciplinary perspectives and human-centered
approaches. As researchers and practitioners begin to leverage XAI algorithms
to build XAI applications, explainability has moved beyond a demand by data
scientists or researchers to comprehend the models they are developing, to
become an essential requirement for people to trust and adopt AI deployed in
numerous domains. Human-computer interaction (HCI) research and user experience
(UX) design in this area are therefore increasingly important. In this chapter,
we begin with a high-level overview of the technical landscape of XAI
algorithms, then selectively survey recent HCI work that takes human-centered
approaches to design, evaluate, provide conceptual and methodological tools for
XAI. We ask the question "what are human-centered approaches doing for XAI" and
highlight three roles that they should play in shaping XAI technologies: to
drive technical choices by understanding users' explainability needs, to
uncover pitfalls of existing XAI methods through empirical studies and inform
new methods, and to provide conceptual frameworks for human-compatible XAI.

    

### [[2008.09150] VisualSem: A High-quality Knowledge Graph for Vision and Language](http://arxiv.org/abs/2008.09150)


  An exciting frontier in natural language understanding (NLU) and generation
(NLG) calls for (vision-and-) language models that can efficiently access
external structured knowledge repositories. However, many existing knowledge
bases only cover limited domains, or suffer from noisy data, and most of all
are typically hard to integrate into neural language pipelines. To fill this
gap, we release VisualSem: a high-quality knowledge graph (KG) which includes
nodes with multilingual glosses, multiple illustrative images, and visually
relevant relations. We also release a neural multi-modal retrieval model that
can use images or sentences as inputs and retrieves entities in the KG. This
multi-modal retrieval model can be integrated into any (neural network) model
pipeline. We encourage the research community to use VisualSem for data
augmentation and/or as a source of grounding, among other possible uses.
VisualSem as well as the multi-modal retrieval models are publicly available
and can be downloaded in this URL: this https URL


### [[2009.01485] SAC: Semantic Attention Composition for Text-Conditioned Image Retrieval](http://arxiv.org/abs/2009.01485)


  The ability to efficiently search for images is essential for improving the
user experiences across various products. Incorporating user feedback, via
multi-modal inputs, to navigate visual search can help tailor retrieved results
to specific user queries. We focus on the task of text-conditioned image
retrieval that utilizes support text feedback alongside a reference image to
retrieve images that concurrently satisfy constraints imposed by both inputs.
The task is challenging since it requires learning composite image-text
features by incorporating multiple cross-granular semantic edits from text
feedback and then applying the same to visual features. To address this, we
propose a novel framework SAC which resolves the above in two major steps:
"where to see" (Semantic Feature Attention) and "how to change" (Semantic
Feature Modification). We systematically show how our architecture streamlines
the generation of text-aware image features by removing the need for various
modules required by other state-of-art techniques. We present extensive
quantitative, qualitative analysis, and ablation studies, to show that our
architecture SAC outperforms existing techniques by achieving state-of-the-art
performance on 3 benchmark datasets: FashionIQ, Shoes, and Birds-to-Words,
while supporting natural language feedback of varying lengths.

    

### [[2010.07668] Inducing Alignment Structure with Gated Graph Attention Networks for Sentence Matching](http://arxiv.org/abs/2010.07668)


  Sentence matching is a fundamental task of natural language processing with
various applications. Most recent approaches adopt attention-based neural
models to build word- or phrase-level alignment between two sentences. However,
these models usually ignore the inherent structure within the sentences and
fail to consider various dependency relationships among text units. To address
these issues, this paper proposes a graph-based approach for sentence matching.
First, we represent a sentence pair as a graph with several carefully design
strategies. We then employ a novel gated graph attention network to encode the
constructed graph for sentence matching. Experimental results demonstrate that
our method substantially achieves state-of-the-art performance on two datasets
across tasks of natural language and paraphrase identification. Further
discussions show that our model can learn meaningful graph structure,
indicating its superiority on improved interpretability.

    

### [[2102.08124] Accelerated Sparse Neural Training: A Provable and Efficient Method to Find N:M Transposable Masks](http://arxiv.org/abs/2102.08124)


  Unstructured pruning reduces the memory footprint in deep neural networks
(DNNs). Recently, researchers proposed different types of structural pruning
intending to reduce also the computation complexity. In this work, we first
suggest a new measure called mask-diversity which correlates with the expected
accuracy of the different types of structural pruning. We focus on the recently
suggested N:M fine-grained block sparsity mask, in which for each block of M
weights, we have at least N zeros. While N:M fine-grained block sparsity allows
acceleration in actual modern hardware, it can be used only to accelerate the
inference phase. In order to allow for similar accelerations in the training
phase, we suggest a novel transposable fine-grained sparsity mask, where the
same mask can be used for both forward and backward passes. Our transposable
mask guarantees that both the weight matrix and its transpose follow the same
sparsity pattern; thus, the matrix multiplication required for passing the
error backward can also be accelerated. We formulate the problem of finding the
optimal transposable-mask as a minimum-cost flow problem. Additionally, to
speed up the minimum-cost flow computation, we also introduce a fast
linear-time approximation that can be used when the masks dynamically change
during training. Our experiments suggest a 2x speed-up in the matrix
multiplications with no accuracy degradation over vision and language models.
Finally, to solve the problem of switching between different structure
constraints, we suggest a method to convert a pre-trained model with
unstructured sparsity to an N:M fine-grained block sparsity model with little
to no training. A reference implementation can be found at
this https URL.

    

### [[2104.08667] SIMMC 2.0: A Task-oriented Dialog Dataset for Immersive Multimodal Conversations](http://arxiv.org/abs/2104.08667)


  Next generation task-oriented dialog systems need to understand
conversational contexts with their perceived surroundings, to effectively help
users in the real-world multimodal environment. Existing task-oriented dialog
datasets aimed towards virtual assistance fall short and do not situate the
dialog in the user's multimodal context. To overcome, we present a new dataset
for Situated and Interactive Multimodal Conversations, SIMMC 2.0, which
includes 11K task-oriented user<->assistant dialogs (117K utterances) in the
shopping domain, grounded in immersive and photo-realistic scenes.
The dialogs are collected using a two-phase pipeline: (1) A novel multimodal
dialog simulator generates simulated dialog flows, with an emphasis on
diversity and richness of interactions, (2) Manual paraphrasing of the
generated utterances to collect diverse referring expressions. We provide an
in-depth analysis of the collected dataset, and describe in detail the four
main benchmark tasks we propose. Our baseline model, powered by the
state-of-the-art language model, shows promising results, and highlights new
challenges and directions for the community to study.

    

### [[2104.13130] Secure and Efficient Federated Learning Through Layering and Sharding Blockchain](http://arxiv.org/abs/2104.13130)


  Federated learning (FL) has emerged as a promising master/slave learning
paradigm to alleviate systemic privacy risks and communication costs incurred
by cloud-centric machine learning methods. However, it is very challenging to
resist the single point of failure of the master aggregator and attacks from
malicious participants while guaranteeing model convergence speed and accuracy.
Recently, blockchain has been brought into FL systems transforming the paradigm
to a decentralized manner thus further improve the system security and learning
reliability. Unfortunately, the traditional consensus mechanism and
architecture of blockchain systems can hardly handle the large-scale FL task
due to the huge resource consumption, limited transaction throughput, and high
communication complexity. To address these issues, this paper proposes a
two-layer blockchaindriven FL framework, called as ChainsFL, which is composed
of multiple subchain networks (subchain layer) and a direct acyclic graph
(DAG)-based mainchain (mainchain layer). In ChainsFL, the subchain layer limits
the scale of each shard for a small range of information exchange, and the
mainchain layer allows each shard to share and validate the learning model in
parallel and asynchronously to improve the efficiency of cross-shard
validation. Furthermore, the FL procedure is customized to deeply integrate
with blockchain technology, and the modified DAG consensus mechanism is
proposed to mitigate the distortion caused by abnormal models. In order to
provide a proof-ofconcept implementation and evaluation, multiple subchains
base on Hyperledger Fabric are deployed as the subchain layer, and the
self-developed DAG-based mainchain is deployed as the mainchain layer. The
experimental results show that ChainsFL provides acceptable and sometimes
better training efficiency and stronger robustness compared with the typical
existing FL systems.

    

### [[2108.00968] Robust Semantic Segmentation with Superpixel-Mix](http://arxiv.org/abs/2108.00968)


  Along with predictive performance and runtime speed, reliability is a key
requirement for real-world semantic segmentation. Reliability encompasses
robustness, predictive uncertainty and reduced bias. To improve reliability, we
introduce Superpixel-mix, a new superpixel-based data augmentation method with
teacher-student consistency training. Unlike other mixing-based augmentation
techniques, mixing superpixels between images is aware of object boundaries,
while yielding consistent gains in segmentation accuracy. Our proposed
technique achieves state-of-the-art results in semi-supervised semantic
segmentation on the Cityscapes dataset. Moreover, Superpixel-mix improves the
reliability of semantic segmentation by reducing network uncertainty and bias,
as confirmed by competitive results under strong distributions shift (adverse
weather, image corruptions) and when facing out-of-distribution data.

    

### [[2110.10357] Fast Bitmap Fit: A CPU Cache Line friendly memory allocator for single object allocations](http://arxiv.org/abs/2110.10357)


  Applications making excessive use of single-object based data structures
(such as linked lists, trees, etc...) can see a drop in efficiency over a
period of time due to the randomization of nodes in memory. This slow down is
due to the ineffective use of the CPU's L1/L2 cache. We present a novel
approach for mitigating this by presenting the design of a single-object memory
allocator that preserves memory locality across randomly ordered memory
allocations and deallocations.

    

### [[2110.10151] Can Fortran's 'do concurrent' replace directives for accelerated computing?](http://arxiv.org/abs/2110.10151)


  Recently, there has been growing interest in using standard language
constructs (e.g. C++'s Parallel Algorithms and Fortran's do concurrent) for
accelerated computing as an alternative to directive-based APIs (e.g. OpenMP
and OpenACC). These constructs have the potential to be more portable, and some
compilers already (or have plans to) support such standards. Here, we look at
the current capabilities, portability, and performance of replacing directives
with Fortran's do concurrent using a mini-app that currently implements OpenACC
for GPU-acceleration and OpenMP for multi-core CPU parallelism. We replace as
many directives as possible with do concurrent, testing various configurations
and compiler options within three major compilers: GNU's gfortran, NVIDIA's
nvfortran, and Intel's ifort. We find that with the right compiler versions and
flags, many directives can be replaced without loss of performance or
portability, and, in the case of nvfortran, they can all be replaced. We
discuss limitations that may apply to more complicated codes and future
language additions that may mitigate them. The software and Singularity
containers are publicly provided to allow the results to be reproduced.

    

### [[1903.00982] Oxide: The Essence of Rust](http://arxiv.org/abs/1903.00982)


  Rust claims to advance industrial programming by bridging the gap between
low-level systems programming and high-level application programming. At the
heart of the argument that this enables programmers to build more reliable and
efficient software is the borrow checker - a novel approach to ownership that
aims to balance type system expressivity with usability. And yet, to date there
is no core type system that captures Rust's notion of ownership and borrowing,
and hence no foundation for research on Rust to build upon.
In this work, we set out to capture the essence of this model of ownership by
developing a type systems account of Rust's borrow checker. We present Oxide, a
formalized programming language close to source-level Rust (but with
fully-annotated types). This presentation takes a new view of lifetimes as an
approximation of the provenances of references, and our type system is able to
automatically compute this information through a substructural typing judgment.
We provide the first syntactic proof of type safety for borrow checking using
progress and preservation. Oxide is a simpler formulation of borrow checking -
including recent features such as non-lexical lifetimes - that we hope
researchers will be able to use as the basis for work on Rust.

    

### [<title>How can I train use grow_gpu_hist but predict on cpu? - XGBoost</title>](https://discuss.xgboost.ai/t/how-can-i-train-use-grow-gpu-hist-but-predict-on-cpu/2509/2)

### [<title>Docker image doesn't run with XGBoost version 1.5.0 - XGBoost</title>](https://discuss.xgboost.ai/t/docker-image-doesnt-run-with-xgboost-version-1-5-0/2513/1)