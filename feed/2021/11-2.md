
## 2021-11-2

### [<title>SIGSEGV errors and python crashing - XGBoost</title>](https://discuss.xgboost.ai/t/sigsegv-errors-and-python-crashing/2519/3)

### [[2111.00008] Reinforced Workload Distribution Fairness](http://arxiv.org/abs/2111.00008)


  Network load balancers are central components in data centers, that
distributes workloads across multiple servers and thereby contribute to
offering scalable services. However, when load balancers operate in dynamic
environments with limited monitoring of application server loads, they rely on
heuristic algorithms that require manual configurations for fairness and
performance. To alleviate that, this paper proposes a distributed asynchronous
reinforcement learning mechanism to-with no active load balancer state
monitoring and limited network observations-improve the fairness of the
workload distribution achieved by a load balancer. The performance of proposed
mechanism is evaluated and compared with stateof-the-art load balancing
algorithms in a simulator, under configurations with progressively increasing
complexities. Preliminary results show promise in RLbased load balancing
algorithms, and identify additional challenges and future research directions,
including reward function design and model scalability.

    

### [[2111.00240] Heuristic and Reinforcement Learning Algorithms for Dynamic Service Placement on Mobile Edge Cloud](http://arxiv.org/abs/2111.00240)


  Edge computing hosts applications close to the end users and enables
low-latency real-time applications. Modern applications inturn have adopted the
microservices architecture which composes applications as loosely coupled
smaller components, or services. This complements edge computing infrastructure
that are often resource constrained and may not handle monolithic applications.
Instead, edge servers can independently deploy application service components,
although at the cost of communication overheads. Consistently meeting
application service level objectives while also optimizing application
deployment (placement and migration of services) cost and communication
overheads in mobile edge cloud environment is non-trivial. In this paper we
propose and evaluate three dynamic placement strategies, two heuristic (greedy
approximation based on set cover, and integer programming based optimization)
and one learning-based algorithm. Their goal is to satisfy the application
constraints, minimize infrastructure deployment cost, while ensuring
availability of services to all clients and User Equipment (UE) in the network
coverage area. The algorithms can be extended to any network topology and
microservice based edge computing applications. For the experiments, we use the
drone swarm navigation as a representative application for edge computing use
cases. Since access to real-world physical testbed for such application is
difficult, we demonstrate the efficacy of our algorithms as a simulation. We
also contrast these algorithms with respect to placement quality, utilization
of clusters, and level of determinism. Our evaluation not only shows that the
learning-based algorithm provides solutions of better quality; it also provides
interesting conclusions regarding when the (more traditional) heuristic
algorithms are actually better suited.

    

### [[2111.00289] Intrusion Prevention through Optimal Stopping](http://arxiv.org/abs/2111.00289)


  We study automated intrusion prevention using reinforcement learning.
Following a novel approach, we formulate the problem of intrusion prevention as
an (optimal) multiple stopping problem. This formulation gives us insight into
the structure of optimal policies, which we show to have threshold properties.
For most practical cases, it is not feasible to obtain an optimal defender
policy using dynamic programming. We therefore develop a reinforcement learning
approach to approximate an optimal policy. Our method for learning and
validating policies includes two systems: a simulation system where defender
policies are incrementally learned and an emulation system where statistics are
produced that drive simulation runs and where learned policies are evaluated.
We show that our approach can produce effective defender policies for a
practical IT infrastructure of limited size. Inspection of the learned policies
confirms that they exhibit threshold properties.

    

### [[2111.00401] On multiple IoT data streams processing using LoRaWAN](http://arxiv.org/abs/2111.00401)


  LoraWAN has turned out to be one of the most successful frameworks in IoT
devices. Real world scenarios demand the use of such networks along with a
robust stream processing application layer. To maintain the exactly once
processing semantics one must ensure that we have proper ways to proactively
detect message drops and handle the same. An important use case where stream
processing plays a crucial role is joining various data streams that are
transmitted via gateways connected to edge devices which are related to each
other as part of some common business requirement. LoraWAN supports
connectivity to multiple gateways for edge devices and by virtue of its
different device classes, the network can send and receive messages in an
effective way that conserves battery power as well as network bandwidth. Rather
than relying on explicit acknowledgements for the transmitted messages we take
the advantage of these characteristics of the devices to detect , handle
missing messages and finally process them.

    

### [[2111.00416] How BlockChain Can Help Enhance The Security And Privacy in Edge Computing?](http://arxiv.org/abs/2111.00416)


  In order to solve security and privacy issues of centralized cloud services,
the edge computing network is introduced, where computing and storage resources
are distributed to the edge of the network. However, native edge computing is
subject to the limited performance of edge devices, which causes challenges in
data authorization, data encryption, user privacy, and other fields. Blockchain
is currently the hottest technology for distributed networks. It solves the
consistent issue of distributed data and is used in many areas, such as
cryptocurrency, smart grid, and the Internet of Things.
Our work discussed the security and privacy challenges of edge computing
networks. From the perspectives of data authorization, encryption, and user
privacy, we analyze the solutions brought by blockchain technology to edge
computing networks. In this work, we deeply present the benefits from the
integration of the edge computing network and blockchain technology, which
effectively controls the data authorization and data encryption of the edge
network and enhances the architecture's scalability under the premise of
ensuring security and privacy. Finally, we investigate challenges on storage,
workload, and latency for future research in this field.

    

### [[2111.00488] Beyond Bufferbloat: End-to-End Congestion Control Cannot Avoid Latency Spikes](http://arxiv.org/abs/2111.00488)


  End-to-end congestion control is the main method of congestion control in the
Internet, and achieving consistent low queuing latency with end-to-end methods
is a very active area of research. Even so, achieving consistent low queuing
latency in the Internet still remains an unsolved problem. Therefore, we ask
"What are the fundamental limits of end-to-end congestion control?" We find
that the unavoidable queuing latency for best-case end-to-end congestion
control is on the order of hundreds of milliseconds under conditions that are
common in the Internet. Our argument depends on two things: The latency of
congestion signaling -- at minimum the speed of light -- and the fact that link
capacity may change rapidly for an end-to-end path in the Internet.

    

### [[2111.00524] Intermodulation Interference Detection in 6G Networks: A Machine Learning Approach](http://arxiv.org/abs/2111.00524)


  This letter demonstrates the use of machine learning to detect the presence
of intermodulation interference across several wireless carriers. We propose an
algorithm that detects presence of intermodulation interference through the use
of linear regression as a supervised machine learning technique. Furthermore,
we show that our proposed algorithm can additionally detect narrow-band
interference. Our proposed algorithm can run in near-constant time complexity,
making it a suitable real-time radio resource management application for the
sixth generation of wireless communication and beyond.

    

### [[2111.00601] Explainable Artificial Intelligence for Smart City Application: A Secure and Trusted Platform](http://arxiv.org/abs/2111.00601)


  Artificial Intelligence (AI) is one of the disruptive technologies that is
shaping the future. It has growing applications for data-driven decisions in
major smart city solutions, including transportation, education, healthcare,
public governance, and power systems. At the same time, it is gaining
popularity in protecting critical cyber infrastructure from cyber threats,
attacks, damages, or unauthorized access. However, one of the significant
issues of those traditional AI technologies (e.g., deep learning) is that the
rapid progress in complexity and sophistication propelled and turned out to be
uninterpretable black boxes. On many occasions, it is very challenging to
understand the decision and bias to control and trust systems' unexpected or
seemingly unpredictable outputs. It is acknowledged that the loss of control
over interpretability of decision-making becomes a critical issue for many
data-driven automated applications. But how may it affect the system's security
and trustworthiness? This chapter conducts a comprehensive study of machine
learning applications in cybersecurity to indicate the need for explainability
to address this question. While doing that, this chapter first discusses the
black-box problems of AI technologies for Cybersecurity applications in smart
city-based solutions. Later, considering the new technological paradigm,
Explainable Artificial Intelligence (XAI), this chapter discusses the
transition from black-box to white-box. This chapter also discusses the
transition requirements concerning the interpretability, transparency,
understandability, and Explainability of AI-based technologies in applying
different autonomous systems in smart cities. Finally, it has presented some
commercial XAI platforms that offer explainability over traditional AI
technologies before presenting future challenges and opportunities.

    

### [[2012.11867] Intelligent Resource Allocation in Dense LoRa Networks using Deep Reinforcement Learning](http://arxiv.org/abs/2012.11867)


  The anticipated increase in the count of IoT devices in the coming years
motivates the development of efficient algorithms that can help in their
effective management while keeping the power consumption low. In this paper, we
propose an intelligent multi-channel resource allocation algorithm for dense
LoRa networks termed LoRaDRL and provide a detailed performance evaluation. Our
results demonstrate that the proposed algorithm not only significantly improves
LoRaWAN's packet delivery ratio (PDR) but is also able to support mobile
end-devices (EDs) while ensuring lower power consumption hence increasing both
the lifetime and capacity of the network.} Most previous works focus on
proposing different MAC protocols for improving the network capacity, i.e.,
LoRaWAN, delay before transmit etc. We show that through the use of LoRaDRL, we
can achieve the same efficiency with ALOHA \textcolor{black}{compared to
LoRaSim, and LoRa-MAB while moving the complexity from EDs to the gateway thus
making the EDs simpler and cheaper. Furthermore, we test the performance of
LoRaDRL under large-scale frequency jamming attacks and show its adaptiveness
to the changes in the environment. We show that LoRaDRL's output improves the
performance of state-of-the-art techniques resulting in some cases an
improvement of more than 500\% in terms of PDR compared to learning-based
techniques.

    

### [[2103.09323] Intelligent Reflecting Surface-aided URLLC in a Factory Automation Scenario](http://arxiv.org/abs/2103.09323)


  Different from conventional wired line connections, industrial control
through wireless transmission is widely regarded as a promising solution due to
its reduced cost, increased long-term reliability, and enhanced reliability.
However, mission-critical applications impose stringent quality of service
(QoS) requirements that entail ultra-reliability low-latency communications
(URLLC). The primary feature of URLLC is that the blocklength of channel codes
is short, and the conventional Shannon's Capacity is not applicable. In this
paper, we consider the URLLC in a factory automation (FA) scenario. Due to
densely deployed equipment in FA, wireless signal are easily blocked by the
obstacles. To address this issue, we propose to deploy intelligent reflecting
surface (IRS) to create an alternative transmission link, which can enhance the
transmission reliability. In this paper, we focus on the performance analysis
for IRS-aided URLLC-enabled communications in a FA scenario. Both the average
data rate (ADR) and the average decoding error probability (ADEP) are derived
under finite channel blocklength for seven cases: 1) Rayleigh fading channel;
2) With direct channel link; 3) Nakagami-m fading channel; 4) Imperfect phase
alignment; 5) Multiple-IRS case; 6) Rician fading channel; 7) Correlated
channels. Extensive numerical results are provided to verify the accuracy of
our derived results.

    

### [[2111.00002] Earning Sans Learning: Noisy Decision-Making and Labor Supply on Gig Economy Platforms](http://arxiv.org/abs/2111.00002)


  We study a gig economy platform's problem of finding optimal compensation
schemes when faced with workers who myopically base their participation
decisions on limited information with respect to their earnings. The stylized
model we consider captures two key, related features absent from prior work on
the operations of on-demand service platforms: (i) workers' lack of information
regarding the distribution from which their earnings are drawn and (ii) worker
decisions that are sensitive to variability in earnings. Despite its stylized
nature, our model induces a complex stochastic optimization problem whose
natural fluid relaxation is also a priori intractable. Nevertheless, we uncover
a surprising structural property of the relaxation that allows us to design a
tractable, fast-converging heuristic policy that is asymptotically optimal
amongst the space of all policies that fulfill a fairness property. In doing
so, via both theory and extensive simulations, we uncover phenomena that may
arise when earnings are volatile and hard to predict, as both the empirical
literature and our own data-driven observations suggest may be prevalent on gig
economy platforms.

    

### [[2111.00006] Adaptive Hierarchical Similarity Metric Learning with Noisy Labels](http://arxiv.org/abs/2111.00006)


  Deep Metric Learning (DML) plays a critical role in various machine learning
tasks. However, most existing deep metric learning methods with binary
similarity are sensitive to noisy labels, which are widely present in
real-world data. Since these noisy labels often cause severe performance
degradation, it is crucial to enhance the robustness and generalization ability
of DML. In this paper, we propose an Adaptive Hierarchical Similarity Metric
Learning method. It considers two noise-insensitive information, \textit{i.e.},
class-wise divergence and sample-wise consistency. Specifically, class-wise
divergence can effectively excavate richer similarity information beyond binary
in modeling by taking advantage of Hyperbolic metric learning, while
sample-wise consistency can further improve the generalization ability of the
model using contrastive augmentation. More importantly, we design an adaptive
strategy to integrate this information in a unified view. It is noteworthy that
the new method can be extended to any pair-based metric loss. Extensive
experimental results on benchmark datasets demonstrate that our method achieves
state-of-the-art performance compared with current deep metric learning
approaches.

    

### [[2111.00007] Domain Agnostic Few-Shot Learning For Document Intelligence](http://arxiv.org/abs/2111.00007)


  Few-shot learning aims to generalize to novel classes with only a few samples
with class labels. Research in few-shot learning has borrowed techniques from
transfer learning, metric learning, meta-learning, and Bayesian methods. These
methods also aim to train models from limited training samples, and while
encouraging performance has been achieved, they often fail to generalize to
novel domains. Many of the existing meta-learning methods rely on training data
for which the base classes are sampled from the same domain as the novel
classes used for meta-testing. However, in many applications in the industry,
such as document classification, collecting large samples of data for
meta-learning is infeasible or impossible. While research in the field of the
cross-domain few-shot learning exists, it is mostly limited to computer vision.
To our knowledge, no work yet exists that examines the use of few-shot learning
for classification of semi-structured documents (scans of paper documents)
generated as part of a business workflow (forms, letters, bills, etc.). Here
the domain shift is significant, going from natural images to the
semi-structured documents of interest. In this work, we address the problem of
few-shot document image classification under domain shift. We evaluate our work
by extensive comparisons with existing methods. Experimental results
demonstrate that the proposed method shows consistent improvements on the
few-shot classification performance under domain shift.

    

### [[2111.00009] Revisiting joint decoding based multi-talker speech recognition with DNN acoustic model](http://arxiv.org/abs/2111.00009)


  In typical multi-talker speech recognition systems, a neural network-based
acoustic model predicts senone state posteriors for each speaker. These are
later used by a single-talker decoder which is applied on each speaker-specific
output stream separately. In this work, we argue that such a scheme is
sub-optimal and propose a principled solution that decodes all speakers
jointly. We modify the acoustic model to predict joint state posteriors for all
speakers, enabling the network to express uncertainty about the attribution of
parts of the speech signal to the speakers. We employ a joint decoder that can
make use of this uncertainty together with higher-level language information.
For this, we revisit decoding algorithms used in factorial generative models in
early multi-talker speech recognition systems. In contrast with these early
works, we replace the GMM acoustic model with DNN, which provides greater
modeling power and simplifies part of the inference. We demonstrate the
advantage of joint decoding in proof of concept experiments on a mixed-TIDIGITS
dataset.

    

### [[2111.00010] Federated Semi-Supervised Learning with Class Distribution Mismatch](http://arxiv.org/abs/2111.00010)


  Many existing federated learning (FL) algorithms are designed for supervised
learning tasks, assuming that the local data owned by the clients are well
labeled. However, in many practical situations, it could be difficult and
expensive to acquire complete data labels. Federated semi-supervised learning
(Fed-SSL) is an attractive solution for fully utilizing both labeled and
unlabeled data. Similar to that encountered in federated supervised learning,
class distribution of labeled/unlabeled data could be non-i.i.d. among clients.
Besides, in each client, the class distribution of labeled data may be distinct
from that of unlabeled data. Unfortunately, both can severely jeopardize the FL
performance. To address such challenging issues, we introduce two proper
regularization terms that can effectively alleviate the class distribution
mismatch problem in Fed-SSL. In addition, to overcome the non-i.i.d. data, we
leverage the variance reduction and normalized averaging techniques to develop
a novel Fed-SSL algorithm. Theoretically, we prove that the proposed method has
a convergence rate of $\mathcal{O}(1/\sqrt{T})$, where $T$ is the number of
communication rounds, even when the data distribution are non-i.i.d. among
clients. To the best of our knowledge, it is the first formal convergence
result for Fed-SSL problems. Numerical experiments based on MNIST data and
CIFAR-10 data show that the proposed method can greatly improve the
classification accuracy compared to baselines.

    

### [[2111.00034] Neural Networks as Kernel Learners: The Silent Alignment Effect](http://arxiv.org/abs/2111.00034)


  Neural networks in the lazy training regime converge to kernel machines. Can
neural networks in the rich feature learning regime learn a kernel machine with
a data-dependent kernel? We demonstrate that this can indeed happen due to a
phenomenon we term silent alignment, which requires that the tangent kernel of
a network evolves in eigenstructure while small and before the loss appreciably
decreases, and grows only in overall scale afterwards. We show that such an
effect takes place in homogenous neural networks with small initialization and
whitened data. We provide an analytical treatment of this effect in the linear
network case. In general, we find that the kernel develops a low-rank
contribution in the early phase of training, and then evolves in overall scale,
yielding a function equivalent to a kernel regression solution with the final
network's tangent kernel. The early spectral learning of the kernel depends on
both depth and on relative learning rates in each layer. We also demonstrate
that non-whitened data can weaken the silent alignment effect.

    

### [[2111.00035] Skyformer: Remodel Self-Attention with Gaussian Kernel and Nyström Method](http://arxiv.org/abs/2111.00035)


  Transformers are expensive to train due to the quadratic time and space
complexity in the self-attention mechanism. On the other hand, although kernel
machines suffer from the same computation bottleneck in pairwise dot products,
several approximation schemes have been successfully incorporated to
considerably reduce their computational cost without sacrificing too much
accuracy. In this work, we leverage the computation methods for kernel machines
to alleviate the high computational cost and introduce Skyformer, which
replaces the softmax structure with a Gaussian kernel to stabilize the model
training and adapts the Nyström method to a non-positive semidefinite matrix
to accelerate the computation. We further conduct theoretical analysis by
showing that the matrix approximation error of our proposed method is small in
the spectral norm. Experiments on Long Range Arena benchmark show that the
proposed method is sufficient in getting comparable or even better performance
than the full self-attention while requiring fewer computation resources.

    

### [[2111.00036] Real-time detection of anomalies in large-scale transient surveys](http://arxiv.org/abs/2111.00036)


  New time-domain surveys, such as the Rubin Observatory Legacy Survey of Space
and Time (LSST), will observe millions of transient alerts each night, making
standard approaches of visually identifying new and interesting transients
infeasible. We present two novel methods of automatically detecting anomalous
transient light curves in real-time. Both methods are based on the simple idea
that if the light curves from a known population of transients can be
accurately modelled, any deviations from model predictions are likely
anomalies. The first modelling approach is a probabilistic neural network built
using Temporal Convolutional Networks (TCNs) and the second is an interpretable
Bayesian parametric model of a transient. We demonstrate our methods' ability
to provide anomaly scores as a function of time on light curves from the Zwicky
Transient Facility. We show that the flexibility of neural networks, the
attribute that makes them such a powerful tool for many regression tasks, is
what makes them less suitable for anomaly detection when compared with our
parametric model. The parametric model is able to identify anomalies with
respect to common supernova classes with low false anomaly rates and high true
anomaly rates achieving Area Under the Receive Operating Characteristic (ROC)
Curve (AUC) scores above 0.8 for most rare classes such as kilonovae, tidal
disruption events, intermediate luminosity transients, and pair-instability
supernovae. Our ability to identify anomalies improves over the lifetime of the
light curves. Our framework, used in conjunction with transient classifiers,
will enable fast and prioritised follow-up of unusual transients from new
large-scale surveys.

    

### [[2111.00043] Learning generative models for valid knockoffs using novel multivariate-rank based statistics](http://arxiv.org/abs/2111.00043)


  We consider the problem of generating valid knockoffs for knockoff filtering
which is a statistical method that provides provable false discovery rate
guarantees for any model selection procedure. To this end, we are motivated by
recent advances in multivariate distribution-free goodness-of-fit tests namely,
the rank energy (RE), that is derived using theoretical results characterizing
the optimal maps in the Monge's Optimal Transport (OT) problem. However, direct
use of use RE for learning generative models is not feasible because of its
high computational and sample complexity, saturation under large support
discrepancy between distributions, and non-differentiability in generative
parameters. To alleviate these, we begin by proposing a variant of the RE,
dubbed as soft rank energy (sRE), and its kernel variant called as soft rank
maximum mean discrepancy (sRMMD) using entropic regularization of Monge's OT
problem. We then use sRMMD to generate deep knockoffs and show via extensive
evaluation that it is a novel and effective method to produce valid knockoffs,
achieving comparable, or in some cases improved tradeoffs between detection
power Vs false discoveries.

    

### [[2111.00047] Robust and efficient change point detection using novel multivariate rank-energy GoF test](http://arxiv.org/abs/2111.00047)


  In this paper, we use and further develop upon a recently proposed
multivariate, distribution-free Goodness-of-Fit (GoF) test based on the theory
of Optimal Transport (OT) called the Rank Energy (RE) [1], for non-parametric
and unsupervised Change Point Detection (CPD) in multivariate time series data.
We show that directly using RE leads to high sensitivity to very small changes
in distributions (causing high false alarms) and it requires large sample
complexity and huge computational cost. To alleviate these drawbacks, we
propose a new GoF test statistic called as soft-Rank Energy (sRE) that is based
on entropy regularized OT and employ it towards CPD. We discuss the advantages
of using sRE over RE and demonstrate that the proposed sRE based CPD
outperforms all the existing methods in terms of Area Under the Curve (AUC) and
F1-score on real and synthetic data sets.

    

### [[2111.00048] On the Power of Edge Independent Graph Models](http://arxiv.org/abs/2111.00048)


  Why do many modern neural-network-based graph generative models fail to
reproduce typical real-world network characteristics, such as high triangle
density? In this work we study the limitations of edge independent random graph
models, in which each edge is added to the graph independently with some
probability. Such models include both the classic Erdös-Rényi and
stochastic block models, as well as modern generative models such as NetGAN,
variational graph autoencoders, and CELL. We prove that subject to a bounded
overlap condition, which ensures that the model does not simply memorize a
single graph, edge independent models are inherently limited in their ability
to generate graphs with high triangle and other subgraph densities. Notably,
such high densities are known to appear in real-world social networks and other
graphs. We complement our negative results with a simple generative model that
balances overlap and accuracy, performing comparably to more complex models in
reconstructing many graph statistics.

    

### [[2111.00053] Symbolic Regression via Neural-Guided Genetic Programming Population Seeding](http://arxiv.org/abs/2111.00053)


  Symbolic regression is the process of identifying mathematical expressions
that fit observed output from a black-box process. It is a discrete
optimization problem generally believed to be NP-hard. Prior approaches to
solving the problem include neural-guided search (e.g. using reinforcement
learning) and genetic programming. In this work, we introduce a hybrid
neural-guided/genetic programming approach to symbolic regression and other
combinatorial optimization problems. We propose a neural-guided component used
to seed the starting population of a random restart genetic programming
component, gradually learning better starting populations. On a number of
common benchmark tasks to recover underlying expressions from a dataset, our
method recovers 65% more expressions than a recently published top-performing
model using the same experimental setup. We demonstrate that running many
genetic programming generations without interdependence on the neural-guided
component performs better for symbolic regression than alternative formulations
where the two are more strongly coupled. Finally, we introduce a new set of 22
symbolic regression benchmark problems with increased difficulty over existing
benchmarks. Source code is provided at
this http URL.

    

### [[2111.00056] Generalized Data Weighting via Class-level Gradient Manipulation](http://arxiv.org/abs/2111.00056)


  Label noise and class imbalance are two major issues coexisting in real-world
datasets. To alleviate the two issues, state-of-the-art methods reweight each
instance by leveraging a small amount of clean and unbiased data. Yet, these
methods overlook class-level information within each instance, which can be
further utilized to improve performance. To this end, in this paper, we propose
Generalized Data Weighting (GDW) to simultaneously mitigate label noise and
class imbalance by manipulating gradients at the class level. To be specific,
GDW unrolls the loss gradient to class-level gradients by the chain rule and
reweights the flow of each gradient separately. In this way, GDW achieves
remarkable performance improvement on both issues. Aside from the performance
gain, GDW efficiently obtains class-level weights without introducing any extra
computational cost compared with instance weighting methods. Specifically, GDW
performs a gradient descent step on class-level weights, which only relies on
intermediate gradients. Extensive experiments in various settings verify the
effectiveness of GDW. For example, GDW outperforms state-of-the-art methods by
$2.56\%$ under the $60\%$ uniform noise setting in CIFAR10. Our code is
available at this https URL.

    

### [[2111.00057] Word embeddings for topic modeling: an application to the estimation of the economic policy uncertainty index](http://arxiv.org/abs/2111.00057)


  Quantification of economic uncertainty is a key concept for the prediction of
macro economic variables such as gross domestic product (GDP), and it becomes
particularly relevant on real-time or short-time predictions methodologies,
such as nowcasting, where it is required a large amount of time series data,
commonly with different structures and frequencies. Most of the data comes from
the official agencies statistics and non-public institutions, however, relying
our estimates in just the traditional data mentioned before, have some
disadvantages. One of them is that economic uncertainty could not be
represented or measured in a proper way based solely in financial or
macroeconomic data, another one, is that they are susceptible to lack of
information due to extraordinary events, such as the current COVID-19 pandemic.
For these reasons, it is very common nowadays to use some non-traditional data
from different sources, such as social networks or digital newspapers, in
addition to the traditional data from official sources. The economic policy
uncertainty (EPU) index, is the most used newspaper-based indicator to quantify
the uncertainty, and is based on topic modeling of newspapers. In this paper,
we propose a methodology to estimate the EPU index, which incorporates a fast
and efficient method for topic modeling of digital news based on semantic
clustering with word embeddings, allowing to update the index in real-time,
which is a drawback with another proposals that use computationally intensive
methods for topic modeling, such as Latent Dirichlet Allocation (LDA). We show
that our proposal allow us to update the index and significantly reduces the
time required for new document assignation into topics.

    

### [[2111.00062] Improving Generalization Bounds for VC Classes Using the Hypergeometric Tail Inversion](http://arxiv.org/abs/2111.00062)


  We significantly improve the generalization bounds for VC classes by using
two main ideas. First, we consider the hypergeometric tail inversion to obtain
a very tight non-uniform distribution-independent risk upper bound for VC
classes. Second, we optimize the ghost sample trick to obtain a further
non-negligible gain. These improvements are then used to derive a relative
deviation bound, a multiclass margin bound, as well as a lower bound. Numerical
comparisons show that the new bound is nearly never vacuous, and is tighter
than other VC bounds for all reasonable data set sizes.

    

### [[2111.00064] Node Feature Extraction by Self-Supervised Multi-scale Neighborhood Prediction](http://arxiv.org/abs/2111.00064)


  Learning on graphs has attracted significant attention in the learning
community due to numerous real-world applications. In particular, graph neural
networks (GNNs), which take numerical node features and graph structure as
inputs, have been shown to achieve state-of-the-art performance on various
graph-related learning tasks. Recent works exploring the correlation between
numerical node features and graph structure via self-supervised learning have
paved the way for further performance improvements of GNNs. However, methods
used for extracting numerical node features from raw data are still
graph-agnostic within standard GNN pipelines. This practice is sub-optimal as
it prevents one from fully utilizing potential correlations between graph
topology and node attributes. To mitigate this issue, we propose a new
self-supervised learning framework, Graph Information Aided Node feature
exTraction (GIANT). GIANT makes use of the eXtreme Multi-label Classification
(XMC) formalism, which is crucial for fine-tuning the language model based on
graph information, and scales to large datasets. We also provide a theoretical
analysis that justifies the use of XMC over link prediction and motivates
integrating XR-Transformers, a powerful method for solving XMC problems, into
the GIANT framework. We demonstrate the superior performance of GIANT over the
standard GNN pipeline on Open Graph Benchmark datasets: For example, we improve
the accuracy of the top-ranked method GAMLP from $68.25\%$ to $69.67\%$, SGC
from $63.29\%$ to $66.10\%$ and MLP from $47.24\%$ to $61.10\%$ on the
ogbn-papers100M dataset by leveraging GIANT.

    

### [[2111.00070] Deep inference of latent dynamics with spatio-temporal super-resolution using selective backpropagation through time](http://arxiv.org/abs/2111.00070)


  Modern neural interfaces allow access to the activity of up to a million
neurons within brain circuits. However, bandwidth limits often create a
trade-off between greater spatial sampling (more channels or pixels) and the
temporal frequency of sampling. Here we demonstrate that it is possible to
obtain spatio-temporal super-resolution in neuronal time series by exploiting
relationships among neurons, embedded in latent low-dimensional population
dynamics. Our novel neural network training strategy, selective backpropagation
through time (SBTT), enables learning of deep generative models of latent
dynamics from data in which the set of observed variables changes at each time
step. The resulting models are able to infer activity for missing samples by
combining observations with learned latent dynamics. We test SBTT applied to
sequential autoencoders and demonstrate more efficient and higher-fidelity
characterization of neural population dynamics in electrophysiological and
calcium imaging data. In electrophysiology, SBTT enables accurate inference of
neuronal population dynamics with lower interface bandwidths, providing an
avenue to significant power savings for implanted neuroelectronic interfaces.
In applications to two-photon calcium imaging, SBTT accurately uncovers
high-frequency temporal structure underlying neural population activity,
substantially outperforming the current state-of-the-art. Finally, we
demonstrate that performance could be further improved by using limited,
high-bandwidth sampling to pretrain dynamics models, and then using SBTT to
adapt these models for sparsely-sampled data.

    

### [[2111.00072] Generalized Proximal Policy Optimization with Sample Reuse](http://arxiv.org/abs/2111.00072)


  In real-world decision making tasks, it is critical for data-driven
reinforcement learning methods to be both stable and sample efficient.
On-policy methods typically generate reliable policy improvement throughout
training, while off-policy methods make more efficient use of data through
sample reuse. In this work, we combine the theoretically supported stability
benefits of on-policy algorithms with the sample efficiency of off-policy
algorithms. We develop policy improvement guarantees that are suitable for the
off-policy setting, and connect these bounds to the clipping mechanism used in
Proximal Policy Optimization. This motivates an off-policy version of the
popular algorithm that we call Generalized Proximal Policy Optimization with
Sample Reuse. We demonstrate both theoretically and empirically that our
algorithm delivers improved performance by effectively balancing the competing
goals of stability and sample efficiency.

    

### [[2111.00077] DeepDoseNet: A Deep Learning model for 3D Dose Prediction in Radiation Therapy](http://arxiv.org/abs/2111.00077)


  The DeepDoseNet 3D dose prediction model based on ResNet and Dilated DenseNet
is proposed. The 340 head-and-neck datasets from the 2020 AAPM OpenKBP
challenge were utilized, with 200 for training, 40 for validation, and 100 for
testing. Structures include 56Gy, 63Gy, 70Gy PTVs, and brainstem, spinal cord,
right parotid, left parotid, larynx, esophagus, and mandible OARs. Mean squared
error (MSE) loss, mean absolute error (MAE) loss, and MAE plus dose-volume
histogram (DVH) based loss functions were investigated. Each model's
performance was compared using a 3D dose score, $\bar{S_{D}}$, (mean absolute
difference between ground truth and predicted 3D dose distributions) and a DVH
score, $\bar{S_{DVH}}$ (mean absolute difference between ground truth and
predicted dose-volume metrics).Furthermore, DVH metrics Mean[Gy] and D0.1cc
[Gy] for OARs and D99%, D95%, D1% for PTVs were computed. DeepDoseNet with the
MAE plus DVH-based loss function had the best dose score performance of the
OpenKBP entries. MAE+DVH model had the lowest prediction error (P<0.0001,
Wilcoxon test) on validation and test datasets (validation:
$\bar{S_{D}}$=2.3Gy, $\bar{S_{DVH}}$=1.9Gy; test: $\bar{S_{D}}$=2.0Gy,
$\bar{S_{DVH}}$=1.6Gy) followed by the MAE model (validation:
$\bar{S_{D}}$=3.6Gy, $\bar{S_{DVH}}$=2.4Gy; test: $\bar{S_{D}}$=3.5Gy,
$\bar{S_{DVH}}$=2.3Gy). The MSE model had the highest prediction error
(validation: $\bar{S_{D}}$=3.7Gy, $\bar{S_{DVH}}$=3.2Gy; test:
$\bar{S_{D}}$=3.6Gy, $\bar{S_{DVH}}$=3.0Gy). No significant difference was
found among models in terms of Mean [Gy], but the MAE+DVH model significantly
outperformed the MAE and MSE models in terms of D0.1cc[Gy], particularly for
mandible and parotids on both validation (P<0.01) and test (P<0.0001) datasets.
MAE+DVH outperformed (P<0.0001) in terms of D99%, D95%, D1% for targets.
MAE+DVH reduced $\bar{S_{D}}$ by ~60% and $\bar{S_{DVH}}$ by ~70%.

    

### [[2111.00079] Deep Deterministic Uncertainty for Semantic Segmentation](http://arxiv.org/abs/2111.00079)


  We extend Deep Deterministic Uncertainty (DDU), a method for uncertainty
estimation using feature space densities, to semantic segmentation. DDU enables
quantifying and disentangling epistemic and aleatoric uncertainty in a single
forward pass through the model. We study the similarity of feature
representations of pixels at different locations for the same class and
conclude that it is feasible to apply DDU location independently, which leads
to a significant reduction in memory consumption compared to pixel dependent
DDU. Using the DeepLab-v3+ architecture on Pascal VOC 2012, we show that DDU
improves upon MC Dropout and Deep Ensembles while being significantly faster to
compute.

    

### [[2111.00083] A Scalable AutoML Approach Based on Graph Neural Networks](http://arxiv.org/abs/2111.00083)


  AutoML systems build machine learning models automatically by performing a
search over valid data transformations and learners, along with hyper-parameter
optimization for each learner. We present a system called KGpip for the
selection of transformations and learners, which (1) builds a database of
datasets and corresponding historically used pipelines using effective static
analysis instead of the typical use of actual runtime information, (2) uses
dataset embeddings to find similar datasets in the database based on its
content instead of metadata-based features, (3) models AutoML pipeline creation
as a graph generation problem, to succinctly characterize the diverse pipelines
seen for a single dataset. KGpip is designed as a sub-component for AutoML
systems. We demonstrate this ability via integrating KGpip with two AutoML
systems and show that it does significantly enhance the performance of existing
state-of-the-art systems.

    

### [[2111.00092] Optimal Compression of Locally Differentially Private Mechanisms](http://arxiv.org/abs/2111.00092)


  Compressing the output of \epsilon-locally differentially private (LDP)
randomizers naively leads to suboptimal utility. In this work, we demonstrate
the benefits of using schemes that jointly compress and privatize the data
using shared randomness. In particular, we investigate a family of schemes
based on Minimal Random Coding (Havasi et al., 2019) and prove that they offer
optimal privacy-accuracy-communication tradeoffs. Our theoretical and empirical
findings show that our approach can compress PrivUnit (Bhowmick et al., 2018)
and Subset Selection (Ye et al., 2018), the best known LDP algorithms for mean
and frequency estimation, to to the order of \epsilon-bits of communication
while preserving their privacy and accuracy guarantees.

    

### [[2111.00095] Online Optimization with Feedback Delay and Nonlinear Switching Cost](http://arxiv.org/abs/2111.00095)


  We study a variant of online optimization in which the learner receives
$k$-round $\textit{delayed feedback}$ about hitting cost and there is a
multi-step nonlinear switching cost, i.e., costs depend on multiple previous
actions in a nonlinear manner. Our main result shows that a novel Iterative
Regularized Online Balanced Descent (iROBD) algorithm has a constant,
dimension-free competitive ratio that is $O(L^{2k})$, where $L$ is the
Lipschitz constant of the switching cost. Additionally, we provide lower bounds
that illustrate the Lipschitz condition is required and the dependencies on $k$
and $L$ are tight. Finally, via reductions, we show that this setting is
closely related to online control problems with delay, nonlinear dynamics, and
adversarial disturbances, where iROBD directly offers constant-competitive
online policies.

    

### [[2111.00097] Evaluation of an Anomaly Detector for Routers using Parameterizable Malware in an IoT Ecosystem](http://arxiv.org/abs/2111.00097)


  This work explores the evaluation of a machine learning anomaly detector
using custom-made parameterizable malware in an Internet of Things (IoT)
Ecosystem. It is assumed that the malware has infected, and resides on, the
Linux router that serves other devices on the network, as depicted in Figure 1.
This IoT Ecosystem was developed as a testbed to evaluate the efficacy of a
behavior-based anomaly detector. The malware consists of three types of
custom-made malware: ransomware, cryptominer, and keylogger, which all have
exfiltration capabilities to the network. The parameterization of the malware
gives the malware samples multiple degrees of freedom, specifically relating to
the rate and size of data exfiltration. The anomaly detector uses feature sets
crafted from system calls and network traffic, and uses a Support Vector
Machine (SVM) for behavioral-based anomaly detection. The custom-made malware
is used to evaluate the situations where the SVM is effective, as well as the
situations where it is not effective.

    

### [[2111.00108] High-dimensional multi-trait GWAS by reverse prediction of genotypes](http://arxiv.org/abs/2111.00108)


  Multi-trait genome-wide association studies (GWAS) use multi-variate
statistical methods to identify associations between genetic variants and
multiple correlated traits simultaneously, and have higher statistical power
than independent univariate analysis of traits. Reverse regression, where
genotypes of genetic variants are regressed on multiple traits simultaneously,
has emerged as a promising approach to perform multi-trait GWAS in
high-dimensional settings where the number of traits exceeds the number of
samples. We extended this approach and analyzed different machine learning
methods (ridge regression, random forests and support vector machines)for
reverse regression in multi-trait GWAS, using genotypes, gene expression data
and ground-truth transcriptional regulatory networks from the DREAM5 SysGen
Challenge and from a cross between two yeast strains to evaluate methods. We
found that genotype prediction performance, in terms of root mean squared error
(RMSE), allowed to distinguish between genomic regions with high and low
transcriptional activity. Moreover, model feature coefficients correlated with
the strength of association between variants and individual traits, and were
predictive of true trans-eQTL target genes, with complementary findings across
methods.

    

### [[2111.00110] FC2T2: The Fast Continuous Convolutional Taylor Transform with Applications in Vision and Graphics](http://arxiv.org/abs/2111.00110)


  Series expansions have been a cornerstone of applied mathematics and
engineering for centuries. In this paper, we revisit the Taylor series
expansion from a modern Machine Learning perspective. Specifically, we
introduce the Fast Continuous Convolutional Taylor Transform (FC2T2), a variant
of the Fast Multipole Method (FMM), that allows for the efficient approximation
of low dimensional convolutional operators in continuous space. We build upon
the FMM which is an approximate algorithm that reduces the computational
complexity of N-body problems from O(NM) to O(N+M) and finds application in
e.g. particle simulations. As an intermediary step, the FMM produces a series
expansion for every cell on a grid and we introduce algorithms that act
directly upon this representation. These algorithms analytically but
approximately compute the quantities required for the forward and backward pass
of the backpropagation algorithm and can therefore be employed as (implicit)
layers in Neural Networks. Specifically, we introduce a root-implicit layer
that outputs surface normals and object distances as well as an
integral-implicit layer that outputs a rendering of a radiance field given a 3D
pose. In the context of Machine Learning, $N$ and $M$ can be understood as the
number of model parameters and model evaluations respectively which entails
that, for applications that require repeated function evaluations which are
prevalent in Computer Vision and Graphics, unlike regular Neural Networks, the
techniques introduce in this paper scale gracefully with parameters. For some
applications, this results in a 200x reduction in FLOPs compared to
state-of-the-art approaches at a reasonable or non-existent loss in accuracy.

    

### [[2111.00115] Combining Public and Private Data](http://arxiv.org/abs/2111.00115)


  Differential privacy is widely adopted to provide provable privacy guarantees
in data analysis. We consider the problem of combining public and private data
(and, more generally, data with heterogeneous privacy needs) for estimating
aggregate statistics. We introduce a mixed estimator of the mean optimized to
minimize the variance. We argue that our mechanism is preferable to techniques
that preserve the privacy of individuals by subsampling data proportionally to
the privacy needs of users. Similarly, we present a mixed median estimator
based on the exponential mechanism. We compare our mechanisms to the methods
proposed in Jorgensen et al. [2015]. Our experiments provide empirical evidence
that our mechanisms often outperform the baseline methods.

    

### [[2111.00116] Visual Explanations for Convolutional Neural Networks via Latent Traversal](http://arxiv.org/abs/2111.00116)


  Lack of explainability in artificial intelligence, specifically deep neural
networks, remains a bottleneck for implementing models in practice. Popular
techniques such as Gradient-weighted Class Activation Mapping (Grad-CAM)
provide a coarse map of salient features in an image, which rarely tells the
whole story of what a convolutional neural network (CNN) learned. Using
COVID-19 chest X-rays, we present a method for interpreting what a CNN has
learned by utilizing Generative Adversarial Networks (GANs). Our GAN framework
disentangles lung structure from COVID-19 features. Using this GAN, we can
visualize the transition of a pair of COVID negative lungs in a chest
radiograph to a COVID positive pair by interpolating in the latent space of the
GAN, which provides fine-grained visualization of how the CNN responds to
varying features within the lungs.

    

### [[2111.00124] Predicting Atlantic Multidecadal Variability](http://arxiv.org/abs/2111.00124)


  Atlantic Multidecadal Variability (AMV) describes variations of North
Atlantic sea surface temperature with a typical cycle of between 60 and 70
years. AMV strongly impacts local climate over North America and Europe,
therefore prediction of AMV, especially the extreme values, is of great
societal utility for understanding and responding to regional climate change.
This work tests multiple machine learning models to improve the state of AMV
prediction from maps of sea surface temperature, salinity, and sea level
pressure in the North Atlantic region. We use data from the Community Earth
System Model 1 Large Ensemble Project, a state-of-the-art climate model with
3,440 years of data. Our results demonstrate that all of the models we use
outperform the traditional persistence forecast baseline. Predicting the AMV is
important for identifying future extreme temperatures and precipitation, as
well as hurricane activity, in Europe and North America up to 25 years in
advance.

    

### [[2111.00126] Predicting Critical Biogeochemistry of the Southern Ocean for Climate Monitoring](http://arxiv.org/abs/2111.00126)


  The Biogeochemical-Argo (BGC-Argo) program is building a network of globally
distributed, sensor-equipped robotic profiling floats, improving our
understanding of the climate system and how it is changing. These floats,
however, are limited in the number of variables measured. In this study, we
train neural networks to predict silicate and phosphate values in the Southern
Ocean from temperature, pressure, salinity, oxygen, nitrate, and location and
apply these models to earth system model (ESM) and BGC-Argo data to expand the
utility of this ocean observation network. We trained our neural networks on
observations from the Global Ocean Ship-Based Hydrographic Investigations
Program (GO-SHIP) and use dropout regularization to provide uncertainty bounds
around our predicted values. Our neural network significantly improves upon
linear regression but shows variable levels of uncertainty across the ranges of
predicted variables. We explore the generalization of our estimators to test
data outside our training distribution from both ESM and BGC-Argo data. Our use
of out-of-distribution test data to examine shifts in biogeochemical parameters
and calculate uncertainty bounds around estimates advance the state-of-the-art
in oceanographic data and climate monitoring. We make our data and code
publicly available.

    

### [[2111.00131] Three approaches to facilitate DNN generalization to objects in out-of-distribution orientations and illuminations: late-stopping, tuning batch normalization and invariance loss](http://arxiv.org/abs/2111.00131)


  The training data distribution is often biased towards objects in certain
orientations and illumination conditions. While humans have a remarkable
capability of recognizing objects in out-of-distribution (OoD) orientations and
illuminations, Deep Neural Networks (DNNs) severely suffer in this case, even
when large amounts of training examples are available. In this paper, we
investigate three different approaches to improve DNNs in recognizing objects
in OoD orientations and illuminations. Namely, these are (i) training much
longer after convergence of the in-distribution (InD) validation accuracy,
i.e., late-stopping, (ii) tuning the momentum parameter of the batch
normalization layers, and (iii) enforcing invariance of the neural activity in
an intermediate layer to orientation and illumination conditions. Each of these
approaches substantially improves the DNN's OoD accuracy (more than 20% in some
cases). We report results in four datasets: two datasets are modified from the
MNIST and iLab datasets, and the other two are novel (one of 3D rendered cars
and another of objects taken from various controlled orientations and
illumination conditions). These datasets allow to study the effects of
different amounts of bias and are challenging as DNNs perform poorly in OoD
conditions. Finally, we demonstrate that even though the three approaches focus
on different aspects of DNNs, they all tend to lead to the same underlying
neural mechanism to enable OoD accuracy gains -- individual neurons in the
intermediate layers become more selective to a category and also invariant to
OoD orientations and illuminations.

    

### [[2111.00134] Context Meta-Reinforcement Learning via Neuromodulation](http://arxiv.org/abs/2111.00134)


  Meta-reinforcement learning (meta-RL) algorithms enable agents to adapt
quickly to tasks from few samples in dynamic environments. Such a feat is
achieved through dynamic representations in an agent's policy network (obtained
via reasoning about task context, model parameter updates, or both). However,
obtaining rich dynamic representations for fast adaptation beyond simple
benchmark problems is challenging due to the burden placed on the policy
network to accommodate different policies. This paper addresses the challenge
by introducing neuromodulation as a modular component to augment a standard
policy network that regulates neuronal activities in order to produce efficient
dynamic representations for task adaptation. The proposed extension to the
policy network is evaluated across multiple discrete and continuous control
environments of increasing complexity. To prove the generality and benefits of
the extension in meta-RL, the neuromodulated network was applied to two
state-of-the-art meta-RL algorithms (CAVIA and PEARL). The result demonstrates
that meta-RL augmented with neuromodulation produces significantly better
result and richer dynamic representations in comparison to the baselines.

    

### [[2111.00137] Efficient Inference Without Trading-off Regret in Bandits: An Allocation Probability Test for Thompson Sampling](http://arxiv.org/abs/2111.00137)


  Using bandit algorithms to conduct adaptive randomised experiments can
minimise regret, but it poses major challenges for statistical inference (e.g.,
biased estimators, inflated type-I error and reduced power). Recent attempts to
address these challenges typically impose restrictions on the exploitative
nature of the bandit algorithm$-$trading off regret$-$and require large sample
sizes to ensure asymptotic guarantees. However, large experiments generally
follow a successful pilot study, which is tightly constrained in its size or
duration. Increasing power in such small pilot experiments, without limiting
the adaptive nature of the algorithm, can allow promising interventions to
reach a larger experimental phase. In this work we introduce a novel hypothesis
test, uniquely based on the allocation probabilities of the bandit algorithm,
and without constraining its exploitative nature or requiring a minimum
experimental size. We characterise our $Allocation\ Probability\ Test$ when
applied to $Thompson\ Sampling$, presenting its asymptotic theoretical
properties, and illustrating its finite-sample performances compared to
state-of-the-art approaches. We demonstrate the regret and inferential
advantages of our approach, particularly in small samples, in both extensive
simulations and in a real-world experiment on mental health aspects.

    

### [[2111.00142] Uncovering IP Address Hosting Types Behind Malicious Websites](http://arxiv.org/abs/2111.00142)


  Hundreds of thousands of malicious domains are created everyday. These
malicious domains are hosted on a wide variety of network infrastructures.
Traditionally, attackers utilize bullet proof hosting services (e.g. MaxiDed,
Cyber Bunker) to take advantage of relatively lenient policies on what content
they can host. However, these IP ranges are increasingly being blocked or the
services are taken down by law enforcement. Hence, attackers are moving towards
utilizing IPs from regular hosting providers while staying under the radar of
these hosting providers. There are several practical advantages of accurately
knowing the type of IP used to host malicious domains. If the IP is a dedicated
IP (i.e. it is leased to a single entity), one may blacklist the IP to block
domains hosted on those IPs as welll as use as a way to identify other
malicious domains hosted the same IP. If the IP is a shared hosting IP, hosting
providers may take measures to clean up such domains and maintain a high
reputation for their users.

    

### [[2111.00149] Temporal-Spatial Feature Extraction Based on Convolutional Neural Networks for Travel Time Prediction](http://arxiv.org/abs/2111.00149)


  In recent years, some traffic information prediction methods have been
proposed to provide the precise information of travel time, vehicle speed, and
traffic flow for highways. However, big errors may be obtained by these methods
for urban roads or the alternative roads of highways. Therefore, this study
proposes a travel time prediction method based on convolutional neural networks
to extract important factors for the improvement of traffic information
prediction. In practical experimental environments, the travel time records of
No. 5 Highway and the alternative roads of its were collected and used to
evaluate the proposed method. The results showed that the mean absolute
percentage error of the proposed method was about 5.69%. Therefore, the
proposed method based on deep learning techniques can improve the accuracy of
travel time prediction.

    

### [[2111.00153] RMSMP: A Novel Deep Neural Network Quantization Framework with Row-wise Mixed Schemes and Multiple Precisions](http://arxiv.org/abs/2111.00153)


  This work proposes a novel Deep Neural Network (DNN) quantization framework,
namely RMSMP, with a Row-wise Mixed-Scheme and Multi-Precision approach.
Specifically, this is the first effort to assign mixed quantization schemes and
multiple precisions within layers -- among rows of the DNN weight matrix, for
simplified operations in hardware inference, while preserving accuracy.
Furthermore, this paper makes a different observation from the prior work that
the quantization error does not necessarily exhibit the layer-wise sensitivity,
and actually can be mitigated as long as a certain portion of the weights in
every layer are in higher precisions. This observation enables layer-wise
uniformality in the hardware implementation towards guaranteed inference
acceleration, while still enjoying row-wise flexibility of mixed schemes and
multiple precisions to boost accuracy. The candidates of schemes and precisions
are derived practically and effectively with a highly hardware-informative
strategy to reduce the problem search space. With the offline determined ratio
of different quantization schemes and precisions for all the layers, the RMSMP
quantization algorithm uses the Hessian and variance-based method to
effectively assign schemes and precisions for each row. The proposed RMSMP is
tested for the image classification and natural language processing (BERT)
applications and achieves the best accuracy performance among state-of-the-arts
under the same equivalent precisions. The RMSMP is implemented on FPGA devices,
achieving 3.65x speedup in the end-to-end inference time for ResNet-18 on
ImageNet, compared with the 4-bit Fixed-point baseline.

    

### [[2111.00155] ILMPQ : An Intra-Layer Multi-Precision Deep Neural Network Quantization framework for FPGA](http://arxiv.org/abs/2111.00155)


  This work targets the commonly used FPGA (field-programmable gate array)
devices as the hardware platform for DNN edge computing. We focus on DNN
quantization as the main model compression technique. The novelty of this work
is: We use a quantization method that supports multiple precisions along the
intra-layer dimension, while the existing quantization methods apply
multi-precision quantization along the inter-layer dimension. The intra-layer
multi-precision method can uniform the hardware configurations for different
layers to reduce computation overhead and at the same time preserve the model
accuracy as the inter-layer approach. Our proposed ILMPQ DNN quantization
framework achieves 70.73 Top1 accuracy in ResNet-18 on the ImageNet dataset. We
also validate the proposed MSP framework on two FPGA devices i.e., Xilinx
XC7Z020 and XC7Z045. We achieve 3.65x speedup in end-to-end inference time on
the ImageNet, compared with the fixed-point quantization method.

    

### [[2111.00160] DSEE: Dually Sparsity-embedded Efficient Tuning of Pre-trained Language Models](http://arxiv.org/abs/2111.00160)


  Gigantic pre-trained models have become central to natural language
processing (NLP), serving as the starting point for fine-tuning towards a range
of downstream tasks. However, two pain points persist for this paradigm: (a) as
the pre-trained models grow bigger (e.g., 175B parameters for GPT-3), even the
fine-tuning process can be time-consuming and computationally expensive; (b)
the fine-tuned model has the same size as its starting point by default, which
is neither sensible due to its more specialized functionality, nor practical
since many fine-tuned models will be deployed in resource-constrained
environments. To address these pain points, we propose a framework for
resource- and parameter-efficient fine-tuning by leveraging the sparsity prior
in both weight updates and the final model weights. Our proposed framework,
dubbed Dually Sparsity-Embedded Efficient Tuning (DSEE), aims to achieve two
key objectives: (i) parameter efficient fine-tuning - by enforcing
sparsity-aware weight updates on top of the pre-trained weights; and (ii)
resource-efficient inference - by encouraging a sparse weight structure towards
the final fine-tuned model. We leverage sparsity in these two directions by
exploiting both unstructured and structured sparse patterns in pre-trained
language models via magnitude-based pruning and $\ell_1$ sparse regularization.
Extensive experiments and in-depth investigations, with diverse network
backbones (i.e., BERT, GPT-2, and DeBERTa) on dozens of datasets, consistently
demonstrate highly impressive parameter-/training-/inference-efficiency, while
maintaining competitive downstream transfer performance. For instance, our
DSEE-BERT obtains about $35\%$ inference FLOPs savings with <1% trainable
parameters and comparable performance to conventional fine-tuning. Codes are
available in this https URL.

    

### [[2111.00162] You are caught stealing my winning lottery ticket! Making a lottery ticket claim its ownership](http://arxiv.org/abs/2111.00162)


  Despite tremendous success in many application scenarios, the training and
inference costs of using deep learning are also rapidly increasing over time.
The lottery ticket hypothesis (LTH) emerges as a promising framework to
leverage a special sparse subnetwork (i.e., winning ticket) instead of a full
model for both training and inference, that can lower both costs without
sacrificing the performance. The main resource bottleneck of LTH is however the
extraordinary cost to find the sparse mask of the winning ticket. That makes
the found winning ticket become a valuable asset to the owners, highlighting
the necessity of protecting its copyright. Our setting adds a new dimension to
the recently soaring interest in protecting against the intellectual property
(IP) infringement of deep models and verifying their ownerships, since they
take owners' massive/unique resources to develop or train. While existing
methods explored encrypted weights or predictions, we investigate a unique way
to leverage sparse topological information to perform lottery verification, by
developing several graph-based signatures that can be embedded as credentials.
By further combining trigger set-based methods, our proposal can work in both
white-box and black-box verification scenarios. Through extensive experiments,
we demonstrate the effectiveness of lottery verification in diverse models
(ResNet-20, ResNet-18, ResNet-50) on CIFAR-10 and CIFAR-100. Specifically, our
verification is shown to be robust to removal attacks such as model fine-tuning
and pruning, as well as several ambiguity attacks. Our codes are available at
this https URL.

    

### [[2111.00173] Dynamic Differential-Privacy Preserving SGD](http://arxiv.org/abs/2111.00173)


  Differentially-Private Stochastic Gradient Descent (DP-SGD) prevents
training-data privacy breaches by adding noise to the clipped gradient during
SGD training to satisfy the differential privacy (DP) definition. On the other
hand, the same clipping operation and additive noise across training steps
results in unstable updates and even a ramp-up period, which significantly
reduces the model's accuracy. In this paper, we extend the Gaussian DP central
limit theorem to calibrate the clipping value and the noise power for each
individual step separately. We, therefore, are able to propose the dynamic
DP-SGD, which has a lower privacy cost than the DP-SGD during updates until
they achieve the same target privacy budget at a target number of updates.
Dynamic DP-SGD, in particular, improves model accuracy without sacrificing
privacy by gradually lowering both clipping value and noise power while
adhering to a total privacy budget constraint. Extensive experiments on a
variety of deep learning tasks, including image classification, natural
language processing, and federated learning, show that the proposed dynamic
DP-SGD algorithm stabilizes updates and, as a result, significantly improves
model accuracy in the strong privacy protection region when compared to DP-SGD.

    

### [[2111.00177] On Quantitative Evaluations of Counterfactuals](http://arxiv.org/abs/2111.00177)


  As counterfactual examples become increasingly popular for explaining
decisions of deep learning models, it is essential to understand what
properties quantitative evaluation metrics do capture and equally important
what they do not capture. Currently, such understanding is lacking, potentially
slowing down scientific progress. In this paper, we consolidate the work on
evaluating visual counterfactual examples through an analysis and experiments.
We find that while most metrics behave as intended for sufficiently simple
datasets, some fail to tell the difference between good and bad counterfactuals
when the complexity increases. We observe experimentally that metrics give good
scores to tiny adversarial-like changes, wrongly identifying such changes as
superior counterfactual examples. To mitigate this issue, we propose two new
metrics, the Label Variation Score and the Oracle score, which are both less
vulnerable to such tiny changes. We conclude that a proper quantitative
evaluation of visual counterfactual examples should combine metrics to ensure
that all aspects of good counterfactuals are quantified.

    

### [[2111.00185] Convergence and Optimality of Policy Gradient Methods in Weakly Smooth Settings](http://arxiv.org/abs/2111.00185)


  Policy gradient methods have been frequently applied to problems in control
and reinforcement learning with great success, yet existing convergence
analysis still relies on non-intuitive, impractical and often opaque
conditions. In particular, existing rates are achieved in limited settings,
under strict smoothness and bounded conditions. In this work, we establish
explicit convergence rates of policy gradient methods without relying on these
conditions, instead extending the convergence regime to weakly smooth policy
classes with $L_2$ integrable gradient. We provide intuitive examples to
illustrate the insight behind these new conditions. We also characterize the
sufficiency conditions for the ergodicity of near-linear MDPs, which represent
an important class of problems. Notably, our analysis also shows that fast
convergence rates are achievable for both the standard policy gradient and the
natural policy gradient algorithms under these assumptions. Lastly we provide
conditions and analysis for optimality of the converged policies.

    

### [[2111.00195] Learning Continuous Representation of Audio for Arbitrary Scale Super Resolution](http://arxiv.org/abs/2111.00195)


  Audio super resolution aims to predict the missing high resolution components
of the low resolution audio signals. While audio in nature is continuous
signal, current approaches treat it as discrete data (i.e., input is defined on
discrete time domain), and consider the super resolution over fixed scale
factor (i.e., it is required to train a new neural network to change output
resolution). To obtain a continuous representation of audio and enable super
resolution for arbitrary scale factor, we propose a method of neural implicit
representation, coined Local Implicit representation for Super resolution of
Arbitrary scale (LISA). Our method locally parameterizes a chunk of audio as a
function of continuous time, and represents each chunk with the local latent
codes of neighboring chunks so that the function can extrapolate the signal at
any time coordinate, i.e., infinite resolution. To learn a continuous
representation for audio, we design a self-supervised learning strategy to
practice super resolution tasks up to the original resolution by stochastic
selection. Our numerical evaluation shows that LISA outperforms the previous
fixed-scale methods with a fraction of parameters, but also is capable of
arbitrary scale super resolution even beyond the resolution of training data.

    

### [[2111.00197] Backdoor Pre-trained Models Can Transfer to All](http://arxiv.org/abs/2111.00197)


  Pre-trained general-purpose language models have been a dominating component
in enabling real-world natural language processing (NLP) applications. However,
a pre-trained model with backdoor can be a severe threat to the applications.
Most existing backdoor attacks in NLP are conducted in the fine-tuning phase by
introducing malicious triggers in the targeted class, thus relying greatly on
the prior knowledge of the fine-tuning task. In this paper, we propose a new
approach to map the inputs containing triggers directly to a predefined output
representation of the pre-trained NLP models, e.g., a predefined output
representation for the classification token in BERT, instead of a target label.
It can thus introduce backdoor to a wide range of downstream tasks without any
prior knowledge. Additionally, in light of the unique properties of triggers in
NLP, we propose two new metrics to measure the performance of backdoor attacks
in terms of both effectiveness and stealthiness. Our experiments with various
types of triggers show that our method is widely applicable to different
fine-tuning tasks (classification and named entity recognition) and to
different models (such as BERT, XLNet, BART), which poses a severe threat.
Furthermore, by collaborating with the popular online model repository Hugging
Face, the threat brought by our method has been confirmed. Finally, we analyze
the factors that may affect the attack performance and share insights on the
causes of the success of our backdoor attack.

    

### [[2111.00199] Personal thermal comfort models using digital twins: Preference prediction with BIM-extracted spatial-temporal proximity data from Build2Vec](http://arxiv.org/abs/2111.00199)


  Conventional thermal preference prediction in buildings has limitations due
to the difficulty in capturing all environmental and personal factors. New
model features can improve the ability of a machine learning model to classify
a person's thermal preference. The spatial context of a building can provide
information to models about the windows, walls, heating and cooling sources,
air diffusers, and other factors that create micro-environments that influence
thermal comfort. Due to spatial heterogeneity, it is impractical to position
sensors at a high enough resolution to capture all conditions. This research
aims to build upon an existing vector-based spatial model, called Build2Vec,
for predicting spatial-temporal occupants' indoor environmental preferences.
Build2Vec utilizes the spatial data from the Building Information Model (BIM)
and indoor localization in a real-world setting. This framework uses
longitudinal intensive thermal comfort subjective feedback from smart
watch-based ecological momentary assessments (EMA). The aggregation of these
data is combined into a graph network structure (i.e., objects and relations)
and used as input for a classification model to predict occupant thermal
preference. The results of a test implementation show 14-28% accuracy
improvement over a set of baselines that use conventional thermal preference
prediction input variables.

    

### [[2111.00206] One Step at a Time: Pros and Cons of Multi-Step Meta-Gradient Reinforcement Learning](http://arxiv.org/abs/2111.00206)


  Self-tuning algorithms that adapt the learning process online encourage more
effective and robust learning. Among all the methods available, meta-gradients
have emerged as a promising approach. They leverage the differentiability of
the learning rule with respect to some hyper-parameters to adapt them in an
online fashion. Although meta-gradients can be accumulated over multiple
learning steps to avoid myopic updates, this is rarely used in practice. In
this work, we demonstrate that whilst multi-step meta-gradients do provide a
better learning signal in expectation, this comes at the cost of a significant
increase in variance, hindering performance. In the light of this analysis, we
introduce a novel method mixing multiple inner steps that enjoys a more
accurate and robust meta-gradient signal, essentially trading off bias and
variance in meta-gradient estimation. When applied to the Snake game, the
mixing meta-gradient algorithm can cut the variance by a factor of 3 while
achieving similar or higher performance.

    

### [[2111.00210] Mastering Atari Games with Limited Data](http://arxiv.org/abs/2111.00210)


  Reinforcement learning has achieved great success in many applications.
However, sample efficiency remains a key challenge, with prominent methods
requiring millions (or even billions) of environment steps to train. Recently,
there has been significant progress in sample efficient image-based RL
algorithms; however, consistent human-level performance on the Atari game
benchmark remains an elusive goal. We propose a sample efficient model-based
visual RL algorithm built on MuZero, which we name EfficientZero. Our method
achieves 190.4% mean human performance and 116.0% median performance on the
Atari 100k benchmark with only two hours of real-time game experience and
outperforms the state SAC in some tasks on the DMControl 100k benchmark. This
is the first time an algorithm achieves super-human performance on Atari games
with such little data. EfficientZero's performance is also close to DQN's
performance at 200 million frames while we consume 500 times less data.
EfficientZero's low sample complexity and high performance can bring RL closer
to real-world applicability. We implement our algorithm in an
easy-to-understand manner and it is available at
this https URL. We hope it will accelerate the research
of MCTS-based RL algorithms in the wider community.

    

### [[2111.00213] Adjacency constraint for efficient hierarchical reinforcement learning](http://arxiv.org/abs/2111.00213)


  Goal-conditioned Hierarchical Reinforcement Learning (HRL) is a promising
approach for scaling up reinforcement learning (RL) techniques. However, it
often suffers from training inefficiency as the action space of the high-level,
i.e., the goal space, is large. Searching in a large goal space poses
difficulty for both high-level subgoal generation and low-level policy
learning. In this paper, we show that this problem can be effectively
alleviated by restricting the high-level action space from the whole goal space
to a $k$-step adjacent region of the current state using an adjacency
constraint. We theoretically prove that in a deterministic Markov Decision
Process (MDP), the proposed adjacency constraint preserves the optimal
hierarchical policy, while in a stochastic MDP the adjacency constraint induces
a bounded state-value suboptimality determined by the MDP's transition
structure. We further show that this constraint can be practically implemented
by training an adjacency network that can discriminate between adjacent and
non-adjacent subgoals. Experimental results on discrete and continuous control
tasks including challenging simulated robot locomotion and manipulation tasks
show that incorporating the adjacency constraint significantly boosts the
performance of state-of-the-art goal-conditioned HRL approaches.

    

### [[2111.00215] Approximation properties of Residual Neural Networks for Kolmogorov PDEs](http://arxiv.org/abs/2111.00215)


  In recent years residual neural networks (ResNets) as introduced by [He, K.,
Zhang, X., Ren, S., and Sun, J., Proceedings of the IEEE conference on computer
vision and pattern recognition (2016), 770-778] have become very popular in a
large number of applications, including in image classification and
segmentation. They provide a new perspective in training very deep neural
networks without suffering the vanishing gradient problem. In this article we
show that ResNets are able to approximate solutions of Kolmogorov partial
differential equations (PDEs) with constant diffusion and possibly nonlinear
drift coefficients without suffering the curse of dimensionality, which is to
say the number of parameters of the approximating ResNets grows at most
polynomially in the reciprocal of the approximation accuracy $\varepsilon > 0$
and the dimension of the considered PDE $d\in\mathbb{N}$. We adapt a proof in
[Jentzen, A., Salimova, D., and Welti, T., Commun. Math. Sci. 19, 5 (2021),
1167-1205] - who showed a similar result for feedforward neural networks (FNNs)
- to ResNets. In contrast to FNNs, the Euler-Maruyama approximation structure
of ResNets simplifies the construction of the approximating ResNets
substantially. Moreover, contrary to the above work, in our proof using ResNets
does not require the existence of an FNN (or a ResNet) representing the
identity map, which enlarges the set of applicable activation functions.

    

### [[2111.00219] Unpaired Learning for High Dynamic Range Image Tone Mapping](http://arxiv.org/abs/2111.00219)


  High dynamic range (HDR) photography is becoming increasingly popular and
available by DSLR and mobile-phone cameras. While deep neural networks (DNN)
have greatly impacted other domains of image manipulation, their use for HDR
tone-mapping is limited due to the lack of a definite notion of ground-truth
solution, which is needed for producing training data.
In this paper we describe a new tone-mapping approach guided by the distinct
goal of producing low dynamic range (LDR) renditions that best reproduce the
visual characteristics of native LDR images. This goal enables the use of an
unpaired adversarial training based on unrelated sets of HDR and LDR images,
both of which are widely available and easy to acquire.
In order to achieve an effective training under this minimal requirements, we
introduce the following new steps and components: (i) a range-normalizing
pre-process which estimates and applies a different level of curve-based
compression, (ii) a loss that preserves the input content while allowing the
network to achieve its goal, and (iii) the use of a more concise discriminator
network, designed to promote the reproduction of low-level attributes native
LDR possess.
Evaluation of the resulting network demonstrates its ability to produce
photo-realistic artifact-free tone-mapped images, and state-of-the-art
performance on different image fidelity indices and visual distances.

    

### [[2111.00231] Two Heads are Better than One: Geometric-Latent Attention for Point Cloud Classification and Segmentation](http://arxiv.org/abs/2111.00231)


  We present an innovative two-headed attention layer that combines geometric
and latent features to segment a 3D scene into semantically meaningful subsets.
Each head combines local and global information, using either the geometric or
latent features, of a neighborhood of points and uses this information to learn
better local relationships. This Geometric-Latent attention layer (Ge-Latto) is
combined with a sub-sampling strategy to capture global features. Our method is
invariant to permutation thanks to the use of shared-MLP layers, and it can
also be used with point clouds with varying densities because the local
attention layer does not depend on the neighbor order. Our proposal is simple
yet robust, which allows it to achieve competitive results in the ShapeNetPart
and ModelNet40 datasets, and the state-of-the-art when segmenting the complex
dataset S3DIS, with 69.2% IoU on Area 5, and 89.7% overall accuracy using
K-fold cross-validation on the 6 areas.

    

### [[2111.00234] On Joint Learning for Solving Placement and Routing in Chip Design](http://arxiv.org/abs/2111.00234)


  For its advantage in GPU acceleration and less dependency on human experts,
machine learning has been an emerging tool for solving the placement and
routing problems, as two critical steps in modern chip design flow. Being still
in its early stage, there are fundamental issues: scalability, reward design,
and end-to-end learning paradigm etc. To achieve end-to-end placement learning,
we first propose a joint learning method termed by DeepPlace for the placement
of macros and standard cells, by the integration of reinforcement learning with
a gradient based optimization scheme. To further bridge the placement with the
subsequent routing task, we also develop a joint learning approach via
reinforcement learning to fulfill both macro placement and routing, which is
called DeepPR. One key design in our (reinforcement) learning paradigm involves
a multi-view embedding model to encode both global graph level and local node
level information of the input macros. Moreover, the random network
distillation is devised to encourage exploration. Experiments on public chip
design benchmarks show that our method can effectively learn from experience
and also provides intermediate placement for the post standard cell placement,
within few hours for training.

    

### [[2111.00243] The CAT SET on the MAT: Cross Attention for Set Matching in Bipartite Hypergraphs](http://arxiv.org/abs/2111.00243)


  Usual relations between entities could be captured using graphs; but those of
a higher-order -- more so between two different types of entities (which we
term "left" and "right") -- calls for a "bipartite hypergraph". For example,
given a left set of symptoms and right set of diseases, the relation between a
set subset of symptoms (that a patient experiences at a given point of time)
and a subset of diseases (that he/she might be diagnosed with) could be
well-represented using a bipartite hyperedge. The state-of-the-art in embedding
nodes of a hypergraph is based on learning the self-attention structure between
node-pairs from a hyperedge. In the present work, given a bipartite hypergraph,
we aim at capturing relations between node pairs from the cross-product between
the left and right hyperedges, and term it a "cross-attention" (CAT) based
model. More precisely, we pose "bipartite hyperedge link prediction" as a
set-matching (SETMAT) problem and propose a novel neural network architecture
called CATSETMAT for the same. We perform extensive experiments on multiple
bipartite hypergraph datasets to show the superior performance of CATSETMAT,
which we compare with multiple techniques from the state-of-the-art. Our
results also elucidate information flow in self- and cross-attention scenarios.

    

### [[2111.00254] Equinox: neural networks in JAX via callable PyTrees and filtered transformations](http://arxiv.org/abs/2111.00254)


  JAX and PyTorch are two popular Python autodifferentiation frameworks. JAX is
based around pure functions and functional programming. PyTorch has popularised
the use of an object-oriented (OO) class-based syntax for defining
parameterised functions, such as neural networks. That this seems like a
fundamental difference means current libraries for building parameterised
functions in JAX have either rejected the OO approach entirely (Stax) or have
introduced OO-to-functional transformations, multiple new abstractions, and
been limited in the extent to which they integrate with JAX (Flax, Haiku,
Objax). Either way this OO/functional difference has been a source of tension.
Here, we introduce `Equinox', a small neural network library showing how a
PyTorch-like class-based approach may be admitted without sacrificing JAX-like
functional programming. We provide two main ideas. One: parameterised functions
are themselves represented as `PyTrees', which means that the parameterisation
of a function is transparent to the JAX framework. Two: we filter a PyTree to
isolate just those components that should be treated when transforming (`jit',
`grad' or `vmap'-ing) a higher-order function of a parameterised function --
such as a loss function applied to a model. Overall Equinox resolves the above
tension without introducing any new programmatic abstractions: only PyTrees and
transformations, just as with regular JAX. Equinox is available at
\url{this https URL}.

    

### [[2111.00256] Love tHy Neighbour: Remeasuring Local Structural Node Similarity in Hypergraph-Derived Networks](http://arxiv.org/abs/2111.00256)


  The problem of node-similarity in networks has motivated a plethora of such
measures between node-pairs, which make use of the underlying graph structure.
However, higher-order relations cannot be losslessly captured by mere graphs
and hence, extensions thereof viz. hypergraphs are used instead. Measuring
proximity between node pairs in such a setting calls for a revision in the
topological measures of similarity, lest the hypergraph structure remains
under-exploited. We, in this work, propose a multitude of hypergraph-oriented
similarity scores between node-pairs, thereby providing novel solutions to the
link prediction problem. As a part of our proposition, we provide theoretical
formulations to extend graph-topology based scores to hypergraphs. We compare
our scores with graph-based scores (over clique-expansions of hypergraphs into
graphs) from the state-of-the-art. Using a combination of the existing
graph-based and the proposed hypergraph-based similarity scores as features for
a classifier predicts links much better than using the former solely.
Experiments on several real-world datasets and both quantitative as well as
qualitative analyses on the same exhibit the superiority of the proposed
similarity scores over the existing ones.

    

### [[2111.00262] Learning Coordinated Terrain-Adaptive Locomotion by Imitating a Centroidal Dynamics Planner](http://arxiv.org/abs/2111.00262)


  Dynamic quadruped locomotion over challenging terrains with precise foot
placements is a hard problem for both optimal control methods and Reinforcement
Learning (RL). Non-linear solvers can produce coordinated constraint satisfying
motions, but often take too long to converge for online application. RL methods
can learn dynamic reactive controllers but require carefully tuned shaping
rewards to produce good gaits and can have trouble discovering precise
coordinated movements. Imitation learning circumvents this problem and has been
used with motion capture data to extract quadruped gaits for flat terrains.
However, it would be costly to acquire motion capture data for a very large
variety of terrains with height differences. In this work, we combine the
advantages of trajectory optimization and learning methods and show that
terrain adaptive controllers can be obtained by training policies to imitate
trajectories that have been planned over procedural terrains by a non-linear
solver. We show that the learned policies transfer to unseen terrains and can
be fine-tuned to dynamically traverse challenging terrains that require precise
foot placements and are very hard to solve with standard RL.

    

### [[2111.00271] Higher-Order Relations Skew Link Prediction in Graphs](http://arxiv.org/abs/2111.00271)


  The problem of link prediction is of active interest. The main approach to
solving the link prediction problem is based on heuristics such as Common
Neighbors (CN) -- more number of common neighbors of a pair of nodes implies a
higher chance of them getting linked. In this article, we investigate this
problem in the presence of higher-order relations. Surprisingly, it is found
that CN works very well, and even better in the presence of higher-order
relations. However, as we prove in the current work, this is due to the
CN-heuristic overestimating its prediction abilities in the presence of
higher-order relations. This statement is proved by considering a theoretical
model for higher-order relations and by showing that AUC scores of CN are
higher than can be achieved from the model. Theoretical justification in simple
cases is also provided. Further, we extend our observations to other similar
link prediction algorithms such as Adamic Adar. Finally, these insights are
used to propose an adjustment factor by taking into conscience that a random
graph would only have a best AUC score of 0.5. This adjustment factor allows
for a better estimation of generalization scores.

    

### [[2111.00295] Get Fooled for the Right Reason: Improving Adversarial Robustness through a Teacher-guided Curriculum Learning Approach](http://arxiv.org/abs/2111.00295)


  Current SOTA adversarially robust models are mostly based on adversarial
training (AT) and differ only by some regularizers either at inner maximization
or outer minimization steps. Being repetitive in nature during the inner
maximization step, they take a huge time to train. We propose a non-iterative
method that enforces the following ideas during training. Attribution maps are
more aligned to the actual object in the image for adversarially robust models
compared to naturally trained models. Also, the allowed set of pixels to
perturb an image (that changes model decision) should be restricted to the
object pixels only, which reduces the attack strength by limiting the attack
space. Our method achieves significant performance gains with a little extra
effort (10-20%) over existing AT models and outperforms all other methods in
terms of adversarial as well as natural accuracy. We have performed extensive
experimentation with CIFAR-10, CIFAR-100, and TinyImageNet datasets and
reported results against many popular strong adversarial attacks to prove the
effectiveness of our method.

    

### [[2111.00298] A fast accurate fine-grain object detection model based on YOLOv4 deep neural network](http://arxiv.org/abs/2111.00298)


  Early identification and prevention of various plant diseases in commercial
farms and orchards is a key feature of precision agriculture technology. This
paper presents a high-performance real-time fine-grain object detection
framework that addresses several obstacles in plant disease detection that
hinder the performance of traditional methods, such as, dense distribution,
irregular morphology, multi-scale object classes, textural similarity, etc. The
proposed model is built on an improved version of the You Only Look Once
(YOLOv4) algorithm. The modified network architecture maximizes both detection
accuracy and speed by including the DenseNet in the back-bone to optimize
feature transfer and reuse, two new residual blocks in the backbone and neck
enhance feature extraction and reduce computing cost; the Spatial Pyramid
Pooling (SPP) enhances receptive field, and a modified Path Aggregation Network
(PANet) preserves fine-grain localized information and improve feature fusion.
Additionally, the use of the Hard-Swish function as the primary activation
improved the model's accuracy due to better nonlinear feature extraction. The
proposed model is tested in detecting four different diseases in tomato plants
under various challenging environments. The model outperforms the existing
state-of-the-art detection models in detection accuracy and speed. At a
detection rate of 70.19 FPS, the proposed model obtained a precision value of
$90.33 \%$, F1-score of $93.64 \%$, and a mean average precision ($mAP$) value
of $96.29 \%$. Current work provides an effective and efficient method for
detecting different plant diseases in complex scenarios that can be extended to
different fruit and crop detection, generic disease detection, and various
automated agricultural detection processes.

    

### [[2111.00299] Throughput and Latency in the Distributed Q-Learning Random Access mMTC Networks](http://arxiv.org/abs/2111.00299)


  In mMTC mode, with thousands of devices trying to access network resources
sporadically, the problem of random access (RA) and collisions between devices
that select the same resources becomes crucial. A promising approach to solve
such an RA problem is to use learning mechanisms, especially the Q-learning
algorithm, where the devices learn about the best time-slot periods to transmit
through rewards sent by the central node. In this work, we propose a
distributed packet-based learning method by varying the reward from the central
node that favors devices having a larger number of remaining packets to
transmit. Our numerical results indicated that the proposed distributed
packet-based Q-learning method attains a much better throughput-latency
trade-off than the alternative independent and collaborative techniques in
practical scenarios of interest. In contrast, the number of payload bits of the
packet-based technique is reduced regarding the collaborative Q-learning RA
technique for achieving the same normalized throughput.

    

### [[2111.00303] Optimizing Binary Symptom Checkers via Approximate Message Passing](http://arxiv.org/abs/2111.00303)


  Symptom checkers have been widely adopted as an intelligent e-healthcare
application during the ongoing pandemic crisis. Their performance have been
limited by the fine-grained quality of the collected medical knowledge between
symptom and diseases. While the binarization of the relationships between
symptoms and diseases simplifies the data collection process, it also leads to
non-convex optimization problems during the inference step. In this paper, we
formulate the symptom checking problem as an underdertermined non-convex
optimization problem, thereby justifying the use of the compressive sensing
framework to solve it. We show that the generalized vector approximate message
passing (G-VAMP) algorithm provides the best performance for binary symptom
checkers.

    

### [[2111.00314] ECG synthesis with Neural ODE and GAN models](http://arxiv.org/abs/2111.00314)


  Continuous medical time series data such as ECG is one of the most complex
time series due to its dynamic and high dimensional characteristics. In
addition, due to its sensitive nature, privacy concerns and legal restrictions,
it is often even complex to use actual data for different medical research. As
a result, generating continuous medical time series is a very critical research
area. Several research works already showed that the ability of generative
adversarial networks (GANs) in the case of continuous medical time series
generation is promising. Most medical data generation works, such as ECG
synthesis, are mainly driven by the GAN model and its variation. On the other
hand, Some recent work on Neural Ordinary Differential Equation (Neural ODE)
demonstrates its strength against informative missingness, high dimension as
well as dynamic nature of continuous time series. Instead of considering
continuous-time series as a discrete-time sequence, Neural ODE can train
continuous time series in real-time continuously. In this work, we used Neural
ODE based model to generate synthetic sine waves and synthetic ECG. We
introduced a new technique to design the generative adversarial network with
Neural ODE based Generator and Discriminator. We developed three new models to
synthesise continuous medical data. Different evaluation metrics are then used
to quantitatively assess the quality of generated synthetic data for real-world
applications and data analysis. Another goal of this work is to combine the
strength of GAN and Neural ODE to generate synthetic continuous medical time
series data such as ECG. We also evaluated both the GAN model and the Neural
ODE model to understand the comparative efficiency of models from the GAN and
Neural ODE family in medical data synthesis.

    

### [[2111.00316] Real-time Speaker counting in a cocktail party scenario using Attention-guided Convolutional Neural Network](http://arxiv.org/abs/2111.00316)


  Most current speech technology systems are designed to operate well even in
the presence of multiple active speakers. However, most solutions assume that
the number of co-current speakers is known. Unfortunately, this information
might not always be available in real-world applications. In this study, we
propose a real-time, single-channel attention-guided Convolutional Neural
Network (CNN) to estimate the number of active speakers in overlapping speech.
The proposed system extracts higher-level information from the speech spectral
content using a CNN model. Next, the attention mechanism summarizes the
extracted information into a compact feature vector without losing critical
information. Finally, the active speakers are classified using a fully
connected network. Experiments on simulated overlapping speech using WSJ corpus
show that the attention solution is shown to improve the performance by almost
3% absolute over conventional temporal average pooling. The proposed
Attention-guided CNN achieves 76.15% for both Weighted Accuracy and average
Recall, and 75.80% Precision on speech segments as short as 20 frames (i.e.,
200 ms). All the classification metrics exceed 92% for the attention-guided
model in offline scenarios where the input signal is more than 100 frames long
(i.e., 1s).

    

### [[2111.00320] Speaker conditioning of acoustic models using affine transformation for multi-speaker speech recognition](http://arxiv.org/abs/2111.00320)


  This study addresses the problem of single-channel Automatic Speech
Recognition of a target speaker within an overlap speech scenario. In the
proposed method, the hidden representations in the acoustic model are modulated
by speaker auxiliary information to recognize only the desired speaker. Affine
transformation layers are inserted into the acoustic model network to integrate
speaker information with the acoustic features. The speaker conditioning
process allows the acoustic model to perform computation in the context of
target-speaker auxiliary information. The proposed speaker conditioning method
is a general approach and can be applied to any acoustic model architecture.
Here, we employ speaker conditioning on a ResNet acoustic model. Experiments on
the WSJ corpus show that the proposed speaker conditioning method is an
effective solution to fuse speaker auxiliary information with acoustic features
for multi-speaker speech recognition, achieving +9% and +20% relative WER
reduction for clean and overlap speech scenarios, respectively, compared to the
original ResNet acoustic model baseline.

    

### [[2111.00326] Neural Network based on Automatic Differentiation Transformation of Numeric Iterate-to-Fixedpoint](http://arxiv.org/abs/2111.00326)


  This work proposes a Neural Network model that can control its depth using an
iterate-to-fixed-point operator. The architecture starts with a standard
layered Network but with added connections from current later to earlier
layers, along with a gate to make them inactive under most circumstances. These
``temporal wormhole'' connections create a shortcut that allows the Neural
Network to use the information available at deeper layers and re-do earlier
computations with modulated inputs. End-to-end training is accomplished by
using appropriate calculations for a numeric iterate-to-fixed-point operator.
In a typical case, where the ``wormhole'' connections are inactive, this is
inexpensive; but when they are active, the network takes a longer time to
settle down, and the gradient calculation is also more laborious, with an
effect similar to making the network deeper. In contrast to the existing
skip-connection concept, this proposed technique enables information to flow up
and down in the network. Furthermore, the flow of information follows a fashion
that seems analogous to the afferent and efferent flow of information through
layers of processing in the brain. We evaluate models that use this novel
mechanism on different long-term dependency tasks. The results are competitive
with other studies, showing that the proposed model contributes significantly
to overcoming traditional deep learning models' vanishing gradient descent
problem. At the same time, the training time is significantly reduced, as the
``easy'' input cases are processed more quickly than ``difficult'' ones.

    

### [[2111.00328] Multi-Task Learning based Convolutional Models with Curriculum Learning for the Anisotropic Reynolds Stress Tensor in Turbulent Duct Flow](http://arxiv.org/abs/2111.00328)


  The Reynolds-averaged Navier-Stokes (RANS) equations require accurate
modeling of the anisotropic Reynolds stress tensor, for which traditional
closure models only give good results in certain flow configurations.
Researchers have started using machine learning approaches to address this
problem. In this work we build upon recent convolutional neural network
architectures used for turbulence modeling and propose a multi-task learning
based fully convolutional neural network that is able to accurately predict the
normalized anisotropic Reynolds stress tensor for turbulent duct flow.
Furthermore, we also explore the application of curriculum learning to
data-driven turbulence modeling.

    

### [[2111.00340] Identifying and mitigating bias in algorithms used to manage patients in a pandemic](http://arxiv.org/abs/2111.00340)


  Numerous COVID-19 clinical decision support systems have been developed.
However many of these systems do not have the merit for validity due to
methodological shortcomings including algorithmic bias. Methods Logistic
regression models were created to predict COVID-19 mortality, ventilator status
and inpatient status using a real-world dataset consisting of four hospitals in
New York City and analyzed for biases against race, gender and age. Simple
thresholding adjustments were applied in the training process to establish more
equitable models. Results Compared to the naively trained models, the
calibrated models showed a 57% decrease in the number of biased trials, while
predictive performance, measured by area under the receiver/operating curve
(AUC), remained unchanged. After calibration, the average sensitivity of the
predictive models increased from 0.527 to 0.955. Conclusion We demonstrate that
naively training and deploying machine learning models on real world data for
predictive analytics of COVID-19 has a high risk of bias. Simple implemented
adjustments or calibrations during model training can lead to substantial and
sustained gains in fairness on subsequent deployment.

    

### [[2111.00341] Causal Discovery in Linear Structural Causal Models with Deterministic Relations](http://arxiv.org/abs/2111.00341)


  Linear structural causal models (SCMs) -- in which each observed variable is
generated by a subset of the other observed variables as well as a subset of
the exogenous sources -- are pervasive in causal inference and casual
discovery. However, for the task of causal discovery, existing work almost
exclusively focus on the submodel where each observed variable is associated
with a distinct source with non-zero variance. This results in the restriction
that no observed variable can deterministically depend on other observed
variables or latent confounders. In this paper, we extend the results on
structure learning by focusing on a subclass of linear SCMs which do not have
this property, i.e., models in which observed variables can be causally
affected by any subset of the sources, and are allowed to be a deterministic
function of other observed variables or latent confounders. This allows for a
more realistic modeling of influence or information propagation in systems. We
focus on the task of causal discovery form observational data generated from a
member of this subclass. We derive a set of necessary and sufficient conditions
for unique identifiability of the causal structure. To the best of our
knowledge, this is the first work that gives identifiability results for causal
discovery under both latent confounding and deterministic relationships.
Further, we propose an algorithm for recovering the underlying causal structure
when the aforementioned conditions are satisfied. We validate our theoretical
results both on synthetic and real datasets.

    

### [[2111.00343] Continuous Convolutional Neural Networks: Coupled Neural PDE and ODE](http://arxiv.org/abs/2111.00343)


  Recent work in deep learning focuses on solving physical systems in the
Ordinary Differential Equation or Partial Differential Equation. This current
work proposed a variant of Convolutional Neural Networks (CNNs) that can learn
the hidden dynamics of a physical system using ordinary differential equation
(ODEs) systems (ODEs) and Partial Differential Equation systems (PDEs). Instead
of considering the physical system such as image, time -series as a system of
multiple layers, this new technique can model a system in the form of
Differential Equation (DEs). The proposed method has been assessed by solving
several steady-state PDEs on irregular domains, including heat equations,
Navier-Stokes equations.

    

### [[2111.00350] AdvCodeMix: Adversarial Attack on Code-Mixed Data](http://arxiv.org/abs/2111.00350)


  Research on adversarial attacks are becoming widely popular in the recent
years. One of the unexplored areas where prior research is lacking is the
effect of adversarial attacks on code-mixed data. Therefore, in the present
work, we have explained the first generalized framework on text perturbation to
attack code-mixed classification models in a black-box setting. We rely on
various perturbation techniques that preserve the semantic structures of the
sentences and also obscure the attacks from the perception of a human user. The
present methodology leverages the importance of a token to decide where to
attack by employing various perturbation strategies. We test our strategies on
various sentiment classification models trained on Bengali-English and
Hindi-English code-mixed datasets, and reduce their F1-scores by nearly 51 %
and 53 % respectively, which can be further reduced if a larger number of
tokens are perturbed in a given sentence.

    

### [[2111.00352] Optimizing Sparse Matrix Multiplications for Graph Neural Networks](http://arxiv.org/abs/2111.00352)


  Graph neural networks (GNNs) are emerging as a powerful technique for
modeling graph structures. Due to the sparsity of real-world graph data, GNN
performance is limited by extensive sparse matrix multiplication (SpMM)
operations involved in computation. While the right sparse matrix storage
format varies across input data, existing deep learning frameworks employ a
single, static storage format, leaving much room for improvement. This paper
investigates how the choice of sparse matrix storage formats affect the GNN
performance. We observe that choosing a suitable sparse matrix storage format
can significantly improve the GNN training performance, but the right format
depends on the input workloads and can change as the GNN iterates over the
input graph. We then develop a predictive model to dynamically choose a sparse
matrix storage format to be used by a GNN layer based on the input matrices.
Our model is first trained offline using training matrix samples, and the
trained model can be applied to any input matrix and GNN kernels with SpMM
computation. We implement our approach on top of PyTorch and apply it to 5
representative GNN models running on a multi-core CPU using real-life and
synthetic datasets. Experimental results show that our approach gives an
average speedup of 1.17x (up to 3x) for GNN running time.

    

### [[2111.00358] A Survey on the Robustness of Feature Importance and Counterfactual Explanations](http://arxiv.org/abs/2111.00358)


  There exist several methods that aim to address the crucial task of
understanding the behaviour of AI/ML models. Arguably, the most popular among
them are local explanations that focus on investigating model behaviour for
individual instances. Several methods have been proposed for local analysis,
but relatively lesser effort has gone into understanding if the explanations
are robust and accurately reflect the behaviour of underlying models. In this
work, we present a survey of the works that analysed the robustness of two
classes of local explanations (feature importance and counterfactual
explanations) that are popularly used in analysing AI/ML models in finance. The
survey aims to unify existing definitions of robustness, introduces a taxonomy
to classify different robustness approaches, and discusses some interesting
results. Finally, the survey introduces some pointers about extending current
robustness analysis approaches so as to identify reliable explainability
methods.

    

### [[2111.00364] Sustainable AI: Environmental Implications, Challenges and Opportunities](http://arxiv.org/abs/2111.00364)


  This paper explores the environmental impact of the super-linear growth
trends for AI from a holistic perspective, spanning Data, Algorithms, and
System Hardware. We characterize the carbon footprint of AI computing by
examining the model development cycle across industry-scale machine learning
use cases and, at the same time, considering the life cycle of system hardware.
Taking a step further, we capture the operational and manufacturing carbon
footprint of AI computing and present an end-to-end analysis for what and how
hardware-software design and at-scale optimization can help reduce the overall
carbon footprint of AI. Based on the industry experience and lessons learned,
we share the key challenges and chart out important development directions
across the many dimensions of AI. We hope the key messages and insights
presented in this paper can inspire the community to advance the field of AI in
an environmentally-responsible manner.

    

### [[2111.00379] EfficientWord-Net: An Open Source Hotword Detection Engine based on One-shot Learning](http://arxiv.org/abs/2111.00379)


  Voice assistants like Siri, Google Assistant, Alexa etc. are used widely
across the globe for home automation, these require the use of special phrases
also known as hotwords to wake it up and perform an action like "Hey Alexa!",
"Ok Google!" and "Hey Siri!" etc. These hotwords are detected with lightweight
real-time engines whose purpose is to detect the hotwords uttered by the user.
This paper presents the design and implementation of a hotword detection engine
based on one-shot learning which detects the hotword uttered by the user in
real-time with just one or few training samples of the hotword. This approach
is efficient when compared to existing implementations because the process of
adding a new hotword in the existing systems requires enormous amounts of
positive and negative training samples and the model needs to retrain for every
hotword. This makes the existing implementations inefficient in terms of
computation and cost. The architecture proposed in this paper has achieved an
accuracy of 94.51%.

    

### [[2111.00396] Efficiently Modeling Long Sequences with Structured State Spaces](http://arxiv.org/abs/2111.00396)


  A central goal of sequence modeling is designing a single principled model
that can address sequence data across a range of modalities and tasks,
particularly on long-range dependencies. Although conventional models including
RNNs, CNNs, and Transformers have specialized variants for capturing long
dependencies, they still struggle to scale to very long sequences of $10000$ or
more steps. A promising recent approach proposed modeling sequences by
simulating the fundamental state space model (SSM) \( x'(t) = Ax(t) + Bu(t),
y(t) = Cx(t) + Du(t) \), and showed that for appropriate choices of the state
matrix \( A \), this system could handle long-range dependencies mathematically
and empirically. However, this method has prohibitive computation and memory
requirements, rendering it infeasible as a general sequence modeling solution.
We propose the Structured State Space (S4) sequence model based on a new
parameterization for the SSM, and show that it can be computed much more
efficiently than prior approaches while preserving their theoretical strengths.
Our technique involves conditioning \( A \) with a low-rank correction,
allowing it to be diagonalized stably and reducing the SSM to the well-studied
computation of a Cauchy kernel. S4 achieves strong empirical results across a
diverse range of established benchmarks, including (i) 91\% accuracy on
sequential CIFAR-10 with no data augmentation or auxiliary losses, on par with
a larger 2-D ResNet, (ii) substantially closing the gap to Transformers on
image and language modeling tasks, while performing generation $60\times$
faster (iii) SoTA on every task from the Long Range Arena benchmark, including
solving the challenging Path-X task of length 16k that all prior work fails on,
while being as efficient as all competitors.

    

### [[2111.00411] Safe Adaptive Learning-based Control for Constrained Linear Quadratic Regulators with Regret Guarantees](http://arxiv.org/abs/2111.00411)


  We study the adaptive control of an unknown linear system with a quadratic
cost function subject to safety constraints on both the states and actions. The
challenges of this problem arise from the tension among safety, exploration,
performance, and computation. To address these challenges, we propose a
polynomial-time algorithm that guarantees feasibility and constraint
satisfaction with high probability under proper conditions. Our algorithm is
implemented on a single trajectory and does not require system restarts.
Further, we analyze the regret of our learning algorithm compared to the
optimal safe linear controller with known model information. The proposed
algorithm can achieve a $\tilde O(T^{2/3})$ regret, where $T$ is the number of
stages and $\tilde O(\cdot)$ absorbs some logarithmic terms of $T$.

    

### [[2111.00418] Deep Learning in Human Activity Recognition with Wearable Sensors: A Review on Advances](http://arxiv.org/abs/2111.00418)


  Mobile and wearable devices have enabled numerous applications, including
activity tracking, wellness monitoring, and human-computer interaction, that
measure and improve our daily lives. Many of these applications are made
possible by leveraging the rich collection of low-power sensors found in many
mobile and wearable devices to perform human activity recognition (HAR).
Recently, deep learning has greatly pushed the boundaries of HAR on mobile and
wearable devices. This paper systematically categorizes and summarizes existing
work that introduces deep learning methods for wearables-based HAR and provides
a comprehensive analysis of the current advancements, developing trends, and
major challenges. We also present cutting-edge frontiers and future directions
for deep learning--based HAR.

    

### [[2111.00426] Using Google Trends as a proxy for occupant behavior to predict building energy consumption](http://arxiv.org/abs/2111.00426)


  In recent years, the availability of larger amounts of energy data and
advanced machine learning algorithms has created a surge in building energy
prediction research. However, one of the variables in energy prediction models,
occupant behavior, is crucial for prediction performance but hard-to-measure or
time-consuming to collect from each building. This study proposes an approach
that utilizes the search volume of topics (e.g., education} or Microsoft Excel)
on the Google Trends platform as a proxy of occupant behavior and use of
buildings. Linear correlations were first examined to explore the relationship
between energy meter data and Google Trends search terms to infer building
occupancy. Prediction errors before and after the inclusion of the trends of
these terms were compared and analyzed based on the ASHRAE Great Energy
Predictor III (GEPIII) competition dataset. The results show that highly
correlated Google Trends data can effectively reduce the overall RMSLE error
for a subset of the buildings to the level of the GEPIII competition's top five
winning teams' performance. In particular, the RMSLE error reduction during
public holidays and days with site-specific schedules are respectively reduced
by 20-30% and 2-5%. These results show the potential of using Google Trends to
improve energy prediction for a portion of the building stock by automatically
identifying site-specific and holiday schedules.

    

### [[2111.00430] Efficient passive membership inference attack in federated learning](http://arxiv.org/abs/2111.00430)


  In cross-device federated learning (FL) setting, clients such as mobiles
cooperate with the server to train a global machine learning model, while
maintaining their data locally. However, recent work shows that client's
private information can still be disclosed to an adversary who just eavesdrops
the messages exchanged between the client and the server. For example, the
adversary can infer whether the client owns a specific data instance, which is
called a passive membership inference attack. In this paper, we propose a new
passive inference attack that requires much less computation power and memory
than existing methods. Our empirical results show that our attack achieves a
higher accuracy on CIFAR100 dataset (more than $4$ percentage points) with
three orders of magnitude less memory space and five orders of magnitude less
calculations.

    

### [[2111.00435] An Actor-Critic Method for Simulation-Based Optimization](http://arxiv.org/abs/2111.00435)


  We focus on a simulation-based optimization problem of choosing the best
design from the feasible space. Although the simulation model can be queried
with finite samples, its internal processing rule cannot be utilized in the
optimization process. We formulate the sampling process as a policy searching
problem and give a solution from the perspective of Reinforcement Learning
(RL). Concretely, Actor-Critic (AC) framework is applied, where the Actor
serves as a surrogate model to predict the performance on unknown designs,
whereas the actor encodes the sampling policy to be optimized. We design the
updating rule and propose two algorithms for the cases where the feasible
spaces are continuous and discrete respectively. Some experiments are designed
to validate the effectiveness of proposed algorithms, including two toy
examples, which intuitively explain the algorithms, and two more complex tasks,
i.e., adversarial attack task and RL task, which validate the effectiveness in
large-scale problems. The results show that the proposed algorithms can
successfully deal with these problems. Especially note that in the RL task, our
methods give a new perspective to robot control by treating the task as a
simulation model and solving it by optimizing the policy generating process,
while existing works commonly optimize the policy itself directly.

    

### [[2111.00438] Decentralized Multi-Agent Reinforcement Learning: An Off-Policy Method](http://arxiv.org/abs/2111.00438)


  We discuss the problem of decentralized multi-agent reinforcement learning
(MARL) in this work. In our setting, the global state, action, and reward are
assumed to be fully observable, while the local policy is protected as privacy
by each agent, and thus cannot be shared with others. There is a communication
graph, among which the agents can exchange information with their neighbors.
The agents make individual decisions and cooperate to reach a higher
accumulated reward.
Towards this end, we first propose a decentralized actor-critic (AC) setting.
Then, the policy evaluation and policy improvement algorithms are designed for
discrete and continuous state-action-space Markov Decision Process (MDP)
respectively. Furthermore, convergence analysis is given under the
discrete-space case, which guarantees that the policy will be reinforced by
alternating between the processes of policy evaluation and policy improvement.
In order to validate the effectiveness of algorithms, we design experiments and
compare them with previous algorithms, e.g., Q-learning \cite{watkins1992q} and
MADDPG \cite{lowe2017multi}. The results show that our algorithms perform
better from the aspects of both learning speed and final performance. Moreover,
the algorithms can be executed in an off-policy manner, which greatly improves
the data efficiency compared with on-policy algorithms.

    

### [[2111.00459] Graph Neural Network based scheduling : Improved throughput under a generalized interference model](http://arxiv.org/abs/2111.00459)


  In this work, we propose a Graph Convolutional Neural Networks (GCN) based
scheduling algorithm for adhoc networks. In particular, we consider a
generalized interference model called the $k$-tolerant conflict graph model and
design an efficient approximation for the well-known Max-Weight scheduling
algorithm. A notable feature of this work is that the proposed method do not
require labelled data set (NP-hard to compute) for training the neural network.
Instead, we design a loss function that utilises the existing greedy approaches
and trains a GCN that improves the performance of greedy approaches. Our
extensive numerical experiments illustrate that using our GCN approach, we can
significantly ($4$-$20$ percent) improve the performance of the conventional
greedy approach.

    

### [[2111.00463] FastCover: An Unsupervised Learning Framework for Multi-Hop Influence Maximization in Social Networks](http://arxiv.org/abs/2111.00463)


  Finding influential users in social networks is a fundamental problem with
many possible useful applications. Viewing the social network as a graph, the
influence of a set of users can be measured by the number of neighbors located
within a given number of hops in the network, where each hop marks a step of
influence diffusion. In this paper, we reduce the problem of IM to a
budget-constrained d-hop dominating set problem (kdDSP). We propose a unified
machine learning (ML) framework, FastCover, to solve kdDSP by learning an
efficient greedy strategy in an unsupervised way. As one critical component of
the framework, we devise a novel graph neural network (GNN) architecture, graph
reversed attention network (GRAT), that captures the diffusion process among
neighbors. Unlike most heuristic algorithms and concurrent ML frameworks for
combinatorial optimization problems, FastCover determines the entire seed set
from the nodes' scores computed with only one forward propagation of the GNN
and has a time complexity quasi-linear in the graph size. Experiments on
synthetic graphs and real-world social networks demonstrate that FastCover
finds solutions with better or comparable quality rendered by the concurrent
algorithms while achieving a speedup of over 1000x.

    

### [[2111.00465] DAdaQuant: Doubly-adaptive quantization for communication-efficient Federated Learning](http://arxiv.org/abs/2111.00465)


  Federated Learning (FL) is a powerful technique for training a model on a
server with data from several clients in a privacy-preserving manner. In FL, a
server sends the model to every client, who then train the model locally and
send it back to the server. The server aggregates the updated models and
repeats the process for several rounds. FL incurs significant communication
costs, in particular when transmitting the updated local models from the
clients back to the server. Recently proposed algorithms quantize the model
parameters to efficiently compress FL communication. These algorithms typically
have a quantization level that controls the compression factor. We find that
dynamic adaptations of the quantization level can boost compression without
sacrificing model quality. First, we introduce a time-adaptive quantization
algorithm that increases the quantization level as training progresses. Second,
we introduce a client-adaptive quantization algorithm that assigns each
individual client the optimal quantization level at every round. Finally, we
combine both algorithms into DAdaQuant, the doubly-adaptive quantization
algorithm. Our experiments show that DAdaQuant consistently improves
client$\rightarrow$server compression, outperforming the strongest non-adaptive
baselines by up to $2.8\times$.

    

### [[2111.00468] Efficient, Anytime Algorithms for Calibration with Isotonic Regression under Strictly Convex Losses](http://arxiv.org/abs/2111.00468)


  We investigate the calibration of estimations to increase performance with an
optimal monotone transform on the estimator outputs. We start by studying the
traditional square error setting with its weighted variant and show that the
optimal monotone transform is in the form of a unique staircase function. We
further show that this staircase behavior is preserved for general strictly
convex loss functions. Their optimal monotone transforms are also unique, i.e.,
there exist a single staircase transform that achieves the minimum loss. We
propose a linear time and space algorithm that can find such optimal transforms
for specific loss settings. Our algorithm has an online implementation where
the optimal transform for the samples observed so far are found in linear space
and amortized time when the samples arrive in an ordered fashion. We also
extend our results to cases where the functions are not trivial to individually
optimize and propose an anytime algorithm, which has linear space and
pseudo-linearithmic time complexity.

    

### [[2111.00484] IGCN: Image-to-graph Convolutional Network for 2D/3D Deformable Registration](http://arxiv.org/abs/2111.00484)


  Organ shape reconstruction based on a single-projection image during
treatment has wide clinical scope, e.g., in image-guided radiotherapy and
surgical guidance. We propose an image-to-graph convolutional network that
achieves deformable registration of a 3D organ mesh for a single-viewpoint 2D
projection image. This framework enables simultaneous training of two types of
transformation: from the 2D projection image to a displacement map, and from
the sampled per-vertex feature to a 3D displacement that satisfies the
geometrical constraint of the mesh structure. Assuming application to radiation
therapy, the 2D/3D deformable registration performance is verified for multiple
abdominal organs that have not been targeted to date, i.e., the liver, stomach,
duodenum, and kidney, and for pancreatic cancer. The experimental results show
shape prediction considering relationships among multiple organs can be used to
predict respiratory motion and deformation from digitally reconstructed
radiographs with clinically acceptable accuracy.

    

### [[2111.00487] Smart(Sampling)Augment: Optimal and Efficient Data Augmentation for Semantic Segmentation](http://arxiv.org/abs/2111.00487)


  Data augmentation methods enrich datasets with augmented data to improve the
performance of neural networks. Recently, automated data augmentation methods
have emerged, which automatically design augmentation strategies. Existing work
focuses on image classification and object detection, whereas we provide the
first study on semantic image segmentation and introduce two new approaches:
\textit{SmartAugment} and \textit{SmartSamplingAugment}. SmartAugment uses
Bayesian Optimization to search over a rich space of augmentation strategies
and achieves a new state-of-the-art performance in all semantic segmentation
tasks we consider. SmartSamplingAugment, a simple parameter-free approach with
a fixed augmentation strategy competes in performance with the existing
resource-intensive approaches and outperforms cheap state-of-the-art data
augmentation methods. Further, we analyze the impact, interaction, and
importance of data augmentation hyperparameters and perform ablation studies,
which confirm our design choices behind SmartAugment and SmartSamplingAugment.
Lastly, we will provide our source code for reproducibility and to facilitate
further research.

    

### [[2111.00513] Automated Hyperparameter Optimization Challenge at CIKM 2021 AnalyticCup](http://arxiv.org/abs/2111.00513)


  In this paper, we describe our method for tackling the automated
hyperparameter optimization challenge in QQ Browser 2021 AI Algorithm
Competiton (ACM CIKM 2021 AnalyticCup Track 2). The competition organizers
provide anonymized realistic industrial tasks and datasets for black-box
optimization. Based on our open-sourced package OpenBox, we adopt the Bayesian
optimization framework for configuration sampling and a heuristic early
stopping strategy. We won first place in both the preliminary and final
contests with the results of 0.938291 and 0.918753, respectively.

    

### [[2111.00517] Classification of fetal compromise during labour: signal processing and feature engineering of the cardiotocograph](http://arxiv.org/abs/2111.00517)


  Cardiotocography (CTG) is the main tool used for fetal monitoring during
labour. Interpretation of CTG requires dynamic pattern recognition in real
time. It is recognised as a difficult task with high inter- and intra-observer
disagreement. Machine learning has provided a viable path towards objective and
reliable CTG assessment. In this study, novel CTG features are developed based
on clinical expertise and system control theory using an autoregressive
moving-average (ARMA) model to characterise the response of the fetal heart
rate to contractions. The features are evaluated in a machine learning model to
assess their efficacy in identifying fetal compromise. ARMA features ranked
amongst the top features for detecting fetal compromise. Additionally,
including clinical factors in the machine learning model and pruning data based
on a signal quality measure improved the performance of the classifier.

    

### [[2111.00531] Learning Debiased and Disentangled Representations for Semantic Segmentation](http://arxiv.org/abs/2111.00531)


  Deep neural networks are susceptible to learn biased models with entangled
feature representations, which may lead to subpar performances on various
downstream tasks. This is particularly true for under-represented classes,
where a lack of diversity in the data exacerbates the tendency. This limitation
has been addressed mostly in classification tasks, but there is little study on
additional challenges that may appear in more complex dense prediction problems
including semantic segmentation. To this end, we propose a model-agnostic and
stochastic training scheme for semantic segmentation, which facilitates the
learning of debiased and disentangled representations. For each class, we first
extract class-specific information from the highly entangled feature map. Then,
information related to a randomly sampled class is suppressed by a feature
selection process in the feature space. By randomly eliminating certain class
information in each training iteration, we effectively reduce feature
dependencies among classes, and the model is able to learn more debiased and
disentangled feature representations. Models trained with our approach
demonstrate strong results on multiple semantic segmentation benchmarks, with
especially notable performance gains on under-represented classes.

    

### [[2111.00552] Fast Global Convergence of Policy Optimization for Constrained MDPs](http://arxiv.org/abs/2111.00552)


  We address the issue of safety in reinforcement learning. We pose the problem
in a discounted infinite-horizon constrained Markov decision process framework.
Existing results have shown that gradient-based methods are able to achieve an
$\mathcal{O}(1/\sqrt{T})$ global convergence rate both for the optimality gap
and the constraint violation. We exhibit a natural policy gradient-based
algorithm that has a faster convergence rate $\mathcal{O}(\log(T)/T)$ for both
the optimality gap and the constraint violation. When Slater's condition is
satisfied and known a priori, zero constraint violation can be further
guaranteed for a sufficiently large $T$ while maintaining the same convergence
rate.

    

### [[2111.00554] Quality Estimation Using Round-trip Translation with Sentence Embeddings](http://arxiv.org/abs/2111.00554)


  Estimating the quality of machine translation systems has been an ongoing
challenge for researchers in this field. Many previous attempts at using
round-trip translation as a measure of quality have failed, and there is much
disagreement as to whether it can be a viable method of quality estimation. In
this paper, we revisit round-trip translation, proposing a system which aims to
solve the previous pitfalls found with the approach. Our method makes use of
recent advances in language representation learning to more accurately gauge
the similarity between the original and round-trip translated sentences.
Experiments show that while our approach does not reach the performance of
current state of the art methods, it may still be an effective approach for
some language pairs.

    

### [[2111.00556] Revealing and Protecting Labels in Distributed Training](http://arxiv.org/abs/2111.00556)


  Distributed learning paradigms such as federated learning often involve
transmission of model updates, or gradients, over a network, thereby avoiding
transmission of private data. However, it is possible for sensitive information
about the training data to be revealed from such gradients. Prior works have
demonstrated that labels can be revealed analytically from the last layer of
certain models (e.g., ResNet), or they can be reconstructed jointly with model
inputs by using Gradients Matching [Zhu et al'19] with additional knowledge
about the current state of the model. In this work, we propose a method to
discover the set of labels of training samples from only the gradient of the
last layer and the id to label mapping. Our method is applicable to a wide
variety of model architectures across multiple domains. We demonstrate the
effectiveness of our method for model training in two domains - image
classification, and automatic speech recognition. Furthermore, we show that
existing reconstruction techniques improve their efficacy when used in
conjunction with our method. Conversely, we demonstrate that gradient
quantization and sparsification can significantly reduce the success of the
attack.

    

### [[2111.00565] Can we learn gradients by Hamiltonian Neural Networks?](http://arxiv.org/abs/2111.00565)


  In this work, we propose a meta-learner based on ODE neural networks that
learns gradients. This approach makes the optimizer is more flexible inducing
an automatic inductive bias to the given task. Using the simplest Hamiltonian
Neural Network we demonstrate that our method outperforms a meta-learner based
on LSTM for an artificial task and the MNIST dataset with ReLU activations in
the optimizee. Furthermore, it also surpasses the classic optimization methods
for the artificial task and achieves comparable results for MNIST.

    

### [[2111.00580] Text Classification for Task-based Source Code Related Questions](http://arxiv.org/abs/2111.00580)


  There is a key demand to automatically generate code for small tasks for
developers. Websites such as StackOverflow provide a simplistic way by offering
solutions in small snippets which provide a complete answer to whatever task
question the developer wants to code. Natural Language Processing and
particularly Question-Answering Systems are very helpful in resolving and
working on these tasks. In this paper, we develop a two-fold deep learning
model: Seq2Seq and a binary classifier that takes in the intent (which is in
natural language) and code snippets in Python. We train both the intent and the
code utterances in the Seq2Seq model, where we decided to compare the effect of
the hidden layer embedding from the encoder for representing the intent and
similarly, using the decoder's hidden layer embeddings for the code sequence.
Then we combine both these embeddings and then train a simple binary neural
network classifier model for predicting if the intent is correctly answered by
the predicted code sequence from the seq2seq model. We find that the hidden
state layer's embeddings perform slightly better than regular standard
embeddings from a constructed vocabulary. We experimented with our tests on the
CoNaLa dataset in addition to the StaQC database consisting of simple task-code
snippet-based pairs. We empirically establish that using additional pre-trained
embeddings for code snippets in Python is less context-based in comparison to
using hidden state context vectors from seq2seq models.

    

### [[2111.00587] A Tensor SVD-based Classification Algorithm Applied to fMRI Data](http://arxiv.org/abs/2111.00587)


  To analyze the abundance of multidimensional data, tensor-based frameworks
have been developed. Traditionally, the matrix singular value decomposition
(SVD) is used to extract the most dominant features from a matrix containing
the vectorized data. While the SVD is highly useful for data that can be
appropriately represented as a matrix, this step of vectorization causes us to
lose the high-dimensional relationships intrinsic to the data. To facilitate
efficient multidimensional feature extraction, we utilize a projection-based
classification algorithm using the t-SVDM, a tensor analog of the matrix SVD.
Our work extends the t-SVDM framework and the classification algorithm, both
initially proposed for tensors of order 3, to any number of dimensions. We then
apply this algorithm to a classification task using the StarPlus fMRI dataset.
Our numerical experiments demonstrate that there exists a superior tensor-based
approach to fMRI classification than the best possible equivalent matrix-based
approach. Our results illustrate the advantages of our chosen tensor framework,
provide insight into beneficial choices of parameters, and could be further
developed for classification of more complex imaging data. We provide our
Python implementation at this https URL.

    

### [[2111.00590] Laplacian Constrained Precision Matrix Estimation: Existence and High Dimensional Consistency](http://arxiv.org/abs/2111.00590)


  This paper considers the problem of estimating high dimensional Laplacian
constrained precision matrices by minimizing Stein's loss. We obtain a
necessary and sufficient condition for existence of this estimator, that boils
down to checking whether a certain data dependent graph is connected. We also
prove consistency in the high dimensional setting under the symmetryzed Stein
loss. We show that the error rate does not depend on the graph sparsity, or
other type of structure, and that Laplacian constraints are sufficient for high
dimensional consistency. Our proofs exploit properties of graph Laplacians, and
a characterization of the proposed estimator based on effective graph
resistances. We validate our theoretical claims with numerical experiments.

    

### [[2111.00592] Unsupervised Learning to Subphenotype Delirium Patients from Electronic Health Records](http://arxiv.org/abs/2111.00592)


  Delirium is a common acute onset brain dysfunction in the emergency setting
and is associated with higher mortality. It is difficult to detect and monitor
since its presentations and risk factors can be different depending on the
underlying medical condition of patients. In our study, we aimed to identify
subtypes within the delirium population and build subgroup-specific predictive
models to detect delirium using Medical Information Mart for Intensive Care IV
(MIMIC-IV) data. We showed that clusters exist within the delirium population.
Differences in feature importance were also observed for subgroup-specific
predictive models. Our work could recalibrate existing delirium prediction
models for each delirium subgroup and improve the precision of delirium
detection and monitoring for ICU or emergency department patients who had
highly heterogeneous medical conditions.

    

### [[2111.00599] Bayesian optimization of distributed neurodynamical controller models for spatial navigation](http://arxiv.org/abs/2111.00599)


  Dynamical systems models for controlling multi-agent swarms have demonstrated
advances toward resilient, decentralized navigation algorithms. We previously
introduced the NeuroSwarms controller, in which agent-based interactions were
modeled by analogy to neuronal network interactions, including attractor
dynamics and phase synchrony, that have been theorized to operate within
hippocampal place-cell circuits in navigating rodents. This complexity
precludes linear analyses of stability, controllability, and performance
typically used to study conventional swarm models. Further, tuning dynamical
controllers by hand or grid search is often inadequate due to the complexity of
objectives, dimensionality of model parameters, and computational costs of
simulation-based sampling. Here, we present a framework for tuning dynamical
controller models of autonomous multi-agent systems based on Bayesian
Optimization (BayesOpt). Our approach utilizes a task-dependent objective
function to train Gaussian Processes (GPs) as surrogate models to achieve
adaptive and efficient exploration of a dynamical controller model's parameter
space. We demonstrate this approach by studying an objective function selecting
for NeuroSwarms behaviors that cooperatively localize and capture spatially
distributed rewards under time pressure. We generalized task performance across
environments by combining scores for simulations in distinct geometries. To
validate search performance, we compared high-dimensional clustering for high-
vs. low-likelihood parameter points by visualizing sample trajectories in
Uniform Manifold Approximation and Projection (UMAP) embeddings. Our findings
show that adaptive, sample-efficient evaluation of the self-organizing
behavioral capacities of complex systems, including dynamical swarm
controllers, can accelerate the translation of neuroscientific theory to
applied domains.

    

### [[1901.07418] CAMR: Coded Aggregated MapReduce](http://arxiv.org/abs/1901.07418)


  Many big data algorithms executed on MapReduce-like systems have a shuffle
phase that often dominates the overall job execution time. Recent work has
demonstrated schemes where the communication load in the shuffle phase can be
traded off for the computation load in the map phase. In this work, we focus on
a class of distributed algorithms, broadly used in deep learning, where
intermediate computations of the same task can be combined. Even though prior
techniques reduce the communication load significantly, they require a number
of jobs that grows exponentially in the system parameters. This limitation is
crucial and may diminish the load gains as the algorithm scales. We propose a
new scheme which achieves the same load as the state-of-the-art while ensuring
that the number of jobs as well as the number of subfiles that the data set
needs to be split into remain small.

    

### [[1904.00326] Medication Recommendation and Lab Test Imputation via Graph Convolutional Networks](http://arxiv.org/abs/1904.00326)


  Laboratory testing and medication prescription are two of the most important
routines in daily clinical practice. Developing an artificial intelligence
system that can automatically make lab test imputations and medication
recommendations can save costs on potentially redundant lab tests and inform
physicians of a more effective prescription. We present an intelligent medical
system (named MedGCN) that can automatically recommend the patients'
medications based on their incomplete lab tests, and can even accurately
estimate the lab values that have not been taken. In our system, we integrate
the complex relations between multiple types of medical entities with their
inherent features in a heterogeneous graph. Then we model the graph to learn a
distributed representation for each entity in the graph based on graph
convolutional networks (GCN). By the propagation of graph convolutional
networks, the entity representations can incorporate multiple types of medical
information that can benefit multiple medical tasks. Moreover, we introduce a
cross regularization strategy to reduce overfitting for multi-task training by
the interaction between the multiple tasks. In this study, we construct a graph
to associate 4 types of medical entities, i.e., patients, encounters, lab
tests, and medications, and applied a graph neural network to learn node
embeddings for medication recommendation and lab test imputation. we validate
our MedGCN model on two real-world datasets: NMEDW and MIMIC-III. The
experimental results on both datasets demonstrate that our model can outperform
the state-of-the-art in both tasks. We believe that our innovative system can
provide a promising and reliable way to assist physicians to make medication
prescriptions and to save costs on potentially redundant lab tests.

    

### [[1906.00066] Optimized Score Transformation for Consistent Fair Classification](http://arxiv.org/abs/1906.00066)


  This paper considers fair probabilistic binary classification where the
outputs of primary interest are predicted probabilities, commonly referred to
as scores. We formulate the problem of transforming scores to satisfy fairness
constraints that are linear in conditional means of scores while minimizing a
cross-entropy objective. The formulation can be applied directly to
post-process classifier outputs and we also explore a pre-processing extension,
thus allowing maximum freedom in selecting a classification algorithm. We
derive a closed-form expression for the optimal transformed scores and a convex
optimization problem for the transformation parameters. In the population
limit, the transformed score function is the fairness-constrained minimizer of
cross-entropy with respect to the true conditional probability of the outcome.
In the finite sample setting, we propose a method called FairScoreTransformer
to approach this solution using a combination of standard probabilistic
classifiers and ADMM. We provide several consistency and finite-sample
guarantees for FairScoreTransformer, relating to the transformation parameters
and transformed score function that it obtains. Comprehensive experiments
comparing to 10 existing methods show that FairScoreTransformer has advantages
for score-based metrics such as Brier score and AUC while remaining competitive
for binary label-based metrics such as accuracy.

    

### [[1906.03563] Adversarial Attack Generation Empowered by Min-Max Optimization](http://arxiv.org/abs/1906.03563)


  The worst-case training principle that minimizes the maximal adversarial
loss, also known as adversarial training (AT), has shown to be a
state-of-the-art approach for enhancing adversarial robustness. Nevertheless,
min-max optimization beyond the purpose of AT has not been rigorously explored
in the adversarial context. In this paper, we show how a general framework of
min-max optimization over multiple domains can be leveraged to advance the
design of different types of adversarial attacks. In particular, given a set of
risk sources, minimizing the worst-case attack loss can be reformulated as a
min-max problem by introducing domain weights that are maximized over the
probability simplex of the domain set. We showcase this unified framework in
three attack generation problems -- attacking model ensembles, devising
universal perturbation under multiple inputs, and crafting attacks resilient to
data transformations. Extensive experiments demonstrate that our approach leads
to substantial attack improvement over the existing heuristic strategies as
well as robustness improvement over state-of-the-art defense methods trained to
be robust against multiple perturbation types. Furthermore, we find that the
self-adjusted domain weights learned from our min-max framework can provide a
holistic tool to explain the difficulty level of attack across domains. Code is
available at this https URL.

    

### [[1911.11855] Asymmetric Correntropy for Robust Adaptive Filtering](http://arxiv.org/abs/1911.11855)


  In recent years, correntropy has been seccessfully applied to robust adaptive
filtering to eliminate adverse effects of impulsive noises or outliers.
Correntropy is generally defined as the expectation of a Gaussian kernel
between two random variables. This definition is reasonable when the error
between the two random variables is symmetrically distributed around zero. For
the case of asymmetric error distribution, the symmetric Gaussian kernel is
however inappropriate and cannot adapt to the error distribution well. To
address this problem, in this brief we propose a new variant of correntropy,
named asymmetric correntropy, which uses an asymmetric Gaussian model as the
kernel function. In addition, a robust adaptive filtering algorithm based on
asymmetric correntropy is developed and its steady-state convergence
performance is analyzed. Simulations are provided to confirm the theoretical
results and good performance of the proposed algorithm.

    

### [[1912.02906] Scalable Reinforcement Learning for Multi-Agent Networked Systems](http://arxiv.org/abs/1912.02906)


  We study reinforcement learning (RL) in a setting with a network of agents
whose states and actions interact in a local manner where the objective is to
find localized policies such that the (discounted) global reward is maximized.
A fundamental challenge in this setting is that the state-action space size
scales exponentially in the number of agents, rendering the problem intractable
for large networks. In this paper, we propose a Scalable Actor Critic (SAC)
framework that exploits the network structure and finds a localized policy that
is an $O(\rho^{\kappa})$-approximation of a stationary point of the objective
for some $\rho\in(0,1)$, with complexity that scales with the local
state-action space size of the largest $\kappa$-hop neighborhood of the
network. We illustrate our model and approach using examples from wireless
communication, epidemics and traffic.

    

### [[1912.08927] Hyperbolic Multiplex Network Embedding with Maps of Random Walk](http://arxiv.org/abs/1912.08927)


  Recent research on network embedding in hyperbolic space have proven
successful in several applications. However, nodes in real world networks tend
to interact through several distinct channels. Simple aggregation or ignorance
of this multiplexity will lead to misleading results. On the other hand, there
exists redundant information between different interaction patterns between
nodes. Recent research reveals the analogy between the community structure and
the hyperbolic coordinate. To learn each node's effective embedding
representation while reducing the redundancy of multiplex network, we then
propose a unified framework combing multiplex network hyperbolic embedding and
multiplex community detection. The intuitive rationale is that high order node
embedding approach is expected to alleviate the observed network's sparse and
noisy structure which will benefit the community detection task. On the
contrary, the improved community structure will also guide the node embedding
task. To incorporate the common features between channels while preserving
unique features, a random walk approach which traversing in latent multiplex
hyperbolic space is proposed to detect the community across channels and bridge
the connection between node embedding and community detection. The proposed
framework is evaluated on several network tasks using different real world
dataset. The results demonstrates that our framework is effective and
efficiency compared with state-of-the-art approaches.

    

### [[2002.05226] Factorization of the Partial Covariance in Singly-Connected Path Diagrams](http://arxiv.org/abs/2002.05226)


  We extend path analysis by showing that, for a singly-connected path diagram,
the partial covariance of two random variables factorizes over the nodes and
edges in the path between the variables. This result allows us to show that
Simpson's paradox cannot occur in singly-connected path diagrams.

    

### [[2002.09565] Adversarial Attacks on Machine Learning Systems for High-Frequency Trading](http://arxiv.org/abs/2002.09565)


  Algorithmic trading systems are often completely automated, and deep learning
is increasingly receiving attention in this domain. Nonetheless, little is
known about the robustness properties of these models. We study valuation
models for algorithmic trading from the perspective of adversarial machine
learning. We introduce new attacks specific to this domain with size
constraints that minimize attack costs. We further discuss how these attacks
can be used as an analysis tool to study and evaluate the robustness properties
of financial models. Finally, we investigate the feasibility of realistic
adversarial attacks in which an adversarial trader fools automated trading
systems into making inaccurate predictions.

    

### [[2002.10316] Bandit Learning with Delayed Impact of Actions](http://arxiv.org/abs/2002.10316)


  We consider a stochastic multi-armed bandit (MAB) problem with delayed impact
of actions. In our setting, actions taken in the past impact the arm rewards in
the subsequent future. This delayed impact of actions is prevalent in the real
world. For example, the capability to pay back a loan for people in a certain
social group might depend on historically how frequently that group has been
approved loan applications. If banks keep rejecting loan applications to people
in a disadvantaged group, it could create a feedback loop and further damage
the chance of getting loans for people in that group. In this paper, we
formulate this delayed and long-term impact of actions within the context of
multi-armed bandits. We generalize the bandit setting to encode the dependency
of this "bias" due to the action history during learning. The goal is to
maximize the collected utilities over time while taking into account the
dynamics created by the delayed impacts of historical actions. We propose an
algorithm that achieves a regret of $\tilde{\mathcal{O}}(KT^{2/3})$ and show a
matching regret lower bound of $\Omega(KT^{2/3})$, where $K$ is the number of
arms and $T$ is the learning horizon. Our results complement the bandit
literature by adding techniques to deal with actions with long-term impacts and
have implications in designing fair algorithms.

    

### [[2003.02455] PAC-Bayes meta-learning with implicit task-specific posteriors](http://arxiv.org/abs/2003.02455)


  We introduce a new and rigorously-formulated PAC-Bayes meta-learning
algorithm that solves few-shot learning. Our proposed method extends the
PAC-Bayes framework from a single task setting to the meta-learning multiple
task setting to upper-bound the error evaluated on any, even unseen, tasks and
samples. We also propose a generative-based approach to estimate the posterior
of task-specific model parameters more expressively compared to the usual
assumption based on a multivariate normal distribution with a diagonal
covariance matrix. We show that the models trained with our proposed
meta-learning algorithm are well calibrated and accurate, with state-of-the-art
calibration and classification results on few-shot classification
(mini-ImageNet and tiered-ImageNet) and regression (multi-modal
task-distribution regression) benchmarks.

    

### [[2003.05328] ENSEI: Efficient Secure Inference via Frequency-Domain Homomorphic Convolution for Privacy-Preserving Visual Recognition](http://arxiv.org/abs/2003.05328)


  In this work, we propose ENSEI, a secure inference (SI) framework based on
the frequency-domain secure convolution (FDSC) protocol for the efficient
execution of privacy-preserving visual recognition. Our observation is that,
under the combination of homomorphic encryption and secret sharing, homomorphic
convolution can be obliviously carried out in the frequency domain,
significantly simplifying the related computations. We provide protocol designs
and parameter derivations for number-theoretic transform (NTT) based FDSC. In
the experiment, we thoroughly study the accuracy-efficiency trade-offs between
time- and frequency-domain homomorphic convolution. With ENSEI, compared to the
best known works, we achieve 5--11x online time reduction, up to 33x setup time
reduction, and up to 10x reduction in the overall inference time. A further 33%
of bandwidth reductions can be obtained on binary neural networks with only 1%
of accuracy degradation on the CIFAR-10 dataset.

    

### [[2004.03281] Teacher-Class Network: A Neural Network Compression Mechanism](http://arxiv.org/abs/2004.03281)


  To reduce the overwhelming size of Deep Neural Networks (DNN) teacher-student
methodology tries to transfer knowledge from a complex teacher network to a
simple student network. We instead propose a novel method called the
teacher-class network consisting of a single teacher and multiple student
networks (i.e. class of students). Instead of transferring knowledge to one
student only, the proposed method transfers a chunk of knowledge to each
student. Our students are not trained for problem-specific logits, they are
trained to mimic knowledge (dense representation) learned by the teacher
network thus the combined knowledge learned by the class of students can be
used to solve other problems as well. The proposed teacher-class architecture
is evaluated on several benchmark datasets such as MNIST, Fashion MNIST, IMDB
Movie Reviews, CAMVid, CIFAR-10 and ImageNet on multiple tasks including image
classification, sentiment classification and segmentation. Our approach
outperforms the state of-the-art single student approach in terms of accuracy
as well as computational cost while achieving 10-30 times reduction in
parameters.

    

### [[2004.07049] Boosting algorithms in energy research: A systematic review](http://arxiv.org/abs/2004.07049)


  Machine learning algorithms have been extensively exploited in energy
research, due to their flexibility, automation and ability to handle big data.
Among the most prominent machine learning algorithms are the boosting ones,
which are known to be "garnering wisdom from a council of fools", thereby
transforming weak learners to strong learners. Boosting algorithms are
characterized by both high flexibility and high interpretability. The latter
property is the result of recent developments by the statistical community. In
this work, we provide understanding on the properties of boosting algorithms to
facilitate a better exploitation of their strengths in energy research. In this
respect, (a) we summarize recent advances on boosting algorithms, (b) we review
relevant applications in energy research with those focusing on renewable
energy (in particular those focusing on wind energy and solar energy)
consisting a significant portion of the total ones, and (c) we describe how
boosting algorithms are implemented and how their use is related to their
properties. We show that boosting has been underexploited so far, while great
advances in the energy field are possible both in terms of explanation and
interpretation, and in terms of predictive performance.

    

### [[2005.04176] In Pursuit of Interpretable, Fair and Accurate Machine Learning for Criminal Recidivism Prediction](http://arxiv.org/abs/2005.04176)


  Objectives: We study interpretable recidivism prediction using machine
learning (ML) models and analyze performance in terms of prediction ability,
sparsity, and fairness. Unlike previous works, this study trains interpretable
models that output probabilities rather than binary predictions, and uses
quantitative fairness definitions to assess the models. This study also
examines whether models can generalize across geographic locations. Methods: We
generated black-box and interpretable ML models on two different criminal
recidivism datasets from Florida and Kentucky. We compared predictive
performance and fairness of these models against two methods that are currently
used in the justice system to predict pretrial recidivism: the Arnold PSA and
COMPAS. We evaluated predictive performance of all models on predicting six
different types of crime over two time spans. Results: Several interpretable ML
models can predict recidivism as well as black-box ML models and are more
accurate than COMPAS or the Arnold PSA. These models are potentially useful in
practice. Similar to the Arnold PSA, some of these interpretable models can be
written down as a simple table. Others can be displayed using a set of
visualizations. Our geographic analysis indicates that ML models should be
trained separately for separate locations and updated over time. We also
present a fairness analysis for the interpretable models. Conclusions:
Interpretable machine learning models can perform just as well as
non-interpretable methods and currently-used risk assessment scales, in terms
of both prediction accuracy and fairness. Machine learning models might be more
accurate when trained separately for distinct locations and kept up-to-date.

    

### [[2005.04692] Topological regularization with information filtering networks](http://arxiv.org/abs/2005.04692)


  A methodology to perform topological regularization via information filtering
network is introduced. This methodology can be directly applied to covariance
selection problem providing an instrument for sparse probabilistic modeling
with both linear and non-linear multivariate probability distributions such as
the elliptical and generalized hyperbolic families. It can also be directly
implemented for $L_0$-norm regularized multicollinear regression. In this
paper, I describe in detail an application to sparse modeling with multivariate
Student-t. A specific $L_0$-norm regularized expectation-maximization
likelihood maximization procedure is proposed for this sparse Student-t case.
Examples with real data from stock prices log-returns and from artificially
generated data demonstrate the applicability, performances, and potentials of
this methodology.

    

### [[2006.09092] Learning Rates as a Function of Batch Size: A Random Matrix Theory Approach to Neural Network Training](http://arxiv.org/abs/2006.09092)


  We study the effect of mini-batching on the loss landscape of deep neural
networks using spiked, field-dependent random matrix theory. We demonstrate
that the magnitude of the extremal values of the batch Hessian are larger than
those of the empirical Hessian. We also derive similar results for the
Generalised Gauss-Newton matrix approximation of the Hessian. As a consequence
of our theorems we derive an analytical expressions for the maximal learning
rates as a function of batch size, informing practical training regimens for
both stochastic gradient descent (linear scaling) and adaptive algorithms, such
as Adam (square root scaling), for smooth, non-convex deep neural networks.
Whilst the linear scaling for stochastic gradient descent has been derived
under more restrictive conditions, which we generalise, the square root scaling
rule for adaptive optimisers is, to our knowledge, completely novel. %For
stochastic second-order methods and adaptive methods, we derive that the
minimal damping coefficient is proportional to the ratio of the learning rate
to batch size. We validate our claims on the VGG/WideResNet architectures on
the CIFAR-$100$ and ImageNet datasets. Based on our investigations of the
sub-sampled Hessian we develop a stochastic Lanczos quadrature based on the fly
learning rate and momentum learner, which avoids the need for expensive
multiple evaluations for these key hyper-parameters and shows good preliminary
results on the Pre-Residual Architecure for CIFAR-$100$.

    

### [[2006.12469] Attention-based Quantum Tomography](http://arxiv.org/abs/2006.12469)


  With rapid progress across platforms for quantum systems, the problem of
many-body quantum state reconstruction for noisy quantum states becomes an
important challenge. Recent works found promise in recasting the problem of
quantum state reconstruction to learning the probability distribution of
quantum state measurement vectors using generative neural network models. Here
we propose the "Attention-based Quantum Tomography" (AQT), a quantum state
reconstruction using an attention mechanism-based generative network that
learns the mixed state density matrix of a noisy quantum state. The AQT is
based on the model proposed in "Attention is all you need" by Vishwani et al
(2017) that is designed to learn long-range correlations in natural language
sentences and thereby outperform previous natural language processing models.
We demonstrate not only that AQT outperforms earlier neural-network-based
quantum state reconstruction on identical tasks but that AQT can accurately
reconstruct the density matrix associated with a noisy quantum state
experimentally realized in an IBMQ quantum computer. We speculate the success
of the AQT stems from its ability to model quantum entanglement across the
entire quantum system much as the attention model for natural language
processing captures the correlations among words in a sentence.

    

### [[2006.12837] SWAG: A Wrapper Method for Sparse Learning](http://arxiv.org/abs/2006.12837)


  The majority of machine learning methods and algorithms give high priority to
prediction performance which may not always correspond to the priority of the
users. In many cases, practitioners and researchers in different fields, going
from engineering to genetics, require interpretability and replicability of the
results especially in settings where, for example, not all attributes may be
available to them. As a consequence, there is the need to make the outputs of
machine learning algorithms more interpretable and to deliver a library of
"equivalent" learners (in terms of prediction performance) that users can
select based on attribute availability in order to test and/or make use of
these learners for predictive/diagnostic purposes. To address these needs, we
propose to study a procedure that combines screening and wrapper approaches
which, based on a user-specified learning method, greedily explores the
attribute space to find a library of sparse learners with consequent low data
collection and storage costs. This new method (i) delivers a low-dimensional
network of attributes that can be easily interpreted and (ii) increases the
potential replicability of results based on the diversity of attribute
combinations defining strong learners with equivalent predictive power. We call
this algorithm "Sparse Wrapper AlGorithm" (SWAG).

    

### [[2006.13365] Bringing Light Into the Dark: A Large-scale Evaluation of Knowledge Graph Embedding Models Under a Unified Framework](http://arxiv.org/abs/2006.13365)


  The heterogeneity in recently published knowledge graph embedding models'
implementations, training, and evaluation has made fair and thorough
comparisons difficult. In order to assess the reproducibility of previously
published results, we re-implemented and evaluated 21 interaction models in the
PyKEEN software package. Here, we outline which results could be reproduced
with their reported hyper-parameters, which could only be reproduced with
alternate hyper-parameters, and which could not be reproduced at all as well as
provide insight as to why this might be the case.
We then performed a large-scale benchmarking on four datasets with several
thousands of experiments and 24,804 GPU hours of computation time. We present
insights gained as to best practices, best configurations for each model, and
where improvements could be made over previously published best configurations.
Our results highlight that the combination of model architecture, training
approach, loss function, and the explicit modeling of inverse relations is
crucial for a model's performances, and not only determined by the model
architecture. We provide evidence that several architectures can obtain results
competitive to the state-of-the-art when configured carefully. We have made all
code, experimental configurations, results, and analyses that lead to our
interpretations available at this https URL and
this https URL


### [[2006.14061] Pareto Active Learning with Gaussian Processes and Adaptive Discretization](http://arxiv.org/abs/2006.14061)


  We consider the problem of optimizing a vector-valued objective function
$\boldsymbol{f}$ sampled from a Gaussian Process (GP) whose index set is a
well-behaved, compact metric space $({\cal X},d)$ of designs. We assume that
$\boldsymbol{f}$ is not known beforehand and that evaluating $\boldsymbol{f}$
at design $x$ results in a noisy observation of $\boldsymbol{f}(x)$. Since
identifying the Pareto optimal designs via exhaustive search is infeasible when
the cardinality of ${\cal X}$ is large, we propose an algorithm, called
Adaptive $\boldsymbol{\epsilon}$-PAL, that exploits the smoothness of the
GP-sampled function and the structure of $({\cal X},d)$ to learn fast. In
essence, Adaptive $\boldsymbol{\epsilon}$-PAL employs a tree-based adaptive
discretization technique to identify an $\boldsymbol{\epsilon}$-accurate Pareto
set of designs in as few evaluations as possible. We provide both
information-type and metric dimension-type bounds on the sample complexity of
$\boldsymbol{\epsilon}$-accurate Pareto set identification. We also
experimentally show that our algorithm outperforms other Pareto set
identification methods on several benchmark datasets.

    

### [[2007.01285] Deep Learning for Neuroimaging-based Diagnosis and Rehabilitation of Autism Spectrum Disorder: A Review](http://arxiv.org/abs/2007.01285)


  Accurate diagnosis of Autism Spectrum Disorder (ASD) followed by effective
rehabilitation is essential for the management of this disorder. Artificial
intelligence (AI) techniques can aid physicians to apply automatic diagnosis
and rehabilitation procedures. AI techniques comprise traditional machine
learning (ML) approaches and deep learning (DL) techniques. Conventional ML
methods employ various feature extraction and classification techniques, but in
DL, the process of feature extraction and classification is accomplished
intelligently and integrally. DL methods for diagnosis of ASD have been focused
on neuroimaging-based approaches. Neuroimaging techniques are non-invasive
disease markers potentially useful for ASD diagnosis. Structural and functional
neuroimaging techniques provide physicians substantial information about the
structure (anatomy and structural connectivity) and function (activity and
functional connectivity) of the brain. Due to the intricate structure and
function of the brain, proposing optimum procedures for ASD diagnosis with
neuroimaging data without exploiting powerful AI techniques like DL may be
challenging. In this paper, studies conducted with the aid of DL networks to
distinguish ASD are investigated. Rehabilitation tools provided for supporting
ASD patients utilizing DL networks are also assessed. Finally, we will present
important challenges in the automated detection and rehabilitation of ASD and
propose some future works.

    

### [[2007.08792] Uncertainty Quantification and Deep Ensembles](http://arxiv.org/abs/2007.08792)


  Deep Learning methods are known to suffer from calibration issues: they
typically produce over-confident estimates. These problems are exacerbated in
the low data regime. Although the calibration of probabilistic models is well
studied, calibrating extremely over-parametrized models in the low-data regime
presents unique challenges. We show that deep-ensembles do not necessarily lead
to improved calibration properties. In fact, we show that standard ensembling
methods, when used in conjunction with modern techniques such as mixup
regularization, can lead to less calibrated models. This text examines the
interplay between three of the most simple and commonly used approaches to
leverage deep learning when data is scarce: data-augmentation, ensembling, and
post-processing calibration methods. Although standard ensembling techniques
certainly help boost accuracy, we demonstrate that the calibration of deep
ensembles relies on subtle trade-offs. We also find that calibration methods
such as temperature scaling need to be slightly tweaked when used with
deep-ensembles and, crucially, need to be executed after the averaging process.
Our simulations indicate that this simple strategy can halve the Expected
Calibration Error (ECE) on a range of benchmark classification problems
compared to standard deep-ensembles in the low data regime.

    

### [[2008.00511] Curriculum Learning with a Progression Function](http://arxiv.org/abs/2008.00511)


  Curriculum Learning for Reinforcement Learning is an increasingly popular
technique that involves training an agent on a sequence of intermediate tasks,
called a Curriculum, to increase the agent's performance and learning speed.
This paper introduces a novel paradigm for curriculum generation based on
progression and mapping functions. While progression functions specify the
complexity of the environment at any given time, mapping functions generate
environments of a specific complexity. Different progression functions are
introduced, including an autonomous online task progression based on the
agent's performance. Our approach's benefits and wide applicability are shown
by empirically comparing its performance to two state-of-the-art Curriculum
Learning algorithms on six domains.

    

### [[2009.08327] Berrut Approximated Coded Computing: Straggler Resistance Beyond Polynomial Computing](http://arxiv.org/abs/2009.08327)


  One of the major challenges in using distributed learning to train
complicated models with large data sets is to deal with stragglers effect. As a
solution, coded computation has been recently proposed to efficiently add
redundancy to the computation tasks. In this technique, coding is used across
data sets, and computation is done over coded data, such that the results of an
arbitrary subset of worker nodes with a certain size are enough to recover the
final results. The major challenges with those approaches are (1) they are
limited to polynomial function computations, (2) the size of the subset of
servers that we need to wait for grows with the multiplication of the size of
the data set and the model complexity (the degree of the polynomial), which can
be prohibitively large, (3) they are not numerically stable for computation
over real numbers. In this paper, we propose Berrut Approximated Coded
Computing (BACC), as an alternative approach, which is not limited to
polynomial function computation. In addition, the master node can approximately
calculate the final results, using the outcomes of any arbitrary subset of
available worker nodes. The approximation approach is proven to be numerically
stable with low computational complexity. In addition, the accuracy of the
approximation is established theoretically and verified by simulation results
in different settings such as distributed learning problems. In particular,
BACC is used to train a deep neural network on a cluster of servers, which
outperforms repetitive computation (repetition coding) in terms of the rate of
convergence.

    

### [[2009.08965] Encoding Robustness to Image Style via Adversarial Feature Perturbations](http://arxiv.org/abs/2009.08965)


  Adversarial training is the industry standard for producing models that are
robust to small adversarial perturbations. However, machine learning
practitioners need models that are robust to other kinds of changes that occur
naturally, such as changes in the style or illumination of input images. Such
changes in input distribution have been effectively modeled as shifts in the
mean and variance of deep image features. We adapt adversarial training by
directly perturbing feature statistics, rather than image pixels, to produce
models that are robust to various unseen distributional shifts. We explore the
relationship between these perturbations and distributional shifts by
visualizing adversarial features. Our proposed method, Adversarial Batch
Normalization (AdvBN), is a single network layer that generates worst-case
feature perturbations during training. By fine-tuning neural networks on
adversarial feature distributions, we observe improved robustness of networks
to various unseen distributional shifts, including style variations and image
corruptions. In addition, we show that our proposed adversarial feature
perturbation can be complementary to existing image space data augmentation
methods, leading to improved performance. The source code and pre-trained
models are released at \url{this https URL}.

    

### [[2010.02745] Image Translation for Medical Image Generation -- Ischemic Stroke Lesions](http://arxiv.org/abs/2010.02745)


  Deep learning based disease detection and segmentation algorithms promise to
improve many clinical processes. However, such algorithms require vast amounts
of annotated training data, which are typically not available in the medical
context due to data privacy, legal obstructions, and non-uniform data
acquisition protocols. Synthetic databases with annotated pathologies could
provide the required amounts of training data. We demonstrate with the example
of ischemic stroke that an improvement in lesion segmentation is feasible using
deep learning based augmentation. To this end, we train different
image-to-image translation models to synthesize magnetic resonance images of
brain volumes with and without stroke lesions from semantic segmentation maps.
In addition, we train a generative adversarial network to generate synthetic
lesion masks. Subsequently, we combine these two components to build a large
database of synthetic stroke images. The performance of the various models is
evaluated using a U-Net which is trained to segment stroke lesions on a
clinical test set. We report a Dice score of $\mathbf{72.8}$%
[$\mathbf{70.8\pm1.0}$%] for the model with the best performance, which
outperforms the model trained on the clinical images alone $\mathbf{67.3}$%
[$\mathbf{63.2\pm1.9}$%], and is close to the human inter-reader Dice score of
$\mathbf{76.9}$%. Moreover, we show that for a small database of only 10 or 50
clinical cases, synthetic data augmentation yields significant improvement
compared to a setting where no synthetic data is used. To the best of our
knowledge, this presents the first comparative analysis of synthetic data
augmentation based on image-to-image translation, and first application to
ischemic stroke.

    

### [[2010.02917] NCP-VAE: Variational Autoencoders with Noise Contrastive Priors](http://arxiv.org/abs/2010.02917)


  Variational autoencoders (VAEs) are one of the powerful likelihood-based
generative models with applications in various domains. However, they struggle
to generate high-quality images, especially when samples are obtained from the
prior without any tempering. One explanation for VAEs' poor generative quality
is the prior hole problem: the prior distribution fails to match the aggregate
approximate posterior. Due to this mismatch, there exist areas in the latent
space with high density under the prior that do not correspond to any encoded
image. Samples from those areas are decoded to corrupted images. To tackle this
issue, we propose an energy-based prior defined by the product of a base prior
distribution and a reweighting factor, designed to bring the base closer to the
aggregate posterior. We train the reweighting factor by noise contrastive
estimation, and we generalize it to hierarchical VAEs with many latent variable
groups. Our experiments confirm that the proposed noise contrastive priors
improve the generative performance of state-of-the-art VAEs by a large margin
on the MNIST, CIFAR-10, CelebA 64, and CelebA HQ 256 datasets.

    

### [[2010.09670] RobustBench: a standardized adversarial robustness benchmark](http://arxiv.org/abs/2010.09670)


  As a research community, we are still lacking a systematic understanding of
the progress on adversarial robustness which often makes it hard to identify
the most promising ideas in training robust models. A key challenge in
benchmarking robustness is that its evaluation is often error-prone leading to
robustness overestimation. Our goal is to establish a standardized benchmark of
adversarial robustness, which as accurately as possible reflects the robustness
of the considered models within a reasonable computational budget. To this end,
we start by considering the image classification task and introduce
restrictions (possibly loosened in the future) on the allowed models. We
evaluate adversarial robustness with AutoAttack, an ensemble of white- and
black-box attacks, which was recently shown in a large-scale study to improve
almost all robustness evaluations compared to the original publications. To
prevent overadaptation of new defenses to AutoAttack, we welcome external
evaluations based on adaptive attacks, especially where AutoAttack flags a
potential overestimation of robustness. Our leaderboard, hosted at
this https URL, contains evaluations of 120+ models and aims at
reflecting the current state of the art in image classification on a set of
well-defined tasks in $\ell_\infty$- and $\ell_2$-threat models and on common
corruptions, with possible extensions in the future. Additionally, we
open-source the library this https URL that
provides unified access to 80+ robust models to facilitate their downstream
applications. Finally, based on the collected models, we analyze the impact of
robustness on the performance on distribution shifts, calibration,
out-of-distribution detection, fairness, privacy leakage, smoothness, and
transferability.

    

### [[2010.11375] Deep Learning for Distinguishing Normal versus Abnormal Chest Radiographs and Generalization to Unseen Diseases](http://arxiv.org/abs/2010.11375)


  Chest radiography (CXR) is the most widely-used thoracic clinical imaging
modality and is crucial for guiding the management of cardiothoracic
conditions. The detection of specific CXR findings has been the main focus of
several artificial intelligence (AI) systems. However, the wide range of
possible CXR abnormalities makes it impractical to build specific systems to
detect every possible condition. In this work, we developed and evaluated an AI
system to classify CXRs as normal or abnormal. For development, we used a
de-identified dataset of 248,445 patients from a multi-city hospital network in
India. To assess generalizability, we evaluated our system using 6
international datasets from India, China, and the United States. Of these
datasets, 4 focused on diseases that the AI was not trained to detect: 2
datasets with tuberculosis and 2 datasets with coronavirus disease 2019. Our
results suggest that the AI system generalizes to new patient populations and
abnormalities. In a simulated workflow where the AI system prioritized abnormal
cases, the turnaround time for abnormal cases reduced by 7-28%. These results
represent an important step towards evaluating whether AI can be safely used to
flag cases in a general setting where previously unseen abnormalities exist.

    

### [[2011.00416] Deep Learning for Text Style Transfer: A Survey](http://arxiv.org/abs/2011.00416)


  Text style transfer is an important task in natural language generation,
which aims to control certain attributes in the generated text, such as
politeness, emotion, humor, and many others. It has a long history in the field
of natural language processing, and recently has re-gained significant
attention thanks to the promising performance brought by deep neural models. In
this paper, we present a systematic survey of the research on neural text style
transfer, spanning over 100 representative articles since the first neural text
style transfer work in 2017. We discuss the task formulation, existing datasets
and subtasks, evaluation, as well as the rich methodologies in the presence of
parallel and non-parallel data. We also provide discussions on a variety of
important topics regarding the future development of this task. Our curated
paper list is at this https URL


### [[2011.01681] Learning Causal Semantic Representation for Out-of-Distribution Prediction](http://arxiv.org/abs/2011.01681)


  Conventional supervised learning methods, especially deep ones, are found to
be sensitive to out-of-distribution (OOD) examples, largely because the learned
representation mixes the semantic factor with the variation factor due to their
domain-specific correlation, while only the semantic factor causes the output.
To address the problem, we propose a Causal Semantic Generative model (CSG)
based on a causal reasoning so that the two factors are modeled separately, and
develop methods for OOD prediction from a single training domain, which is
common and challenging. The methods are based on the causal invariance
principle, with a novel design in variational Bayes for both efficient learning
and easy prediction. Theoretically, we prove that under certain conditions, CSG
can identify the semantic factor by fitting training data, and this
semantic-identification guarantees the boundedness of OOD generalization error
and the success of adaptation. Empirical study shows improved OOD performance
over prevailing baselines.

    

### [[2011.02966] Absence of Barren Plateaus in Quantum Convolutional Neural Networks](http://arxiv.org/abs/2011.02966)


  Quantum neural networks (QNNs) have generated excitement around the
possibility of efficiently analyzing quantum data. But this excitement has been
tempered by the existence of exponentially vanishing gradients, known as barren
plateau landscapes, for many QNN architectures. Recently, Quantum Convolutional
Neural Networks (QCNNs) have been proposed, involving a sequence of
convolutional and pooling layers that reduce the number of qubits while
preserving information about relevant data features. In this work we rigorously
analyze the gradient scaling for the parameters in the QCNN architecture. We
find that the variance of the gradient vanishes no faster than polynomially,
implying that QCNNs do not exhibit barren plateaus. This provides an analytical
guarantee for the trainability of randomly initialized QCNNs, which highlights
QCNNs as being trainable under random initialization unlike many other QNN
architectures. To derive our results we introduce a novel graph-based method to
analyze expectation values over Haar-distributed unitaries, which will likely
be useful in other contexts. Finally, we perform numerical simulations to
verify our analytical results.

    

### [[2011.09719] Adversarial Examples for $k$-Nearest Neighbor Classifiers Based on Higher-Order Voronoi Diagrams](http://arxiv.org/abs/2011.09719)


  Adversarial examples are a widely studied phenomenon in machine learning
models. While most of the attention has been focused on neural networks, other
practical models also suffer from this issue. In this work, we propose an
algorithm for evaluating the adversarial robustness of $k$-nearest neighbor
classification, i.e., finding a minimum-norm adversarial example. Diverging
from previous proposals, we take a geometric approach by performing a search
that expands outwards from a given input point. On a high level, the search
radius expands to the nearby Voronoi cells until we find a cell that classifies
differently from the input point. To scale the algorithm to a large $k$, we
introduce approximation steps that find perturbations with smaller norm,
compared to the baselines, in a variety of datasets. Furthermore, we analyze
the structural properties of a dataset where our approach outperforms the
competition.

    

### [[2011.13311] Data-Efficient Classification of Radio Galaxies](http://arxiv.org/abs/2011.13311)


  The continuum emission from radio galaxies can be generally classified into
different morphological classes such as FRI, FRII, Bent, or Compact. In this
paper, we explore the task of radio galaxy classification based on morphology
using deep learning methods with a focus on using a small scale dataset ($\sim
2000$ samples). We apply few-shot learning techniques based on Twin Networks
and transfer learning techniques using a pre-trained DenseNet model with
advanced techniques like cyclical learning rate and discriminative learning to
train the model rapidly. We achieve a classification accuracy of over 92\%
using our best performing model with the biggest source of confusion being
between Bent and FRII type galaxies. Our results show that focusing on a small
but curated dataset along with the use of best practices to train the neural
network can lead to good results. Automated classification techniques will be
crucial for upcoming surveys with next generation radio telescopes which are
expected to detect hundreds of thousands of new radio galaxies in the near
future.

    

### [[2012.07962] Iterative label cleaning for transductive and semi-supervised few-shot learning](http://arxiv.org/abs/2012.07962)


  Few-shot learning amounts to learning representations and acquiring knowledge
such that novel tasks may be solved with both supervision and data being
limited. Improved performance is possible by transductive inference, where the
entire test set is available concurrently, and semi-supervised learning, where
more unlabeled data is available. Focusing on these two settings, we introduce
a new algorithm that leverages the manifold structure of the labeled and
unlabeled data distribution to predict pseudo-labels, while balancing over
classes and using the loss value distribution of a limited-capacity classifier
to select the cleanest labels, iteratively improving the quality of
pseudo-labels. Our solution surpasses or matches the state of the art results
on four benchmark datasets, namely miniImageNet, tieredImageNet, CUB and
CIFAR-FS, while being robust over feature space pre-processing and the quantity
of available data. The publicly available source code can be found in
this https URL.

    

### [[2012.08073] Chernoff Sampling for Active Testing and Extension to Active Regression](http://arxiv.org/abs/2012.08073)


  Active learning can reduce the number of samples needed to perform a
hypothesis test and to estimate the parameters of a model. In this paper, we
revisit the work of Chernoff that described an asymptotically optimal algorithm
for performing a hypothesis test. We obtain a novel sample complexity bound for
Chernoff's algorithm, with a non-asymptotic term that characterizes its
performance at a fixed confidence level. We also develop an extension of
Chernoff sampling that can be used to estimate the parameters of a wide variety
of models and we obtain a non-asymptotic bound on the estimation error. We
apply our extension of Chernoff sampling to actively learn neural network
models and to estimate parameters in real-data linear and non-linear regression
problems, where our approach performs favorably to state-of-the-art methods.

    

### [[2012.08868] Using Spatio-temporal Deep Learning for Forecasting Demand and Supply-demand Gap in Ride-hailing System with Anonymized Spatial Adjacency Information](http://arxiv.org/abs/2012.08868)


  To reduce passenger waiting time and driver search friction, ride-hailing
companies need to accurately forecast spatio-temporal demand and supply-demand
gap. However, due to spatio-temporal dependencies pertaining to demand and
supply-demand gap in a ride-hailing system, making accurate forecasts for both
demand and supply-demand gap is a difficult task. Furthermore, due to
confidentiality and privacy issues, ride-hailing data are sometimes released to
the researchers by removing spatial adjacency information of the zones, which
hinders the detection of spatio-temporal dependencies. To that end, a novel
spatio-temporal deep learning architecture is proposed in this paper for
forecasting demand and supply-demand gap in a ride-hailing system with
anonymized spatial adjacency information, which integrates feature importance
layer with a spatio-temporal deep learning architecture containing
one-dimensional convolutional neural network (CNN) and zone-distributed
independently recurrent neural network (IndRNN). The developed architecture is
tested with real-world datasets of Didi Chuxing, which shows that our models
based on the proposed architecture can outperform conventional time-series
models (e.g., ARIMA) and machine learning models (e.g., gradient boosting
machine, distributed random forest, generalized linear model, artificial neural
network). Additionally, the feature importance layer provides an interpretation
of the model by revealing the contribution of the input features utilized in
prediction.

    

### [[2101.02307] Directed mixed membership stochastic blockmodel](http://arxiv.org/abs/2101.02307)


  Mixed membership problem for undirected network has been well studied in
network analysis recent years. However, the more general case of mixed
membership for directed network remains a challenge. Here, we propose an
interpretable and identifiable model: directed mixed membership stochastic
blockmodel (DiMMSB for short) for directed mixed membership networks. DiMMSB
allows that row nodes and column nodes of the adjacency matrix can be different
and these nodes may have distinct community structure in a directed network. We
also develop an efficient spectral algorithm called DiSP designed based on
simplex structures inherent in the left and right singular vectors of the
population adjacency matrix to estimate the mixed memberships for both row
nodes and column nodes in a directed network. We show that DiSP is
asymptotically consistent under mild conditions by providing error bounds for
the inferred membership vectors of each row node and each column node using
delicate spectral analysis. We demonstrate the advantages of DiSP with
applications to simulated directed mixed membership network, the directed
Political blogs network and the Papers Citation network.

    

### [[2102.02976] Analyzing the Generalization Capability of SGLD Using Properties of Gaussian Channels](http://arxiv.org/abs/2102.02976)


  Optimization is a key component for training machine learning models and has
a strong impact on their generalization. In this paper, we consider a
particular optimization method -- the stochastic gradient Langevin dynamics
(SGLD) algorithm -- and investigate the generalization of models trained by
SGLD. We derive a new generalization bound by connecting SGLD with Gaussian
channels found in information and communication theory. Our bound can be
computed from the training data and incorporates the variance of gradients for
quantifying a particular kind of "sharpness" of the loss landscape. We also
consider a closely related algorithm with SGLD, namely differentially private
SGD (DP-SGD). We prove that the generalization capability of DP-SGD can be
amplified by iteration. Specifically, our bound can be sharpened by including a
time-decaying factor if the DP-SGD algorithm outputs the last iterate while
keeping other iterates hidden. This decay factor enables the contribution of
early iterations to our bound to reduce with time and is established by strong
data processing inequalities -- a fundamental tool in information theory. We
demonstrate our bound through numerical experiments, showing that it can
predict the behavior of the true generalization gap.

    

### [[2102.03782] Using Gaussian Processes to Design Dynamic Experiments for Black-Box Model Discrimination under Uncertainty](http://arxiv.org/abs/2102.03782)


  Diverse domains of science and engineering use parameterised mechanistic
models. Engineers and scientists can often hypothesise several rival models to
explain a specific process or phenomenon. Consider a model discrimination
setting where we wish to find the best mechanistic, dynamic model candidate and
the best model parameter estimates. Typically, several rival mechanistic models
can explain the available data, so design of dynamic experiments for model
discrimination helps optimally collect additional data by finding experimental
settings that maximise model prediction divergence. We argue there are two main
approaches in the literature for solving the optimal design problem: (i) the
analytical approach, using linear and Gaussian approximations to find
closed-form expressions for the design objective, and (ii) the data-driven
approach, which often relies on computationally intensive Monte Carlo
techniques. Olofsson et al. (ICML 35, 2018) introduced Gaussian process (GP)
surrogate models to hybridise the analytical and data-driven approaches, which
allowed for computationally efficient design of experiments for discriminating
between black-box models. In this study, we demonstrate that we can extend
existing methods for optimal design of dynamic experiments to incorporate a
wider range of problem uncertainty. We also extend the Olofsson et al. (2018)
method of using GP surrogate models for discriminating between dynamic
black-box models. We evaluate our approach on a well-known case study from
literature, and explore the consequences of using GP surrogates to approximate
gradient-based methods.

    

### [[2102.05034] SLAPS: Self-Supervision Improves Structure Learning for Graph Neural Networks](http://arxiv.org/abs/2102.05034)


  Graph neural networks (GNNs) work well when the graph structure is provided.
However, this structure may not always be available in real-world applications.
One solution to this problem is to infer a task-specific latent structure and
then apply a GNN to the inferred graph. Unfortunately, the space of possible
graph structures grows super-exponentially with the number of nodes and so the
task-specific supervision may be insufficient for learning both the structure
and the GNN parameters. In this work, we propose the Simultaneous Learning of
Adjacency and GNN Parameters with Self-supervision, or SLAPS, a method that
provides more supervision for inferring a graph structure through
self-supervision. A comprehensive experimental study demonstrates that SLAPS
scales to large graphs with hundreds of thousands of nodes and outperforms
several models that have been proposed to learn a task-specific graph structure
on established benchmarks.

    

### [[2102.09526] Convex regularization in statistical inverse learning problems](http://arxiv.org/abs/2102.09526)


  We consider a statistical inverse learning problem, where the task is to
estimate a function $f$ based on noisy point evaluations of $Af$, where $A$ is
a linear operator. The function $Af$ is evaluated at i.i.d. random design
points $u_n$, $n=1,...,N$ generated by an unknown general probability
distribution. We consider Tikhonov regularization with general convex and
$p$-homogeneous penalty functionals and derive concentration rates of the
regularized solution to the ground truth measured in the symmetric Bregman
distance induced by the penalty functional. We derive concrete rates for Besov
norm penalties and numerically demonstrate the correspondence with the observed
rates in the context of X-ray tomography.

    

### [[2102.12086] Modern Koopman Theory for Dynamical Systems](http://arxiv.org/abs/2102.12086)


  The field of dynamical systems is being transformed by the mathematical tools
and algorithms emerging from modern computing and data science.
First-principles derivations and asymptotic reductions are giving way to
data-driven approaches that formulate models in operator theoretic or
probabilistic frameworks. Koopman spectral theory has emerged as a dominant
perspective over the past decade, in which nonlinear dynamics are represented
in terms of an infinite-dimensional linear operator acting on the space of all
possible measurement functions of the system. This linear representation of
nonlinear dynamics has tremendous potential to enable the prediction,
estimation, and control of nonlinear systems with standard textbook methods
developed for linear systems. However, obtaining finite-dimensional coordinate
systems and embeddings in which the dynamics appear approximately linear
remains a central open challenge. The success of Koopman analysis is due
primarily to three key factors: 1) there exists rigorous theory connecting it
to classical geometric approaches for dynamical systems, 2) the approach is
formulated in terms of measurements, making it ideal for leveraging big-data
and machine learning techniques, and 3) simple, yet powerful numerical
algorithms, such as the dynamic mode decomposition (DMD), have been developed
and extended to reduce Koopman theory to practice in real-world applications.
In this review, we provide an overview of modern Koopman operator theory,
describing recent theoretical and algorithmic developments and highlighting
these methods with a diverse range of applications. We also discuss key
advances and challenges in the rapidly growing field of machine learning that
are likely to drive future developments and significantly transform the
theoretical landscape of dynamical systems.

    

### [[2103.02696] On the Importance of Sampling in Training GCNs: Tighter Analysis and Variance Reduction](http://arxiv.org/abs/2103.02696)


  Graph Convolutional Networks (GCNs) have achieved impressive empirical
advancement across a wide variety of semi-supervised node classification tasks.
Despite their great success, training GCNs on large graphs suffers from
computational and memory issues. A potential path to circumvent these obstacles
is sampling-based methods, where at each layer a subset of nodes is sampled.
Although recent studies have empirically demonstrated the effectiveness of
sampling-based methods, these works lack theoretical convergence guarantees
under realistic settings and cannot fully leverage the information of evolving
parameters during optimization. In this paper, we describe and analyze a
general doubly variance reduction schema that can accelerate any sampling
method under the memory budget. The motivating impetus for the proposed schema
is a careful analysis of the variance of sampling methods where it is shown
that the induced variance can be decomposed into node embedding approximation
variance (zeroth-order variance) during forward propagation and
layerwise-gradient variance (first-order variance) during backward propagation.
We theoretically analyze the convergence of the proposed schema and show that
it enjoys an $\mathcal{O}(1/T)$ convergence rate. We complement our theoretical
results by integrating the proposed schema in different sampling methods and
applying them to different large real-world graphs.

    

### [[2103.05825] ELLA: Exploration through Learned Language Abstraction](http://arxiv.org/abs/2103.05825)


  Building agents capable of understanding language instructions is critical to
effective and robust human-AI collaboration. Recent work focuses on training
these agents via reinforcement learning in environments with synthetic
language; however, instructions often define long-horizon, sparse-reward tasks,
and learning policies requires many episodes of experience. We introduce ELLA:
Exploration through Learned Language Abstraction, a reward shaping approach
geared towards boosting sample efficiency in sparse reward environments by
correlating high-level instructions with simpler low-level constituents. ELLA
has two key elements: 1) A termination classifier that identifies when agents
complete low-level instructions, and 2) A relevance classifier that correlates
low-level instructions with success on high-level tasks. We learn the
termination classifier offline from pairs of instructions and terminal states.
Notably, in departure from prior work in language and abstraction, we learn the
relevance classifier online, without relying on an explicit decomposition of
high-level instructions to low-level instructions. On a suite of complex BabyAI
environments with varying instruction complexities and reward sparsity, ELLA
shows gains in sample efficiency relative to language-based shaping and
traditional RL methods.

    

### [[2103.06624] Beta-CROWN: Efficient Bound Propagation with Per-neuron Split Constraints for Complete and Incomplete Neural Network Robustness Verification](http://arxiv.org/abs/2103.06624)


  Bound propagation based incomplete neural network verifiers such as CROWN are
very efficient and can significantly accelerate branch-and-bound (BaB) based
complete verification of neural networks. However, bound propagation cannot
fully handle the neuron split constraints introduced by BaB commonly handled by
expensive linear programming (LP) solvers, leading to loose bounds and hurting
verification efficiency. In this work, we develop $\beta$-CROWN, a new bound
propagation based method that can fully encode neuron splits via optimizable
parameters $\beta$ constructed from either primal or dual space. When jointly
optimized in intermediate layers, $\beta$-CROWN generally produces better
bounds than typical LP verifiers with neuron split constraints, while being as
efficient and parallelizable as CROWN on GPUs. Applied to complete robustness
verification benchmarks, $\beta$-CROWN with BaB is up to three orders of
magnitude faster than LP-based BaB methods, and is notably faster than all
existing approaches while producing lower timeout rates. By terminating BaB
early, our method can also be used for efficient incomplete verification. We
consistently achieve higher verified accuracy in many settings compared to
powerful incomplete verifiers, including those based on convex barrier breaking
techniques. Compared to the typically tightest but very costly semidefinite
programming (SDP) based incomplete verifiers, we obtain higher verified
accuracy with three orders of magnitudes less verification time. Our algorithm
empowered the $\alpha,\!\beta$-CROWN (alpha-beta-CROWN) verifier, the winning
tool in VNN-COMP 2021. Our code is available at this http URL


### [[2103.11870] A Federated Learning Framework for Smart Grids: Securing Power Traces in Collaborative Learning](http://arxiv.org/abs/2103.11870)


  With the deployment of smart sensors and advancements in communication
technologies, big data analytics have become vastly popular in the smart grid
domain, informing stakeholders of the best power utilization strategy. However,
these power-related data are stored and owned by different parties. For
example, power consumption data are stored in numerous transformer stations
across cities; mobility data of the population, which are important indicators
of power consumption, are held by mobile companies. Direct data sharing might
compromise party benefits, individual privacy and even national security.
Inspired by the federated learning scheme from Google AI, we propose a
federated learning framework for smart grids, which enables collaborative
learning of power consumption patterns without leaking individual power traces.
Horizontal federated learning is employed when data are scattered in the sample
space; vertical federated learning, on the other hand, is designed for the case
with data scattered in the feature space. Case studies show that, with proper
encryption schemes such as Paillier encryption, the machine learning models
constructed from the proposed framework are lossless, privacy-preserving and
effective. Finally, the promising future of federated learning in other facets
of the smart grid is discussed, including electric vehicles, distributed
generation/consumption and integrated energy systems.

    

### [[2103.16211] iVPF: Numerical Invertible Volume Preserving Flow for Efficient Lossless Compression](http://arxiv.org/abs/2103.16211)


  It is nontrivial to store rapidly growing big data nowadays, which demands
high-performance lossless compression techniques. Likelihood-based generative
models have witnessed their success on lossless compression, where flow based
models are desirable in allowing exact data likelihood optimisation with
bijective mappings. However, common continuous flows are in contradiction with
the discreteness of coding schemes, which requires either 1) imposing strict
constraints on flow models that degrades the performance or 2) coding numerous
bijective mapping errors which reduces the efficiency. In this paper, we
investigate volume preserving flows for lossless compression and show that a
bijective mapping without error is possible. We propose Numerical Invertible
Volume Preserving Flow (iVPF) which is derived from the general volume
preserving flows. By introducing novel computation algorithms on flow models,
an exact bijective mapping is achieved without any numerical error. We also
propose a lossless compression algorithm based on iVPF. Experiments on various
datasets show that the algorithm based on iVPF achieves state-of-the-art
compression ratio over lightweight compression algorithms.

    

### [[2104.00793] Effect of Radiology Report Labeler Quality on Deep Learning Models for Chest X-Ray Interpretation](http://arxiv.org/abs/2104.00793)


  Although deep learning models for chest X-ray interpretation are commonly
trained on labels generated by automatic radiology report labelers, the impact
of improvements in report labeling on the performance of chest X-ray
classification models has not been systematically investigated. We first
compare the CheXpert, CheXbert, and VisualCheXbert labelers on the task of
extracting accurate chest X-ray image labels from radiology reports, reporting
that the VisualCheXbert labeler outperforms the CheXpert and CheXbert labelers.
Next, after training image classification models using labels generated from
the different radiology report labelers on one of the largest datasets of chest
X-rays, we show that an image classification model trained on labels from the
VisualCheXbert labeler outperforms image classification models trained on
labels from the CheXpert and CheXbert labelers. Our work suggests that recent
improvements in radiology report labeling can translate to the development of
higher performing chest X-ray classification models.

    

### [[2104.03066] Distributional Robustness Loss for Long-tail Learning](http://arxiv.org/abs/2104.03066)


  Real-world data is often unbalanced and long-tailed, but deep models struggle
to recognize rare classes in the presence of frequent classes. To address
unbalanced data, most studies try balancing the data, the loss, or the
classifier to reduce classification bias towards head classes. Far less
attention has been given to the latent representations learned with unbalanced
data. We show that the feature extractor part of deep networks suffers greatly
from this bias. We propose a new loss based on robustness theory, which
encourages the model to learn high-quality representations for both head and
tail classes. While the general form of the robustness loss may be hard to
compute, we further derive an easy-to-compute upper bound that can be minimized
efficiently. This procedure reduces representation bias towards head classes in
the feature space and achieves new SOTA results on CIFAR100-LT, ImageNet-LT,
and iNaturalist long-tail benchmarks. We find that training with robustness
increases recognition accuracy of tail classes while largely maintaining the
accuracy of head classes. The new robustness loss can be combined with various
classifier balancing techniques and can be applied to representations at
several layers of the deep model.

    

### [[2104.05062] Achieving Model Robustness through Discrete Adversarial Training](http://arxiv.org/abs/2104.05062)


  Discrete adversarial attacks are symbolic perturbations to a language input
that preserve the output label but lead to a prediction error. While such
attacks have been extensively explored for the purpose of evaluating model
robustness, their utility for improving robustness has been limited to offline
augmentation only. Concretely, given a trained model, attacks are used to
generate perturbed (adversarial) examples, and the model is re-trained exactly
once. In this work, we address this gap and leverage discrete attacks for
online augmentation, where adversarial examples are generated at every training
step, adapting to the changing nature of the model. We propose (i) a new
discrete attack, based on best-first search, and (ii) random sampling attacks
that unlike prior work are not based on expensive search-based procedures.
Surprisingly, we find that random sampling leads to impressive gains in
robustness, outperforming the commonly-used offline augmentation, while leading
to a speedup at training time of ~10x. Furthermore, online augmentation with
search-based attacks justifies the higher training cost, significantly
improving robustness on three datasets. Last, we show that our new attack
substantially improves robustness compared to prior methods.

    

### [[2104.05154] Machine Learning Approach to Uncovering Residential Energy Consumption Patterns Based on Socioeconomic and Smart Meter Data](http://arxiv.org/abs/2104.05154)


  The smart meter data analysis contributes to better planning and operations
for the power system. This study aims to identify the drivers of residential
energy consumption patterns from the socioeconomic perspective based on the
consumption and demographic data using machine learning. We model consumption
patterns by representative loads and reveal the relationship between load
patterns and socioeconomic characteristics. Specifically, we analyze the
real-world smart meter data and extract load patterns by clustering in a robust
way. We further identify the influencing socioeconomic attributes on load
patterns to improve our method's interpretability. The relationship between
consumers' load patterns and selected socioeconomic features is characterized
via machine learning models. The findings are as follows. (1) Twelve load
clusters, consisting of six for weekdays and six for weekends, exhibit a
diverse pattern of lifestyle and a difference between weekdays and weekends.
(2) Among various socioeconomic features, age and education level are suggested
to influence the load patterns. (3) Our proposed analytical model using feature
selection and machine learning is proved to be more effective than XGBoost and
conventional neural network model in mapping the relationship between load
patterns and socioeconomic features.

    

### [[2104.11470] Random Noise Defense Against Query-Based Black-Box Attacks](http://arxiv.org/abs/2104.11470)


  The query-based black-box attacks have raised serious threats to machine
learning models in many real applications. In this work, we study a lightweight
defense method, dubbed Random Noise Defense (RND), which adds proper Gaussian
noise to each query. We conduct the theoretical analysis about the
effectiveness of RND against query-based black-box attacks and the
corresponding adaptive attacks. Our theoretical results reveal that the defense
performance of RND is determined by the magnitude ratio between the noise
induced by RND and the noise added by the attackers for gradient estimation or
local search. The large magnitude ratio leads to the stronger defense
performance of RND, and it's also critical for mitigating adaptive attacks.
Based on our analysis, we further propose to combine RND with a plausible
Gaussian augmentation Fine-tuning (RND-GF). It enables RND to add larger noise
to each query while maintaining the clean accuracy to obtain a better trade-off
between clean accuracy and defense performance. Additionally, RND can be
flexibly combined with the existing defense methods to further boost the
adversarial robustness, such as adversarial training (AT). Extensive
experiments on CIFAR-10 and ImageNet verify our theoretical findings and the
effectiveness of RND and RND-GF.

    

### [[2104.14657] Revisiting the dynamics of Bose-Einstein condensates in a double well by deep learning with a hybrid network](http://arxiv.org/abs/2104.14657)


  Deep learning, accounting for the use of an elaborate neural network, has
recently been developed as an efficient and powerful tool to solve diverse
problems in physics and other sciences. In the present work, we propose a novel
learning method based on a hybrid network integrating two different kinds of
neural networks: Long Short-Term Memory(LSTM) and Deep Residual
Network(ResNet), in order to overcome the difficulty met in numerically
simulating strongly-oscillating dynamical evolutions of physical systems. By
taking the dynamics of Bose-Einstein condensates in a double-well potential as
an example, we show that our new method makes a high efficient pre-learning and
a high-fidelity prediction about the whole dynamics. This benefits from the
advantage of the combination of the LSTM and the ResNet and is impossibly
achieved by a single network in the case of direct learning. Our method can be
applied for simulating complex cooperative dynamics in a system with fast
multiple-frequency oscillations with the aid of auxiliary spectrum analysis.

    

### [[2105.06791] Agree to Disagree: When Deep Learning Models With Identical Architectures Produce Distinct Explanations](http://arxiv.org/abs/2105.06791)


  Deep Learning of neural networks has progressively become more prominent in
healthcare with models reaching, or even surpassing, expert accuracy levels.
However, these success stories are tainted by concerning reports on the lack of
model transparency and bias against some medical conditions or patients'
sub-groups. Explainable methods are considered the gateway to alleviate many of
these concerns. In this study we demonstrate that the generated explanations
are volatile to changes in model training that are perpendicular to the
classification task and model structure. This raises further questions about
trust in deep learning models for healthcare. Mainly, whether the models
capture underlying causal links in the data or just rely on spurious
correlations that are made visible via explanation methods. We demonstrate that
the output of explainability methods on deep neural networks can vary
significantly by changes of hyper-parameters, such as the random seed or how
the training set is shuffled. We introduce a measure of explanation consistency
which we use to highlight the identified problems on the MIMIC-CXR dataset. We
find explanations of identical models but with different training setups have a
low consistency: $\approx$ 33% on average. On the contrary, kernel methods are
robust against any orthogonal changes, with explanation consistency at 94%. We
conclude that current trends in model explanation are not sufficient to
mitigate the risks of deploying models in real life healthcare applications.

    

### [[2105.09536] On the $α$-lazy version of Markov chains in estimation and testing problems](http://arxiv.org/abs/2105.09536)


  Given access to a single long trajectory generated by an unknown irreducible
Markov chain $M$, we simulate an $\alpha$-lazy version of $M$ which is ergodic.
This enables us to generalize recent results on estimation and identity testing
that were stated for ergodic Markov chains in a way that allows fully empirical
inference. In particular, our approach shows that the pseudo spectral gap
introduced by Paulin [2015] and defined for ergodic Markov chains may be given
a meaning already in the case of irreducible but possibly periodic Markov
chains.

    

### [[2105.13937] Polygonal Unadjusted Langevin Algorithms: Creating stable and efficient adaptive algorithms for neural networks](http://arxiv.org/abs/2105.13937)


  We present a new class of Langevin based algorithms, which overcomes many of
the known shortcomings of popular adaptive optimizers that are currently used
for the fine tuning of deep learning models. Its underpinning theory relies on
recent advances of Euler's polygonal approximations for stochastic differential
equations (SDEs) with monotone coefficients. As a result, it inherits the
stability properties of tamed algorithms, while it addresses other known
issues, e.g. vanishing gradients in neural networks. In particular, we provide
a nonasymptotic analysis and full theoretical guarantees for the convergence
properties of an algorithm of this novel class, which we named TH$\varepsilon$O
POULA (or, simply, TheoPouLa). Finally, several experiments are presented with
different types of deep learning models, which show the superior performance of
TheoPouLa over many popular adaptive optimization algorithms.

    

### [[2105.14260] Understanding Bandits with Graph Feedback](http://arxiv.org/abs/2105.14260)


  The bandit problem with graph feedback, proposed in [Mannor and Shamir,
NeurIPS 2011], is modeled by a directed graph $G=(V,E)$ where $V$ is the
collection of bandit arms, and once an arm is triggered, all its incident arms
are observed. A fundamental question is how the structure of the graph affects
the min-max regret. We propose the notions of the fractional weak domination
number $\delta^*$ and the $k$-packing independence number capturing upper bound
and lower bound for the regret respectively. We show that the two notions are
inherently connected via aligning them with the linear program of the weakly
dominating set and its dual -- the fractional vertex packing set respectively.
Based on this connection, we utilize the strong duality theorem to prove a
general regret upper bound $O\left(\left( \delta^*\log
|V|\right)^{\frac{1}{3}}T^{\frac{2}{3}}\right)$ and a lower bound
$\Omega\left(\left(\delta^*/\alpha\right)^{\frac{1}{3}}T^{\frac{2}{3}}\right)$
where $\alpha$ is the integrality gap of the dual linear program. Therefore,
our bounds are tight up to a $\left(\log |V|\right)^{\frac{1}{3}}$ factor on
graphs with bounded integrality gap for the vertex packing problem including
trees and graphs with bounded degree. Moreover, we show that for several
special families of graphs, we can get rid of the $\left(\log
|V|\right)^{\frac{1}{3}}$ factor and establish optimal regret.

    

### [[2106.00072] Early Detection of COVID-19 Hotspots Using Spatio-Temporal Data](http://arxiv.org/abs/2106.00072)


  Recently, the Centers for Disease Control and Prevention (CDC) has worked
with other federal agencies to identify counties with increasing coronavirus
disease 2019 (COVID-19) incidence (hotspots) and offers support to local health
departments to limit the spread of the disease. Understanding the
spatio-temporal dynamics of hotspot events is of great importance to support
policy decisions and prevent large-scale outbreaks. This paper presents a
spatio-temporal Bayesian framework for early detection of COVID-19 hotspots (at
the county level) in the United States. We assume both the observed number of
cases and hotspots depend on a class of latent random variables, which encode
the underlying spatio-temporal dynamics of the transmission of COVID-19. Such
latent variables follow a zero-mean Gaussian process, whose covariance is
specified by a non-stationary kernel function. The most salient feature of our
kernel function is that deep neural networks are introduced to enhance the
model's representative power while still enjoying the interpretability of the
kernel. We derive a sparse model and fit the model using a variational learning
strategy to circumvent the computational intractability for large data sets.
Our model demonstrates better interpretability and superior hotspot-detection
performance compared to other baseline methods.

    

### [[2106.02195] Celebrating Diversity in Shared Multi-Agent Reinforcement Learning](http://arxiv.org/abs/2106.02195)


  Recently, deep multi-agent reinforcement learning (MARL) has shown the
promise to solve complex cooperative tasks. Its success is partly because of
parameter sharing among agents. However, such sharing may lead agents to behave
similarly and limit their coordination capacity. In this paper, we aim to
introduce diversity in both optimization and representation of shared
multi-agent reinforcement learning. Specifically, we propose an
information-theoretical regularization to maximize the mutual information
between agents' identities and their trajectories, encouraging extensive
exploration and diverse individualized behaviors. In representation, we
incorporate agent-specific modules in the shared neural network architecture,
which are regularized by L1-norm to promote learning sharing among agents while
keeping necessary diversity. Empirical results show that our method achieves
state-of-the-art performance on Google Research Football and super hard
StarCraft II micromanagement tasks.

    

### [[2106.02261] Out-of-Distribution Generalization in Kernel Regression](http://arxiv.org/abs/2106.02261)


  In real word applications, data generating process for training a machine
learning model often differs from what the model encounters in the test stage.
Understanding how and whether machine learning models generalize under such
distributional shifts have been a theoretical challenge. Here, we study
generalization in kernel regression when the training and test distributions
are different using methods from statistical physics. Using the replica method,
we derive an analytical formula for the out-of-distribution generalization
error applicable to any kernel and real datasets. We identify an overlap matrix
that quantifies the mismatch between distributions for a given kernel as a key
determinant of generalization performance under distribution shift. Using our
analytical expressions we elucidate various generalization phenomena including
possible improvement in generalization when there is a mismatch. We develop
procedures for optimizing training and test distributions for a given data
budget to find best and worst case generalizations under the shift. We present
applications of our theory to real and synthetic datasets and for many kernels.
We compare results of our theory applied to Neural Tangent Kernel with
simulations of wide networks and show agreement. We analyze linear regression
in further depth.

    

### [[2106.02925] Tensor Normal Training for Deep Learning Models](http://arxiv.org/abs/2106.02925)


  Despite the predominant use of first-order methods for training deep learning
models, second-order methods, and in particular, natural gradient methods,
remain of interest because of their potential for accelerating training through
the use of curvature information. Several methods with non-diagonal
preconditioning matrices, including KFAC, Shampoo, and K-BFGS, have been
proposed and shown to be effective. Based on the so-called tensor normal (TN)
distribution, we propose and analyze a brand new approximate natural gradient
method, Tensor Normal Training (TNT), which like Shampoo, only requires
knowledge of the shape of the training parameters. By approximating the
probabilistically based Fisher matrix, as opposed to the empirical Fisher
matrix, our method uses the block-wise covariance of the sampling based
gradient as the pre-conditioning matrix. Moreover, the assumption that the
sampling-based (tensor) gradient follows a TN distribution, ensures that its
covariance has a Kronecker separable structure, which leads to a tractable
approximation to the Fisher matrix. Consequently, TNT's memory requirements and
per-iteration computational costs are only slightly higher than those for
first-order methods. In our experiments, TNT exhibited superior optimization
performance to state-of-the-art first-order methods, and comparable
optimization performance to the state-of-the-art second-order methods KFAC and
Shampoo. Moreover, TNT demonstrated its ability to generalize as well as
first-order methods, while using fewer epochs.

    

### [[2106.03408] Antipodes of Label Differential Privacy: PATE and ALIBI](http://arxiv.org/abs/2106.03408)


  We consider the privacy-preserving machine learning (ML) setting where the
trained model must satisfy differential privacy (DP) with respect to the labels
of the training examples. We propose two novel approaches based on,
respectively, the Laplace mechanism and the PATE framework, and demonstrate
their effectiveness on standard benchmarks.
While recent work by Ghazi et al. proposed Label DP schemes based on a
randomized response mechanism, we argue that additive Laplace noise coupled
with Bayesian inference (ALIBI) is a better fit for typical ML tasks. Moreover,
we show how to achieve very strong privacy levels in some regimes, with our
adaptation of the PATE framework that builds on recent advances in
semi-supervised learning.
We complement theoretical analysis of our algorithms' privacy guarantees with
empirical evaluation of their memorization properties. Our evaluation suggests
that comparing different algorithms according to their provable DP guarantees
can be misleading and favor a less private algorithm with a tighter analysis.
Code for implementation of algorithms and memorization attacks is available
from this https URL.

    

### [[2106.03442] Average-Reward Reinforcement Learning with Trust Region Methods](http://arxiv.org/abs/2106.03442)


  Most of reinforcement learning algorithms optimize the discounted criterion
which is beneficial to accelerate the convergence and reduce the variance of
estimates. Although the discounted criterion is appropriate for certain tasks
such as financial related problems, many engineering problems treat future
rewards equally and prefer a long-run average criterion. In this paper, we
study the reinforcement learning problem with the long-run average criterion.
Firstly, we develop a unified trust region theory with discounted and average
criteria and derive a novel performance bound within the trust region with the
Perturbation Analysis (PA) theory. Secondly, we propose a practical algorithm
named Average Policy Optimization (APO), which improves the value estimation
with a novel technique named Average Value Constraint. Finally, experiments are
conducted in the continuous control environment MuJoCo. In most tasks, APO
performs better than the discounted PPO, which demonstrates the effectiveness
of our approach. Our work provides a unified framework of the trust region
approach including both the discounted and average criteria, which may
complement the framework of reinforcement learning beyond the discounted
objectives.

    

### [[2106.03579] Forward Looking Best-Response Multiplicative Weights Update Methods for Bilinear Zero-sum Games](http://arxiv.org/abs/2106.03579)


  Our work focuses on extra gradient learning algorithms for finding Nash
equilibria in bilinear zero-sum games. The proposed method, which can be
formally considered as a variant of Optimistic Mirror Descent
\cite{DBLP:conf/iclr/MertikopoulosLZ19}, uses a large learning rate for the
intermediate gradient step which essentially leads to computing (approximate)
best response strategies against the profile of the previous iteration.
Although counter-intuitive at first sight due to the irrationally large, for an
iterative algorithm, intermediate learning step, we prove that the method
guarantees last-iterate convergence to an equilibrium. Particularly, we show
that the algorithm reaches first an $\eta^{1/\rho}$-approximate Nash
equilibrium, with $\rho > 1$, by decreasing the Kullback-Leibler divergence of
each iterate by at least $\Omega(\eta^{1+\frac{1}{\rho}})$, for sufficiently
small learning rate, $\eta$, until the method becomes a contracting map, and
converges to the exact equilibrium. Furthermore, we perform experimental
comparisons with the optimistic variant of the multiplicative weights update
method, by \cite{Daskalakis2019LastIterateCZ} and show that our algorithm has
significant practical potential since it offers substantial gains in terms of
accelerated convergence.

    

### [[2106.03609] High-Dimensional Bayesian Optimisation with Variational Autoencoders and Deep Metric Learning](http://arxiv.org/abs/2106.03609)


  We introduce a method combining variational autoencoders (VAEs) and deep
metric learning to perform Bayesian optimisation (BO) over high-dimensional and
structured input spaces. By adapting ideas from deep metric learning, we use
label guidance from the blackbox function to structure the VAE latent space,
facilitating the Gaussian process fit and yielding improved BO performance.
Importantly for BO problem settings, our method operates in semi-supervised
regimes where only few labelled data points are available. We run experiments
on three real-world tasks, achieving state-of-the-art results on the penalised
logP molecule generation benchmark using just 3% of the labelled data required
by previous approaches. As a theoretical contribution, we present a proof of
vanishing regret for VAE BO.

    

### [[2106.03743] Proxy-Normalizing Activations to Match Batch Normalization while Removing Batch Dependence](http://arxiv.org/abs/2106.03743)


  We investigate the reasons for the performance degradation incurred with
batch-independent normalization. We find that the prototypical techniques of
layer normalization and instance normalization both induce the appearance of
failure modes in the neural network's pre-activations: (i) layer normalization
induces a collapse towards channel-wise constant functions; (ii) instance
normalization induces a lack of variability in instance statistics, symptomatic
of an alteration of the expressivity. To alleviate failure mode (i) without
aggravating failure mode (ii), we introduce the technique "Proxy Normalization"
that normalizes post-activations using a proxy distribution. When combined with
layer normalization or group normalization, this batch-independent
normalization emulates batch normalization's behavior and consistently matches
or exceeds its performance.

    

### [[2106.03783] An Information-theoretic Approach to Distribution Shifts](http://arxiv.org/abs/2106.03783)


  Safely deploying machine learning models to the real world is often a
challenging process. Models trained with data obtained from a specific
geographic location tend to fail when queried with data obtained elsewhere,
agents trained in a simulation can struggle to adapt when deployed in the real
world or novel environments, and neural networks that are fit to a subset of
the population might carry some selection bias into their decision process. In
this work, we describe the problem of data shift from a novel
information-theoretic perspective by (i) identifying and describing the
different sources of error, (ii) comparing some of the most promising
objectives explored in the recent domain generalization, and fair
classification literature. From our theoretical analysis and empirical
evaluation, we conclude that the model selection procedure needs to be guided
by careful considerations regarding the observed data, the factors used for
correction, and the structure of the data-generating process.

    

### [[2106.05241] Multi-Facet Clustering Variational Autoencoders](http://arxiv.org/abs/2106.05241)


  Work in deep clustering focuses on finding a single partition of data.
However, high-dimensional data, such as images, typically feature multiple
interesting characteristics one could cluster over. For example, images of
objects against a background could be clustered over the shape of the object
and separately by the colour of the background. In this paper, we introduce
Multi-Facet Clustering Variational Autoencoders (MFCVAE), a novel class of
variational autoencoders with a hierarchy of latent variables, each with a
Mixture-of-Gaussians prior, that learns multiple clusterings simultaneously,
and is trained fully unsupervised and end-to-end. MFCVAE uses a
progressively-trained ladder architecture which leads to highly stable
performance. We provide novel theoretical results for optimising the ELBO
analytically with respect to the categorical variational posterior
distribution, correcting earlier influential theoretical work. On image
benchmarks, we demonstrate that our approach separates out and clusters over
different aspects of the data in a disentangled manner. We also show other
advantages of our model: the compositionality of its latent space and that it
provides controlled generation of samples.

    

### [[2106.05739] Separation Results between Fixed-Kernel and Feature-Learning Probability Metrics](http://arxiv.org/abs/2106.05739)


  Several works in implicit and explicit generative modeling empirically
observed that feature-learning discriminators outperform fixed-kernel
discriminators in terms of the sample quality of the models. We provide
separation results between probability metrics with fixed-kernel and
feature-learning discriminators using the function classes $\mathcal{F}_2$ and
$\mathcal{F}_1$ respectively, which were developed to study overparametrized
two-layer neural networks. In particular, we construct pairs of distributions
over hyper-spheres that can not be discriminated by fixed kernel
$(\mathcal{F}_2)$ integral probability metric (IPM) and Stein discrepancy (SD)
in high dimensions, but that can be discriminated by their feature learning
($\mathcal{F}_1$) counterparts. To further study the separation we provide
links between the $\mathcal{F}_1$ and $\mathcal{F}_2$ IPMs with sliced
Wasserstein distances. Our work suggests that fixed-kernel discriminators
perform worse than their feature learning counterparts because their
corresponding metrics are weaker.

    

### [[2106.06610] Scalars are universal: Equivariant machine learning, structured like classical physics](http://arxiv.org/abs/2106.06610)


  There has been enormous progress in the last few years in designing neural
networks that respect the fundamental symmetries and coordinate freedoms of
physical law. Some of these frameworks make use of irreducible representations,
some make use of high-order tensor objects, and some apply symmetry-enforcing
constraints. Different physical laws obey different combinations of fundamental
symmetries, but a large fraction (possibly all) of classical physics is
equivariant to translation, rotation, reflection (parity), boost (relativity),
and permutations. Here we show that it is simple to parameterize universally
approximating polynomial functions that are equivariant under these symmetries,
or under the Euclidean, Lorentz, and Poincaré groups, at any dimensionality
$d$. The key observation is that nonlinear O($d$)-equivariant (and
related-group-equivariant) functions can be universally expressed in terms of a
lightweight collection of scalars -- scalar products and scalar contractions of
the scalar, vector, and tensor inputs. We complement our theory with numerical
examples that show that the scalar-based method is simple, efficient, and
scalable.

    

### [[2106.07175] Learning to Combine Per-Example Solutions for Neural Program Synthesis](http://arxiv.org/abs/2106.07175)


  The goal of program synthesis from examples is to find a computer program
that is consistent with a given set of input-output examples. Most
learning-based approaches try to find a program that satisfies all examples at
once. Our work, by contrast, considers an approach that breaks the problem into
two stages: (a) find programs that satisfy only one example, and (b) leverage
these per-example solutions to yield a program that satisfies all examples. We
introduce the Cross Aggregator neural network module based on a multi-head
attention mechanism that learns to combine the cues present in these
per-example solutions to synthesize a global solution. Evaluation across
programs of different lengths and under two different experimental settings
reveal that when given the same time budget, our technique significantly
improves the success rate over PCCoder [Zohar et. al 2018] and other ablation
baselines. The code, data and trained models for our work can be found at
this https URL.

    

### [[2106.07218] Physics-Aware Downsampling with Deep Learning for Scalable Flood Modeling](http://arxiv.org/abs/2106.07218)


  Background: Floods are the most common natural disaster in the world,
affecting the lives of hundreds of millions. Flood forecasting is therefore a
vitally important endeavor, typically achieved using physical water flow
simulations, which rely on accurate terrain elevation maps. However, such
simulations, based on solving partial differential equations, are
computationally prohibitive on a large scale. This scalability issue is
commonly alleviated using a coarse grid representation of the elevation map,
though this representation may distort crucial terrain details, leading to
significant inaccuracies in the simulation. Contributions: We train a deep
neural network to perform physics-informed downsampling of the terrain map: we
optimize the coarse grid representation of the terrain maps, so that the flood
prediction will match the fine grid solution. For the learning process to
succeed, we configure a dataset specifically for this task. We demonstrate that
with this method, it is possible to achieve a significant reduction in
computational cost, while maintaining an accurate solution. A reference
implementation accompanies the paper as well as documentation and code for
dataset reproduction.

    

### [[2106.08208] SUPER-ADAM: Faster and Universal Framework of Adaptive Gradients](http://arxiv.org/abs/2106.08208)


  Adaptive gradient methods have shown excellent performances for solving many
machine learning problems. Although multiple adaptive methods were recently
studied, they mainly focus on either empirical or theoretical aspects and also
only work for specific problems by using some specific adaptive learning rates.
It is desired to design a universal framework for practical algorithms of
adaptive gradients with theoretical guarantee to solve general problems. To
fill this gap, we propose a faster and universal framework of adaptive
gradients (\emph{i.e.}, SUPER-ADAM) by introducing a universal adaptive matrix
that includes most existing adaptive gradient forms. Moreover, our framework
can flexibly integrate the momentum and variance reduced techniques. In
particular, our novel framework provides the convergence analysis support for
adaptive gradient methods under the nonconvex setting. In theoretical analysis,
we prove that our SUPER-ADAM algorithm can achieve the best known complexity of
$\tilde{O}(\epsilon^{-3})$ for finding an $\epsilon$-stationary point of
nonconvex optimization, which matches the lower bound for stochastic smooth
nonconvex optimization. In numerical experiments, we employ various deep
learning tasks to validate that our algorithm consistently outperforms the
existing adaptive algorithms. Code is available at
this https URL


### [[2106.08567] Optimal Accounting of Differential Privacy via Characteristic Function](http://arxiv.org/abs/2106.08567)


  Characterizing the privacy degradation over compositions, i.e., privacy
accounting, is a fundamental topic in differential privacy (DP) with many
applications to differentially private machine learning and federated learning.
We propose a unification of recent advances (Renyi DP, privacy profiles, $f$-DP
and the PLD formalism) via the \emph{characteristic function} ($\phi$-function)
of a certain \emph{dominating} privacy loss random variable. We show that our
approach allows \emph{natural} adaptive composition like Renyi DP, provides
\emph{exactly tight} privacy accounting like PLD, and can be (often
\emph{losslessly}) converted to privacy profile and $f$-DP, thus providing
$(\epsilon,\delta)$-DP guarantees and interpretable tradeoff functions.
Algorithmically, we propose an \emph{analytical Fourier accountant} that
represents the \emph{complex} logarithm of $\phi$-functions symbolically and
uses Gaussian quadrature for numerical computation. On several popular DP
mechanisms and their subsampled counterparts, we demonstrate the flexibility
and tightness of our approach in theory and experiments.

    

### [[2106.08619] Locality defeats the curse of dimensionality in convolutional teacher-student scenarios](http://arxiv.org/abs/2106.08619)


  Convolutional neural networks perform a local and translationally-invariant
treatment of the data: quantifying which of these two aspects is central to
their success remains a challenge. We study this problem within a
teacher-student framework for kernel regression, using `convolutional' kernels
inspired by the neural tangent kernel of simple convolutional architectures of
given filter size. Using heuristic methods from physics, we find in the
ridgeless case that locality is key in determining the learning curve exponent
$\beta$ (that relates the test error $\epsilon_t\sim P^{-\beta}$ to the size of
the training set $P$), whereas translational invariance is not. In particular,
if the filter size of the teacher $t$ is smaller than that of the student $s$,
$\beta$ is a function of $s$ only and does not depend on the input dimension.
We confirm our predictions on $\beta$ empirically. We conclude by proving,
using a natural universality assumption, that performing kernel regression with
a ridge that decreases with the size of the training set leads to similar
learning curve exponents to those we obtain in the ridgeless case.

    

### [[2106.12753] DeepAuditor: Distributed Online Intrusion Detection System for IoT devices via Power Side-channel Auditing](http://arxiv.org/abs/2106.12753)


  As the number of IoT devices has increased rapidly, IoT botnets have
exploited the vulnerabilities of IoT devices. However, it is still challenging
to detect the initial intrusion on IoT devices prior to massive attacks. Recent
studies have utilized power side-channel information to identify this intrusion
behavior on IoT devices but still lack accurate models in real-time for
ubiquitous botnet detection.
We proposed the first online intrusion detection system called DeepAuditor
for IoT devices via power auditing. To develop the real-time system, we
proposed a lightweight power auditing device called Power Auditor. We also
designed a distributed CNN classifier for online inference in a laboratory
setting. In order to protect data leakage and reduce networking redundancy, we
then proposed a privacy-preserved inference protocol via Packed Homomorphic
Encryption and a sliding window protocol in our system. The classification
accuracy and processing time were measured, and the proposed classifier
outperformed a baseline classifier, especially against unseen patterns. We also
demonstrated that the distributed CNN design is secure against any distributed
components. Overall, the measurements were shown to the feasibility of our
real-time distributed system for intrusion detection on IoT devices.

    

### [[2106.13897] Implicit Gradient Alignment in Distributed and Federated Learning](http://arxiv.org/abs/2106.13897)


  A major obstacle to achieving global convergence in distributed and federated
learning is the misalignment of gradients across clients, or mini-batches due
to heterogeneity and stochasticity of the distributed data. In this work, we
show that data heterogeneity can in fact be exploited to improve generalization
performance through implicit regularization. One way to alleviate the effects
of heterogeneity is to encourage the alignment of gradients across different
clients throughout training. Our analysis reveals that this goal can be
accomplished by utilizing the right optimization method that replicates the
implicit regularization effect of SGD, leading to gradient alignment as well as
improvements in test accuracies. Since the existence of this regularization in
SGD completely relies on the sequential use of different mini-batches during
training, it is inherently absent when training with large mini-batches. To
obtain the generalization benefits of this regularization while increasing
parallelism, we propose a novel GradAlign algorithm that induces the same
implicit regularization while allowing the use of arbitrarily large batches in
each update. We experimentally validate the benefits of our algorithm in
different distributed and federated learning settings.

    

### [[2106.14806] Laplace Redux -- Effortless Bayesian Deep Learning](http://arxiv.org/abs/2106.14806)


  Bayesian formulations of deep learning have been shown to have compelling
theoretical properties and offer practical functional benefits, such as
improved predictive uncertainty quantification and model selection. The Laplace
approximation (LA) is a classic, and arguably the simplest family of
approximations for the intractable posteriors of deep neural networks. Yet,
despite its simplicity, the LA is not as popular as alternatives like
variational Bayes or deep ensembles. This may be due to assumptions that the LA
is expensive due to the involved Hessian computation, that it is difficult to
implement, or that it yields inferior results. In this work we show that these
are misconceptions: we (i) review the range of variants of the LA including
versions with minimal cost overhead; (ii) introduce "laplace", an easy-to-use
software library for PyTorch offering user-friendly access to all major flavors
of the LA; and (iii) demonstrate through extensive experiments that the LA is
competitive with more popular alternatives in terms of performance, while
excelling in terms of computational cost. We hope that this work will serve as
a catalyst to a wider adoption of the LA in practical deep learning, including
in domains where Bayesian approaches are not typically considered at the
moment.

    

### [[2107.00591] Offline-to-Online Reinforcement Learning via Balanced Replay and Pessimistic Q-Ensemble](http://arxiv.org/abs/2107.00591)


  Recent advance in deep offline reinforcement learning (RL) has made it
possible to train strong robotic agents from offline datasets. However,
depending on the quality of the trained agents and the application being
considered, it is often desirable to fine-tune such agents via further online
interactions. In this paper, we observe that state-action distribution shift
may lead to severe bootstrap error during fine-tuning, which destroys the good
initial policy obtained via offline RL. To address this issue, we first propose
a balanced replay scheme that prioritizes samples encountered online while also
encouraging the use of near-on-policy samples from the offline dataset.
Furthermore, we leverage multiple Q-functions trained pessimistically offline,
thereby preventing overoptimism concerning unfamiliar actions at novel states
during the initial training phase. We show that the proposed method improves
sample-efficiency and final performance of the fine-tuned robotic agents on
various locomotion and manipulation tasks. Our code is available at:
this https URL.

    

### [[2107.02071] Unsupervised Ensemble Selection for Multilayer Bootstrap Networks](http://arxiv.org/abs/2107.02071)


  It is known that unsupervised nonlinear dimensionality reduction and
clustering is sensitive to the selection of hyperparameters, particularly for
deep learning based methods, which hinder its practical use. How to select a
proper network structure that may be dramatically different in different
applications is a hard issue for deep models, given little prior knowledge of
data. In this paper, we explore ensemble learning and selection techniques for
automatically determining the optimal network structure of a deep model, named
multilayer bootstrap networks (MBN). Specifically, we first propose an MBN
ensemble (MBN-E) algorithm which concatenates the sparse outputs of a set of
MBN base models with different network structures into a new representation.
Because training an ensemble of MBN is expensive, we propose a fast version of
MBN-E (fMBN-E), which replaces the step of random data resampling in MBN-E by
the resampling of random similarity scores. Theoretically, fMBN-E is even
faster than a single standard MBN. Then, we take the new representation
produced by MBN-E as a reference for selecting the optimal MBN base models. Two
kinds of ensemble selection criteria, named optimization-like selection
criteria and distribution divergence criteria, are applied. Importantly, MBN-E
and its ensemble selection techniques maintain the simple formulation of MBN
that is based on one-nearest-neighbor learning, and reach the state-of-the-art
performance without manual hyperparameter tuning. fMBN-E is empirically even
hundreds of times faster than MBN-E without suffering performance degradation.
The source code is available at this http URL.

    

### [[2110.09338] Contextual Hate Speech Detection in Code Mixed Text using Transformer Based Approaches](http://arxiv.org/abs/2110.09338)


  In the recent past, social media platforms have helped people in connecting
and communicating to a wider audience. But this has also led to a drastic
increase in cyberbullying. It is essential to detect and curb hate speech to
keep the sanity of social media platforms. Also, code mixed text containing
more than one language is frequently used on these platforms. We, therefore,
propose automated techniques for hate speech detection in code mixed text from
scraped Twitter. We specifically focus on code mixed English-Hindi text and
transformer-based approaches. While regular approaches analyze the text
independently, we also make use of content text in the form of parent tweets.
We try to evaluate the performances of multilingual BERT and Indic-BERT in
single-encoder and dual-encoder settings. The first approach is to concatenate
the target text and context text using a separator token and get a single
representation from the BERT model. The second approach encodes the two texts
independently using a dual BERT encoder and the corresponding representations
are averaged. We show that the dual-encoder approach using independent
representations yields better performance. We also employ simple ensemble
methods to further improve the performance. Using these methods we report the
best F1 score of 73.07% on the HASOC 2021 ICHCL code mixed data set.

    

### [[2110.09485] Learning in High Dimension Always Amounts to Extrapolation](http://arxiv.org/abs/2110.09485)


  The notion of interpolation and extrapolation is fundamental in various
fields from deep learning to function approximation. Interpolation occurs for a
sample $x$ whenever this sample falls inside or on the boundary of the given
dataset's convex hull. Extrapolation occurs when $x$ falls outside of that
convex hull. One fundamental (mis)conception is that state-of-the-art
algorithms work so well because of their ability to correctly interpolate
training data. A second (mis)conception is that interpolation happens
throughout tasks and datasets, in fact, many intuitions and theories rely on
that assumption. We empirically and theoretically argue against those two
points and demonstrate that on any high-dimensional ($>$100) dataset,
interpolation almost surely never happens. Those results challenge the validity
of our current interpolation/extrapolation definition as an indicator of
generalization performances.

    

### [[2110.13008] Logsig-RNN: a novel network for robust and efficient skeleton-based action recognition](http://arxiv.org/abs/2110.13008)


  This paper contributes to the challenge of skeleton-based human action
recognition in videos. The key step is to develop a generic network
architecture to extract discriminative features for the spatio-temporal
skeleton data. In this paper, we propose a novel module, namely Logsig-RNN,
which is the combination of the log-signature layer and recurrent type neural
networks (RNNs). The former one comes from the mathematically principled
technology of signatures and log-signatures as representations for streamed
data, which can manage high sample rate streams, non-uniform sampling and time
series of variable length. It serves as an enhancement of the recurrent layer,
which can be conveniently plugged into neural networks. Besides we propose two
path transformation layers to significantly reduce path dimension while
retaining the essential information fed into the Logsig-RNN module. Finally,
numerical results demonstrate that replacing the RNN module by the Logsig-RNN
module in SOTA networks consistently improves the performance on both Chalearn
gesture data and NTU RGB+D 120 action data in terms of accuracy and robustness.
In particular, we achieve the state-of-the-art accuracy on Chalearn2013 gesture
data by combining simple path transformation layers with the Logsig-RNN. Codes
are available at this https URL.

    

### [[2009.11459] Robust Finite-State Controllers for Uncertain POMDPs](http://arxiv.org/abs/2009.11459)


  Uncertain partially observable Markov decision processes (uPOMDPs) allow the
probabilistic transition and observation functions of standard POMDPs to belong
to a so-called uncertainty set. Such uncertainty, referred to as epistemic
uncertainty, captures uncountable sets of probability distributions caused by,
for instance, a lack of data available. We develop an algorithm to compute
finite-memory policies for uPOMDPs that robustly satisfy specifications against
any admissible distribution. In general, computing such policies is
theoretically and practically intractable. We provide an efficient solution to
this problem in four steps. (1) We state the underlying problem as a nonconvex
optimization problem with infinitely many constraints. (2) A dedicated
dualization scheme yields a dual problem that is still nonconvex but has
finitely many constraints. (3) We linearize this dual problem and (4) solve the
resulting finite linear program to obtain locally optimal solutions to the
original problem. The resulting problem formulation is exponentially smaller
than those resulting from existing methods. We demonstrate the applicability of
our algorithm using large instances of an aircraft collision-avoidance scenario
and a novel spacecraft motion planning case study.

    

### [[2111.00082] PiDRAM: A Holistic End-to-end FPGA-based Framework for Processing-in-DRAM](http://arxiv.org/abs/2111.00082)


  Processing-using-memory (PuM) techniques leverage the analog operation of
memory cells to perform computation. Several recent works have demonstrated PuM
techniques in off-the-shelf DRAM devices. Since DRAM is the dominant memory
technology as main memory in current computing systems, these PuM techniques
represent an opportunity for alleviating the data movement bottleneck at very
low cost. However, system integration of PuM techniques imposes non-trivial
challenges that are yet to be solved. Design space exploration of potential
solutions to the PuM integration challenges requires appropriate tools to
develop necessary hardware and software components. Unfortunately, current
specialized DRAM-testing platforms, or system simulators do not provide the
flexibility and/or the holistic system view that is necessary to deal with PuM
integration challenges.
We design and develop PiDRAM, the first flexible end-to-end framework that
enables system integration studies and evaluation of real PuM techniques.
PiDRAM provides software and hardware components to rapidly integrate PuM
techniques across the whole system software and hardware stack (e.g., necessary
modifications in the operating system, memory controller). We implement PiDRAM
on an FPGA-based platform along with an open-source RISC-V system. Using
PiDRAM, we implement and evaluate two state-of-the-art PuM techniques: in-DRAM
(i) copy and initialization, (ii) true random number generation. Our results
show that the in-memory copy and initialization techniques can improve the
performance of bulk copy operations by 12.6x and bulk initialization operations
by 14.6x on a real system. Implementing the true random number generator
requires only 190 lines of Verilog and 74 lines of C code using PiDRAM's
software and hardware components.

    

### [[2011.00448] Mitigating Write Disturbance Errors of Phase-Change Memory as In-Module Approach](http://arxiv.org/abs/2011.00448)


  With the growing demand for technology scaling and storage capacity in server
systems to support high-performance computing, phase-change memory (PCM) has
garnered attention as the next-generation non-volatile memory to satisfy these
requirements. However, write disturbance error (WDE) appears as a serious
reliability problem preventing PCM from general commercialization. WDE occurs
on the neighboring cells of a written cell due to heat dissipation. Previous
studies for the prevention of WDEs are based on the write cache or
verify-n-correction while they often suffer from significant area overhead and
performance degradation, making it unsuitable for high-performance computing.
Therefore, an on-demand correction is required to minimize the performance
overhead. In this paper, an in-module disturbance barrier (IMDB) mitigating
WDEs is proposed. IMDB includes two sets of SRAMs into two levels and evicts
entries with a policy that leverages the characteristics of WDE. In this work,
the comparator dedicated to the replacement policy requires significant
hardware resources and latency. Thus, an approximate comparator is designed to
reduce the area and latency considerably. Furthermore, the exploration of
architecture parameters is conducted to obtain cost-effective design. The
proposed work significantly reduces WDEs without a noticeable speed degradation
and additional energy consumption compared to previous methods.

    

### [[2105.09666] On the Optimization of Behavioral Logic Locking for High-Level Synthesis](http://arxiv.org/abs/2105.09666)


  The globalization of the electronics supply chain requires effective methods
to thwart reverse engineering and IP theft. Logic locking is a promising
solution, but there are still several open concerns. First, even when applied
at a higher level of abstraction, locking has significant overhead without
improving the security metric. Second, optimizing a security metric is
application-dependent and designers must evaluate and compare alternative
solutions. We propose a framework to optimize the use of behavioral locking
during the high-level synthesis (HLS) of IP cores. Our method operates on
chip's specification (before HLS) and it is compatible with all HLS tools,
complementing industrial EDA flows. The framework supports different
meta-heuristics to explore the design space and to select points to lock
automatically. Our method optimizes a given security metric better than
topological locking: 1) we always identify a valid solution that optimizes the
security metric; 2) we minimize the number of bits used for locking; and 3) we
make a better use of hardware resources.

    

### [[2111.00339] Constraint-Aware Trajectory for Drone Delivery Services](http://arxiv.org/abs/2111.00339)


  Drones are becoming a novel means for delivery services. We present a
demonstration of drone delivery services in a skyway network that uses the
service paradigm. A set of experiments is conducted using Crazyflie drones to
collect the data on various positions of drones, wind speed, wind direction,
and battery consumption. We run the experiments for a range of flight patterns
including linear, rectangular, and triangular shapes.

    

### [[2111.00579] RRFT: A Rank-Based Resource Aware Fault Tolerant Strategy for Cloud Platforms](http://arxiv.org/abs/2111.00579)


  The applications that are deployed in the cloud to provide services to the
users encompass a large number of interconnected dependent cloud components.
Multiple identical components are scheduled to run concurrently in order to
handle unexpected failures and provide uninterrupted service to the end user,
which introduces resource overhead problem for the cloud service provider.
Furthermore such resource-intensive fault tolerant strategies bring extra
monetary overhead to the cloud service provider and eventually to the cloud
users. In order to address these issues, a novel fault tolerant strategy based
on the significance level of each component is developed. The communication
topology among the application components, their historical performance,
failure rate, failure impact on other components, dependencies among them,
etc., are used to rank those application components to further decide on the
importance of one component over others. Based on the rank, a Markov Decision
Process (MDP) model is presented to determine the number of replicas that
varies from one component to another. A rigorous performance evaluation is
carried out using some of the most common practically useful metrics such as,
recovery time upon a fault, average number of components needed, number of
parallel components successfully executed, etc., to quote a few, with similar
component ranking and fault tolerant strategies. Simulation results demonstrate
that the proposed algorithm reduces the required number of virtual and physical
machines by approximately 10% and 4.2%, respectively, compared to other similar
algorithms.

    

### [[2102.12770] BeFaaS: An Application-Centric Benchmarking Framework for FaaS Platforms](http://arxiv.org/abs/2102.12770)


  Following the increasing interest and adoption of FaaS systems, benchmarking
frameworks for determining non-functional properties have also emerged. While
existing (microbenchmark) frameworks only evaluate single aspects of FaaS
platforms, a more holistic, application-driven approach is still missing. In
this paper, we design and present BeFaaS, an extensible application-centric
benchmarking framework for FaaS environments that focuses on the evaluation of
FaaS platforms through realistic and typical examples of FaaS applications.
BeFaaS includes a built-in e-commerce benchmark, is extensible for new workload
profiles and new platforms, supports federated benchmark runs in which the
benchmark application is distributed over multiple providers, and supports a
fine-grained result analysis. Our evaluation compares three major FaaS
providers in single cloud provider setups and shows that BeFaaS is capable of
running each benchmark automatically with minimal configuration effort and
providing detailed insights for each interaction.

    

### [[2103.06647] Compiler-Guided Throughput Scheduling for Many-core Machines](http://arxiv.org/abs/2103.06647)


  Typical schedulers in multi-tenancy environments make use of reactive,
feedback-oriented mechanisms based on performance counters to avoid resource
contention but suffer from detection lag and loss of performance. In this
paper, we address these limitations by exploring the utility of predictive
analysis through dynamic forecasting of applications' resource-heavy regions
during its execution. Our compiler framework classifies loops in programs and
leverages traditional compiler analysis along with learning mechanisms to
quantify their behaviour. Based on the predictability of their execution time,
it then inserts different types of beacons at their entry/exit points. The
information produced by beacons in multiple processes is aggregated and
analyzed by the proactive scheduler to respond to the anticipated workload
requirements. For throughput environments, we develop a framework that
demonstrates high-quality predictions and improvements in throughput over CFS
by 76.78% on an average and up to 3.2x on Amazon Graviton2 Machine on
consolidated workloads across 45 benchmarks.

    

### [[2105.13980] Coloring Trees in Massively Parallel Computation](http://arxiv.org/abs/2105.13980)


  We present $O(\log^2 \log n)$ time 3-coloring, maximal independent set and
maximal matching algorithms for trees in the Massively Parallel Computation
(MPC) model. Our algorithms are deterministic, apply to arbitrary-degree trees
and work in the low-space MPC model, where local memory is $O(n^\delta)$ for
$\delta \in (0,1)$ and global memory is $O(m)$. Our main result is the
3-coloring algorithm, which contrasts the randomized, state-of-the-art
4-coloring algorithm of Ghaffari, Grunau and Jin [DISC'20]. The maximal
independent set and maximal matching algorithms follow in $O(1)$ time after
obtaining the coloring. The key ingredient of our 3-coloring algorithm is an
$O(\log^2 \log n)$ time adaptation of the rake-and-compress tree decomposition
used by Chang and Pettie [FOCS'17], and established by Miller and Reif. When
restricting our attention to trees of constant degree, we bring the runtime
down to $O(\log \log n)$.

    

### [[2111.00003] A New Algorithm based on Extent Bit-array for Computing Formal Concepts](http://arxiv.org/abs/2111.00003)


  The emergence of Formal Concept Analysis (FCA) as a data analysis technique
has increased the need for developing algorithms which can compute formal
concepts quickly. The current efficient algorithms for FCA are variants of the
Close-By-One (CbO) algorithm, such as In-Close2, In-Close3 and In-Close4, which
are all based on horizontal storage of contexts. In this paper, based on
algorithm In-Close4, a new algorithm based on the vertical storage of contexts,
called In-Close5, is proposed, which can significantly reduce both the time
complexity and space complexity of algorithm In-Close4. Technically, the new
algorithm stores both context and extent of a concept as a vertical bit-array,
while within In-Close4 algorithm the context is stored only as a horizontal
bit-array, which is very slow in finding the intersection of two extent sets.
Experimental results demonstrate that the proposed algorithm is much more
effective than In-Close4 algorithm, and it also has a broader scope of
applicability in computing formal concept in which one can solve the problems
that cannot be solved by the In-Close4 algorithm.

    

### [[2111.00004] Granule Description based on Compound Concepts](http://arxiv.org/abs/2111.00004)


  Concise granule descriptions for describable granules and approaching
description methods for indescribable granules are challenging and important
issues in granular computing. The concept with only common attributes has been
frequently studied. To investigate the granules with some special needs, we
propose two new types of compound concepts in this paper: bipolar concept and
common-and-necessary concept. Based on the definitions of concept-forming
operations, the logical formulas are derived for each of the following types of
concepts: formal concept, three-way concept, object oriented concept, bipolar
concept and common-and-necessary concept. Furthermore, by utilizing the logical
relationship among various concepts, we have derived concise and unified
equivalent conditions for describable granules and approaching description
methods for indescribable granules for all five kinds of concepts.

    

### [[2111.00005] Concept and Attribute Reduction Based on Rectangle Theory of Formal Concept](http://arxiv.org/abs/2111.00005)


  Based on rectangle theory of formal concept and set covering theory, the
concept reduction preserving binary relations is investigated in this paper. It
is known that there are three types of formal concepts: core concepts, relative
necessary concepts and unnecessary concepts. First, we present the new judgment
results for relative necessary concepts and unnecessary concepts. Second, we
derive the bounds for both the maximum number of relative necessary concepts
and the maximum number of unnecessary concepts and it is a difficult problem as
either in concept reduction preserving binary relations or attribute reduction
of decision formal contexts, the computation of formal contexts from formal
concepts is a challenging problem. Third, based on rectangle theory of formal
concept, a fast algorithm for reducing attributes while preserving the
extensions for a set of formal concepts is proposed using the extension
bit-array technique, which allows multiple context cells to be processed by a
single 32-bit or 64-bit operator. Technically, the new algorithm could store
both formal context and extent of a concept as bit-arrays, and we can use
bit-operations to process set operations "or" as well as "and". One more merit
is that the new algorithm does not need to consider other concepts in the
concept lattice, thus the algorithm is explicit to understand and fast.
Experiments demonstrate that the new algorithm is effective in the computation
of attribute reductions.

    

### [[2111.00052] Diagnosing Web Data of ICTs to Provide Focused Assistance in Agricultural Adoptions](http://arxiv.org/abs/2111.00052)


  The past decade has witnessed a rapid increase in technology ownership across
rural areas of India, signifying the potential for ICT initiatives to empower
rural households. In our work, we focus on the web infrastructure of one such
ICT - Digital Green that started in 2008. Following a participatory approach
for content production, Digital Green disseminates instructional agricultural
videos to smallholder farmers via human mediators to improve the adoption of
farming practices. Their web-based data tracker, CoCo, captures data related to
these processes, storing the attendance and adoption logs of over 2.3 million
farmers across three continents and twelve countries. Using this data, we model
the components of the Digital Green ecosystem involving the past
attendance-adoption behaviours of farmers, the content of the videos screened
to them and their demographic features across five states in India. We use
statistical tests to identify different factors which distinguish farmers with
higher adoption rates to understand why they adopt more than others. Our
research finds that farmers with higher adoption rates adopt videos of shorter
duration and belong to smaller villages. The co-attendance and co-adoption
networks of farmers indicate that they greatly benefit from past adopters of a
video from their village and group when it comes to adopting practices from the
same video. Following our analysis, we model the adoption of practices from a
video as a prediction problem to identify and assist farmers who might face
challenges in adoption in each of the five states. We experiment with different
model architectures and achieve macro-f1 scores ranging from 79% to 89% using a
Random Forest classifier. Finally, we measure the importance of different
features using SHAP values and provide implications for improving the adoption
rates of nearly a million farmers across five states in India.

    

### [[2111.00071] ReSkin: versatile, replaceable, lasting tactile skins](http://arxiv.org/abs/2111.00071)


  Soft sensors have continued growing interest in robotics, due to their
ability to enable both passive conformal contact from the material properties
and active contact data from the sensor properties. However, the same
properties of conformal contact result in faster deterioration of soft sensors
and larger variations in their response characteristics over time and across
samples, inhibiting their ability to be long-lasting and replaceable. ReSkin is
a tactile soft sensor that leverages machine learning and magnetic sensing to
offer a low-cost, diverse and compact solution for long-term use. Magnetic
sensing separates the electronic circuitry from the passive interface, making
it easier to replace interfaces as they wear out while allowing for a wide
variety of form factors. Machine learning allows us to learn sensor response
models that are robust to variations across fabrication and time, and our
self-supervised learning algorithm enables finer performance enhancement with
small, inexpensive data collection procedures. We believe that ReSkin opens the
door to more versatile, scalable and inexpensive tactile sensation modules than
existing alternatives.

    

### [[2111.00086] Measuring a Texts Fairness Dimensions Using Machine Learning Based on Social Psychological Factors](http://arxiv.org/abs/2111.00086)


  Fairness is a principal social value that can be observed in civilisations
around the world. A manifestations of this is in social agreements, often
described in texts, such as contracts. Yet, despite the prevalence of such, a
fairness metric for texts describing a social act remains wanting. To address
this, we take a step back to consider the problem based on first principals.
Instead of using rules or templates, we utilise social psychology literature to
determine the principal factors that humans use when making a fairness
assessment. We then attempt to digitise these using word embeddings into a
multi-dimensioned sentence level fairness perceptions vector to serve as an
approximation for these fairness perceptions. The method leverages a pro-social
bias within word embeddings, for which we obtain an F1= 81.0. A second
approach, using PCA and ML based on the said fairness approximation vector
produces an F1 score of 86.2. We details improvements that can be made in the
methodology to incorporate the projection of sentence embedding on to a
subspace representation of fairness.

    

### [[2111.00107] The Golden Rule as a Heuristic to Measure the Fairness of Texts Using Machine Learning](http://arxiv.org/abs/2111.00107)


  To treat others as one would wish to be treated is a common formulation of
the Golden Rule (GR). Yet, despite its prevalence as an axiom throughout
history, no digitisation of the moral philosophy exists. In this paper we
consider how to digitise it so that it may be used to measure sentences such
as: the boy harmed the girl, and categorise them as fair or unfair. A review
and reply to criticisms of the GR is made. We share the code for the
digitisation of the GR, and test it with a list of sentences. Implementing two
approaches, one using the USE, and a second using ALBERT. We find F1 scores of
78.0, 85.0, respectively. A suggestion of how the technology may be implemented
to avoid unfair biases in word embeddings is made - given that individuals
would typically not wish to be on the receiving end of an unfair act, such as
racism, irrespective of whether the corpus being used deems such discrimination
as praiseworthy.

    

### [[2111.00166] Advanced Algorithms of Collision Free Navigation and Flocking for Autonomous UAVs](http://arxiv.org/abs/2111.00166)


  Unmanned aerial vehicles (UAVs) have become very popular for many military
and civilian applications including in agriculture, construction, mining,
environmental monitoring, etc. A desirable feature for UAVs is the ability to
navigate and perform tasks autonomously with least human interaction. This is a
very challenging problem due to several factors such as the high complexity of
UAV applications, operation in harsh environments, limited payload and onboard
computing power and highly nonlinear dynamics. The work presented in this
report contributes towards the state-of-the-art in UAV control for safe
autonomous navigation and motion coordination of multi-UAV systems. The first
part of this report deals with single-UAV systems. The complex problem of
three-dimensional (3D) collision-free navigation in unknown/dynamic
environments is addressed. To that end, advanced 3D reactive control strategies
are developed adopting the sense-and-avoid paradigm to produce quick reactions
around obstacles. A special case of navigation in 3D unknown confined
environments (i.e. tunnel-like) is also addressed. General 3D kinematic models
are considered in the design which makes these methods applicable to different
UAV types in addition to underwater vehicles. Moreover, different
implementation methods for these strategies with quadrotor-type UAVs are also
investigated considering UAV dynamics in the control design. Practical
experiments and simulations were carried out to analyze the performance of the
developed methods. The second part of this report addresses safe navigation for
multi-UAV systems. Distributed motion coordination methods of multi-UAV systems
for flocking and 3D area coverage are developed. These methods offer good
computational cost for large-scale systems. Simulations were performed to
verify the performance of these methods considering systems with different
sizes.

    

### [[2111.00180] Hierarchical Heterogeneous Graph Representation Learning for Short Text Classification](http://arxiv.org/abs/2111.00180)


  Short text classification is a fundamental task in natural language
processing. It is hard due to the lack of context information and labeled data
in practice. In this paper, we propose a new method called SHINE, which is
based on graph neural network (GNN), for short text classification. First, we
model the short text dataset as a hierarchical heterogeneous graph consisting
of word-level component graphs which introduce more semantic and syntactic
information. Then, we dynamically learn a short document graph that facilitates
effective label propagation among similar short texts. Thus, compared with
existing GNN-based methods, SHINE can better exploit interactions between nodes
of the same types and capture similarities between short texts. Extensive
experiments on various benchmark short text datasets show that SHINE
consistently outperforms state-of-the-art methods, especially with fewer
labels.

    

### [[2111.00191] How should human translation coexist with NMT? Efficient tool for building high quality parallel corpus](http://arxiv.org/abs/2111.00191)


  This paper proposes a tool for efficiently constructing high-quality parallel
corpora with minimizing human labor and making this tool publicly available.
Our proposed construction process is based on neural machine translation (NMT)
to allow for it to not only coexist with human translation, but also improve
its efficiency by combining data quality control with human translation in a
data-centric approach.

    

### [[2111.00200] AutoDrone: Shortest Optimized Obstacle-Free Path Planning for Autonomous Drones](http://arxiv.org/abs/2111.00200)


  With technological advancement, drone has emerged as unmanned aerial vehicle
that can be controlled by humans to fly or reach a destination. This may be
autonomous as well, where the drone itself is intelligent enough to find a
shortest obstacle-free path to reach the destination from a designated source.
Be it a planned smart city or even a wreckage site affected by natural
calamity, we may imagine the buildings, any surface-erected structure or other
blockage as obstacles for the drone to fly in a direct line-of-sight path. So,
the whole bird's eye-view of the landscape can be transformed to a graph of
grid-cells, where some are occupied to indicate the obstacles and some are free
to indicate the free path. The autonomous drone (AutoDrone) will be able to
find out the shortest hindrance-free path while travelling in two-dimensional
space and move from one place to another. In this paper, we propose a method to
find out an obstacle-free shortest path in the coordinate system guided by GPS.
This can be especially beneficial in rescue operations and fast delivery or
pick-up in an energy-efficient way, where our algorithm will help in finding
out the shortest path and angle along which it should fly. Our work shows
different scenarios to path-tracing, through the shortest feasible path
computed by the autonomous drone.

    

### [[2111.00201] A Comparative Review of Recent Few-Shot Object Detection Algorithms](http://arxiv.org/abs/2111.00201)


  Few-shot object detection, learning to adapt to the novel classes with a few
labeled data, is an imperative and long-lasting problem due to the inherent
long-tail distribution of real-world data and the urgent demands to cut costs
of data collection and annotation. Recently, some studies have explored how to
use implicit cues in extra datasets without target-domain supervision to help
few-shot detectors refine robust task notions. This survey provides a
comprehensive overview from current classic and latest achievements for
few-shot object detection to future research expectations from manifold
perspectives. In particular, we first propose a data-based taxonomy of the
training data and the form of corresponding supervision which are accessed
during the training stage. Following this taxonomy, we present a significant
review of the formal definition, main challenges, benchmark datasets,
evaluation metrics, and learning strategies. In addition, we present a detailed
investigation of how to interplay the object detection methods to develop this
issue systematically. Finally, we conclude with the current status of few-shot
object detection, along with potential research directions for this field.

    

### [[2111.00229] Fuzzy Conceptual Graphs: a comparative discussion](http://arxiv.org/abs/2111.00229)


  Conceptual Graphs (CG) are a graph-based knowledge representation and
reasoning formalism; fuzzy Conceptual Graphs (fCG) constitute an extension that
enriches their expressiveness, exploiting the fuzzy set theory so as to relax
their constraints at various levels. This paper proposes a comparative study of
existing approaches over their respective advantages and possible limitations.
The discussion revolves around three axes: (a) Critical view of each approach
and comparison with previous propositions from the state of the art; (b)
Presentation of the many possible interpretations of each definition to
illustrate its potential and its limits; (c) Clarification of the part of CG
impacted by the definition as well as the relaxed constraint.

    

### [[2111.00278] A Decentralized Reinforcement Learning Framework for Efficient Passage of Emergency Vehicles](http://arxiv.org/abs/2111.00278)


  Emergency vehicles (EMVs) play a critical role in a city's response to
time-critical events such as medical emergencies and fire outbreaks. The
existing approaches to reduce EMV travel time employ route optimization and
traffic signal pre-emption without accounting for the coupling between route
these two subproblems. As a result, the planned route often becomes suboptimal.
In addition, these approaches also do not focus on minimizing disruption to the
overall traffic flow. To address these issues, we introduce EMVLight in this
paper. This is a decentralized reinforcement learning (RL) framework for
simultaneous dynamic routing and traffic signal control. EMVLight extends
Dijkstra's algorithm to efficiently update the optimal route for an EMV in
real-time as it travels through the traffic network. Consequently, the
decentralized RL agents learn network-level cooperative traffic signal phase
strategies that reduce EMV travel time and the average travel time of non-EMVs
in the network. We have carried out comprehensive experiments with synthetic
and real-world maps to demonstrate this benefit. Our results show that EMVLight
outperforms benchmark transportation engineering techniques as well as existing
RL-based traffic signal control methods.

    

### [[2111.00293] Long-Range Route-planning for Autonomous Vehicles in the Polar Oceans](http://arxiv.org/abs/2111.00293)


  There is an increasing demand for piloted autonomous underwater vehicles
(AUVs) to operate in polar ice conditions. At present, AUVs are deployed from
ships and directly human-piloted in these regions, entailing a high carbon cost
and limiting the scope of operations. A key requirement for long-term
autonomous missions is a long-range route planning capability that is aware of
the changing ice conditions. In this paper we address the problem of automating
long-range route-planning for AUVs operating in the Southern Ocean. We present
the route-planning method and results showing that efficient, ice-avoiding,
long-distance traverses can be planned.

    

### [[2111.00309] TargetUM: Targeted High-Utility Itemset Querying](http://arxiv.org/abs/2111.00309)


  Traditional high-utility itemset mining (HUIM) aims to determine all
high-utility itemsets (HUIs) that satisfy the minimum utility threshold
(\textit{minUtil}) in transaction databases. However, in most applications, not
all HUIs are interesting because only specific parts are required. Thus,
targeted mining based on user preferences is more important than traditional
mining tasks. This paper is the first to propose a target-based HUIM problem
and to provide a clear formulation of the targeted utility mining task in a
quantitative transaction database. A tree-based algorithm known as Target-based
high-Utility iteMset querying using (TargetUM) is proposed. The algorithm uses
a lexicographic querying tree and three effective pruning strategies to improve
the mining efficiency. We implemented experimental validation on several real
and synthetic databases, and the results demonstrate that the performance of
\textbf{TargetUM} is satisfactory, complete, and correct. Finally, owing to the
lexicographic querying tree, the database no longer needs to be scanned
repeatedly for multiple queries.

    

### [[2111.00310] EmpBot: A T5-based Empathetic Chatbot focusing on Sentiments](http://arxiv.org/abs/2111.00310)


  In this paper, we introduce EmpBot: an end-to-end empathetic chatbot.
Empathetic conversational agents should not only understand what is being
discussed, but also acknowledge the implied feelings of the conversation
partner and respond appropriately. To this end, we propose a method based on a
transformer pretrained language model (T5). Specifically, during finetuning we
propose to use three objectives: response language modeling, sentiment
understanding, and empathy forcing. The first objective is crucial for
generating relevant and coherent responses, while the next ones are significant
for acknowledging the sentimental state of the conversational partner and for
favoring empathetic responses. We evaluate our model on the EmpatheticDialogues
dataset using both automated metrics and human evaluation. The inclusion of the
sentiment understanding and empathy forcing auxiliary losses favor empathetic
responses, as human evaluation results indicate, comparing with the current
state-of-the-art.

    

### [[2111.00312] 3DP3: 3D Scene Perception via Probabilistic Programming](http://arxiv.org/abs/2111.00312)


  We present 3DP3, a framework for inverse graphics that uses inference in a
structured generative model of objects, scenes, and images. 3DP3 uses (i) voxel
models to represent the 3D shape of objects, (ii) hierarchical scene graphs to
decompose scenes into objects and the contacts between them, and (iii) depth
image likelihoods based on real-time graphics. Given an observed RGB-D image,
3DP3's inference algorithm infers the underlying latent 3D scene, including the
object poses and a parsimonious joint parametrization of these poses, using
fast bottom-up pose proposals, novel involutive MCMC updates of the scene graph
structure, and, optionally, neural object detectors and pose estimators. We
show that 3DP3 enables scene understanding that is aware of 3D shape,
occlusion, and contact structure. Our results demonstrate that 3DP3 is more
accurate at 6DoF object pose estimation from real images than deep learning
baselines and shows better generalization to challenging scenes with novel
viewpoints, contact, and partial observability.

    

### [[2111.00345] Multi-Agent Advisor Q-Learning](http://arxiv.org/abs/2111.00345)


  In the last decade, there have been significant advances in multi-agent
reinforcement learning (MARL) but there are still numerous challenges, such as
high sample complexity and slow convergence to stable policies, that need to be
overcome before wide-spread deployment is possible. However, many real-world
environments already, in practice, deploy sub-optimal or heuristic approaches
for generating policies. An interesting question which arises is how to best
use such approaches as advisors to help improve reinforcement learning in
multi-agent domains. In this paper, we provide a principled framework for
incorporating action recommendations from online sub-optimal advisors in
multi-agent settings. We describe the problem of ADvising Multiple Intelligent
Reinforcement Agents (ADMIRAL) in nonrestrictive general-sum stochastic game
environments and present two novel Q-learning based algorithms: ADMIRAL -
Decision Making (ADMIRAL-DM) and ADMIRAL - Advisor Evaluation (ADMIRAL-AE),
which allow us to improve learning by appropriately incorporating advice from
an advisor (ADMIRAL-DM), and evaluate the effectiveness of an advisor
(ADMIRAL-AE). We analyze the algorithms theoretically and provide fixed-point
guarantees regarding their learning in general-sum stochastic games.
Furthermore, extensive experiments illustrate that these algorithms: can be
used in a variety of environments, have performances that compare favourably to
other related baselines, can scale to large state-action spaces, and are robust
to poor advice from advisors.

    

### [[2111.00375] Conical Classification For Computationally Efficient One-Class Topic Determination](http://arxiv.org/abs/2111.00375)


  As the Internet grows in size, so does the amount of text based information
that exists. For many application spaces it is paramount to isolate and
identify texts that relate to a particular topic. While one-class
classification would be ideal for such analysis, there is a relative lack of
research regarding efficient approaches with high predictive power. By noting
that the range of documents we wish to identify can be represented as positive
linear combinations of the Vector Space Model representing our text, we propose
Conical classification, an approach that allows us to identify if a document is
of a particular topic in a computationally efficient manner. We also propose
Normal Exclusion, a modified version of Bi-Normal Separation that makes it more
suitable within the one-class classification context. We show in our analysis
that our approach not only has higher predictive power on our datasets, but is
also faster to compute.

    

### [[2111.00395] A robust single-pixel particle image velocimetry based on fully convolutional networks with cross-correlation embedded](http://arxiv.org/abs/2111.00395)


  Particle image velocimetry (PIV) is essential in experimental fluid dynamics.
In the current work, we propose a new velocity field estimation paradigm, which
achieves a synergetic combination of the deep learning method and the
traditional cross-correlation method. Specifically, the deep learning method is
used to optimize and correct a coarse velocity guess to achieve a
super-resolution calculation. And the cross-correlation method provides the
initial velocity field based on a coarse correlation with a large interrogation
window. As a reference, the coarse velocity guess helps with improving the
robustness of the proposed algorithm. This fully convolutional network with
embedded cross-correlation is named as CC-FCN. CC-FCN has two types of input
layers, one is for the particle images, and the other is for the initial
velocity field calculated using cross-correlation with a coarse resolution.
Firstly, two pyramidal modules extract features of particle images and initial
velocity field respectively. Then the fusion module appropriately fuses these
features. Finally, CC-FCN achieves the super-resolution calculation through a
series of deconvolution layers to obtain the single-pixel velocity field. As
the supervised learning strategy is considered, synthetic data sets including
ground-truth fluid motions are generated to train the network parameters.
Synthetic and real experimental PIV data sets are used to test the trained
neural network in terms of accuracy, precision, spatial resolution and
robustness. The test results show that these attributes of CC-FCN are further
improved compared with those of other tested PIV algorithms. The proposed model
could therefore provide competitive and robust estimations for PIV experiments.

    

### [[2111.00398] A Simple Approach to Image Tilt Correction with Self-Attention MobileNet for Smartphones](http://arxiv.org/abs/2111.00398)


  The main contributions of our work are two-fold. First, we present a
Self-Attention MobileNet, called SA-MobileNet Network that can model long-range
dependencies between the image features instead of processing the local region
as done by standard convolutional kernels. SA-MobileNet contains self-attention
modules integrated with the inverted bottleneck blocks of the MobileNetV3 model
which results in modeling of both channel-wise attention and spatial attention
of the image features and at the same time introduce a novel self-attention
architecture for low-resource devices. Secondly, we propose a novel training
pipeline for the task of image tilt detection. We treat this problem in a
multi-label scenario where we predict multiple angles for a tilted input image
in a narrow interval of range 1-2 degrees, depending on the dataset used. This
process induces an implicit correlation between labels without any
computational overhead of the second or higher-order methods in multi-label
learning. With the combination of our novel approach and the architecture, we
present state-of-the-art results on detecting the image tilt angle on mobile
devices as compared to the MobileNetV3 model. Finally, we establish that
SA-MobileNet is more accurate than MobileNetV3 on SUN397, NYU-V1, and ADE20K
datasets by 6.42%, 10.51%, and 9.09% points respectively, and faster by at
least 4 milliseconds on Snapdragon 750 Octa-core.

    

### [[2111.00419] Interpreting Deep Knowledge Tracing Model on EdNet Dataset](http://arxiv.org/abs/2111.00419)


  With more deep learning techniques being introduced into the knowledge
tracing domain, the interpretability issue of the knowledge tracing models has
aroused researchers' attention. Our previous study(Lu et al. 2020) on building
and interpreting the KT model mainly adopts the ASSISTment dataset(Feng,
Heffernan, and Koedinger 2009),, whose size is relatively small. In this work,
we perform the similar tasks but on a large and newly available dataset, called
EdNet(Choi et al. 2020). The preliminary experiment results show the
effectiveness of the interpreting techniques, while more questions and tasks
are worthy to be further explored and accomplished.

    

### [[2111.00424] Graph Tree Neural Networks](http://arxiv.org/abs/2111.00424)


  Graph neural networks (GNNs) have recently shown good performance in various
fields. In this paper, we propose graph tree neural networks (GTNNs) designed
to solve the problems of existing networks by analyzing the structure of human
neural networks. In GTNNs, information units are related to the form of a graph
and then they become a bigger unit of information again and have a relationship
with other information units. At this point, the unit of information is a set
of neurons, and we can express it as a vector with GTNN. Defining the starting
and ending points in a single graph is difficult, and a tree cannot express the
relationship among sibling nodes. However, a graph tree can be expressed using
leaf and root nodes as its starting and ending points and the relationship
among sibling nodes. Depth-first convolution (DFC) encodes the interaction
result from leaf nodes to the root node in a bottom-up approach, and
depth-first deconvolution (DFD) decodes the interaction result from the root
node to the leaf nodes in a top-down approach. GTNN is data-driven learning in
which the number of convolutions varies according to the depth of the tree.
Moreover, learning features of different types together is possible.
Supervised, unsupervised, and semi-supervised learning using graph tree
recursive neural network (GTR) , graph tree recursive attention networks
(GTRAs), and graph tree recursive autoencoders (GTRAEs) are introduced in this
paper. We experimented with a simple toy test with source code dataset.

    

### [[2111.00490] DSC-IITISM at FinCausal 2021: Combining POS tagging with Attention-based Contextual Representations for Identifying Causal Relationships in Financial Documents](http://arxiv.org/abs/2111.00490)


  Causality detection draws plenty of attention in the field of Natural
Language Processing and linguistics research. It has essential applications in
information retrieval, event prediction, question answering, financial
analysis, and market research. In this study, we explore several methods to
identify and extract cause-effect pairs in financial documents using
transformers. For this purpose, we propose an approach that combines POS
tagging with the BIO scheme, which can be integrated with modern transformer
models to address this challenge of identifying causality in a given text. Our
best methodology achieves an F1-Score of 0.9551, and an Exact Match Score of
0.8777 on the blind test in the FinCausal-2021 Shared Task at the FinCausal
2021 Workshop.

    

### [[2111.00506] PnPOOD : Out-Of-Distribution Detection for Text Classification via Plug andPlay Data Augmentation](http://arxiv.org/abs/2111.00506)


  While Out-of-distribution (OOD) detection has been well explored in computer
vision, there have been relatively few prior attempts in OOD detection for NLP
classification. In this paper we argue that these prior attempts do not fully
address the OOD problem and may suffer from data leakage and poor calibration
of the resulting models. We present PnPOOD, a data augmentation technique to
perform OOD detection via out-of-domain sample generation using the recently
proposed Plug and Play Language Model (Dathathri et al., 2020). Our method
generates high quality discriminative samples close to the class boundaries,
resulting in accurate OOD detection at test time. We demonstrate that our model
outperforms prior models on OOD sample detection, and exhibits lower
calibration error on the 20 newsgroup text and Stanford Sentiment Treebank
dataset (Lang, 1995; Socheret al., 2013). We further highlight an important
data leakage issue with datasets used in prior attempts at OOD detection, and
share results on a new dataset for OOD detection that does not suffer from the
same problem.

    

### [[2111.00528] Calibrating the Dice loss to handle neural network overconfidence for biomedical image segmentation](http://arxiv.org/abs/2111.00528)


  The Dice similarity coefficient (DSC) is both a widely used metric and loss
function for biomedical image segmentation due to its robustness to class
imbalance. However, it is well known that the DSC loss is poorly calibrated,
resulting in overconfident predictions that cannot be usefully interpreted in
biomedical and clinical practice. Performance is often the only metric used to
evaluate segmentations produced by deep neural networks, and calibration is
often neglected. However, calibration is important for translation into
biomedical and clinical practice, providing crucial contextual information to
model predictions for interpretation by scientists and clinicians. In this
study, we identify poor calibration as an emerging challenge of deep learning
based biomedical image segmentation. We provide a simple yet effective
extension of the DSC loss, named the DSC++ loss, that selectively modulates the
penalty associated with overconfident, incorrect predictions. As a standalone
loss function, the DSC++ loss achieves significantly improved calibration over
the conventional DSC loss across five well-validated open-source biomedical
imaging datasets. Similarly, we observe significantly improved when integrating
the DSC++ loss into four DSC-based loss functions. Finally, we use softmax
thresholding to illustrate that well calibrated outputs enable tailoring of
precision-recall bias, an important post-processing technique to adapt the
model predictions to suit the biomedical or clinical task. The DSC++ loss
overcomes the major limitation of the DSC, providing a suitable loss function
for training deep learning segmentation models for use in biomedical and
clinical practice.

    

### [[2111.00533] Incorporating Boundary Uncertainty into loss functions for biomedical image segmentation](http://arxiv.org/abs/2111.00533)


  Manual segmentation is used as the gold-standard for evaluating neural
networks on automated image segmentation tasks. Due to considerable
heterogeneity in shapes, colours and textures, demarcating object boundaries is
particularly difficult in biomedical images, resulting in significant inter and
intra-rater variability. Approaches, such as soft labelling and distance
penalty term, apply a global transformation to the ground truth, redefining the
loss function with respect to uncertainty. However, global operations are
computationally expensive, and neither approach accurately reflects the
uncertainty underlying manual annotation. In this paper, we propose the
Boundary Uncertainty, which uses morphological operations to restrict soft
labelling to object boundaries, providing an appropriate representation of
uncertainty in ground truth labels, and may be adapted to enable robust model
training where systematic manual segmentation errors are present. We
incorporate Boundary Uncertainty with the Dice loss, achieving consistently
improved performance across three well-validated biomedical imaging datasets
compared to soft labelling and distance-weighted penalty. Boundary Uncertainty
not only more accurately reflects the segmentation process, but it is also
efficient, robust to segmentation errors and exhibits better generalisation.

    

### [[2111.00534] Focal Attention Networks: optimising attention for biomedical image segmentation](http://arxiv.org/abs/2111.00534)


  In recent years, there has been increasing interest to incorporate attention
into deep learning architectures for biomedical image segmentation. The modular
design of attention mechanisms enables flexible integration into convolutional
neural network architectures, such as the U-Net. Whether attention is
appropriate to use, what type of attention to use, and where in the network to
incorporate attention modules, are all important considerations that are
currently overlooked. In this paper, we investigate the role of the Focal
parameter in modulating attention, revealing a link between attention in loss
functions and networks. By incorporating a Focal distance penalty term, we
extend the Unified Focal loss framework to include boundary-based losses.
Furthermore, we develop a simple and interpretable, dataset and model-specific
heuristic to integrate the Focal parameter into the Squeeze-and-Excitation
block and Attention Gate, achieving optimal performance with fewer number of
attention modules on three well-validated biomedical imaging datasets,
suggesting judicious use of attention modules results in better performance and
efficiency.

    

### [[2111.00539] Cross-Domain Reasoning via Template Filling](http://arxiv.org/abs/2111.00539)


  In this paper, we explore the ability of sequence to sequence models to
perform cross-domain reasoning. Towards this, we present a
prompt-template-filling approach to enable sequence to sequence models to
perform cross-domain reasoning. We also present a case-study with commonsense
and health and well-being domains, where we study how prompt-template-filling
enables pretrained sequence to sequence models across domains. Our experiments
across several pretrained encoder-decoder models show that cross-domain
reasoning is challenging for current models. We also show an in-depth error
analysis and avenues for future research for reasoning across domains

    

### [[2111.00570] An Approach to Inference-Driven Dialogue Management within a Social Chatbot](http://arxiv.org/abs/2111.00570)


  We present a chatbot implementing a novel dialogue management approach based
on logical inference. Instead of framing conversation a sequence of response
generation tasks, we model conversation as a collaborative inference process in
which speakers share information to synthesize new knowledge in real time. Our
chatbot pipeline accomplishes this modelling in three broad stages. The first
stage translates user utterances into a symbolic predicate representation. The
second stage then uses this structured representation in conjunction with a
larger knowledge base to synthesize new predicates using efficient graph
matching. In the third and final stage, our bot selects a small subset of
predicates and translates them into an English response. This approach lends
itself to understanding latent semantics of user inputs, flexible initiative
taking, and responses that are novel and coherent with the dialogue context.

    

### [[2111.00572] What Went Wrong? Explaining Overall Dialogue Quality through Utterance-Level Impacts](http://arxiv.org/abs/2111.00572)


  Improving user experience of a dialogue system often requires intensive
developer effort to read conversation logs, run statistical analyses, and
intuit the relative importance of system shortcomings. This paper presents a
novel approach to automated analysis of conversation logs that learns the
relationship between user-system interactions and overall dialogue quality.
Unlike prior work on utterance-level quality prediction, our approach learns
the impact of each interaction from the overall user rating without
utterance-level annotation, allowing resultant model conclusions to be derived
on the basis of empirical evidence and at low cost. Our model identifies
interactions that have a strong correlation with the overall dialogue quality
in a chatbot setting. Experiments show that the automated analysis from our
model agrees with expert judgments, making this work the first to show that
such weakly-supervised learning of utterance-level quality prediction is highly
achievable.

    

### [[2111.00585] JEDAI Explains Decision-Making AI](http://arxiv.org/abs/2111.00585)


  This paper presents JEDAI, an AI system designed for outreach and educational
efforts aimed at non-AI experts. JEDAI features a novel synthesis of research
ideas from integrated task and motion planning and explainable AI. JEDAI helps
users create high-level, intuitive plans while ensuring that they will be
executable by the robot. It also provides users customized explanations about
errors and helps improve their understanding of AI planning as well as the
limits and capabilities of the underlying robot system.

    

### [[2111.00595] TorchXRayVision: A library of chest X-ray datasets and models](http://arxiv.org/abs/2111.00595)


  TorchXRayVision is an open source software library for working with chest
X-ray datasets and deep learning models. It provides a common interface and
common pre-processing chain for a wide set of publicly available chest X-ray
datasets. In addition, a number of classification and representation learning
models with different architectures, trained on different data combinations,
are available through the library to serve as baselines or feature extractors.

    

### [[2111.00604] Graph Embedding with Hierarchical Attentive Membership](http://arxiv.org/abs/2111.00604)


  The exploitation of graph structures is the key to effectively learning
representations of nodes that preserve useful information in graphs. A
remarkable property of graph is that a latent hierarchical grouping of nodes
exists in a global perspective, where each node manifests its membership to a
specific group based on the context composed by its neighboring nodes. Most
prior works ignore such latent groups and nodes' membership to different
groups, not to mention the hierarchy, when modeling the neighborhood structure.
Thus, they fall short of delivering a comprehensive understanding of the nodes
under different contexts in a graph. In this paper, we propose a novel
hierarchical attentive membership model for graph embedding, where the latent
memberships for each node are dynamically discovered based on its neighboring
context. Both group-level and individual-level attentions are performed when
aggregating neighboring states to generate node embeddings. We introduce
structural constraints to explicitly regularize the inferred memberships of
each node, such that a well-defined hierarchical grouping structure is
captured. The proposed model outperformed a set of state-of-the-art graph
embedding solutions on node classification and link prediction tasks in a
variety of graphs including citation networks and social networks. Qualitative
evaluations visualize the learned node embeddings along with the inferred
memberships, which proved the concept of membership hierarchy and enables
explainable embedding learning in graphs.

    

### [[2010.10041] Looking for Clues of Language in Multilingual BERT to Improve Cross-lingual Generalization](http://arxiv.org/abs/2010.10041)


  Token embeddings in multilingual BERT (m-BERT) contain both language and
semantic information. We find that the representation of a language can be
obtained by simply averaging the embeddings of the tokens of the language.
Given this language representation, we control the output languages of
multilingual BERT by manipulating the token embeddings, thus achieving
unsupervised token translation. We further propose a computationally cheap but
effective approach to improve the cross-lingual ability of m-BERT based on this
observation.

    

### [[2010.11003] Unsupervised Multiple Choices Question Answering: Start Learning from Basic Knowledge](http://arxiv.org/abs/2010.11003)


  In this paper, we study the possibility of almost unsupervised Multiple
Choices Question Answering (MCQA). Starting from very basic knowledge, MCQA
model knows that some choices have higher probabilities of being correct than
the others. The information, though very noisy, guides the training of an MCQA
model. The proposed method is shown to outperform the baseline approaches on
RACE and even comparable with some supervised learning approaches on MC500.

    

### [[2102.11764] Quantum Entropic Causal Inference](http://arxiv.org/abs/2102.11764)


  The class of problems in causal inference which seeks to isolate causal
correlations solely from observational data even without interventions has come
to the forefront of machine learning, neuroscience and social sciences. As new
large scale quantum systems go online, it opens interesting questions of
whether a quantum framework exists on isolating causal correlations without any
interventions on a quantum system. We put forth a theoretical framework for
merging quantum information science and causal inference by exploiting entropic
principles. At the root of our approach is the proposition that the true causal
direction minimizes the entropy of exogenous variables in a non-local hidden
variable theory. The proposed framework uses a quantum causal structural
equation model to build the connection between two fields: entropic causal
inference and the quantum marginal problem. First, inspired by the definition
of geometric quantum discord, we fill the gap between classical and quantum
conditional density matrices to define quantum causal models. Subsequently,
using a greedy approach, we develop a scalable algorithm for quantum entropic
causal inference unifying classical and quantum causality in a principled way.
We apply our proposed algorithm to an experimentally relevant scenario of
identifying the subsystem impacted by noise starting from an entangled state.
This successful inference on a synthetic quantum dataset can have practical
applications in identifying originators of malicious activity on future
multi-node quantum networks as well as quantum error correction. As quantum
datasets and systems grow in complexity, our framework can play a foundational
role in bringing observational causal inference from the classical to the
quantum domain.

    

### [[2103.02167] Touchless Palmprint Recognition based on 3D Gabor Template and Block Feature Refinement](http://arxiv.org/abs/2103.02167)


  With the growing demand for hand hygiene and convenience of use, palmprint
recognition with touchless manner made a great development recently, providing
an effective solution for person identification. Despite many efforts that have
been devoted to this area, it is still uncertain about the discriminative
ability of the contactless palmprint, especially for large-scale datasets. To
tackle the problem, in this paper, we build a large-scale touchless palmprint
dataset containing 2334 palms from 1167 individuals. To our best knowledge, it
is the largest contactless palmprint image benchmark ever collected with regard
to the number of individuals and palms. Besides, we propose a novel deep
learning framework for touchless palmprint recognition named 3DCPN (3D
Convolution Palmprint recognition Network) which leverages 3D convolution to
dynamically integrate multiple Gabor features. In 3DCPN, a novel variant of
Gabor filter is embedded into the first layer for enhancement of curve feature
extraction. With a well-designed ensemble scheme,low-level 3D features are then
convolved to extract high-level features. Finally on the top, we set a
region-based loss function to strengthen the discriminative ability of both
global and local descriptors. To demonstrate the superiority of our method,
extensive experiments are conducted on our dataset and other popular databases
TongJi and IITD, where the results show the proposed 3DCPN achieves
state-of-the-art or comparable performances.

    

### [[2104.11559] Optimizing small BERTs trained for German NER](http://arxiv.org/abs/2104.11559)


  Currently, the most widespread neural network architecture for training
language models is the so called BERT which led to improvements in various
Natural Language Processing (NLP) tasks. In general, the larger the number of
parameters in a BERT model, the better the results obtained in these NLP tasks.
Unfortunately, the memory consumption and the training duration drastically
increases with the size of these models. In this article, we investigate
various training techniques of smaller BERT models: We combine different
methods from other BERT variants like ALBERT, RoBERTa, and relative positional
encoding. In addition, we propose two new fine-tuning modifications leading to
better performance: Class-Start-End tagging and a modified form of Linear Chain
Conditional Random Fields. Furthermore, we introduce Whole-Word Attention which
reduces BERTs memory usage and leads to a small increase in performance
compared to classical Multi-Head-Attention. We evaluate these techniques on
five public German Named Entity Recognition (NER) tasks of which two are
introduced by this article.

    

### [[2105.07540] Deep learning for detecting pulmonary tuberculosis via chest radiography: an international study across 10 countries](http://arxiv.org/abs/2105.07540)


  Tuberculosis (TB) is a top-10 cause of death worldwide. Though the WHO
recommends chest radiographs (CXRs) for TB screening, the limited availability
of CXR interpretation is a barrier. We trained a deep learning system (DLS) to
detect active pulmonary TB using CXRs from 9 countries across Africa, Asia, and
Europe, and utilized large-scale CXR pretraining, attention pooling, and noisy
student semi-supervised learning. Evaluation was on (1) a combined test set
spanning China, India, US, and Zambia, and (2) an independent mining population
in South Africa. Given WHO targets of 90% sensitivity and 70% specificity, the
DLS's operating point was prespecified to favor sensitivity over specificity.
On the combined test set, the DLS's ROC curve was above all 9 India-based
radiologists, with an AUC of 0.90 (95%CI 0.87-0.92). The DLS's sensitivity
(88%) was higher than the India-based radiologists (75% mean sensitivity),
p<0.001 for superiority; and its specificity (79%) was non-inferior to the
radiologists (84% mean specificity), p=0.004. Similar trends were observed
within HIV positive and sputum smear positive sub-groups, and in the South
Africa test set. We found that 5 US-based radiologists (where TB isn't endemic)
were more sensitive and less specific than the India-based radiologists (where
TB is endemic). The DLS also remained non-inferior to the US-based
radiologists. In simulations, using the DLS as a prioritization tool for
confirmatory testing reduced the cost per positive case detected by 40-80%
compared to using confirmatory testing alone. To conclude, our DLS generalized
to 5 countries, and merits prospective evaluation to assist cost-effective
screening efforts in radiologist-limited settings. Operating point flexibility
may permit customization of the DLS to account for site-specific factors such
as TB prevalence, demographics, clinical resources, and customary practice
patterns.

    

### [[2105.08847] Beyond "Fairness:" Structural (In)justice Lenses on AI for Education](http://arxiv.org/abs/2105.08847)


  Educational technologies, and the systems of schooling in which they are
deployed, enact particular ideologies about what is important to know and how
learners should learn. As artificial intelligence technologies -- in education
and beyond -- may contribute to inequitable outcomes for marginalized
communities, various approaches have been developed to evaluate and mitigate
the harmful impacts of AI. However, we argue in this paper that the dominant
paradigm of evaluating fairness on the basis of performance disparities in AI
models is inadequate for confronting the systemic inequities that educational
AI systems (re)produce. We draw on a lens of structural injustice informed by
critical theory and Black feminist scholarship to critically interrogate
several widely-studied and widely-adopted categories of educational AI and
explore how they are bound up in and reproduce historical legacies of
structural injustice and inequity, regardless of the parity of their models'
performance. We close with alternative visions for a more equitable future for
educational AI.

    

### [[2106.02745] Neural Auto-Curricula](http://arxiv.org/abs/2106.02745)


  When solving two-player zero-sum games, multi-agent reinforcement learning
(MARL) algorithms often create populations of agents where, at each iteration,
a new agent is discovered as the best response to a mixture over the opponent
population. Within such a process, the update rules of "who to compete with"
(i.e., the opponent mixture) and "how to beat them" (i.e., finding best
responses) are underpinned by manually developed game theoretical principles
such as fictitious play and Double Oracle. In this paper, we introduce a novel
framework -- Neural Auto-Curricula (NAC) -- that leverages meta-gradient
descent to automate the discovery of the learning update rule without explicit
human design. Specifically, we parameterise the opponent selection module by
neural networks and the best-response module by optimisation subroutines, and
update their parameters solely via interaction with the game engine, where both
players aim to minimise their exploitability. Surprisingly, even without human
design, the discovered MARL algorithms achieve competitive or even better
performance with the state-of-the-art population-based game solvers (e.g.,
PSRO) on Games of Skill, differentiable Lotto, non-transitive Mixture Games,
Iterated Matching Pennies, and Kuhn Poker. Additionally, we show that NAC is
able to generalise from small games to large games, for example training on
Kuhn Poker and outperforming PSRO on Leduc Poker. Our work inspires a promising
future direction to discover general MARL algorithms solely from data.

    

### [[2106.08796] Tactile Sim-to-Real Policy Transfer via Real-to-Sim Image Translation](http://arxiv.org/abs/2106.08796)


  Simulation has recently become key for deep reinforcement learning to safely
and efficiently acquire general and complex control policies from visual and
proprioceptive inputs. Tactile information is not usually considered despite
its direct relation to environment interaction. In this work, we present a
suite of simulated environments tailored towards tactile robotics and
reinforcement learning. A simple and fast method of simulating optical tactile
sensors is provided, where high-resolution contact geometry is represented as
depth images. Proximal Policy Optimisation (PPO) is used to learn successful
policies across all considered tasks. A data-driven approach enables
translation of the current state of a real tactile sensor to corresponding
simulated depth images. This policy is implemented within a real-time control
loop on a physical robot to demonstrate zero-shot sim-to-real policy transfer
on several physically-interactive tasks requiring a sense of touch.

    

### [[2106.14855] K-Net: Towards Unified Image Segmentation](http://arxiv.org/abs/2106.14855)


  Semantic, instance, and panoptic segmentations have been addressed using
different and specialized frameworks despite their underlying connections. This
paper presents a unified, simple, and effective framework for these essentially
similar tasks. The framework, named K-Net, segments both instances and semantic
categories consistently by a group of learnable kernels, where each kernel is
responsible for generating a mask for either a potential instance or a stuff
class. To remedy the difficulties of distinguishing various instances, we
propose a kernel update strategy that enables each kernel dynamic and
conditional on its meaningful group in the input image. K-Net can be trained in
an end-to-end manner with bipartite matching, and its training and inference
are naturally NMS-free and box-free. Without bells and whistles, K-Net
surpasses all previous published state-of-the-art single-model results of
panoptic segmentation on MS COCO test-dev split and semantic segmentation on
ADE20K val split with 55.2% PQ and 54.3% mIoU, respectively. Its instance
segmentation performance is also on par with Cascade Mask R-CNN on MS COCO with
60%-90% faster inference speeds. Code and models will be released at
this https URL.

    

### [[2110.09456] NormFormer: Improved Transformer Pretraining with Extra Normalization](http://arxiv.org/abs/2110.09456)


  During pretraining, the Pre-LayerNorm transformer suffers from a gradient
magnitude mismatch: gradients at early layers are much larger than at later
layers. These issues can be alleviated by our proposed NormFormer architecture,
which adds three normalization operations to each layer: a Layer Norm after
self attention, head-wise scaling of self-attention outputs, and a Layer Norm
after the first fully connected layer. The extra operations incur negligible
compute cost (+0.4% parameter increase), but improve pretraining perplexity and
downstream task performance for both causal and masked language models ranging
from 125 Million to 2.7 Billion parameters. For example, adding NormFormer on
top of our strongest 1.3B parameter baseline can reach equal perplexity 24%
faster, or converge 0.27 perplexity better in the same compute budget. This
model reaches GPT3-Large (1.3B) zero shot performance 60% faster. For masked
language modeling, NormFormer improves fine-tuned GLUE performance by 1.9% on
average. Code to train NormFormer models is available in fairseq
this https URL .

    

### [[2110.12541] Partially Intervenable Causal Models](http://arxiv.org/abs/2110.12541)


  Graphical causal models led to the development of complete non-parametric
identification theory in arbitrary structured systems, and general approaches
to efficient inference. Nevertheless, graphical approaches to causal inference
have not been embraced by the statistics and public health communities. In
those communities causal assumptions are instead expressed in terms of
potential outcomes, or responses to hypothetical interventions. Such
interventions are generally conceptualized only on a limited set of variables,
where the corresponding experiment could, in principle, be performed. By
contrast, graphical approaches to causal inference generally assume
interventions on all variables are well defined - an overly restrictive and
unrealistic assumption that may have limited the adoption of these approaches
in applied work in statistics and public health. In this paper, we build on a
unification of graphical and potential outcomes approaches to causality
exemplified by Single World Intervention Graphs (SWIGs) to define graphical
models with a restricted set of allowed interventions. We give a complete
identification theory for such models, and develop a complete calculus of
interventions based on a generalization of the do-calculus, and axioms that
govern probabilistic operations on Markov kernels. A corollary of our results
is a complete identification theory for causal effects in another graphical
framework with a restricted set of interventions, the decision theoretic
graphical formulation of causality.

    

### [[1812.10487] Early Prediction of Post-acute Care Discharge Disposition Using Predictive Analytics: Preponing Prior Health Insurance Authorization Thus Reducing the Inpatient Length of Stay](http://arxiv.org/abs/1812.10487)


  Objective: A patient medical insurance coverage plays an essential role in
determining the post-acute care (PAC) discharge disposition. The prior health
insurance authorization process postpones the PAC discharge disposition,
increases the inpatient length of stay, and effects patient health. Our study
implements predictive analytics for the early prediction of the PAC discharge
disposition to reduce the deferments caused by prior health insurance
authorization, the inpatient length of stay and inpatient stay expenses.
Methodology: We conducted a group discussion involving 25 patient care
facilitators (PCFs) and two registered nurses (RNs) and retrieved 1600 patient
data records from the initial nursing assessment and discharge notes to conduct
a retrospective analysis of PAC discharge dispositions using predictive
analytics. Results: The chi-squared automatic interaction detector (CHAID)
algorithm enabled the early prediction of the PAC discharge disposition,
accelerated the prior health insurance process, decreased the inpatient length
of stay by an average of 22.22%, and reduced inpatient stay expenses by \$1,974
for state government hospitals, \$2,346 for non-profit hospitals and \$1,798
for for-profit hospitals per day. The CHAID algorithm produced an overall
accuracy of 84.16% and an area under the receiver operating characteristic
(ROC) curve value of 0.81. Conclusion: The early prediction of PAC discharge
dispositions can condense the PAC deferment caused by the prior health
insurance authorization process and simultaneously minimize the inpatient
length of stay and related expenses incurred by the hospital.

    

### [[2103.02398] Filter-Based Abstractions with Correctness Guarantees for Planning under Uncertainty](http://arxiv.org/abs/2103.02398)


  We study planning problems for continuous control systems with uncertainty
caused by measurement and process noise. The goal is to find an optimal plan
that guarantees that the system reaches a desired goal state within finite
time. Measurement noise causes limited observability of system states, and
process noise causes uncertainty in the outcome of a given plan. These factors
render the problem undecidable in general. Our key contribution is a novel
abstraction scheme that employs Kalman filtering as a state estimator to obtain
a finite-state model, which we formalize as a Markov decision process (MDP).
For this MDP, we employ state-of-the-art model checking techniques to
efficiently compute plans that maximize the probability of reaching goal
states. Moreover, we account for numerical imprecision in computing the
abstraction by extending the MDP with intervals of probabilities as a more
robust model. We show the correctness of the abstraction and provide several
optimizations that aim to balance the quality of the plan and the scalability
of the approach. We demonstrate that our method can handle systems that result
in MDPs with thousands of states and millions of transitions.

    

### [[2110.12662] Sampling-Based Robust Control of Autonomous Systems with Non-Gaussian Noise](http://arxiv.org/abs/2110.12662)


  Controllers for autonomous systems that operate in safety-critical settings
must account for stochastic disturbances. Such disturbances are often modelled
as process noise, and common assumptions are that the underlying distributions
are known and/or Gaussian. In practice, however, these assumptions may be
unrealistic and can lead to poor approximations of the true noise distribution.
We present a novel planning method that does not rely on any explicit
representation of the noise distributions. In particular, we address the
problem of computing a controller that provides probabilistic guarantees on
safely reaching a target. First, we abstract the continuous system into a
discrete-state model that captures noise by probabilistic transitions between
states. As a key contribution, we adapt tools from the scenario approach to
compute probably approximately correct (PAC) bounds on these transition
probabilities, based on a finite number of samples of the noise. We capture
these bounds in the transition probability intervals of a so-called interval
Markov decision process (iMDP). This iMDP is robust against uncertainty in the
transition probabilities, and the tightness of the probability intervals can be
controlled through the number of samples. We use state-of-the-art verification
techniques to provide guarantees on the iMDP, and compute a controller for
which these guarantees carry over to the autonomous system. Realistic
benchmarks show the practical applicability of our method, even when the iMDP
has millions of states or transitions.

    

### [[2012.06613] Beyond Scaling: Calculable Error Bounds of the Power-of-Two-Choices Mean-Field Model in Heavy-Traffic](http://arxiv.org/abs/2012.06613)


  This paper provides a recipe for deriving calculable approximation errors of
mean-field models in heavy-traffic with the focus on the well-known load
balancing algorithm -- power-of-two-choices (Po2). The recipe combines Stein's
method for linearized mean-field models and State Space Concentration (SSC)
based on geometric tail bounds. In particular, we divide the state space into
two regions, a neighborhood near the mean-field equilibrium and the complement
of that. We first use a tail bound to show that the steady-state probability
being outside the neighborhood is small. Then, we use a linearized mean-field
model and Stein's method to characterize the generator difference, which
provides the dominant term of the approximation error. From the dominant term,
we are able to obtain an asymptotically-tight bound and a nonasymptotic upper
bound, both are calculable bounds, not order-wise scaling results like most
results in the literature. Finally, we compared the theoretical bounds with
numerical evaluations to show the effectiveness of our results. We note that
the simulation results show that both bounds are valid even for small size
systems such as a system with only ten servers.

    

### [[2111.00169] Trojan Source: Invisible Vulnerabilities](http://arxiv.org/abs/2111.00169)


  We present a new type of attack in which source code is maliciously encoded
so that it appears different to a compiler and to the human eye. This attack
exploits subtleties in text-encoding standards such as Unicode to produce
source code whose tokens are logically encoded in a different order from the
one in which they are displayed, leading to vulnerabilities that cannot be
perceived directly by human code reviewers. 'Trojan Source' attacks, as we call
them, pose an immediate threat both to first-party software and of supply-chain
compromise across the industry. We present working examples of Trojan-Source
attacks in C, C++, C#, JavaScript, Java, Rust, Go, and Python. We propose
definitive compiler-level defenses, and describe other mitigating controls that
can be deployed in editors, repositories, and build pipelines while compilers
are upgraded to block this attack.

    

### [[2111.00222] A Hybrid Software Test Automation for Educational Portals](http://arxiv.org/abs/2111.00222)


  Educational portal (EP) is a multi-function website that allows access to
activities such as public and private sections, data retrieval and submission,
personalized content and so on for the educational system. This study
investigated the specific requirement for the enhancement of quality and
behavior of EP with regards to time and cost using Obafemi Awolowo University
(OAU), Ile-Ife, Nigeria as a case study. A test automation framework was
designed using unified modelling language and implemented in Java programming
language. MySQL and Excel database were used to store test data. The framework
developed was evaluated using Test Time Performance (TTP), Performance Test
Efficiency (PTE) and Automation Scripting Productivity (ASP) metrics. The
results from the evaluation of the sample data provided showed that ASP
produced a tested outcome of 360 operations per hour, PTE yielded 80% and TTP
was just 4%. Based on the recorded performance, it is evident that the research
can provide quick and firsthand information to quality assurance analyst and
software testers, thereby reducing maintenance cost during software
development.

    

### [[2111.00324] Property-Directed Reachability as Abstract Interpretation in the Monotone Theory](http://arxiv.org/abs/2111.00324)


  Inferring inductive invariants is one of the main challenges of formal
verification. The theory of abstract interpretation provides a rich framework
to devise invariant inference algorithms. One of the latest breakthroughs in
invariant inference is property-directed reachability (PDR), but the research
community views PDR and abstract interpretation as mostly unrelated techniques.
This paper shows that, surprisingly, propositional PDR can be formulated as
an abstract interpretation algorithm in a logical domain. More precisely, we
define a version of PDR, called $\Lambda$-PDR, in which all generalizations of
counterexamples are used to strengthen a frame. In this way, there is no need
to refine frames after their creation, because all the possible supporting
facts are included in advance. We analyze this algorithm using notions from
Bshouty's monotone theory, originally developed in the context of exact
learning. We show that there is an inherent overapproximation between the
algorithm's frames that is related to the monotone theory. We then define a new
abstract domain in which the best abstract transformer performs this
overapproximation, and show that it captures the invariant inference process,
i.e., $\Lambda$-PDR corresponds to Kleene iterations with the best transformer
in this abstract domain. We provide some sufficient conditions for when this
process converges in a small number of iterations, with sometimes an
exponential gap from the number of iterations required for naive exact forward
reachability. These results provide a firm theoretical foundation for the
benefits of how PDR tackles forward reachability.

    

### [<title data-react-helmet="true">Prompt Pre-training：迈向更强大的Parameter-Efficient Prompt Tuning - 知乎</title>](https://zhuanlan.zhihu.com/p/428512183)

### [<title>How can I train use grow_gpu_hist but predict on cpu? - XGBoost</title>](https://discuss.xgboost.ai/t/how-can-i-train-use-grow-gpu-hist-but-predict-on-cpu/2509/4)

### [<title>How can I train use grow_gpu_hist but predict on cpu? - XGBoost</title>](https://discuss.xgboost.ai/t/how-can-i-train-use-grow-gpu-hist-but-predict-on-cpu/2509/3)