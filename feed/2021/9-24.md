
## 2021-9-24

### [[2109.10903] In-network Computation for Large-scale Federated Learning over Wireless Edge Networks](http://arxiv.org/abs/2109.10903)


  Most conventional Federated Learning (FL) models are using a star network
topology where all users aggregate their local models at a single server (e.g.,
a cloud server). That causes significant overhead in terms of both
communications and computing at the server, delaying the training process,
especially for large scale FL systems with straggling nodes. This paper
proposes a novel edge network architecture that enables decentralizing the
model aggregation process at the server, thereby significantly reducing the
training delay for the whole FL network. Specifically, we design a
highly-effective in-network computation protocol (INC) consisting of a user
scheduling mechanism, an in-network aggregation process (INA) which is designed
for both primal- and primal-dual methods in distributed machine learning
problems, and a network routing algorithm. Under the proposed INA, we then
formulate a joint routing and resource optimization problem, aiming to minimize
the aggregation latency. The problem is NP-hard, and thus we propose a
polynomial time routing algorithm which can achieve near optimal performance
with a theoretical bound. Simulation results showed that the proposed INC
framework can not only help reduce the FL training latency, up to 5.6 times,
but also significantly decrease cloud's traffic and computing overhead. This
can enable large-scale FL.

    

### [[2109.11161] Subband Random Sensing Grant Free Uplink for URLLC in Unlicensed Spectrum](http://arxiv.org/abs/2109.11161)


  In this paper, we propose a novel scheme called subband random sensing (SRS)
grant free uplink for ultra-reliable low-latency communication (URLLC) in
unlicensed spectrum. The SRS grant free uplink creatively combines the subband
sensing, user grouping and random access, which allows a user sensing the
unlicensed spectrum to use available sub-resources in a wideband and reduce
collisions with other users. The new scheme overcomes the severe spectrum
wastage problem of the semi-persistent scheduling (SPS) uplink adopted by new
radio (NR) release 16 in applications with sporadic traffic. Compared with the
contention based grant free uplink, the new scheme achieves much lower
collision probability, which directly leads to higher reliability. Analysis and
simulations are provided to prove the exceptional performances.

    

### [[2109.11224] An Anomaly-based Multi-class Classifier for Network Intrusion Detection](http://arxiv.org/abs/2109.11224)


  Network intrusion detection systems (NIDS) are one of several solutions that
make up a computer security system. They are responsible for inspecting network
traffic and triggering alerts when detecting intrusion attempts. One of the
most popular approaches in NIDS research today is the Anomaly-based technique,
characterized by the ability to recognize previously unobserved attacks. Some
A-NIDS systems go beyond the separation into normal and anomalous classes by
trying to identify the type of detected anomalies. This is an important
capability of a security system, as it allows a more effective response to an
intrusion attempt. The existing systems with this ability are often subject to
limitations such as high complexity and incorrect labeling of unknown attacks.
In this work, we propose an algorithm to be used in NIDS that overcomes these
limitations. Our proposal is an adaptation of the Anomaly-based classifier EFC
to perform multi-class classification. It has a single layer, with low temporal
complexity, and can correctly classify not only the known attacks, but also
unprecedented attacks. Our proposal was evaluated in two up-to-date flow-based
intrusion detection datasets: CIDDS-001 and CICIDS2017. We also conducted a
specific experiment to assess our classifier's ability to correctly label
unknown attacks. Our results show that the multi-class EFC is a promising
classifier to be used in NIDS.

    

### [[2109.11226] Keeping data at the edge of smart irrigation networks: A case study in strawberry greenhouses](http://arxiv.org/abs/2109.11226)


  Strawberries are widely appreciated for their aroma, bright red color, juicy
texture, and sweetness. They are, however, among the most sensitive fruits when
it comes to the quality of the end product. The recent commercial trends show a
rising number of farmers who directly sell their products and are interested in
using smart solutions for a continuous quality control of the product.
Cloud-based approaches for smart irrigation have been widely used in the recent
years. However, the network traffic, security and regulatory challenges, which
come hand in hand with sharing the crop data with third parties outside the
edge of the network, lead strawberry farmers and data owners to rely on global
clouds and potentially lose control over their data, which are usually
transferred to third party data centers. In this paper, we follow a three-step
methodological approach in order to design, implement and validate a solution
for smart strawberry irrigation in greenhouses, while keeping the corresponding
data at the edge of the network: (i) We develop a small-scale smart irrigation
prototype solution with off-the-shelf hardware and software equipment, which we
test and evaluate on different kinds of plants in order to gain useful insights
for larger scale deployments, (ii) we introduce a reference network
architecture, specifically targeting smart irrigation and edge data
distribution for strawberry greenhouses, and (iii) adopting the proposed
reference architecture, we implement a full-scale system in an actual
strawberry greenhouse environment in Greece, and we compare its performance
against that of conventional strawberries irrigation. We show that our design
significantly outperforms the conventional approach, both in terms of soil
moisture variation and in terms of water consumption, and conclude by
critically appraising the costs and benefits of our approach in the
agricultural industry.

    

### [[2109.11268] Cyber Resilience in IoT network: Methodology and example of assessment through epidemic spreading](http://arxiv.org/abs/2109.11268)


  Cyber Resilience is an important property of complex systems and is important
consideration in developing specific IoT applications. This work aims at
introducing a novel approach to assess IoT resilience adopting the risk
perception in network based epidemic spreading approach. In particular IoT has
been considered a network of devices where the probability of infection and
interactions (communication), needs to be balanced in order to reduce the
malware outbreack while maintaining the network functionalities at an
acceptable level. The mathematical model and the simulation results reveal the
benefit of a shift from a risk-based to a resilience based approach to threat
management in IoT.

    

### [[2109.11289] Smartphone-based geolocation of Internet hosts](http://arxiv.org/abs/2109.11289)


  The location of Internet hosts is frequently used in distributed applications
and networking services. Examples include customized advertising, distribution
of content, and position-based security. Unfortunately the relationship between
an IP address and its position is in general very weak. This motivates the
study of measurement-based IP geolocation techniques, where the position of the
target host is actively estimated using the delays between a number of
landmarks and the target itself. This paper discusses an IP geolocation method
based on crowdsourcing where the smartphones of users operate as landmarks.
Since smartphones rely on wireless connections, a specific delay-distance model
was derived to capture the characteristics of this novel operating scenario.

    

### [[2109.11323] Federated Feature Selection for Cyber-Physical Systems of Systems](http://arxiv.org/abs/2109.11323)


  Autonomous systems generate a huge amount of multimodal data that are
collected and processed on the Edge, in order to enable AI-based services. The
collected datasets are pre-processed in order to extract informative
attributes, called features, which are used to feed AI algorithms. Due to the
limited computational and communication resources of some CPS, like autonomous
vehicles, selecting the subset of relevant features from a dataset is of the
utmost importance, in order to improve the result achieved by learning methods
and to reduce computation and communication costs. Precisely, feature selection
is the candidate approach, which assumes that data contain a certain number of
redundant or irrelevant attributes that can be eliminated. The quality of our
methods is confirmed by the promising results achieved on two different data
sets. In this work, we propose, for the first time, a federated feature
selection method suitable for being executed in a distributed manner.
Precisely, our results show that a fleet of autonomous vehicles finds a
consensus on the optimal set of features that they exploit to reduce data
transmission up to 99% with negligible information loss.

    

### [[2109.11362] An optimized application-context relocation approach for Connected and Automated Mobility (CAM)](http://arxiv.org/abs/2109.11362)


  In this paper, we study and present a management and orchestration framework
for vehicular communications, which enables service continuity for the vehicle
via an optimized application-context relocation approach. To optimize the
transfer of the application-context for Connected and Automated Mobility (CAM)
services, our MEC orchestrator performs prediction of resource availability in
the edge infrastructure based on the Long Short-Term Memory (LSTM) model, and
it makes a final decision on relocation by calculating the outcome of a
Multi-Criteria Decision-Making (MCDM) algorithm, taking into account the i)
resource prediction, ii) latency and bandwidth on the communication links, and
iii) geographical locations of the vehicle and edge hosts in the network
infrastructure. Furthermore, we have built a proof-of-concept for the
orchestration framework in a real-life distributed testbed environment, to
showcase the efficiency in optimizing the edge host selection and application
context relocation towards achieving continuity of a service that informs
vehicle about the driving conditions on the road.

    

### [[2109.11386] Energy efficient distributed analytics at the edge of the network for IoT environments](http://arxiv.org/abs/2109.11386)


  Due to the pervasive diffusion of personal mobile and IoT devices, many
"smart environments" (e.g., smart cities and smart factories) will be,
generators of huge amounts of data. Currently, analysis of this data is
typically achieved through centralised cloud-based services. However, according
to many studies, this approach may present significant issues from the
standpoint of data ownership, as well as wireless network capacity. In this
paper, we exploit the fog computing paradigm to move computation close to where
data is produced. We exploit a well-known distributed machine learning
framework (Hypothesis Transfer Learning), and perform data analytics on mobile
nodes passing by IoT devices, in addition to fog gateways at the edge of the
network infrastructure. We analyse the performance of different configurations
of the distributed learning framework, in terms of (i) accuracy obtained in the
learning task and (ii) energy spent to send data between the involved nodes.
Specifically, we consider reference wireless technologies for communication
between the different types of nodes we consider, e.g. LTE, Nb-IoT, 802.15.4,
802.11, etc. Our results show that collecting data through the mobile nodes and
executing the distributed analytics using short-range communication
technologies, such as 802.15.4 and 802.11, allows to strongly reduce the energy
consumption of the system up to $94\%$ with a loss in accuracy w.r.t. a
centralised cloud solution up to $2\%$.

    

### [[1910.07266] A new method for flow-based network intrusion detection using the inverse Potts model](http://arxiv.org/abs/1910.07266)


  Network Intrusion Detection Systems (NIDS) play an important role as tools
for identifying potential network threats. In the context of ever-increasing
traffic volume on computer networks, flow-based NIDS arise as good solutions
for real-time traffic classification. In recent years, different flow-based
classifiers have been proposed using Machine Learning (ML) algorithms.
Nevertheless, classical ML-based classifiers have some limitations. For
instance, they require large amounts of labeled data for training, which might
be difficult to obtain. Additionally, most ML-based classifiers are not capable
of domain adaptation, i.e. after being trained on an specific data
distribution, they are not general enough to be applied to other related data
distributions. And, finally, many of the models inferred by these algorithms
are black boxes, which do not provide explainable results. To overcome these
limitations, we propose a new algorithm, called Energy-based Flow Classifier
(EFC). This anomaly-based classifier uses inverse statistics to infer a
statistical model based on labeled benign examples. We show that EFC is capable
of accurately performing binary flow classification and is more adaptable to
different data distributions than classical ML-based classifiers. Given the
positive results obtained on three different datasets (CIDDS-001, CICIDS17 and
CICDDoS19), we consider EFC to be a promising algorithm to perform robust
flow-based traffic classification.

    

### [[2101.02772] TODG: Distributed Task Offloading with Delay Guarantees for Edge Computing](http://arxiv.org/abs/2101.02772)


  Edge computing has been an efficient way to provide prompt and near-data
computing services for resource-and-delay sensitive IoT applications via
computation offloading. Effective computation offloading strategies need to
comprehensively cope with several major issues, including the allocation of
dynamic communication and computational resources, the deadline constraints of
heterogeneous tasks, and the requirements for computationally inexpensive and
distributed algorithms. However, most of the existing works mainly focus on
part of these issues, which would not suffice to achieve expected performance
in complex and practical scenarios. To tackle this challenge, in this paper, we
systematically study a distributed computation offloading problem with hard
delay constraints, where heterogeneous computational tasks require continually
offloading to a set of edge servers via a limiting number of stochastic
communication channels. The task offloading problem is then cast as a
delay-constrained long-term stochastic optimization problem under unknown
priori statistical knowledge. To resolve this problem, we first provide a
technical path to transform and decompose it into several slot-level
subproblems, then we develop a distributed online algorithm, namely TODG, to
efficiently allocate the resources and schedule the offloading tasks with delay
guarantees. Further, we present a comprehensive analysis for TODG, in terms of
the optimality gap, the delay guarantees, and the impact of system parameters.
Extensive simulation results demonstrate the effectiveness and efficiency of
TODG.

    

### [[2106.12684] A Pure HTTP/3 Alternative to MQTT-over-QUIC in Resource-Constrained IoT](http://arxiv.org/abs/2106.12684)


  In this letter, we address the issue of scalable and timely dissemination of
information in resource-constrained IoT networks. The scalability is addressed
by adopting a publishsubscribe architecture. To address the timely
dissemination, we propose an HTTP/3 (H3) publish-subscribe solution that
exploits the wide-ranging improvements offered by H3. We evaluated our solution
by comparing it to a state-of-the-art work which maps MQTT to QUIC. Because
QUIC and H3 have been developed in tandem, we hypothesized that H3 would take
better advantage of QUIC transport than an MQTT mapping would. Performance,
network overhead, and device overhead were investigated for both
implementations. Our H3-based solution satisfied our timely dissemination
requirement by offering a key performance savings of 1 RoundTrip Time (RTT) for
publish messages to arrive at the broker. In IoT networks, with typically high
RTT, this savings is significant. On the other hand, we found that
MQTT-over-QUIC put marginally less strain over the network.

    

### [[2109.10902] Mixed-supervised segmentation: Confidence maximization helps knowledge distillation](http://arxiv.org/abs/2109.10902)


  Despite achieving promising results in a breadth of medical image
segmentation tasks, deep neural networks require large training datasets with
pixel-wise annotations. Obtaining these curated datasets is a cumbersome
process which limits the application in scenarios where annotated images are
scarce. Mixed supervision is an appealing alternative for mitigating this
obstacle, where only a small fraction of the data contains complete pixel-wise
annotations and other images have a weaker form of supervision. In this work,
we propose a dual-branch architecture, where the upper branch (teacher)
receives strong annotations, while the bottom one (student) is driven by
limited supervision and guided by the upper branch. Combined with a standard
cross-entropy loss over the labeled pixels, our novel formulation integrates
two important terms: (i) a Shannon entropy loss defined over the
less-supervised images, which encourages confident student predictions in the
bottom branch; and (ii) a Kullback-Leibler (KL) divergence term, which
transfers the knowledge of the strongly supervised branch to the
less-supervised branch and guides the entropy (student-confidence) term to
avoid trivial solutions. We show that the synergy between the entropy and KL
divergence yields substantial improvements in performance. We also discuss an
interesting link between Shannon-entropy minimization and standard pseudo-mask
generation, and argue that the former should be preferred over the latter for
leveraging information from unlabeled pixels. Quantitative and qualitative
results on two publicly available datasets demonstrate that our method
significantly outperforms other strategies for semantic segmentation within a
mixed-supervision framework, as well as recent semi-supervised approaches.
Moreover, we show that the branch trained with reduced supervision and guided
by the top branch largely outperforms the latter.

    

### [[2109.10915] The CAMELS Multifield Dataset: Learning the Universe's Fundamental Parameters with Artificial Intelligence](http://arxiv.org/abs/2109.10915)


  We present the Cosmology and Astrophysics with MachinE Learning Simulations
(CAMELS) Multifield Dataset, CMD, a collection of hundreds of thousands of 2D
maps and 3D grids containing many different properties of cosmic gas, dark
matter, and stars from 2,000 distinct simulated universes at several cosmic
times. The 2D maps and 3D grids represent cosmic regions that span $\sim$100
million light years and have been generated from thousands of state-of-the-art
hydrodynamic and gravity-only N-body simulations from the CAMELS project.
Designed to train machine learning models, CMD is the largest dataset of its
kind containing more than 70 Terabytes of data. In this paper we describe CMD
in detail and outline a few of its applications. We focus our attention on one
such task, parameter inference, formulating the problems we face as a challenge
to the community. We release all data and provide further technical details at
this https URL.

    

### [[2109.10919] An Exploration of Learnt Representations of W Jets](http://arxiv.org/abs/2109.10919)


  I present a Variational Autoencoder (VAE) trained on collider physics data
(specifically boosted $W$ jets), with reconstruction error given by an
approximation to the Earth Movers Distance (EMD) between input and output jets.
This VAE learns a concrete representation of the data manifold, with
semantically meaningful and interpretable latent space directions which are
hierarchically organized in terms of their relation to physical EMD scales in
the underlying physical generative process. A hyperparameter $\beta$ controls
the resolution at which the VAE is sensitive to structures in the data
manifold. The variation of the latent space structure with $\beta$, and the
scaling of some VAE properties, provide insight into scale dependent structure
of the dataset and its information complexity. I introduce two measures of the
dimensionality of the learnt representation that are calculated from this
scaling.

    

### [[2109.10933] On the equivalence of different adaptive batch size selection strategies for stochastic gradient descent methods](http://arxiv.org/abs/2109.10933)


  In this study, we demonstrate that the norm test and inner
product/orthogonality test presented in \cite{Bol18} are equivalent in terms of
the convergence rates associated with Stochastic Gradient Descent (SGD) methods
if $\epsilon^2=\theta^2+\nu^2$ with specific choices of $\theta$ and $\nu$.
Here, $\epsilon$ controls the relative statistical error of the norm of the
gradient while $\theta$ and $\nu$ control the relative statistical error of the
gradient in the direction of the gradient and in the direction orthogonal to
the gradient, respectively. Furthermore, we demonstrate that the inner
product/orthogonality test can be as inexpensive as the norm test in the best
case scenario if $\theta$ and $\nu$ are optimally selected, but the inner
product/orthogonality test will never be more computationally affordable than
the norm test if $\epsilon^2=\theta^2+\nu^2$. Finally, we present two
stochastic optimization problems to illustrate our results.

    

### [[2109.10935] Robust Generalization of Quadratic Neural Networks via Function Identification](http://arxiv.org/abs/2109.10935)


  A key challenge facing deep learning is that neural networks are often not
robust to shifts in the underlying data distribution. We study this problem
from the perspective of the statistical concept of parameter identification.
Generalization bounds from learning theory often assume that the test
distribution is close to the training distribution. In contrast, if we can
identify the "true" parameters, then the model generalizes to arbitrary
distribution shifts. However, neural networks are typically overparameterized,
making parameter identification impossible. We show that for quadratic neural
networks, we can identify the function represented by the model even though we
cannot identify its parameters. Thus, we can obtain robust generalization
bounds even in the overparameterized setting. We leverage this result to obtain
new bounds for contextual bandits and transfer learning with quadratic neural
networks. Overall, our results suggest that we can improve robustness of neural
networks by designing models that can represent the true data generating
process. In practice, the true data generating process is often very complex;
thus, we study how our framework might connect to neural module networks, which
are designed to break down complex tasks into compositions of simpler ones. We
prove robust generalization bounds when individual neural modules are
identifiable.

    

### [[2109.10947] Causal Discovery in High-Dimensional Point Process Networks with Hidden Nodes](http://arxiv.org/abs/2109.10947)


  Thanks to technological advances leading to near-continuous time
observations, emerging multivariate point process data offer new opportunities
for causal discovery. However, a key obstacle in achieving this goal is that
many relevant processes may not be observed in practice. Naive estimation
approaches that ignore these hidden variables can generate misleading results
because of the unadjusted confounding. To plug this gap, we propose a
deconfounding procedure to estimate high-dimensional point process networks
with only a subset of the nodes being observed. Our method allows flexible
connections between the observed and unobserved processes. It also allows the
number of unobserved processes to be unknown and potentially larger than the
number of observed nodes. Theoretical analyses and numerical studies highlight
the advantages of the proposed method in identifying causal interactions among
the observed processes.

    

### [[2109.10963] On Optimal Robustness to Adversarial Corruption in Online Decision Problems](http://arxiv.org/abs/2109.10963)


  This paper considers two fundamental sequential decision-making problems: the
problem of prediction with expert advice and the multi-armed bandit problem. We
focus on stochastic regimes in which an adversary may corrupt losses, and we
investigate what level of robustness can be achieved against adversarial
corruptions. The main contribution of this paper is to show that optimal
robustness can be expressed by a square-root dependency on the amount of
corruption. More precisely, we show that two classes of algorithms, anytime
Hedge with decreasing learning rate and algorithms with second-order regret
bounds, achieve $O( \frac{\log N}{\Delta} + \sqrt{ \frac{C \log N }{\Delta} }
)$-regret, where $N, \Delta$, and $C$ represent the number of experts, the gap
parameter, and the corruption level, respectively. We further provide a
matching lower bound, which means that this regret bound is tight up to a
constant factor. For the multi-armed bandit problem, we also provide a nearly
tight lower bound up to a logarithmic factor.

    

### [[2109.10964] Multi-Objective Bayesian Optimization over High-Dimensional Search Spaces](http://arxiv.org/abs/2109.10964)


  The ability to optimize multiple competing objective functions with high
sample efficiency is imperative in many applied problems across science and
industry. Multi-objective Bayesian optimization (BO) achieves strong empirical
performance on such problems, but even with recent methodological advances, it
has been restricted to simple, low-dimensional domains. Most existing BO
methods exhibit poor performance on search spaces with more than a few dozen
parameters. In this work we propose MORBO, a method for multi-objective
Bayesian optimization over high-dimensional search spaces. MORBO performs local
Bayesian optimization within multiple trust regions simultaneously, allowing it
to explore and identify diverse solutions even when the objective functions are
difficult to model globally. We show that MORBO significantly advances the
state-of-the-art in sample-efficiency for several high-dimensional synthetic
and real-world multi-objective problems, including a vehicle design problem
with 222 parameters, demonstrating that MORBO is a practical approach for
challenging and important problems that were previously out of reach for BO
methods.

    

### [[2109.10966] A Profile-Based Binary Feature Extraction Method Using Frequent Itemsets for Improving Coronary Artery Disease Diagnosis](http://arxiv.org/abs/2109.10966)


  Recent years have seen growing interest in the diagnosis of Coronary Artery
Disease (CAD) with machine learning methods to reduce the cost and health
implications of conventional diagnosis. This paper introduces a CAD diagnosis
method with a novel feature extraction technique called the Profile-Based
Binary Feature Extraction (PBBFE). In this method, after partitioning numerical
features, frequent itemsets are extracted by the Apriori algorithm and then
used as features to increase the CAD diagnosis accuracy. The proposed method
consists of two main phases. In the first phase, each patient is assigned a
profile based on age, gender, and medical condition, and then all numerical
features are discretized based on assigned profiles. All features then undergo
a binarization process to become ready for feature extraction by Apriori. In
the last step of this phase, frequent itemsets are extracted from the dataset
by Apriori and used to build a new dataset. In the second phase, the Genetic
Algorithm and the Support Vector Machine are used to identify the best subset
of extracted features for classification. The proposed method was tested on the
Z-Alizadeh Sani dataset, which is one the richest databases in the field of
CAD. Performance comparisons conducted on this dataset showed that the proposed
method outperforms all major alternative methods with 98.35% accuracy, 100%
sensitivity, and 94.25% specificity. The proposed method also achieved the
highest accuracy on several other datasets.

    

### [[2109.11010] Alzheimers Dementia Detection using Acoustic & Linguistic features and Pre-Trained BERT](http://arxiv.org/abs/2109.11010)


  Alzheimers disease is a fatal progressive brain disorder that worsens with
time. It is high time we have inexpensive and quick clinical diagnostic
techniques for early detection and care. In previous studies, various Machine
Learning techniques and Pre-trained Deep Learning models have been used in
conjunction with the extraction of various acoustic and linguistic features.
Our study focuses on three models for the classification task in the ADReSS
(The Alzheimers Dementia Recognition through Spontaneous Speech) 2021
Challenge. We use the well-balanced dataset provided by the ADReSS Challenge
for training and validating our models. Model 1 uses various acoustic features
from the eGeMAPs feature-set, Model 2 uses various linguistic features that we
generated from auto-generated transcripts and Model 3 uses the auto-generated
transcripts directly to extract features using a Pre-trained BERT and TF-IDF.
These models are described in detail in the models section.

    

### [[2109.11018] Making Human-Like Trade-offs in Constrained Environments by Learning from Demonstrations](http://arxiv.org/abs/2109.11018)


  Many real-life scenarios require humans to make difficult trade-offs: do we
always follow all the traffic rules or do we violate the speed limit in an
emergency? These scenarios force us to evaluate the trade-off between
collective norms and our own personal objectives. To create effective AI-human
teams, we must equip AI agents with a model of how humans make trade-offs in
complex, constrained environments. These agents will be able to mirror human
behavior or to draw human attention to situations where decision making could
be improved. To this end, we propose a novel inverse reinforcement learning
(IRL) method for learning implicit hard and soft constraints from
demonstrations, enabling agents to quickly adapt to new settings. In addition,
learning soft constraints over states, actions, and state features allows
agents to transfer this knowledge to new domains that share similar aspects. We
then use the constraint learning method to implement a novel system
architecture that leverages a cognitive model of human decision making,
multi-alternative decision field theory (MDFT), to orchestrate competing
objectives. We evaluate the resulting agent on trajectory length, number of
violated constraints, and total reward, demonstrating that our agent
architecture is both general and achieves strong performance. Thus we are able
to capture and replicate human-like trade-offs from demonstrations in
environments when constraints are not explicit.

    

### [[2109.11020] Exploring Decomposition for Table-based Fact Verification](http://arxiv.org/abs/2109.11020)


  Fact verification based on structured data is challenging as it requires
models to understand both natural language and symbolic operations performed
over tables. Although pre-trained language models have demonstrated a strong
capability in verifying simple statements, they struggle with complex
statements that involve multiple operations. In this paper, we improve fact
verification by decomposing complex statements into simpler subproblems.
Leveraging the programs synthesized by a weakly supervised semantic parser, we
propose a program-guided approach to constructing a pseudo dataset for
decomposition model training. The subproblems, together with their predicted
answers, serve as the intermediate evidence to enhance our fact verification
model. Experiments show that our proposed approach achieves the new
state-of-the-art performance, an 82.7\% accuracy, on the TabFact benchmark.

    

### [[2109.11024] Social-Media Activity Forecasting with Exogenous Information Signals](http://arxiv.org/abs/2109.11024)


  Due to their widespread adoption, social media platforms present an ideal
environment for studying and understanding social behavior, especially on
information spread. Modeling social media activity has numerous practical
implications such as supporting efforts to analyze strategic information
operations, designing intervention techniques to mitigate disinformation, or
delivering critical information during disaster relief operations. In this
paper we propose a modeling technique that forecasts topic-specific daily
volume of social media activities by using both exogenous signals, such as news
or armed conflicts records, and endogenous data from the social media platform
we model. Empirical evaluations with real datasets from two different platforms
and two different contexts each composed of multiple interrelated topics
demonstrate the effectiveness of our solution.

    

### [[2109.11027] Quantile-based fuzzy C-means clustering of multivariate time series: Robust techniques](http://arxiv.org/abs/2109.11027)


  Three robust methods for clustering multivariate time series from the point
of view of generating processes are proposed. The procedures are robust
versions of a fuzzy C-means model based on: (i) estimates of the quantile
cross-spectral density and (ii) the classical principal component analysis.
Robustness to the presence of outliers is achieved by using the so-called
metric, noise and trimmed approaches. The metric approach incorporates in the
objective function a distance measure aimed at neutralizing the effect of the
outliers, the noise approach builds an artificial cluster expected to contain
the outlying series and the trimmed approach eliminates the most atypical
series in the dataset. All the proposed techniques inherit the nice properties
of the quantile cross-spectral density, as being able to uncover general types
of dependence. Results from a broad simulation study including multivariate
linear, nonlinear and GARCH processes indicate that the algorithms are
substantially effective in coping with the presence of outlying series (i.e.,
series exhibiting a dependence structure different from that of the majority),
clearly poutperforming alternative procedures. The usefulness of the suggested
methods is highlighted by means of two specific applications regarding
financial and environmental series.

    

### [[2109.11034] Conditional Poisson Stochastic Beam Search](http://arxiv.org/abs/2109.11034)


  Beam search is the default decoding strategy for many sequence generation
tasks in NLP. The set of approximate K-best items returned by the algorithm is
a useful summary of the distribution for many applications; however, the
candidates typically exhibit high overlap and may give a highly biased estimate
for expectations under our model. These problems can be addressed by instead
using stochastic decoding strategies. In this work, we propose a new method for
turning beam search into a stochastic process: Conditional Poisson stochastic
beam search. Rather than taking the maximizing set at each iteration, we sample
K candidates without replacement according to the conditional Poisson sampling
design. We view this as a more natural alternative to Kool et. al. 2019's
stochastic beam search (SBS). Furthermore, we show how samples generated under
the CPSBS design can be used to build consistent estimators and sample diverse
sets from sequence models. In our experiments, we observe CPSBS produces lower
variance and more efficient estimators than SBS, even showing improvements in
high entropy settings.

    

### [[2109.11041] Security Analysis of Capsule Network Inference using Horizontal Collaboration](http://arxiv.org/abs/2109.11041)


  The traditional convolution neural networks (CNN) have several drawbacks like
the Picasso effect and the loss of information by the pooling layer. The
Capsule network (CapsNet) was proposed to address these challenges because its
architecture can encode and preserve the spatial orientation of input images.
Similar to traditional CNNs, CapsNet is also vulnerable to several malicious
attacks, as studied by several researchers in the literature. However, most of
these studies focus on single-device-based inference, but horizontally
collaborative inference in state-of-the-art systems, like intelligent edge
services in self-driving cars, voice controllable systems, and drones, nullify
most of these analyses. Horizontal collaboration implies partitioning the
trained CNN models or CNN tasks to multiple end devices or edge nodes.
Therefore, it is imperative to examine the robustness of the CapsNet against
malicious attacks when deployed in horizontally collaborative environments.
Towards this, we examine the robustness of the CapsNet when subjected to
noise-based inference attacks in a horizontal collaborative environment. In
this analysis, we perturbed the feature maps of the different layers of four
DNN models, i.e., CapsNet, Mini-VGG, LeNet, and an in-house designed CNN
(ConvNet) with the same number of parameters as CapsNet, using two types of
noised-based attacks, i.e., Gaussian Noise Attack and FGSM noise attack. The
experimental results show that similar to the traditional CNNs, depending upon
the access of the attacker to the DNN layer, the classification accuracy of the
CapsNet drops significantly. For example, when Gaussian Noise Attack
classification is performed at the DigitCap layer of the CapsNet, the maximum
classification accuracy drop is approximately 97%.

    

### [[2109.11043] Learning Predictive and Interpretable Timeseries Summaries from ICU Data](http://arxiv.org/abs/2109.11043)


  Machine learning models that utilize patient data across time (rather than
just the most recent measurements) have increased performance for many risk
stratification tasks in the intensive care unit. However, many of these models
and their learned representations are complex and therefore difficult for
clinicians to interpret, creating challenges for validation. Our work proposes
a new procedure to learn summaries of clinical time-series that are both
predictive and easily understood by humans. Specifically, our summaries consist
of simple and intuitive functions of clinical data (e.g. falling mean arterial
pressure). Our learned summaries outperform traditional interpretable model
classes and achieve performance comparable to state-of-the-art deep learning
models on an in-hospital mortality classification task.

    

### [[2109.11045] Training Deep Spiking Auto-encoders without Bursting or Dying Neurons through Regularization](http://arxiv.org/abs/2109.11045)


  Spiking neural networks are a promising approach towards next-generation
models of the brain in computational neuroscience. Moreover, compared to
classic artificial neural networks, they could serve as an energy-efficient
deployment of AI by enabling fast computation in specialized neuromorphic
hardware. However, training deep spiking neural networks, especially in an
unsupervised manner, is challenging and the performance of a spiking model is
significantly hindered by dead or bursting neurons. Here, we apply end-to-end
learning with membrane potential-based backpropagation to a spiking
convolutional auto-encoder with multiple trainable layers of leaky
integrate-and-fire neurons. We propose bio-inspired regularization methods to
control the spike density in latent representations. In the experiments, we
show that applying regularization on membrane potential and spiking output
successfully avoids both dead and bursting neurons and significantly decreases
the reconstruction error of the spiking auto-encoder. Training regularized
networks on the MNIST dataset yields image reconstruction quality comparable to
non-spiking baseline models (deterministic and variational auto-encoder) and
indicates improvement upon earlier approaches. Importantly, we show that,
unlike the variational auto-encoder, the spiking latent representations display
structure associated with the image class.

    

### [[2109.11052] On Bonus-Based Exploration Methods in the Arcade Learning Environment](http://arxiv.org/abs/2109.11052)


  Research on exploration in reinforcement learning, as applied to Atari 2600
game-playing, has emphasized tackling difficult exploration problems such as
Montezuma's Revenge (Bellemare et al., 2016). Recently, bonus-based exploration
methods, which explore by augmenting the environment reward, have reached
above-human average performance on such domains. In this paper we reassess
popular bonus-based exploration methods within a common evaluation framework.
We combine Rainbow (Hessel et al., 2018) with different exploration bonuses and
evaluate its performance on Montezuma's Revenge, Bellemare et al.'s set of hard
of exploration games with sparse rewards, and the whole Atari 2600 suite. We
find that while exploration bonuses lead to higher score on Montezuma's Revenge
they do not provide meaningful gains over the simpler $\epsilon$-greedy scheme.
In fact, we find that methods that perform best on that game often underperform
$\epsilon$-greedy on easy exploration Atari 2600 games. We find that our
conclusions remain valid even when hyperparameters are tuned for these
easy-exploration games. Finally, we find that none of the methods surveyed
benefit from additional training samples (1 billion frames, versus Rainbow's
200 million) on Bellemare et al.'s hard exploration games. Our results suggest
that recent gains in Montezuma's Revenge may be better attributed to
architecture change, rather than better exploration schemes; and that the real
pace of progress in exploration research for Atari 2600 games may have been
obfuscated by good results on a single domain.

    

### [[2109.11053] Automated Feature-Topic Pairing: Aligning Semantic and Embedding Spaces in Spatial Representation Learning](http://arxiv.org/abs/2109.11053)


  Automated characterization of spatial data is a kind of critical geographical
intelligence. As an emerging technique for characterization, Spatial
Representation Learning (SRL) uses deep neural networks (DNNs) to learn
non-linear embedded features of spatial data for characterization. However, SRL
extracts features by internal layers of DNNs, and thus suffers from lacking
semantic labels. Texts of spatial entities, on the other hand, provide semantic
understanding of latent feature labels, but is insensible to deep SRL models.
How can we teach a SRL model to discover appropriate topic labels in texts and
pair learned features with the labels? This paper formulates a new problem:
feature-topic pairing, and proposes a novel Particle Swarm Optimization (PSO)
based deep learning framework. Specifically, we formulate the feature-topic
pairing problem into an automated alignment task between 1) a latent embedding
feature space and 2) a textual semantic topic space. We decompose the alignment
of the two spaces into: 1) point-wise alignment, denoting the correlation
between a topic distribution and an embedding vector; 2) pair-wise alignment,
denoting the consistency between a feature-feature similarity matrix and a
topic-topic similarity matrix. We design a PSO based solver to simultaneously
select an optimal set of topics and learn corresponding features based on the
selected topics. We develop a closed loop algorithm to iterate between 1)
minimizing losses of representation reconstruction and feature-topic alignment
and 2) searching the best topics. Finally, we present extensive experiments to
demonstrate the enhanced performance of our method.

    

### [[2109.11057] Weighted Low Rank Matrix Approximation and Acceleration](http://arxiv.org/abs/2109.11057)


  Low-rank matrix approximation is one of the central concepts in machine
learning, with applications in dimension reduction, de-noising, multivariate
statistical methodology, and many more. A recent extension to LRMA is called
low-rank matrix completion (LRMC). It solves the LRMA problem when some
observations are missing and is especially useful for recommender systems. In
this paper, we consider an element-wise weighted generalization of LRMA. The
resulting weighted low-rank matrix approximation technique therefore covers
LRMC as a special case with binary weights. WLRMA has many applications. For
example, it is an essential component of GLM optimization algorithms, where an
exponential family is used to model the entries of a matrix, and the matrix of
natural parameters admits a low-rank structure. We propose an algorithm for
solving the weighted problem, as well as two acceleration techniques. Further,
we develop a non-SVD modification of the proposed algorithm that is able to
handle extremely high-dimensional data. We compare the performance of all the
methods on a small simulation example as well as a real-data application.

    

### [[2109.11066] A two-step machine learning approach for crop disease detection: an application of GAN and UAV technology](http://arxiv.org/abs/2109.11066)


  Automated plant diagnosis is a technology that promises large increases in
cost-efficiency for agriculture. However, multiple problems reduce the
effectiveness of drones, including the inverse relationship between resolution
and speed and the lack of adequate labeled training data. This paper presents a
two-step machine learning approach that analyzes low-fidelity and high-fidelity
images in sequence, preserving efficiency as well as accuracy. Two
data-generators are also used to minimize class imbalance in the high-fidelity
dataset and to produce low-fidelity data that is representative of UAV images.
The analysis of applications and methods is conducted on a database of
high-fidelity apple tree images which are corrupted with class imbalance. The
application begins by generating high-fidelity data using generative networks
and then uses this novel data alongside the original high-fidelity data to
produce low-fidelity images. A machine-learning identifier identifies plants
and labels them as potentially diseased or not. A machine learning classifier
is then given the potentially diseased plant images and returns actual
diagnoses for these plants. The results show an accuracy of 96.3% for the
high-fidelity system and a 75.5% confidence level for our low-fidelity system.
Our drone technology shows promising results in accuracy when compared to
labor-based methods of diagnosis.

    

### [[2109.11067] Serving DNN Models with Multi-Instance GPUs: A Case of the Reconfigurable Machine Scheduling Problem](http://arxiv.org/abs/2109.11067)


  Multi-Instance GPU (MIG) is a new feature introduced by NVIDIA A100 GPUs that
partitions one physical GPU into multiple GPU instances. With MIG, A100 can be
the most cost-efficient GPU ever for serving Deep Neural Networks (DNNs).
However, discovering the most efficient GPU partitions is challenging. The
underlying problem is NP-hard; moreover, it is a new abstract problem, which we
define as the Reconfigurable Machine Scheduling Problem (RMS). This paper
studies serving DNNs with MIG, a new case of RMS. We further propose a
solution, MIG-serving. MIG- serving is an algorithm pipeline that blends a
variety of newly designed algorithms and customized classic algorithms,
including a heuristic greedy algorithm, Genetic Algorithm (GA), and Monte Carlo
Tree Search algorithm (MCTS). We implement MIG-serving on Kubernetes. Our
experiments show that compared to using A100 as-is, MIG-serving can save up to
40% of GPUs while providing the same throughput.

    

### [[2109.11071] Learning to Downsample for Segmentation of Ultra-High Resolution Images](http://arxiv.org/abs/2109.11071)


  Segmentation of ultra-high resolution images with deep learning is
challenging because of their enormous size, often millions or even billions of
pixels. Typical solutions drastically downsample the image uniformly to meet
memory constraints, implicitly assuming all pixels equally important by
sampling at the same density at all spatial locations. However this assumption
is not true and compromises the performance of deep learning techniques that
have proved powerful on standard-sized images. For example with uniform
downsampling, see green boxed region in Fig.1, the rider and bike do not have
enough corresponding samples while the trees and buildings are oversampled, and
lead to a negative effect on the segmentation prediction from the
low-resolution downsampled image. In this work we show that learning the
spatially varying downsampling strategy jointly with segmentation offers
advantages in segmenting large images with limited computational budget. Fig.1
shows that our method adapts the sampling density over different locations so
that more samples are collected from the small important regions and less from
the others, which in turn leads to better segmentation accuracy. We show on two
public and one local high-resolution datasets that our method consistently
learns sampling locations preserving more information and boosting segmentation
accuracy over baseline methods.

    

### [[2109.11076] Predicting Stress in Remote Learning via Advanced Deep Learning Technologies](http://arxiv.org/abs/2109.11076)


  COVID-19 has driven most schools to remote learning through online meeting
software such as Zoom and Google Meet. Although this trend helps students
continue learning without in-person classes, it removes a vital tool that
teachers use to teach effectively: visual cues. By not being able to see a
student's face clearly, the teacher may not notice when the student needs
assistance, or when the student is not paying attention. In order to help
remedy the teachers of this challenge, this project proposes a machine learning
based approach that provides real-time student mental state monitoring and
classifications for the teachers to better conduct remote teaching. Using
publicly available electroencephalogram (EEG) data collections, this research
explored four different classification techniques: the classic deep neural
network, the traditionally popular support vector machine, the latest
convolutional neural network, and the XGBoost model, which has gained
popularity recently. This study defined three mental classes: an engaged
learning mode, a confused learning mode, and a relaxed mode. The experimental
results from this project showed that these selected classifiers have varying
potentials in classifying EEG signals for mental states. While some of the
selected classifiers only yield around 50% accuracy with some delay, the best
ones can achieve 80% accurate classification in real-time. This could be very
beneficial for teachers in need of help making remote teaching adjustments, and
for many other potential applications where in-person interactions are not
possible.

    

### [[2109.11077] A Novel Factor Graph-Based Optimization Technique for Stereo Correspondence Estimation](http://arxiv.org/abs/2109.11077)


  Dense disparities among multiple views is essential for estimating the 3D
architecture of a scene based on the geometrical relationship among the scene
and the views or cameras. Scenes with larger extents of heterogeneous textures,
differing scene illumination among the multiple views and with occluding
objects affect the accuracy of the estimated disparities. Markov random fields
(MRF) based methods for disparity estimation address these limitations using
spatial dependencies among the observations and among the disparity estimates.
These methods, however, are limited by spatially fixed and smaller neighborhood
systems or cliques. In this work, we present a new factor graph-based
probabilistic graphical model for disparity estimation that allows a larger and
a spatially variable neighborhood structure determined based on the local scene
characteristics. We evaluated our method using the Middlebury benchmark stereo
datasets and the Middlebury evaluation dataset version 3.0 and compared its
performance with recent state-of-the-art disparity estimation algorithms. The
new factor graph-based method provided disparity estimates with higher accuracy
when compared to the recent non-learning- and learning-based disparity
estimation algorithms. In addition to disparity estimation, our factor graph
formulation can be useful for obtaining maximum a posteriori solution to
optimization problems with complex and variable dependency structures as well
as for other dense estimation problems such as optical flow estimation.

    

### [[2109.11078] Improved Evolutionary Generative Adversarial Networks](http://arxiv.org/abs/2109.11078)


  Evolutionary generative adversarial networks (E-GAN) tries to alleviate mode
collapse and gradient vanish that plague generative adversarial networks by
introducing evolutionary computation. But because of the lack of a reasonable
evaluation mechanism, it did not achieve its design purpose. And it contains
only mutation operators in its evolutionary step, but not crossover operator
which are equally common with it. In this paper, we firstly point out the
shortcomings of the diversity fitness function of E-GAN and propose a new
function. Then we propose a universal crossover operator over knowledge
distillation, which can be widely applied to evolutionary GANs and complement
the missing crossover variation of E-GAN. Incorporating the fitness function
and crossover operator we design an evolutionary GAN framework named improved
evolutionary generative adversarial networks (IE-GAN) and combine E-GAN to
complete an algorithm implementation. Experiments on various datasets
demonstrate the effectiveness of IE-GAN and show that our method is competitive
in terms of generated samples quality and time efficiency.

    

### [[2109.11086] Scenario Aware Speech Recognition: Advancements for Apollo Fearless Steps & CHiME-4 Corpora](http://arxiv.org/abs/2109.11086)


  In this study, we propose to investigate triplet loss for the purpose of an
alternative feature representation for ASR. We consider a general non-semantic
speech representation, which is trained with a self-supervised criteria based
on triplet loss called TRILL, for acoustic modeling to represent the acoustic
characteristics of each audio. This strategy is then applied to the CHiME-4
corpus and CRSS-UTDallas Fearless Steps Corpus, with emphasis on the 100-hour
challenge corpus which consists of 5 selected NASA Apollo-11 channels. An
analysis of the extracted embeddings provides the foundation needed to
characterize training utterances into distinct groups based on acoustic
distinguishing properties. Moreover, we also demonstrate that triplet-loss
based embedding performs better than i-Vector in acoustic modeling, confirming
that the triplet loss is more effective than a speaker feature. With additional
techniques such as pronunciation and silence probability modeling, plus
multi-style training, we achieve a +5.42% and +3.18% relative WER improvement
for the development and evaluation sets of the Fearless Steps Corpus. To
explore generalization, we further test the same technique on the 1 channel
track of CHiME-4 and observe a +11.90% relative WER improvement for real test
data.

    

### [[2109.11094] PredictionNet: Real-Time Joint Probabilistic Traffic Prediction for Planning, Control, and Simulation](http://arxiv.org/abs/2109.11094)


  Predicting the future motion of traffic agents is crucial for safe and
efficient autonomous driving. To this end, we present PredictionNet, a deep
neural network (DNN) that predicts the motion of all surrounding traffic agents
together with the ego-vehicle's motion. All predictions are probabilistic and
are represented in a simple top-down rasterization that allows an arbitrary
number of agents. Conditioned on a multilayer map with lane information, the
network outputs future positions, velocities, and backtrace vectors jointly for
all agents including the ego-vehicle in a single pass. Trajectories are then
extracted from the output. The network can be used to simulate realistic
traffic, and it produces competitive results on popular benchmarks. More
importantly, it has been used to successfully control a real-world vehicle for
hundreds of kilometers, by combining it with a motion planning/control
subsystem. The network runs faster than real-time on an embedded GPU, and the
system shows good generalization (across sensory modalities and locations) due
to the choice of input representation. Furthermore, we demonstrate that by
extending the DNN with reinforcement learning (RL), it can better handle rare
or unsafe events like aggressive maneuvers and crashes.

    

### [[2109.11125] Adversarial Transfer Attacks With Unknown Data and Class Overlap](http://arxiv.org/abs/2109.11125)


  The ability to transfer adversarial attacks from one model (the surrogate) to
another model (the victim) has been an issue of concern within the machine
learning (ML) community. The ability to successfully evade unseen models
represents an uncomfortable level of ease toward implementing attacks. In this
work we note that as studied, current transfer attack research has an
unrealistic advantage for the attacker: the attacker has the exact same
training data as the victim. We present the first study of transferring
adversarial attacks focusing on the data available to attacker and victim under
imperfect settings without querying the victim, where there is some variable
level of overlap in the exact data used or in the classes learned by each
model. This threat model is relevant to applications in medicine, malware, and
others. Under this new threat model attack success rate is not correlated with
data or class overlap in the way one would expect, and varies with dataset.
This makes it difficult for attacker and defender to reason about each other
and contributes to the broader study of model robustness and security. We
remedy this by developing a masked version of Projected Gradient Descent that
simulates class disparity, which enables the attacker to reliably estimate a
lower-bound on their attack's success.

    

### [[2109.11126] A Framework for Cluster and Classifier Evaluation in the Absence of Reference Labels](http://arxiv.org/abs/2109.11126)


  In some problem spaces, the high cost of obtaining ground truth labels
necessitates use of lower quality reference datasets. It is difficult to
benchmark model performance using these datasets, as evaluation results may be
biased. We propose a supplement to using reference labels, which we call an
approximate ground truth refinement (AGTR). Using an AGTR, we prove that bounds
on specific metrics used to evaluate clustering algorithms and multi-class
classifiers can be computed without reference labels. We also introduce a
procedure that uses an AGTR to identify inaccurate evaluation results produced
from datasets of dubious quality. Creating an AGTR requires domain knowledge,
and malware family classification is a task with robust domain knowledge
approaches that support the construction of an AGTR. We demonstrate our AGTR
evaluation framework by applying it to a popular malware labeling tool to
diagnose over-fitting in prior testing and evaluate changes whose impact could
not be meaningfully quantified under previous data.

    

### [[2109.11135] Memory-Efficient Convex Optimization for Self-Dictionary Separable Nonnegative Matrix Factorization: A Frank-Wolfe Approach](http://arxiv.org/abs/2109.11135)


  Nonnegative matrix factorization (NMF) often relies on the separability
condition for tractable algorithm design. Separability-based NMF is mainly
handled by two types of approaches, namely, greedy pursuit and convex
programming. A notable convex NMF formulation is the so-called self-dictionary
multiple measurement vectors (SD-MMV), which can work without knowing the
matrix rank a priori, and is arguably more resilient to error propagation
relative to greedy pursuit. However, convex SD-MMV renders a large memory cost
that scales quadratically with the problem size. This memory challenge has been
around for a decade, and a major obstacle for applying convex SD-MMV to big
data analytics. This work proposes a memory-efficient algorithm for convex
SD-MMV. Our algorithm capitalizes on the special update rules of a classic
algorithm from the 1950s, namely, the Frank-Wolfe (FW) algorithm. It is shown
that, under reasonable conditions, the FW algorithm solves the noisy SD-MMV
problem with a memory cost that grows linearly with the amount of data. To
handle noisier scenarios, a smoothed group sparsity regularizer is proposed to
improve robustness while maintaining the low memory footprint with guarantees.
The proposed approach presents the first linear memory complexity algorithmic
framework for convex SD-MMV based NMF. The method is tested over a couple of
unsupervised learning tasks, i.e., text mining and community detection, to
showcase its effectiveness and memory efficiency.

    

### [[2109.11140] Joint speaker diarisation and tracking in switching state-space model](http://arxiv.org/abs/2109.11140)


  Speakers may move around while diarisation is being performed. When a
microphone array is used, the instantaneous locations of where the sounds
originated from can be estimated, and previous investigations have shown that
such information can be complementary to speaker embeddings in the diarisation
task. However, these approaches often assume that speakers are fairly
stationary throughout a meeting. This paper relaxes this assumption, by
proposing to explicitly track the movements of speakers while jointly
performing diarisation within a unified model. A state-space model is proposed,
where the hidden state expresses the identity of the current active speaker and
the predicted locations of all speakers. The model is implemented as a particle
filter. Experiments on a Microsoft rich meeting transcription task show that
the proposed joint location tracking and diarisation approach is able to
perform comparably with other methods that use location information.

    

### [[2109.11154] Rank Overspecified Robust Matrix Recovery: Subgradient Method and Exact Recovery](http://arxiv.org/abs/2109.11154)


  We study the robust recovery of a low-rank matrix from sparsely and grossly
corrupted Gaussian measurements, with no prior knowledge on the intrinsic rank.
We consider the robust matrix factorization approach. We employ a robust
$\ell_1$ loss function and deal with the challenge of the unknown rank by using
an overspecified factored representation of the matrix variable. We then solve
the associated nonconvex nonsmooth problem using a subgradient method with
diminishing stepsizes. We show that under a regularity condition on the sensing
matrices and corruption, which we call restricted direction preserving property
(RDPP), even with rank overspecified, the subgradient method converges to the
exact low-rank solution at a sublinear rate. Moreover, our result is more
general in the sense that it automatically speeds up to a linear rate once the
factor rank matches the unknown rank. On the other hand, we show that the RDPP
condition holds under generic settings, such as Gaussian measurements under
independent or adversarial sparse corruptions, where the result could be of
independent interest. Both the exact recovery and the convergence rate of the
proposed subgradient method are numerically verified in the overspecified
regime. Moreover, our experiment further shows that our particular design of
diminishing stepsize effectively prevents overfitting for robust recovery under
overparameterized models, such as robust matrix sensing and learning robust
deep image prior. This regularization effect is worth further investigation.

    

### [[2109.11160] Toward a Unified Framework for Debugging Gray-box Models](http://arxiv.org/abs/2109.11160)


  We are concerned with debugging concept-based gray-box models (GBMs). These
models acquire task-relevant concepts appearing in the inputs and then compute
a prediction by aggregating the concept activations. This work stems from the
observation that in GBMs both the concepts and the aggregation function can be
affected by different bugs, and that correcting these bugs requires different
kinds of corrective supervision. To this end, we introduce a simple schema for
identifying and prioritizing bugs in both components, discuss possible
implementations and open problems. At the same time, we introduce a new loss
function for debugging the aggregation step that extends existing approaches to
align the model's explanations to GBMs by making them robust to how the
concepts change during training.

    

### [[2109.11171] Zero-Shot Information Extraction as a Unified Text-to-Triple Translation](http://arxiv.org/abs/2109.11171)


  We cast a suite of information extraction tasks into a text-to-triple
translation framework. Instead of solving each task relying on task-specific
datasets and models, we formalize the task as a translation between
task-specific input text and output triples. By taking the task-specific input,
we enable a task-agnostic translation by leveraging the latent knowledge that a
pre-trained language model has about the task. We further demonstrate that a
simple pre-training task of predicting which relational information corresponds
to which input text is an effective way to produce task-specific outputs. This
enables the zero-shot transfer of our framework to downstream tasks. We study
the zero-shot performance of this framework on open information extraction
(OIE2016, NYT, WEB, PENN), relation classification (FewRel and TACRED), and
factual probe (Google-RE and T-REx). The model transfers non-trivially to most
tasks and is often competitive with a fully supervised method without the need
for any task-specific training. For instance, we significantly outperform the
F1 score of the supervised open information extraction without needing to use
its training set.

    

### [[2109.11172] Clustering performance analysis using new correlation based cluster validity indices](http://arxiv.org/abs/2109.11172)


  There are various cluster validity measures used for evaluating clustering
results. One of the main objective of using these measures is to seek the
optimal unknown number of clusters. Some measures work well for clusters with
different densities, sizes and shapes. Yet, one of the weakness that those
validity measures share is that they sometimes provide only one clear optimal
number of clusters. That number is actually unknown and there might be more
than one potential sub-optimal options that a user may wish to choose based on
different applications. We develop two new cluster validity indices based on a
correlation between an actual distance between a pair of data points and a
centroid distance of clusters that the two points locate in. Our proposed
indices constantly yield several peaks at different numbers of clusters which
overcome the weakness previously stated. Furthermore, the introduced
correlation can also be used for evaluating the quality of a selected
clustering result. Several experiments in different scenarios including the
well-known iris data set and a real-world marketing application have been
conducted in order to compare the proposed validity indices with several
well-known ones.

    

### [[2109.11178] Hierarchies of Planning and Reinforcement Learning for Robot Navigation](http://arxiv.org/abs/2109.11178)


  Solving robotic navigation tasks via reinforcement learning (RL) is
challenging due to their sparse reward and long decision horizon nature.
However, in many navigation tasks, high-level (HL) task representations, like a
rough floor plan, are available. Previous work has demonstrated efficient
learning by hierarchal approaches consisting of path planning in the HL
representation and using sub-goals derived from the plan to guide the RL policy
in the source task. However, these approaches usually neglect the complex
dynamics and sub-optimal sub-goal-reaching capabilities of the robot during
planning. This work overcomes these limitations by proposing a novel
hierarchical framework that utilizes a trainable planning policy for the HL
representation. Thereby robot capabilities and environment conditions can be
learned utilizing collected rollout data. We specifically introduce a planning
policy based on value iteration with a learned transition model (VI-RL). In
simulated robotic navigation tasks, VI-RL results in consistent strong
improvement over vanilla RL, is on par with vanilla hierarchal RL on single
layouts but more broadly applicable to multiple layouts, and is on par with
trainable HL path planning baselines except for a parking task with difficult
non-holonomic dynamics where it shows marked improvements.

    

### [[2109.11192] Predicting the Timing of Camera Movements From the Kinematics of Instruments in Robotic-Assisted Surgery Using Artificial Neural Networks](http://arxiv.org/abs/2109.11192)


  Robotic-assisted surgeries benefit both surgeons and patients, however,
surgeons frequently need to adjust the endoscopic camera to achieve good
viewpoints. Simultaneously controlling the camera and the surgical instruments
is impossible, and consequentially, these camera adjustments repeatedly
interrupt the surgery. Autonomous camera control could help overcome this
challenge, but most existing systems are reactive, e.g., by having the camera
follow the surgical instruments. We propose a predictive approach for
anticipating when camera movements will occur using artificial neural networks.
We used the kinematic data of the surgical instruments, which were recorded
during robotic-assisted surgical training on porcine models. We split the data
into segments, and labeled each either as a segment that immediately precedes a
camera movement, or one that does not. Due to the large class imbalance, we
trained an ensemble of networks, each on a balanced sub-set of the training
data. We found that the instruments' kinematic data can be used to predict when
camera movements will occur, and evaluated the performance on different segment
durations and ensemble sizes. We also studied how much in advance an upcoming
camera movement can be predicted, and found that predicting a camera movement
0.25, 0.5, and 1 second before they occurred achieved 98%, 94%, and 84%
accuracy relative to the prediction of an imminent camera movement. This
indicates that camera movement events can be predicted early enough to leave
time for computing and executing an autonomous camera movement and suggests
that an autonomous camera controller for RAMIS may one day be feasible.

    

### [[2109.11196] Fast and Efficient MMD-based Fair PCA via Optimization over Stiefel Manifold](http://arxiv.org/abs/2109.11196)


  This paper defines fair principal component analysis (PCA) as minimizing the
maximum mean discrepancy (MMD) between dimensionality-reduced conditional
distributions of different protected classes. The incorporation of MMD
naturally leads to an exact and tractable mathematical formulation of fairness
with good statistical properties. We formulate the problem of fair PCA subject
to MMD constraints as a non-convex optimization over the Stiefel manifold and
solve it using the Riemannian Exact Penalty Method with Smoothing (REPMS; Liu
and Boumal, 2019). Importantly, we provide local optimality guarantees and
explicitly show the theoretical effect of each hyperparameter in practical
settings, extending previous results. Experimental comparisons based on
synthetic and UCI datasets show that our approach outperforms prior work in
explained variance, fairness, and runtime.

    

### [[2109.11200] Secure PAC Bayesian Regression via Real Shamir Secret Sharing](http://arxiv.org/abs/2109.11200)


  Common approach of machine learning is to generate a model by using huge
amount of training data to predict the test data instances as accurate as
possible. Nonetheless, concerns about data privacy are increasingly raised, but
not always addressed. We present a secure protocol for obtaining a linear model
relying on recently described technique called real number secret sharing. We
take as our starting point the PAC Bayesian bounds and deduce a closed form for
the model parameters which depends on the data and the prior from the PAC
Bayesian bounds. To obtain the model parameters one need to solve a linear
system. However, we consider the situation where several parties hold different
data instances and they are not willing to give up the privacy of the data.
Hence, we suggest to use real number secret sharing and multiparty computation
to share the data and solve the linear regression in a secure way without
violating the privacy of data. We suggest two methods; an inverse method and a
Gaussian elimination method, and compare these methods at the end.

    

### [[2109.11201] Multi-view Contrastive Self-Supervised Learning of Accounting Data Representations for Downstream Audit Tasks](http://arxiv.org/abs/2109.11201)


  International audit standards require the direct assessment of a financial
statement's underlying accounting transactions, referred to as journal entries.
Recently, driven by the advances in artificial intelligence, deep learning
inspired audit techniques have emerged in the field of auditing vast quantities
of journal entry data. Nowadays, the majority of such methods rely on a set of
specialized models, each trained for a particular audit task. At the same time,
when conducting a financial statement audit, audit teams are confronted with
(i) challenging time-budget constraints, (ii) extensive documentation
obligations, and (iii) strict model interpretability requirements. As a result,
auditors prefer to harness only a single preferably `multi-purpose' model
throughout an audit engagement. We propose a contrastive self-supervised
learning framework designed to learn audit task invariant accounting data
representations to meet this requirement. The framework encompasses deliberate
interacting data augmentation policies that utilize the attribute
characteristics of journal entry data. We evaluate the framework on two
real-world datasets of city payments and transfer the learned representations
to three downstream audit tasks: anomaly detection, audit sampling, and audit
documentation. Our experimental results provide empirical evidence that the
proposed framework offers the ability to increase the efficiency of audits by
learning rich and interpretable `multi-task' representations.

    

### [[2109.11225] ChannelAugment: Improving generalization of multi-channel ASR by training with input channel randomization](http://arxiv.org/abs/2109.11225)


  End-to-end (E2E) multi-channel ASR systems show state-of-the-art performance
in far-field ASR tasks by joint training of a multi-channel front-end along
with the ASR model. The main limitation of such systems is that they are
usually trained with data from a fixed array geometry, which can lead to
degradation in accuracy when a different array is used in testing. This makes
it challenging to deploy these systems in practice, as it is costly to retrain
and deploy different models for various array configurations. To address this,
we present a simple and effective data augmentation technique, which is based
on randomly dropping channels in the multi-channel audio input during training,
in order to improve the robustness to various array configurations at test
time. We call this technique ChannelAugment, in contrast to SpecAugment (SA)
which drops time and/or frequency components of a single channel input audio.
We apply ChannelAugment to the Spatial Filtering (SF) and Minimum Variance
Distortionless Response (MVDR) neural beamforming approaches. For SF, we
observe 10.6% WER improvement across various array configurations employing
different numbers of microphones. For MVDR, we achieve a 74% reduction in
training time without causing degradation of recognition accuracy.

    

### [[2109.11234] Tactile Grasp Refinement using Deep Reinforcement Learning and Analytic Grasp Stability Metrics](http://arxiv.org/abs/2109.11234)


  Reward functions are at the heart of every reinforcement learning (RL)
algorithm. In robotic grasping, rewards are often complex and manually
engineered functions that do not rely on well-justified physical models from
grasp analysis. This work demonstrates that analytic grasp stability metrics
constitute powerful optimization objectives for RL algorithms that refine
grasps on a three-fingered hand using only tactile and joint position
information. We outperform a binary-reward baseline by 42.9% and find that a
combination of geometric and force-agnostic grasp stability metrics yields the
highest average success rates of 95.4% for cuboids, 93.1% for cylinders, and
62.3% for spheres across wrist position errors between 0 and 7 centimeters and
rotational errors between 0 and 14 degrees. In a second experiment, we show
that grasp refinement algorithms trained with contact feedback (contact
positions, normals, and forces) perform up to 6.6% better than a baseline that
receives no tactile information.

    

### [[2109.11246] Coded Computation across Shared Heterogeneous Workers with Communication Delay](http://arxiv.org/abs/2109.11246)


  Distributed computing enables large-scale computation tasks to be processed
over multiple workers in parallel. However, the randomness of communication and
computation delays across workers causes the straggler effect, which may
degrade the performance. Coded computation helps to mitigate the straggler
effect, but the amount of redundant load and their assignment to the workers
should be carefully optimized. In this work, we consider a multi-master
heterogeneous-worker distributed computing scenario, where multiple matrix
multiplication tasks are encoded and allocated to workers for parallel
computation. The goal is to minimize the communication plus computation delay
of the slowest task. We propose worker assignment, resource allocation and load
allocation algorithms under both dedicated and fractional worker assignment
policies, where each worker can process the encoded tasks of either a single
master or multiple masters, respectively. Then, the non-convex delay
minimization problem is solved by employing the Markov's inequality-based
approximation, Karush-Kuhn-Tucker conditions, and successive convex
approximation methods. Through extensive simulations, we show that the proposed
algorithms can reduce the task completion delay compared to the benchmarks, and
observe that dedicated and fractional worker assignment policies have different
scopes of applications.

    

### [[2109.11249] FooBaR: Fault Fooling Backdoor Attack on Neural Network Training](http://arxiv.org/abs/2109.11249)


  Neural network implementations are known to be vulnerable to physical attack
vectors such as fault injection attacks. As of now, these attacks were only
utilized during the inference phase with the intention to cause a
misclassification. In this work, we explore a novel attack paradigm by
injecting faults during the training phase of a neural network in a way that
the resulting network can be attacked during deployment without the necessity
of further faulting. In particular, we discuss attacks against ReLU activation
functions that make it possible to generate a family of malicious inputs, which
are called fooling inputs, to be used at inference time to induce controlled
misclassifications. Such malicious inputs are obtained by mathematically
solving a system of linear equations that would cause a particular behaviour on
the attacked activation functions, similar to the one induced in training
through faulting. We call such attacks fooling backdoors as the fault attacks
at the training phase inject backdoors into the network that allow an attacker
to produce fooling inputs. We evaluate our approach against multi-layer
perceptron networks and convolutional networks on a popular image
classification task obtaining high attack success rates (from 60% to 100%) and
high classification confidence when as little as 25 neurons are attacked while
preserving high accuracy on the originally intended classification task.

    

### [[2109.11282] Unbiased Loss Functions for Multilabel Classification with Missing Labels](http://arxiv.org/abs/2109.11282)


  This paper considers binary and multilabel classification problems in a
setting where labels are missing independently and with a known rate. Missing
labels are a ubiquitous phenomenon in extreme multi-label classification (XMC)
tasks, such as matching Wikipedia articles to a small subset out of the
hundreds of thousands of possible tags, where no human annotator can possibly
check the validity of all the negative samples. For this reason,
propensity-scored precision -- an unbiased estimate for precision-at-k under a
known noise model -- has become one of the standard metrics in XMC. Few methods
take this problem into account already during the training phase, and all are
limited to loss functions that can be decomposed into a sum of contributions
from each individual label. A typical approach to training is to reduce the
multilabel problem into a series of binary or multiclass problems, and it has
been shown that if the surrogate task should be consistent for optimizing
recall, the resulting loss function is not decomposable over labels. Therefore,
this paper derives the unique unbiased estimators for the different multilabel
reductions, including the non-decomposable ones. These estimators suffer from
increased variance and may lead to ill-posed optimization problems, which we
address by switching to convex upper-bounds. The theoretical considerations are
further supplemented by an experimental study showing that the switch to
unbiased estimators significantly alters the bias-variance trade-off and may
thus require stronger regularization, which in some cases can negate the
benefits of unbiased estimation.

    

### [[2109.11288] Enhancing Navigational Safety in Crowded Environments using Semantic-Deep-Reinforcement-Learning-based Navigation](http://arxiv.org/abs/2109.11288)


  Intelligent navigation among social crowds is an essential aspect of mobile
robotics for applications such as delivery, health care, or assistance. Deep
Reinforcement Learning emerged as an alternative planning method to
conservative approaches and promises more efficient and flexible navigation.
However, in highly dynamic environments employing different kinds of obstacle
classes, safe navigation still presents a grand challenge. In this paper, we
propose a semantic Deep-reinforcement-learning-based navigation approach that
teaches object-specific safety rules by considering high-level obstacle
information. In particular, the agent learns object-specific behavior by
contemplating the specific danger zones to enhance safety around vulnerable
object classes. We tested the approach against a benchmark obstacle avoidance
approach and found an increase in safety. Furthermore, we demonstrate that the
agent could learn to navigate more safely by keeping an individual safety
distance dependent on the semantic information.

    

### [[2109.11295] Dynamic Knowledge Distillation for Pre-trained Language Models](http://arxiv.org/abs/2109.11295)


  Knowledge distillation~(KD) has been proved effective for compressing
large-scale pre-trained language models. However, existing methods conduct KD
statically, e.g., the student model aligns its output distribution to that of a
selected teacher model on the pre-defined training dataset. In this paper, we
explore whether a dynamic knowledge distillation that empowers the student to
adjust the learning procedure according to its competency, regarding the
student performance and learning efficiency. We explore the dynamical
adjustments on three aspects: teacher model adoption, data selection, and KD
objective adaptation. Experimental results show that (1) proper selection of
teacher model can boost the performance of student model; (2) conducting KD
with 10% informative instances achieves comparable performance while greatly
accelerates the training; (3) the student performance can be boosted by
adjusting the supervision contribution of different alignment objective. We
find dynamic knowledge distillation is promising and provide discussions on
potential future directions towards more efficient KD methods. Our code is
available at this https URL.

    

### [[2109.11301] A Survey on Cost Types, Interaction Schemes, and Annotator Performance Models in Selection Algorithms for Active Learning in Classification](http://arxiv.org/abs/2109.11301)


  Pool-based active learning (AL) aims to optimize the annotation process
(i.e., labeling) as the acquisition of annotations is often time-consuming and
therefore expensive. For this purpose, an AL strategy queries annotations
intelligently from annotators to train a high-performance classification model
at a low annotation cost. Traditional AL strategies operate in an idealized
framework. They assume a single, omniscient annotator who never gets tired and
charges uniformly regardless of query difficulty. However, in real-world
applications, we often face human annotators, e.g., crowd or in-house workers,
who make annotation mistakes and can be reluctant to respond if tired or faced
with complex queries. Recently, a wide range of novel AL strategies has been
proposed to address these issues. They differ in at least one of the following
three central aspects from traditional AL: (1) They explicitly consider
(multiple) human annotators whose performances can be affected by various
factors, such as missing expertise. (2) They generalize the interaction with
human annotators by considering different query and annotation types, such as
asking an annotator for feedback on an inferred classification rule. (3) They
take more complex cost schemes regarding annotations and misclassifications
into account. This survey provides an overview of these AL strategies and
refers to them as real-world AL. Therefore, we introduce a general real-world
AL strategy as part of a learning cycle and use its elements, e.g., the query
and annotator selection algorithm, to categorize about 60 real-world AL
strategies. Finally, we outline possible directions for future research in the
field of AL.

    

### [[2109.11311] Multi-resolution deep learning pipeline for dense large scale point clouds](http://arxiv.org/abs/2109.11311)


  Recent development of 3D sensors allows the acquisition of extremely dense 3D
point clouds of large-scale scenes. The main challenge of processing such large
point clouds remains in the size of the data, which induce expensive
computational and memory cost. In this context, the full resolution cloud is
particularly hard to process, and details it brings are rarely exploited.
Although fine-grained details are important for detection of small objects,
they can alter the local geometry of large structural parts and mislead deep
learning networks. In this paper, we introduce a new generic deep learning
pipeline to exploit the full precision of large scale point clouds, but only
for objects that require details. The core idea of our approach is to split up
the process into multiple sub-networks which operate on different resolutions
and with each their specific classes to retrieve. Thus, the pipeline allows
each class to benefit either from noise and memory cost reduction of a
sub-sampling or from fine-grained details.

    

### [[2109.11319] Active Learning for Argument Strength Estimation](http://arxiv.org/abs/2109.11319)


  High-quality arguments are an essential part of decision-making.
Automatically predicting the quality of an argument is a complex task that
recently got much attention in argument mining. However, the annotation effort
for this task is exceptionally high. Therefore, we test uncertainty-based
active learning (AL) methods on two popular argument-strength data sets to
estimate whether sample-efficient learning can be enabled. Our extensive
empirical evaluation shows that uncertainty-based acquisition functions can not
surpass the accuracy reached with the random acquisition on these data sets.

    

### [[2109.11328] Reinforcement Learning Under Algorithmic Triage](http://arxiv.org/abs/2109.11328)


  Methods to learn under algorithmic triage have predominantly focused on
supervised learning settings where each decision, or prediction, is independent
of each other. Under algorithmic triage, a supervised learning model predicts a
fraction of the instances and humans predict the remaining ones. In this work,
we take a first step towards developing reinforcement learning models that are
optimized to operate under algorithmic triage. To this end, we look at the
problem through the framework of options and develop a two-stage actor-critic
method to learn reinforcement learning models under triage. The first stage
performs offline, off-policy training using human data gathered in an
environment where the human has operated on their own. The second stage
performs on-policy training to account for the impact that switching may have
on the human policy, which may be difficult to anticipate from the above human
data. Extensive simulation experiments in a synthetic car driving task show
that the machine models and the triage policies trained using our two-stage
method effectively complement human policies and outperform those provided by
several competitive baselines.

    

### [[2109.11330] Quantum algorithms for group convolution, cross-correlation, and equivariant transformations](http://arxiv.org/abs/2109.11330)


  Group convolutions and cross-correlations, which are equivariant to the
actions of group elements, are commonly used in mathematics to analyze or take
advantage of symmetries inherent in a given problem setting. Here, we provide
efficient quantum algorithms for performing linear group convolutions and
cross-correlations on data stored as quantum states. Runtimes for our
algorithms are logarithmic in the dimension of the group thus offering an
exponential speedup compared to classical algorithms when input data is
provided as a quantum state and linear operations are well conditioned.
Motivated by the rich literature on quantum algorithms for solving algebraic
problems, our theoretical framework opens a path for quantizing many algorithms
in machine learning and numerical methods that employ group operations.

    

### [[2109.11338] Orthogonal Graph Neural Networks](http://arxiv.org/abs/2109.11338)


  Graph neural networks (GNNs) have received tremendous attention due to their
superiority in learning node representations. These models rely on message
passing and feature transformation functions to encode the structural and
feature information from neighbors. However, stacking more convolutional layers
significantly decreases the performance of GNNs. Most recent studies attribute
this limitation to the over-smoothing issue, where node embeddings converge to
indistinguishable vectors. Through a number of experimental observations, we
argue that the main factor degrading the performance is the unstable forward
normalization and backward gradient resulted from the improper design of the
feature transformation, especially for shallow GNNs where the over-smoothing
has not happened. Therefore, we propose a novel orthogonal feature
transformation, named Ortho-GConv, which could generally augment the existing
GNN backbones to stabilize the model training and improve the model's
generalization performance. Specifically, we maintain the orthogonality of the
feature transformation comprehensively from three perspectives, namely hybrid
weight initialization, orthogonal transformation, and orthogonal
regularization. By equipping the existing GNNs (e.g. GCN, JKNet, GCNII) with
Ortho-GConv, we demonstrate the generality of the orthogonal feature
transformation to enable stable training, and show its effectiveness for node
and graph classification tasks.

    

### [[2109.11345] Graph Neural Netwrok with Interaction Pattern for Group Recommendation](http://arxiv.org/abs/2109.11345)


  With the development of social platforms, people are more and more inclined
to combine into groups to participate in some activities, so group
recommendation has gradually become a problem worthy of research. For group
recommendation, an important issue is how to obtain the characteristic
representation of the group and the item through personal interaction history,
and obtain the group's preference for the item. For this problem, we proposed
the model GIP4GR (Graph Neural Network with Interaction Pattern For Group
Recommendation). Specifically, our model use the graph neural network framework
with powerful representation capabilities to represent the interaction between
group-user-items in the topological structure of the graph, and at the same
time, analyze the interaction pattern of the graph to adjust the feature output
of the graph neural network, the feature representations of groups, and items
are obtained to calculate the group's preference for items. We conducted a lot
of experiments on two real-world datasets to illustrate the superior
performance of our model.

    

### [[2109.11354] Arbitrary-Depth Universal Approximation Theorems for Operator Neural Networks](http://arxiv.org/abs/2109.11354)


  The standard Universal Approximation Theorem for operator neural networks
(NNs) holds for arbitrary width and bounded depth. Here, we prove that operator
NNs of bounded width and arbitrary depth are universal approximators for
continuous nonlinear operators. In our main result, we prove that for
non-polynomial activation functions that are continuously differentiable at a
point with a nonzero derivative, one can construct an operator NN of width
five, whose inputs are real numbers with finite decimal representations, that
is arbitrarily close to any given continuous nonlinear operator. We derive an
analogous result for non-affine polynomial activation functions. We also show
that depth has theoretical advantages by constructing operator ReLU NNs of
depth $2k^3+8$ and constant width that cannot be well-approximated by any
operator ReLU NN of depth $k$, unless its width is exponential in $k$.

    

### [[2109.11375] Stochastic Normalizing Flows for Inverse Problems: a Markov Chains Viewpoint](http://arxiv.org/abs/2109.11375)


  To overcome topological constraints and improve the expressiveness of
normalizing flow architectures, Wu, Köhler and Noé introduced stochastic
normalizing flows which combine deterministic, learnable flow transformations
with stochastic sampling methods. In this paper, we consider stochastic
normalizing flows from a Markov chain point of view. In particular, we replace
transition densities by general Markov kernels and establish proofs via
Radon-Nikodym derivatives which allows to incorporate distributions without
densities in a sound way. Further, we generalize the results for sampling from
posterior distributions as required in inverse problems. The performance of the
proposed conditional stochastic normalizing flow is demonstrated by numerical
examples.

    

### [[2109.11377] WRENCH: A Comprehensive Benchmark for Weak Supervision](http://arxiv.org/abs/2109.11377)


  Recent \emph{Weak Supervision (WS)} approaches have had widespread success in
easing the bottleneck of labeling training data for machine learning by
synthesizing labels from multiple potentially noisy supervision sources.
However, proper measurement and analysis of these approaches remain a
challenge. First, datasets used in existing works are often private and/or
custom, limiting standardization. Second, WS datasets with the same name and
base data often vary in terms of the labels and weak supervision sources used,
a significant "hidden" source of evaluation variance. Finally, WS studies often
diverge in terms of the evaluation protocol and ablations used. To address
these problems, we introduce a benchmark platform, \benchmark, for a thorough
and standardized evaluation of WS approaches. It consists of 22 varied
real-world datasets for classification and sequence tagging; a range of real,
synthetic, and procedurally-generated weak supervision sources; and a modular,
extensible framework for WS evaluation, including implementations for popular
WS methods. We use \benchmark to conduct extensive comparisons over more than
100 method variants to demonstrate its efficacy as a benchmark platform. The
code is available at \url{this https URL}.

    

### [[2109.11383] Fast Density Estimation for Density-based Clustering Methods](http://arxiv.org/abs/2109.11383)


  Density-based clustering algorithms are widely used for discovering clusters
in pattern recognition and machine learning since they can deal with
non-hyperspherical clusters and are robustness to handle outliers. However, the
runtime of density-based algorithms is heavily dominated by finding neighbors
and calculating the density of each point which is time-consuming. To address
this issue, this paper proposes a density-based clustering framework by using
the fast principal component analysis, which can be applied to density based
methods to prune unnecessary distance calculations when finding neighbors and
estimating densities. By applying this clustering framework to the Density
Based Spatial Clustering of Applications with Noise (DBSCAN) algorithm, an
improved DBSCAN (called IDBSCAN) is obtained, which preserves the advantage of
DBSCAN and meanwhile, greatly reduces the computation of redundant distances.
Experiments on five benchmark datasets demonstrate that the proposed IDBSCAN
algorithm improves the computational efficiency significantly.

    

### [[2109.11395] Generalisations and improvements of New Q-Newton's method Backtracking](http://arxiv.org/abs/2109.11395)


  In this paper, we propose a general framework for the algorithm New
Q-Newton's method Backtracking, developed in the author's previous work. For a
symmetric, square real matrix $A$, we define $minsp(A):=\min _{||e||=1}
||Ae||$. Given a $C^2$ cost function $f:\mathbb{R}^m\rightarrow \mathbb{R}$ and
a real number $0<\tau $, as well as $m+1$ fixed real numbers $\delta _0,\ldots
,\delta _m$, we define for each $x\in \mathbb{R}^m$ with $\nabla f(x)\not= 0$
the following quantities:
$\kappa :=\min _{i\not= j}|\delta _i-\delta _j|$;
$A(x):=\nabla ^2f(x)+\delta ||\nabla f(x)||^{\tau}Id$, where $\delta$ is the
first element in the sequence $\{\delta _0,\ldots ,\delta _m\}$ for which
$minsp(A(x))\geq \kappa ||\nabla f(x)||^{\tau}$;
$e_1(x),\ldots ,e_m(x)$ are an orthonormal basis of $\mathbb{R}^m$, chosen
appropriately;
$w(x)=$ the step direction, given by the formula: $$w(x)=\sum
_{i=1}^m\frac{<\nabla f(x),e_i(x)>}{||A(x)e_i(x)||}e_i(x);$$ (we can also
normalise by $w(x)/\max \{1,||w(x)||\}$ when needed)
$\gamma (x)>0$ learning rate chosen by Backtracking line search so that
Armijo's condition is satisfied: $$f(x-\gamma (x)w(x))-f(x)\leq
-\frac{1}{3}\gamma (x)<\nabla f(x),w(x)>.$$
The update rule for our algorithm is $x\mapsto H(x)=x-\gamma (x)w(x)$.
In New Q-Newton's method Backtracking, the choices are $\tau =1+\alpha >1$
and $e_1(x),\ldots ,e_m(x)$'s are eigenvectors of $\nabla ^2f(x)$. In this
paper, we allow more flexibility and generality, for example $\tau$ can be
chosen to be $<1$ or $e_1(x),\ldots ,e_m(x)$'s are not necessarily eigenvectors
of $\nabla ^2f(x)$.
New Q-Newton's method Backtracking (as well as Backtracking gradient descent)
is a special case, and some versions have flavours of quasi-Newton's methods.
Several versions allow good theoretical guarantees. An application to solving
systems of polynomial equations is given.

    

### [[2109.11405] Learning the noise fingerprint of quantum devices](http://arxiv.org/abs/2109.11405)


  Noise sources unavoidably affect any quantum technological device. Noise's
main features are expected to strictly depend on the physical platform on which
the quantum device is realized, in the form of a distinguishable fingerprint.
Noise sources are also expected to evolve and change over time. Here, we first
identify and then characterize experimentally the noise fingerprint of IBM
cloud-available quantum computers, by resorting to machine learning techniques
designed to classify noise distributions using time-ordered sequences of
measured outcome probabilities.

    

### [[2109.11406] Named Entity Recognition and Classification on Historical Documents: A Survey](http://arxiv.org/abs/2109.11406)


  After decades of massive digitisation, an unprecedented amount of historical
documents is available in digital format, along with their machine-readable
texts. While this represents a major step forward with respect to preservation
and accessibility, it also opens up new opportunities in terms of content
mining and the next fundamental challenge is to develop appropriate
technologies to efficiently search, retrieve and explore information from this
'big data of the past'. Among semantic indexing opportunities, the recognition
and classification of named entities are in great demand among humanities
scholars. Yet, named entity recognition (NER) systems are heavily challenged
with diverse, historical and noisy inputs. In this survey, we present the array
of challenges posed by historical documents to NER, inventory existing
resources, describe the main approaches deployed so far, and identify key
priorities for future developments.

    

### [[2109.11410] Learning to Robustly Aggregate Labeling Functions for Semi-supervised Data Programming](http://arxiv.org/abs/2109.11410)


  A critical bottleneck in supervised machine learning is the need for large
amounts of labeled data which is expensive and time consuming to obtain.
However, it has been shown that a small amount of labeled data, while
insufficient to re-train a model, can be effectively used to generate
human-interpretable labeling functions (LFs). These LFs, in turn, have been
used to generate a large amount of additional noisy labeled data, in a paradigm
that is now commonly referred to as data programming. However, previous
approaches to automatically generate LFs make no attempt to further use the
given labeled data for model training, thus giving up opportunities for
improved performance. Moreover, since the LFs are generated from a relatively
small labeled dataset, they are prone to being noisy, and naively aggregating
these LFs can lead to very poor performance in practice. In this work, we
propose an LF based reweighting framework \ouralgo{} to solve these two
critical limitations. Our algorithm learns a joint model on the (same) labeled
dataset used for LF induction along with any unlabeled data in a
semi-supervised manner, and more critically, reweighs each LF according to its
goodness, influencing its contribution to the semi-supervised loss using a
robust bi-level optimization algorithm. We show that our algorithm
significantly outperforms prior approaches on several text classification
datasets.

    

### [[2109.11415] A survey of Bayesian Network structure learning](http://arxiv.org/abs/2109.11415)


  Bayesian Networks (BNs) have become increasingly popular over the last few
decades as a tool for reasoning under uncertainty in fields as diverse as
medicine, biology, epidemiology, economics and the social sciences. This is
especially true in real-world areas where we seek to answer complex questions
based on hypothetical evidence to determine actions for intervention. However,
determining the graphical structure of a BN remains a major challenge,
especially when modelling a problem under causal assumptions. Solutions to this
problem include the automated discovery of BN graphs from data, constructing
them based on expert knowledge, or a combination of the two. This paper
provides a comprehensive review of combinatoric algorithms proposed for
learning BN structure from data, describing 61 algorithms including
prototypical, well-established and state-of-the-art approaches. The basic
approach of each algorithm is described in consistent terms, and the
similarities and differences between them highlighted. Methods of evaluating
algorithms and their comparative performance are discussed including the
consistency of claims made in the literature. Approaches for dealing with data
noise in real-world datasets and incorporating expert knowledge into the
learning process are also covered.

    

### [[2109.11422] Programming and Training Rate-Independent Chemical Reaction Networks](http://arxiv.org/abs/2109.11422)


  Embedding computation in biochemical environments incompatible with
traditional electronics is expected to have wide-ranging impact in synthetic
biology, medicine, nanofabrication and other fields. Natural biochemical
systems are typically modeled by chemical reaction networks (CRNs), and CRNs
can be used as a specification language for synthetic chemical computation. In
this paper, we identify a class of CRNs called non-competitive (NC) whose
equilibria are absolutely robust to reaction rates and kinetic rate law,
because their behavior is captured solely by their stoichiometric structure.
Unlike prior work on rate-independent CRNs, checking non-competition and using
it as a design criterion is easy and promises robust output. We also present a
technique to program NC-CRNs using well-founded deep learning methods, showing
a translation procedure from rectified linear unit (ReLU) neural networks to
NC-CRNs. In the case of binary weight ReLU networks, our translation procedure
is surprisingly tight in the sense that a single bimolecular reaction
corresponds to a single ReLU node and vice versa. This compactness argues that
neural networks may be a fitting paradigm for programming rate-independent
chemical computation. As proof of principle, we demonstrate our scheme with
numerical simulations of CRNs translated from neural networks trained on
traditional machine learning datasets (IRIS and MNIST), as well as tasks better
aligned with potential biological applications including virus detection and
spatial pattern formation.

    

### [[2109.11428] An Evaluation of Anomaly Detection and Diagnosis in Multivariate Time Series](http://arxiv.org/abs/2109.11428)


  Several techniques for multivariate time series anomaly detection have been
proposed recently, but a systematic comparison on a common set of datasets and
metrics is lacking. This paper presents a systematic and comprehensive
evaluation of unsupervised and semi-supervised deep-learning based methods for
anomaly detection and diagnosis on multivariate time series data from
cyberphysical systems. Unlike previous works, we vary the model and
post-processing of model errors, i.e. the scoring functions independently of
each other, through a grid of 10 models and 4 scoring functions, comparing
these variants to state of the art methods. In time-series anomaly detection,
detecting anomalous events is more important than detecting individual
anomalous time-points. Through experiments, we find that the existing
evaluation metrics either do not take events into account, or cannot
distinguish between a good detector and trivial detectors, such as a random or
an all-positive detector. We propose a new metric to overcome these drawbacks,
namely, the composite F-score ($Fc_1$), for evaluating time-series anomaly
detection.
Our study highlights that dynamic scoring functions work much better than
static ones for multivariate time series anomaly detection, and the choice of
scoring functions often matters more than the choice of the underlying model.
We also find that a simple, channel-wise model - the Univariate Fully-Connected
Auto-Encoder, with the dynamic Gaussian scoring function emerges as a winning
candidate for both anomaly detection and diagnosis, beating state of the art
algorithms.

    

### [[2109.11429] Robin Hood and Matthew Effects -- Differential Privacy Has Disparate Impact on Synthetic Data](http://arxiv.org/abs/2109.11429)


  Generative models trained using Differential Privacy (DP) are increasingly
used to produce and share synthetic data in a privacy-friendly manner. In this
paper, we set out to analyze the impact of DP on these models vis-a-vis
underrepresented classes and subgroups of data. We do so from two angles: 1)
the size of classes and subgroups in the synthetic data, and 2) classification
accuracy on them. We also evaluate the effect of various levels of imbalance
and privacy budgets.
Our experiments, conducted using three state-of-the-art DP models (PrivBayes,
DP-WGAN, and PATE-GAN), show that DP results in opposite size distributions in
the generated synthetic data. More precisely, it affects the gap between the
majority and minority classes and subgroups, either reducing it (a "Robin Hood"
effect) or increasing it ("Matthew" effect). However, both of these size shifts
lead to similar disparate impacts on a classifier's accuracy, affecting
disproportionately more the underrepresented subparts of the data. As a result,
we call for caution when analyzing or training a model on synthetic data, or
risk treating different subpopulations unevenly, which might also lead to
unreliable conclusions.

    

### [[2109.11434] Exploring Machine Teaching with Children](http://arxiv.org/abs/2109.11434)


  Iteratively building and testing machine learning models can help children
develop creativity, flexibility, and comfort with machine learning and
artificial intelligence. We explore how children use machine teaching
interfaces with a team of 14 children (aged 7-13 years) and adult co-designers.
Children trained image classifiers and tested each other's models for
robustness. Our study illuminates how children reason about ML concepts,
offering these insights for designing machine teaching experiences for
children: (i) ML metrics (e.g. confidence scores) should be visible for
experimentation; (ii) ML activities should enable children to exchange models
for promoting reflection and pattern recognition; and (iii) the interface
should allow quick data inspection (e.g. images vs. gestures).

    

### [[2109.11446] Learning Dynamics from Noisy Measurements using Deep Learning with a Runge-Kutta Constraint](http://arxiv.org/abs/2109.11446)


  Measurement noise is an integral part while collecting data of a physical
process. Thus, noise removal is a necessary step to draw conclusions from these
data, and it often becomes quite essential to construct dynamical models using
these data. We discuss a methodology to learn differential equation(s) using
noisy and sparsely sampled measurements. In our methodology, the main
innovation can be seen in of integration of deep neural networks with a
classical numerical integration method. Precisely, we aim at learning a neural
network that implicitly represents the data and an additional neural network
that models the vector fields of the dependent variables. We combine these two
networks by enforcing the constraint that the data at the next time-steps can
be given by following a numerical integration scheme such as the fourth-order
Runge-Kutta scheme. The proposed framework to learn a model predicting the
vector field is highly effective under noisy measurements. The approach can
handle scenarios where dependent variables are not available at the same
temporal grid. We demonstrate the effectiveness of the proposed method to
learning models using data obtained from various differential equations. The
proposed approach provides a promising methodology to learn dynamic models,
where the first-principle understanding remains opaque.

    

### [[2109.11452] Revisit Geophysical Imaging in A New View of Physics-informed Generative Adversarial Learning](http://arxiv.org/abs/2109.11452)


  Seismic full waveform inversion (FWI) is a powerful geophysical imaging
technique that produces high-resolution subsurface models by iteratively
minimizing the misfit between the simulated and observed seismograms.
Unfortunately, conventional FWI with least-squares function suffers from many
drawbacks such as the local-minima problem and computation of explicit
gradient. It is particularly challenging with the contaminated measurements or
poor starting models. Recent works relying on partial differential equations
and neural networks show promising performance for two-dimensional FWI.
Inspired by the competitive learning of generative adversarial networks, we
proposed an unsupervised learning paradigm that integrates wave equation with a
discriminate network to accurately estimate the physically consistent models in
a distribution sense. Our framework needs no labelled training data nor
pretraining of the network, is flexible to achieve multi-parameters inversion
with minimal user interaction. The proposed method faithfully recovers the
well-known synthetic models that outperforms the classical algorithms.
Furthermore, our work paves the way to sidestep the local-minima issue via
reducing the sensitivity to initial models and noise.

    

### [[2109.11480] Improving Tuberculosis (TB) Prediction using Synthetically Generated Computed Tomography (CT) Images](http://arxiv.org/abs/2109.11480)


  The evaluation of infectious disease processes on radiologic images is an
important and challenging task in medical image analysis. Pulmonary infections
can often be best imaged and evaluated through computed tomography (CT) scans,
which are often not available in low-resource environments and difficult to
obtain for critically ill patients. On the other hand, X-ray, a different type
of imaging procedure, is inexpensive, often available at the bedside and more
widely available, but offers a simpler, two dimensional image. We show that by
relying on a model that learns to generate CT images from X-rays synthetically,
we can improve the automatic disease classification accuracy and provide
clinicians with a different look at the pulmonary disease process.
Specifically, we investigate Tuberculosis (TB), a deadly bacterial infectious
disease that predominantly affects the lungs, but also other organ systems. We
show that relying on synthetically generated CT improves TB identification by
7.50% and distinguishes TB properties up to 12.16% better than the X-ray
baseline.

    

### [[2109.11495] DeepAID: Interpreting and Improving Deep Learning-based Anomaly Detection in Security Applications](http://arxiv.org/abs/2109.11495)


  Unsupervised Deep Learning (DL) techniques have been widely used in various
security-related anomaly detection applications, owing to the great promise of
being able to detect unforeseen threats and superior performance provided by
Deep Neural Networks (DNN). However, the lack of interpretability creates key
barriers to the adoption of DL models in practice. Unfortunately, existing
interpretation approaches are proposed for supervised learning models and/or
non-security domains, which are unadaptable for unsupervised DL models and fail
to satisfy special requirements in security domains.
In this paper, we propose DeepAID, a general framework aiming to (1)
interpret DL-based anomaly detection systems in security domains, and (2)
improve the practicality of these systems based on the interpretations. We
first propose a novel interpretation method for unsupervised DNNs by
formulating and solving well-designed optimization problems with special
constraints for security domains. Then, we provide several applications based
on our Interpreter as well as a model-based extension Distiller to improve
security systems by solving domain-specific problems. We apply DeepAID over
three types of security-related anomaly detection systems and extensively
evaluate our Interpreter with representative prior works. Experimental results
show that DeepAID can provide high-quality interpretations for unsupervised DL
models while meeting the special requirements of security domains. We also
provide several use cases to show that DeepAID can help security operators to
understand model decisions, diagnose system mistakes, give feedback to models,
and reduce false positives.

    

### [[2109.11500] LSTM Hyper-Parameter Selection for Malware Detection: Interaction Effects and Hierarchical Selection Approach](http://arxiv.org/abs/2109.11500)


  Long-Short-Term-Memory (LSTM) networks have shown great promise in artificial
intelligence (AI) based language modeling. Recently, LSTM networks have also
become popular for designing AI-based Intrusion Detection Systems (IDS).
However, its applicability in IDS is studied largely in the default settings as
used in language models. Whereas security applications offer distinct
conditions and hence warrant careful consideration while applying such
recurrent networks. Therefore, we conducted one of the most exhaustive works on
LSTM hyper-parameters for IDS and experimented with approx. 150 LSTM
configurations to determine its hyper-parameters relative importance,
interaction effects, and optimal selection approach for designing an IDS. We
conducted multiple analyses of the results of these experiments and empirically
controlled for the interaction effects of different hyper-parameters covariate
levels. We found that for security applications, especially for designing an
IDS, neither similar relative importance as applicable to language models is
valid, nor is the standard linear method for hyper-parameter selection ideal.
We ascertained that the interaction effect plays a crucial role in determining
the relative importance of hyper-parameters. We also discovered that after
controlling for the interaction effect, the correct relative importance for
LSTMs for an IDS is batch-size, followed by dropout ratio and padding. The
findings are significant because when LSTM was first used for language models,
the focus had mostly been on increasing the number of layers to enhance
performance.

    

### [[2109.11502] Inequality Constrained Stochastic Nonlinear Optimization via Active-Set Sequential Quadratic Programming](http://arxiv.org/abs/2109.11502)


  We study nonlinear optimization problems with stochastic objective and
deterministic equality and inequality constraints, which emerge in numerous
applications including finance, manufacturing, power systems and, recently,
deep neural networks. We propose an active-set stochastic sequential quadratic
programming algorithm, using a differentiable exact augmented Lagrangian as the
merit function. The algorithm adaptively selects the penalty parameters of
augmented Lagrangian and performs stochastic line search to decide the
stepsize. The global convergence is established: for any initialization, the
"liminf" of the KKT residuals converges to zero almost surely. Our algorithm
and analysis further develop the prior work \cite{Na2021Adaptive} by allowing
nonlinear inequality constraints. We demonstrate the performance of the
algorithm on a subset of nonlinear problems collected in the CUTEst test set.

    

### [[2109.11505] Multidimensional Scaling: Approximation and Complexity](http://arxiv.org/abs/2109.11505)


  Metric Multidimensional scaling (MDS) is a classical method for generating
meaningful (non-linear) low-dimensional embeddings of high-dimensional data.
MDS has a long history in the statistics, machine learning, and graph drawing
communities. In particular, the Kamada-Kawai force-directed graph drawing
method is equivalent to MDS and is one of the most popular ways in practice to
embed graphs into low dimensions. Despite its ubiquity, our theoretical
understanding of MDS remains limited as its objective function is highly
non-convex. In this paper, we prove that minimizing the Kamada-Kawai objective
is NP-hard and give a provable approximation algorithm for optimizing it, which
in particular is a PTAS on low-diameter graphs. We supplement this result with
experiments suggesting possible connections between our greedy approximation
algorithm and gradient-based methods.

    

### [[2109.11515] Outlier-Robust Sparse Estimation via Non-Convex Optimization](http://arxiv.org/abs/2109.11515)


  We explore the connection between outlier-robust high-dimensional statistics
and non-convex optimization in the presence of sparsity constraints, with a
focus on the fundamental tasks of robust sparse mean estimation and robust
sparse PCA. We develop novel and simple optimization formulations for these
problems such that any approximate stationary point of the associated
optimization problem yields a near-optimal solution for the underlying robust
estimation task. As a corollary, we obtain that any first-order method that
efficiently converges to stationarity yields an efficient algorithm for these
tasks. The obtained algorithms are simple, practical, and succeed under broader
distributional assumptions compared to prior work.

    

### [[2109.11519] wsGAT: Weighted and Signed Graph Attention Networks for Link Prediction](http://arxiv.org/abs/2109.11519)


  Graph Neural Networks (GNNs) have been widely used to learn representations
on graphs and tackle many real-world problems from a wide range of domains. In
this paper we propose wsGAT, an extension of the Graph Attention Network (GAT)
layers, meant to address the lack of GNNs that can handle graphs with signed
and weighted links, which are ubiquitous, for instance, in trust and
correlation networks. We first evaluate the performance of our proposal by
comparing against GCNII in the weighed link prediction task, and against SGCN
in the link sign prediction task. After that, we combine the two tasks and show
their performance on predicting the signed weight of links, and their
existence. Our results on real-world networks show that models with wsGAT
layers outperform the ones with GCNII and SGCN layers, and that there is no
loss in performance when signed weights are predicted.

    

### [[2109.11523] How much "human-like" visual experience do current self-supervised learning algorithms need to achieve human-level object recognition?](http://arxiv.org/abs/2109.11523)


  This paper addresses a fundamental question: how good are our current
self-supervised visual representation learning algorithms relative to humans?
More concretely, how much "human-like", natural visual experience would these
algorithms need in order to reach human-level performance in a complex,
realistic visual object recognition task such as ImageNet? Using a scaling
experiment, here we estimate that the answer is on the order of a million years
of natural visual experience, in other words several orders of magnitude longer
than a human lifetime. However, this estimate is quite sensitive to some
underlying assumptions, underscoring the need to run carefully controlled human
experiments. We discuss the main caveats surrounding our estimate and the
implications of this rather surprising result.

    

### [[2109.11524] End-to-End AI-based MRI Reconstruction and Lesion Detection Pipeline for Evaluation of Deep Learning Image Reconstruction](http://arxiv.org/abs/2109.11524)


  Deep learning techniques have emerged as a promising approach to highly
accelerated MRI. However, recent reconstruction challenges have shown several
drawbacks in current deep learning approaches, including the loss of fine image
details even using models that perform well in terms of global quality metrics.
In this study, we propose an end-to-end deep learning framework for image
reconstruction and pathology detection, which enables a clinically aware
evaluation of deep learning reconstruction quality. The solution is
demonstrated for a use case in detecting meniscal tears on knee MRI studies,
ultimately finding a loss of fine image details with common reconstruction
methods expressed as a reduced ability to detect important pathology like
meniscal tears. Despite the common practice of quantitative reconstruction
methodology evaluation with metrics such as SSIM, impaired pathology detection
as an automated pathology-based reconstruction evaluation approach suggests
existing quantitative methods do not capture clinically important
reconstruction outcomes.

    

### [[2109.11526] MARMOT: A Deep Learning Framework for Constructing Multimodal Representations for Vision-and-Language Tasks](http://arxiv.org/abs/2109.11526)


  Political activity on social media presents a data-rich window into political
behavior, but the vast amount of data means that almost all content analyses of
social media require a data labeling step. However, most automated machine
classification methods ignore the multimodality of posted content, focusing
either on text or images. State-of-the-art vision-and-language models are
unusable for most political science research: they require all observations to
have both image and text and require computationally expensive pretraining.
This paper proposes a novel vision-and-language framework called multimodal
representations using modality translation (MARMOT). MARMOT presents two
methodological contributions: it can construct representations for observations
missing image or text, and it replaces the computationally expensive
pretraining with modality translation. MARMOT outperforms an ensemble text-only
classifier in 19 of 20 categories in multilabel classifications of tweets
reporting election incidents during the 2016 U.S. general election. Moreover,
MARMOT shows significant improvements over the results of benchmark multimodal
models on the Hateful Memes dataset, improving the best result set by
VisualBERT in terms of accuracy from 0.6473 to 0.6760 and area under the
receiver operating characteristic curve (AUC) from 0.7141 to 0.7530.

    

### [[1905.10214] Partially Encrypted Machine Learning using Functional Encryption](http://arxiv.org/abs/1905.10214)


  Machine learning on encrypted data has received a lot of attention thanks to
recent breakthroughs in homomorphic encryption and secure multi-party
computation. It allows outsourcing computation to untrusted servers without
sacrificing privacy of sensitive data. We propose a practical framework to
perform partially encrypted and privacy-preserving predictions which combines
adversarial training and functional encryption. We first present a new
functional encryption scheme to efficiently compute quadratic functions so that
the data owner controls what can be computed but is not involved in the
calculation: it provides a decryption key which allows one to learn a specific
function evaluation of some encrypted data. We then show how to use it in
machine learning to partially encrypt neural networks with quadratic activation
functions at evaluation time, and we provide a thorough analysis of the
information leaks based on indistinguishability of data items of the same
label. Last, since most encryption schemes cannot deal with the last
thresholding operation used for classification, we propose a training method to
prevent selected sensitive features from leaking, which adversarially optimizes
the network against an adversary trying to identify these features. This is
interesting for several existing works using partially encrypted machine
learning as it comes with little reduction on the model's accuracy and
significantly improves data privacy.

    

### [[1905.13695] RKHSMetaMod: An R package to estimate the Hoeffding decomposition of a complex model by solving RKHS ridge group sparse optimization problem](http://arxiv.org/abs/1905.13695)


  In this paper, we propose an R package, called RKHSMetaMod, that implements a
procedure for estimating a meta-model of a complex model. The meta-model
approximates the Hoeffding decomposition of the complex model and allows us to
perform sensitivity analysis on it. It belongs to a reproducing kernel Hilbert
space that is constructed as a direct sum of Hilbert spaces. The estimator of
the meta-model is the solution of a penalized empirical least-squares
minimization with the sum of the Hilbert norm and the empirical $L^2$-norm.
This procedure, called RKHS ridge group sparse, allows both to select and
estimate the terms in the Hoeffding decomposition, and therefore, to select and
estimate the Sobol indices that are non-zero. The RKHSMetaMod package provides
an interface from R statistical computing environment to the C++ libraries
Eigen and GSL. In order to speed up the execution time and optimize the storage
memory, except for a function that is written in R, all of the functions of
this package are written using the efficient C++ libraries through RcppEigen
and RcppGSL packages. These functions are then interfaced in the R environment
in order to propose a user-friendly package.

    

### [[1906.10502] SampleFix: Learning to Generate Functionally Diverse Fixes](http://arxiv.org/abs/1906.10502)


  Automatic program repair holds the potential of dramatically improving the
productivity of programmers during the software development process and
correctness of software in general. Recent advances in machine learning, deep
learning, and NLP have rekindled the hope to eventually fully automate the
process of repairing programs. However, previous approaches that aim to predict
a single fix are prone to fail due to uncertainty about the true intend of the
programmer. Therefore, we propose a generative model that learns a distribution
over potential fixes. Our model is formulated as a deep conditional variational
autoencoder that can efficiently sample fixes for a given erroneous program. In
order to ensure diverse solutions, we propose a novel regularizer that
encourages diversity over a semantic embedding space. Our evaluations on common
programming errors show for the first time the generation of diverse fixes and
strong improvements over the state-of-the-art approaches by fixing up to 45% of
the erroneous programs. We additionally show that for the 65% of the repaired
programs, our approach was able to generate multiple programs with diverse
functionalities.

    

### [[1909.09345] Does SLOPE outperform bridge regression?](http://arxiv.org/abs/1909.09345)


  A recently proposed SLOPE estimator (arXiv:1407.3824) has been shown to
adaptively achieve the minimax $\ell_2$ estimation rate under high-dimensional
sparse linear regression models (arXiv:1503.08393). Such minimax optimality
holds in the regime where the sparsity level $k$, sample size $n$, and
dimension $p$ satisfy $k/p \rightarrow 0$, $k\log p/n \rightarrow 0$. In this
paper, we characterize the estimation error of SLOPE under the complementary
regime where both $k$ and $n$ scale linearly with $p$, and provide new insights
into the performance of SLOPE estimators. We first derive a concentration
inequality for the finite sample mean square error (MSE) of SLOPE. The quantity
that MSE concentrates around takes a complicated and implicit form. With
delicate analysis of the quantity, we prove that among all SLOPE estimators,
LASSO is optimal for estimating $k$-sparse parameter vectors that do not have
tied non-zero components in the low noise scenario. On the other hand, in the
large noise scenario, the family of SLOPE estimators are sub-optimal compared
with bridge regression such as the Ridge estimator.

    

### [[2002.08949] Improving Sampling Accuracy of Stochastic Gradient MCMC Methods via Non-uniform Subsampling of Gradients](http://arxiv.org/abs/2002.08949)


  Many Markov Chain Monte Carlo (MCMC) methods leverage gradient information of
the potential function of target distribution to explore sample space
efficiently. However, computing gradients can often be computationally
expensive for large scale applications, such as those in contemporary machine
learning. Stochastic Gradient (SG-)MCMC methods approximate gradients by
stochastic ones, commonly via uniformly subsampled data points, and achieve
improved computational efficiency, however at the price of introducing sampling
error. We propose a non-uniform subsampling scheme to improve the sampling
accuracy. The proposed exponentially weighted stochastic gradient (EWSG) is
designed so that a non-uniform-SG-MCMC method mimics the statistical behavior
of a batch-gradient-MCMC method, and hence the inaccuracy due to SG
approximation is reduced. EWSG differs from classical variance reduction (VR)
techniques as it focuses on the entire distribution instead of just the
variance; nevertheless, its reduced local variance is also proved. EWSG can
also be viewed as an extension of the importance sampling idea, successful for
stochastic-gradient-based optimizations, to sampling tasks. In our practical
implementation of EWSG, the non-uniform subsampling is performed efficiently
via a Metropolis-Hastings chain on the data index, which is coupled to the MCMC
algorithm. Numerical experiments are provided, not only to demonstrate EWSG's
effectiveness, but also to guide hyperparameter choices, and validate our
\emph{non-asymptotic global error bound} despite of approximations in the
implementation. Notably, while statistical accuracy is improved, convergence
speed can be comparable to the uniform version, which renders EWSG a practical
alternative to VR (but EWSG and VR can be combined too).

    

### [[2005.07649] Convolutional Neural Network for emotion recognition to assist psychiatrists and psychologists during the COVID-19 pandemic: experts opinion](http://arxiv.org/abs/2005.07649)


  A web application with real-time emotion recognition for psychologists and
psychiatrists is presented. Mental health effects during COVID-19 quarantine
need to be handled because society is being emotionally impacted. The human
micro-expressions can describe genuine emotions that can be captured by
Convolutional Neural Networks (CNN) models. But the challenge is to implement
it under the poor performance of a part of society computers and the low speed
of internet connection, i.e., improve the computational efficiency and reduce
the data transfer. To validate the computational efficiency premise, we compare
CNN architectures results, collecting the floating-point operations per second
(FLOPS), the Number of Parameters (NP) and accuracy from the MobileNet,
PeleeNet, Extended Deep Neural Network (EDNN), Inception- Based Deep Neural
Network (IDNN) and our proposed Residual mobile-based Network model (ResmoNet).
Also, we compare the trained models results in terms of Main Memory Utilization
(MMU) and Response Time to complete the Emotion (RTE) recognition. Besides, we
design a data transfer that includes the raw data of emotions and the basic
patient information. The web application was evaluated with the System
Usability Scale (SUS) and a utility questionnaire by psychologists and
psychiatrists. ResmoNet model generated the most reduced NP, FLOPS, and MMU
results, only EDNN overcomes ResmoNet in 0.01sec in RTE. The optimizations to
our model impacted the accuracy, therefore IDNN and EDNN are 0.02 and 0.05 more
accurate than our model respectively. Finally, according to psychologists and
psychiatrists, the web application has good usability (73.8 of 100) and utility
(3.94 of 5).

    

### [[2006.02032] A Unified Single-loop Alternating Gradient Projection Algorithm for Nonconvex-Concave and Convex-Nonconcave Minimax Problems](http://arxiv.org/abs/2006.02032)


  Much recent research effort has been directed to the development of efficient
algorithms for solving minimax problems with theoretical convergence guarantees
due to the relevance of these problems to a few emergent applications. In this
paper, we propose a unified single-loop alternating gradient projection (AGP)
algorithm for solving smooth nonconvex-(strongly) concave and (strongly)
convex-nonconcave minimax problems. AGP employs simple gradient projection
steps for updating the primal and dual variables alternatively at each
iteration. We show that it can find an $\varepsilon$-stationary point of the
objective function in $\mathcal{O}\left( \varepsilon ^{-2} \right)$ (resp.
$\mathcal{O}\left( \varepsilon ^{-4} \right)$) iterations under
nonconvex-strongly concave (resp. nonconvex-concave) setting. Moreover, its
gradient complexity to obtain an $\varepsilon$-stationary point of the
objective function is bounded by $\mathcal{O}\left( \varepsilon ^{-2} \right)$
(resp., $\mathcal{O}\left( \varepsilon ^{-4} \right)$) under the strongly
convex-nonconcave (resp., convex-nonconcave) setting. To the best of our
knowledge, this is the first time that a simple and unified single-loop
algorithm is developed for solving both nonconvex-(strongly) concave and
(strongly) convex-nonconcave minimax problems. Moreover, the complexity results
for solving the latter (strongly) convex-nonconcave minimax problems have never
been obtained before in the literature. Numerical results show the efficiency
of the proposed AGP algorithm. Furthermore, we extend the AGP algorithm by
presenting a block alternating proximal gradient (BAPG) algorithm for solving
more general multi-block nonsmooth nonconvex-(strongly) concave and (strongly)
convex-nonconcave minimax problems. We can similarly establish the gradient
complexity of the proposed algorithm under these four different settings.

    

### [[2006.10720] IReEn: Reverse-Engineering of Black-Box Functions via Iterative Neural Program Synthesis](http://arxiv.org/abs/2006.10720)


  In this work, we investigate the problem of revealing the functionality of a
black-box agent. Notably, we are interested in the interpretable and formal
description of the behavior of such an agent. Ideally, this description would
take the form of a program written in a high-level language. This task is also
known as reverse engineering and plays a pivotal role in software engineering,
computer security, but also most recently in interpretability. In contrast to
prior work, we do not rely on privileged information on the black box, but
rather investigate the problem under a weaker assumption of having only access
to inputs and outputs of the program. We approach this problem by iteratively
refining a candidate set using a generative neural program synthesis approach
until we arrive at a functionally equivalent program. We assess the performance
of our approach on the Karel dataset. Our results show that the proposed
approach outperforms the state-of-the-art on this challenge by finding an
approximately functional equivalent program in 78% of cases -- even exceeding
prior work that had privileged information on the black-box.

    

### [[2006.15134] Critic Regularized Regression](http://arxiv.org/abs/2006.15134)


  Offline reinforcement learning (RL), also known as batch RL, offers the
prospect of policy optimization from large pre-recorded datasets without online
environment interaction. It addresses challenges with regard to the cost of
data collection and safety, both of which are particularly pertinent to
real-world applications of RL. Unfortunately, most off-policy algorithms
perform poorly when learning from a fixed dataset. In this paper, we propose a
novel offline RL algorithm to learn policies from data using a form of
critic-regularized regression (CRR). We find that CRR performs surprisingly
well and scales to tasks with high-dimensional state and action spaces --
outperforming several state-of-the-art offline RL algorithms by a significant
margin on a wide range of benchmark tasks.

    

### [[2007.10229] Regret Analysis of a Markov Policy Gradient Algorithm for Multi-arm Bandits](http://arxiv.org/abs/2007.10229)


  We consider a policy gradient algorithm applied to a finite-arm bandit
problem with Bernoulli rewards. We allow learning rates to depend on the
current state of the algorithm, rather than use a deterministic time-decreasing
learning rate. The state of the algorithm forms a Markov chain on the
probability simplex. We apply Foster-Lyapunov techniques to analyse the
stability of this Markov chain. We prove that if learning rates are well chosen
then the policy gradient algorithm is a transient Markov chain and the state of
the chain converges on the optimal arm with logarithmic or poly-logarithmic
regret.

    

### [[2007.12684] Deep Co-Training with Task Decomposition for Semi-Supervised Domain Adaptation](http://arxiv.org/abs/2007.12684)


  Semi-supervised domain adaptation (SSDA) aims to adapt models trained from a
labeled source domain to a different but related target domain, from which
unlabeled data and a small set of labeled data are provided. Current methods
that treat source and target supervision without distinction overlook their
inherent discrepancy, resulting in a source-dominated model that has not
effectively used the target supervision. In this paper, we argue that the
labeled target data needs to be distinguished for effective SSDA, and propose
to explicitly decompose the SSDA task into two sub-tasks: a semi-supervised
learning (SSL) task in the target domain and an unsupervised domain adaptation
(UDA) task across domains. By doing so, the two sub-tasks can better leverage
the corresponding supervision and thus yield very different classifiers. To
integrate the strengths of the two classifiers, we apply the well-established
co-training framework, in which the two classifiers exchange their high
confident predictions to iteratively "teach each other" so that both
classifiers can excel in the target domain. We call our approach Deep
Co-training with Task decomposition (DeCoTa). DeCoTa requires no adversarial
training and is easy to implement. Moreover, DeCoTa is well-founded on the
theoretical condition of when co-training would succeed. As a result, DeCoTa
achieves state-of-the-art results on several SSDA datasets, outperforming the
prior art by a notable 4% margin on DomainNet. Code is available at
this https URL


### [[2009.03202] The Seven-League Scheme: Deep learning for large time step Monte Carlo simulations of stochastic differential equations](http://arxiv.org/abs/2009.03202)


  We propose an accurate data-driven numerical scheme to solve Stochastic
Differential Equations (SDEs), by taking large time steps. The SDE
discretization is built up by means of a polynomial chaos expansion method, on
the basis of accurately determined stochastic collocation (SC) points. By
employing an artificial neural network to learn these SC points, we can perform
Monte Carlo simulations with large time steps. Error analysis confirms that
this data-driven scheme results in accurate SDE solutions in the sense of
strong convergence, provided the learning methodology is robust and accurate.
With a method variant called the compression-decompression collocation and
interpolation technique, we can drastically reduce the number of neural network
functions that have to be learned, so that computational speed is enhanced.
Numerical experiments confirm a high-quality strong convergence error when
using large time steps, and the novel scheme outperforms some classical
numerical SDE discretizations. Some applications, here in financial option
valuation, are also presented.

    

### [[2009.11839] A Gradient Flow Framework For Analyzing Network Pruning](http://arxiv.org/abs/2009.11839)


  Recent network pruning methods focus on pruning models early-on in training.
To estimate the impact of removing a parameter, these methods use importance
measures that were originally designed to prune trained models. Despite lacking
justification for their use early-on in training, such measures result in
surprisingly low accuracy loss. To better explain this behavior, we develop a
general framework that uses gradient flow to unify state-of-the-art importance
measures through the norm of model parameters. We use this framework to
determine the relationship between pruning measures and evolution of model
parameters, establishing several results related to pruning models early-on in
training: (i) magnitude-based pruning removes parameters that contribute least
to reduction in loss, resulting in models that converge faster than
magnitude-agnostic methods; (ii) loss-preservation based pruning preserves
first-order model evolution dynamics and is therefore appropriate for pruning
minimally trained models; and (iii) gradient-norm based pruning affects
second-order model evolution dynamics, such that increasing gradient norm via
pruning can produce poorly performing models. We validate our claims on several
VGG-13, MobileNet-V1, and ResNet-56 models trained on CIFAR-10/CIFAR-100. Code
available at this https URL.

    

### [[2010.01413] Correlation between Air and Urban Travelling with New Confirmed Cases of COVID-19 A Case Study](http://arxiv.org/abs/2010.01413)


  COVID-19 which has spread in Iran from February 19, 2020, has infected
202,584 people and killed 9,507 people until June 20, 2020. The immediate
suggested solution to prevent the spread of this virus was to avoid traveling
around. In this study, the correlation between traveling between cities with
new confirmed cases of COVID-19 in Iran is demonstrated. The data, used in the
study, consisted of the daily inter-state traffic, air traffic data, and daily
new COVID-19 confirmed cases. The data is used to train a regression model and
voting was used to show the highest correlation between travels made between
cities and new cases of COVID-19. Although the available data was very coarse
and there was no detail of inner-city commute, an accuracy of 81% was achieved
showing a positive correlation between the number of inter-state travels and
the new cases of COVID-19. Consequently, the result suggests that one of the
best ways to avoid the spread of the virus is limiting or eliminating traveling
around.

    

### [[2010.03002] OneFlow: One-class flow for anomaly detection based on a minimal volume region](http://arxiv.org/abs/2010.03002)


  We propose OneFlow - a flow-based one-class classifier for anomaly (outlier)
detection that finds a minimal volume bounding region. Contrary to
density-based methods, OneFlow is constructed in such a way that its result
typically does not depend on the structure of outliers. This is caused by the
fact that during training the gradient of the cost function is propagated only
over the points located near to the decision boundary (behavior similar to the
support vectors in SVM). The combination of flow models and a Bernstein
quantile estimator allows OneFlow to find a parametric form of bounding region,
which can be useful in various applications including describing shapes from 3D
point clouds. Experiments show that the proposed model outperforms related
methods on real-world anomaly detection problems.

    

### [[2010.08584] Improving significance of binary black hole mergers in Advanced LIGO data using deep learning : Confirmation of GW151216](http://arxiv.org/abs/2010.08584)


  We present a novel Machine Learning (ML) based strategy to search for binary
black hole (BBH) mergers in data from ground-based gravitational wave (GW)
observatories. This is the first ML-based search that not only recovers all the
compact binary coalescences (CBCs) in the first GW transients catalog (GWTC-1),
but also makes a clean detection of GW151216 by only adding a new coincident
ranking statistic (MLStat) to a standard analysis that was used for GWTC-1. In
CBC searches, reducing contamination by terrestrial and instrumental
transients, which create a loud noise background by triggering numerous false
alarms, is crucial to improving the sensitivity for detecting true events. The
sheer volume of data and a large number of expected detections also prompts the
use of ML techniques. We perform transfer learning to train "InceptionV3", a
pre-trained deep neural network, along with curriculum learning to distinguish
GW signals from noisy events by analysing their continuous wavelet transform
(CWT) maps. MLStat incorporates information from this ML classifier into the
coincident search likelihood used by the standard PyCBC search. This leads to
at least an order of magnitude improvement in the inverse false-alarm-rate
(IFAR) for the previously "low significance" events GW151012, GW170729 and
GW151216. We also perform the parameter estimation of GW151216 using
SEOBNRv4HM_ROM. We carry out an injection study to show that MLStat brings
substantial improvement to the detection sensitivity of Advanced LIGO for all
compact binary coalescences. The average improvement in the sensitive volume is
~10% for low chirp masses (0.8-5 Msun), and ~30% for higher masses (5-50 Msun).
This work demonstrates the immense potential and readiness of MLStat for
finding new sources in current data and the possibility of its adaptation in
similar searches.

    

### [[2012.04885] Annotation-efficient deep learning for automatic medical image segmentation](http://arxiv.org/abs/2012.04885)


  Automatic medical image segmentation plays a critical role in scientific
research and medical care. Existing high-performance deep learning methods
typically rely on large training datasets with high-quality manual annotations,
which are difficult to obtain in many clinical applications. Here, we introduce
Annotation-effIcient Deep lEarning (AIDE), an open-source framework to handle
imperfect training datasets. Methodological analyses and empirical evaluations
are conducted, and we demonstrate that AIDE surpasses conventional
fully-supervised models by presenting better performance on open datasets
possessing scarce or noisy annotations. We further test AIDE in a real-life
case study for breast tumor segmentation. Three datasets containing 11,852
breast images from three medical centers are employed, and AIDE, utilizing 10%
training annotations, consistently produces segmentation maps comparable to
those generated by fully-supervised counterparts or provided by independent
radiologists. The 10-fold enhanced efficiency in utilizing expert labels has
the potential to promote a wide range of biomedical applications.

    

### [[2101.00575] Improved Convergence Guarantees for Learning Gaussian Mixture Models by EM and Gradient EM](http://arxiv.org/abs/2101.00575)


  We consider the problem of estimating the parameters a Gaussian Mixture Model
with K components of known weights, all with an identity covariance matrix. We
make two contributions. First, at the population level, we present a sharper
analysis of the local convergence of EM and gradient EM, compared to previous
works. Assuming a separation of $\Omega(\sqrt{\log K})$, we prove convergence
of both methods to the global optima from an initialization region larger than
those of previous works. Specifically, the initial guess of each component can
be as far as (almost) half its distance to the nearest Gaussian. This is
essentially the largest possible contraction region. Our second contribution
are improved sample size requirements for accurate estimation by EM and
gradient EM. In previous works, the required number of samples had a quadratic
dependence on the maximal separation between the K components, and the
resulting error estimate increased linearly with this maximal separation. In
this manuscript we show that both quantities depend only logarithmically on the
maximal separation.

    

### [[2102.02805] How do Quadratic Regularizers Prevent Catastrophic Forgetting: The Role of Interpolation](http://arxiv.org/abs/2102.02805)


  Catastrophic forgetting undermines the effectiveness of deep neural networks
(DNNs) in scenarios such as continual learning and lifelong learning. While
several methods have been proposed to tackle this problem, there is limited
work explaining why these methods work well. This paper has the goal of better
explaining a popularly used technique for avoiding catastrophic forgetting:
quadratic regularization. We show that quadratic regularizers prevent
forgetting of past tasks by interpolating current and previous values of model
parameters at every training iteration. Over multiple training iterations, this
interpolation operation reduces the learning rates of more important model
parameters, thereby minimizing their movement. Our analysis also reveals two
drawbacks of quadratic regularization: (a) dependence of parameter
interpolation on training hyperparameters, which often leads to training
instability and (b) assignment of lower importance to deeper layers, which are
generally the place forgetting occurs in DNNs. Via a simple modification to the
order of operations, we show these drawbacks can be easily avoided, resulting
in 6.2% higher average accuracy at 4.5% lower average forgetting. Code
available at \url{this https URL}

    

### [[2102.04327] An Unbiased Estimator of the Full-sky CMB Angular Power Spectrum at Large Scales using Neural Networks](http://arxiv.org/abs/2102.04327)


  Accurate estimation of the Cosmic Microwave Background (CMB) angular power
spectrum is enticing due to the prospect for precision cosmology it presents.
Galactic foreground emissions, however, contaminate the CMB signal and need to
be subtracted reliably in order to lessen systematic errors on the CMB
temperature estimates. Typically bright foregrounds in a region lead to further
uncertainty in temperature estimates in the area even after some foreground
removal technique is performed and hence determining the underlying full-sky
angular power spectrum poses a challenge. We explore the feasibility of
utilizing artificial neural networks to predict the angular power spectrum of
the full sky CMB temperature maps from the observed angular power spectrum of
the partial sky in which CMB temperatures in some bright foreground regions are
masked. We present our analysis at large angular scales with two different
masks. We produce unbiased predictions of the full-sky angular power spectrum
and recover the underlying theoretical power spectrum using neural networks.
Our predictions are also uncorrelated to a large extent. We further show that
the multipole-space covariances of the predictions of full-sky spectra made by
the ANNs are much smaller than those of the estimates obtained using the
pseudo-$C_\ell$ method.

    

### [[2103.06406] Distributed Principal Subspace Analysis for Partitioned Big Data: Algorithms, Analysis, and Implementation](http://arxiv.org/abs/2103.06406)


  Principal Subspace Analysis (PSA) is one of the most popular approaches for
dimensionality reduction in signal processing and machine learning. But
centralized PSA solutions are fast becoming irrelevant in the modern era of big
data, in which the number of samples and/or the dimensionality of samples often
exceed the storage and/or computational capabilities of individual machines.
This has led to study of distributed PSA solutions, in which the data are
partitioned across multiple machines and an estimate of the principal subspace
is obtained through collaboration among the machines. It is in this vein that
this paper revisits the problem of distributed PSA under the general framework
of an arbitrarily connected network of machines that lacks a central server.
The main contributions of the paper in this regard are threefold. First, two
algorithms are proposed in the paper that can be used for distributed PSA in
the case of data that are partitioned across either samples or (raw) features.
Second, in the case of sample-wise partitioned data, the proposed algorithm and
a variant of it are analyzed, and their convergence to the true subspace at
linear rates is established. Third, extensive experiments on both synthetic and
real-world data are carried out to validate the usefulness of the proposed
algorithms. In particular, in the case of sample-wise partitioned data, an
MPI-based distributed implementation is carried out to study the interplay
between network topology and communications cost as well as to study of effect
of straggler machines on the proposed algorithms.

    

### [[2103.06995] Performance of a Geometric Deep Learning Pipeline for HL-LHC Particle Tracking](http://arxiv.org/abs/2103.06995)


  The Exa.TrkX project has applied geometric learning concepts such as metric
learning and graph neural networks to HEP particle tracking. Exa.TrkX's
tracking pipeline groups detector measurements to form track candidates and
filters them. The pipeline, originally developed using the TrackML dataset (a
simulation of an LHC-inspired tracking detector), has been demonstrated on
other detectors, including DUNE Liquid Argon TPC and CMS High-Granularity
Calorimeter. This paper documents new developments needed to study the physics
and computing performance of the Exa.TrkX pipeline on the full TrackML dataset,
a first step towards validating the pipeline using ATLAS and CMS data. The
pipeline achieves tracking efficiency and purity similar to production tracking
algorithms. Crucially for future HEP applications, the pipeline benefits
significantly from GPU acceleration, and its computational requirements scale
close to linearly with the number of particles in the event.

    

### [[2103.08116] Improving Generalization of Transfer Learning Across Domains Using Spatio-Temporal Features in Autonomous Driving](http://arxiv.org/abs/2103.08116)


  Practical learning-based autonomous driving models must be capable of
generalizing learned behaviors from simulated to real domains, and from
training data to unseen domains with unusual image properties. In this paper,
we investigate transfer learning methods that achieve robustness to domain
shifts by taking advantage of the invariance of spatio-temporal features across
domains. In this paper, we propose a transfer learning method to improve
generalization across domains via transfer of spatio-temporal features and
salient data augmentation. Our model uses a CNN-LSTM network with Inception
modules for image feature extraction. Our method runs in two phases: Phase 1
involves training on source domain data, while Phase 2 performs training on
target domain data that has been supplemented by feature maps generated using
the Phase 1 model. Our model significantly improves performance in unseen test
cases for both simulation-to-simulation transfer as well as simulation-to-real
transfer by up to +37.3\% in test accuracy and up to +40.8\% in steering angle
prediction, compared to other SOTA methods across multiple datasets.

    

### [[2105.00813] Transformers: "The End of History" for NLP?](http://arxiv.org/abs/2105.00813)


  Recent advances in neural architectures, such as the Transformer, coupled
with the emergence of large-scale pre-trained models such as BERT, have
revolutionized the field of Natural Language Processing (NLP), pushing the
state of the art for a number of NLP tasks. A rich family of variations of
these models has been proposed, such as RoBERTa, ALBERT, and XLNet, but
fundamentally, they all remain limited in their ability to model certain kinds
of information, and they cannot cope with certain information sources, which
was easy for pre-existing models. Thus, here we aim to shed light on some
important theoretical limitations of pre-trained BERT-style models that are
inherent in the general Transformer architecture. First, we demonstrate in
practice on two general types of tasks -- segmentation and segment labeling --
and on four datasets that these limitations are indeed harmful and that
addressing them, even in some very simple and naive ways, can yield sizable
improvements over vanilla RoBERTa and XLNet models. Then, we offer a more
general discussion on desiderata for future additions to the Transformer
architecture that would increase its expressiveness, which we hope could help
in the design of the next generation of deep NLP architectures.

    

### [[2105.10816] Novel Deep Learning Architecture for Heart Disease Prediction using Convolutional Neural Network](http://arxiv.org/abs/2105.10816)


  Healthcare is one of the most important aspects of human life. Heart disease
is known to be one of the deadliest diseases which is hampering the lives of
many people around the world. Heart disease must be detected early so the loss
of lives can be prevented. The availability of large-scale data for medical
diagnosis has helped developed complex machine learning and deep learning-based
models for automated early diagnosis of heart diseases. The classical
approaches have been limited in terms of not generalizing well to new data
which have not been seen in the training set. This is indicated by a large gap
in training and test accuracies. This paper proposes a novel deep learning
architecture using a 1D convolutional neural network for classification between
healthy and non-healthy persons to overcome the limitations of classical
approaches. Various clinical parameters are used for assessing the risk profile
in the patients which helps in early diagnosis. Various techniques are used to
avoid overfitting in the proposed network. The proposed network achieves over
97% training accuracy and 96% test accuracy on the dataset. The accuracy of the
model is compared in detail with other classification algorithms using various
performance parameters which proves the effectiveness of the proposed
architecture.

    

### [[2105.13331] Quantization and Deployment of Deep Neural Networks on Microcontrollers](http://arxiv.org/abs/2105.13331)


  Embedding Artificial Intelligence onto low-power devices is a challenging
task that has been partly overcome with recent advances in machine learning and
hardware design. Presently, deep neural networks can be deployed on embedded
targets to perform different tasks such as speech recognition,object detection
or Human Activity Recognition. However, there is still room for optimization of
deep neural networks onto embedded devices. These optimizations mainly address
power consumption,memory and real-time constraints, but also an easier
deployment at the edge. Moreover, there is still a need for a better
understanding of what can be achieved for different use cases. This work
focuses on quantization and deployment of deep neural networks onto low-power
32-bit microcontrollers. The quantization methods, relevant in the context of
an embedded execution onto a microcontroller, are first outlined. Then, a new
framework for end-to-end deep neural networks training, quantization and
deployment is presented. This framework, called MicroAI, is designed as an
alternative to existing inference engines (TensorFlow Lite for Microcontrollers
and STM32CubeAI). Our framework can indeed be easily adjusted and/or extended
for specific use cases. Execution using single precision 32-bit floating-point
as well as fixed-point on 8- and 16-bit integers are supported. The proposed
quantization method is evaluated with three different datasets (UCI-HAR, Spoken
MNIST and GTSRB). Finally, a comparison study between MicroAI and both existing
embedded inference engines is provided in terms of memory and power efficiency.
On-device evaluation is done using ARM Cortex-M4F-based microcontrollers (Ambiq
Apollo3 and STM32L452RE).

    

### [[2105.14257] Diffusion-Based Representation Learning](http://arxiv.org/abs/2105.14257)


  Score-based methods represented as stochastic differential equations on a
continuous time domain have recently proven successful as a non-adversarial
generative model. Training such models relies on denoising score matching,
which can be seen as multi-scale denoising autoencoders. Here, we augment the
denoising score-matching framework to enable representation learning without
any supervised signal. GANs and VAEs learn representations by directly
transforming latent codes to data samples. In contrast, the introduced
diffusion based representation learning relies on a new formulation of the
denoising score-matching objective and thus encodes information needed for
denoising. We illustrate how this difference allows for manual control of the
level of details encoded in the representation. Using the same approach, we
propose to learn an infinite-dimensional latent code which achieves
improvements of state-of-the-art models on semi-supervised image
classification. As a side contribution, we show how adversarial training in
score-based models can improve sample quality and improve sampling speed using
a new approximation of the prior at smaller noise scales.

    

### [[2106.01048] Expected Scalarised Returns Dominance: A New Solution Concept for Multi-Objective Decision Making](http://arxiv.org/abs/2106.01048)


  In many real-world scenarios, the utility of a user is derived from the
single execution of a policy. In this case, to apply multi-objective
reinforcement learning, the expected utility of the returns must be optimised.
Various scenarios exist where a user's preferences over objectives (also known
as the utility function) are unknown or difficult to specify. In such
scenarios, a set of optimal policies must be learned. However, settings where
the expected utility must be maximised have been largely overlooked by the
multi-objective reinforcement learning community and, as a consequence, a set
of optimal solutions has yet to be defined. In this paper we address this
challenge by proposing first-order stochastic dominance as a criterion to build
solution sets to maximise expected utility. We also propose a new dominance
criterion, known as expected scalarised returns (ESR) dominance, that extends
first-order stochastic dominance to allow a set of optimal policies to be
learned in practice. We then define a new solution concept called the ESR set,
which is a set of policies that are ESR dominant. Finally, we define a new
multi-objective distributional tabular reinforcement learning (MOT-DRL)
algorithm to learn the ESR set in a multi-objective multi-armed bandit setting.

    

### [[2106.13323] Domain-guided Machine Learning for Remotely Sensed In-Season Crop Growth Estimation](http://arxiv.org/abs/2106.13323)


  Advanced machine learning techniques have been used in remote sensing (RS)
applications such as crop mapping and yield prediction, but remain
under-utilized for tracking crop progress. In this study, we demonstrate the
use of agronomic knowledge of crop growth drivers in a Long Short-Term
Memory-based, domain-guided neural network (DgNN) for in-season crop progress
estimation. The DgNN uses a branched structure and attention to separate
independent crop growth drivers and capture their varying importance throughout
the growing season. The DgNN is implemented for corn, using RS data in Iowa for
the period 2003-2019, with USDA crop progress reports used as ground truth.
State-wide DgNN performance shows significant improvement over sequential and
dense-only NN structures, and a widely-used Hidden Markov Model method. The
DgNN had a 4.0% higher Nash-Sutfliffe efficiency over all growth stages and 39%
more weeks with highest cosine similarity than the next best NN during test
years. The DgNN and Sequential NN were more robust during periods of abnormal
crop progress, though estimating the Silking-Grainfill transition was difficult
for all methods. Finally, Uniform Manifold Approximation and Projection
visualizations of layer activations showed how LSTM-based NNs separate crop
growth time-series differently from a dense-only structure. Results from this
study exhibit both the viability of NNs in crop growth stage estimation (CGSE)
and the benefits of using domain knowledge. The DgNN methodology presented here
can be extended to provide near-real time CGSE of other crops.

    

### [[2109.09363] Performance and accuracy assessments of an incompressible fluid solver coupled with a deep Convolutional Neural Network](http://arxiv.org/abs/2109.09363)


  The resolution of the Poisson equation is usually one of the most
computationally intensive steps for incompressible fluid solvers. Lately, Deep
Learning, and especially Convolutional Neural Networks (CNN), has been
introduced to solve this equation, leading to significant inference time
reduction at the cost of a lack of guarantee on the accuracy of the solution.
This drawback might lead to inaccuracies and potentially unstable simulations.
It also makes impossible a fair assessment of the CNN speedup, for instance,
when changing the network architecture, since evaluated at different error
levels. To circumvent this issue, a hybrid strategy is developed, which couples
a CNN with a traditional iterative solver to ensure a user-defined accuracy
level. The CNN hybrid method is tested on two flow cases, consisting of a
variable-density plume with and without obstacles, demostrating remarkable
generalization capabilities, ensuring both the accuracy and stability of the
simulations. The error distribution of the predictions using several network
architectures is further investigated. Results show that the threshold of the
hybrid strategy defined as the mean divergence of the velocity field is
ensuring a consistent physical behavior of the CNN-based hybrid computational
strategy. This strategy allows a systematic evaluation of the CNN performance
at the same accuracy level for various network architectures. In particular,
the importance of incorporating multiple scales in the network architecture is
demonstrated, since improving both the accuracy and the inference performance
compared with feedforward CNN architectures, as these networks can provide
solutions 1 10-25 faster than traditional iterative solvers.

    

### [[2109.11069] DAS: Dynamic Adaptive Scheduling for Energy-Efficient Heterogeneous SoCs](http://arxiv.org/abs/2109.11069)


  Domain-specific systems-on-chip (DSSoCs) aim at bridging the gap between
application-specific integrated circuits (ASICs) and general-purpose
processors. Traditional operating system (OS) schedulers can undermine the
potential of DSSoCs since their execution times can be orders of magnitude
larger than the execution time of the task itself. To address this problem, we
propose a dynamic adaptive scheduling (DAS) framework that combines the
benefits of a fast (low-overhead) scheduler and a slow (sophisticated,
high-performance but high-overhead) scheduler. Experiments with five real-world
streaming applications show that DAS consistently outperforms both the fast and
slow schedulers. For 40 different workloads, DAS achieves on average 1.29x
speedup and 45% lower EDP compared to the sophisticated scheduler at low data
rates and 1.28x speedup and 37% lower EDP than the fast scheduler when the
workload complexity increases.

    

### [[2109.11074] GPU4S: Embedded GPUs in Space -- Latest Project Updates](http://arxiv.org/abs/2109.11074)


  Following the trend of other safety-critical industries like automotive and
avionics, the space domain is witnessing an increase in the on-board computing
performance demands. This raise in performance needs comes from both control
and payload parts of the spacecraft and calls for advanced electronics systems
able to provide high computational power under the constraints of the harsh
space environment. On the non-technical side, for strategic reasons it is
mandatory to get European independence on the used computing technology. In
this project, we study the applicability of embedded GPUs in space, which have
shown a dramatic improvement of their performance per-watt ratio coming from
their proliferation in consumer markets based on competitive European
technology. To that end, we perform an analysis of the existing space
application domains to identify which software domains can benefit from their
use. Moreover, we survey the embedded GPU domain in order to assess whether
embedded GPUs can provide the required computational power and identify the
challenges which need to be addressed for their adoption in space. In this
paper, we describe the steps followed in the project, as well as a summary of
results obtained from our analyses so far in the project.

    

### [[2109.11081] Sextans: A Streaming Accelerator for General-Purpose Sparse-Matrix Dense-Matrix Multiplication](http://arxiv.org/abs/2109.11081)


  Sparse-Matrix Dense-Matrix multiplication (SpMM) is the key operator for a
wide range of applications, including scientific computing, graph processing,
and deep learning. Architecting accelerators for SpMM is faced with three
challenges - (1) the random memory accessing and unbalanced load in processing
because of random distribution of elements in sparse matrices, (2) inefficient
data handling of the large matrices which can not be fit on-chip, and (3)
anon-general-purpose accelerator design where one accelerator can only process
a fixed-size problem. In this paper, we present Sextans, an accelerator for
general-purpose SpMM processing. Sextans accelerator features (1) fast random
access using on-chip memory, (2) streaming access to off-chip large matrices,
(3) PE-aware non-zero scheduling for balanced workload with an II=1 pipeline,
and (4) hardware flexibility to enable prototyping the hardware once to support
SpMMs of different size as a general-purpose accelerator. We leverage high
bandwidth memory (HBM) for the efficient accessing of both sparse and dense
matrices. In the evaluation, we present an FPGA prototype Sextans which is
executable on a Xilinx U280 HBM FPGA board and a projected prototype Sextans-P
with higher bandwidth comparable to V100 and more frequency optimization. We
conduct a comprehensive evaluation on 1,400 SpMMs on a wide range of sparse
matrices including 50 matrices from SNAP and 150 from SuiteSparse.
WecompareSextanswith NVIDIA K80 and V100 GPUs.Sextansachieves a 2.50x geomean
speedup over K80 GPU andSextans-Pachieves a 1.14x geomean speedup over V100 GPU
(4.94x over K80).

    

### [[2104.12760] Capstan: A Vector RDA for Sparsity](http://arxiv.org/abs/2104.12760)


  This paper proposes Capstan: a scalable, parallel-patterns-based,
reconfigurable dataflow accelerator (RDA) for sparse and dense tensor
applications. Instead of designing for one application, we start with common
sparse data formats, each of which supports multiple applications. Using a
declarative programming model, Capstan supports application-independent sparse
iteration and memory primitives that can be mapped to vectorized,
high-performance hardware. We optimize random-access sparse memories with
configurable out-of-order execution to increase SRAM random-access throughput
from 32% to 80%.
For a variety of sparse applications, Capstan with DDR4 memory is 18x faster
than a multi-core CPU baseline, while Capstan with HBM2 memory is 16x faster
than an Nvidia V100 GPU. For sparse applications that can be mapped to
Plasticine, a recent dense RDA, Capstan is 7.6x to 365x faster and only 16%
larger.

    

### [[2109.10951] Naming Schema for a Human Brain-Scale Neural Network](http://arxiv.org/abs/2109.10951)


  Deep neural networks have become increasingly large and sparse, allowing for
the storage of large-scale neural networks with decreased costs of storage and
computation. Storage of a neural network with as many connections as the human
brain is possible with current versions of the high-performance Apache Accumulo
database and the Distributed Dimensional Data Model (D4M) software. Neural
networks of such large scale may be of particular interest to scientists within
the human brain Connectome community. To aid in research and understanding of
artificial neural networks that parallel existing neural networks like the
brain, a naming schema can be developed to label groups of neurons in the
artificial network that parallel those in the brain. Groups of artificial
neurons are able to be specifically labeled in small regions for future study.

    

### [[2109.11021] Intel Optane DCPMM and Serverless Computing](http://arxiv.org/abs/2109.11021)


  This report describes 1) how we use Intel's Optane DCPMM in the memory Mode.
We investigate the the scalability of applications on a single Optane machine,
using Subgraph counting as memory-intensive graph problem. We test with various
input graph and subtemplate sizes to determine its performance for different
memory and CPU loads, as well as a comparison of performance on a single node
Optane with a distributed set of nodes in a cluster using MPI. 2) We
investigate the end-to-end execution delays in serverless computing and study
concurrent function executions with cold starts. In future work, we will show
that persistent memory machines may significantly improve concurrent function
invocations in serverless computing including Amazon Lambda, Microsoft Azure
Functions, Google Cloud Functions and IBM Cloud Functions (Apache OpenWhisk).

    

### [[2109.11037] A Computational Approach for Checking Compliance with European View and Sunlight Exposure Criteria](http://arxiv.org/abs/2109.11037)


  The paper presents open-source computational workflows for assessing the
"Exposure to sunlight" and "View out" criteria as defined in the European
standard EN 17037 "Daylight in Buildings", issued by the European Committee for
Standardization. In addition to these factors, the standard document also
addresses daylight provision and protection from glare, both of which fall out
of the scope of this paper. The purpose of the standard is stated as
'encouraging building designers to assess and ensure successfully daylit
spaces'. The standard document proposes verification methods for performing
such assessments, albeit without recommending a simulation procedure for
computing the aforementioned criteria. The workflows proposed in this paper are
arguably the first attempt to standardize these assessment methods using
de-facto open-source standard technologies currently used in practice. The
approach of this work is twofold: establish that the compliance check can be
systematically performed on a 3D model by a novel simulation tool developed by
the authors; and highlighting the additional assumptions that need to be
implemented to build a robust and unambiguous tool within existing open-source
frameworks.

    

### [[2109.11390] Fault Localization in Cloud using Centrality Measures](http://arxiv.org/abs/2109.11390)


  Fault localization is an imperative method in fault tolerance in a
distributed environment that designs a blueprint for continuing the ongoing
process even when one or many modules are non-functional. Visualizing a
distributed environment as a graph, whose nodes represent faults (fault graph),
allows us to introduce probabilistic weights to both edges and nodes that cause
the faults. With multiple modules like databases, run-time cloud, etc. making
up a distributed environment and extensively, a cloud environment, we aim to
address the problem of optimally and accurately performing fault localization
in a distributed environment by modifying the Graph optimization approach to
localization and centrality, specific to fault graphs.

    

### [[2008.02352] Efficient Migrations Between Storage Tiers with PrismDB](http://arxiv.org/abs/2008.02352)


  In recent years, emerging hardware storage technologies have focused on
divergent goals: better performance or lower cost-per-bit of storage.
Correspondingly, data systems that employ these new technologies are typically
optimized either to be fast (but expensive) or cheap (but slow). We take a
different approach: by designing a storage engine to natively utilize two tiers
of fast and low-cost storage technologies, we can achieve a Pareto-efficient
balance between performance and cost-per-bit.
This paper presents the design and implementation of PrismDB, a novel
key-value store that exploits two extreme ends of the spectrum of modern NVMe
storage technologies (3D XPoint and QLC NAND) simultaneously. Unlike prior work
that has retrofitted data structures designed for a single tier to multi-tier
storage, PrismDB's data structures and migration mechanism are tailored for
different storage layers. The key design contributions of PrismDB is a novel
hybrid data structure for tiered storage, and an adaptive and lightweight
migration mechanism that is able to efficiently demote cold objects from the
fast to the slow storage tier, and promote hot objects to the fast tier.
Compared to the standard use of RocksDB on flash in datacenters today,
PrismDB's average throughput on tiered storage is 3.3$\times$ faster and its
read tail latency is 2$\times$ better, using equivalently-priced hardware.

    

### [[2105.06075] The Availability-Accountability Dilemma and its Resolution via Accountability Gadgets](http://arxiv.org/abs/2105.06075)


  For applications of Byzantine fault tolerant (BFT) consensus protocols where
the participants are economic agents, recent works highlighted the importance
of accountability: the ability to identify participants who provably violate
the protocol. At the same time, being able to reach consensus under dynamic
levels of participation is desirable for censorship resistance. We identify an
availability-accountability dilemma: in an environment with dynamic
participation, no protocol can simultaneously be accountably-safe and live. We
provide a resolution to this dilemma by constructing an optimally-resilient
accountability gadget to checkpoint a longest chain protocol, such that the
full ledger is live under dynamic participation and the checkpointed prefix
ledger is accountable. Our accountability gadget construction is black-box and
can use any BFT protocol which is accountable under static participation. Using
HotStuff as the black box, we implemented our construction as a protocol for
the Ethereum 2.0 beacon chain, and our Internet-scale experiments with more
than 4000 nodes show that the protocol can achieve the required scalability and
has better latency than the current solution Gasper, while having the advantage
of being provably secure.

    

### [[2106.12322] Distributed coloring and the local structure of unit-disk graphs](http://arxiv.org/abs/2106.12322)


  Coloring unit-disk graphs efficiently is an important problem in the global
and distributed setting, with applications in radio channel assignment problems
when the communication relies on omni-directional antennas of the same power.
In this context it is important to bound not only the complexity of the
coloring algorithms, but also the number of colors used. In this paper, we
consider two natural distributed settings. In the location-aware setting (when
nodes know their coordinates in the plane), we give a constant time distributed
algorithm coloring any unit-disk graph $G$ with at most
$(3+\epsilon)\omega(G)+6$ colors, for any constant $\epsilon>0$, where
$\omega(G)$ is the clique number of $G$. This improves upon a classical
3-approximation algorithm for this problem, for all unit-disk graphs whose
chromatic number significantly exceeds their clique number. When nodes do not
know their coordinates in the plane, we give a distributed algorithm in the
LOCAL model that colors every unit-disk graph $G$ with at most $5.68\omega(G)$
colors in (see \cref{sec:comput}) rounds. Moreover, when $\omega(G)=O(1)$, the
algorithm runs in $O(\log^* n)$ rounds. This algorithm is based on a study of
the local structure of unit-disk graphs, which is of independent interest. We
conjecture that every unit-disk graph $G$ has average degree at most
$4\omega(G)$, which would imply the existence of a $O(\log n)$ round algorithm
coloring any unit-disk graph $G$ with (approximatively) $4\omega(G)$ colors.

    

### [[2109.11064] Actionable Conversational Quality Indicators for Improving Task-Oriented Dialog Systems](http://arxiv.org/abs/2109.11064)


  Automatic dialog systems have become a mainstream part of online customer
service. Many such systems are built, maintained, and improved by customer
service specialists, rather than dialog systems engineers and computer
programmers. As conversations between people and machines become commonplace,
it is critical to understand what is working, what is not, and what actions can
be taken to reduce the frequency of inappropriate system responses. These
analyses and recommendations need to be presented in terms that directly
reflect the user experience rather than the internal dialog processing.
This paper introduces and explains the use of Actionable Conversational
Quality Indicators (ACQIs), which are used both to recognize parts of dialogs
that can be improved, and to recommend how to improve them. This combines
benefits of previous approaches, some of which have focused on producing dialog
quality scoring while others have sought to categorize the types of errors the
dialog system is making.
We demonstrate the effectiveness of using ACQIs on LivePerson internal dialog
systems used in commercial customer service applications, and on the publicly
available CMU LEGOv2 conversational dataset (Raux et al. 2005). We report on
the annotation and analysis of conversational datasets showing which ACQIs are
important to fix in various situations.
The annotated datasets are then used to build a predictive model which uses a
turn-based vector embedding of the message texts and achieves an 79% weighted
average f1-measure at the task of finding the correct ACQI for a given
conversation. We predict that if such a model worked perfectly, the range of
potential improvement actions a bot-builder must consider at each turn could be
reduced by an average of 81%.

    

### [[2109.11216] Union and Intersection of all Justifications](http://arxiv.org/abs/2109.11216)


  We present new algorithm for computing the union and intersection of all
justifications for a given ontological consequence without first computing the
set of all justifications. Through an empirical evaluation, we show that our
approach works well in practice for expressive DLs. In particular, the union of
all justifications can be computed much faster than with existing
justification-enumeration approaches. We further discuss how to use these
results to repair ontologies efficiently.

    

### [[2109.11223] Individual and Collective Autonomous Development](http://arxiv.org/abs/2109.11223)


  The increasing complexity and unpredictability of many ICT scenarios let us
envision that future systems will have to dynamically learn how to act and
adapt to face evolving situations with little or no a priori knowledge, both at
the level of individual components and at the collective level. In other words,
such systems should become able to autonomously develop models of themselves
and of their environment. Autonomous development includes: learning models of
own capabilities; learning how to act purposefully towards the achievement of
specific goals; and learning how to act collectively, i.e., accounting for the
presence of others. In this paper, we introduce the vision of autonomous
development in ICT systems, by framing its key concepts and by illustrating
suitable application domains. Then, we overview the many research areas that
are contributing or can potentially contribute to the realization of the
vision, and identify some key research challenges.

    

### [[2109.11251] Trust Region Policy Optimisation in Multi-Agent Reinforcement Learning](http://arxiv.org/abs/2109.11251)


  Trust region methods rigorously enabled reinforcement learning (RL) agents to
learn monotonically improving policies, leading to superior performance on a
variety of tasks. Unfortunately, when it comes to multi-agent reinforcement
learning (MARL), the property of monotonic improvement may not simply apply;
this is because agents, even in cooperative games, could have conflicting
directions of policy updates. As a result, achieving a guaranteed improvement
on the joint policy where each agent acts individually remains an open
challenge. In this paper, we extend the theory of trust region learning to
MARL. Central to our findings are the multi-agent advantage decomposition lemma
and the sequential policy update scheme. Based on these, we develop
Heterogeneous-Agent Trust Region Policy Optimisation (HATPRO) and
Heterogeneous-Agent Proximal Policy Optimisation (HAPPO) algorithms. Unlike
many existing MARL algorithms, HATRPO/HAPPO do not need agents to share
parameters, nor do they need any restrictive assumptions on decomposibility of
the joint value function. Most importantly, we justify in theory the monotonic
improvement property of HATRPO/HAPPO. We evaluate the proposed methods on a
series of Multi-Agent MuJoCo and StarCraftII tasks. Results show that HATRPO
and HAPPO significantly outperform strong baselines such as IPPO, MAPPO and
MADDPG on all tested tasks, therefore establishing a new state of the art.

    

### [[2109.11256] Can Question Generation Debias Question Answering Models? A Case Study on Question-Context Lexical Overlap](http://arxiv.org/abs/2109.11256)


  Question answering (QA) models for reading comprehension have been
demonstrated to exploit unintended dataset biases such as question-context
lexical overlap. This hinders QA models from generalizing to under-represented
samples such as questions with low lexical overlap. Question generation (QG), a
method for augmenting QA datasets, can be a solution for such performance
degradation if QG can properly debias QA datasets. However, we discover that
recent neural QG models are biased towards generating questions with high
lexical overlap, which can amplify the dataset bias. Moreover, our analysis
reveals that data augmentation with these QG models frequently impairs the
performance on questions with low lexical overlap, while improving that on
questions with high lexical overlap. To address this problem, we use a synonym
replacement-based approach to augment questions with low lexical overlap. We
demonstrate that the proposed data augmentation approach is simple yet
effective to mitigate the degradation problem with only 70k synthetic examples.
Our data is publicly available at
this https URL.

    

### [[2109.11343] Towards Explainable Scientific Venue Recommendations](http://arxiv.org/abs/2109.11343)


  Selecting the best scientific venue (i.e., conference/journal) for the
submission of a research article constitutes a multifaceted challenge.
Important aspects to consider are the suitability of research topics, a venue's
prestige, and the probability of acceptance. The selection problem is
exacerbated through the continuous emergence of additional venues. Previously
proposed approaches for supporting authors in this process rely on complex
recommender systems, e.g., based on Word2Vec or TextCNN. These, however, often
elude an explanation for their recommendations. In this work, we propose an
unsophisticated method that advances the state-of-the-art in two aspects:
First, we enhance the interpretability of recommendations through non-negative
matrix factorization based topic models; Second, we surprisingly can obtain
competitive recommendation performance while using simpler learning methods.

    

### [[2109.11344] Causal Inference in Network Economics](http://arxiv.org/abs/2109.11344)


  Network economics is the study of a rich class of equilibrium problems that
occur in the real world, from traffic management to supply chains and two-sided
online marketplaces. In this paper we explore causal inference in network
economics, building on the mathematical framework of variational inequalities,
which is a generalization of classical optimization. Our framework can be
viewed as a synthesis of the well-known variational inequality formalism with
the broad principles of causal inference

    

### [[2109.11421] An Algorithm for Generating Gap-Fill Multiple Choice Questions of an Expert System](http://arxiv.org/abs/2109.11421)


  This research is aimed to propose an artificial intelligence algorithm
comprising an ontology-based design, text mining, and natural language
processing for automatically generating gap-fill multiple choice questions
(MCQs). The simulation of this research demonstrated an application of the
algorithm in generating gap-fill MCQs about software testing. The simulation
results revealed that by using 103 online documents as inputs, the algorithm
could automatically produce more than 16 thousand valid gap-fill MCQs covering
a variety of topics in the software testing domain. Finally, in the discussion
section of this paper we suggest how the proposed algorithm should be applied
to produce gap-fill MCQs being collected in a question pool used by a knowledge
expert system.

    

### [[2109.11431] Deep Learning for Ultrasound Beamforming](http://arxiv.org/abs/2109.11431)


  Diagnostic imaging plays a critical role in healthcare, serving as a
fundamental asset for timely diagnosis, disease staging and management as well
as for treatment choice, planning, guidance, and follow-up. Among the
diagnostic imaging options, ultrasound imaging is uniquely positioned, being a
highly cost-effective modality that offers the clinician an unmatched and
invaluable level of interaction, enabled by its real-time nature. Ultrasound
probes are becoming increasingly compact and portable, with the market demand
for low-cost pocket-sized and (in-body) miniaturized devices expanding. At the
same time, there is a strong trend towards 3D imaging and the use of
high-frame-rate imaging schemes; both accompanied by dramatically increasing
data rates that pose a heavy burden on the probe-system communication and
subsequent image reconstruction algorithms.
With the demand for high-quality image reconstruction and signal extraction
from less (e.g unfocused or parallel) transmissions that facilitate fast
imaging, and a push towards compact probes, modern ultrasound imaging leans
heavily on innovations in powerful digital receive channel processing.
Beamforming, the process of mapping received ultrasound echoes to the spatial
image domain, naturally lies at the heart of the ultrasound image formation
chain. In this chapter on Deep Learning for Ultrasound Beamforming, we discuss
why and when deep learning methods can play a compelling role in the digital
beamforming pipeline, and then show how these data-driven systems can be
leveraged for improved ultrasound image reconstruction.

    

### [[2109.11439] DeepRare: Generic Unsupervised Visual Attention Models](http://arxiv.org/abs/2109.11439)


  Human visual system is modeled in engineering field providing
feature-engineered methods which detect contrasted/surprising/unusual data into
images. This data is "interesting" for humans and leads to numerous
applications. Deep learning (DNNs) drastically improved the algorithms
efficiency on the main benchmark datasets. However, DNN-based models are
counter-intuitive: surprising or unusual data is by definition difficult to
learn because of its low occurrence probability. In reality, DNN-based models
mainly learn top-down features such as faces, text, people, or animals which
usually attract human attention, but they have low efficiency in extracting
surprising or unusual data in the images. In this paper, we propose a new
visual attention model called DeepRare2021 (DR21) which uses the power of DNNs
feature extraction and the genericity of feature-engineered algorithms. This
algorithm is an evolution of a previous version called DeepRare2019 (DR19)
based on a common framework. DR21 1) does not need any training and uses the
default ImageNet training, 2) is fast even on CPU, 3) is tested on four very
different eye-tracking datasets showing that the DR21 is generic and is always
in the within the top models on all datasets and metrics while no other model
exhibits such a regularity and genericity. Finally DR21 4) is tested with
several network architectures such as VGG16 (V16), VGG19 (V19) and MobileNetV2
(MN2) and 5) it provides explanation and transparency on which parts of the
image are the most surprising at different levels despite the use of a
DNN-based feature extractor. DeepRare2021 code can be found at
this https URL}.

    

### [[2109.11471] Safe-Planner: A Single-Outcome Replanner for Computing Strong Cyclic Policies in Fully Observable Non-Deterministic Domains](http://arxiv.org/abs/2109.11471)


  Replanners are efficient methods for solving non-deterministic planning
problems. Despite showing good scalability, existing replanners often fail to
solve problems involving a large number of misleading plans, i.e., weak plans
that do not lead to strong solutions, however, due to their minimal lengths,
are likely to be found at every replanning iteration. The poor performance of
replanners in such problems is due to their all-outcome determinization. That
is, when compiling from non-deterministic to classical, they include all
compiled classical operators in a single deterministic domain which leads
replanners to continually generate misleading plans. We introduce an offline
replanner, called Safe-Planner (SP), that relies on a single-outcome
determinization to compile a non-deterministic domain to a set of classical
domains, and ordering heuristics for ranking the obtained classical domains.
The proposed single-outcome determinization and the heuristics allow for
alternating between different classical domains. We show experimentally that
this approach can allow SP to avoid generating misleading plans but to generate
weak plans that directly lead to strong solutions. The experiments show that SP
outperforms state-of-the-art non-deterministic solvers by solving a broader
range of problems. We also validate the practical utility of SP in real-world
non-deterministic robotic tasks.

    

### [[2109.11503] Finding a Balanced Degree of Automation for Summary Evaluation](http://arxiv.org/abs/2109.11503)


  Human evaluation for summarization tasks is reliable but brings in issues of
reproducibility and high costs. Automatic metrics are cheap and reproducible
but sometimes poorly correlated with human judgment. In this work, we propose
flexible semiautomatic to automatic summary evaluation metrics, following the
Pyramid human evaluation method. Semi-automatic Lite2Pyramid retains the
reusable human-labeled Summary Content Units (SCUs) for reference(s) but
replaces the manual work of judging SCUs' presence in system summaries with a
natural language inference (NLI) model. Fully automatic Lite3Pyramid further
substitutes SCUs with automatically extracted Semantic Triplet Units (STUs) via
a semantic role labeling (SRL) model. Finally, we propose in-between metrics,
Lite2.xPyramid, where we use a simple regressor to predict how well the STUs
can simulate SCUs and retain SCUs that are more difficult to simulate, which
provides a smooth transition and balance between automation and manual
evaluation. Comparing to 15 existing metrics, we evaluate human-metric
correlations on 3 existing meta-evaluation datasets and our newly-collected
PyrXSum (with 100/10 XSum examples/systems). It shows that Lite2Pyramid
consistently has the best summary-level correlations; Lite3Pyramid works better
than or comparable to other automatic metrics; Lite2.xPyramid trades off small
correlation drops for larger manual effort reduction, which can reduce costs
for future data collection. Our code and data are publicly available at:
this https URL


### [[2109.11513] Temporal Inference with Finite Factored Sets](http://arxiv.org/abs/2109.11513)


  We propose a new approach to temporal inference, inspired by the Pearlian
causal inference paradigm - though quite different from Pearl's approach
formally. Rather than using directed acyclic graphs, we make use of factored
sets, which are sets expressed as Cartesian products. We show that finite
factored sets are powerful tools for inferring temporal relations. We introduce
an analog of d-separation for factored sets, conditional orthogonality, and we
demonstrate that this notion is equivalent to conditional independence in all
probability distributions on a finite factored set.

    

### [[1911.04766] Investigating Constraint Programming and Hybrid Methods for Real World Industrial Test Laboratory Scheduling](http://arxiv.org/abs/1911.04766)


  In this paper we deal with a complex real world scheduling problem closely
related to the well-known Resource-Constrained Project Scheduling Problem
(RCPSP). The problem concerns industrial test laboratories in which a large
number of tests has to be performed by qualified personnel using specialised
equipment, while respecting deadlines and other constraints. We present
different constraint programming models and search strategies for this problem.
Furthermore, we propose a Very Large Neighborhood Search approach based on our
CP methods. Our models are evaluated using CP solvers and a MIP solver both on
real-world test laboratory data and on a set of generated instances of
different sizes based on the real-world data. Further, we compare the exact
approaches with VLNS and a Simulated Annealing heuristic. We could find
feasible solutions for all instances and several optimal solutions and we show
that using VLNS we can improve upon the results of the other approaches.

    

### [[2101.08698] Validating Label Consistency in NER Data Annotation](http://arxiv.org/abs/2101.08698)


  Data annotation plays a crucial role in ensuring your named entity
recognition (NER) projects are trained with the right information to learn
from. Producing the most accurate labels is a challenge due to the complexity
involved with annotation. Label inconsistency between multiple subsets of data
annotation (e.g., training set and test set, or multiple training subsets) is
an indicator of label mistakes. In this work, we present an empirical method to
explore the relationship between label (in-)consistency and NER model
performance. It can be used to validate the label consistency (or catches the
inconsistency) in multiple sets of NER data annotation. In experiments, our
method identified the label inconsistency of test data in SCIERC and CoNLL03
datasets (with 26.7% and 5.4% label mistakes). It validated the consistency in
the corrected version of both datasets.

    

### [[2101.11253] Puzzle-CAM: Improved localization via matching partial and full features](http://arxiv.org/abs/2101.11253)


  Weakly-supervised semantic segmentation (WSSS) is introduced to narrow the
gap for semantic segmentation performance from pixel-level supervision to
image-level supervision. Most advanced approaches are based on class activation
maps (CAMs) to generate pseudo-labels to train the segmentation network. The
main limitation of WSSS is that the process of generating pseudo-labels from
CAMs that use an image classifier is mainly focused on the most discriminative
parts of the objects. To address this issue, we propose Puzzle-CAM, a process
that minimizes differences between the features from separate patches and the
whole image. Our method consists of a puzzle module and two regularization
terms to discover the most integrated region in an object. Puzzle-CAM can
activate the overall region of an object using image-level supervision without
requiring extra parameters. % In experiments, Puzzle-CAM outperformed previous
state-of-the-art methods using the same labels for supervision on the PASCAL
VOC 2012 test dataset. In experiments, Puzzle-CAM outperformed previous
state-of-the-art methods using the same labels for supervision on the PASCAL
VOC 2012 dataset. Code associated with our experiments is available at
this https URL.

    

### [[2104.04269] Morpho-evolution with learning using a controller archive as an inheritance mechanism](http://arxiv.org/abs/2104.04269)


  The joint optimisation of body-plan and control via evolutionary processes
can be challenging in rich morphological spaces in which offspring can have
body-plans that are very different from either of their parents. This causes a
potential mismatch between the structure of an inherited controller and the new
body. To address this, we propose a framework that combines an evolutionary
algorithm to generate body-plans and a learning algorithm to optimise the
parameters of a neural controller. The topology of this controller is created
once the body-plan of each offspring body-plan is generated. The key novelty of
the approach is to add an external archive for storing learned controllers that
map to explicit `types' of robots (where this is defined with respect the
features of the body-plan). By learning from a controller with an appropriate
structure inherited from the archive, rather than from a randomly initialised
one, we show that both the speed and magnitude of learning increases over time
when compared to an approach that starts from scratch, using two tasks and
three environments. The framework also provides new insights into the complex
interactions between evolution and learning.

    

### [[2104.08066] Effect of Visual Extensions on Natural Language Understanding in Vision-and-Language Models](http://arxiv.org/abs/2104.08066)


  A method for creating a vision-and-language (V&L) model is to extend a
language model through structural modifications and V&L pre-training. Such an
extension aims to make a V&L model inherit the capability of natural language
understanding (NLU) from the original language model. To see how well this is
achieved, we propose to evaluate V&L models using an NLU benchmark (GLUE). We
compare five V&L models, including single-stream and dual-stream models,
trained with the same pre-training. Dual-stream models, with their higher
modality independence achieved by approximately doubling the number of
parameters, are expected to preserve the NLU capability better. Our main
finding is that the dual-stream scores are not much different than the
single-stream scores, contrary to expectation. Further analysis shows that
pre-training causes the performance drop in NLU tasks with few exceptions.
These results suggest that adopting a single-stream structure and devising the
pre-training could be an effective method for improving the maintenance of
language knowledge in V&L extensions.

    

### [[2105.07354] Order Effects in Bayesian Updates](http://arxiv.org/abs/2105.07354)


  Order effects occur when judgments about a hypothesis's probability given a
sequence of information do not equal the probability of the same hypothesis
when the information is reversed. Different experiments have been performed in
the literature that supports evidence of order effects.
We proposed a Bayesian update model for order effects where each question can
be thought of as a mini-experiment where the respondents reflect on their
beliefs. We showed that order effects appear, and they have a simple cognitive
explanation: the respondent's prior belief that two questions are correlated.
The proposed Bayesian model allows us to make several predictions: (1) we
found certain conditions on the priors that limit the existence of order
effects; (2) we show that, for our model, the QQ equality is not necessarily
satisfied (due to symmetry assumptions); and (3) the proposed Bayesian model
has the advantage of possessing fewer parameters than its quantum counterpart.

    

### [[2109.09396] Dynamic Gesture Recognition](http://arxiv.org/abs/2109.09396)


  The Human-Machine Interaction (HMI) research field is an important topic in
machine learning that has been deeply investigated thanks to the rise of
computing power in the last years. The first time, it is possible to use
machine learning to classify images and/or videos instead of the traditional
computer vision algorithms. The aim of this project is to builda symbiosis
between a convolutional neural network (CNN)[1] and a recurrent neural network
(RNN) [2] to recognize cultural/anthropological Italian sign language gestures
from videos. The CNN extracts important features that later areused by the RNN.
With RNNs we are able to store temporal information inside the model to provide
contextual information from previous frames to enhance the prediction accuracy.
Our novel approach uses different data augmentation techniquesand
regularization methods from only RGB frames to avoid overfitting and provide a
small generalization error.

    

### [[2109.09725] Model Bias in NLP -- Application to Hate Speech Classification](http://arxiv.org/abs/2109.09725)


  This document sums up our results forthe NLP lecture at ETH in the spring
semester 2021. In this work, a BERT based neural network model (Devlin et
al.,2018) is applied to the JIGSAW dataset (Jigsaw/Conversation AI, 2019) in
order to create a model identifying hateful and toxic comments (strictly
seperated from offensive language) in online social platforms (English
language), inthis case Twitter. Three other neural network architectures and a
GPT-2 (Radfordet al., 2019) model are also applied on the provided data set in
order to compare these different models. The trained BERT model is then applied
on two different data sets to evaluate its generalisation power, namely on
another Twitter data set (Tom Davidson, 2017) (Davidsonet al., 2017) and the
data set HASOC 2019 (Thomas Mandl, 2019) (Mandl et al.,2019) which includes
Twitter and also Facebook comments; we focus on the English HASOC 2019 data. In
addition, it can be shown that by fine-tuning the trained BERT model on these
two datasets by applying different transfer learning scenarios via retraining
partial or all layers the predictive scores improve compared to simply applying
the model pre-trained on the JIGSAW data set. Withour results, we get
precisions from 64% to around 90% while still achieving acceptable recall
values of at least lower 60s%, proving that BERT is suitable for real usecases
in social platforms.

    

### [[2109.11016] Report on the "The Future of the Shell" Panel at HotOS 2021](http://arxiv.org/abs/2109.11016)


  This document summarizes the challenges and possible research directions
around the shell and its ecosystem, collected during and after the HotOS21
Panel on the future of the shell. The goal is to create a snapshot of what a
number of researchers from various disciplines -- connected to the shell to
varying degrees -- think about its future. We hope that this document will
serve as a reference for future research on the shell and its ecosystem.

    

### [[2109.11397] Position Paper: Goals of the Luau Type System](http://arxiv.org/abs/2109.11397)


  Luau is the scripting language that powers user-generated experiences on the
Roblox platform. It is a statically-typed language, based on the
dynamically-typed Lua language, with type inference. These types are used for
providing editor assistance in Roblox Studio, the IDE for authoring Roblox
experiences. Due to Roblox's uniquely heterogeneous developer community, Luau
must operate in a somewhat different fashion than a traditional
statically-typed language. In this paper, we describe some of the goals of the
Luau type system, focusing on where the goals differ from those of other type
systems.

    

### [[1907.01257] A robust graph-based approach to observational equivalence](http://arxiv.org/abs/1907.01257)


  We propose a new approach to defining programming languages with effects, and
proving observational equivalence. Operational machinery is given by a
hypergraph-rewriting abstract machine inspired by Girard's Geometry of
Interaction. It interprets a graph calculus of only three intrinsic constructs:
variable binding, atom binding, and thunking. Everything else, including
features which are commonly thought of as intrinsic, such as arithmetic or
function abstraction and application, must be provided as extrinsic operations,
with associated rewrite rules. The graph representation naturally leads to two
new principles about equational reasoning, which we call \emph{locality} and
\emph{robustness}. These concepts enable a novel flexible and powerful
reasoning methodology about (type-free) languages with effects. The methodology
is additionally capable of proving a generalised notion of observational
equivalence that can be quantified over syntactically restricted contexts
instead of all contexts, and also can be quantitatively constrained in terms of
the number of reduction steps. We illustrate the methodology using the
call-by-value lambda-calculus extended with (higher-order) state.

    

### [[2011.06171] The Usability of Ownership](http://arxiv.org/abs/2011.06171)


  Ownership is the concept of tracking aliases and mutations to data, useful
for both memory safety and system design. The Rust programming language
implements ownership via the borrow checker, a static analyzer that extends the
core type system. The borrow checker is a notorious learning barrier for new
Rust users. In this paper, I focus on the gap between understanding ownership
in theory versus its implementation in the borrow checker. As a sound and
incomplete analysis, compiler errors may arise from either ownership-unsound
behavior or limitations of the analyzer. Understanding this distinction is
essential for fixing ownership errors. But how are users actually supposed to
make the correct inference? Drawing on my experience with using and teaching
Rust, I explore the many challenges in interpreting and responding to ownership
errors. I also suggest educational and automated interventions that could
improve the usability of ownership.

    

### [<title>New tool: an nginx playground</title>](https://jvns.ca/blog/2021/09/24/new-tool--an-nginx-playground/)

### [<title>[XGBoost4J-Spark] Model training stuck at `foreachPartition at XGBoost.scala:565` - RFC - XGBoost</title>](https://discuss.xgboost.ai/t/xgboost4j-spark-model-training-stuck-at-foreachpartition-at-xgboost-scala-565/2458/2)