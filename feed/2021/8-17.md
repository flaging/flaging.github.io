
## 2021-8-17

### [<title>专注于)宁波酒店住宿发票-宁波本地宝 - DockOne.io</title>](http://dockone.io/question/933963)

### [<title>关注于天津哪里能开餐饮费电子发票-天津本地宝fc5 - DockOne.io</title>](http://dockone.io/question/933960)

### [<title>分享)杭州哪里能开具酒店专用发票-杭州本地宝py6 - DockOne.io</title>](http://dockone.io/question/933962)

### [<title>分享)成都哪里能开具酒店专用发票-成都本地宝xm7 - DockOne.io</title>](http://dockone.io/question/933961)

### [<title>关注于杭州哪里能开餐饮费电子发票-杭州本地宝gu1 - DockOne.io</title>](http://dockone.io/question/933959)

### [<title>专注于)郑州酒店住宿发票-郑州本地宝 - DockOne.io</title>](http://dockone.io/question/933958)

### [<title>专注于)重庆酒店住宿发票-重庆本地宝 - DockOne.io</title>](http://dockone.io/question/933957)

### [<title>关注于成都哪里能开餐饮费电子发票-成都本地宝hm6 - DockOne.io</title>](http://dockone.io/question/933956)

### [<title>分享)广州哪里能开具酒店专用发票-广州本地宝vb5 - DockOne.io</title>](http://dockone.io/question/933955)

### [<title>分享)深圳哪里能开具酒店专用发票-深圳本地宝de6 - DockOne.io</title>](http://dockone.io/question/933954)

### [<title>关注于广州哪里能开餐饮费电子发票-广州本地宝ug9 - DockOne.io</title>](http://dockone.io/question/933953)

### [<title>专注于)天津酒店住宿发票-天津本地宝 - DockOne.io</title>](http://dockone.io/question/933952)

### [<title>关注于深圳哪里能开餐饮费电子发票-深圳本地宝so8 - DockOne.io</title>](http://dockone.io/question/933951)

### [<title>专注于)杭州酒店住宿发票-杭州本地宝 - DockOne.io</title>](http://dockone.io/question/933950)

### [<title>分享)北京哪里能开具酒店专用发票-北京本地宝dq7 - DockOne.io</title>](http://dockone.io/question/933949)

### [<title>分享)上海哪里能开具酒店专用发票-上海本地宝my9 - DockOne.io</title>](http://dockone.io/question/933948)

### [<title>关注于北京哪里能开餐饮费电子发票-北京本地宝ow7 - DockOne.io</title>](http://dockone.io/question/933947)

### [<title>专注于)成都酒店住宿发票-成都本地宝 - DockOne.io</title>](http://dockone.io/question/933946)

### [<title>专注于)广州酒店住宿发票-广州本地宝 - DockOne.io</title>](http://dockone.io/question/933945)

### [<title>关注于上海哪里能开餐饮费电子发票-上海本地宝iz9 - DockOne.io</title>](http://dockone.io/question/933944)

### [<title>Data type of XGBoost - XGBoost</title>](https://discuss.xgboost.ai/t/data-type-of-xgboost/2436/2)

### [<title>Data type of XGBoost - XGBoost</title>](https://discuss.xgboost.ai/t/data-type-of-xgboost/2436/1)

### [<title>Negative labels in XGBRanker w/ listwise objective - XGBoost</title>](https://discuss.xgboost.ai/t/negative-labels-in-xgbranker-w-listwise-objective/1655/6)

### [<title>Negative labels in XGBRanker w/ listwise objective - XGBoost</title>](https://discuss.xgboost.ai/t/negative-labels-in-xgbranker-w-listwise-objective/1655/5)

### [[2108.06410] SINA - Smart Interoperability Architecture An architecture fostering the interoperability between smart building technology from different manufacturers and smart grid infrastructure to enable new business models for energy services](http://arxiv.org/abs/2108.06410)


  More and more household appliances connect to the Internet and exchange data
freely. This is the foundation for true smart buildings. However, there is
still no uniform communication technology available, which can connect all
appliances from all vendors. Protocols differ between manufacturers making
interoperability difficult or even impossible. Manufacturers cannot rely on a
reference for the implementation and real estate developers and operators are
reluctant to commit to a system until it is clear which one will prevail. A
similar situation is evident in smart grids and applies equally to the energy
supply industry. This fragmentation ultimately leads to missed opportunities in
terms of business models which could connect customers with service providers.
We present a first draft of an architecture: SINA - Smart Interoperability
Architecture. SINA is based on existing decentralized infrastructure, which
avoids creating a dependency of the market participants on an overpowering
service provider. The core element of the technical solution is an open-source
module integrated in the private clouds of the manufacturers, energy suppliers
and service providers. The architecture addresses problems of data ownership,
privacy and data security avoiding central administrative structures. It
manages data access and transfer in a decentralized and distributed system.
SINA uses a blockchain and smart contracts to make sure that the pieces of
information about which data are accessed, by whom they are accessed, how they
are processed, and which monetary transactions take place are immutably stored
and made available. This allows providers to offer services to users in a
transparent and trustworthy manner. Finally, SINA includes a matchmaking block
which helps service providers find potential customers and vice versa. This set
of features makes SINA unique.

    

### [[2108.06527] Evolution Toward 6G Wireless Networks: A Resource Management Perspective](http://arxiv.org/abs/2108.06527)


  In this article, we first present the vision, key performance indicators, key
enabling techniques (KETs), and services of 6G wireless networks. Then, we
highlight a series of general resource management (RM) challenges as well as
unique RM challenges corresponding to each KET. The unique RM challenges in 6G
necessitate the transformation of existing optimization-based solutions to
artificial intelligence/machine learning-empowered solutions. In the sequel, we
formulate a joint network selection and subchannel allocation problem for 6G
multi-band network that provides both further enhanced mobile broadband (FeMBB)
and extreme ultra reliable low latency communication (eURLLC) services to the
terrestrial and aerial users. Our solution highlights the efficacy of
multi-band network and demonstrates the robustness of dueling deep Q-learning
in obtaining efficient RM solution with faster convergence rate compared to
deep-Q network and double deep Q-network algorithms.

    

### [[2108.06606] Prediction Analysis of Optical Tracker Parameters using Machine Learning Approaches for efficient Head Tracking](http://arxiv.org/abs/2108.06606)


  A head tracker is a crucial part of the head mounted display systems, as it
tracks the head of the pilot in the plane/cockpit simulator. The operational
flaws of head trackers are also dependent on different environmental conditions
like different lighting conditions and stray light interference. In this
letter, an optical tracker has been employed to gather the 6-DoF data of head
movements under different environmental conditions. Also, the effect of
different environmental conditions and variation in distance between the
receiver and optical transmitter on the 6-DoF data was analyzed.

    

### [[2108.06650] Vertical, Temporal, and Horizontal Scaling of Hierarchical Hypersparse GraphBLAS Matrices](http://arxiv.org/abs/2108.06650)


  Hypersparse matrices are a powerful enabler for a variety of network, health,
finance, and social applications. Hierarchical hypersparse GraphBLAS matrices
enable rapid streaming updates while preserving algebraic analytic power and
convenience. In many contexts, the rate of these updates sets the bounds on
performance. This paper explores hierarchical hypersparse update performance on
a variety of hardware with identical software configurations. The high-level
language bindings of the GraphBLAS readily enable performance experiments on
simultaneous diverse hardware. The best single process performance measured was
4,000,000 updates per second. The best single node performance measured was
170,000,000 updates per second. The hardware used spans nearly a decade and
allows a direct comparison of hardware improvements for this computation over
this time range; showing a 2x increase in single-core performance, a 3x
increase in single process performance, and a 5x increase in single node
performance. Running on nearly 2,000 MIT SuperCloud nodes simultaneously
achieved a sustained update rate of over 200,000,000,000 updates per second.
Hierarchical hypersparse GraphBLAS allows the MIT SuperCloud to analyze
extremely large streaming network data sets.

    

### [[2108.06653] Spatial Temporal Analysis of 40,000,000,000,000 Internet Darkspace Packets](http://arxiv.org/abs/2108.06653)


  The Internet has never been more important to our society, and understanding
the behavior of the Internet is essential. The Center for Applied Internet Data
Analysis (CAIDA) Telescope observes a continuous stream of packets from an
unsolicited darkspace representing 1/256 of the Internet. During 2019 and 2020
over 40,000,000,000,000 unique packets were collected representing the largest
ever assembled public corpus of Internet traffic. Using the combined resources
of the Supercomputing Centers at UC San Diego, Lawrence Berkeley National
Laboratory, and MIT, the spatial temporal structure of anonymized
source-destination pairs from the CAIDA Telescope data has been analyzed with
GraphBLAS hierarchical hypersparse matrices. These analyses provide unique
insight on this unsolicited Internet darkspace traffic with the discovery of
many previously unseen scaling relations. The data show a significant sustained
increase in unsolicited traffic corresponding to the start of the COVID19
pandemic, but relatively little change in the underlying scaling relations
associated with unique sources, source fan-outs, unique links, destination
fan-ins, and unique destinations. This work provides a demonstration of the
practical feasibility and benefit of the safe collection and analysis of
significant quantities of anonymized Internet traffic.

    

### [[2108.06696] Automated Enterprise Architecture Model Mining](http://arxiv.org/abs/2108.06696)


  Metadata are like the steam engine of the 21st century, driving businesses
and offer multiple enhancements. Nevertheless, many companies are unaware that
these data can be used efficiently to improve their own operation. This is
where the Enterprise Architecture Framework comes in. It empowers an
organisation to get a clear view of their business, application, technical and
physical layer. This modelling approach is an established method for
organizations to take a deeper look into their structure and processes. The
development of such models requires a great deal of effort, is carried out
manually by interviewing stakeholders and requires continuous maintenance. Our
new approach enables the automated mining of Enterprise Architecture models.
The system uses common technologies to collect the metadata based on network
traffic, log files and other information in an organisation. Based on this, the
new approach generates EA models with the desired views points. Furthermore, a
rule and knowledge-based reasoning is used to obtain a holistic overview. This
offers a strategic decision support from business structure over process design
up to planning the appropriate support technology. Therefore, it forms the base
for organisations to act in an agile way. The modelling can be performed in
different modelling languages, including ArchiMate and the Nato Architecture
Framework (NAF). The designed approach is already evaluated on a small company
with multiple services and an infrastructure with several nodes.

    

### [[2108.06701] Reference Service Model for Federated Identity Management](http://arxiv.org/abs/2108.06701)


  With the pandemic of COVID-19, people around the world increasingly work from
home. Each natural person typically has several digital identities with
different associated information. During the last years, various identity and
access management approaches have gained attraction, helping for example to
access other organization's services within trust boundaries. The resulting
heterogeneity creates a high complexity to differentiate between these
approaches and scenarios as participating entity; combining them is even
harder. Last but not least, various actors have a different understanding or
perspective of the terms, like 'service', in this context. Our paper describes
a reference service with standard components in generic federated identity
management. This is utilized with modern Enterprise Architecture using the
framework ArchiMate. The proposed universal federated identity management
service model (FIMSM) is applied to describe various federated identity
management scenarios in a generic service-oriented way. The presented reference
design is approved in multiple aspects and is easily applicable in numerous
scenarios.

    

### [[2108.06707] SwarMS: Swarm-Computations for Mobile Scenarios](http://arxiv.org/abs/2108.06707)


  Nowadays, most network systems are based on fixed and reliable
infrastructure. In this context Information Centric Networking (ICN) is a novel
network approach, where data is in the focus instead of hosts. Therefore,
requests for data are independent from the location where the data is actually
stored on. This property is optimal for infrastructure-less and swarm
networking. However, the ICN does not provide support for networks without
infrastructure. In this paper we present SwarMS, an architecture for swarm
on-boarding, swarm coordination and swarm computations in ICN style networks.
We use append-only logs to solve the challenges of loose mobile swarm
organisation and executing computations. By replicating tasks on multiple nodes
we achieve more reliability and trust in results.

    

### [[2108.06710] Tangle Centric Networking (TCN)](http://arxiv.org/abs/2108.06710)


  Today's Internet is heavily used for multimedia streaming from cloud
backends, while the Internet of Things (IoT) reverses the traditional data
flow, with high data volumes produced at the network edge. Information Centric
Networking (ICN) advocates against a host-centric communication model which is
promising for distributed edge computing environments and the execution of IoT
applications in a decentralized fashion. By using naming schemes, data is
tightly coupled to names instead of hosts which simplifies discovery and access
to data and services. However, the tight coupling challenges network
performance due to additional synchronization overhead of large data volumes
and services. We present Tangle Centric Networking (TCN) -- a decentralized
data structure for coordinated distributed applications and data exchange
following principles of ICN. TCN can react on data and service changes and
update them accordingly in network nodes, provide distributed data structures
and enable cooperative work on the same data without huge overhead by using
Tangles for coordination. We implemented TCN in simulations and evaluated the
concept against a base line scenario.

    

### [[2108.06779] Decentralized Power Allocation and Beamforming Using Non-Convex Nash Game for Energy-Aware mmWave Networks](http://arxiv.org/abs/2108.06779)


  This paper focuses on the problem of joint beamforming control and power
allocation in the ad-hoc mmWave network. Over the shared spectrum, a number of
multi-input-multi-output links attempt to minimize their supply power by
simultaneously finding the locally optimal power allocation and beamformers in
a self-interested manner. Our design considers a category of non-convex
quality-of-service constraints, which are a function of the coupled strategies
adopted by the mutually interfering ad-hoc links. We propose a two-stage,
decentralized searching scheme, where the adaptation of power-levels and
beamformer filters are performed in two separated sub-stages iteratively at
each link. By introducing the analysis based on the generalized Nash
equilibrium, we provide the theoretical proof of the convergence of our
proposed power adaptation algorithm based on the local best response together
with an iterative minimum mean square error receiver. Several transmit
beamforming schemes requiring different levels of information exchange are
compared. Our simulation results show that with a minimum-level requirement on
the channel state information acquisition, a locally optimal transmit filter
design based on the optimization of the local signal-to-interference-plus-noise
ratio is able to achieve an acceptable tradeoff between link performance and
the need for decentralization.

    

### [[2108.06817] Learning from Images: Proactive Caching with Parallel Convolutional Neural Networks](http://arxiv.org/abs/2108.06817)


  With the continuous trend of data explosion, delivering packets from data
servers to end users causes increased stress on both the fronthaul and backhaul
traffic of mobile networks. To mitigate this problem, caching popular content
closer to the end-users has emerged as an effective method for reducing network
congestion and improving user experience. To find the optimal locations for
content caching, many conventional approaches construct various mixed integer
linear programming (MILP) models. However, such methods may fail to support
online decision making due to the inherent curse of dimensionality. In this
paper, a novel framework for proactive caching is proposed. This framework
merges model-based optimization with data-driven techniques by transforming an
optimization problem into a grayscale image. For parallel training and simple
design purposes, the proposed MILP model is first decomposed into a number of
sub-problems and, then, convolutional neural networks (CNNs) are trained to
predict content caching locations of these sub-problems. Furthermore, since the
MILP model decomposition neglects the internal effects among sub-problems, the
CNNs' outputs have the risk to be infeasible solutions. Therefore, two
algorithms are provided: the first uses predictions from CNNs as an extra
constraint to reduce the number of decision variables; the second employs CNNs'
outputs to accelerate local search. Numerical results show that the proposed
scheme can reduce 71.6% computation time with only 0.8% additional performance
cost compared to the MILP solution, which provides high quality decision making
in real-time.

    

### [[2108.06854] Demo Abstract: WiSwitch: A Low-Cost WiFi-based Remote Switch Control for Smart Homes](http://arxiv.org/abs/2108.06854)


  With the emergence of the IoT, there is an increasing trend towards designing
a new low-cost cyber-physical platform accessible through the Internet. In this
Demo paper, we present WiSwitch, a homemade prototype of a low-cost wireless
switch control allowing the user to control the room light through the cloud.
The hardware is based on a commercial-off-the-shelf (COTS) ESP 12 WiFi module
with basic electronic circuitry. We develop an embedded client application in
the ESP 12 that allows us to connect it to the Amazon Web Services cloud and
update the status of the switch remotely. We also present an experimental
validation of the WiSwitch platform.

    

### [[2108.06884] Seirios: Leveraging Multiple Channels for LoRaWAN Indoor and Outdoor Localization](http://arxiv.org/abs/2108.06884)


  Localization is important for a large number of Internet of Things (IoT)
endpoint devices connected by LoRaWAN. Due to the bandwidth limitations of
LoRaWAN, existing localization methods without specialized hardware (e.g., GPS)
produce poor performance. To increase the localization accuracy, we propose a
super-resolution localization method, called Seirios, which features a novel
algorithm to synchronize multiple non-overlapped communication channels by
exploiting the unique features of the radio physical layer to increase the
overall bandwidth. By exploiting both the original and the conjugate of the
physical layer, Seirios can resolve the direct path from multiple reflectors in
both indoor and outdoor environments. We design a Seirios prototype and
evaluate its performance in an outdoor area of 100 m $\times$ 60 m, and an
indoor area of 25 m $\times$ 15 m, which shows that Seirios can achieve a
median error of 4.4 m outdoors (80% samples < 6.4 m), and 2.4 m indoors (80%
samples < 6.1 m), respectively. The results show that Seirios produces 42% less
localization error than the baseline approach. Our evaluation also shows that,
different to previous studies in Wi-Fi localization systems that have wider
bandwidth, time-of-fight (ToF) estimation is less effective for LoRaWAN
localization systems with narrowband radio signals.

    

### [[2108.06891] Efficient Network Analysis Under Link Deletion](http://arxiv.org/abs/2108.06891)


  The problem of worst case edge deletion from a network is considered. Suppose
that you have a communication network and you can delete a single edge. Which
edge deletion causes the largest disruption? More formally, given a graph,
which edge after deletion disconnects the maximum number of pairs of vertices,
where ties for number of pairs disconnected are broken by finding an edge that
increases the average shortest path length the maximum amount. This problem is
interesting both practically and theoretically. In practice, it is a good tool
to measure network robustness and find vulnerable edges. In theory, it is
related, though subtly different, to classical problems including the dynamic
all-pairs shortest paths problem, and the decremental shortest paths problem.

    

### [[2108.06935] Speed Scaling with Multiple Servers Under A Sum Power Constraint](http://arxiv.org/abs/2108.06935)


  The problem of scheduling jobs and choosing their respective speeds with
multiple servers under a sum power constraint to minimize the flow time +
energy is considered. This problem is a generalization of the flow time
minimization problem with multiple unit-speed servers, when jobs can be
parallelized, however, with a sub-linear, concave speedup function
$k^{1/\alpha}, \alpha>1$ when allocated $k$ servers, i.e., jobs experience
diminishing returns from being allocated additional servers. When all jobs are
available at time $0$, we show that a very simple algorithm EQUI, that
processes all available jobs at the same speed is
$\left(2-\frac{1}{\alpha}\right)
\frac{2}{\left(1-\left(\frac{1}{\alpha}\right)\right)}$-competitive, while in
the general case, when jobs arrive over time, an LCFS based algorithm is shown
to have a constant (dependent only on $\alpha$) competitive ratio.

    

### [[2108.07092] Routing in Delay-Tolerant Networks under Uncertain Contact Plans](http://arxiv.org/abs/2108.07092)


  Delay-Tolerant Networks (DTN) enable store-carry-and-forward data
transmission in networks challenged by frequent disruptions and high latency.
Existing classification distinguishes between scheduled and probabilistic DTNs,
for which specific routing solutions have been developed. In this paper, we
uncover a gap in-between where uncertain contact plans can be exploited to
enhance data delivery in many practical scenarios described by probabilistic
schedules available a priori. Routing under uncertain contact plans (RUCoP) is
next formulated as a multiple-copy Markov Decision Process and then exported to
local-knowledge (L-RUCoP) and Contact Graph Routing extensions (CGR-UCoP) which
can be implemented in the existing DTN protocol stack. RUCoP and its
derivations are evaluated in a first extensive simulation benchmark for DTNs
under uncertain contact plans comprising both random and realistic scenarios.
Results confirm that RUCoP and L-RUCoP closely approach the ideal delivery
ratio of an oracle, while CGR-UCoP improves state-of-the-art DTN routing
schemes delivery ratio up to 25%.

    

### [[2108.07124] Using Cyber Terrain in Reinforcement Learning for Penetration Testing](http://arxiv.org/abs/2108.07124)


  Reinforcement learning (RL) has been applied to attack graphs for penetration
testing, however, trained agents do not reflect reality because the attack
graphs lack operational nuances typically captured within the intelligence
preparation of the battlefield (IPB) that include notions of (cyber) terrain.
In particular, current practice constructs attack graphs exclusively using the
Common Vulnerability Scoring System (CVSS) and its components. We present
methods for constructing attack graphs using notions from IPB on cyber terrain
analysis of obstacles, avenues of approach, key terrain, observation and fields
of fire, and cover and concealment. We demonstrate our methods on an example
where firewalls are treated as obstacles and represented in (1) the reward
space and (2) the state dynamics. We show that terrain analysis can be used to
bring realism to attack graphs for RL.

    

### [[2011.07397] Quantum communication capacity transition of complex quantum networks](http://arxiv.org/abs/2011.07397)


  Quantum network is the key to enable distributed quantum information
processing. As the single-link communication rate decays exponentially with the
distance, to enable reliable end-to-end quantum communication, the number of
nodes needs to grow with the network scale. For highly connected networks, we
identify a threshold transition in the capacity as the density of network nodes
increases---below a critical density, the rate is almost zero, while above the
threshold the rate increases linearly with the density. Surprisingly, above the
threshold the typical communication capacity between two nodes is independent
of the distance between them, due to multi-path routing enabled by the quantum
network. In contrast, for less connected networks such as scale-free networks,
the end-to-end capacity saturates to constants as the number of nodes
increases, and always decays with the distance. Our results are based on
capacity evaluations, therefore the minimum density requirement for an
appreciable capacity applies to any general protocols of quantum networks.

    

### [[2012.02241] Quantum Internet under random breakdowns and intentional attacks](http://arxiv.org/abs/2012.02241)


  Quantum networks will play a key role in distributed quantum information
processing. As the network size increases, network-level errors like random
breakdown and intentional attack are inevitable; therefore, it is important to
understand the robustness of large-scale quantum networks, similar to what has
been done for the classical counterpart---the Internet. For exponential
networks such as Waxman networks, errors simply re-parameterize the network and
lead to a linear decrease of the quantum capacity with the probability of
error. The same linear decay happens for scale-free quantum networks under
random breakdowns, despite the previously discovered robustness in terms of the
connectivity. In presence of attack, however, the capacity of scale-free
quantum networks shows a sharp exponential decay with the increasing attack
fraction. Our results apply to quantum internet based on fibers for all kinds
of quantum communications and provide implications for the future construction
of quantum networks with regard to its robustness.

    

### [[2103.10650] Low-Complexity Dynamic Resource Scheduling for Downlink MC-NOMA Over Fading Channels](http://arxiv.org/abs/2103.10650)


  In this paper, we investigate dynamic resource scheduling (i.e., joint user,
subchannel, and power scheduling) for downlink multi-channel non-orthogonal
multiple access (MC-NOMA) systems over time-varying fading channels.
Specifically, we address the weighted average sum rate maximization problem
with quality-of-service (QoS) constraints. In particular, to facilitate fast
resource scheduling, we focus on developing a very low-complexity algorithm. To
this end, by leveraging Lagrangian duality and the stochastic optimization
theory, we first develop an opportunistic MC-NOMA scheduling algorithm whereby
the original problem is decomposed into a series of subproblems, one for each
time slot. Accordingly, resource scheduling works in an online manner by
solving one subproblem per time slot, making it more applicable to practical
systems. Then, we further develop a heuristic joint subchannel assignment and
power allocation (Joint-SAPA) algorithm with very low computational complexity,
called Joint-SAPA-LCC, that solves each subproblem. Finally, through
simulation, we show that our Joint-SAPA-LCC algorithm provides good performance
comparable to the existing Joint-SAPA algorithms despite requiring much lower
computational complexity. We also demonstrate that our opportunistic MC-NOMA
scheduling algorithm in which the Joint-SAPA-LCC algorithm is embedded works
well while satisfying given QoS requirements.

    

### [[2108.06339] Asymptotic optimality and minimal complexity of classification by random projection](http://arxiv.org/abs/2108.06339)


  The generalization error of a classifier is related to the complexity of the
set of functions among which the classifier is chosen. Roughly speaking, the
more complex the family, the greater the potential disparity between the
training error and the population error of the classifier. This principle is
embodied in layman's terms by Occam's razor principle, which suggests favoring
low-complexity hypotheses over complex ones. We study a family of
low-complexity classifiers consisting of thresholding the one-dimensional
feature obtained by projecting the data on a random line after embedding it
into a higher dimensional space parametrized by monomials of order up to k.
More specifically, the extended data is projected n-times and the best
classifier among those n (based on its performance on training data) is chosen.
We obtain a bound on the generalization error of these low-complexity
classifiers. The bound is less than that of any classifier with a non-trivial
VC dimension, and thus less than that of a linear classifier. We also show
that, given full knowledge of the class conditional densities, the error of the
classifiers would converge to the optimal (Bayes) error as k and n go to
infinity; if only a training dataset is given, we show that the classifiers
will perfectly classify all the training points as k and n go to infinity.

    

### [[2108.06380] Detecting OODs as datapoints with High Uncertainty](http://arxiv.org/abs/2108.06380)


  Deep neural networks (DNNs) are known to produce incorrect predictions with
very high confidence on out-of-distribution inputs (OODs). This limitation is
one of the key challenges in the adoption of DNNs in high-assurance systems
such as autonomous driving, air traffic management, and medical diagnosis. This
challenge has received significant attention recently, and several techniques
have been developed to detect inputs where the model's prediction cannot be
trusted. These techniques detect OODs as datapoints with either high epistemic
uncertainty or high aleatoric uncertainty. We demonstrate the difference in the
detection ability of these techniques and propose an ensemble approach for
detection of OODs as datapoints with high uncertainty (epistemic or aleatoric).
We perform experiments on vision datasets with multiple DNN architectures,
achieving state-of-the-art results in most cases.

    

### [[2108.06394] A Machine-Learning-Ready Dataset Prepared from the Solar and Heliospheric Observatory Mission](http://arxiv.org/abs/2108.06394)


  We present a Python tool to generate a standard dataset from solar images
that allows for user-defined selection criteria and a range of pre-processing
steps. Our Python tool works with all image products from both the Solar and
Heliospheric Observatory (SoHO) and Solar Dynamics Observatory (SDO) missions.
We discuss a dataset produced from the SoHO mission's multi-spectral images
which is free of missing or corrupt data as well as planetary transits in
coronagraph images, and is temporally synced making it ready for input to a
machine learning system. Machine-learning-ready images are a valuable resource
for the community because they can be used, for example, for forecasting space
weather parameters. We illustrate the use of this data with a 3-5 day-ahead
forecast of the north-south component of the interplanetary magnetic field
(IMF) observed at Lagrange point one (L1). For this use case, we apply a deep
convolutional neural network (CNN) to a subset of the full SoHO dataset and
compare with baseline results from a Gaussian Naive Bayes classifier.

    

### [[2108.06411] Optimal and Efficient Algorithms for General Mixable Losses against Switching Oracles](http://arxiv.org/abs/2108.06411)


  We investigate the problem of online learning, which has gained significant
attention in recent years due to its applicability in a wide range of fields
from machine learning to game theory. Specifically, we study the online
optimization of mixable loss functions in a dynamic environment. We introduce
online mixture schemes that asymptotically achieves the performance of the best
dynamic estimation sequence of the switching oracle with optimal regret
redundancies. The best dynamic estimation sequence that we compete against is
selected in hindsight with full observation of the loss functions and is
allowed to select different optimal estimations in different time intervals
(segments). We propose two mixtures in our work. Firstly, we propose a
tractable polynomial time complexity algorithm that can achieve the optimal
redundancy of the intractable brute force approach. Secondly, we propose an
efficient logarithmic time complexity algorithm that can achieve the optimal
redundancy up to a constant multiplicity gap. Our results are guaranteed to
hold in a strong deterministic sense in an individual sequence manner.

    

### [[2108.06415] The Sharpe predictor for fairness in machine learning](http://arxiv.org/abs/2108.06415)


  In machine learning (ML) applications, unfair predictions may discriminate
against a minority group. Most existing approaches for fair machine learning
(FML) treat fairness as a constraint or a penalization term in the optimization
of a ML model, which does not lead to the discovery of the complete landscape
of the trade-offs among learning accuracy and fairness metrics, and does not
integrate fairness in a meaningful way.
Recently, we have introduced a new paradigm for FML based on Stochastic
Multi-Objective Optimization (SMOO), where accuracy and fairness metrics stand
as conflicting objectives to be optimized simultaneously. The entire trade-offs
range is defined as the Pareto front of the SMOO problem, which can then be
efficiently computed using stochastic-gradient type algorithms. SMOO also
allows defining and computing new meaningful predictors for FML, a novel one
being the Sharpe predictor that we introduce and explore in this paper, and
which gives the highest ratio of accuracy-to-unfairness. Inspired from SMOO in
finance, the Sharpe predictor for FML provides the highest prediction return
(accuracy) per unit of prediction risk (unfairness).

    

### [[2108.06422] Metadata-based Multi-Task Bandits with Bayesian Hierarchical Models](http://arxiv.org/abs/2108.06422)


  How to explore efficiently is a central problem in multi-armed bandits. In
this paper, we introduce the metadata-based multi-task bandit problem, where
the agent needs to solve a large number of related multi-armed bandit tasks and
can leverage some task-specific features (i.e., metadata) to share knowledge
across tasks. As a general framework, we propose to capture task relations
through the lens of Bayesian hierarchical models, upon which a Thompson
sampling algorithm is designed to efficiently learn task relations, share
information, and minimize the cumulative regrets. Two concrete examples for
Gaussian bandits and Bernoulli bandits are carefully analyzed. The Bayes regret
for Gaussian bandits clearly demonstrates the benefits of information sharing
with our algorithm. The proposed method is further supported by extensive
experiments.

    

### [[2108.06430] Hybrid Gaussian Process Modeling Applied to Economic Stochastic Model Predictive Control of Batch Processes](http://arxiv.org/abs/2108.06430)


  Nonlinear model predictive control (NMPC) is an efficient approach for the
control of nonlinear multivariable dynamic systems with constraints, which
however requires an accurate plant model. Plant models can often be determined
from first principles, parts of the model are however difficult to derive using
physical laws alone. In this paper a hybrid Gaussian process (GP) first
principles modeling scheme is proposed to overcome this issue, which exploits
GPs to model the parts of the dynamic system that are difficult to describe
using first principles. GPs not only give accurate predictions, but also
quantify the residual uncertainty of this model. It is vital to account for
this uncertainty in the control algorithm, to prevent constraint violations and
performance deterioration. Monte Carlo samples of the GPs are generated offline
to tighten constraints of the NMPC to ensure joint probabilistic constraint
satisfaction online. Advantages of our method include fast online evaluation
times, possibility to account for online learning alleviating conservativeness,
and exploiting the flexibility of GPs and the data efficiency of first
principle models. The algorithm is verified on a case study involving a
challenging semi-batch bioreactor.

    

### [[2108.06435] Focus on the Positives: Self-Supervised Learning for Biodiversity Monitoring](http://arxiv.org/abs/2108.06435)


  We address the problem of learning self-supervised representations from
unlabeled image collections. Unlike existing approaches that attempt to learn
useful features by maximizing similarity between augmented versions of each
input image or by speculatively picking negative samples, we instead also make
use of the natural variation that occurs in image collections that are captured
using static monitoring cameras. To achieve this, we exploit readily available
context data that encodes information such as the spatial and temporal
relationships between the input images. We are able to learn representations
that are surprisingly effective for downstream supervised classification, by
first identifying high probability positive pairs at training time, i.e. those
images that are likely to depict the same visual concept. For the critical task
of global biodiversity monitoring, this results in image features that can be
adapted to challenging visual species classification tasks with limited human
supervision. We present results on four different camera trap image
collections, across three different families of self-supervised learning
methods, and show that careful image selection at training time results in
superior performance compared to existing baselines such as conventional
self-supervised training and transfer learning.

    

### [[2108.06452] AdaGNN: A multi-modal latent representation meta-learner for GNNs based on AdaBoosting](http://arxiv.org/abs/2108.06452)


  As a special field in deep learning, Graph Neural Networks (GNNs) focus on
extracting intrinsic network features and have drawn unprecedented popularity
in both academia and industry. Most of the state-of-the-art GNN models offer
expressive, robust, scalable and inductive solutions empowering social network
recommender systems with rich network features that are computationally
difficult to leverage with graph traversal based methods.
Most recent GNNs follow an encoder-decoder paradigm to encode high
dimensional heterogeneous information from a subgraph onto one low dimensional
embedding space. However, one single embedding space usually fails to capture
all aspects of graph signals. In this work, we propose boosting-based meta
learner for GNNs, which automatically learns multiple projections and the
corresponding embedding spaces that captures different aspects of the graph
signals. As a result, similarities between sub-graphs are quantified by
embedding proximity on multiple embedding spaces. AdaGNN performs exceptionally
well for applications with rich and diverse node neighborhood information.
Moreover, AdaGNN is compatible with any inductive GNNs for both node-level and
edge-level tasks.

    

### [[2108.06453] Efficient Federated Meta-Learning over Multi-Access Wireless Networks](http://arxiv.org/abs/2108.06453)


  Federated meta-learning (FML) has emerged as a promising paradigm to cope
with the data limitation and heterogeneity challenges in today's edge learning
arena. However, its performance is often limited by slow convergence and
corresponding low communication efficiency. Besides, since the wireless
bandwidth and IoT devices' energy capacity are usually insufficient, it is
crucial to control the resource allocation and energy consumption when
deploying FML in realistic wireless networks. To overcome these challenges, in
this paper, we first rigorously analyze each device's contribution to the
global loss reduction in each round and develop an FML algorithm (called NUFM)
with a non-uniform device selection scheme to accelerate the convergence. After
that, we formulate a resource allocation problem integrating NUFM in
multi-access wireless systems to jointly improve the convergence rate and
minimize the wall-clock time along with energy cost. By deconstructing the
original problem step by step, we devise a joint device selection and resource
allocation strategy (called URAL) to solve the problem and provide theoretical
guarantees. Further, we show that the computational complexity of NUFM can be
reduced from $O(d^2)$ to $O(d)$ (with $d$ being the model dimension) via
combining two first-order approximation techniques. Extensive simulation
results demonstrate the effectiveness and superiority of the proposed methods
by comparing with the existing baselines.

    

### [[2108.06460] High-dimensional Assisted Generative Model for Color Image Restoration](http://arxiv.org/abs/2108.06460)


  This work presents an unsupervised deep learning scheme that exploiting
high-dimensional assisted score-based generative model for color image
restoration tasks. Considering that the sample number and internal dimension in
score-based generative model have key influence on estimating the gradients of
data distribution, two different high-dimensional ways are proposed: The
channel-copy transformation increases the sample number and the pixel-scale
transformation decreases feasible space dimension. Subsequently, a set of
high-dimensional tensors represented by these transformations are used to train
the network through denoising score matching. Then, sampling is performed by
annealing Langevin dynamics and alternative data-consistency update.
Furthermore, to alleviate the difficulty of learning high-dimensional
representation, a progressive strategy is proposed to leverage the performance.
The proposed unsupervised learning and iterative restoration algo-rithm, which
involves a pre-trained generative network to obtain prior, has transparent and
clear interpretation compared to other data-driven approaches. Experimental
results on demosaicking and inpainting conveyed the remarkable performance and
diversity of our proposed method.

    

### [[2108.06467] Optimal Approximation with Sparse Neural Networks and Applications](http://arxiv.org/abs/2108.06467)


  We use deep sparsely connected neural networks to measure the complexity of a
function class in $L^2(\mathbb R^d)$ by restricting connectivity and memory
requirement for storing the neural networks. We also introduce representation
system - a countable collection of functions to guide neural networks, since
approximation theory with representation system has been well developed in
Mathematics. We then prove the fundamental bound theorem, implying a quantity
intrinsic to the function class itself can give information about the
approximation ability of neural networks and representation system. We also
provides a method for transferring existing theories about approximation by
representation systems to that of neural networks, greatly amplifying the
practical values of neural networks. Finally, we use neural networks to
approximate B-spline functions, which are used to generate the B-spline curves.
Then, we analyse the complexity of a class called $\beta$ cartoon-like
functions using rate-distortion theory and wedgelets construction.

    

### [[2108.06487] Investigating Bias In Automatic Toxic Comment Detection: An Empirical Study](http://arxiv.org/abs/2108.06487)


  With surge in online platforms, there has been an upsurge in the user
engagement on these platforms via comments and reactions. A large portion of
such textual comments are abusive, rude and offensive to the audience. With
machine learning systems in-place to check such comments coming onto platform,
biases present in the training data gets passed onto the classifier leading to
discrimination against a set of classes, religion and gender. In this work, we
evaluate different classifiers and feature to estimate the bias in these
classifiers along with their performance on downstream task of toxicity
classification. Results show that improvement in performance of automatic toxic
comment detection models is positively correlated to mitigating biases in these
models. In our work, LSTM with attention mechanism proved to be a better
modelling strategy than a CNN model. Further analysis shows that fasttext
embeddings is marginally preferable than glove embeddings on training models
for toxicity comment detection. Deeper analysis reveals the findings that such
automatic models are particularly biased to specific identity groups even
though the model has a high AUC score. Finally, in effort to mitigate bias in
toxicity detection models, a multi-task setup trained with auxiliary task of
toxicity sub-types proved to be useful leading to upto 0.26% (6% relative) gain
in AUC scores.

    

### [[2108.06491] DQN Control Solution for KDD Cup 2021 City Brain Challenge](http://arxiv.org/abs/2108.06491)


  We took part in the city brain challenge competition and achieved the 8th
place. In this competition, the players are provided with a real-world
city-scale road network and its traffic demand derived from real traffic data.
The players are asked to coordinate the traffic signals with a self-designed
agent to maximize the number of vehicles served while maintaining an acceptable
delay. In this abstract paper, we present an overall analysis and our detailed
solution to this competition. Our approach is mainly based on the adaptation of
the deep Q-network (DQN) for real-time traffic signal control. From our
perspective, the major challenge of this competition is how to extend the
classical DQN framework to traffic signals control in real-world complex road
network and traffic flow situation. After trying and implementing several
classical reward functions, we finally chose to apply our newly-designed reward
in our agent. By applying our newly-proposed reward function and carefully
tuning the control scheme, an agent based on a single DQN model can rank among
the top 15 teams. We hope this paper could serve, to some extent, as a baseline
solution to traffic signal control of real-world road network and inspire
further attempts and researches.

    

### [[2108.06492] Collaborative Unsupervised Visual Representation Learning from Decentralized Data](http://arxiv.org/abs/2108.06492)


  Unsupervised representation learning has achieved outstanding performances
using centralized data available on the Internet. However, the increasing
awareness of privacy protection limits sharing of decentralized unlabeled image
data that grows explosively in multiple parties (e.g., mobile phones and
cameras). As such, a natural problem is how to leverage these data to learn
visual representations for downstream tasks while preserving data privacy. To
address this problem, we propose a novel federated unsupervised learning
framework, FedU. In this framework, each party trains models from unlabeled
data independently using contrastive learning with an online network and a
target network. Then, a central server aggregates trained models and updates
clients' models with the aggregated model. It preserves data privacy as each
party only has access to its raw data. Decentralized data among multiple
parties are normally non-independent and identically distributed (non-IID),
leading to performance degradation. To tackle this challenge, we propose two
simple but effective methods: 1) We design the communication protocol to upload
only the encoders of online networks for server aggregation and update them
with the aggregated encoder; 2) We introduce a new module to dynamically decide
how to update predictors based on the divergence caused by non-IID. The
predictor is the other component of the online network. Extensive experiments
and ablations demonstrate the effectiveness and significance of FedU. It
outperforms training with only one party by over 5% and other methods by over
14% in linear and semi-supervised evaluation on non-IID data.

    

### [[2108.06493] Joint Optimization in Edge-Cloud Continuum for Federated Unsupervised Person Re-identification](http://arxiv.org/abs/2108.06493)


  Person re-identification (ReID) aims to re-identify a person from
non-overlapping camera views. Since person ReID data contains sensitive
personal information, researchers have adopted federated learning, an emerging
distributed training method, to mitigate the privacy leakage risks. However,
existing studies rely on data labels that are laborious and time-consuming to
obtain. We present FedUReID, a federated unsupervised person ReID system to
learn person ReID models without any labels while preserving privacy. FedUReID
enables in-situ model training on edges with unlabeled data. A cloud server
aggregates models from edges instead of centralizing raw data to preserve data
privacy. Moreover, to tackle the problem that edges vary in data volumes and
distributions, we personalize training in edges with joint optimization of
cloud and edge. Specifically, we propose personalized epoch to reassign
computation throughout training, personalized clustering to iteratively predict
suitable labels for unlabeled data, and personalized update to adapt the server
aggregated model to each edge. Extensive experiments on eight person ReID
datasets demonstrate that FedUReID not only achieves higher accuracy but also
reduces computation cost by 29%. Our FedUReID system with the joint
optimization will shed light on implementing federated learning to more
multimedia tasks without data labels.

    

### [[2108.06504] LinkTeller: Recovering Private Edges from Graph Neural Networks via Influence Analysis](http://arxiv.org/abs/2108.06504)


  Graph structured data have enabled several successful applications such as
recommendation systems and traffic prediction, given the rich node features and
edges information. However, these high-dimensional features and high-order
adjacency information are usually heterogeneous and held by different data
holders in practice. Given such vertical data partition (e.g., one data holder
will only own either the node features or edge information), different data
holders have to develop efficient joint training protocols rather than directly
transfer data to each other due to privacy concerns. In this paper, we focus on
the edge privacy, and consider a training scenario where Bob with node features
will first send training node features to Alice who owns the adjacency
information. Alice will then train a graph neural network (GNN) with the joint
information and release an inference API. During inference, Bob is able to
provide test node features and query the API to obtain the predictions for test
nodes. Under this setting, we first propose a privacy attack LinkTeller via
influence analysis to infer the private edge information held by Alice via
designing adversarial queries for Bob. We then empirically show that LinkTeller
is able to recover a significant amount of private edges, outperforming
existing baselines. To further evaluate the privacy leakage, we adapt an
existing algorithm for differentially private graph convolutional network (DP
GCN) training and propose a new DP GCN mechanism LapGraph. We show that these
DP GCN mechanisms are not always resilient against LinkTeller empirically under
mild privacy guarantees ($\varepsilon>5$). Our studies will shed light on
future research towards designing more resilient privacy-preserving GCN models;
in the meantime, provide an in-depth understanding of the tradeoff between GCN
model utility and robustness against potential privacy attacks.

    

### [[2108.06514] Active Assessment of Prediction Services as Accuracy Surface Over Attribute Combinations](http://arxiv.org/abs/2108.06514)


  Our goal is to evaluate the accuracy of a black-box classification model, not
as a single aggregate on a given test data distribution, but as a surface over
a large number of combinations of attributes characterizing multiple test data
distributions. Such attributed accuracy measures become important as machine
learning models get deployed as a service, where the training data distribution
is hidden from clients, and different clients may be interested in diverse
regions of the data distribution. We present Attributed Accuracy Assay (AAA)--a
Gaussian Process (GP)--based probabilistic estimator for such an accuracy
surface. Each attribute combination, called an 'arm', is associated with a Beta
density from which the service's accuracy is sampled. We expect the GP to
smooth the parameters of the Beta density over related arms to mitigate
sparsity. We show that obvious application of GPs cannot address the challenge
of heteroscedastic uncertainty over a huge attribute space that is sparsely and
unevenly populated. In response, we present two enhancements: pooling sparse
observations, and regularizing the scale parameter of the Beta densities. After
introducing these innovations, we establish the effectiveness of AAA in terms
of both its estimation accuracy and exploration efficiency, through extensive
experiments and analysis.

    

### [[2108.06526] Fractional Transfer Learning for Deep Model-Based Reinforcement Learning](http://arxiv.org/abs/2108.06526)


  Reinforcement learning (RL) is well known for requiring large amounts of data
in order for RL agents to learn to perform complex tasks. Recent progress in
model-based RL allows agents to be much more data-efficient, as it enables them
to learn behaviors of visual environments in imagination by leveraging an
internal World Model of the environment. Improved sample efficiency can also be
achieved by reusing knowledge from previously learned tasks, but transfer
learning is still a challenging topic in RL. Parameter-based transfer learning
is generally done using an all-or-nothing approach, where the network's
parameters are either fully transferred or randomly initialized. In this work
we present a simple alternative approach: fractional transfer learning. The
idea is to transfer fractions of knowledge, opposed to discarding potentially
useful knowledge as is commonly done with random initialization. Using the
World Model-based Dreamer algorithm, we identify which type of components this
approach is applicable to, and perform experiments in a new multi-source
transfer learning setting. The results show that fractional transfer learning
often leads to substantially improved performance and faster learning compared
to learning from scratch and random initialization.

    

### [[2108.06530] Neuron Campaign for Initialization Guided by Information Bottleneck Theory](http://arxiv.org/abs/2108.06530)


  Initialization plays a critical role in the training of deep neural networks
(DNN). Existing initialization strategies mainly focus on stabilizing the
training process to mitigate gradient vanish/explosion problems. However, these
initialization methods are lacking in consideration about how to enhance
generalization ability. The Information Bottleneck (IB) theory is a well-known
understanding framework to provide an explanation about the generalization of
DNN. Guided by the insights provided by IB theory, we design two criteria for
better initializing DNN. And we further design a neuron campaign initialization
algorithm to efficiently select a good initialization for a neural network on a
given dataset. The experiments on MNIST dataset show that our method can lead
to a better generalization performance with faster convergence.

    

### [[2108.06552] Weakly Supervised Continual Learning](http://arxiv.org/abs/2108.06552)


  Continual Learning (CL) investigates how to train Deep Networks on a stream
of tasks without incurring catastrophic forgetting. CL settings proposed in the
literature assume that every incoming example is paired with ground-truth
annotations. However, this clashes with many real-world applications: gathering
labeled data, which is in itself tedious and expensive, becomes indeed
infeasible when data flow as a stream and must be consumed in real-time. This
work explores Weakly Supervised Continual Learning (WSCL): here, only a small
fraction of labeled input examples are shown to the learner. We assess how
current CL methods (e.g.: EWC, LwF, iCaRL, ER, GDumb, DER) perform in this
novel and challenging scenario, in which overfitting entangles forgetting.
Subsequently, we design two novel WSCL methods which exploit metric learning
and consistency regularization to leverage unsupervised data while learning. In
doing so, we show that not only our proposals exhibit higher flexibility when
supervised information is scarce, but also that less than 25% labels can be
enough to reach or even outperform SOTA methods trained under full supervision.

    

### [[2108.06554] Stacked Hourglass Network with a Multi-level Attention Mechanism: Where to Look for Intervertebral Disc Labeling](http://arxiv.org/abs/2108.06554)


  Labeling vertebral discs from MRI scans is important for the proper diagnosis
of spinal related diseases, including multiple sclerosis, amyotrophic lateral
sclerosis, degenerative cervical myelopathy and cancer. Automatic labeling of
the vertebral discs in MRI data is a difficult task because of the similarity
between discs and bone area, the variability in the geometry of the spine and
surrounding tissues across individuals, and the variability across scans
(manufacturers, pulse sequence, image contrast, resolution and artefacts). In
previous studies, vertebral disc labeling is often done after a disc detection
step and mostly fails when the localization algorithm misses discs or has false
positive detection. In this work, we aim to mitigate this problem by
reformulating the semantic vertebral disc labeling using the pose estimation
technique. To do so, we propose a stacked hourglass network with multi-level
attention mechanism to jointly learn intervertebral disc position and their
skeleton structure. The proposed deep learning model takes into account the
strength of semantic segmentation and pose estimation technique to handle the
missing area and false positive detection. To further improve the performance
of the proposed method, we propose a skeleton-based search space to reduce
false positive detection. The proposed method evaluated on spine generic public
multi-center dataset and demonstrated better performance comparing to previous
work, on both T1w and T2w contrasts. The method is implemented in ivadomed
(this https URL).

    

### [[2108.06558] The Neural Network shifted-Proper Orthogonal Decomposition: a Machine Learning Approach for Non-linear Reduction of Hyperbolic Equations](http://arxiv.org/abs/2108.06558)


  Models with dominant advection always posed a difficult challenge for
projection-based reduced order modelling. Many methodologies that have recently
been proposed are based on the pre-processing of the full-order solutions to
accelerate the Kolmogorov N-width decay thereby obtaining smaller linear
subspaces with improved accuracy. These methods however must rely on the
knowledge of the characteristic speeds in phase space of the solution, limiting
their range of applicability to problems with explicit functional form for the
advection field. In this work we approach the problem of automatically
detecting the correct pre-processing transformation in a statistical learning
framework by implementing a deep-learning architecture. The purely data-driven
method allowed us to generalise the existing approaches of linear subspace
manipulation to non-linear hyperbolic problems with unknown advection fields.
The proposed algorithm has been validated against simple test cases to
benchmark its performances and later successfully applied to a multiphase
simulation.

    

### [[2108.06589] A Microscopic Pandemic Simulator for Pandemic Prediction Using Scalable Million-Agent Reinforcement Learning](http://arxiv.org/abs/2108.06589)


  Microscopic epidemic models are powerful tools for government policy makers
to predict and simulate epidemic outbreaks, which can capture the impact of
individual behaviors on the macroscopic phenomenon. However, existing models
only consider simple rule-based individual behaviors, limiting their
applicability. This paper proposes a deep-reinforcement-learning-powered
microscopic model named Microscopic Pandemic Simulator (MPS). By replacing
rule-based agents with rational agents whose behaviors are driven to maximize
rewards, the MPS provides a better approximation of real world dynamics. To
efficiently simulate with massive amounts of agents in MPS, we propose Scalable
Million-Agent DQN (SMADQN). The MPS allows us to efficiently evaluate the
impact of different government strategies. This paper first calibrates the MPS
against real-world data in Allegheny, US, then demonstratively evaluates two
government strategies: information disclosure and quarantine. The results
validate the effectiveness of the proposed method. As a broad impact, this
paper provides novel insights for the application of DRL in large scale
agent-based networks such as economic and social networks.

    

### [[2108.06594] Offline-Online Reinforcement Learning for Energy Pricing in Office Demand Response: Lowering Energy and Data Costs](http://arxiv.org/abs/2108.06594)


  Our team is proposing to run a full-scale energy demand response experiment
in an office building. Although this is an exciting endeavor which will provide
value to the community, collecting training data for the reinforcement learning
agent is costly and will be limited. In this work, we examine how offline
training can be leveraged to minimize data costs (accelerate convergence) and
program implementation costs. We present two approaches to doing so:
pretraining our model to warm start the experiment with simulated tasks, and
using a planning model trained to simulate the real world's rewards to the
agent. We present results that demonstrate the utility of offline reinforcement
learning to efficient price-setting in the energy demand response problem.

    

### [[2108.06613] Unsupervised Disentanglement without Autoencoding: Pitfalls and Future Directions](http://arxiv.org/abs/2108.06613)


  Disentangled visual representations have largely been studied with generative
models such as Variational AutoEncoders (VAEs). While prior work has focused on
generative methods for disentangled representation learning, these approaches
do not scale to large datasets due to current limitations of generative models.
Instead, we explore regularization methods with contrastive learning, which
could result in disentangled representations that are powerful enough for large
scale datasets and downstream applications. However, we find that unsupervised
disentanglement is difficult to achieve due to optimization and initialization
sensitivity, with trade-offs in task performance. We evaluate disentanglement
with downstream tasks, analyze the benefits and disadvantages of each
regularization used, and discuss future directions.

    

### [[2108.06618] Adaptive Selection of Informative Path Planning Strategies via Reinforcement Learning](http://arxiv.org/abs/2108.06618)


  In our previous work, we designed a systematic policy to prioritize sampling
locations to lead significant accuracy improvement in spatial interpolation by
using the prediction uncertainty of Gaussian Process Regression (GPR) as
"attraction force" to deployed robots in path planning. Although the
integration with Traveling Salesman Problem (TSP) solvers was also shown to
produce relatively short travel distance, we here hypothesise several factors
that could decrease the overall prediction precision as well because
sub-optimal locations may eventually be included in their paths. To address
this issue, in this paper, we first explore "local planning" approaches
adopting various spatial ranges within which next sampling locations are
prioritized to investigate their effects on the prediction performance as well
as incurred travel distance. Also, Reinforcement Learning (RL)-based high-level
controllers are trained to adaptively produce blended plans from a particular
set of local planners to inherit unique strengths from that selection depending
on latest prediction states. Our experiments on use cases of temperature
monitoring robots demonstrate that the dynamic mixtures of planners can not
only generate sophisticated, informative plans that a single planner could not
create alone but also ensure significantly reduced travel distances at no cost
of prediction reliability without any assist of additional modules for shortest
path calculation.

    

### [[2108.06624] Equity-Directed Bootstrapping: Examples and Analysis](http://arxiv.org/abs/2108.06624)


  When faced with severely imbalanced binary classification problems, we often
train models on bootstrapped data in which the number of instances of each
class occur in a more favorable ratio, e.g., one. We view algorithmic inequity
through the lens of imbalanced classification: in order to balance the
performance of a classifier across groups, we can bootstrap to achieve training
sets that are balanced with respect to both labels and group identity. For an
example problem with severe class imbalance---prediction of suicide death from
administrative patient records---we illustrate how an equity-directed bootstrap
can bring test set sensitivities and specificities much closer to satisfying
the equal odds criterion. In the context of naïve Bayes and logistic
regression, we analyze the equity-directed bootstrap, demonstrating that it
works by bringing odds ratios close to one, and linking it to methods involving
intercept adjustment, thresholding, and weighting.

    

### [[2108.06625] Continuous-Time Sequential Recommendation with Temporal Graph Collaborative Transformer](http://arxiv.org/abs/2108.06625)


  In order to model the evolution of user preference, we should learn user/item
embeddings based on time-ordered item purchasing sequences, which is defined as
Sequential Recommendation (SR) problem. Existing methods leverage sequential
patterns to model item transitions. However, most of them ignore crucial
temporal collaborative signals, which are latent in evolving user-item
interactions and coexist with sequential patterns. Therefore, we propose to
unify sequential patterns and temporal collaborative signals to improve the
quality of recommendation, which is rather challenging. Firstly, it is hard to
simultaneously encode sequential patterns and collaborative signals. Secondly,
it is non-trivial to express the temporal effects of collaborative signals.
Hence, we design a new framework Temporal Graph Sequential Recommender
(TGSRec) upon our defined continuous-time bi-partite graph. We propose a novel
Temporal Collaborative Trans-former (TCT) layer in TGSRec, which advances the
self-attention mechanism by adopting a novel collaborative attention. TCT layer
can simultaneously capture collaborative signals from both users and items, as
well as considering temporal dynamics inside sequential patterns. We propagate
the information learned fromTCTlayerover the temporal graph to unify sequential
patterns and temporal collaborative signals. Empirical results on five datasets
show that TGSRec significantly outperforms other baselines, in average up to
22.5% and 22.1%absolute improvements in Recall@10and MRR, respectively.

    

### [[2108.06626] A Survey on GAN Acceleration Using Memory Compression Technique](http://arxiv.org/abs/2108.06626)


  Since its invention, Generative adversarial networks (GANs) have shown
outstanding results in many applications. Generative Adversarial Networks are
powerful yet, resource-hungry deep-learning models. Their main difference from
ordinary deep learning models is the nature of their output. For example, GAN
output can be a whole image versus other models detecting objects or
classifying images. Thus, the architecture and numeric precision of the network
affect the quality and speed of the solution. Hence, accelerating GANs is
pivotal. Accelerating GANs can be classified into three main tracks: (1) Memory
compression, (2) Computation optimization, and (3) Data-flow optimization.
Because data transfer is the main source of energy usage, memory compression
leads to the most savings. Thus, in this paper, we survey memory compression
techniques for CNN-Based GANs. Additionally, the paper summarizes opportunities
and challenges in GANs acceleration and suggests open research problems to be
further investigated.

    

### [[2108.06628] Investigating the Relationship Between Dropout Regularization and Model Complexity in Neural Networks](http://arxiv.org/abs/2108.06628)


  Dropout Regularization, serving to reduce variance, is nearly ubiquitous in
Deep Learning models. We explore the relationship between the dropout rate and
model complexity by training 2,000 neural networks configured with random
combinations of the dropout rate and the number of hidden units in each dense
layer, on each of the three data sets we selected. The generated figures, with
binary cross entropy loss and binary accuracy on the z-axis, question the
common assumption that adding depth to a dense layer while increasing the
dropout rate will certainly enhance performance. We also discover a complex
correlation between the two hyperparameters that we proceed to quantify by
building additional machine learning and Deep Learning models which predict the
optimal dropout rate given some hidden units in each dense layer. Linear
regression and polynomial logistic regression require the use of arbitrary
thresholds to select the cost data points included in the regression and to
assign the cost data points a binary classification, respectively. These
machine learning models have mediocre performance because their naive nature
prevented the modeling of complex decision boundaries. Turning to Deep Learning
models, we build neural networks that predict the optimal dropout rate given
the number of hidden units in each dense layer, the desired cost, and the
desired accuracy of the model. Though, this attempt encounters a mathematical
error that can be attributed to the failure of the vertical line test. The
ultimate Deep Learning model is a neural network whose decision boundary
represents the 2,000 previously generated data points. This final model leads
us to devise a promising method for tuning hyperparameters to minimize
computational expense yet maximize performance. The strategy can be applied to
any model hyperparameters, with the prospect of more efficient tuning in
industrial models.

    

### [[2108.06629] LayerPipe: Accelerating Deep Neural Network Training by Intra-Layer and Inter-Layer Gradient Pipelining and Multiprocessor Scheduling](http://arxiv.org/abs/2108.06629)


  The time required for training the neural networks increases with size,
complexity, and depth. Training model parameters by backpropagation inherently
creates feedback loops. These loops hinder efficient pipelining and scheduling
of the tasks within the layer and between consecutive layers. Prior approaches,
such as PipeDream, have exploited the use of delayed gradient to achieve
inter-layer pipelining. However, these approaches treat the entire
backpropagation as a single task; this leads to an increase in computation time
and processor underutilization. This paper presents novel optimization
approaches where the gradient computations with respect to the weights and the
activation functions are considered independently; therefore, these can be
computed in parallel. This is referred to as intra-layer optimization.
Additionally, the gradient computation with respect to the activation function
is further divided into two parts and distributed to two consecutive layers.
This leads to balanced scheduling where the computation time of each layer is
the same. This is referred to as inter-layer optimization. The proposed system,
referred to as LayerPipe, reduces the number of clock cycles required for
training while maximizing processor utilization with minimal inter-processor
communication overhead. LayerPipe achieves an average speedup of 25% and
upwards of 80% with 7 to 9 processors with less communication overhead when
compared to PipeDream.

    

### [[2108.06633] DEXTER: Deep Encoding of External Knowledge for Named Entity Recognition in Virtual Assistants](http://arxiv.org/abs/2108.06633)


  Named entity recognition (NER) is usually developed and tested on text from
well-written sources. However, in intelligent voice assistants, where NER is an
important component, input to NER may be noisy because of user or speech
recognition error. In applications, entity labels may change frequently, and
non-textual properties like topicality or popularity may be needed to choose
among alternatives.
We describe a NER system intended to address these problems. We test and
train this system on a proprietary user-derived dataset. We compare with a
baseline text-only NER system; the baseline enhanced with external gazetteers;
and the baseline enhanced with the search and indirect labelling techniques we
describe below. The final configuration gives around 6% reduction in NER error
rate. We also show that this technique improves related tasks, such as semantic
parsing, with an improvement of up to 5% in error rate.

    

### [[2108.06643] SAPPHIRE: Approaches for Enhanced Concept-to-Text Generation](http://arxiv.org/abs/2108.06643)


  We motivate and propose a suite of simple but effective improvements for
concept-to-text generation called SAPPHIRE: Set Augmentation and Post-hoc
PHrase Infilling and REcombination. We demonstrate their effectiveness on
generative commonsense reasoning, a.k.a. the CommonGen task, through
experiments using both BART and T5 models. Through extensive automatic and
human evaluation, we show that SAPPHIRE noticeably improves model performance.
An in-depth qualitative analysis illustrates that SAPPHIRE effectively
addresses many issues of the baseline model generations, including lack of
commonsense, insufficient specificity, and poor fluency.

    

### [[2108.06645] On Multi-Modal Learning of Editing Source Code](http://arxiv.org/abs/2108.06645)


  In recent years, Neural Machine Translator (NMT) has shown promise in
automatically editing source code. Typical NMT based code editor only considers
the code that needs to be changed as input and suggests developers with a
ranked list of patched code to choose from - where the correct one may not
always be at the top of the list. While NMT based code editing systems generate
a broad spectrum of plausible patches, the correct one depends on the
developers' requirement and often on the context where the patch is applied.
Thus, if developers provide some hints, using natural language, or providing
patch context, NMT models can benefit from them. As a proof of concept, in this
research, we leverage three modalities of information: edit location, edit code
context, commit messages (as a proxy of developers' hint in natural language)
to automatically generate edits with NMT models. To that end, we build MODIT, a
multi-modal NMT based code editing engine. With in-depth investigation and
analysis, we show that developers' hint as an input modality can narrow the
search space for patches and outperform state-of-the-art models to generate
correctly patched code in top-1 position.

    

### [[2108.06651] Topology-Guided Sampling for Fast and Accurate Community Detection](http://arxiv.org/abs/2108.06651)


  Community detection is a well-studied problem with applications in domains
ranging from computer networking to bioinformatics. While there are many
algorithms that perform community detection, the more accurate and
statistically robust algorithms tend to be slow and hard to parallelize. One
way to speed up such algorithms is through data reduction. However, this
approach has not been thoroughly studied, and the quality of results obtained
with this approach varies with the graph it is applied to. In this manuscript,
we present an approach based on topology-guided sampling for accelerating
stochastic block partitioning - a community detection algorithm that works well
on graphs with complex and heterogeneous community structure. We also introduce
a degree-based thresholding scheme that improves the efficacy of our approach
at the expense of speedup. Finally, we perform a series of experiments on
synthetically generated graphs to determine how various graph parameters affect
the quality of results and speedup obtained with our approach, and we validate
our approach on real-world data. Our results show that our approach can lead to
a speedup of up to 15X over stochastic block partitioning without sampling
while maintaining result quality and can even lead to improvements of over 150%
in result quality in terms of F1 score on certain kinds of graphs.

    

### [[2108.06655] Policy Evaluation and Temporal-Difference Learning in Continuous Time and Space: A Martingale Approach](http://arxiv.org/abs/2108.06655)


  We propose a unified framework to study policy evaluation (PE) and the
associated temporal difference (TD) methods for reinforcement learning in
continuous time and space. We show that PE is equivalent to maintaining the
martingale condition of a process. From this perspective, we find that the
mean--square TD error approximates the quadratic variation of the martingale
and thus is not a suitable objective for PE. We present two methods to use the
martingale characterization for designing PE algorithms. The first one
minimizes a "martingale loss function", whose solution is proved to be the best
approximation of the true value function in the mean--square sense. This method
interprets the classical gradient Monte-Carlo algorithm. The second method is
based on a system of equations called the "martingale orthogonality conditions"
with "test functions". Solving these equations in different ways recovers
various classical TD algorithms, such as TD($\lambda$), LSTD, and GTD.
Different choices of test functions determine in what sense the resulting
solutions approximate the true value function. Moreover, we prove that any
convergent time-discretized algorithm converges to its continuous-time
counterpart as the mesh size goes to zero. We demonstrate the theoretical
results and corresponding algorithms with numerical experiments and
applications.

    

### [[2108.06663] HCR-Net: A deep learning based script independent handwritten character recognition network](http://arxiv.org/abs/2108.06663)


  Handwritten character recognition (HCR) is a challenging learning problem in
pattern recognition, mainly due to similarity in structure of characters,
different handwriting styles, noisy datasets and a large variety of languages
and scripts. HCR problem is studied extensively for a few decades but there is
very limited research on script independent models. This is because of factors,
like, diversity of scripts, focus of the most of conventional research efforts
on handcrafted feature extraction techniques which are language/script specific
and are not always available, and unavailability of public datasets and codes
to reproduce the results. On the other hand, deep learning has witnessed huge
success in different areas of pattern recognition, including HCR, and provides
end-to-end learning, i.e., automated feature extraction and recognition. In
this paper, we have proposed a novel deep learning architecture which exploits
transfer learning and image-augmentation for end-to-end learning for script
independent handwritten character recognition, called HCR-Net. The network is
based on a novel transfer learning approach for HCR, where some of lower layers
of a pre-trained VGG16 network are utilised. Due to transfer learning and
image-augmentation, HCR-Net provides faster training, better performance and
better generalisations. The experimental results on publicly available datasets
of Bangla, Punjabi, Hindi, English, Swedish, Urdu, Farsi, Tibetan, Kannada,
Malayalam, Telugu, Marathi, Nepali and Arabic languages prove the efficacy of
HCR-Net and establishes several new benchmarks. For reproducibility of the
results and for the advancements of the HCR research, complete code is publicly
released at \href{this https URL}{GitHub}.

    

### [[2108.06670] Deep Geospatial Interpolation Networks](http://arxiv.org/abs/2108.06670)


  Interpolation in Spatio-temporal data has applications in various domains
such as climate, transportation, and mining. Spatio-Temporal interpolation is
highly challenging due to the complex spatial and temporal relationships.
However, traditional techniques such as Kriging suffer from high running time
and poor performance on data that exhibit high variance across space and time
dimensions. To this end, we propose a novel deep neural network called as Deep
Geospatial Interpolation Network(DGIN), which incorporates both spatial and
temporal relationships and has significantly lower training time. DGIN consists
of three major components: Spatial Encoder to capture the spatial dependencies,
Sequential module to incorporate the temporal dynamics, and an Attention block
to learn the importance of the temporal neighborhood around the gap. We
evaluate DGIN on the MODIS reflectance dataset from two different regions. Our
experimental results indicate that DGIN has two advantages: (a) it outperforms
alternative approaches (has lower MSE with p-value < 0.01) and, (b) it has
significantly low execution time than Kriging.

    

### [[2108.06702] Deepfake Representation with Multilinear Regression](http://arxiv.org/abs/2108.06702)


  Generative neural network architectures such as GANs, may be used to generate
synthetic instances to compensate for the lack of real data. However, they may
be employed to create media that may cause social, political or economical
upheaval. One emerging media is "Deepfake".Techniques that can discriminate
between such media is indispensable. In this paper, we propose a modified
multilinear (tensor) method, a combination of linear and multilinear
regressions for representing fake and real data. We test our approach by
representing Deepfakes with our modified multilinear (tensor) approach and
perform SVM classification with encouraging results.

    

### [[2108.06711] Towards Understanding Theoretical Advantages of Complex-Reaction Networks](http://arxiv.org/abs/2108.06711)


  Complex-valued neural networks have attracted increasing attention in recent
years, while it remains open on the advantages of complex-valued neural
networks in comparison with real-valued networks. This work takes one step on
this direction by introducing the \emph{complex-reaction network} with
fully-connected feed-forward architecture. We prove the universal approximation
property for complex-reaction networks, and show that a class of radial
functions can be approximated by a complex-reaction network using the
polynomial number of parameters, whereas real-valued networks need at least
exponential parameters to reach the same approximation level. For empirical
risk minimization, our theoretical result shows that the critical point set of
complex-reaction networks is a proper subset of that of real-valued networks,
which may show some insights on finding the optimal solutions more easily for
complex-reaction networks.

    

### [[2108.06717] Time Delay Estimation of Traffic Congestion Propagation based on Transfer Entropy](http://arxiv.org/abs/2108.06717)


  Considering how congestion will propagate in the near future, understanding
traffic congestion propagation has become crucial in GPS navigation systems for
providing users with a more accurate estimated time of arrival (ETA). However,
providing the exact ETA during congestion is a challenge owing to the complex
propagation process between roads and high uncertainty regarding the future
behavior of the process. Recent studies have focused on finding frequent
congestion propagation patterns and determining the propagation probabilities.
By contrast, this study proposes a novel time delay estimation method for
traffic congestion propagation between roads using lag-specific transfer
entropy (TE). Nonlinear normalization with a sliding window is used to
effectively reveal the causal relationship between the source and target time
series in calculating the TE. Moreover, Markov bootstrap techniques were
adopted to quantify the uncertainty in the time delay estimator. To the best of
our knowledge, the time delay estimation method presented in this article is
the first to determine the time delay between roads for any congestion
propagation pattern. The proposed method was validated using simulated data as
well as real user trajectory data obtained from a major GPS navigation system
applied in South Korea.

    

### [[2108.06721] Training for the Future: A Simple Gradient Interpolation Loss to Generalize Along Time](http://arxiv.org/abs/2108.06721)


  In several real world applications, machine learning models are deployed to
make predictions on data whose distribution changes gradually along time,
leading to a drift between the train and test distributions. Such models are
often re-trained on new data periodically, and they hence need to generalize to
data not too far into the future. In this context, there is much prior work on
enhancing temporal generalization, e.g. continuous transportation of past data,
kernel smoothed time-sensitive parameters and more recently, adversarial
learning of time-invariant features. However, these methods share several
limitations, e.g, poor scalability, training instability, and dependence on
unlabeled data from the future. Responding to the above limitations, we propose
a simple method that starts with a model with time-sensitive parameters but
regularizes its temporal complexity using a Gradient Interpolation (GI) loss.
GI allows the decision boundary to change along time and can still prevent
overfitting to the limited training time snapshots by allowing task-specific
control over changes along time. We compare our method to existing baselines on
multiple real-world datasets, which show that GI outperforms more complicated
generative and adversarial approaches on the one hand, and simpler gradient
regularization methods on the other.

    

### [[2108.06723] Self-supervised Contrastive Learning of Multi-view Facial Expressions](http://arxiv.org/abs/2108.06723)


  Facial expression recognition (FER) has emerged as an important component of
human-computer interaction systems. Despite recent advancements in FER,
performance often drops significantly for non-frontal facial images. We propose
Contrastive Learning of Multi-view facial Expressions (CL-MEx) to exploit
facial images captured simultaneously from different angles towards FER. CL-MEx
is a two-step training framework. In the first step, an encoder network is
pre-trained with the proposed self-supervised contrastive loss, where it learns
to generate view-invariant embeddings for different views of a subject. The
model is then fine-tuned with labeled data in a supervised setting. We
demonstrate the performance of the proposed method on two multi-view FER
datasets, KDEF and DDCF, where state-of-the-art performances are achieved.
Further experiments show the robustness of our method in dealing with
challenging angles and reduced amounts of labeled data.

    

### [[2108.06734] Effective and Efficient Graph Learning for Multi-view Clustering](http://arxiv.org/abs/2108.06734)


  Despite the impressive clustering performance and efficiency in
characterizing both the relationship between data and cluster structure,
existing graph-based multi-view clustering methods still have the following
drawbacks. They suffer from the expensive time burden due to both the
construction of graphs and eigen-decomposition of Laplacian matrix, and fail to
explore the cluster structure of large-scale data. Moreover, they require a
post-processing to get the final clustering, resulting in suboptimal
performance. Furthermore, rank of the learned view-consensus graph cannot
approximate the target rank. In this paper, drawing the inspiration from the
bipartite graph, we propose an effective and efficient graph learning model for
multi-view clustering. Specifically, our method exploits the view-similar
between graphs of different views by the minimization of tensor Schatten
p-norm, which well characterizes both the spatial structure and complementary
information embedded in graphs of different views. We learn view-consensus
graph with adaptively weighted strategy and connectivity constraint such that
the connected components indicates clusters directly. Our proposed algorithm is
time-economical and obtains the stable results and scales well with the data
size. Extensive experimental results indicate that our method is superior to
state-of-the-art methods.

    

### [[2108.06758] An Investigation of Replay-based Approaches for Continual Learning](http://arxiv.org/abs/2108.06758)


  Continual learning (CL) is a major challenge of machine learning (ML) and
describes the ability to learn several tasks sequentially without catastrophic
forgetting (CF). Recent works indicate that CL is a complex topic, even more so
when real-world scenarios with multiple constraints are involved. Several
solution classes have been proposed, of which so-called replay-based approaches
seem very promising due to their simplicity and robustness. Such approaches
store a subset of past samples in a dedicated memory for later processing:
while this does not solve all problems, good results have been obtained. In
this article, we empirically investigate replay-based approaches of continual
learning and assess their potential for applications. Selected recent
approaches as well as own proposals are compared on a common set of benchmarks,
with a particular focus on assessing the performance of different sample
selection strategies. We find that the impact of sample selection increases
when a smaller number of samples is stored. Nevertheless, performance varies
strongly between different replay approaches. Surprisingly, we find that the
most naive rehearsal-based approaches that we propose here can outperform
recent state-of-the-art methods.

    

### [[2108.06761] Multi-Slice Dense-Sparse Learning for Efficient Liver and Tumor Segmentation](http://arxiv.org/abs/2108.06761)


  Accurate automatic liver and tumor segmentation plays a vital role in
treatment planning and disease monitoring. Recently, deep convolutional neural
network (DCNNs) has obtained tremendous success in 2D and 3D medical image
segmentation. However, 2D DCNNs cannot fully leverage the inter-slice
information, while 3D DCNNs are computationally expensive and memory intensive.
To address these issues, we first propose a novel dense-sparse training flow
from a data perspective, in which, densely adjacent slices and sparsely
adjacent slices are extracted as inputs for regularizing DCNNs, thereby
improving the model performance. Moreover, we design a 2.5D light-weight
nnU-Net from a network perspective, in which, depthwise separable convolutions
are adopted to improve the efficiency. Extensive experiments on the LiTS
dataset have demonstrated the superiority of the proposed method.

    

### [[2108.06764] Optimal Scheduling of Isolated Microgrids Using Automated Reinforcement Learning-based Multi-period Forecasting](http://arxiv.org/abs/2108.06764)


  In order to reduce the negative impact of the uncertainty of load and
renewable energies outputs on microgrid operation, an optimal scheduling model
is proposed for isolated microgrids by using automated reinforcement
learning-based multi-period forecasting of renewable power generations and
loads. Firstly, a prioritized experience replay automated reinforcement
learning (PER-AutoRL) is designed to simplify the deployment of deep
reinforcement learning (DRL)-based forecasting model in a customized manner,
the single-step multi-period forecasting method based on PER-AutoRL is proposed
for the first time to address the error accumulation issue suffered by existing
multi-step forecasting methods, then the prediction values obtained by the
proposed forecasting method are revised via the error distribution to improve
the prediction accuracy; secondly, a scheduling model considering demand
response is constructed to minimize the total microgrid operating costs, where
the revised forecasting values are used as the dispatch basis, and a spinning
reserve chance constraint is set according to the error distribution; finally,
by transforming the original scheduling model into a readily solvable mixed
integer linear programming via the sequence operation theory (SOT), the
transformed model is solved by using CPLEX solver. The simulation results show
that compared with the traditional scheduling model without forecasting, this
approach manages to significantly reduce the system operating costs by
improving the prediction accuracy.

    

### [[2108.06772] Dilated Inception U-Net (DIU-Net) for Brain Tumor Segmentation](http://arxiv.org/abs/2108.06772)


  Magnetic resonance imaging (MRI) is routinely used for brain tumor diagnosis,
treatment planning, and post-treatment surveillance. Recently, various models
based on deep neural networks have been proposed for the pixel-level
segmentation of tumors in brain MRIs. However, the structural variations,
spatial dissimilarities, and intensity inhomogeneity in MRIs make segmentation
a challenging task. We propose a new end-to-end brain tumor segmentation
architecture based on U-Net that integrates Inception modules and dilated
convolutions into its contracting and expanding paths. This allows us to
extract local structural as well as global contextual information. We performed
segmentation of glioma sub-regions, including tumor core, enhancing tumor, and
whole tumor using Brain Tumor Segmentation (BraTS) 2018 dataset. Our proposed
model performed significantly better than the state-of-the-art U-Net-based
model ($p<0.05$) for tumor core and whole tumor segmentation.

    

### [[2108.06783] Event2Graph: Event-driven Bipartite Graph for Multivariate Time-series Anomaly Detection](http://arxiv.org/abs/2108.06783)


  Modeling inter-dependencies between time-series is the key to achieve high
performance in anomaly detection for multivariate time-series data. The
de-facto solution to model the dependencies is to feed the data into a
recurrent neural network (RNN). However, the fully connected network structure
underneath the RNN (either GRU or LSTM) assumes a static and complete
dependency graph between time-series, which may not hold in many real-world
applications. To alleviate this assumption, we propose a dynamic bipartite
graph structure to encode the inter-dependencies between time-series. More
concretely, we model time series as one type of nodes, and the time series
segments (regarded as event) as another type of nodes, where the edge between
two types of nodes describe a temporal pattern occurred on a specific time
series at a certain time. Based on this design, relations between time series
can be explicitly modelled via dynamic connections to event nodes, and the
multivariate time-series anomaly detection problem can be formulated as a
self-supervised, edge stream prediction problem in dynamic graphs. We conducted
extensive experiments to demonstrate the effectiveness of the design.

    

### [[2108.06797] Deep Adversarially-Enhanced k-Nearest Neighbors](http://arxiv.org/abs/2108.06797)


  Recent works have theoretically and empirically shown that deep neural
networks (DNNs) have an inherent vulnerability to small perturbations. Applying
the Deep k-Nearest Neighbors (DkNN) classifier, we observe a dramatically
increasing robustness-accuracy trade-off as the layer goes deeper. In this
work, we propose a Deep Adversarially-Enhanced k-Nearest Neighbors (DAEkNN)
method which achieves higher robustness than DkNN and mitigates the
robustness-accuracy trade-off in deep layers through two key elements. First,
DAEkNN is based on an adversarially trained model. Second, DAEkNN makes
predictions by leveraging a weighted combination of benign and adversarial
training data. Empirically, we find that DAEkNN improves both the robustness
and the robustness-accuracy trade-off on MNIST and CIFAR-10 datasets.

    

### [[2108.06808] Implicit Regularization of Bregman Proximal Point Algorithm and Mirror Descent on Separable Data](http://arxiv.org/abs/2108.06808)


  Bregman proximal point algorithm (BPPA), as one of the centerpieces in the
optimization toolbox, has been witnessing emerging applications. With simple
and easy to implement update rule, the algorithm bears several compelling
intuitions for empirical successes, yet rigorous justifications are still
largely unexplored. We study the computational properties of BPPA through
classification tasks with separable data, and demonstrate provable algorithmic
regularization effects associated with BPPA. We show that BPPA attains
non-trivial margin, which closely depends on the condition number of the
distance generating function inducing the Bregman divergence. We further
demonstrate that the dependence on the condition number is tight for a class of
problems, thus showing the importance of divergence in affecting the quality of
the obtained solutions. In addition, we extend our findings to mirror descent
(MD), for which we establish similar connections between the margin and Bregman
divergence. We demonstrate through a concrete example, and show BPPA/MD
converges in direction to the maximal margin solution with respect to the
Mahalanobis distance. Our theoretical findings are among the first to
demonstrate the benign learning properties BPPA/MD, and also provide
corroborations for a careful choice of divergence in the algorithmic design.

    

### [[2108.06812] Batched Thompson Sampling for Multi-Armed Bandits](http://arxiv.org/abs/2108.06812)


  We study Thompson Sampling algorithms for stochastic multi-armed bandits in
the batched setting, in which we want to minimize the regret over a sequence of
arm pulls using a small number of policy changes (or, batches). We propose two
algorithms and demonstrate their effectiveness by experiments on both synthetic
and real datasets. We also analyze the proposed algorithms from the theoretical
aspect and obtain almost tight regret-batches tradeoffs for the two-arm case.

    

### [[2108.06816] Weakly Supervised Temporal Anomaly Segmentation with Dynamic Time Warping](http://arxiv.org/abs/2108.06816)


  Most recent studies on detecting and localizing temporal anomalies have
mainly employed deep neural networks to learn the normal patterns of temporal
data in an unsupervised manner. Unlike them, the goal of our work is to fully
utilize instance-level (or weak) anomaly labels, which only indicate whether
any anomalous events occurred or not in each instance of temporal data. In this
paper, we present WETAS, a novel framework that effectively identifies
anomalous temporal segments (i.e., consecutive time points) in an input
instance. WETAS learns discriminative features from the instance-level labels
so that it infers the sequential order of normal and anomalous segments within
each instance, which can be used as a rough segmentation mask. Based on the
dynamic time warping (DTW) alignment between the input instance and its
segmentation mask, WETAS obtains the result of temporal segmentation, and
simultaneously, it further enhances itself by using the mask as additional
supervision. Our experiments show that WETAS considerably outperforms other
baselines in terms of the localization of temporal anomalies, and also it
provides more informative results than point-level detection methods.

    

### [[2108.06822] CONet: Channel Optimization for Convolutional Neural Networks](http://arxiv.org/abs/2108.06822)


  Neural Architecture Search (NAS) has shifted network design from using human
intuition to leveraging search algorithms guided by evaluation metrics. We
study channel size optimization in convolutional neural networks (CNN) and
identify the role it plays in model accuracy and complexity. Current channel
size selection methods are generally limited by discrete sample spaces while
suffering from manual iteration and simple heuristics. To solve this, we
introduce an efficient dynamic scaling algorithm -- CONet -- that automatically
optimizes channel sizes across network layers for a given CNN. Two metrics --
``\textit{Rank}" and "\textit{Rank Average Slope}" -- are introduced to
identify the information accumulated in training. The algorithm dynamically
scales channel sizes up or down over a fixed searching phase. We conduct
experiments on CIFAR10/100 and ImageNet datasets and show that CONet can find
efficient and accurate architectures searched in ResNet, DARTS, and DARTS+
spaces that outperform their baseline models.

    

### [[2108.06842] Maps Search Misspelling Detection Leveraging Domain-Augmented Contextual Representations](http://arxiv.org/abs/2108.06842)


  Building an independent misspelling detector and serve it before correction
can bring multiple benefits to speller and other search components, which is
particularly true for the most commonly deployed noisy-channel based speller
systems. With rapid development of deep learning and substantial advancement in
contextual representation learning such as BERTology, building a decent
misspelling detector without having to rely on hand-crafted features associated
with noisy-channel architecture becomes more-than-ever accessible. However
BERTolgy models are trained with natural language corpus but Maps Search is
highly domain specific, would BERTology continue its success. In this paper we
design 4 stages of models for misspeling detection ranging from the most basic
LSTM to single-domain augmented fine-tuned BERT. We found for Maps Search in
our case, other advanced BERTology family model such as RoBERTa does not
necessarily outperform BERT, and a classic cross-domain fine-tuned full BERT
even underperforms a smaller single-domain fine-tuned BERT. We share more
findings through comprehensive modeling experiments and analysis, we also
briefly cover the data generation algorithm breakthrough.

    

### [[2108.06846] Do Proportionate Algorithms Exploit Sparsity?](http://arxiv.org/abs/2108.06846)


  Adaptive filters exploiting sparsity have been a very active research field,
among which the algorithms that follow the "proportional-update principle", the
so-called proportionate-type algorithms, are very popular. Indeed, there are
hundreds of works on proportionate-type algorithms and, therefore, their
advantages are widely known. This paper addresses the unexplored drawbacks and
limitations of using proportional updates and their practical impacts. Our
findings include the theoretical justification for the poor performance of
these algorithms in several sparse scenarios, and also when dealing with
non-stationary and compressible systems. Simulation results corroborating the
theory are presented.

    

### [[2108.06847] Interpreting and improving deep-learning models with reality checks](http://arxiv.org/abs/2108.06847)


  Recent deep-learning models have achieved impressive predictive performance
by learning complex functions of many variables, often at the cost of
interpretability. This chapter covers recent work aiming to interpret models by
attributing importance to features and feature groups for a single prediction.
Importantly, the proposed attributions assign importance to interactions
between features, in addition to features in isolation. These attributions are
shown to yield insights across real-world domains, including bio-imaging,
cosmology image and natural-language processing. We then show how these
attributions can be used to directly improve the generalization of a neural
network or to distill it into a simple model. Throughout the chapter, we
emphasize the use of reality checks to scrutinize the proposed interpretation
techniques.

    

### [[2108.06849] Introduction to Quantum Reinforcement Learning: Theory and PennyLane-based Implementation](http://arxiv.org/abs/2108.06849)


  The emergence of quantum computing enables for researchers to apply quantum
circuit on many existing studies. Utilizing quantum circuit and quantum
differential programming, many research are conducted such as \textit{Quantum
Machine Learning} (QML). In particular, quantum reinforcement learning is a
good field to test the possibility of quantum machine learning, and a lot of
research is being done. This work will introduce the concept of quantum
reinforcement learning using a variational quantum circuit, and confirm its
possibility through implementation and experimentation. We will first present
the background knowledge and working principle of quantum reinforcement
learning, and then guide the implementation method using the PennyLane library.
We will also discuss the power and possibility of quantum reinforcement
learning from the experimental results obtained through this work.

    

### [[2108.06853] Clustering Filipino Disaster-Related Tweets Using Incremental and Density-Based Spatiotemporal Algorithm with Support Vector Machines for Needs Assessment 2](http://arxiv.org/abs/2108.06853)


  Social media has played a huge part on how people get informed and
communicate with one another. It has helped people express their needs due to
distress especially during disasters. Because posts made through it are
publicly accessible by default, Twitter is among the most helpful social media
sites in times of disaster. With this, the study aims to assess the needs
expressed during calamities by Filipinos on Twitter. Data were gathered and
classified as either disaster-related or unrelated with the use of Naïve
Bayes classifier. After this, the disaster-related tweets were clustered per
disaster type using Incremental Clustering Algorithm, and then sub-clustered
based on the location and time of the tweet using Density-based Spatiotemporal
Clustering Algorithm. Lastly, using Support Vector Machines, the tweets were
classified according to the expressed need, such as shelter, rescue, relief,
cash, prayer, and others. After conducting the study, results showed that the
Incremental Clustering Algorithm and Density-Based Spatiotemporal Clustering
Algorithm were able to cluster the tweets with f-measure scores of 47.20% and
82.28% respectively. Also, the Naïve Bayes and Support Vector Machines were
able to classify with an average f-measure score of 97% and an average accuracy
of 77.57% respectively.

    

### [[2108.06862] Generating Cyber Threat Intelligence to Discover Potential Security Threats Using Classification and Topic Modeling](http://arxiv.org/abs/2108.06862)


  Due to the variety of cyber-attacks or threats, the cybersecurity community
has been enhancing the traditional security control mechanisms to an advanced
level so that automated tools can encounter potential security threats. Very
recently a term, Cyber Threat Intelligence (CTI) has been represented as one of
the proactive and robust mechanisms because of its automated cybersecurity
threat prediction based on data. In general, CTI collects and analyses data
from various sources e.g. online security forums, social media where cyber
enthusiasts, analysts, even cybercriminals discuss cyber or computer security
related topics and discovers potential threats based on the analysis. As the
manual analysis of every such discussion i.e. posts on online platforms is
time-consuming, inefficient, and susceptible to errors, CTI as an automated
tool can perform uniquely to detect cyber threats. In this paper, our goal is
to identify and explore relevant CTI from hacker forums by using different
supervised and unsupervised learning techniques. To this end, we collect data
from a real hacker forum and constructed two datasets: a binary dataset and a
multi-class dataset. Our binary dataset contains two classes one containing
cybersecurity-relevant posts and another one containing posts that are not
related to security. This dataset is constructed using simple keyword search
technique. Using a similar approach, we further categorize posts from
security-relevant posts into five different threat categories. We then applied
several machine learning classifiers along with deep neural network-based
classifiers and use them on the datasets to compare their performances. We also
tested the classifiers on a leaked dataset with labels named this http URL as our
ground truth. We further explore the datasets using unsupervised techniques
i.e. Latent Dirichlet Allocation (LDA) and Non-negative Matrix Factorization
(NMF).

    

### [[2108.06868] Nowcasting-Nets: Deep Neural Network Structures for Precipitation Nowcasting Using IMERG](http://arxiv.org/abs/2108.06868)


  Accurate and timely estimation of precipitation is critical for issuing
hazard warnings (e.g., for flash floods or landslides). Current remotely sensed
precipitation products have a few hours of latency, associated with the
acquisition and processing of satellite data. By applying a robust nowcasting
system to these products, it is (in principle) possible to reduce this latency
and improve their applicability, value, and impact. However, the development of
such a system is complicated by the chaotic nature of the atmosphere, and the
consequent rapid changes that can occur in the structures of precipitation
systems In this work, we develop two approaches (hereafter referred to as
Nowcasting-Nets) that use Recurrent and Convolutional deep neural network
structures to address the challenge of precipitation nowcasting. A total of
five models are trained using Global Precipitation Measurement (GPM) Integrated
Multi-satellitE Retrievals for GPM (IMERG) precipitation data over the Eastern
Contiguous United States (CONUS) and then tested against independent data for
the Eastern and Western CONUS. The models were designed to provide forecasts
with a lead time of up to 1.5 hours and, by using a feedback loop approach, the
ability of the models to extend the forecast time to 4.5 hours was also
investigated. Model performance was compared against the Random Forest (RF) and
Linear Regression (LR) machine learning methods, and also against a persistence
benchmark (BM) that used the most recent observation as the forecast.
Independent IMERG observations were used as a reference, and experiments were
conducted to examine both overall statistics and case studies involving
specific precipitation events. Overall, the forecasts provided by the
Nowcasting-Net models are superior, with the Convolutional Nowcasting Network
with Residual Head (CNC-R) achieving 25%, 28%, and 46% improvement in the test
...

    

### [[2108.06869] Reducing the Communication Cost of Federated Learning through Multistage Optimization](http://arxiv.org/abs/2108.06869)


  A central question in federated learning (FL) is how to design optimization
algorithms that minimize the communication cost of training a model over
heterogeneous data distributed across many clients. A popular technique for
reducing communication is the use of local steps, where clients take multiple
optimization steps over local data before communicating with the server (e.g.,
FedAvg, SCAFFOLD). This contrasts with centralized methods, where clients take
one optimization step per communication round (e.g., Minibatch SGD). A recent
lower bound on the communication complexity of first-order methods shows that
centralized methods are optimal over highly-heterogeneous data, whereas local
methods are optimal over purely homogeneous data [Woodworth et al., 2020]. For
intermediate heterogeneity levels, no algorithm is known to match the lower
bound. In this paper, we propose a multistage optimization scheme that nearly
matches the lower bound across all heterogeneity levels. The idea is to first
run a local method up to a heterogeneity-induced error floor; next, we switch
to a centralized method for the remaining steps. Our analysis may help explain
empirically-successful stepsize decay methods in FL [Charles et al., 2020;
Reddi et al., 2020]. We demonstrate the scheme's practical utility in image
classification tasks.

    

### [[2108.06871] IADA: Iterative Adversarial Data Augmentation Using Formal Verification and Expert Guidance](http://arxiv.org/abs/2108.06871)


  Neural networks (NNs) are widely used for classification tasks for their
remarkable performance. However, the robustness and accuracy of NNs heavily
depend on the training data. In many applications, massive training data is
usually not available. To address the challenge, this paper proposes an
iterative adversarial data augmentation (IADA) framework to learn neural
network models from an insufficient amount of training data. The method uses
formal verification to identify the most "confusing" input samples, and
leverages human guidance to safely and iteratively augment the training data
with these samples. The proposed framework is applied to an artificial 2D
dataset, the MNIST dataset, and a human motion dataset. By applying IADA to
fully-connected NN classifiers, we show that our training method can improve
the robustness and accuracy of the learned model. By comparing to regular
supervised training, on the MNIST dataset, the average perturbation bound
improved 107.4%. The classification accuracy improved 1.77%, 3.76%, 10.85% on
the 2D dataset, the MNIST dataset, and the human motion dataset respectively.

    

### [[2108.06885] Neural Architecture Dilation for Adversarial Robustness](http://arxiv.org/abs/2108.06885)


  With the tremendous advances in the architecture and scale of convolutional
neural networks (CNNs) over the past few decades, they can easily reach or even
exceed the performance of humans in certain tasks. However, a recently
discovered shortcoming of CNNs is that they are vulnerable to adversarial
attacks. Although the adversarial robustness of CNNs can be improved by
adversarial training, there is a trade-off between standard accuracy and
adversarial robustness. From the neural architecture perspective, this paper
aims to improve the adversarial robustness of the backbone CNNs that have a
satisfactory accuracy. Under a minimal computational overhead, the introduction
of a dilation architecture is expected to be friendly with the standard
performance of the backbone CNN while pursuing adversarial robustness.
Theoretical analyses on the standard and adversarial error bounds naturally
motivate the proposed neural architecture dilation algorithm. Experimental
results on real-world datasets and benchmark neural networks demonstrate the
effectiveness of the proposed algorithm to balance the accuracy and adversarial
robustness.

    

### [[2108.06888] Provable Data Clustering via Innovation Search](http://arxiv.org/abs/2108.06888)


  This paper studies the subspace clustering problem in which data points
collected from high-dimensional ambient space lie in a union of linear
subspaces. Subspace clustering becomes challenging when the dimension of
intersection between subspaces is large and most of the self-representation
based methods are sensitive to the intersection between the span of clusters.
In sharp contrast to the self-representation based methods, a recently proposed
clustering method termed Innovation Pursuit, computed a set of optimal
directions (directions of innovation) to build the adjacency matrix. This paper
focuses on the Innovation Pursuit Algorithm to shed light on its impressive
performance when the subspaces are heavily intersected. It is shown that in
contrast to most of the existing methods which require the subspaces to be
sufficiently incoherent with each other, Innovation Pursuit only requires the
innovative components of the subspaces to be sufficiently incoherent with each
other. These new sufficient conditions allow the clusters to be strongly close
to each other. Motivated by the presented theoretical analysis, a simple yet
effective projection based technique is proposed which we show with both
numerical and theoretical results that it can boost the performance of
Innovation Pursuit.

    

### [[2108.06889] Causal Incremental Graph Convolution for Recommender System Retraining](http://arxiv.org/abs/2108.06889)


  Real-world recommender system needs to be regularly retrained to keep with
the new data. In this work, we consider how to efficiently retrain graph
convolution network (GCN) based recommender models, which are state-of-the-art
techniques for collaborative recommendation. To pursue high efficiency, we set
the target as using only new data for model updating, meanwhile not sacrificing
the recommendation accuracy compared with full model retraining. This is
non-trivial to achieve, since the interaction data participates in both the
graph structure for model construction and the loss function for model
learning, whereas the old graph structure is not allowed to use in model
updating. Towards the goal, we propose a \textit{Causal Incremental Graph
Convolution} approach, which consists of two new operators named
\textit{Incremental Graph Convolution} (IGC) and \textit{Colliding Effect
Distillation} (CED) to estimate the output of full graph convolution. In
particular, we devise simple and effective modules for IGC to ingeniously
combine the old representations and the incremental graph and effectively fuse
the long-term and short-term preference signals. CED aims to avoid the
out-of-date issue of inactive nodes that are not in the incremental graph,
which connects the new data with inactive nodes through causal inference. In
particular, CED estimates the causal effect of new data on the representation
of inactive nodes through the control of their collider. Extensive experiments
on three real-world datasets demonstrate both accuracy gains and significant
speed-ups over the existing retraining mechanism.

    

### [[2108.06890] GC-TTS: Few-shot Speaker Adaptation with Geometric Constraints](http://arxiv.org/abs/2108.06890)


  Few-shot speaker adaptation is a specific Text-to-Speech (TTS) system that
aims to reproduce a novel speaker's voice with a few training data. While
numerous attempts have been made to the few-shot speaker adaptation system,
there is still a gap in terms of speaker similarity to the target speaker
depending on the amount of data. To bridge the gap, we propose GC-TTS which
achieves high-quality speaker adaptation with significantly improved speaker
similarity. Specifically, we leverage two geometric constraints to learn
discriminative speaker representations. Here, a TTS model is pre-trained for
base speakers with a sufficient amount of data, and then fine-tuned for novel
speakers on a few minutes of data with two geometric constraints. Two geometric
constraints enable the model to extract discriminative speaker embeddings from
limited data, which leads to the synthesis of intelligible speech. We discuss
and verify the effectiveness of GC-TTS by comparing it with popular and
essential methods. The experimental results demonstrate that GC-TTS generates
high-quality speech from only a few minutes of training data, outperforming
standard techniques in terms of speaker similarity to the target speaker.

    

### [[2108.06895] Interpreting Attributions and Interactions of Adversarial Attacks](http://arxiv.org/abs/2108.06895)


  This paper aims to explain adversarial attacks in terms of how adversarial
perturbations contribute to the attacking task. We estimate attributions of
different image regions to the decrease of the attacking cost based on the
Shapley value. We define and quantify interactions among adversarial
perturbation pixels, and decompose the entire perturbation map into relatively
independent perturbation components. The decomposition of the perturbation map
shows that adversarially-trained DNNs have more perturbation components in the
foreground than normally-trained DNNs. Moreover, compared to the
normally-trained DNN, the adversarially-trained DNN have more components which
mainly decrease the score of the true category. Above analyses provide new
insights into the understanding of adversarial attacks.

    

### [[2108.06896] Challenges for cognitive decoding using deep learning methods](http://arxiv.org/abs/2108.06896)


  In cognitive decoding, researchers aim to characterize a brain region's
representations by identifying the cognitive states (e.g., accepting/rejecting
a gamble) that can be identified from the region's activity. Deep learning (DL)
methods are highly promising for cognitive decoding, with their unmatched
ability to learn versatile representations of complex data. Yet, their
widespread application in cognitive decoding is hindered by their general lack
of interpretability as well as difficulties in applying them to small datasets
and in ensuring their reproducibility and robustness. We propose to approach
these challenges by leveraging recent advances in explainable artificial
intelligence and transfer learning, while also providing specific
recommendations on how to improve the reproducibility and robustness of DL
modeling results.

    

### [[2108.06898] Neural-to-Tree Policy Distillation with Policy Improvement Criterion](http://arxiv.org/abs/2108.06898)


  While deep reinforcement learning has achieved promising results in
challenging decision-making tasks, the main bones of its success --- deep
neural networks are mostly black-boxes. A feasible way to gain insight into a
black-box model is to distill it into an interpretable model such as a decision
tree, which consists of if-then rules and is easy to grasp and be verified.
However, the traditional model distillation is usually a supervised learning
task under a stationary data distribution assumption, which is violated in
reinforcement learning. Therefore, a typical policy distillation that clones
model behaviors with even a small error could bring a data distribution shift,
resulting in an unsatisfied distilled policy model with low fidelity or low
performance. In this paper, we propose to address this issue by changing the
distillation objective from behavior cloning to maximizing an advantage
evaluation. The novel distillation objective maximizes an approximated
cumulative reward and focuses more on disastrous behaviors in critical states,
which controls the data shift effect. We evaluate our method on several Gym
tasks, a commercial fight game, and a self-driving car simulator. The empirical
results show that the proposed method can preserve a higher cumulative reward
than behavior cloning and learn a more consistent policy to the original one.
Moreover, by examining the extracted rules from the distilled decision trees,
we demonstrate that the proposed method delivers reasonable and robust
decisions.

    

### [[2108.06905] A physics-informed variational DeepONet for predicting the crack path in brittle materials](http://arxiv.org/abs/2108.06905)


  Failure trajectories, identifying the probable failure zones, and damage
statistics are some of the key quantities of relevance in brittle fracture
applications. High-fidelity numerical solvers that reliably estimate these
relevant quantities exist but they are computationally demanding requiring a
high resolution of the crack. Moreover, independent intensive simulations need
to be carried out even for a small change in domain parameters and/or material
properties. Therefore, fast and generalizable surrogate models are needed to
alleviate the computational burden but the discontinuous nature of fracture
mechanics presents a major challenge to developing such models. We propose a
physics-informed variational formulation of DeepONet (V-DeepONet) for brittle
fracture analysis. V-DeepONet is trained to map the initial configuration of
the defect to the relevant fields of interests (e.g., damage and displacement
fields). Once the network is trained, the entire global solution can be rapidly
obtained for any initial crack configuration and loading steps on that domain.
While the original DeepONet is solely data-driven, we take a different path to
train the V-DeepONet by imposing the governing equations in variational form
and we also use some labelled data. We demonstrate the effectiveness of
V-DeepOnet through two benchmarks of brittle fracture, and we verify its
accuracy using results from high-fidelity solvers. Encoding the physical laws
and also some data to train the network renders the surrogate model capable of
accurately performing both interpolation and extrapolation tasks, considering
that fracture modeling is very sensitive to fluctuations. The proposed hybrid
training of V-DeepONet is superior to state-of-the-art methods and can be
applied to a wide array of dynamical systems with complex responses.

    

### [[2108.06907] Locally Interpretable Model Agnostic Explanations using Gaussian Processes](http://arxiv.org/abs/2108.06907)


  Owing to tremendous performance improvements in data-intensive domains,
machine learning (ML) has garnered immense interest in the research community.
However, these ML models turn out to be black boxes, which are tough to
interpret, resulting in a direct decrease in productivity. Local Interpretable
Model-Agnostic Explanations (LIME) is a popular technique for explaining the
prediction of a single instance. Although LIME is simple and versatile, it
suffers from instability in the generated explanations. In this paper, we
propose a Gaussian Process (GP) based variation of locally interpretable
models. We employ a smart sampling strategy based on the acquisition functions
in Bayesian optimization. Further, we employ the automatic relevance
determination based covariance function in GP, with separate length-scale
parameters for each feature, where the reciprocal of lengthscale parameters
serve as feature explanations. We illustrate the performance of the proposed
technique on two real-world datasets, and demonstrate the superior stability of
the proposed technique. Furthermore, we demonstrate that the proposed technique
is able to generate faithful explanations using much fewer samples as compared
to LIME.

    

### [[2108.06910] A Novel Attribute Reconstruction Attack in Federated Learning](http://arxiv.org/abs/2108.06910)


  Federated learning (FL) emerged as a promising learning paradigm to enable a
multitude of participants to construct a joint ML model without exposing their
private training data. Existing FL designs have been shown to exhibit
vulnerabilities which can be exploited by adversaries both within and outside
of the system to compromise data privacy. However, most current works conduct
attacks by leveraging gradients on a small batch of data, which is less
practical in FL. In this work, we consider a more practical and interesting
scenario in which participants share their epoch-averaged gradients (share
gradients after at least 1 epoch of local training) rather than per-example or
small batch-averaged gradients as in previous works. We perform the first
systematic evaluation of attribute reconstruction attack (ARA) launched by the
malicious server in the FL system, and empirically demonstrate that the shared
epoch-averaged local model gradients can reveal sensitive attributes of local
training data of any victim participant. To achieve this goal, we develop a
more effective and efficient gradient matching based method called cos-matching
to reconstruct the training data attributes. We evaluate our attacks on a
variety of real-world datasets, scenarios, assumptions. Our experiments show
that our proposed method achieves better attribute attack performance than most
existing baselines.

    

### [[2108.06911] Optimal Actor-Critic Policy with Optimized Training Datasets](http://arxiv.org/abs/2108.06911)


  Actor-critic (AC) algorithms are known for their efficacy and high
performance in solving reinforcement learning problems, but they also suffer
from low sampling efficiency. An AC based policy optimization process is
iterative and needs to frequently access the agent-environment system to
evaluate and update the policy by rolling out the policy, collecting rewards
and states (i.e. samples), and learning from them. It ultimately requires a
huge number of samples to learn an optimal policy. To improve sampling
efficiency, we propose a strategy to optimize the training dataset that
contains significantly less samples collected from the AC process. The dataset
optimization is made of a best episode only operation, a policy
parameter-fitness model, and a genetic algorithm module. The optimal policy
network trained by the optimized training dataset exhibits superior performance
compared to many contemporary AC algorithms in controlling autonomous dynamical
systems. Evaluation on standard benchmarks show that the method improves
sampling efficiency, ensures faster convergence to optima, and is more
data-efficient than its counterparts.

    

### [[2108.06912] Blockchain-based Trustworthy Federated Learning Architecture](http://arxiv.org/abs/2108.06912)


  Federated learning is an emerging privacy-preserving AI technique where
clients (i.e., organisations or devices) train models locally and formulate a
global model based on the local model updates without transferring local data
externally. However, federated learning systems struggle to achieve
trustworthiness and embody responsible AI principles. In particular, federated
learning systems face accountability and fairness challenges due to
multi-stakeholder involvement and heterogeneity in client data distribution. To
enhance the accountability and fairness of federated learning systems, we
present a blockchain-based trustworthy federated learning architecture. We
first design a smart contract-based data-model provenance registry to enable
accountability. Additionally, we propose a weighted fair data sampler algorithm
to enhance fairness in training data. We evaluate the proposed approach using a
COVID-19 X-ray detection use case. The evaluation results show that the
approach is feasible to enable accountability and improve fairness. The
proposed algorithm can achieve better performance than the default federated
learning setting in terms of the model's generalisation and accuracy.

    

### [[2108.06920] A complex network approach to time series analysis with application in diagnosis of neuromuscular disorders](http://arxiv.org/abs/2108.06920)


  Electromyography (EMG) refers to a biomedical signal indicating neuromuscular
activity and muscle morphology. Experts accurately diagnose neuromuscular
disorders using this time series. Modern data analysis techniques have recently
led to introducing novel approaches for mapping time series data to graphs and
complex networks with applications in diverse fields, including medicine. The
resulting networks develop a completely different visual acuity that can be
used to complement physician findings of time series. This can lead to a more
enriched analysis, reduced error, more accurate diagnosis of the disease, and
increased accuracy and speed of the treatment process. The mapping process may
cause the loss of essential data from the time series and not retain all the
time series features. As a result, achieving an approach that can provide a
good representation of the time series while maintaining essential features is
crucial. This paper proposes a new approach to network development named
GraphTS to overcome the limited accuracy of existing methods through EMG time
series using the visibility graph method. For this purpose, EMG signals are
pre-processed and mapped to a complex network by a standard visibility graph
algorithm. The resulting networks can differentiate between healthy and patient
samples. In the next step, the properties of the developed networks are given
in the form of a feature matrix as input to classifiers after extracting
optimal features. Performance evaluation of the proposed approach with deep
neural network shows 99.30% accuracy for training data and 99.18% for test
data. Therefore, in addition to enriched network representation and covering
the features of time series for healthy, myopathy, and neuropathy EMG, the
proposed technique improves accuracy, precision, recall, and F-score.

    

### [[2108.06924] Near-Optimal No-Regret Learning in General Games](http://arxiv.org/abs/2108.06924)


  We show that Optimistic Hedge -- a common variant of
multiplicative-weights-updates with recency bias -- attains ${\rm poly}(\log
T)$ regret in multi-player general-sum games. In particular, when every player
of the game uses Optimistic Hedge to iteratively update her strategy in
response to the history of play so far, then after $T$ rounds of interaction,
each player experiences total regret that is ${\rm poly}(\log T)$. Our bound
improves, exponentially, the $O({T}^{1/2})$ regret attainable by standard
no-regret learners in games, the $O(T^{1/4})$ regret attainable by no-regret
learners with recency bias (Syrgkanis et al., 2015), and the ${O}(T^{1/6})$
bound that was recently shown for Optimistic Hedge in the special case of
two-player games (Chen & Pen, 2020). A corollary of our bound is that
Optimistic Hedge converges to coarse correlated equilibrium in general games at
a rate of $\tilde{O}\left(\frac 1T\right)$.

    

### [[2108.06958] Aegis: A Trusted, Automatic and Accurate Verification Framework for Vertical Federated Learning](http://arxiv.org/abs/2108.06958)


  Vertical federated learning (VFL) leverages various privacy-preserving
algorithms, e.g., homomorphic encryption or secret sharing based SecureBoost,
to ensure data privacy. However, these algorithms all require a semi-honest
secure definition, which raises concerns in real-world applications. In this
paper, we present Aegis, a trusted, automatic, and accurate verification
framework to verify the security of VFL jobs. Aegis is separated from local
parties to ensure the security of the framework. Furthermore, it automatically
adapts to evolving VFL algorithms by defining the VFL job as a finite state
machine to uniformly verify different algorithms and reproduce the entire job
to provide more accurate verification. We implement and evaluate Aegis with
different threat models on financial and medical datasets. Evaluation results
show that: 1) Aegis can detect 95% threat models, and 2) it provides
fine-grained verification results within 84% of the total VFL job time.

    

### [[2108.06959] WikiChurches: A Fine-Grained Dataset of Architectural Styles with Real-World Challenges](http://arxiv.org/abs/2108.06959)


  We introduce a novel dataset for architectural style classification,
consisting of 9,485 images of church buildings. Both images and style labels
were sourced from Wikipedia. The dataset can serve as a benchmark for various
research fields, as it combines numerous real-world challenges: fine-grained
distinctions between classes based on subtle visual features, a comparatively
small sample size, a highly imbalanced class distribution, a high variance of
viewpoints, and a hierarchical organization of labels, where only some images
are labeled at the most precise level. In addition, we provide 631 bounding box
annotations of characteristic visual features for 139 churches from four major
categories. These annotations can, for example, be useful for research on
fine-grained classification, where additional expert knowledge about
distinctive object parts is often available. Images and annotations are
available at: this https URL


### [[2108.06980] Task-Sensitive Concept Drift Detector with Metric Learning](http://arxiv.org/abs/2108.06980)


  Detecting drifts in data is essential for machine learning applications, as
changes in the statistics of processed data typically has a profound influence
on the performance of trained models. Most of the available drift detection
methods require access to true labels during inference time. In a real-world
scenario, true labels usually available only during model training. In this
work, we propose a novel task-sensitive drift detection framework, which is
able to detect drifts without access to true labels during inference. It
utilizes metric learning of a constrained low-dimensional embedding
representation of the input data, which is best suited for the classification
task. It is able to detect real drift, where the drift affects the
classification performance, while it properly ignores virtual drift, where the
classification performance is not affected by the drift. In the proposed
framework, the actual method to detect a change in the statistics of incoming
data samples can be chosen freely. We also propose the two change detection
methods, which are based on the exponential moving average and a modified
$z$-score, respectively. We evaluate the performance of the proposed framework
with a novel metric, which accumulates the standard metrics of detection
accuracy, false positive rate and detection delay into one value. Experimental
evaluation on nine benchmarks datasets, with different types of drift,
demonstrates that the proposed framework can reliably detect drifts, and
outperforms state-of-the-art unsupervised drift detection approaches.

    

### [[2108.06988] A diffusion-map-based algorithm for gradient computation on manifolds and applications](http://arxiv.org/abs/2108.06988)


  We recover the gradient of a given function defined on interior points of a
submanifold with boundary of the Euclidean space based on a (normally
distributed) random sample of function evaluations at points in the manifold.
This approach is based on the estimates of the Laplace-Beltrami operator
proposed in the theory of Diffusion-Maps. Analytical convergence results of the
resulting expansion are proved, and an efficient algorithm is proposed to deal
with non-convex optimization problems defined on Euclidean submanifolds. We
test and validate our methodology as a post-processing tool in Cryogenic
electron microscopy (Cryo-EM). We also apply the method to the classical sphere
packing problem.

    

### [[2108.07019] Towards a Safety Case for Hardware Fault Tolerance in Convolutional Neural Networks Using Activation Range Supervision](http://arxiv.org/abs/2108.07019)


  Convolutional neural networks (CNNs) have become an established part of
numerous safety-critical computer vision applications, including human robot
interactions and automated driving. Real-world implementations will need to
guarantee their robustness against hardware soft errors corrupting the
underlying platform memory. Based on the previously observed efficacy of
activation clipping techniques, we build a prototypical safety case for
classifier CNNs by demonstrating that range supervision represents a highly
reliable fault detector and mitigator with respect to relevant bit flips,
adopting an eight-exponent floating point data representation. We further
explore novel, non-uniform range restriction methods that effectively suppress
the probability of silent data corruptions and uncorrectable errors. As a
safety-relevant end-to-end use case, we showcase the benefit of our approach in
a vehicle classification scenario, using ResNet-50 and the traffic camera data
set MIOVision. The quantitative evidence provided in this work can be leveraged
to inspire further and possibly more complex CNN safety arguments.

    

### [[2108.07028] Non-Local Feature Aggregation on Graphs via Latent Fixed Data Structures](http://arxiv.org/abs/2108.07028)


  In contrast to image/text data whose order can be used to perform non-local
feature aggregation in a straightforward way using the pooling layers, graphs
lack the tensor representation and mostly the element-wise max/mean function is
utilized to aggregate the locally extracted feature vectors. In this paper, we
present a novel approach for global feature aggregation in Graph Neural
Networks (GNNs) which utilizes a Latent Fixed Data Structure (LFDS) to
aggregate the extracted feature vectors. The locally extracted feature vectors
are sorted/distributed on the LFDS and a latent neural network (CNN/GNN) is
utilized to perform feature aggregation on the LFDS. The proposed approach is
used to design several novel global feature aggregation methods based on the
choice of the LFDS. We introduce multiple LFDSs including loop, 3D tensor
(image), sequence, data driven graphs and an algorithm which sorts/distributes
the extracted local feature vectors on the LFDS. While the computational
complexity of the proposed methods are linear with the order of input graphs,
they achieve competitive or better results.

    

### [[2108.07041] Implicitly Regularized RL with Implicit Q-Values](http://arxiv.org/abs/2108.07041)


  The $Q$-function is a central quantity in many Reinforcement Learning (RL)
algorithms for which RL agents behave following a (soft)-greedy policy w.r.t.
to $Q$. It is a powerful tool that allows action selection without a model of
the environment and even without explicitly modeling the policy. Yet, this
scheme can only be used in discrete action tasks, with small numbers of
actions, as the softmax cannot be computed exactly otherwise. Especially the
usage of function approximation, to deal with continuous action spaces in
modern actor-critic architectures, intrinsically prevents the exact computation
of a softmax. We propose to alleviate this issue by parametrizing the
$Q$-function implicitly, as the sum of a log-policy and of a value function. We
use the resulting parametrization to derive a practical off-policy deep RL
algorithm, suitable for large action spaces, and that enforces the softmax
relation between the policy and the $Q$-value. We provide a theoretical
analysis of our algorithm: from an Approximate Dynamic Programming perspective,
we show its equivalence to a regularized version of value iteration, accounting
for both entropy and Kullback-Leibler regularization, and that enjoys
beneficial error propagation results. We then evaluate our algorithm on classic
control tasks, where its results compete with state-of-the-art methods.

    

### [[2108.07046] WiseR: An end-to-end structure learning and deployment framework for causal graphical models](http://arxiv.org/abs/2108.07046)


  Structure learning offers an expressive, versatile and explainable approach
to causal and mechanistic modeling of complex biological data. We present
wiseR, an open source application for learning, evaluating and deploying robust
causal graphical models using graph neural networks and Bayesian networks. We
demonstrate the utility of this application through application on for
biomarker discovery in a COVID-19 clinical dataset.

    

### [[2108.07060] Detecting and interpreting faults in vulnerable power grids with machine learning](http://arxiv.org/abs/2108.07060)


  Unscheduled power disturbances cause severe consequences both for customers
and grid operators. To defend against such events, it is necessary to identify
the causes of interruptions in the power distribution network. In this work, we
focus on the power grid of a Norwegian community in the Arctic that experiences
several faults whose sources are unknown. First, we construct a data set
consisting of relevant meteorological data and information about the current
power quality logged by power-quality meters. Then, we adopt machine-learning
techniques to predict the occurrence of faults. Experimental results show that
both linear and non-linear classifiers achieve good classification performance.
This indicates that the considered power-quality and weather variables explain
well the power disturbances. Interpreting the decision process of the
classifiers provides valuable insights to understand the main causes of
disturbances. Traditional features selection methods can only indicate which
are the variables that, on average, mostly explain the fault occurrences in the
dataset. Besides providing such a global interpretation, it is also important
to identify the specific set of variables that explain each individual fault.
To address this challenge, we adopt a recent technique to interpret the
decision process of a deep learning model, called Integrated Gradients. The
proposed approach allows to gain detailed insights on the occurrence of a
specific fault, which are valuable for the distribution system operators to
implement strategies to prevent and mitigate power disturbances.

    

### [[2108.07063] Multistream Graph Attention Networks for Wind Speed Forecasting](http://arxiv.org/abs/2108.07063)


  Reliable and accurate wind speed prediction has significant impact in many
industrial sectors such as economic, business and management among others. This
paper presents a new model for wind speed prediction based on Graph Attention
Networks (GAT). In particular, the proposed model extends GAT architecture by
equipping it with a learnable adjacency matrix as well as incorporating a new
attention mechanism with the aim of obtaining attention scores per weather
variable. The output of the GAT based model is combined with the LSTM layer in
order to exploit both the spatial and temporal characteristics of the
multivariate multidimensional historical weather data. Real weather data
collected from several cities in Denmark and Netherlands are used to conduct
the experiments and evaluate the performance of the proposed model. We show
that in comparison to previous architectures used for wind speed prediction,
the proposed model is able to better learn the complex input-output
relationships of the weather data. Furthermore, thanks to the learned attention
weights, the model provides an additional insights on the most important
weather variables and cities for the studied prediction task.

    

### [[2108.07083] Identifying and Exploiting Structures for Reliable Deep Learning](http://arxiv.org/abs/2108.07083)


  Deep learning research has recently witnessed an impressively fast-paced
progress in a wide range of tasks including computer vision, natural language
processing, and reinforcement learning. The extraordinary performance of these
systems often gives the impression that they can be used to revolutionise our
lives for the better. However, as recent works point out, these systems suffer
from several issues that make them unreliable for use in the real world,
including vulnerability to adversarial attacks (Szegedy et al. [248]), tendency
to memorise noise (Zhang et al. [292]), being over-confident on incorrect
predictions (miscalibration) (Guo et al. [99]), and unsuitability for handling
private data (Gilad-Bachrach et al. [88]). In this thesis, we look at each of
these issues in detail, investigate their causes, and propose computationally
cheap algorithms for mitigating them in practice. To do this, we identify
structures in deep neural networks that can be exploited to mitigate the above
causes of unreliability of deep learning algorithms.

    

### [[2108.07107] Task-wise Split Gradient Boosting Trees for Multi-center Diabetes Prediction](http://arxiv.org/abs/2108.07107)


  Diabetes prediction is an important data science application in the social
healthcare domain. There exist two main challenges in the diabetes prediction
task: data heterogeneity since demographic and metabolic data are of different
types, data insufficiency since the number of diabetes cases in a single
medical center is usually limited. To tackle the above challenges, we employ
gradient boosting decision trees (GBDT) to handle data heterogeneity and
introduce multi-task learning (MTL) to solve data insufficiency. To this end,
Task-wise Split Gradient Boosting Trees (TSGB) is proposed for the multi-center
diabetes prediction task. Specifically, we firstly introduce task gain to
evaluate each task separately during tree construction, with a theoretical
analysis of GBDT's learning objective. Secondly, we reveal a problem when
directly applying GBDT in MTL, i.e., the negative task gain problem. Finally,
we propose a novel split method for GBDT in MTL based on the task gain
statistics, named task-wise split, as an alternative to standard feature-wise
split to overcome the mentioned negative task gain problem. Extensive
experiments on a large-scale real-world diabetes dataset and a commonly used
benchmark dataset demonstrate TSGB achieves superior performance against
several state-of-the-art methods. Detailed case studies further support our
analysis of negative task gain problems and provide insightful findings. The
proposed TSGB method has been deployed as an online diabetes risk assessment
software for early diagnosis.

    

### [[2108.07120] AIREX: Neural Network-based Approach for Air Quality Inference in Unmonitored Cities](http://arxiv.org/abs/2108.07120)


  Urban air pollution is a major environmental problem affecting human health
and quality of life. Monitoring stations have been established to continuously
obtain air quality information, but they do not cover all areas. Thus, there
are numerous methods for spatially fine-grained air quality inference. Since
existing methods aim to infer air quality of locations only in monitored
cities, they do not assume inferring air quality in unmonitored cities. In this
paper, we first study the air quality inference in unmonitored cities. To
accurately infer air quality in unmonitored cities, we propose a neural
network-based approach AIREX. The novelty of AIREX is employing a
mixture-of-experts approach, which is a machine learning technique based on the
divide-and-conquer principle, to learn correlations of air quality between
multiple cities. To further boost the performance, it employs attention
mechanisms to compute impacts of air quality inference from the monitored
cities to the locations in the unmonitored city. We show, through experiments
on a real-world air quality dataset, that AIREX achieves higher accuracy than
state-of-the-art methods.

    

### [[2108.07130] Semi-Supervised Siamese Network for Identifying Bad Data in Medical Imaging Datasets](http://arxiv.org/abs/2108.07130)


  Noisy data present in medical imaging datasets can often aid the development
of robust models that are equipped to handle real-world data. However, if the
bad data contains insufficient anatomical information, it can have a severe
negative effect on the model's performance. We propose a novel methodology
using a semi-supervised Siamese network to identify bad data. This method
requires only a small pool of 'reference' medical images to be reviewed by a
non-expert human to ensure the major anatomical structures are present in the
Field of View. The model trains on this reference set and identifies bad data
by using the Siamese network to compute the distance between the reference set
and all other medical images in the dataset. This methodology achieves an Area
Under the Curve (AUC) of 0.989 for identifying bad data. Code will be available
at this https URL.

    

### [[2108.07134] Neural Predictive Monitoring under Partial Observabilit](http://arxiv.org/abs/2108.07134)


  We consider the problem of predictive monitoring (PM), i.e., predicting at
runtime future violations of a system from the current state. We work under the
most realistic settings where only partial and noisy observations of the state
are available at runtime. Such settings directly affect the accuracy and
reliability of the reachability predictions, jeopardizing the safety of the
system. In this work, we present a learning-based method for PM that produces
accurate and reliable reachability predictions despite partial observability
(PO). We build on Neural Predictive Monitoring (NPM), a PM method that uses
deep neural networks for approximating hybrid systems reachability, and extend
it to the PO case. We propose and compare two solutions, an end-to-end
approach, which directly operates on the rough observations, and a two-step
approach, which introduces an intermediate state estimation step. Both
solutions rely on conformal prediction to provide 1) probabilistic guarantees
in the form of prediction regions and 2) sound estimates of predictive
uncertainty. We use the latter to identify unreliable (and likely erroneous)
predictions and to retrain and improve the monitors on these uncertain inputs
(i.e., active learning). Our method results in highly accurate reachability
predictions and error detection, as well as tight prediction regions with
guaranteed coverage.

    

### [[2108.07135] Vehicle-counting with Automatic Region-of-Interest and Driving-Trajectory detection](http://arxiv.org/abs/2108.07135)


  Vehicle counting systems can help with vehicle analysis and traffic incident
detection. Unfortunately, most existing methods require some level of human
input to identify the Region of interest (ROI), movements of interest, or to
establish a reference point or line to count vehicles from traffic cameras.
This work introduces a method to count vehicles from traffic videos that
automatically identifies the ROI for the camera, as well as the driving
trajectories of the vehicles. This makes the method feasible to use with
Pan-Tilt-Zoom cameras, which are frequently used in developing countries.
Preliminary results indicate that the proposed method achieves an average
intersection over the union of 57.05% for the ROI and a mean absolute error of
just 17.44% at counting vehicles of the traffic video cameras tested.

    

### [[2108.07139] Efficient Feature Representations for Cricket Data Analysis using Deep Learning based Multi-Modal Fusion Model](http://arxiv.org/abs/2108.07139)


  Data analysis has become a necessity in the modern era of cricket. Everything
from effective team management to match win predictions use some form of
analytics. Meaningful data representations are necessary for efficient analysis
of data. In this study we investigate the use of adaptive (learnable)
embeddings to represent inter-related features (such as players, teams, etc).
The data used for this study is collected from a classical T20 tournament IPL
(Indian Premier League). To naturally facilitate the learning of meaningful
representations of features for accurate data analysis, we formulate a deep
representation learning framework which jointly learns a custom set of
embeddings (which represents our features of interest) through the minimization
of a contrastive loss. We base our objective on a set of classes obtained as a
result of hierarchical clustering on the overall run rate of an innings. It's
been assessed that the framework ensures greater generality in the obtained
embeddings, on top of which a task based analysis of overall run rate
prediction was done to show the reliability of the framework.

    

### [[1801.04062] MINE: Mutual Information Neural Estimation](http://arxiv.org/abs/1801.04062)


  We argue that the estimation of mutual information between high dimensional
continuous random variables can be achieved by gradient descent over neural
networks. We present a Mutual Information Neural Estimator (MINE) that is
linearly scalable in dimensionality as well as in sample size, trainable
through back-prop, and strongly consistent. We present a handful of
applications on which MINE can be used to minimize or maximize mutual
information. We apply MINE to improve adversarially trained generative models.
We also use MINE to implement Information Bottleneck, applying it to supervised
classification; our results demonstrate substantial improvement in flexibility
and performance in these settings.

    

### [[1810.08102] First-order and second-order variants of the gradient descent in a unified framework](http://arxiv.org/abs/1810.08102)


  In this paper, we provide an overview of first-order and second-order
variants of the gradient descent method that are commonly used in machine
learning. We propose a general framework in which 6 of these variants can be
interpreted as different instances of the same approach. They are the vanilla
gradient descent, the classical and generalized Gauss-Newton methods, the
natural gradient descent method, the gradient covariance matrix approach, and
Newton's method. Besides interpreting these methods within a single framework,
we explain their specificities and show under which conditions some of them
coincide.

    

### [[1905.01722] Intra-clip Aggregation for Video Person Re-identification](http://arxiv.org/abs/1905.01722)


  Video-based person re-identification has drawn massive attention in recent
years due to its extensive applications in video surveillance. While deep
learning-based methods have led to significant progress, these methods are
limited by ineffectively using complementary information, which is blamed on
necessary data augmentation in the training process. Data augmentation has been
widely used to mitigate the over-fitting trap and improve the ability of
network representation. However, the previous methods adopt image-based data
augmentation scheme to individually process the input frames, which corrupts
the complementary information between consecutive frames and causes performance
degradation. Extensive experiments on three benchmark datasets demonstrate that
our framework outperforms the most recent state-of-the-art methods. We also
perform cross-dataset validation to prove the generality of our method.

    

### [[1906.09526] Learning Waveform-Based Acoustic Models using Deep Variational Convolutional Neural Networks](http://arxiv.org/abs/1906.09526)


  We investigate the potential of stochastic neural networks for learning
effective waveform-based acoustic models. The waveform-based setting, inherent
to fully end-to-end speech recognition systems, is motivated by several
comparative studies of automatic and human speech recognition that associate
standard non-adaptive feature extraction techniques with information loss which
can adversely affect robustness. Stochastic neural networks, on the other hand,
are a class of models capable of incorporating rich regularization mechanisms
into the learning process. We consider a deep convolutional neural network that
first decomposes speech into frequency sub-bands via an adaptive parametric
convolutional block where filters are specified by cosine modulations of
compactly supported windows. The network then employs standard non-parametric
1D convolutions to extract relevant spectro-temporal patterns while gradually
compressing the structured high dimensional representation generated by the
parametric block. We rely on a probabilistic parametrization of the proposed
neural architecture and learn the model using stochastic variational inference.
This requires evaluation of an analytically intractable integral defining the
Kullback-Leibler divergence term responsible for regularization, for which we
propose an effective approximation based on the Gauss-Hermite quadrature. Our
empirical results demonstrate a superior performance of the proposed approach
over comparable waveform-based baselines and indicate that it could lead to
robustness. Moreover, the approach outperforms a recently proposed deep
convolutional neural network for learning of robust acoustic models with
standard FBANK features.

    

### [[1908.06022] SCARLET-NAS: Bridging the Gap between Stability and Scalability in Weight-sharing Neural Architecture Search](http://arxiv.org/abs/1908.06022)


  To discover powerful yet compact models is an important goal of neural
architecture search. Previous two-stage one-shot approaches are limited by
search space with a fixed depth. It seems handy to include an additional skip
connection in the search space to make depths variable. However, it creates a
large range of perturbation during supernet training and it has difficulty
giving a confident ranking for subnetworks. In this paper, we discover that
skip connections bring about significant feature inconsistency compared with
other operations, which potentially degrades the supernet performance. Based on
this observation, we tackle the problem by imposing an equivariant learnable
stabilizer to homogenize such disparities. Experiments show that our proposed
stabilizer helps to improve the supernet's convergence as well as ranking
performance. With an evolutionary search backend that incorporates the
stabilized supernet as an evaluator, we derive a family of state-of-the-art
architectures, the SCARLET series of several depths, especially SCARLET-A
obtains 76.9% top-1 accuracy on ImageNet. Code is available at
this https URL.

    

### [[1911.09721] Communication-Efficient and Byzantine-Robust Distributed Learning with Error Feedback](http://arxiv.org/abs/1911.09721)


  We develop a communication-efficient distributed learning algorithm that is
robust against Byzantine worker machines. We propose and analyze a distributed
gradient-descent algorithm that performs a simple thresholding based on
gradient norms to mitigate Byzantine failures. We show the (statistical)
error-rate of our algorithm matches that of Yin et al.~\cite{dong}, which uses
more complicated schemes (coordinate-wise median, trimmed mean). Furthermore,
for communication efficiency, we consider a generic class of
$\delta$-approximate compressors from Karimireddi et al.~\cite{errorfeed} that
encompasses sign-based compressors and top-$k$ sparsification. Our algorithm
uses compressed gradients and gradient norms for aggregation and Byzantine
removal respectively. We establish the statistical error rate for non-convex
smooth loss functions. We show that, in certain range of the compression factor
$\delta$, the (order-wise) rate of convergence is not affected by the
compression operation. Moreover, we analyze the compressed gradient descent
algorithm with error feedback (proposed in \cite{errorfeed}) in a distributed
setting and in the presence of Byzantine worker machines. We show that
exploiting error feedback improves the statistical error rate. Finally, we
experimentally validate our results and show good performance in convergence
for convex (least-square regression) and non-convex (neural network training)
problems.

    

### [[1911.11897] AttentionGAN: Unpaired Image-to-Image Translation using Attention-Guided Generative Adversarial Networks](http://arxiv.org/abs/1911.11897)


  State-of-the-art methods in image-to-image translation are capable of
learning a mapping from a source domain to a target domain with unpaired image
data. Though the existing methods have achieved promising results, they still
produce visual artifacts, being able to translate low-level information but not
high-level semantics of input images. One possible reason is that generators do
not have the ability to perceive the most discriminative parts between the
source and target domains, thus making the generated images low quality. In
this paper, we propose a new Attention-Guided Generative Adversarial Networks
(AttentionGAN) for the unpaired image-to-image translation task. AttentionGAN
can identify the most discriminative foreground objects and minimize the change
of the background. The attention-guided generators in AttentionGAN are able to
produce attention masks, and then fuse the generation output with the attention
masks to obtain high-quality target images. Accordingly, we also design a novel
attention-guided discriminator which only considers attended regions. Extensive
experiments are conducted on several generative tasks with eight public
datasets, demonstrating that the proposed method is effective to generate
sharper and more realistic images compared with existing competitive models.
The code is available at this https URL.

    

### [[2002.00558] The Price of Incentivizing Exploration: A Characterization via Thompson Sampling and Sample Complexity](http://arxiv.org/abs/2002.00558)


  We consider incentivized exploration: a version of multi-armed bandits where
the choice of arms is controlled by self-interested agents, and the algorithm
can only issue recommendations. The algorithm controls the flow of information,
and the information asymmetry can incentivize the agents to explore. Prior work
achieves optimal regret rates up to multiplicative factors that become
arbitrarily large depending on the Bayesian priors, and scale exponentially in
the number of arms. A more basic problem of sampling each arm once runs into
similar factors.
We focus on the price of incentives: the loss in performance, broadly
construed, incurred for the sake of incentive-compatibility. We prove that
Thompson Sampling, a standard bandit algorithm, is incentive-compatible if
initialized with sufficiently many data points. The performance loss due to
incentives is therefore limited to the initial rounds when these data points
are collected. The problem is largely reduced to that of sample complexity: how
many rounds are needed? We address this question, providing matching upper and
lower bounds and instantiating them in various corollaries. Typically, the
optimal sample complexity is polynomial in the number of arms and exponential
in the "strength of beliefs".

    

### [[2002.04720] Improving Molecular Design by Stochastic Iterative Target Augmentation](http://arxiv.org/abs/2002.04720)


  Generative models in molecular design tend to be richly parameterized,
data-hungry neural models, as they must create complex structured objects as
outputs. Estimating such models from data may be challenging due to the lack of
sufficient training data. In this paper, we propose a surprisingly effective
self-training approach for iteratively creating additional molecular targets.
We first pre-train the generative model together with a simple property
predictor. The property predictor is then used as a likelihood model for
filtering candidate structures from the generative model. Additional targets
are iteratively produced and used in the course of stochastic EM iterations to
maximize the log-likelihood that the candidate structures are accepted. A
simple rejection (re-weighting) sampler suffices to draw posterior samples
since the generative model is already reasonable after pre-training. We
demonstrate significant gains over strong baselines for both unconditional and
conditional molecular design. In particular, our approach outperforms the
previous state-of-the-art in conditional molecular design by over 10% in
absolute gain. Finally, we show that our approach is useful in other domains as
well, such as program synthesis.

    

### [[2002.09843] An Accuracy-Lossless Perturbation Method for Defending Privacy Attacks in Federated Learning](http://arxiv.org/abs/2002.09843)


  Although federated learning improves privacy of training data by exchanging
local gradients or parameters rather than raw data, the adversary still can
leverage local gradients and parameters to obtain local training data by
launching reconstruction and membership inference attacks. To defend such
privacy attacks, many noises perturbation methods (like differential privacy or
CountSketch matrix) have been widely designed. However, the strong defence
ability and high learning accuracy of these schemes cannot be ensured at the
same time, which will impede the wide application of FL in practice (especially
for medical or financial institutions that require both high accuracy and
strong privacy guarantee). To overcome this issue, in this paper, we propose
\emph{an efficient model perturbation method for federated learning} to defend
reconstruction and membership inference attacks launched by curious clients. On
the one hand, similar to the differential privacy, our method also selects
random numbers as perturbed noises added to the global model parameters, and
thus it is very efficient and easy to be integrated in practice. Meanwhile, the
random selected noises are positive real numbers and the corresponding value
can be arbitrarily large, and thus the strong defence ability can be ensured.
On the other hand, unlike differential privacy or other perturbation methods
that cannot eliminate the added noises, our method allows the server to recover
the true gradients by eliminating the added noises. Therefore, our method does
not hinder learning accuracy at all.

    

### [[2002.11304] PaDGAN: A Generative Adversarial Network for Performance Augmented Diverse Designs](http://arxiv.org/abs/2002.11304)


  Deep generative models are proven to be a useful tool for automatic design
synthesis and design space exploration. When applied in engineering design,
existing generative models face three challenges: 1) generated designs lack
diversity and do not cover all areas of the design space, 2) it is difficult to
explicitly improve the overall performance or quality of generated designs, and
3) existing models generally do not generate novel designs, outside the domain
of the training data. In this paper, we simultaneously address these challenges
by proposing a new Determinantal Point Processes based loss function for
probabilistic modeling of diversity and quality. With this new loss function,
we develop a variant of the Generative Adversarial Network, named "Performance
Augmented Diverse Generative Adversarial Network" or PaDGAN, which can generate
novel high-quality designs with good coverage of the design space. Using three
synthetic examples and one real-world airfoil design example, we demonstrate
that PaDGAN can generate diverse and high-quality designs. In comparison to a
vanilla Generative Adversarial Network, on average, it generates samples with a
28% higher mean quality score with larger diversity and without the mode
collapse issue. Unlike typical generative models that usually generate new
designs by interpolating within the boundary of training data, we show that
PaDGAN expands the design space boundary outside the training data towards
high-quality regions. The proposed method is broadly applicable to many tasks
including design space exploration, design optimization, and creative solution
recommendation.

    

### [[2003.06814] Towards Face Encryption by Generating Adversarial Identity Masks](http://arxiv.org/abs/2003.06814)


  As billions of personal data being shared through social media and network,
the data privacy and security have drawn an increasing attention. Several
attempts have been made to alleviate the leakage of identity information from
face photos, with the aid of, e.g., image obfuscation techniques. However, most
of the present results are either perceptually unsatisfactory or ineffective
against face recognition systems. Our goal in this paper is to develop a
technique that can encrypt the personal photos such that they can protect users
from unauthorized face recognition systems but remain visually identical to the
original version for human beings. To achieve this, we propose a targeted
identity-protection iterative method (TIP-IM) to generate adversarial identity
masks which can be overlaid on facial images, such that the original identities
can be concealed without sacrificing the visual quality. Extensive experiments
demonstrate that TIP-IM provides 95\%+ protection success rate against various
state-of-the-art face recognition models under practical test scenarios.
Besides, we also show the practical and effective applicability of our method
on a commercial API service.

    

### [[2004.13446] MultiMBNN: Matched and Balanced Causal Inference with Neural Networks](http://arxiv.org/abs/2004.13446)


  Causal inference (CI) in observational studies has received a lot of
attention in healthcare, education, ad attribution, policy evaluation, etc.
Confounding is a typical hazard, where the context affects both, the treatment
assignment and response. In a multiple treatment scenario, we propose the
neural network based MultiMBNN, where we overcome confounding by employing
generalized propensity score based matching, and learning balanced
representations. We benchmark the performance on synthetic and real-world
datasets using PEHE, and mean absolute percentage error over ATE as metrics.
MultiMBNN outperforms the state-of-the-art algorithms for CI such as TARNet and
Perfect Match (PM).

    

### [[2005.01699] Guarantees on learning depth-2 neural networks under a data-poisoning attack](http://arxiv.org/abs/2005.01699)


  In this work, we study the possibility of defending against "data-poisoning"
attacks while learning a neural net. We focus on the supervised learning setup
for a class of finite-sized depth-2 nets - which include the standard single
filter convolutional nets. For this setup we attempt to learn the true label
generating weights in the presence of a malicious oracle doing stochastic
bounded and additive adversarial distortions on the true labels being accessed
by the algorithm during training. For the non-gradient stochastic algorithm
that we instantiate we prove (worst case nearly optimal) trade-offs among the
magnitude of the adversarial attack, the accuracy, and the confidence achieved
by the proposed algorithm. Additionally, our algorithm uses mini-batching and
we keep track of how the mini-batch size affects the convergence.

    

### [[2006.02854] Analogical Proportions](http://arxiv.org/abs/2006.02854)


  Analogy-making is at the core of human and artificial intelligence and
creativity. This paper introduces from first principles an abstract algebraic
framework of analogical proportions of the form `$a$ is to $b$ what $c$ is to
$d$' in the general setting of universal algebra. This enables us to compare
mathematical objects possibly across different domains in a uniform way which
is crucial for AI-systems. The main idea is to define solutions to analogical
equations in terms of maximal sets of algebraic justifications, which amounts
to deriving abstract terms of concrete elements from a `known' source domain
which can then be instantiated in an `unknown' target domain to obtain
analogous elements. It turns out that our notion of analogical proportions has
appealing mathematical properties. We compare our framework with two recently
introduced frameworks of analogical proportions from the literature in the
concrete domains of sets and numbers, and we show that in each case we either
disagree with the notion from the literature justified by some counter-example
or we can show that our model yields strictly more solutions. As we construct
our model from first principles using only elementary concepts of universal
algebra, and since our model questions some basic properties of analogical
proportions presupposed in the literature, to convince the reader of the
plausibility of our model we show that it can be naturally embedded into
first-order logic via model-theoretic types, and prove that analogical
proportions are compatible with structure-preserving mappings from that
perspective. This provides strong evidence for its applicability. In a broader
sense, this paper is a first step towards a theory of analogical reasoning and
learning systems with potential applications to fundamental AI-problems like
commonsense reasoning and computational learning and creativity.

    

### [[2006.05648] Evaluating Graph Vulnerability and Robustness using TIGER](http://arxiv.org/abs/2006.05648)


  Network robustness plays a crucial role in our understanding of complex
interconnected systems such as transportation, communication, and computer
networks. While significant research has been conducted in the area of network
robustness, no comprehensive open-source toolbox currently exists to assist
researchers and practitioners in this important topic. This lack of available
tools hinders reproducibility and examination of existing work, development of
new research, and dissemination of new ideas. We contribute TIGER, an
open-sourced Python toolbox to address these challenges. TIGER contains 22
graph robustness measures with both original and fast approximate versions; 17
failure and attack strategies; 15 heuristic and optimization-based defense
techniques; and 4 simulation tools. By democratizing the tools required to
study network robustness, our goal is to assist researchers and practitioners
in analyzing their own networks; and facilitate the development of new research
in the field. TIGER has been integrated into the Nvidia Data Science Teaching
Kit available to educators across the world; and Georgia Tech's Data and Visual
Analytics class with over 1,000 students. TIGER is open sourced at:
this https URL


### [[2006.10600] Efficient Hyperparameter Optimization under Multi-Source Covariate Shift](http://arxiv.org/abs/2006.10600)


  A typical assumption in supervised machine learning is that the train
(source) and test (target) datasets follow completely the same distribution.
This assumption is, however, often violated in uncertain real-world
applications, which motivates the study of learning under covariate shift. In
this setting, the naive use of adaptive hyperparameter optimization methods
such as Bayesian optimization does not work as desired since it does not
address the distributional shift among different datasets. In this work, we
consider a novel hyperparameter optimization problem under the multi-source
covariate shift whose goal is to find the optimal hyperparameters for a target
task of interest using only unlabeled data in a target task and labeled data in
multiple source tasks. To conduct efficient hyperparameter optimization for the
target task, it is essential to estimate the target objective using only the
available information. To this end, we construct the variance reduced estimator
that unbiasedly approximates the target objective with a desirable variance
property. Building on the proposed estimator, we provide a general and
tractable hyperparameter optimization procedure, which works preferably in our
setting with a no-regret guarantee. The experiments demonstrate that the
proposed framework broadens the applications of automated hyperparameter
optimization.

    

### [[2007.01777] Interpretable Sequence Classification Via Prototype Trajectory](http://arxiv.org/abs/2007.01777)


  We propose a novel interpretable deep neural network for text classification,
called ProtoryNet, based on a new concept of prototype trajectories. Motivated
by the prototype theory in modern linguistics, ProtoryNet makes a prediction by
finding the most similar prototype for each sentence in a text sequence and
feeding an RNN backbone with the proximity of each sentence to the
corresponding active prototype. The RNN backbone then captures the temporal
pattern of the prototypes, which we refer to as prototype trajectories.
Prototype trajectories enable intuitive and fine-grained interpretation of the
reasoning process of the RNN model, in resemblance to how humans analyze texts.
We also design a prototype pruning procedure to reduce the total number of
prototypes used by the model for better interpretability. Experiments on
multiple public data sets show that ProtoryNet is more accurate than the
baseline prototype-based deep neural net and reduces the performance gap
compared to state-of-the-art black-box models. In addition, after prototype
pruning, the resulting ProtoryNet models only need less than or around 20
prototypes for all datasets, which significantly benefits interpretability.
Furthermore, we report a survey result indicating that human users find
ProtoryNet more intuitive and easier to understand than other prototype-based
methods.

    

### [[2007.03260] ResRep: Lossless CNN Pruning via Decoupling Remembering and Forgetting](http://arxiv.org/abs/2007.03260)


  We propose ResRep, a novel method for lossless channel pruning (a.k.a. filter
pruning), which slims down a CNN by reducing the width (number of output
channels) of convolutional layers. Inspired by the neurobiology research about
the independence of remembering and forgetting, we propose to re-parameterize a
CNN into the remembering parts and forgetting parts, where the former learn to
maintain the performance and the latter learn to prune. Via training with
regular SGD on the former but a novel update rule with penalty gradients on the
latter, we realize structured sparsity. Then we equivalently merge the
remembering and forgetting parts into the original architecture with narrower
layers. In this sense, ResRep can be viewed as a successful application of
Structural Re-parameterization. Such a methodology distinguishes ResRep from
the traditional learning-based pruning paradigm that applies a penalty on
parameters to produce sparsity, which may suppress the parameters essential for
the remembering. ResRep slims down a standard ResNet-50 with 76.15% accuracy on
ImageNet to a narrower one with only 45% FLOPs and no accuracy drop, which is
the first to achieve lossless pruning with such a high compression ratio. The
code and models are at this https URL.

    

### [[2007.06226] AMITE: A Novel Polynomial Expansion for Analyzing Neural Network Nonlinearities](http://arxiv.org/abs/2007.06226)


  Polynomial expansions are important in the analysis of neural network
nonlinearities. They have been applied thereto addressing well-known
difficulties in verification, explainability, and security. Existing approaches
span classical Taylor and Chebyshev methods, asymptotics, and many numerical
approaches. We find that while these individually have useful properties such
as exact error formulas, adjustable domain, and robustness to undefined
derivatives, there are no approaches that provide a consistent method yielding
an expansion with all these properties. To address this, we develop an
analytically modified integral transform expansion (AMITE), a novel expansion
via integral transforms modified using derived criteria for convergence. We
show the general expansion and then demonstrate application for two popular
activation functions, hyperbolic tangent and rectified linear units. Compared
with existing expansions (i.e., Chebyshev, Taylor, and numerical) employed to
this end, AMITE is the first to provide six previously mutually exclusive
desired expansion properties such as exact formulas for the coefficients and
exact expansion errors (Table II). We demonstrate the effectiveness of AMITE in
two case studies. First, a multivariate polynomial form is efficiently
extracted from a single hidden layer black-box MLP to facilitate equivalence
testing from noisy stimulus-response pairs. Second, a variety of FFNN
architectures having between 3 and 7 layers are range bounded using Taylor
models improved by the AMITE polynomials and error formulas. AMITE presents a
new dimension of expansion methods suitable for analysis/approximation of
nonlinearities in neural networks, opening new directions and opportunities for
the theoretical analysis and systematic testing of neural networks.

    

### [[2007.12066] A Computation-Efficient CNN System for High-Quality Brain Tumor Segmentation](http://arxiv.org/abs/2007.12066)


  The work presented in this paper is to propose a reliable high-quality system
of Convolutional Neural Network (CNN) for brain tumor segmentation with a low
computation requirement. The system consists of a CNN for the main processing
for the segmentation, a pre-CNN block for data reduction and post-CNN
refinement block. The unique CNN consists of 7 convolution layers involving
only 108 kernels and 20308 trainable parameters. It is custom-designed,
following the proposed paradigm of ASCNN (application specific CNN), to perform
mono-modality and cross-modality feature extraction, tumor localization and
pixel classification. Each layer fits the task assigned to it, by means of (i)
appropriate normalization applied to its input data, (ii) correct convolution
modes for the assigned task, and (iii) suitable nonlinear transformation to
optimize the convolution results. In this specific design context, the number
of kernels in each of the 7 layers is made to be just-sufficient for its task,
instead of exponentially growing over the layers, to increase information
density and to reduce randomness in the processing. The proposed activation
function Full-ReLU helps to halve the number of kernels in convolution layers
of high-pass filtering without degrading processing quality. A large number of
experiments with BRATS2018 dataset have been conducted to measure the
processing quality and reproducibility of the proposed system. The results
demonstrate that the system reproduces reliably almost the same output to the
same input after retraining. The mean dice scores for enhancing tumor, whole
tumor and tumor core are 77.2%, 89.2% and 76.3%, respectively. The simple
structure and reliable high processing quality of the proposed system will
facilitate its implementation and medical applications.

    

### [[2008.09264] CITISEN: A Deep Learning-Based Speech Signal-Processing Mobile Application](http://arxiv.org/abs/2008.09264)


  In this paper, we present a deep learning-based speech signal-processing
mobile application, CITISEN, which can perform three functions: speech
enhancement (SE), acoustic scene conversion (ASC), and model adaptation (MA).
For SE, CITISEN can effectively reduce noise components from speech signals and
accordingly enhance their clarity and intelligibility. For ASC, CITISEN can
convert the current background sound to a different background sound. Finally,
for MA, CITISEN can effectively adapt an SE model, with a few audio files, when
it encounters unknown speakers or noise types; the adapted SE model is used to
enhance the upcoming noisy utterances. Experimental results confirmed the
effectiveness of CITISEN in performing these three functions via objective
evaluation and subjective listening tests. The promising results reveal that
the developed CITISEN mobile application can potentially be used as a front-end
processor for various speech-related services such as voice communication,
assistive hearing devices, and virtual reality headsets.

    

### [[2009.05224] HAA500: Human-Centric Atomic Action Dataset with Curated Videos](http://arxiv.org/abs/2009.05224)


  We contribute HAA500, a manually annotated human-centric atomic action
dataset for action recognition on 500 classes with over 591K labeled frames. To
minimize ambiguities in action classification, HAA500 consists of highly
diversified classes of fine-grained atomic actions, where only consistent
actions fall under the same label, e.g., "Baseball Pitching" vs "Free Throw in
Basketball". Thus HAA500 is different from existing atomic action datasets,
where coarse-grained atomic actions were labeled with coarse action-verbs such
as "Throw". HAA500 has been carefully curated to capture the precise movement
of human figures with little class-irrelevant motions or spatio-temporal label
noises. The advantages of HAA500 are fourfold: 1) human-centric actions with a
high average of 69.7% detectable joints for the relevant human poses; 2) high
scalability since adding a new class can be done under 20-60 minutes; 3)
curated videos capturing essential elements of an atomic action without
irrelevant frames; 4) fine-grained atomic action classes. Our extensive
experiments including cross-data validation using datasets collected in the
wild demonstrate the clear benefits of human-centric and atomic characteristics
of HAA500, which enable training even a baseline deep learning model to improve
prediction by attending to atomic human poses. We detail the HAA500 dataset
statistics and collection methodology and compare quantitatively with existing
action recognition datasets.

    

### [[2009.08435] Large Norms of CNN Layers Do Not Hurt Adversarial Robustness](http://arxiv.org/abs/2009.08435)


  Since the Lipschitz properties of convolutional neural networks (CNNs) are
widely considered to be related to adversarial robustness, we theoretically
characterize the $\ell_1$ norm and $\ell_\infty$ norm of 2D multi-channel
convolutional layers and provide efficient methods to compute the exact
$\ell_1$ norm and $\ell_\infty$ norm. Based on our theorem, we propose a novel
regularization method termed norm decay, which can effectively reduce the norms
of convolutional layers and fully-connected layers. Experiments show that
norm-regularization methods, including norm decay, weight decay, and singular
value clipping, can improve generalization of CNNs. However, they can slightly
hurt adversarial robustness. Observing this unexpected phenomenon, we compute
the norms of layers in the CNNs trained with three different adversarial
training frameworks and surprisingly find that adversarially robust CNNs have
comparable or even larger layer norms than their non-adversarially robust
counterparts. Furthermore, we prove that under a mild assumption, adversarially
robust classifiers can be achieved using neural networks, and an adversarially
robust neural network can have an arbitrarily large Lipschitz constant. For
this reason, enforcing small norms on CNN layers may be neither necessary nor
effective in achieving adversarial robustness. The code is available at
this https URL.

    

### [[2009.08626] Identification of Abnormal States in Videos of Ants Undergoing Social Phase Change](http://arxiv.org/abs/2009.08626)


  Biology is both an important application area and a source of motivation for
development of advanced machine learning techniques. Although much attention
has been paid to large and complex data sets resulting from high-throughput
sequencing, advances in high-quality video recording technology have begun to
generate similarly rich data sets requiring sophisticated techniques from both
computer vision and time-series analysis. Moreover, just as studying gene
expression patterns in one organism can reveal general principles that apply to
other organisms, the study of complex social interactions in an experimentally
tractable model system, such as a laboratory ant colony, can provide general
principles about the dynamics of other social groups. Here, we focus on one
such example from the study of reproductive regulation in small laboratory
colonies of more than 50 Harpegnathos ants. These ants can be artificially
induced to begin a ~20 day process of hierarchy reformation. Although the
conclusion of this process is conspicuous to a human observer, it remains
unclear which behaviors during the transient period are contributing to the
process. To address this issue, we explore the potential application of
One-class Classification (OC) to the detection of abnormal states in ant
colonies for which behavioral data is only available for the normal societal
conditions during training. Specifically, we build upon the Deep Support Vector
Data Description (DSVDD) and introduce the Inner-Outlier Generator (IO-GEN)
that synthesizes fake "inner outlier" observations during training that are
near the center of the DSVDD data description. We show that IO-GEN increases
the reliability of the final OC classifier relative to other DSVDD baselines.
This method can be used to screen video frames for which additional human
observation is needed.

    

### [[2009.09823] Latent State Inference in a Spatiotemporal Generative Model](http://arxiv.org/abs/2009.09823)


  Knowledge about the hidden factors that determine particular system dynamics
is crucial for both explaining them and pursuing goal-directed interventions.
Inferring these factors from time series data without supervision remains an
open challenge. Here, we focus on spatiotemporal processes, including wave
propagation and weather dynamics, for which we assume that universal causes
(e.g. physics) apply throughout space and time. A recently introduced
DIstributed SpatioTemporal graph Artificial Neural network Architecture
(DISTANA) is used and enhanced to learn such processes, requiring fewer
parameters and achieving significantly more accurate predictions compared to
temporal convolutional neural networks and other related approaches. We show
that DISTANA, when combined with a retrospective latent state inference
principle called active tuning, can reliably derive location-respective hidden
causal factors. In a current weather prediction benchmark, DISTANA infers our
planet's land-sea mask solely by observing temperature dynamics and, meanwhile,
uses the self inferred information to improve its own future temperature
predictions.

    

### [[2010.01279] Do Wider Neural Networks Really Help Adversarial Robustness?](http://arxiv.org/abs/2010.01279)


  Adversarial training is a powerful type of defense against adversarial
examples. Previous empirical results suggest that adversarial training requires
wider networks for better performances. However, it remains elusive how neural
network width affects model robustness. In this paper, we carefully examine the
relationship between network width and model robustness. Specifically, we show
that the model robustness is closely related to the tradeoff between natural
accuracy and perturbation stability, which is controlled by the robust
regularization parameter $\lambda$. With the same $\lambda$, wider networks can
achieve better natural accuracy but worse perturbation stability, leading to a
potentially worse overall model robustness. To understand the origin of this
phenomenon, we further relate the perturbation stability with the network's
local Lipschitzness. By leveraging recent results on neural tangent kernels, we
theoretically show that wider networks tend to have worse perturbation
stability. Our analyses suggest that: 1) the common strategy of first
fine-tuning $\lambda$ on small networks and then directly use it for wide model
training could lead to deteriorated model robustness; 2) one needs to properly
enlarge $\lambda$ to unleash the robustness potential of wider models fully.
Finally, we propose a new Width Adjusted Regularization (WAR) method that
adaptively enlarges $\lambda$ on wide models and significantly saves the tuning
time.

    

### [[2010.02068] Deep Reinforcement Learning for Electric Vehicle Routing Problem with Time Windows](http://arxiv.org/abs/2010.02068)


  The past decade has seen a rapid penetration of electric vehicles (EV) in the
market, more and more logistics and transportation companies start to deploy
EVs for service provision. In order to model the operations of a commercial EV
fleet, we utilize the EV routing problem with time windows (EVRPTW). In this
research, we propose an end-to-end deep reinforcement learning framework to
solve the EVRPTW. In particular, we develop an attention model incorporating
the pointer network and a graph embedding technique to parameterize a
stochastic policy for solving the EVRPTW. The model is then trained using
policy gradient with rollout baseline. Our numerical studies show that the
proposed model is able to efficiently solve EVRPTW instances of large sizes
that are not solvable with any existing approaches.

    

### [[2010.02164] A Streaming Approach For Efficient Batched Beam Search](http://arxiv.org/abs/2010.02164)


  We propose an efficient batching strategy for variable-length decoding on GPU
architectures. During decoding, when candidates terminate or are pruned
according to heuristics, our streaming approach periodically "refills" the
batch before proceeding with a selected subset of candidates. We apply our
method to variable-width beam search on a state-of-the-art machine translation
model. Our method decreases runtime by up to 71% compared to a fixed-width beam
search baseline and 17% compared to a variable-width baseline, while matching
baselines' BLEU. Finally, experiments show that our method can speed up
decoding in other domains, such as semantic and syntactic parsing.

    

### [[2010.15979] A Greedy Algorithm for Quantizing Neural Networks](http://arxiv.org/abs/2010.15979)


  We propose a new computationally efficient method for quantizing the weights
of pre- trained neural networks that is general enough to handle both
multi-layer perceptrons and convolutional neural networks. Our method
deterministically quantizes layers in an iterative fashion with no complicated
re-training required. Specifically, we quantize each neuron, or hidden unit,
using a greedy path-following algorithm. This simple algorithm is equivalent to
running a dynamical system, which we prove is stable for quantizing a
single-layer neural network (or, alternatively, for quantizing the first layer
of a multi-layer network) when the training data are Gaussian. We show that
under these assumptions, the quantization error decays with the width of the
layer, i.e., its level of over-parametrization. We provide numerical
experiments, on multi-layer networks, to illustrate the performance of our
methods on MNIST and CIFAR10 data, as well as for quantizing the VGG16 network
using ImageNet data.

    

### [[2011.00109] Knowledge-Based Construction of Confusion Matrices for Multi-Label Classification Algorithms using Semantic Similarity Measures](http://arxiv.org/abs/2011.00109)


  So far, multi-label classification algorithms have been evaluated using
statistical methods that do not consider the semantics of the considered
classes and that fully depend on abstract computations such as Bayesian
Reasoning. Currently, there are several attempts to develop ontology-based
methods for a better assessment of supervised classification algorithms. In
this research paper, we define a novel approach that aligns expected labels
with predicted labels in multi-label classification using ontology-driven
feature-based semantic similarity measures and we use it to develop a method
for creating precise confusion matrices for a more effective evaluation of
multi-label classification algorithms.

    

### [[2011.01354] Unsupervised Monocular Depth Learning with Integrated Intrinsics and Spatio-Temporal Constraints](http://arxiv.org/abs/2011.01354)


  Monocular depth inference has gained tremendous attention from researchers in
recent years and remains as a promising replacement for expensive
time-of-flight sensors, but issues with scale acquisition and implementation
overhead still plague these systems. To this end, this work presents an
unsupervised learning framework that is able to predict at-scale depth maps and
egomotion, in addition to camera intrinsics, from a sequence of monocular
images via a single network. Our method incorporates both spatial and temporal
geometric constraints to resolve depth and pose scale factors, which are
enforced within the supervisory reconstruction loss functions at training time.
Only unlabeled stereo sequences are required for training the weights of our
single-network architecture, which reduces overall implementation overhead as
compared to previous methods. Our results demonstrate strong performance when
compared to the current state-of-the-art on multiple sequences of the KITTI
driving dataset and can provide faster training times with its reduced network
complexity.

    

### [[2011.01424] Distilling Knowledge by Mimicking Features](http://arxiv.org/abs/2011.01424)


  Knowledge distillation (KD) is a popular method to train efficient networks
("student") with the help of high-capacity networks ("teacher"). Traditional
methods use the teacher's soft logits as extra supervision to train the student
network. In this paper, we argue that it is more advantageous to make the
student mimic the teacher's features in the penultimate layer. Not only the
student can directly learn more effective information from the teacher feature,
feature mimicking can also be applied for teachers trained without a softmax
layer. Experiments show that it can achieve higher accuracy than traditional
KD. To further facilitate feature mimicking, we decompose a feature vector into
the magnitude and the direction. We argue that the teacher should give more
freedom to the student feature's magnitude, and let the student pay more
attention on mimicking the feature direction. To meet this requirement, we
propose a loss term based on locality-sensitive hashing (LSH). With the help of
this new loss, our method indeed mimics feature directions more accurately,
relaxes constraints on feature magnitudes, and achieves state-of-the-art
distillation accuracy. We provide theoretical analyses of how LSH facilitates
feature direction mimicking, and further extend feature mimicking to
multi-label recognition and object detection.

    

### [[2011.08434] Simple and optimal methods for stochastic variational inequalities, II: Markovian noise and policy evaluation in reinforcement learning](http://arxiv.org/abs/2011.08434)


  The focus of this paper is on stochastic variational inequalities (VI) under
Markovian noise. A prominent application of our algorithmic developments is the
stochastic policy evaluation problem in reinforcement learning. Prior
investigations in the literature focused on temporal difference (TD) learning
by employing nonsmooth finite time analysis motivated by stochastic subgradient
descent leading to certain limitations. These encompass the requirement of
analyzing a modified TD algorithm that involves projection to an a-priori
defined Euclidean ball, achieving a non-optimal convergence rate and no clear
way of deriving the beneficial effects of parallel implementation. Our approach
remedies these shortcomings in the broader context of stochastic VIs and in
particular when it comes to stochastic policy evaluation. We developed a
variety of simple TD learning type algorithms motivated by its original version
that maintain its simplicity, while offering distinct advantages from a
non-asymptotic analysis point of view. We first provide an improved analysis of
the standard TD algorithm that can benefit from parallel implementation. Then
we present versions of a conditional TD algorithm (CTD), that involves periodic
updates of the stochastic iterates, which reduce the bias and therefore exhibit
improved iteration complexity. This brings us to the fast TD (FTD) algorithm
which combines elements of CTD and the stochastic operator extrapolation method
of the companion paper. For a novel index resetting policy FTD exhibits the
best known convergence rate. We also devised a robust version of the algorithm
that is particularly suitable for discounting factors close to 1.

    

### [[2012.01118] Neural Teleportation](http://arxiv.org/abs/2012.01118)


  In this paper, we explore a process called neural teleportation, a
mathematical consequence of applying quiver representation theory to neural
networks. Neural teleportation "teleports" a network to a new position in the
weight space and preserves its function. This phenomenon comes directly from
the definitions of representation theory applied to neural networks and it
turns out to be a very simple operation that has remarkable properties. We shed
light on surprising and counter-intuitive consequences neural teleportation has
on the loss landscape. In particular, we show that teleportation can be used to
explore loss level curves, that it changes the local loss landscape, sharpens
global minima and boosts back-propagated gradients at any moment during the
learning process. Our results can be reproduced with the code available here:
this https URL


### [[2012.01499] Instance-Sensitive Algorithms for Pure Exploration in Multinomial Logit Bandit](http://arxiv.org/abs/2012.01499)


  Motivated by real-world applications such as fast fashion retailing and
online advertising, the Multinomial Logit Bandit (MNL-bandit) is a popular
model in online learning and operations research, and has attracted much
attention in the past decade. However, it is a bit surprising that pure
exploration, a basic problem in bandit theory, has not been well studied in
MNL-bandit so far. In this paper we give efficient algorithms for pure
exploration in MNL-bandit. Our algorithms achieve instance-sensitive pull
complexities. We also complement the upper bounds by an almost matching lower
bound.

    

### [[2012.08112] Amata: An Annealing Mechanism for Adversarial Training Acceleration](http://arxiv.org/abs/2012.08112)


  Despite the empirical success in various domains, it has been revealed that
deep neural networks are vulnerable to maliciously perturbed input data that
much degrade their performance. This is known as adversarial attacks. To
counter adversarial attacks, adversarial training formulated as a form of
robust optimization has been demonstrated to be effective. However, conducting
adversarial training brings much computational overhead compared with standard
training. In order to reduce the computational cost, we propose an annealing
mechanism, Amata, to reduce the overhead associated with adversarial training.
The proposed Amata is provably convergent, well-motivated from the lens of
optimal control theory and can be combined with existing acceleration methods
to further enhance performance. It is demonstrated that on standard datasets,
Amata can achieve similar or better robustness with around 1/3 to 1/2 the
computational time compared with traditional methods. In addition, Amata can be
incorporated into other adversarial training acceleration algorithms (e.g.
YOPO, Free, Fast, and ATTA), which leads to further reduction in computational
time on large-scale problems.

    

### [[2101.01669] A Survey of Community Detection Approaches: From Statistical Modeling to Deep Learning](http://arxiv.org/abs/2101.01669)


  Community detection, a fundamental task for network analysis, aims to
partition a network into multiple sub-structures to help reveal their latent
functions. Community detection has been extensively studied in and broadly
applied to many real-world network problems. Classical approaches to community
detection typically utilize probabilistic graphical models and adopt a variety
of prior knowledge to infer community structures. As the problems that network
methods try to solve and the network data to be analyzed become increasingly
more sophisticated, new approaches have also been proposed and developed,
particularly those that utilize deep learning and convert networked data into
low dimensional representation. Despite all the recent advancement, there is
still a lack of insightful understanding of the theoretical and methodological
underpinning of community detection, which will be critically important for
future development of the area of network analysis. In this paper, we develop
and present a unified architecture of network community-finding methods to
characterize the state-of-the-art of the field of community detection.
Specifically, we provide a comprehensive review of the existing community
detection methods and introduce a new taxonomy that divides the existing
methods into two categories, namely probabilistic graphical model and deep
learning. We then discuss in detail the main idea behind each method in the two
categories. Furthermore, to promote future development of community detection,
we release several benchmark datasets from several problem domains and
highlight their applications to various network analysis tasks. We conclude
with discussions of the challenges of the field and suggestions of possible
directions for future research.

    

### [[2101.06395] Free Lunch for Few-shot Learning: Distribution Calibration](http://arxiv.org/abs/2101.06395)


  Learning from a limited number of samples is challenging since the learned
model can easily become overfitted based on the biased distribution formed by
only a few training examples. In this paper, we calibrate the distribution of
these few-sample classes by transferring statistics from the classes with
sufficient examples, then an adequate number of examples can be sampled from
the calibrated distribution to expand the inputs to the classifier. We assume
every dimension in the feature representation follows a Gaussian distribution
so that the mean and the variance of the distribution can borrow from that of
similar classes whose statistics are better estimated with an adequate number
of samples. Our method can be built on top of off-the-shelf pretrained feature
extractors and classification models without extra parameters. We show that a
simple logistic regression classifier trained using the features sampled from
our calibrated distribution can outperform the state-of-the-art accuracy on two
datasets (~5% improvement on miniImageNet compared to the next best). The
visualization of these generated features demonstrates that our calibrated
distribution is an accurate estimation.

    

### [[2102.02811] CrossNorm and SelfNorm for Generalization under Distribution Shifts](http://arxiv.org/abs/2102.02811)


  Traditional normalization techniques (e.g., Batch Normalization and Instance
Normalization) generally and simplistically assume that training and test data
follow the same distribution. As distribution shifts are inevitable in
real-world applications, well-trained models with previous normalization
methods can perform badly in new environments. Can we develop new normalization
methods to improve generalization robustness under distribution shifts? In this
paper, we answer the question by proposing CrossNorm and SelfNorm. CrossNorm
exchanges channel-wise mean and variance between feature maps to enlarge
training distribution, while SelfNorm uses attention to recalibrate the
statistics to bridge gaps between training and test distributions. CrossNorm
and SelfNorm can complement each other, though exploring different directions
in statistics usage. Extensive experiments on different fields (vision and
language), tasks (classification and segmentation), settings (supervised and
semi-supervised), and distribution shift types (synthetic and natural) show the
effectiveness. Code is available at
this https URL


### [[2102.03932] Automatic Breast Lesion Detection in Ultrafast DCE-MRI Using Deep Learning](http://arxiv.org/abs/2102.03932)


  Purpose: We propose a deep learning-based computer-aided detection (CADe)
method to detect breast lesions in ultrafast DCE-MRI sequences. This method
uses both the three-dimensional spatial information and temporal information
obtained from the early-phase of the dynamic acquisition. Methods: The proposed
CADe method, based on a modified 3D RetinaNet model, operates on ultrafast T1
weighted sequences, which are preprocessed for motion compensation, temporal
normalization, and are cropped before passing into the model. The model is
optimized to enable the detection of relatively small breast lesions in a
screening setting, focusing on detection of lesions that are harder to
differentiate from confounding structures inside the breast. Results: The
method was developed based on a dataset consisting of 489 ultrafast MRI studies
obtained from 462 patients containing a total of 572 lesions (365 malignant,
207 benign) and achieved a detection rate, sensitivity, and detection rate of
benign lesions of 0.90 (0.876-0.934), 0.95 (0.934-0.980), and 0.81
(0.751-0.871) at 4 false positives per normal breast with 10-fold
cross-testing, respectively. Conclusions: The deep learning architecture used
for the proposed CADe application can efficiently detect benign and malignant
lesions on ultrafast DCE-MRI. Furthermore, utilizing the less visible hard-to
detect-lesions in training improves the learning process and, subsequently,
detection of malignant breast lesions.

    

### [[2102.07358] Weak Adaptation Learning -- Addressing Cross-domain Data Insufficiency with Weak Annotator](http://arxiv.org/abs/2102.07358)


  Data quantity and quality are crucial factors for data-driven learning
methods. In some target problem domains, there are not many data samples
available, which could significantly hinder the learning process. While data
from similar domains may be leveraged to help through domain adaptation,
obtaining high-quality labeled data for those source domains themselves could
be difficult or costly. To address such challenges on data insufficiency for
classification problem in a target domain, we propose a weak adaptation
learning (WAL) approach that leverages unlabeled data from a similar source
domain, a low-cost weak annotator that produces labels based on task-specific
heuristics, labeling rules, or other methods (albeit with inaccuracy), and a
small amount of labeled data in the target domain. Our approach first conducts
a theoretical analysis on the error bound of the trained classifier with
respect to the data quantity and the performance of the weak annotator, and
then introduces a multi-stage weak adaptation learning method to learn an
accurate classifier by lowering the error bound. Our experiments demonstrate
the effectiveness of our approach in learning an accurate classifier with
limited labeled data in the target domain and unlabeled data in the source
domain.

    

### [[2102.08517] Transferability of Neural Network-based De-identification Systems](http://arxiv.org/abs/2102.08517)


  Methods and Materials: We investigated transferability of neural
network-based de-identification sys-tems with and without domain
generalization. We used two domain generalization approaches: a novel approach
Joint-Domain Learning (JDL) as developed in this paper, and a state-of-the-art
domain general-ization approach Common-Specific Decomposition (CSD) from the
literature. First, we measured trans-ferability from a single external source.
Second, we used two external sources and evaluated whether domain
generalization can improve transferability of de-identification models across
domains which rep-resent different note types from the same institution. Third,
using two external sources with in-domain training data, we studied whether
external source data are useful even in cases where sufficient in-domain
training data are available. Finally, we investigated transferability of the
de-identification mod-els across institutions. Results and Conclusions: We
found transferability from a single external source gave inconsistent re-sults.
Using additional external sources consistently yielded an F1-score of
approximately 80%, but domain generalization was not always helpful to improve
transferability. We also found that external sources were useful even in cases
where in-domain training data were available by reducing the amount of needed
in-domain training data or by improving performance. Transferability across
institutions was differed by note type and annotation label. External sources
from a different institution were also useful to further improve performance.

    

### [[2103.03330] Large Graph Convolutional Network Training with GPU-Oriented Data Communication Architecture](http://arxiv.org/abs/2103.03330)


  Graph Convolutional Networks (GCNs) are increasingly adopted in large-scale
graph-based recommender systems. Training GCN requires the minibatch generator
traversing graphs and sampling the sparsely located neighboring nodes to obtain
their features. Since real-world graphs often exceed the capacity of GPU
memory, current GCN training systems keep the feature table in host memory and
rely on the CPU to collect sparse features before sending them to the GPUs.
This approach, however, puts tremendous pressure on host memory bandwidth and
the CPU. This is because the CPU needs to (1) read sparse features from memory,
(2) write features into memory as a dense format, and (3) transfer the features
from memory to the GPUs. In this work, we propose a novel GPU-oriented data
communication approach for GCN training, where GPU threads directly access
sparse features in host memory through zero-copy accesses without much CPU
help. By removing the CPU gathering stage, our method significantly reduces the
consumption of the host resources and data access latency. We further present
two important techniques to achieve high host memory access efficiency by the
GPU: (1) automatic data access address alignment to maximize PCIe packet
efficiency, and (2) asynchronous zero-copy access and kernel execution to fully
overlap data transfer with training. We incorporate our method into PyTorch and
evaluate its effectiveness using several graphs with sizes up to 111 million
nodes and 1.6 billion edges. In a multi-GPU training setup, our method is
65-92% faster than the conventional data transfer method, and can even match
the performance of all-in-GPU-memory training for some graphs that fit in GPU
memory.

    

### [[2103.07578] Democratic Source Coding: An Optimal Fixed-Length Quantization Scheme for Distributed Optimization Under Communication Constraints](http://arxiv.org/abs/2103.07578)


  The communication cost of distributed optimization algorithms is a major
bottleneck in their scalability. This work considers a parameter-server setting
in which the worker is constrained to communicate information to the server
using only $R$ bits per dimension. We show that $\mathbf{democratic}$
$\mathbf{embeddings}$ from random matrix theory are significantly useful for
designing efficient and optimal vector quantizers that respect this bit budget.
The resulting polynomial complexity source coding schemes are used to design
distributed optimization algorithms with convergence rates matching the minimax
optimal lower bounds for (i) Smooth and Strongly-Convex objectives with access
to an Exact Gradient oracle, as well as (ii) General Convex and Non-Smooth
objectives with access to a Noisy Subgradient oracle. We further propose a
relaxation of this coding scheme which is nearly minimax optimal. Numerical
simulations validate our theoretical claims.

    

### [[2103.08250] Hierarchical forecasting with a top-down alignment of independent level forecasts](http://arxiv.org/abs/2103.08250)


  Hierarchical forecasting with intermittent time series is a challenge in both
research and empirical studies. Vast research focuses on improving the accuracy
of each hierarchy, especially the intermittent time series at bottom levels. It
then reconciles forecasts at each hierarchy to further improve the overall
performance. In this paper, we present a hierarchical forecasting approach that
treats the bottom level forecasts as mutable to ensure higher forecasting
accuracy on the upper levels of the hierarchy. We employ a pure deep learning
forecasting approach N-BEATS for continuous time series on top levels and a
widely used tree-based algorithm LightGBM for the bottom level intermittent
time series. The hierarchical forecasting with alignment approach is a simple
yet effective variant of the bottom-up method, which accounts for biases that
are difficult to observe at the bottom level. It allows suboptimal forecasts at
the lower level to retain a higher overall performance. The approach in this
empirical study was developed by the first author during the M5 Forecasting
Accuracy competition, ranking second place. The approach is also business
orientated and could be beneficial for business strategic planning.

    

### [[2103.09656] Set-to-Sequence Methods in Machine Learning: a Review](http://arxiv.org/abs/2103.09656)


  Machine learning on sets towards sequential output is an important and
ubiquitous task, with applications ranging from language modeling and
meta-learning to multi-agent strategy games and power grid optimization.
Combining elements of representation learning and structured prediction, its
two primary challenges include obtaining a meaningful, permutation invariant
set representation and subsequently utilizing this representation to output a
complex target permutation. This paper provides a comprehensive introduction to
the field as well as an overview of important machine learning methods tackling
both of these key challenges, with a detailed qualitative comparison of
selected model architectures.

    

### [[2103.11154] Low Dimensional Landscape Hypothesis is True: DNNs can be Trained in Tiny Subspaces](http://arxiv.org/abs/2103.11154)


  Deep neural networks (DNNs) usually contain massive parameters, but there is
redundancy such that it is guessed that the DNNs could be trained in
low-dimensional subspaces. In this paper, we propose a Dynamic Linear
Dimensionality Reduction (DLDR) based on low-dimensional properties of the
training trajectory. The reduction is efficient, which is supported by
comprehensive experiments: optimization in 40 dimensional spaces can achieve
comparable performance as regular training over thousands or even millions of
parameters. Since there are only a few optimization variables, we develop a
quasi-Newton-based algorithm and also obtain robustness against label noises,
which are two follow-up experiments to show the advantages of finding
low-dimensional subspaces.

    

### [[2103.14201] Image2Reverb: Cross-Modal Reverb Impulse Response Synthesis](http://arxiv.org/abs/2103.14201)


  Measuring the acoustic characteristics of a space is often done by capturing
its impulse response (IR), a representation of how a full-range stimulus sound
excites it. This work generates an IR from a single image, which can then be
applied to other signals using convolution, simulating the reverberant
characteristics of the space shown in the image. Recording these IRs is both
time-intensive and expensive, and often infeasible for inaccessible locations.
We use an end-to-end neural network architecture to generate plausible audio
impulse responses from single images of acoustic environments. We evaluate our
method both by comparisons to ground truth data and by human expert evaluation.
We demonstrate our approach by generating plausible impulse responses from
diverse settings and formats including well known places, musical halls, rooms
in paintings, images from animations and computer games, synthetic environments
generated from text, panoramic images, and video conference backgrounds.

    

### [[2104.00878] Contrastively Learning Visual Attention as Affordance Cues from Demonstrations for Robotic Grasping](http://arxiv.org/abs/2104.00878)


  Conventional works that learn grasping affordance from demonstrations need to
explicitly predict grasping configurations, such as gripper approaching angles
or grasping preshapes. Classic motion planners could then sample trajectories
by using such predicted configurations. In this work, our goal is instead to
fill the gap between affordance discovery and affordance-based policy learning
by integrating the two objectives in an end-to-end imitation learning framework
based on deep neural networks. From a psychological perspective, there is a
close association between attention and affordance. Therefore, with an
end-to-end neural network, we propose to learn affordance cues as visual
attention that serves as a useful indicating signal of how a demonstrator
accomplishes tasks, instead of explicitly modeling affordances. To achieve
this, we propose a contrastive learning framework that consists of a Siamese
encoder and a trajectory decoder. We further introduce a coupled triplet loss
to encourage the discovered affordance cues to be more affordance-relevant. Our
experimental results demonstrate that our model with the coupled triplet loss
achieves the highest grasping success rate in a simulated robot environment.
Our project website can be accessed at
this https URL.

    

### [[2104.02057] An Empirical Study of Training Self-Supervised Vision Transformers](http://arxiv.org/abs/2104.02057)


  This paper does not describe a novel method. Instead, it studies a
straightforward, incremental, yet must-know baseline given the recent progress
in computer vision: self-supervised learning for Vision Transformers (ViT).
While the training recipes for standard convolutional networks have been highly
mature and robust, the recipes for ViT are yet to be built, especially in the
self-supervised scenarios where training becomes more challenging. In this
work, we go back to basics and investigate the effects of several fundamental
components for training self-supervised ViT. We observe that instability is a
major issue that degrades accuracy, and it can be hidden by apparently good
results. We reveal that these results are indeed partial failure, and they can
be improved when training is made more stable. We benchmark ViT results in MoCo
v3 and several other self-supervised frameworks, with ablations in various
aspects. We discuss the currently positive evidence as well as challenges and
open questions. We hope that this work will provide useful data points and
experience for future research.

    

### [[2104.05177] GarmentNets: Category-Level Pose Estimation for Garments via Canonical Space Shape Completion](http://arxiv.org/abs/2104.05177)


  This paper tackles the task of category-level pose estimation for garments.
With a near infinite degree of freedom, a garment's full configuration (i.e.,
poses) is often described by the per-vertex 3D locations of its entire 3D
surface. However, garments are also commonly subject to extreme cases of
self-occlusion, especially when folded or crumpled, making it challenging to
perceive their full 3D surface. To address these challenges, we propose
GarmentNets, where the key idea is to formulate the deformable object pose
estimation problem as a shape completion task in the canonical space. This
canonical space is defined across garments instances within a category,
therefore, specifies the shared category-level pose. By mapping the observed
partial surface to the canonical space and completing it in this space, the
output representation describes the garment's full configuration using a
complete 3D mesh with the per-vertex canonical coordinate label. To properly
handle the thin 3D structure presented on garments, we proposed a novel 3D
shape representation using the generalized winding number field. Experiments
demonstrate that GarmentNets is able to generalize to unseen garment instances
and achieve significantly better performance compared to alternative
approaches.

    

### [[2104.05218] FUDGE: Controlled Text Generation With Future Discriminators](http://arxiv.org/abs/2104.05218)


  We propose Future Discriminators for Generation (FUDGE), a flexible and
modular method for controlled text generation. Given a pre-existing model G for
generating text from a distribution of interest, FUDGE enables conditioning on
a desired attribute a (for example, formality) while requiring access only to
G's output logits. FUDGE learns an attribute predictor operating on a partial
sequence, and uses this predictor's outputs to adjust G's original
probabilities. We show that FUDGE models terms corresponding to a Bayesian
decomposition of the conditional distribution of G given attribute a. Moreover,
FUDGE can easily compose predictors for multiple desired attributes. We
evaluate FUDGE on three tasks -- couplet completion in poetry, topic control in
language generation, and formality change in machine translation -- and observe
gains in all three tasks.

    

### [[2104.07767] Exploring Visual Engagement Signals for Representation Learning](http://arxiv.org/abs/2104.07767)


  Visual engagement in social media platforms comprises interactions with photo
posts including comments, shares, and likes. In this paper, we leverage such
visual engagement clues as supervisory signals for representation learning.
However, learning from engagement signals is non-trivial as it is not clear how
to bridge the gap between low-level visual information and high-level social
interactions. We present VisE, a weakly supervised learning approach, which
maps social images to pseudo labels derived by clustered engagement signals. We
then study how models trained in this way benefit subjective downstream
computer vision tasks such as emotion recognition or political bias detection.
Through extensive studies, we empirically demonstrate the effectiveness of VisE
across a diverse set of classification tasks beyond the scope of conventional
recognition.

    

### [[2104.09261] Latent-Optimized Adversarial Neural Transfer for Sarcasm Detection](http://arxiv.org/abs/2104.09261)


  The existence of multiple datasets for sarcasm detection prompts us to apply
transfer learning to exploit their commonality. The adversarial neural transfer
(ANT) framework utilizes multiple loss terms that encourage the source-domain
and the target-domain feature distributions to be similar while optimizing for
domain-specific performance. However, these objectives may be in conflict,
which can lead to optimization difficulties and sometimes diminished transfer.
We propose a generalized latent optimization strategy that allows different
losses to accommodate each other and improves training dynamics. The proposed
method outperforms transfer learning and meta-learning baselines. In
particular, we achieve 10.02% absolute performance gain over the previous state
of the art on the iSarcasm dataset.

    

### [[2104.11434] Graph Neural Network Reinforcement Learning for Autonomous Mobility-on-Demand Systems](http://arxiv.org/abs/2104.11434)


  Autonomous mobility-on-demand (AMoD) systems represent a rapidly developing
mode of transportation wherein travel requests are dynamically handled by a
coordinated fleet of robotic, self-driving vehicles. Given a graph
representation of the transportation network - one where, for example, nodes
represent areas of the city, and edges the connectivity between them - we argue
that the AMoD control problem is naturally cast as a node-wise decision-making
problem. In this paper, we propose a deep reinforcement learning framework to
control the rebalancing of AMoD systems through graph neural networks.
Crucially, we demonstrate that graph neural networks enable reinforcement
learning agents to recover behavior policies that are significantly more
transferable, generalizable, and scalable than policies learned through other
approaches. Empirically, we show how the learned policies exhibit promising
zero-shot transfer capabilities when faced with critical portability tasks such
as inter-city generalization, service area expansion, and adaptation to
potentially complex urban topologies.

    

### [[2104.13020] Simple yet Sharp Sensitivity Analysis for Unmeasured Confounding](http://arxiv.org/abs/2104.13020)


  We present a method for assessing the sensitivity of the true causal effect
to unmeasured confounding. The method requires the analyst to set two intuitive
parameters. Otherwise, the method is assumption-free. The method returns an
interval that contains the true causal effect, and whose bounds are sharp, i.e.
attainable. We show experimentally that our bounds can be sharper than those
obtained by the method of Ding and VanderWeele (2016a) which, moreover,
requires to set one more parameter than our method. Finally, we extend our
method to bound the natural direct and indirect effects when there are measured
mediators and unmeasured exposure-outcome confounding.

    

### [[2105.00324] Neko: a Library for Exploring Neuromorphic Learning Rules](http://arxiv.org/abs/2105.00324)


  The field of neuromorphic computing is in a period of active exploration.
While many tools have been developed to simulate neuronal dynamics or convert
deep networks to spiking models, general software libraries for learning rules
remain underexplored. This is partly due to the diverse, challenging nature of
efforts to design new learning rules, which range from encoding methods to
gradient approximations, from population approaches that mimic the Bayesian
brain to constrained learning algorithms deployed on memristor crossbars. To
address this gap, we present Neko, a modular, extensible library with a focus
on aiding the design of new learning algorithms. We demonstrate the utility of
Neko in three exemplar cases: online local learning, probabilistic learning,
and analog on-device learning. Our results show that Neko can replicate the
state-of-the-art algorithms and, in one case, lead to significant
outperformance in accuracy and speed. Further, it offers tools including
gradient comparison that can help develop new algorithmic variants. Neko is an
open source Python library that supports PyTorch and TensorFlow backends.

    

### [[2105.01386] Canonical Saliency Maps: Decoding Deep Face Models](http://arxiv.org/abs/2105.01386)


  As Deep Neural Network models for face processing tasks approach human-like
performance, their deployment in critical applications such as law enforcement
and access control has seen an upswing, where any failure may have far-reaching
consequences. We need methods to build trust in deployed systems by making
their working as transparent as possible. Existing visualization algorithms are
designed for object recognition and do not give insightful results when applied
to the face domain. In this work, we present 'Canonical Saliency Maps', a new
method that highlights relevant facial areas by projecting saliency maps onto a
canonical face model. We present two kinds of Canonical Saliency Maps:
image-level maps and model-level maps. Image-level maps highlight facial
features responsible for the decision made by a deep face model on a given
image, thus helping to understand how a DNN made a prediction on the image.
Model-level maps provide an understanding of what the entire DNN model focuses
on in each task and thus can be used to detect biases in the model. Our
qualitative and quantitative results show the usefulness of the proposed
canonical saliency maps, which can be used on any deep face model regardless of
the architecture.

    

### [[2105.03736] PIM-DRAM: Accelerating Machine Learning Workloads using Processing in Commodity DRAM](http://arxiv.org/abs/2105.03736)


  Deep Neural Networks (DNNs) have transformed the field of machine learning
and are widely deployed in many applications involving image, video, speech and
natural language processing. The increasing compute demands of DNNs have been
widely addressed through Graphics Processing Units (GPUs) and specialized
accelerators. However, as model sizes grow, these von Neumann architectures
require very high memory bandwidth to keep the processing elements utilized as
a majority of the data resides in the main memory. Processing in memory has
been proposed as a promising solution for the memory wall bottleneck for ML
workloads. In this work, we propose a new DRAM-based processing-in-memory (PIM)
multiplication primitive coupled with intra-bank accumulation to accelerate
matrix vector operations in ML workloads. The proposed multiplication primitive
adds < 1% area overhead and does not require any change in the DRAM
peripherals. Therefore, the proposed multiplication can be easily adopted in
commodity DRAM chips. Subsequently, we design a DRAM-based PIM architecture,
data mapping scheme and dataflow for executing DNNs within DRAM. System
evaluations performed on networks like AlexNet, VGG16 and ResNet18 show that
the proposed architecture, mapping, and data flow can provide up to 19.5x
speedup over an NVIDIA Titan Xp GPU highlighting the need to overcome the
memory bottleneck in future generations of DNN hardware.

    

### [[2105.05553] Principal Components Bias in Deep Neural Networks](http://arxiv.org/abs/2105.05553)


  Recent work suggests that convolutional neural networks of different
architectures learn to classify images in the same order. To understand this
phenomenon, we revisit the over-parametrized deep linear network model. Our
asymptotic analysis, assuming that the hidden layers are wide enough, reveals
that the convergence rate of this model's parameters is exponentially faster
along directions corresponding to the larger principal components of the data,
at a rate governed by the singular values. We term this convergence pattern the
Principal Components bias (PC-bias). We show how the PC-bias streamlines the
order of learning of both linear and non-linear networks, more prominently at
earlier stages of learning. We then compare our results to the simplicity bias,
showing that both biases can be seen independently, and affect the order of
learning in different ways. Finally, we discuss how the PC-bias may explain
some benefits of early stopping and its connection to PCA, and why deep
networks converge more slowly when given random labels.

    

### [[2105.07615] Differentially Private Federated Knowledge Graphs Embedding](http://arxiv.org/abs/2105.07615)


  Knowledge graph embedding plays an important role in knowledge
representation, reasoning, and data mining applications. However, for multiple
cross-domain knowledge graphs, state-of-the-art embedding models cannot make
full use of the data from different knowledge domains while preserving the
privacy of exchanged data. In addition, the centralized embedding model may not
scale to the extensive real-world knowledge graphs. Therefore, we propose a
novel decentralized scalable learning framework, \emph{Federated Knowledge
Graphs Embedding} (FKGE), where embeddings from different knowledge graphs can
be learnt in an asynchronous and peer-to-peer manner while being
privacy-preserving. FKGE exploits adversarial generation between pairs of
knowledge graphs to translate identical entities and relations of different
domains into near embedding spaces. In order to protect the privacy of the
training data, FKGE further implements a privacy-preserving neural network
structure to guarantee no raw data leakage. We conduct extensive experiments to
evaluate FKGE on 11 knowledge graphs, demonstrating a significant and
consistent improvement in model quality with at most 17.85\% and 7.90\%
increases in performance on triple classification and link prediction tasks.

    

### [[2105.08275] ModelPS: An Interactive and Collaborative Platform for Editing Pre-trained Models at Scale](http://arxiv.org/abs/2105.08275)


  AI engineering has emerged as a crucial discipline to democratize deep neural
network (DNN) models among software developers with a diverse background. In
particular, altering these DNN models in the deployment stage posits a
tremendous challenge. In this research, we propose and develop a low-code
solution, ModelPS (an acronym for "Model Photoshop"), to enable and empower
collaborative DNN model editing and intelligent model serving. The ModelPS
solution embodies two transformative features: 1) a user-friendly web interface
for a developer team to share and edit DNN models pictorially, in a low-code
fashion, and 2) a model genie engine in the backend to aid developers in
customizing model editing configurations for given deployment requirements or
constraints. Our case studies with a wide range of deep learning (DL) models
show that the system can tremendously reduce both development and communication
overheads with improved productivity.

    

### [[2105.09492] DeepCAD: A Deep Generative Network for Computer-Aided Design Models](http://arxiv.org/abs/2105.09492)


  Deep generative models of 3D shapes have received a great deal of research
interest. Yet, almost all of them generate discrete shape representations, such
as voxels, point clouds, and polygon meshes. We present the first 3D generative
model for a drastically different shape representation --- describing a shape
as a sequence of computer-aided design (CAD) operations. Unlike meshes and
point clouds, CAD models encode the user creation process of 3D shapes, widely
used in numerous industrial and engineering design tasks. However, the
sequential and irregular structure of CAD operations poses significant
challenges for existing 3D generative models. Drawing an analogy between CAD
operations and natural language, we propose a CAD generative network based on
the Transformer. We demonstrate the performance of our model for both shape
autoencoding and random shape generation. To train our network, we create a new
CAD dataset consisting of 178,238 models and their CAD construction sequences.
We have made this dataset publicly available to promote future research on this
topic.

    

### [[2105.10709] Inclusion of Domain-Knowledge into GNNs using Mode-Directed Inverse Entailment](http://arxiv.org/abs/2105.10709)


  We present a general technique for constructing Graph Neural Networks (GNNs)
capable of using multi-relational domain knowledge. The technique is based on
mode-directed inverse entailment (MDIE) developed in Inductive Logic
Programming (ILP). Given a data instance $e$ and background knowledge $B$, MDIE
identifies a most-specific logical formula $\bot_B(e)$ that contains all the
relational information in $B$ that is related to $e$. We represent $\bot_B(e)$
by a "bottom-graph" that can be converted into a form suitable for GNN
implementations. This transformation allows a principled way of incorporating
generic background knowledge into GNNs: we use the term `BotGNN' for this form
of graph neural networks. For several GNN variants, using real-world datasets
with substantial background knowledge, we show that BotGNNs perform
significantly better than both GNNs without background knowledge and a recently
proposed simplified technique for including domain knowledge into GNNs. We also
provide experimental evidence comparing BotGNNs favourably to multi-layer
perceptrons (MLPs) that use features representing a "propositionalised" form of
the background knowledge; and BotGNNs to a standard ILP based on the use of
most-specific clauses. Taken together, these results point to BotGNNs as
capable of combining the computational efficacy of GNNs with the
representational versatility of ILP.

    

### [[2105.11492] Adaptive Local Kernels Formulation of Mutual Information with Application to Active Post-Seismic Building Damage Inference](http://arxiv.org/abs/2105.11492)


  The abundance of training data is not guaranteed in various supervised
learning applications. One of these situations is the post-earthquake regional
damage assessment of buildings. Querying the damage label of each building
requires a thorough inspection by experts, and thus, is an expensive task. A
practical approach is to sample the most informative buildings in a sequential
learning scheme. Active learning methods recommend the most informative cases
that are able to maximally reduce the generalization error. The information
theoretic measure of mutual information (MI) is one of the most effective
criteria to evaluate the effectiveness of the samples in a pool-based sample
selection scenario. However, the computational complexity of the standard MI
algorithm prevents the utilization of this method on large datasets. A local
kernels strategy was proposed to reduce the computational costs, but the
adaptability of the kernels to the observed labels was not considered in the
original formulation of this strategy. In this article, an adaptive local
kernels methodology is developed that allows for the conformability of the
kernels to the observed output data while enhancing the computational
complexity of the standard MI algorithm. The proposed algorithm is developed to
work on a Gaussian process regression (GPR) framework, where the kernel
hyperparameters are updated after each label query using the maximum likelihood
estimation. In the sequential learning procedure, the updated hyperparameters
can be used in the MI kernel matrices to improve the sample suggestion
performance. The advantages are demonstrated on a simulation of the 2018
Anchorage, AK, earthquake. It is shown that while the proposed algorithm
enables GPR to reach acceptable performance with fewer training data, the
computational demands remain lower than the standard local kernels strategy.

    

### [[2105.13817] Achieving Fairness with a Simple Ridge Penalty](http://arxiv.org/abs/2105.13817)


  In this paper we present a general framework for estimating regression models
subject to a user-defined level of fairness. We enforce fairness as a model
selection step in which we choose the value of a ridge penalty to control the
effect of sensitive attributes. We then estimate the parameters of the model
conditional on the chosen penalty value. Our proposal is mathematically simple,
with a solution that is partly in closed form, and produces estimates of the
regression coefficients that are intuitive to interpret as a function of the
level of fairness. Furthermore, it is easily extended to generalised linear
models, kernelised regression models and other penalties; and it can
accommodate multiple definitions of fairness.
We compare our approach with the regression model from Komiyama et al.
(2018), which implements a provably-optimal linear regression model; and with
the fair models from Zafar et al. (2019). We evaluate these approaches
empirically on six different data sets, and we find that our proposal provides
better goodness of fit and better predictive accuracy for the same level of
fairness. In addition, we highlight a source of bias in the original
experimental evaluation in Komiyama et al. (2018).

    

### [[2106.02493] Homological Time Series Analysis of Sensor Signals from Power Plants](http://arxiv.org/abs/2106.02493)


  In this paper, we use topological data analysis techniques to construct a
suitable neural network classifier for the task of learning sensor signals of
entire power plants according to their reference designation system. We use
representations of persistence diagrams to derive necessary preprocessing steps
and visualize the large amounts of data. We derive deep architectures with
one-dimensional convolutional layers combined with stacked long short-term
memories as residual networks suitable for processing the persistence features.
We combine three separate sub-networks, obtaining as input the time series
itself and a representation of the persistent homology for the zeroth and first
dimension. We give a mathematical derivation for most of the used
hyper-parameters. For validation, numerical experiments were performed with
sensor data from four power plants of the same construction type.

    

### [[2106.07736] Unique sparse decomposition of low rank matrices](http://arxiv.org/abs/2106.07736)


  The problem of finding the unique low dimensional decomposition of a given
matrix has been a fundamental and recurrent problem in many areas. In this
paper, we study the problem of seeking a unique decomposition of a low rank
matrix $Y\in \mathbb{R}^{p\times n}$ that admits a sparse representation.
Specifically, we consider $Y = A X\in \mathbb{R}^{p\times n}$ where the matrix
$A\in \mathbb{R}^{p\times r}$ has full column rank, with $r < \min\{n,p\}$, and
the matrix $X\in \mathbb{R}^{r\times n}$ is element-wise sparse. We prove that
this sparse decomposition of $Y$ can be uniquely identified, up to some
intrinsic signed permutation. Our approach relies on solving a nonconvex
optimization problem constrained over the unit sphere. Our geometric analysis
for the nonconvex optimization landscape shows that any {\em strict} local
solution is close to the ground truth solution, and can be recovered by a
simple data-driven initialization followed with any second order descent
algorithm. At last, we corroborate these theoretical results with numerical
experiments.

    

### [[2106.10471] Neural Network Classifier as Mutual Information Evaluator](http://arxiv.org/abs/2106.10471)


  Cross-entropy loss with softmax output is a standard choice to train neural
network classifiers. We give a new view of neural network classifiers with
softmax and cross-entropy as mutual information evaluators. We show that when
the dataset is balanced, training a neural network with cross-entropy maximises
the mutual information between inputs and labels through a variational form of
mutual information. Thereby, we develop a new form of softmax that also
converts a classifier to a mutual information evaluator when the dataset is
imbalanced. Experimental results show that the new form leads to better
classification accuracy, in particular for imbalanced datasets.

    

### [[2106.10472] Informative Class Activation Maps](http://arxiv.org/abs/2106.10472)


  We study how to evaluate the quantitative information content of a region
within an image for a particular label. To this end, we bridge class activation
maps with information theory. We develop an informative class activation map
(infoCAM). Given a classification task, infoCAM depict how to accumulate
information of partial regions to that of the entire image toward a label.
Thus, we can utilise infoCAM to locate the most informative features for a
label. When applied to an image classification task, infoCAM performs better
than the traditional classification map in the weakly supervised object
localisation task. We achieve state-of-the-art results on Tiny-ImageNet.

    

### [[2106.15382] Multiple Graph Learning for Scalable Multi-view Clustering](http://arxiv.org/abs/2106.15382)


  Graph-based multi-view clustering has become an active topic due to the
efficiency in characterizing both the complex structure and relationship
between multimedia data. However, existing methods have the following
shortcomings: (1) They are inefficient or even fail for graph learning in large
scale due to the graph construction and eigen-decomposition. (2) They cannot
well exploit both the complementary information and spatial structure embedded
in graphs of different views. To well exploit complementary information and
tackle the scalability issue plaguing graph-based multi-view clustering, we
propose an efficient multiple graph learning model via a small number of anchor
points and tensor Schatten p-norm minimization. Specifically, we construct a
hidden and tractable large graph by anchor graph for each view and well exploit
complementary information embedded in anchor graphs of different views by
tensor Schatten p-norm regularizer. Finally, we develop an efficient algorithm,
which scales linearly with the data size, to solve our proposed model.
Extensive experimental results on several datasets indicate that our proposed
method outperforms some state-of-the-art multi-view clustering algorithms.

    

### [[2107.08957] Clinical Relation Extraction Using Transformer-based Models](http://arxiv.org/abs/2107.08957)


  The newly emerged transformer technology has a tremendous impact on NLP
research. In the general English domain, transformer-based models have achieved
state-of-the-art performances on various NLP benchmarks. In the clinical
domain, researchers also have investigated transformer models for clinical
applications. The goal of this study is to systematically explore three widely
used transformer-based models (i.e., BERT, RoBERTa, and XLNet) for clinical
relation extraction and develop an open-source package with clinical
pre-trained transformer-based models to facilitate information extraction in
the clinical domain. We developed a series of clinical RE models based on three
transformer architectures, namely BERT, RoBERTa, and XLNet. We evaluated these
models using 2 publicly available datasets from 2018 MADE1.0 and 2018 n2c2
challenges. We compared two classification strategies (binary vs. multi-class
classification) and investigated two approaches to generate candidate relations
in different experimental settings. In this study, we compared three
transformer-based (BERT, RoBERTa, and XLNet) models for relation extraction. We
demonstrated that the RoBERTa-clinical RE model achieved the best performance
on the 2018 MADE1.0 dataset with an F1-score of 0.8958. On the 2018 n2c2
dataset, the XLNet-clinical model achieved the best F1-score of 0.9610. Our
results indicated that the binary classification strategy consistently
outperformed the multi-class classification strategy for clinical relation
extraction. Our methods and models are publicly available at
this https URL.
We believe this work will improve current practice on clinical relation
extraction and other related NLP tasks in the biomedical domain.

    

### [[2108.06569] LILLIPUT: A Lightweight Low-Latency Lookup-Table Based Decoder for Near-term Quantum Error Correction](http://arxiv.org/abs/2108.06569)


  The error rates of quantum devices are orders of magnitude higher than what
is needed to run most quantum applications. To close this gap, Quantum Error
Correction (QEC) encodes logical qubits and distributes information using
several physical qubits. By periodically executing a syndrome extraction
circuit on the logical qubits, information about errors (called syndrome) is
extracted while running programs. A decoder uses these syndromes to identify
and correct errors in real time, which is required to use feedback implemented
in quantum algorithms. Unfortunately, software decoders are slow and hardware
decoders are fast but less accurate. Thus, almost all QEC studies so far have
relied on offline decoding.
To enable real-time decoding in near-term QEC, we propose LILLIPUT-- a
Lightweight Low Latency Look-Up Table decoder. LILLIPUT consists of two parts--
First, it translates syndromes into error detection events that index into a
Look-Up Table (LUT) whose entry provides the error information in real-time.
Second, it programs the LUTs with error assignments for all possible error
events by running a software decoder offline. LILLIPUT tolerates an error on
any operation in the quantum hardware, including gates and measurement, and the
number of tolerated errors grows with the size of the code. It needs <7% logic
on off-the-shelf FPGAs that allows it to be easily integrated alongside the
control and readout circuits in existing systems. LILLIPUT incurs a latency of
few nanoseconds and enables real-time decoding. We also propose Compressed LUTs
(CLUTs) to reduce the memory needed by LILLIPUT. By exploiting the fact that
not all error events are equally likely and only storing data for the most
probable error events, CLUTs reduce the memory needed by up-to 107x (from 148
MB to 1.38 MB) without degrading accuracy.

    

### [[2108.06703] Mithril: Cooperative Row Hammer Protection on Commodity DRAM Leveraging Managed Refresh](http://arxiv.org/abs/2108.06703)


  Since its public introduction in the mid-2010s, the Row Hammer (RH)
phenomenon has drawn significant attention from the research community due to
its security implications. Although many RH-protection schemes have been
proposed by processor vendors, DRAM manufacturers, and academia, they still
have shortcomings. Solutions implemented in the memory controller (MC) pay an
increasingly higher cost due to their conservative design for the worst case in
terms of the number of DRAM banks and RH threshold to support. Meanwhile, the
DRAM-side implementation has a limited time margin for RH protection measures
or requires extensive modifications to the standard DRAM interface. Recently, a
new command for RH protection has been introduced in the DDR5/LPDDR5 standards,
called refresh management (RFM). RFM enables the separation of the tasks for RH
protection to both MC and DRAM. It does so by having the former generate an RFM
command at a specific activation frequency, and the latter take proper RH
protection measures within a given time window. Although promising, no existing
study presents and analyzes RFM-based solutions for RH protection. In this
paper, we propose Mithril, the first RFM interface-compatible, DRAM-MC
cooperative RH protection scheme providing deterministic protection guarantees.
Mithril has minimal energy overheads for common use cases without adversarial
memory access patterns. We also introduce Mithril+, an extension to provide
minimal performance overheads at the expense of a tiny modification to the MC
while utilizing an existing DRAM command.

    

### [[2103.14951] A First Look at RISC-V Virtualization from an Embedded Systems Perspective](http://arxiv.org/abs/2103.14951)


  This article describes the first public implementation and evaluation of the
latest version of the RISC-V hypervisor extension (H-extension v0.6.1)
specification in a Rocket chip core. To perform a meaningful evaluation for
modern multi-core embedded and mixedcriticality systems, we have ported Bao, an
open-source static partitioning hypervisor, to RISC-V. We have also extended
the RISC-V platformlevel interrupt controller (PLIC) to enable direct guest
interrupt injection with low and deterministic latency and we have enhanced the
timer infrastructure to avoid trap and emulation overheads. Experiments were
carried out in FireSim, a cycle-accurate, FPGA-accelerated simulator, and the
system was also successfully deployed and tested in a Zynq UltraScale+ MPSoC
ZCU104. Our hardware implementation was opensourced and is currently in use by
the RISC-V community towards the ratification of the H-extension specification.

    

### [[2108.06389] Time Transitive Functions for Zero Knowledge Proofs](http://arxiv.org/abs/2108.06389)


  Verifiable delay functions have found a lot of applications in blockchain
technology in recent times. Continuous verifiable delay functions are an
improvement over the basic notion of VDFs with recursive capabilities. We are
proposing the application of VDF for constructing more space time-efficient
provers and simulators required for the iterative non-interactive
zero-knowledge systems.

    

### [[2108.06658] Efficient Byzantine-Resilient Stochastic Gradient Desce](http://arxiv.org/abs/2108.06658)


  Distributed Learning often suffers from Byzantine failures, and there have
been a number of works studying the problem of distributed stochastic
optimization under Byzantine failures, where only a portion of workers, instead
of all the workers in a distributed learning system, compute stochastic
gradients at each iteration. These methods, albeit workable under Byzantine
failures, have the shortcomings of either a sub-optimal convergence rate or
high computation cost. To this end, we propose a new Byzantine-resilient
stochastic gradient descent algorithm (BrSGD for short) which is provably
robust against Byzantine failures. BrSGD obtains the optimal statistical
performance and efficient computation simultaneously. In particular, BrSGD can
achieve an order-optimal statistical error rate for strongly convex loss
functions. The computation complexity of BrSGD is O(md), where d is the model
dimension and m is the number of machines. Experimental results show that BrSGD
can obtain competitive results compared with non-Byzantine machines in terms of
effectiveness and convergence.

    

### [[2108.06893] Memtrade: A Disaggregated-Memory Marketplace for Public Clouds](http://arxiv.org/abs/2108.06893)


  We present Memtrade, the first memory disaggregation system for public
clouds. Public clouds introduce a set of unique challenges for resource
disaggregation across different tenants, including security, isolation and
pricing. Memtrade allows producer virtual machines (VMs) to lease both their
unallocated memory and allocated-but-idle application memory to remote consumer
VMs for a limited period of time. Memtrade does not require any modifications
to host-level system software or support from the cloud provider. It harvests
producer memory using an application-aware control loop to form a distributed
transient remote memory pool with minimal performance impact; it employs a
broker to match producers with consumers while satisfying performance
constraints; and it exposes the matched memory to consumers as a secure KV
cache. Our evaluation using real-world cluster traces shows that Memtrade
provides significant performance benefit for consumers (improving average read
latency up to 2.8x) while preserving confidentiality and integrity, with little
impact on producer applications (degrading performance by less than 2.1%).

    

### [[2006.13432] Local-Search Based Heuristics for Advertisement Scheduling](http://arxiv.org/abs/2006.13432)


  In the MAXSPACE problem, given a set of ads A, one wants to place a subset A'
of A into K slots B_1, ..., B_K of size L. Each ad A_i in A has size s_i and
frequency w_i. A schedule is feasible if the total size of ads in any slot is
at most L, and each ad A_i in A' appears in exactly w_i slots. The goal is to
find a feasible schedule that maximizes the space occupied in all slots. We
introduce MAXSPACE-RDWV, a MAXSPACE generalization with release dates,
deadlines, variable frequency, and generalized profit. In MAXSPACE-RDWV each ad
A_i has a release date r_i >= 1, a deadline d_i >= r_i, a profit v_i that may
not be related with s_i and lower and upper bounds w^min_i and w^max_i for
frequency. In this problem, an ad may only appear in a slot B_j with r_i <= j
<= d_i, and the goal is to find a feasible schedule that maximizes the sum of
values of scheduled ads. This paper presents some algorithms based on
meta-heuristics Greedy Randomized Adaptive Search Procedure (GRASP), Variable
Neighborhood Search (VNS), Local Search, and Tabu Search for MAXSPACE and
MAXSPACE-RDWV. We compare our proposed algorithms with Hybrid-GA proposed by
Kumar et al. (2006). We also create a version of Hybrid-GA for MAXSPACE-RDWV
and compare it with our meta-heuristics for this problem. Some meta-heuristics,
such as VNS and GRASP+VNS, have better results than Hybrid-GA for both
problems.

    

### [[2103.09636] Accretive Computation of Global Transformations](http://arxiv.org/abs/2103.09636)


  Global transformations form a categorical framework adapting graph
transformations to describe fully synchronous rule systems on a given data
this http URL this work we focus on data structures that can be captured as
presheaves and study the computational aspects of such synchronous rule
this http URL obtain an online algorithm, a complete study of the sub-steps within
each synchronous step is done at the semantic level.This leads to the
definition of accretive rule systems and a local criterion to characterize
these systems.Finally an online computation algorithm for theses systems is
given.

    

### [[2104.07122] Coalescent Computing](http://arxiv.org/abs/2104.07122)


  As computational infrastructure extends to the edge, it will increasingly
offer the same fine-grained resource provisioning mechanisms used in
large-scale cloud datacenters, and advances in low-latency, wireless networking
technology will allow service providers to blur the distinction between local
and remote resources for commodity computing. From the users' perspectives,
their devices will no longer have fixed computational power, but rather will
appear to have flexible computational capabilities that vary subject to the
shared, disaggregated edge resources available in their physical proximity.
System software will transparently leverage these ephemeral resources to
provide a better end-user experience. We discuss key systems challenges to
enabling such tightly-coupled, disaggregated, and ephemeral infrastructure
provisioning, advocate for more research in the area, and outline possible
paths forward.

    

### [[2104.12197] RDMAbox : Optimizing RDMA for Memory Intensive Workloads](http://arxiv.org/abs/2104.12197)


  We present RDMAbox, a set of low level RDMA optimizations that provide better
performance than previous approaches. The optimizations are packaged in
easy-to-use kernel and user space libraries for applications and systems in
data center. We demonstrate the flexibility and effectiveness of RDMAbox by
implementing a kernel remote paging system and a user space file system using
RDMAbox. RDMAbox employs two optimization techniques. First, we suggest RDMA
request merging and chaining to further reduce the total number of I/O
operations to the RDMA NIC. The I/O merge queue at the same time functions as a
traffic regulator to enforce admission control and avoid overloading the NIC.
Second, we propose Adaptive Polling to achieve higher efficiency of polling
Work Completion than existing busy polling while maintaining the low CPU
overhead of event trigger. Our implementation of a remote paging system with
RDMAbox outperforms existing representative solutions with up to 4? throughput
improvement and up to 83% decrease in average tail latency in bigdata
workloads, and up to 83% reduction in completion time in machine learning
workloads. Our implementation of a user space file system based on RDMAbox
achieves up to 5.9? higher throughput over existing representative solutions.

    

### [[2105.05085] TinyStack: A Minimal GPU Stack for Client ML](http://arxiv.org/abs/2105.05085)


  TinyStack is a novel way for deploying GPU-accelerated computation on mobile
and embedded devices. It addresses high complexity of a modern GPU stack for
deployment ease and security. The idea is to record GPU executions on the full
GPU stack ahead of time and replay the executions on new input at run time. We
address key challenges towards making TinyStack feasible, sound, and practical
to use. The resultant replayer is a drop-in replacement of the original GPU
stack. It is tiny (50 KB of executable), robust (replaying long executions
without divergence), portable (running in a commodity OS, in TEE, and
baremetal), and quick to launch (speeding up startup by up to two orders of
magnitude). We show that TinyStack works with a variety of integrated GPU
hardware, GPU APIs, ML frameworks, and 33 neural network (NN) implementations
for inference or training.

    

### [[2108.06371] Near-Optimal Reviewer Splitting in Two-Phase Paper Reviewing and Conference Experiment Design](http://arxiv.org/abs/2108.06371)


  Many scientific conferences employ a two-phase paper review process, where
some papers are assigned additional reviewers after the initial reviews are
submitted. Many conferences also design and run experiments on their paper
review process, where some papers are assigned reviewers who provide reviews
under an experimental condition. In this paper, we consider the question: how
should reviewers be divided between phases or conditions in order to maximize
total assignment similarity? We make several contributions towards answering
this question. First, we prove that when the set of papers requiring additional
review is unknown, a simplified variant of this problem is NP-hard. Second, we
empirically show that across several datasets pertaining to real conference
data, dividing reviewers between phases/conditions uniformly at random allows
an assignment that is nearly as good as the oracle optimal assignment. This
uniformly random choice is practical for both the two-phase and conference
experiment design settings. Third, we provide explanations of this phenomenon
by providing theoretical bounds on the suboptimality of this random strategy
under certain natural conditions. From these easily-interpretable conditions,
we provide actionable insights to conference program chairs about whether a
random reviewer split is suitable for their conference.

    

### [[2108.06405] Planning with Incomplete Information in Quantified Answer Set Programming](http://arxiv.org/abs/2108.06405)


  We present a general approach to planning with incomplete information in
Answer Set Programming (ASP). More precisely, we consider the problems of
conformant and conditional planning with sensing actions and assumptions. We
represent planning problems using a simple formalism where logic programs
describe the transition function between states, the initial states and the
goal states. For solving planning problems, we use Quantified Answer Set
Programming (QASP), an extension of ASP with existential and universal
quantifiers over atoms that is analogous to Quantified Boolean Formulas (QBFs).
We define the language of quantified logic programs and use it to represent the
solutions to different variants of conformant and conditional planning. On the
practical side, we present a translation-based QASP solver that converts
quantified logic programs into QBFs and then executes a QBF solver, and we
evaluate experimentally the approach on conformant and conditional planning
benchmarks. Under consideration for acceptance in TPLP.

    

### [[2108.06479] Contrastive Self-supervised Sequential Recommendation with Robust Augmentation](http://arxiv.org/abs/2108.06479)


  Sequential Recommendationdescribes a set of techniques to model dynamic user
behavior in order to predict future interactions in sequential user data. At
their core, such approaches model transition probabilities between items in a
sequence, whether through Markov chains, recurrent networks, or more recently,
Transformers. However both old and new issues remain, including data-sparsity
and noisy data; such issues can impair the performance, especially in complex,
parameter-hungry models. In this paper, we investigate the application of
contrastive Self-Supervised Learning (SSL) to the sequential recommendation, as
a way to alleviate some of these issues. Contrastive SSL constructs
augmentations from unlabelled instances, where agreements among positive pairs
are maximized. It is challenging to devise a contrastive SSL framework for a
sequential recommendation, due to its discrete nature, correlations among
items, and skewness of length distributions. To this end, we propose a novel
framework, Contrastive Self-supervised Learning for sequential Recommendation
(CoSeRec). We introduce two informative augmentation operators leveraging item
correlations to create high-quality views for contrastive learning.
Experimental results on three real-world datasets demonstrate the effectiveness
of the proposed method on improving model performance and the robustness
against sparse and noisy data. Our implementation is available online at
\url{this https URL}

    

### [[2108.06481] MatSat: a matrix-based differentiable SAT solver](http://arxiv.org/abs/2108.06481)


  We propose a new approach to SAT solving which solves SAT problems in vector
spaces as a cost minimization problem of a non-negative differentiable cost
function J^sat. In our approach, a solution, i.e., satisfying assignment, for a
SAT problem in n variables is represented by a binary vector u in {0,1}^n that
makes J^sat(u) zero. We search for such u in a vector space R^n by cost
minimization, i.e., starting from an initial u_0 and minimizing J to zero while
iteratively updating u by Newton's method. We implemented our approach as a
matrix-based differential SAT solver MatSat. Although existing main-stream SAT
solvers decide each bit of a solution assignment one by one, be they of
conflict driven clause learning (CDCL) type or of stochastic local search (SLS)
type, MatSat fundamentally differs from them in that it continuously approach a
solution in a vector space. We conducted an experiment to measure the
scalability of MatSat with random 3-SAT problems in which MatSat could find a
solution up to n=10^5 variables. We also compared MatSat with four
state-of-the-art SAT solvers including winners of SAT competition 2018 and SAT
Race 2019 in terms of time for finding a solution, using a random benchmark set
from SAT 2018 competition and an artificial random 3-SAT instance set. The
result shows that MatSat comes in second in both test sets and outperforms all
the CDCL type solvers.

    

### [[2108.06500] Appropriate Fairness Perceptions? On the Effectiveness of Explanations in Enabling People to Assess the Fairness of Automated Decision Systems](http://arxiv.org/abs/2108.06500)


  It is often argued that one goal of explaining automated decision systems
(ADS) is to facilitate positive perceptions (e.g., fairness or trustworthiness)
of users towards such systems. This viewpoint, however, makes the implicit
assumption that a given ADS is fair and trustworthy, to begin with. If the ADS
issues unfair outcomes, then one might expect that explanations regarding the
system's workings will reveal its shortcomings and, hence, lead to a decrease
in fairness perceptions. Consequently, we suggest that it is more meaningful to
evaluate explanations against their effectiveness in enabling people to
appropriately assess the quality (e.g., fairness) of an associated ADS. We
argue that for an effective explanation, perceptions of fairness should
increase if and only if the underlying ADS is fair. In this in-progress work,
we introduce the desideratum of appropriate fairness perceptions, propose a
novel study design for evaluating it, and outline next steps towards a
comprehensive experiment.

    

### [[2108.06682] ST3D++: Denoised Self-training for Unsupervised Domain Adaptation on 3D Object Detection](http://arxiv.org/abs/2108.06682)


  In this paper, we present a self-training method, named ST3D++, with a
holistic pseudo label denoising pipeline for unsupervised domain adaptation on
3D object detection. ST3D++ aims at reducing noise in pseudo label generation
as well as alleviating the negative impacts of noisy pseudo labels on model
training. First, ST3D++ pre-trains the 3D object detector on the labeled source
domain with random object scaling (ROS) which is designed to reduce target
domain pseudo label noise arising from object scale bias of the source domain.
Then, the detector is progressively improved through alternating between
generating pseudo labels and training the object detector with pseudo-labeled
target domain data. Here, we equip the pseudo label generation process with a
hybrid quality-aware triplet memory to improve the quality and stability of
generated pseudo labels. Meanwhile, in the model training stage, we propose a
source data assisted training strategy and a curriculum data augmentation
policy to effectively rectify noisy gradient directions and avoid model
over-fitting to noisy pseudo labeled data. These specific designs enable the
detector to be trained on meticulously refined pseudo labeled target data with
denoised training signals, and thus effectively facilitate adapting an object
detector to a target domain without requiring annotations. Finally, our method
is assessed on four 3D benchmark datasets (i.e., Waymo, KITTI, Lyft, and
nuScenes) for three common categories (i.e., car, pedestrian and bicycle).
ST3D++ achieves state-of-the-art performance on all evaluated settings,
outperforming the corresponding baseline by a large margin (e.g., 9.6% $\sim$
38.16% on Waymo $\rightarrow$ KITTI in terms of AP$_{\text{3D}}$), and even
surpasses the fully supervised oracle results on the KITTI 3D object detection
benchmark with target prior. Code will be available.

    

### [[2108.06730] Towards Visual Explainable Active Learning for Zero-Shot Classification](http://arxiv.org/abs/2108.06730)


  Zero-shot classification is a promising paradigm to solve an applicable
problem when the training classes and test classes are disjoint. Achieving this
usually needs experts to externalize their domain knowledge by manually
specifying a class-attribute matrix to define which classes have which
attributes. Designing a suitable class-attribute matrix is the key to the
subsequent procedure, but this design process is tedious and trial-and-error
with no guidance. This paper proposes a visual explainable active learning
approach with its design and implementation called semantic navigator to solve
the above problems. This approach promotes human-AI teaming with four actions
(ask, explain, recommend, respond) in each interaction loop. The machine asks
contrastive questions to guide humans in the thinking process of attributes. A
novel visualization called semantic map explains the current status of the
machine. Therefore analysts can better understand why the machine misclassifies
objects. Moreover, the machine recommends the labels of classes for each
attribute to ease the labeling burden. Finally, humans can steer the model by
modifying the labels interactively, and the machine adjusts its
recommendations. The visual explainable active learning approach improves
humans' efficiency of building zero-shot classification models interactively,
compared with the method without guidance. We justify our results with user
studies using the standard benchmarks for zero-shot classification.

    

### [[2108.06735] A Two-Layer Near-Optimal Strategy for Substation Constraint Management via Home Batteries](http://arxiv.org/abs/2108.06735)


  Within electrical distribution networks, substation constraints management
requires that aggregated power demand from residential users is kept within
suitable bounds. Efficiency of substation constraints management can be
measured as the reduction of constraints violations w.r.t. unmanaged demand.
Home batteries hold the promise of enabling efficient and user-oblivious
substation constraints management. Centralized control of home batteries would
achieve optimal efficiency. However, it is hardly acceptable by users, since
service providers (e.g., utilities or aggregators) would directly control
batteries at user premises. Unfortunately, devising efficient hierarchical
control strategies, thus overcoming the above problem, is far from easy.
We present a novel two-layer control strategy for home batteries that avoids
direct control of home devices by the service provider and at the same time
yields near-optimal substation constraints management efficiency. Our
simulation results on field data from 62 households in Denmark show that the
substation constraints management efficiency achieved with our approach is at
least 82% of the one obtained with a theoretical optimal centralized strategy.

    

### [[2108.06742] Development of the InBan_CIDO Ontology by Reusing the Concepts along with Detecting Overlapping Information](http://arxiv.org/abs/2108.06742)


  The covid19 pandemic is a global emergency that badly impacted the economies
of various countries. Covid19 hit India when the growth rate of the country was
at the lowest in the last 10 years. To semantically analyze the impact of this
pandemic on the economy, it is curial to have an ontology. CIDO ontology is a
well standardized ontology that is specially designed to assess the impact of
coronavirus disease and utilize its results for future decision forecasting for
the government, industry experts, and professionals in the field of various
domains like research, medical advancement, technical innovative adoptions, and
so on. However, this ontology does not analyze the impact of the Covid19
pandemic on the Indian banking sector. On the other side, Covid19IBO ontology
has been developed to analyze the impact of the Covid19 pandemic on the Indian
banking sector but this ontology does not reflect complete information of
Covid19 data. Resultantly, users cannot get all the relevant information about
Covid19 and its impact on the Indian economy. This article aims to extend the
CIDO ontology to show the impact of Covid19 on the Indian economy sector by
reusing the concepts from other data sources. We also provide a simplified
schema matching approach that detects the overlapping information among the
ontologies. The experimental analysis proves that the proposed approach has
reasonable results.

    

### [[2108.06743] Exploring Generalization Ability of Pretrained Language Models on Arithmetic and Logical Reasoning](http://arxiv.org/abs/2108.06743)


  To quantitatively and intuitively explore the generalization ability of
pre-trained language models (PLMs), we have designed several tasks of
arithmetic and logical reasoning. We both analyse how well PLMs generalize when
the test data is in the same distribution as the train data and when it is
different, for the latter analysis, we have also designed a cross-distribution
test set other than the in-distribution test set. We conduct experiments on one
of the most advanced and publicly released generative PLM - BART. Our research
finds that the PLMs can easily generalize when the distribution is the same,
however, it is still difficult for them to generalize out of the distribution.

    

### [[2108.06763] Two Eyes Are Better Than One: Exploiting Binocular Correlation for Diabetic Retinopathy Severity Grading](http://arxiv.org/abs/2108.06763)


  Diabetic retinopathy (DR) is one of the most common eye conditions among
diabetic patients. However, vision loss occurs primarily in the late stages of
DR, and the symptoms of visual impairment, ranging from mild to severe, can
vary greatly, adding to the burden of diagnosis and treatment in clinical
practice. Deep learning methods based on retinal images have achieved
remarkable success in automatic DR grading, but most of them neglect that the
presence of diabetes usually affects both eyes, and ophthalmologists usually
compare both eyes concurrently for DR diagnosis, leaving correlations between
left and right eyes unexploited. In this study, simulating the diagnostic
process, we propose a two-stream binocular network to capture the subtle
correlations between left and right eyes, in which, paired images of eyes are
fed into two identical subnetworks separately during training. We design a
contrastive grading loss to learn binocular correlation for five-class DR
detection, which maximizes inter-class dissimilarity while minimizing the
intra-class difference. Experimental results on the EyePACS dataset show the
superiority of the proposed binocular model, outperforming monocular methods by
a large margin.

    

### [[2108.06789] Augmenting GRIPS with Heuristic Sampling for Planning Feasible Trajectories of a Car-Like Robot](http://arxiv.org/abs/2108.06789)


  Kinodynamic motion planning for non-holomonic mobile robots is a challenging
problem that is lacking a universal solution. One of the computationally
efficient ways to solve it is to build a geometric path first and then
transform this path into a kinematically feasible one. Gradient-informed Path
Smoothing (GRIPS) is a recently introduced method for such transformation.
GRIPS iteratively deforms the path and adds/deletes the waypoints while trying
to connect each consecutive pair of them via the provided steering function
that respects the kinematic constraints. The algorithm is relatively fast but,
unfortunately, does not provide any guarantees that it will succeed. In
practice, it often fails to produce feasible trajectories for car-like robots
with large turning radius. In this work, we introduce a range of modifications
that are aimed at increasing the success rate of GRIPS for car-like robots. The
main enhancement is adding the additional step that heuristically samples
waypoints along the bottleneck parts of the geometric paths (such as sharp
turns). The results of the experimental evaluation provide a clear evidence
that the success rate of the suggested algorithm is up to 40% higher compared
to the original GRIPS and hits the bar of 90%, while its runtime is lower.

    

### [[2108.06818] The Proximal ID Algorithm](http://arxiv.org/abs/2108.06818)


  Unobserved confounding is a fundamental obstacle to establishing valid causal
conclusions from observational data. Two complementary types of approaches have
been developed to address this obstacle. An extensive line of work is based on
taking advantage of fortuitous external aids (such as the presence of an
instrumental variable or other proxy), along with additional assumptions to
ensure identification. A recent line of work of proximal causal inference (Miao
et al., 2018a) has aimed to provide a novel approach to using proxies to deal
with unobserved confounding without relying on stringent parametric
assumptions. On the other hand, a complete characterization of identifiability
of a large class of causal parameters in arbitrary causal models with hidden
variables has been developed using the language of graphical models, resulting
in the ID algorithm and related extensions (Tian and Pearl, 2002; Shpitser and
Pearl, 2006a,b). Celebrated special cases of this approach, such as the
front-door model, are able to obtain non-parametric identification in seemingly
counter-intuitive situations when a treatment and an outcome share an
arbitrarily complicated unobserved common cause.
In this paper we aim to develop a synthesis of the proximal and graphical
approaches to identification in causal inference to yield the most general
identification algorithm in multi- variate systems currently known - the
proximal ID algorithm. In addition to being able to obtain non-parametric
identification in all cases where the ID algorithm succeeds, our approach
allows us to systematically exploit proxies to adjust for the presence of
unobserved confounders that would have otherwise prevented identification.
In addition, we outline a class of estimation strategies for causal
parameters identified by our method in an important special case. We
illustration our approach by simulation studies.

    

### [[2108.06832] A Fast Algorithm for Computing the Deficiency Number of a Mahjong Hand](http://arxiv.org/abs/2108.06832)


  The tile-based multiplayer game Mahjong is widely played in Asia and has also
become increasingly popular worldwide. Face-to-face or online, each player
begins with a hand of 13 tiles and players draw and discard tiles in turn until
they complete a winning hand. An important notion in Mahjong is the deficiency
number (a.k.a. shanten number in Japanese Mahjong) of a hand, which estimates
how many tile changes are necessary to complete the hand into a winning hand.
The deficiency number plays an essential role in major decision-making tasks
such as selecting a tile to discard. This paper proposes a fast algorithm for
computing the deficiency number of a Mahjong hand. Compared with the baseline
algorithm, the new algorithm is usually 100 times faster and, more importantly,
respects the agent's knowledge about available tiles. The algorithm can be used
as a basic procedure in all Mahjong variants by both rule-based and machine
learning-based Mahjong AI.

    

### [[2108.06839] Bayesian Parameter Estimations for Grey System Models in Online Traffic Speed Predictions](http://arxiv.org/abs/2108.06839)


  This paper presents Bayesian parameter estimation for first order Grey system
models' parameters (or sometimes referred to as hyperparameters). There are
different forms of first-order Grey System Models. These include $GM(1,1)$,
$GM(1,1| \cos(\omega t)$, $GM(1,1| \sin(\omega t)$, and $GM(1,1| \cos(\omega
t), \sin(\omega t)$. The whitenization equation of these models is a
first-order linear differential equation of the form \[ \frac{dx}{dt} + a x =
f(t) \] where $a$ is a parameter and $f(t) = b$ in $GM(1,1|)$ , $f(t) =
b_1\cos(\omega t) + b_2$ in $GM(1,1| cos(\omega t)$, $f(t) = b_1\sin(\omega
t)+b_2$ in $GM(1,1| \sin(\omega t)$, $f(t) = b_1\sin(\omega t) + b_2\cos(\omega
t) + b_3$ in $GM(1,1| \cos(\omega t), \sin(\omega t)$, $f(t) = b x^2$ in Grey
Verhulst model (GVM),
and where $b, b_1, b_2$, and $b_3$ are parameters. The results from Bayesian
estimations are compared to the least square estimated models with fixed
$\omega$. We found that using rolling Bayesian estimations for GM parameters
can allow us to estimate the parameters in all possible forms. Based on the
data used for the comparison, the numerical results showed that models with
Bayesian parameter estimations are up to 45\% more accurate in mean squared
errors.

    

### [[2108.06850] AdaCon: Adaptive Context-Aware Object Detection for Resource-Constrained Embedded Devices](http://arxiv.org/abs/2108.06850)


  Convolutional Neural Networks achieve state-of-the-art accuracy in object
detection tasks. However, they have large computational and energy requirements
that challenge their deployment on resource-constrained edge devices. Object
detection takes an image as an input, and identifies the existing object
classes as well as their locations in the image. In this paper, we leverage the
prior knowledge about the probabilities that different object categories can
occur jointly to increase the efficiency of object detection models. In
particular, our technique clusters the object categories based on their spatial
co-occurrence probability. We use those clusters to design an adaptive network.
During runtime, a branch controller decides which part(s) of the network to
execute based on the spatial context of the input frame. Our experiments using
COCO dataset show that our adaptive object detection model achieves up to 45%
reduction in the energy consumption, and up to 27% reduction in the latency,
with a small loss in the average precision (AP) of object detection.

    

### [[2108.06897] AutoChart: A Dataset for Chart-to-Text Generation Task](http://arxiv.org/abs/2108.06897)


  The analytical description of charts is an exciting and important research
area with many applications in academia and industry. Yet, this challenging
task has received limited attention from the computational linguistics research
community. This paper proposes \textsf{AutoChart}, a large dataset for the
analytical description of charts, which aims to encourage more research into
this important area. Specifically, we offer a novel framework that generates
the charts and their analytical description automatically. We conducted
extensive human and machine evaluations on the generated charts and
descriptions and demonstrate that the generated texts are informative,
coherent, and relevant to the corresponding charts.

    

### [[2108.06939] TL-SDD: A Transfer Learning-Based Method for Surface Defect Detection with Few Samples](http://arxiv.org/abs/2108.06939)


  Surface defect detection plays an increasingly important role in
manufacturing industry to guarantee the product quality. Many deep learning
methods have been widely used in surface defect detection tasks, and have been
proven to perform well in defects classification and location. However, deep
learning-based detection methods often require plenty of data for training,
which fail to apply to the real industrial scenarios since the distribution of
defect categories is often imbalanced. In other words, common defect classes
have many samples but rare defect classes have extremely few samples, and it is
difficult for these methods to well detect rare defect classes. To solve the
imbalanced distribution problem, in this paper we propose TL-SDD: a novel
Transfer Learning-based method for Surface Defect Detection. First, we adopt a
two-phase training scheme to transfer the knowledge from common defect classes
to rare defect classes. Second, we propose a novel Metric-based Surface Defect
Detection (M-SDD) model. We design three modules for this model: (1) feature
extraction module: containing feature fusion which combines high-level semantic
information with low-level structural information. (2) feature reweighting
module: transforming examples to a reweighting vector that indicates the
importance of features. (3) distance metric module: learning a metric space in
which defects are classified by computing distances to representations of each
category. Finally, we validate the performance of our proposed method on a real
dataset including surface defects of aluminum profiles. Compared to the
baseline methods, the performance of our proposed method has improved by up to
11.98% for rare defect classes.

    

### [[2108.06957] An Effective System for Multi-format Information Extraction](http://arxiv.org/abs/2108.06957)


  The multi-format information extraction task in the 2021 Language and
Intelligence Challenge is designed to comprehensively evaluate information
extraction from different dimensions. It consists of an multiple slots relation
extraction subtask and two event extraction subtasks that extract events from
both sentence-level and document-level. Here we describe our system for this
multi-format information extraction competition task. Specifically, for the
relation extraction subtask, we convert it to a traditional triple extraction
task and design a voting based method that makes full use of existing models.
For the sentence-level event extraction subtask, we convert it to a NER task
and use a pointer labeling based method for extraction. Furthermore,
considering the annotated trigger information may be helpful for event
extraction, we design an auxiliary trigger recognition model and use the
multi-task learning mechanism to integrate the trigger features into the event
extraction model. For the document-level event extraction subtask, we design an
Encoder-Decoder based method and propose a Transformer-alike decoder.
Finally,our system ranks No.4 on the test set leader-board of this multi-format
information extraction task, and its F1 scores for the subtasks of relation
extraction, event extractions of sentence-level and document-level are 79.887%,
85.179%, and 70.828% respectively. The codes of our model are available at
{this https URL}.

    

### [[2108.07005] An Effective Non-Autoregressive Model for Spoken Language Understanding](http://arxiv.org/abs/2108.07005)


  Spoken Language Understanding (SLU), a core component of the task-oriented
dialogue system, expects a shorter inference latency due to the impatience of
humans. Non-autoregressive SLU models clearly increase the inference speed but
suffer uncoordinated-slot problems caused by the lack of sequential dependency
information among each slot chunk. To gap this shortcoming, in this paper, we
propose a novel non-autoregressive SLU model named Layered-Refine Transformer,
which contains a Slot Label Generation (SLG) task and a Layered Refine
Mechanism (LRM). SLG is defined as generating the next slot label with the
token sequence and generated slot labels. With SLG, the non-autoregressive
model can efficiently obtain dependency information during training and spend
no extra time in inference. LRM predicts the preliminary SLU results from
Transformer's middle states and utilizes them to guide the final prediction.
Experiments on two public datasets indicate that our model significantly
improves SLU performance (1.5\% on Overall accuracy) while substantially speed
up (more than 10 times) the inference process over the state-of-the-art
baseline.

    

### [[2108.07081] Toward the Understanding of Deep Text Matching Models for Information Retrieval](http://arxiv.org/abs/2108.07081)


  Semantic text matching is a critical problem in information retrieval.
Recently, deep learning techniques have been widely used in this area and
obtained significant performance improvements. However, most models are black
boxes and it is hard to understand what happened in the matching process, due
to the poor interpretability of deep learning. This paper aims at tackling this
problem. The key idea is to test whether existing deep text matching methods
satisfy some fundamental heuristics in information retrieval. Specifically,
four heuristics are used in our study, i.e., term frequency constraint, term
discrimination constraint, length normalization constraints, and TF-length
constraint. Since deep matching models usually contain many parameters, it is
difficult to conduct a theoretical study for these complicated functions. In
this paper, We propose an empirical testing method. Specifically, We first
construct some queries and documents to make them satisfy the assumption in a
constraint, and then test to which extend a deep text matching model trained on
the original dataset satisfies the corresponding constraint. Besides, a famous
attribution based interpretation method, namely integrated gradient, is adopted
to conduct detailed analysis and guide for feasible improvement. Experimental
results on LETOR 4.0 and MS Marco show that all the investigated deep text
matching methods, both representation and interaction based methods, satisfy
the above constraints with high probabilities in statistics. We further extend
these constraints to the semantic settings, which are shown to be better
satisfied for all the deep text matching models. These empirical findings give
clear understandings on why deep text matching models usually perform well in
information retrieval. We believe the proposed evaluation methodology will be
useful for testing future deep text matching models.

    

### [[2108.07084] Learning Canonical View Representation for 3D Shape Recognition with Arbitrary Views](http://arxiv.org/abs/2108.07084)


  In this paper, we focus on recognizing 3D shapes from arbitrary views, i.e.,
arbitrary numbers and positions of viewpoints. It is a challenging and
realistic setting for view-based 3D shape recognition. We propose a canonical
view representation to tackle this challenge. We first transform the original
features of arbitrary views to a fixed number of view features, dubbed
canonical view representation, by aligning the arbitrary view features to a set
of learnable reference view features using optimal transport. In this way, each
3D shape with arbitrary views is represented by a fixed number of canonical
view features, which are further aggregated to generate a rich and robust 3D
shape representation for shape recognition. We also propose a canonical view
feature separation constraint to enforce that the view features in canonical
view representation can be embedded into scattered points in a Euclidean space.
Experiments on the ModelNet40, ScanObjectNN, and RGBD datasets show that our
method achieves competitive results under the fixed viewpoint settings, and
significantly outperforms the applicable methods under the arbitrary view
setting.

    

### [[2108.07118] NIST SRE CTS Superset: A large-scale dataset for telephony speaker recognition](http://arxiv.org/abs/2108.07118)


  This document provides a brief description of the National Institute of
Standards and Technology (NIST) speaker recognition evaluation (SRE)
conversational telephone speech (CTS) Superset. The CTS Superset has been
created in an attempt to provide the research community with a large-scale
dataset along with uniform metadata that can be used to effectively train and
develop telephony (narrowband) speaker recognition systems. It contains a large
number of telephony speech segments from more than 6800 speakers with speech
durations distributed uniformly in the [10s, 60s] range. The segments have been
extracted from the source corpora used to compile prior SRE datasets
(SRE1996-2012), including the Greybeard corpus as well as the Switchboard and
Mixer series collected by the Linguistic Data Consortium (LDC). In addition to
the brief description, we also report speaker recognition results on the NIST
2020 CTS Speaker Recognition Challenge, obtained using a system trained with
the CTS Superset. The results will serve as a reference baseline for the
challenge.

    

### [[2108.07119] Creating and Querying Personalized Versions ofWikidata on a Laptop](http://arxiv.org/abs/2108.07119)


  Application developers today have three choices for exploiting the knowledge
present in Wikidata: they can download the Wikidata dumps in JSON or RDF
format, they can use the Wikidata API to get data about individual entities, or
they can use the Wikidata SPARQL endpoint. None of these methods can support
complex, yet common, query use cases, such as retrieval of large amounts of
data or aggregations over large fractions of Wikidata. This paper introduces
KGTK Kypher, a query language and processor that allows users to create
personalized variants of Wikidata on a laptop. We present several use cases
that illustrate the types of analyses that Kypher enables users to run on the
full Wikidata KG on a laptop, combining data from external resources such as
DBpedia. The Kypher queries for these use cases run much faster on a laptop
than the equivalent SPARQL queries on a Wikidata clone running on a powerful
server with 24h time-out limits.

    

### [[2108.07129] Autoencoders as Tools for Program Synthesis](http://arxiv.org/abs/2108.07129)


  Recently there have been many advances in research on language modeling of
source code. Applications range from code suggestion and completion to code
summarization. However, complete program synthesis of industry-grade
programming languages has not been researched extensively. In this work, we
introduce a variational autoencoder model for program synthesis of
industry-grade programming languages. Our model incorporates the internal
hierarchical structure of source codes and operates on parse trees. By learning
a latent representation of source code over trees, we capture more information
and achieve a higher performance than standard autoregressive autoencoder
models. Furthermore, due to the tree-structured nature of our model, the
autoregressive operations are performed on paths of trees instead of linear
sequences. Therefore, the size of the sequences that the autoregressive model
processes, scales proportionally to the width and depth of the tree instead of
the total size of the tree which mitigates the common problem of exploding and
vanishing gradients.

    

### [[1802.07810] Manipulating and Measuring Model Interpretability](http://arxiv.org/abs/1802.07810)


  With machine learning models being increasingly used to aid decision making
even in high-stakes domains, there has been a growing interest in developing
interpretable models. Although many supposedly interpretable models have been
proposed, there have been relatively few experimental studies investigating
whether these models achieve their intended effects, such as making people more
closely follow a model's predictions when it is beneficial for them to do so or
enabling them to detect when a model has made a mistake. We present a sequence
of pre-registered experiments (N=3,800) in which we showed participants
functionally identical models that varied only in two factors commonly thought
to make machine learning models more or less interpretable: the number of
features and the transparency of the model (i.e., whether the model internals
are clear or black box). Predictably, participants who saw a clear model with
few features could better simulate the model's predictions. However, we did not
find that participants more closely followed its predictions. Furthermore,
showing participants a clear model meant that they were less able to detect and
correct for the model's sizable mistakes, seemingly due to information
overload. These counterintuitive findings emphasize the importance of testing
over intuition when developing interpretable models.

    

### [[2011.12024] RIN: Textured Human Model Recovery and Imitation with a Single Image](http://arxiv.org/abs/2011.12024)


  Human imitation has become topical recently, driven by GAN's ability to
disentangle human pose and body content. However, the latest methods hardly
focus on 3D information, and to avoid self-occlusion, a massive amount of input
images are needed. In this paper, we propose RIN, a novel volume-based
framework for reconstructing a textured 3D model from a single picture and
imitating a subject with the generated model. Specifically, to estimate most of
the human texture, we propose a U-Net-like front-to-back translation network.
With both front and back images input, the textured volume recovery module
allows us to color a volumetric human. A sequence of 3D poses then guides the
colored volume via Flowable Disentangle Networks as a volume-to-volume
translation task. To project volumes to a 2D plane during training, we design a
differentiable depth-aware renderer. Our experiments demonstrate that our
volume-based model is adequate for human imitation, and the back view can be
estimated reliably using our network. While prior works based on either 2D pose
or semantic map often fail for the unstable appearance of a human, our
framework can still produce concrete results, which are competitive to those
imagined from multi-view input.

    

### [[2012.14395] Enhanced Regularizers for Attributional Robustness](http://arxiv.org/abs/2012.14395)


  Deep neural networks are the default choice of learning models for computer
vision tasks. Extensive work has been carried out in recent years on explaining
deep models for vision tasks such as classification. However, recent work has
shown that it is possible for these models to produce substantially different
attribution maps even when two very similar images are given to the network,
raising serious questions about trustworthiness. To address this issue, we
propose a robust attribution training strategy to improve attributional
robustness of deep neural networks. Our method carefully analyzes the
requirements for attributional robustness and introduces two new regularizers
that preserve a model's attribution map during attacks. Our method surpasses
state-of-the-art attributional robustness methods by a margin of approximately
3% to 9% in terms of attribution robustness measures on several datasets
including MNIST, FMNIST, Flower and GTSRB.

    

### [[2103.05927] Deep Sensing of Urban Waterlogging](http://arxiv.org/abs/2103.05927)


  In the monsoon season, sudden flood events occur frequently in urban areas,
which hamper the social and economic activities and may threaten the
infrastructure and lives. The use of an efficient large-scale waterlogging
sensing and information system can provide valuable real-time disaster
information to facilitate disaster management and enhance awareness of the
general public to alleviate losses during and after flood disasters. Therefore,
in this study, a visual sensing approach driven by deep neural networks and
information and communication technology was developed to provide an end-to-end
mechanism to realize waterlogging sensing and event-location mapping. The use
of a deep sensing system in the monsoon season in Taiwan was demonstrated, and
waterlogging events were predicted on the island-wide scale. The system could
sense approximately 2379 vision sources through an internet of video things
framework and transmit the event-location information in 5 min. The proposed
approach can sense waterlogging events at a national scale and provide an
efficient and highly scalable alternative to conventional waterlogging sensing
methods.

    

### [[2104.03349] Enabling Integration and Interaction for Decentralized Artificial Intelligence in Airline Disruption Management](http://arxiv.org/abs/2104.03349)


  Airline disruption management traditionally seeks to address three problem
dimensions: aircraft scheduling, crew scheduling, and passenger scheduling, in
that order. However, current efforts have, at most, only addressed the first
two problem dimensions concurrently and do not account for the propagative
effects that uncertain scheduling outcomes in one dimension can have on another
dimension. In addition, existing approaches for airline disruption management
include human specialists who decide on necessary corrective actions for
airline schedule disruptions on the day of operation. However, human
specialists are limited in their ability to process copious amounts of
information imperative for making robust decisions that simultaneously address
all problem dimensions during disruption management. Therefore, there is a need
to augment the decision-making capabilities of a human specialist with
quantitative and qualitative tools that can rationalize complex interactions
amongst all dimensions in airline disruption management, and provide objective
insights to the specialists in the airline operations control center. To that
effect, we provide a discussion and demonstration of an agnostic and systematic
paradigm for enabling expeditious simultaneously-integrated recovery of all
problem dimensions during airline disruption management, through an intelligent
multi-agent system that employs principles from artificial intelligence and
distributed ledger technology. Results indicate that our paradigm for
simultaneously-integrated recovery is effective when all the flights in the
airline route network are disrupted.

    

### [[2105.10369] Hierarchical Consistency Regularized Mean Teacher for Semi-supervised 3D Left Atrium Segmentation](http://arxiv.org/abs/2105.10369)


  Deep learning has achieved promising segmentation performance on 3D left
atrium MR images. However, annotations for segmentation tasks are expensive,
costly and difficult to obtain. In this paper, we introduce a novel
hierarchical consistency regularized mean teacher framework for 3D left atrium
segmentation. In each iteration, the student model is optimized by multi-scale
deep supervision and hierarchical consistency regularization, concurrently.
Extensive experiments have shown that our method achieves competitive
performance as compared with full annotation, outperforming other
state-of-the-art semi-supervised segmentation methods.

    

### [[2106.04011] JANUS: Parallel Tempered Genetic Algorithm Guided by Deep Neural Networks for Inverse Molecular Design](http://arxiv.org/abs/2106.04011)


  Inverse molecular design, i.e., designing molecules with specific target
properties, can be posed as an optimization problem. High-dimensional
optimization tasks in the natural sciences are commonly tackled via
population-based metaheuristic optimization algorithms such as evolutionary
algorithms. However, expensive property evaluation, which is often required,
can limit the widespread use of such approaches as the associated cost can
become prohibitive. Herein, we present JANUS, a genetic algorithm that is
inspired by parallel tempering. It propagates two populations, one for
exploration and another for exploitation, improving optimization by reducing
expensive property evaluations. Additionally, JANUS is augmented by a deep
neural network that approximates molecular properties via active learning for
enhanced sampling of the chemical space. Our method uses the SELFIES molecular
representation and the STONED algorithm for the efficient generation of
structures, and outperforms other generative models in common inverse molecular
design tasks achieving state-of-the-art performance.

    

### [[2106.04026] Subject-Independent Brain-Computer Interface for Decoding High-Level Visual Imagery Tasks](http://arxiv.org/abs/2106.04026)


  Brain-computer interface (BCI) is used for communication between humans and
devices by recognizing status and intention of humans. Communication between
humans and a drone using electroencephalogram (EEG) signals is one of the most
challenging issues in the BCI domain. In particular, the control of drone
swarms (the direction and formation) has more advantages compared to the
control of a drone. The visual imagery (VI) paradigm is that subjects visually
imagine specific objects or scenes. Reduction of the variability among EEG
signals of subjects is essential for practical BCI-based systems. In this
study, we proposed the subepoch-wise feature encoder (SEFE) to improve the
performances in the subject-independent tasks by using the VI dataset. This
study is the first attempt to demonstrate the possibility of generalization
among subjects in the VI-based BCI. We used the leave-one-subject-out
cross-validation for evaluating the performances. We obtained higher
performances when including our proposed module than excluding our proposed
module. The DeepConvNet with SEFE showed the highest performance of 0.72 among
six different decoding models. Hence, we demonstrated the feasibility of
decoding the VI dataset in the subject-independent task with robust
performances by using our proposed module.

    

### [[2106.07340] MathBERT: A Pre-trained Language Model for General NLP Tasks in Mathematics Education](http://arxiv.org/abs/2106.07340)


  Since the introduction of the original BERT (i.e., BASE BERT), researchers
have developed various customized BERT models with improved performance for
specific domains and tasks by exploiting the benefits of {\em transfer
learning}. Due to the nature of mathematical texts, which often use domain
specific vocabulary along with equations and math symbols, we posit that the
development of a new BERT model for mathematics would be useful for many
mathematical downstream tasks. In this resource paper, we introduce our
multi-institutional effort (i.e., two learning platforms and three academic
institutions in the US) toward this need: MathBERT, a model created by
pre-training the BASE BERT model on a large mathematical corpus ranging from
pre-kindergarten (pre-k), to high-school, to college graduate level
mathematical content. In addition, we select three general NLP tasks that are
often used in mathematics education: prediction of knowledge component,
auto-grading open-ended Q\&A, and knowledge tracing, to demonstrate the
superiority of MathBERT over BASE BERT. Our experiments show that MathBERT
outperforms prior best methods by 1.2-22\% and BASE BERT by 2-8\% on these
tasks. In addition, we build a mathematics specific vocabulary `mathVocab' to
train with MathBERT. We discover that MathBERT pre-trained with `mathVocab'
outperforms MathBERT trained with the BASE BERT vocabulary (i.e., `origVocab').
MathBERT is currently being adopted at the participated leaning platforms:
Stride, Inc, a commercial educational resource provider, and this http URL, a
free online educational platform. We release MathBERT for public usage at:
this https URL.

    

### [[2106.15150] The Price of Selfishness: Conjunctive Query Entailment for ALCSelf is 2ExpTime-hard](http://arxiv.org/abs/2106.15150)


  In logic-based knowledge representation, query answering has essentially
replaced mere satisfiability checking as the inferencing problem of primary
interest. For knowledge bases in the basic description logic ALC, the
computational complexity of conjunctive query (CQ) answering is well known to
be ExpTime-complete and hence not harder than satisfiability. This does not
change when the logic is extended by certain features (such as counting or role
hierarchies), whereas adding others (inverses, nominals or transitivity
together with role-hierarchies) turns CQ answering exponentially harder. We
contribute to this line of results by showing the surprising fact that even
extending ALC by just the Self operator - which proved innocuous in many other
contexts - increases the complexity of CQ entailment to 2ExpTime. As common for
this type of problem, our proof establishes a reduction from alternating Turing
machines running in exponential space, but several novel ideas and encoding
tricks are required to make the approach work in that specific, restricted
setting.

    

### [[2108.04106] Noisy Channel Language Model Prompting for Few-Shot Text Classification](http://arxiv.org/abs/2108.04106)


  We introduce a noisy channel approach for language model prompting in
few-shot text classification. Instead of computing the likelihood of the label
given the input (referred as direct models), channel models compute the
conditional probability of the input given the label, and are thereby required
to explain every word in the input. We use channel models for recently proposed
few-shot learning methods with no or very limited updates to the language model
parameters, via either in-context demonstration or prompt tuning. Our
experiments show that, for both methods, channel models significantly
outperform their direct counterparts, which we attribute to their stability,
i.e., lower variance and higher worst-case accuracy. We also present extensive
ablations that provide recommendations for when to use channel prompt tuning
instead of other competitive models (e.g., direct head tuning): channel prompt
tuning is preferred when the number of training examples is small, labels in
the training data are imbalanced, or generalization to unseen labels is
required.

    

### [[2108.07031] On the performance of GPU accelerated q-LSKUM based meshfree solvers in Fortran, C++, Python, and Julia](http://arxiv.org/abs/2108.07031)


  This report presents a comprehensive analysis of the performance of GPU
accelerated meshfree CFD solvers for two-dimensional compressible flows in
Fortran, C++, Python, and Julia. The programming model CUDA is used to develop
the GPU codes. The meshfree solver is based on the least squares kinetic upwind
method with entropy variables (q-LSKUM). To assess the computational efficiency
of the GPU solvers and to compare their relative performance, benchmark
calculations are performed on seven levels of point distribution. To analyse
the difference in their run-times, the computationally intensive kernel is
profiled. Various performance metrics are investigated from the profiled data
to determine the cause of observed variation in run-times. To address some of
the performance related issues, various optimisation strategies are employed.
The optimised GPU codes are compared with the naive codes, and conclusions are
drawn from their performance.

    

### [[2108.06363] Augmenting Decompiler Output with Learned Variable Names and Types](http://arxiv.org/abs/2108.06363)


  A common tool used by security professionals for reverse-engineering binaries
found in the wild is the decompiler. A decompiler attempts to reverse
compilation, transforming a binary to a higher-level language such as C.
High-level languages ease reasoning about programs by providing useful
abstractions such as loops, typed variables, and comments, but these
abstractions are lost during compilation. Decompilers are able to
deterministically reconstruct structural properties of code, but comments,
variable names, and custom variable types are technically impossible to
recover.
In this paper we present DIRTY (DecompIled variable ReTYper), a novel
technique for improving the quality of decompiler output that automatically
generates meaningful variable names and types. Empirical evaluation on a novel
dataset of C code mined from GitHub shows that DIRTY outperforms prior work
approaches by a sizable margin, recovering the original names written by
developers 66.4% of the time and the original types 75.8% of the time.

    

### [[2108.06562] Data Type Inference for Logic Programming](http://arxiv.org/abs/2108.06562)


  In this paper we present a new static data type inference algorithm for logic
programming. Without the need of declaring types for predicates, our algorithm
is able to automatically assign types to predicates which, in most cases,
correspond to the data types processed by their intended meaning. The algorithm
is also able to infer types given data type definitions similar to data
definitions in Haskell and, in this case, the inferred types are more
informative in general. We present the type inference algorithm, prove some
properties and finally, we evaluate our approach on example programs that deal
with different data structures.

    

### [[2108.06944] Verifying C11-Style Weak Memory Libraries via Refinement](http://arxiv.org/abs/2108.06944)


  Deductive verification of concurrent programs under weak memory has thus far
been limited to simple programs over a monolithic state space. For scalability,
we also require modular techniques with verifiable library abstractions. This
paper addresses this challenge in the context of RC11 RAR, a subset of the C11
memory model that admits relaxed and release-acquire accesses, but disallows,
so-called, load-buffering cycles. We develop a simple framework for specifying
abstract objects that precisely characterises the observability guarantees of
abstract method calls. We show how this framework can be integrated with an
operational semantics that enables verification of client programs that execute
abstract method calls from a library they use. Finally, we show how
implementations of such abstractions in RC11 RAR can be verified by developing
a (contextual) refinement framework for abstract objects. Our framework,
including the operational semantics, verification technique for client-library
programs, and simulation between abstract libraries and their implementations,
has been mechanised in Isabelle/HOL.

    

### [[2108.07075] Systematic Generation of Conformance Tests for JavaScript](http://arxiv.org/abs/2108.07075)


  JavaScript implementations are tested for conformance to the ECMAScript
standard using a large hand-written test suite. Not only in this a tedious
approach, it also relies solely on the natural language specification for
differentiating behaviors, while hidden implementation details can also affect
behavior and introduce divergences. We propose to generate conformance tests
through dynamic symbolic execution of polyfills, drop-in replacements for newer
JavaScript language features that are not yet widely supported. We then run
these generated tests against multiple implementations of JavaScript, using a
majority vote to identify the correct behavior. To facilitate test generation
for polyfill code, we introduce a model for structured symbolic inputs that is
suited to the dynamic nature of JavaScript. In our evaluation, we found 17
divergences in the widely used core-js polyfill and were able to increase
branch coverage in interpreter code by up to 15%. Because polyfills are
typically written even before standardization, our approach will allow to
maintain and extend standardization test suites with reduced effort.

    

### [[2108.07132] Automating the Functional Correspondence between Higher-Order Evaluators and Abstract Machines](http://arxiv.org/abs/2108.07132)


  The functional correspondence is a manual derivation technique transforming
higher-order evaluators into the semantically equivalent abstract machines. The
transformation consists of two well-known program transformations: translation
to continuation-passing style that uncovers the control flow of the evaluator
and Reynolds's defunctionalization that generates a first-order transition
function. Ever since the transformation was first described by Danvy et al. it
has found numerous applications in connecting known evaluators and abstract
machines, but also in discovering new abstract machines for a variety of
$\lambda$-calculi as well as for logic-programming, imperative and
object-oriented languages.
We present an algorithm that automates the functional correspondence. The
algorithm accepts an evaluator written in a dedicated minimal functional
meta-language and it first transforms it to administrative normal form, which
facilitates program analysis, before performing selective translation to
continuation-passing style, and selective defunctionalization. The two
selective transformations are driven by a control-flow analysis that is
computed by an abstract interpreter obtained using the abstracting abstract
machines methodology, which makes it possible to transform only the desired
parts of the evaluator. The article is accompanied by an implementation of the
algorithm in the form of a command-line tool that allows for automatic
transformation of an evaluator embedded in a Racket source file and gives
fine-grained control over the resulting machine.

    

### [[2101.11030] Enabling Dataflow Optimization for Quantum Programs](http://arxiv.org/abs/2101.11030)


  We propose an IR for quantum computing that directly exposes quantum and
classical data dependencies for the purpose of optimization. The Quantum
Intermediate Representation for Optimization (QIRO) consists of two dialects,
one input dialect and one that is specifically tailored to enable
quantum-classical co-optimization. While the first employs a perhaps more
intuitive memory-semantics (quantum operations act as side-effects), the latter
uses value-semantics (operations consume and produce states). Crucially, this
encodes the dataflow directly in the IR, allowing for a host of optimizations
that leverage dataflow analysis. We discuss how to map existing quantum
programming languages to the input dialect and how to lower the resulting IR to
the optimization dialect. We present a prototype implementation based on MLIR
that includes several quantum-specific optimization passes. Our benchmarks show
that significant improvements in resource requirements are possible even
through static optimization. In contrast to circuit optimization at run time,
this is achieved while incurring only a small constant overhead in compilation
time, making this a compelling approach for quantum program optimization at
application scale.

    

### [<title>XGBoost Error: must have exactly ngroup * nrow gpairs - XGBoost</title>](https://discuss.xgboost.ai/t/xgboost-error-must-have-exactly-ngroup-nrow-gpairs/2438/1)

### [<title>Negative labels in XGBRanker w/ listwise objective - XGBoost</title>](https://discuss.xgboost.ai/t/negative-labels-in-xgbranker-w-listwise-objective/1655/8)

### [<title>Negative labels in XGBRanker w/ listwise objective - XGBoost</title>](https://discuss.xgboost.ai/t/negative-labels-in-xgbranker-w-listwise-objective/1655/7)

### [<title>专注于)南京如何开具不锈钢发票-南京本地宝pu - DockOne.io</title>](http://dockone.io/question/985986)

### [<title>关于长沙如何开住宿费发票-长沙本地宝xq - DockOne.io</title>](http://dockone.io/question/985985)

### [<title>关于哈尔滨如何开住宿费发票-哈尔滨本地宝xw - DockOne.io</title>](http://dockone.io/question/985984)

### [<title>关于长春哪里可以开具手撕发票-长春本地宝wt - DockOne.io</title>](http://dockone.io/question/985983)

### [<title>关于海口哪里可以开具手撕发票-海口本地宝en - DockOne.io</title>](http://dockone.io/question/985982)

### [<title>专注于)苏州如何开具不锈钢发票-苏州本地宝uu - DockOne.io</title>](http://dockone.io/question/985981)

### [<title>专注于)江苏如何开具不锈钢发票-江苏本地宝xl - DockOne.io</title>](http://dockone.io/question/985980)

### [<title>关于三亚哪里可以开具手撕发票-三亚本地宝ui - DockOne.io</title>](http://dockone.io/question/985979)

### [<title>关于石家庄如何开住宿费发票-石家庄本地宝ci - DockOne.io</title>](http://dockone.io/question/985978)

### [<title>关于济南如何开住宿费发票-济南本地宝qi - DockOne.io</title>](http://dockone.io/question/985977)

### [<title>关于太原哪里可以开具手撕发票-太原本地宝zk - DockOne.io</title>](http://dockone.io/question/985976)

### [<title>专注于)郑州如何开具不锈钢发票-郑州本地宝ew - DockOne.io</title>](http://dockone.io/question/985975)

### [<title>专注于)河南如何开具不锈钢发票-郑州本地宝bf - DockOne.io</title>](http://dockone.io/question/985974)

### [<title>关于合肥哪里可以开具手撕发票-合肥本地宝fm - DockOne.io</title>](http://dockone.io/question/985973)

### [<title>关于沈阳如何开住宿费发票-沈阳本地宝lq - DockOne.io</title>](http://dockone.io/question/985972)

### [<title>关于长春如何开住宿费发票-长春本地宝wo - DockOne.io</title>](http://dockone.io/question/985971)

### [<title>专注于)重庆如何开具不锈钢发票-重庆本地宝am - DockOne.io</title>](http://dockone.io/question/985970)

### [<title>关于西安哪里可以开具手撕发票-西安本地宝vx - DockOne.io</title>](http://dockone.io/question/985969)

### [<title>关于武汉哪里可以开具手撕发票-武汉本地宝sx - DockOne.io</title>](http://dockone.io/question/985968)

### [<title>专注于)天津如何开具不锈钢发票-天津本地宝aq - DockOne.io</title>](http://dockone.io/question/985967)