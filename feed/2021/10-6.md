
## 2021-10-6

### [<title>Error during installation R GPU win64: 'R' is not recognized as an internal or external command, operable program or batch file - XGBoost</title>](https://discuss.xgboost.ai/t/error-during-installation-r-gpu-win64-r-is-not-recognized-as-an-internal-or-external-command-operable-program-or-batch-file/2487/2)

### [[2110.01686] Learning, Computing, and Trustworthiness in Intelligent IoT Environments: Performance-Energy Tradeoffs](http://arxiv.org/abs/2110.01686)


  An Intelligent IoT Environment (iIoTe) is comprised of heterogeneous devices
that can collaboratively execute semi-autonomous IoT applications, examples of
which include highly automated manufacturing cells or autonomously interacting
harvesting machines. Energy efficiency is key in such edge environments, since
they are often based on an infrastructure that consists of wireless and
battery-run devices, e.g., e-tractors, drones, Automated Guided Vehicle (AGV)s
and robots. The total energy consumption draws contributions from multipleiIoTe
technologies that enable edge computing and communication, distributed
learning, as well as distributed ledgers and smart contracts. This paper
provides a state-of-the-art overview of these technologies and illustrates
their functionality and performance, with special attention to the tradeoff
among resources, latency, privacy and energy consumption. Finally, the paper
provides a vision for integrating these enabling technologies in
energy-efficient iIoTe and a roadmap to address the open research challenges

    

### [[2110.01726] On-Demand Networking for Ubiquitous Connectivity and Network Resilience: A Network-in-a-Box Solution](http://arxiv.org/abs/2110.01726)


  Recently, the wireless community has initiated research on the sixth
generation (6G) cellular network for the next decade. The 6G visions are still
under development but are converging toward ubiquitous, sustainable, and
automated digital society. A network-in-a-box (NIB) is a portable and
fully-fledged networking solution that has many potentials to stimulate 6G
visions, especially for ubiquitous and resilient network connectivity. In this
article, we highlight how NIB features suit 6G use cases and requirements and
how it can be used for 6G communications. In addition, we discuss the
challenges of the potential enabling technologies of 6G that can reinforce the
NIB performance.

    

### [[2110.01863] DeepEdge: A Deep Reinforcement Learning based Task Orchestrator for Edge Computing](http://arxiv.org/abs/2110.01863)


  The improvements in the edge computing technology pave the road for
diversified applications that demand real-time interaction. However, due to the
mobility of the end-users and the dynamic edge environment, it becomes
challenging to handle the task offloading with high performance. Moreover,
since each application in mobile devices has different characteristics, a task
orchestrator must be adaptive and have the ability to learn the dynamics of the
environment. For this purpose, we develop a deep reinforcement learning based
task orchestrator, DeepEdge, which learns to meet different task requirements
without needing human interaction even under the heavily-loaded stochastic
network conditions in terms of mobile users and applications. Given the dynamic
offloading requests and time-varying communication conditions, we successfully
model the problem as a Markov process and then apply the Double Deep Q-Network
(DDQN) algorithm to implement DeepEdge. To evaluate the robustness of DeepEdge,
we experiment with four different applications including image rendering,
infotainment, pervasive health, and augmented reality in the network under
various loads. Furthermore, we compare the performance of our agent with the
four different task offloading approaches in the literature. Our results show
that DeepEdge outperforms its competitors in terms of the percentage of
satisfactorily completed tasks.

    

### [[2110.01910] Remote and Rural Connectivity: Infrastructure and Resource Sharing Principles](http://arxiv.org/abs/2110.01910)


  As Mobile Networks (MNs) are advancing towards meeting mobile users
requirements, the rural-urban divide still remains a major challenge. While
areas within the urban space (metropolitan mobile space) are being developed,
rural areas are left behind. Due to challenges of low population density, low
income, difficult terrain, non-existent infrastructure, lack of power grid,
remote areas have low digital penetration. This situation makes remote areas
less attractive towards investments and to operate connectivity networks, thus
failing to achieve universal access to the Internet. In addressing this issue,
this paper proposes a new BS deployment and resource management method for
remote and rural areas. Here, two MN operators share their resources towards
the procurement and deployment of green energy-powered BSs equipped with
computing capabilities. Then, the network infrastructure is shared between the
mobile operators, with the main goal of enabling energy-efficient
infrastructure sharing, i.e., BS and its co-located computing platform. Using
this resource management strategy in rural communication sites guarantees a
Quality of Service (QoS) comparable to that of urban communication sites. The
performance evaluation conducted through simulations validates our analysis as
the prediction variations observed shows greater accuracy between the harvested
energy and the traffic load. Also, the energy savings decrease as the number of
mobile users (50 users in our case) connected to the remote site increases.
Lastly, the proposed algorithm achieves 51% energy savings when compared with
the 43% obtained by our benchmark algorithm. The proposed method demonstrates
superior performance over the benchmark algorithm as it uses foresighted
optimization where the harvested energy and the expected load are predicted
over a given short-term horizon.

    

### [[2110.02040] An Approach of Replicating Multi-Staged Cyber-Attacks and Countermeasures in a Smart Grid Co-Simulation Environment](http://arxiv.org/abs/2110.02040)


  While the digitization of power distribution grids brings many benefits, it
also introduces new vulnerabilities for cyber-attacks. To maintain secure
operations in the emerging threat landscape, detecting and implementing
countermeasures against cyber-attacks are paramount. However, due to the lack
of publicly available attack data against Smart Grids (SGs) for countermeasure
development, simulation-based data generation approaches offer the potential to
provide the needed data foundation. Therefore, our proposed approach provides
flexible and scalable replication of multi-staged cyber-attacks in an SG
Co-Simulation Environment (COSE). The COSE consists of an energy grid
simulator, simulators for Operation Technology (OT) devices, and a network
emulator for realistic IT process networks. Focusing on defensive and offensive
use cases in COSE, our simulated attacker can perform network scans, find
vulnerabilities, exploit them, gain administrative privileges, and execute
malicious commands on OT devices. As an exemplary countermeasure, we present a
built-in Intrusion Detection System (IDS) that analyzes generated network
traffic using anomaly detection with Machine Learning (ML) approaches. In this
work, we provide an overview of the SG COSE, present a multi-stage attack model
with the potential to disrupt grid operations, and show exemplary performance
evaluations of the IDS in specific scenarios.

    

### [[2012.12179] Timely Monitoring of Dynamic Sources with Observations from Multiple Wireless Sensors](http://arxiv.org/abs/2012.12179)


  Age of Information (AoI) has recently received much attention due to its
relevance in IoT sensing and monitoring applications. In this paper, we
consider the problem of minimizing the AoI in a system in which a set of
sources are observed by multiple sensors in a many-to-many relationship, and
the probability that a sensor observes a source depends on the state of the
source. This model represents many practical scenarios, such as the ones in
which multiple cameras or microphones are deployed to monitor objects moving in
certain areas. We formulate the scheduling problem as a Markov Decision
Process, and show how the age-optimal scheduling policy can be obtained. We
further consider partially observable variants of the problem, and devise
approximate policies for large state spaces. Our evaluations show that the
approximate policies work well in the considered scenarios, and that the fact
that sensors can observe multiple sources is beneficial, especially when there
is high uncertainty of the source states.

    

### [[2105.00013] Cybersecurity in Power Grids: Challenges and Opportunities](http://arxiv.org/abs/2105.00013)


  Increasing volatilities within power transmission and distribution force
power grid operators to amplify their use of communication infrastructure to
monitor and control their grid. The resulting increase in communication creates
a larger attack surface for malicious actors. Indeed, cyber attacks on power
grids have already succeeded in causing temporary, large-scale blackouts in the
recent past. In this paper, we analyze the communication infrastructure of
power grids to derive resulting fundamental challenges of power grids with
respect to cybersecurity. Based on these challenges, we identify a broad set of
resulting attack vectors and attack scenarios that threaten the security of
power grids. To address these challenges, we propose to rely on a
defense-in-depth strategy, which encompasses measures for (i) device and
application security, (ii) network security, (iii) physical security, as well
as (iv) policies, procedures, and awareness. For each of these categories, we
distill and discuss a comprehensive set of state-of-the art approaches, and
identify further opportunities to strengthen cybersecurity in interconnected
power grids.

    

### [[2106.05407] OVRseen: Auditing Network Traffic and Privacy Policies in Oculus VR](http://arxiv.org/abs/2106.05407)


  Virtual reality (VR) is an emerging technology that enables new applications
but also introduces privacy risks. In this paper, we focus on Oculus VR (OVR),
the leading platform in the VR space and we provide the first comprehensive
analysis of personal data exposed by OVR apps and the platform itself, from a
combined networking and privacy policy perspective. We experimented with the
Quest 2 headset and tested the most popular VR apps available on the official
Oculus and the SideQuest app stores. We developed OVRseen, a methodology and
system for collecting, analyzing, and comparing network traffic and privacy
policies on OVR. On the networking side, we captured and decrypted network
traffic of VR apps, which was previously not possible on OVR, and we extracted
data flows, defined as <app, data type, destination>. Compared to the mobile
and other app ecosystems, we found OVR to be more centralized and driven by
tracking and analytics, rather than by third-party advertising. We show that
the data types exposed by VR apps include personally identifiable information
(PII), device information that can be used for fingerprinting, and VR-specific
data types. By comparing the data flows found in the network traffic with
statements made in the apps' privacy policies, we found that approximately 70%
of OVR data flows were not properly disclosed. Furthermore, we extracted
additional context from the privacy policies, and we observed that 69% of the
data flows were used for purposes unrelated to the core functionality of apps.

    

### [[2107.00238] Optimal Power Allocation for Rate Splitting Communications with Deep Reinforcement Learning](http://arxiv.org/abs/2107.00238)


  This letter introduces a novel framework to optimize the power allocation for
users in a Rate Splitting Multiple Access (RSMA) network. In the network,
messages intended for users are split into different parts that are a single
common part and respective private parts. This mechanism enables RSMA to
flexibly manage interference and thus enhance energy and spectral efficiency.
Although possessing outstanding advantages, optimizing power allocation in RSMA
is very challenging under the uncertainty of the communication channel and the
transmitter has limited knowledge of the channel information. To solve the
problem, we first develop a Markov Decision Process framework to model the
dynamic of the communication channel. The deep reinforcement algorithm is then
proposed to find the optimal power allocation policy for the transmitter
without requiring any prior information of the channel. The simulation results
show that the proposed scheme can outperform baseline schemes in terms of
average sum-rate under different power and QoS requirements.

    

### [[2110.01241] A Scalable Factory Backbone for Multiple Independent Time-Sensitive Networks](http://arxiv.org/abs/2110.01241)


  Convergence of time-sensitive machine control networks as part of the
operational technology (OT) with the ubiquitous information technology (IT)
networks is an essential requirement for the ongoing digitalization of
production. In this paper, we review the fundamental differences between both
technologies, the challenges to be solved, existing and upcoming solutions like
TSN and their limitations. Furthermore, we introduce an Ethernet extension for
a backbone network at factory scale and line rates of 10 - 100Gbit/s. The
backbone is intended to carry massive amounts of IT traffic together with the
traffic of multiple independent OT networks at the precision of leading-edge
field bus technologies in the sub-microsecond range. The backbone remains
transparent and does not require changes to the attached OT sub-networks. We
prove our claims by prototype measurements, interoperability tests and field
trials.

    

### [[2110.01605] CCS-GAN: COVID-19 CT-scan classification with very few positive training images](http://arxiv.org/abs/2110.01605)


  We present a novel algorithm that is able to classify COVID-19 pneumonia from
CT Scan slices using a very small sample of training images exhibiting COVID-19
pneumonia in tandem with a larger number of normal images. This algorithm is
able to achieve high classification accuracy using as few as 10 positive
training slices (from 10 positive cases), which to the best of our knowledge is
one order of magnitude fewer than the next closest published work at the time
of writing. Deep learning with extremely small positive training volumes is a
very difficult problem and has been an important topic during the COVID-19
pandemic, because for quite some time it was difficult to obtain large volumes
of COVID-19 positive images for training. Algorithms that can learn to screen
for diseases using few examples are an important area of research. We present
the Cycle Consistent Segmentation Generative Adversarial Network (CCS-GAN).
CCS-GAN combines style transfer with pulmonary segmentation and relevant
transfer learning from negative images in order to create a larger volume of
synthetic positive images for the purposes of improving diagnostic
classification performance. The performance of a VGG-19 classifier plus CCS-GAN
was trained using a small sample of positive image slices ranging from at most
50 down to as few as 10 COVID-19 positive CT-scan images. CCS-GAN achieves high
accuracy with few positive images and thereby greatly reduces the barrier of
acquiring large training volumes in order to train a diagnostic classifier for
COVID-19.

    

### [[2110.01614] Neural Implicit Surfaces for Efficient and Accurate Collisions in Physically Based Simulations](http://arxiv.org/abs/2110.01614)


  Current trends in the computer graphics community propose leveraging the
massive parallel computational power of GPUs to accelerate physically based
simulations. Collision detection and solving is a fundamental part of this
process. It is also the most significant bottleneck on physically based
simulations and it easily becomes intractable as the number of vertices in the
scene increases. Brute force approaches carry a quadratic growth in both
computational time and memory footprint. While their parallelization is trivial
in GPUs, their complexity discourages from using such approaches. Acceleration
structures -- such as BVH -- are often applied to increase performance,
achieving logarithmic computational times for individual point queries.
Nonetheless, their memory footprint also grows rapidly and their
parallelization in a GPU is problematic due to their branching nature. We
propose using implicit surface representations learnt through deep learning for
collision handling in physically based simulations. Our proposed architecture
has a complexity of O(n) -- or O(1) for a single point query -- and has no
parallelization issues. We will show how this permits accurate and efficient
collision handling in physically based simulations, more specifically, for
cloth. In our experiments, we query up to 1M points in 300 milliseconds.

    

### [[2110.01620] Inferring dark matter substructure with astrometric lensing beyond the power spectrum](http://arxiv.org/abs/2110.01620)


  Astrometry -- the precise measurement of positions and motions of celestial
objects -- has emerged as a promising avenue for characterizing the dark matter
population in our Galaxy. By leveraging recent advances in simulation-based
inference and neural network architectures, we introduce a novel method to
search for global dark matter-induced gravitational lensing signatures in
astrometric datasets. Our method based on neural likelihood-ratio estimation
shows significantly enhanced sensitivity to a cold dark matter population and
more favorable scaling with measurement noise compared to existing approaches
based on two-point correlation statistics, establishing machine learning as a
powerful tool for characterizing dark matter using astrometric data.

    

### [[2110.01639] An energy-based model for neuro-symbolic reasoning on knowledge graphs](http://arxiv.org/abs/2110.01639)


  Machine learning on graph-structured data has recently become a major topic
in industry and research, finding many exciting applications such as
recommender systems and automated theorem proving. We propose an energy-based
graph embedding algorithm to characterize industrial automation systems,
integrating knowledge from different domains like industrial automation,
communications and cybersecurity. By combining knowledge from multiple domains,
the learned model is capable of making context-aware predictions regarding
novel system events and can be used to evaluate the severity of anomalies that
might be indicative of, e.g., cybersecurity breaches. The presented model is
mappable to a biologically-inspired neural architecture, serving as a first
bridge between graph embedding methods and neuromorphic computing - uncovering
a promising edge application for this upcoming technology.

    

### [[2110.01640] An Experimental Evaluation on Deepfake Detection using Deep Face Recognition](http://arxiv.org/abs/2110.01640)


  Significant advances in deep learning have obtained hallmark accuracy rates
for various computer vision applications. However, advances in deep generative
models have also led to the generation of very realistic fake content, also
known as deepfakes, causing a threat to privacy, democracy, and national
security. Most of the current deepfake detection methods are deemed as a binary
classification problem in distinguishing authentic images or videos from fake
ones using two-class convolutional neural networks (CNNs). These methods are
based on detecting visual artifacts, temporal or color inconsistencies produced
by deep generative models. However, these methods require a large amount of
real and fake data for model training and their performance drops significantly
in cross dataset evaluation with samples generated using advanced deepfake
generation techniques. In this paper, we thoroughly evaluate the efficacy of
deep face recognition in identifying deepfakes, using different loss functions
and deepfake generation techniques. Experimental investigations on challenging
Celeb-DF and FaceForensics++ deepfake datasets suggest the efficacy of deep
face recognition in identifying deepfakes over two-class CNNs and the ocular
modality. Reported results suggest a maximum Area Under Curve (AUC) of 0.98 and
an Equal Error Rate (EER) of 7.1% in detecting deepfakes using face recognition
on the Celeb-DF dataset. This EER is lower by 16.6% compared to the EER
obtained for the two-class CNN and the ocular modality on the Celeb-DF dataset.
Further on the FaceForensics++ dataset, an AUC of 0.99 and EER of 2.04% were
obtained. The use of biometric facial recognition technology has the advantage
of bypassing the need for a large amount of fake data for model training and
obtaining better generalizability to evolving deepfake creation techniques.

    

### [[2110.01641] Investigating Fairness of Ocular Biometrics Among Young, Middle-Aged, and Older Adults](http://arxiv.org/abs/2110.01641)


  A number of studies suggest bias of the face biometrics, i.e., face
recognition and soft-biometric estimation methods, across gender, race, and age
groups. There is a recent urge to investigate the bias of different biometric
modalities toward the deployment of fair and trustworthy biometric solutions.
Ocular biometrics has obtained increased attention from academia and industry
due to its high accuracy, security, privacy, and ease of use in mobile devices.
A recent study in $2020$ also suggested the fairness of ocular-based user
recognition across males and females. This paper aims to evaluate the fairness
of ocular biometrics in the visible spectrum among age groups; young, middle,
and older adults. Thanks to the availability of the latest large-scale 2020
UFPR ocular biometric dataset, with subjects acquired in the age range 18 - 79
years, to facilitate this study. Experimental results suggest the overall
equivalent performance of ocular biometrics across gender and age groups in
user verification and gender classification. Performance difference for older
adults at lower false match rate and young adults was noted at user
verification and age classification, respectively. This could be attributed to
inherent characteristics of the biometric data from these age groups impacting
specific applications, which suggest a need for advancement in sensor
technology and software solutions.

    

### [[2110.01648] Robust Linear Classification from Limited Training Data](http://arxiv.org/abs/2110.01648)


  We consider the problem of linear classification under general loss functions
in the limited-data setting. Overfitting is a common problem here. The standard
approaches to prevent overfitting are dimensionality reduction and
regularization. But dimensionality reduction loses information, while
regularization requires the user to choose a norm, or a prior, or a distance
metric. We propose an algorithm called RoLin that needs no user choice and
applies to a large class of loss functions. RoLin combines reliable information
from the top principal components with a robust optimization to extract any
useful information from unreliable subspaces. It also includes a new robust
cross-validation that is better than existing cross-validation methods in the
limited-data setting. Experiments on $25$ real-world datasets and three
standard loss functions show that RoLin broadly outperforms both dimensionality
reduction and regularization. Dimensionality reduction has $14\%-40\%$ worse
test loss on average as compared to RoLin. Against $L_1$ and $L_2$
regularization, RoLin can be up to 3x better for logistic loss and 12x better
for squared hinge loss. The differences are greatest for small sample sizes,
where RoLin achieves the best loss on 2x to 3x more datasets than any competing
method. For some datasets, RoLin with $15$ training samples is better than the
best norm-based regularization with $1500$ samples.

    

### [[2110.01653] Learning to Solve the AC Optimal Power Flow via a Lagrangian Approach](http://arxiv.org/abs/2110.01653)


  Using deep neural networks to predict the solutions of AC optimal power flow
(ACOPF) problems has been an active direction of research. However, because the
ACOPF is nonconvex, it is difficult to construct a good data set that contains
mostly globally optimal solutions. To overcome the challenge that the training
data may contain suboptimal solutions, we propose a Lagrangian-based approach.
First, we use a neural network to learn the dual variables of the ACOPF
problem. Then we use a second neural network to predict solutions of the
partial Lagrangian from the predicted dual variables. Since the partial
Lagrangian has a much better optimization landscape, we use the predicted
solutions from the neural network as a warm start for the ACOPF problem. Using
standard and modified IEEE 22-bus, 39-bus, and 118-bus networks, we show that
our approach is able to obtain the globally optimal cost even when the training
data is mostly comprised of suboptimal solutions.

    

### [[2110.01654] Improved architectures and training algorithms for deep operator networks](http://arxiv.org/abs/2110.01654)


  Operator learning techniques have recently emerged as a powerful tool for
learning maps between infinite-dimensional Banach spaces. Trained under
appropriate constraints, they can also be effective in learning the solution
operator of partial differential equations (PDEs) in an entirely
self-supervised manner. In this work we analyze the training dynamics of deep
operator networks (DeepONets) through the lens of Neural Tangent Kernel (NTK)
theory, and reveal a bias that favors the approximation of functions with
larger magnitudes. To correct this bias we propose to adaptively re-weight the
importance of each training example, and demonstrate how this procedure can
effectively balance the magnitude of back-propagated gradients during training
via gradient descent. We also propose a novel network architecture that is more
resilient to vanishing gradient pathologies. Taken together, our developments
provide new insights into the training of DeepONets and consistently improve
their predictive accuracy by a factor of 10-50x, demonstrated in the
challenging setting of learning PDE solution operators in the absence of paired
input-output observations. All code and data accompanying this manuscript are
publicly available at
\url{this https URL.}

    

### [[2110.01659] Cross-Modal Virtual Sensing for Combustion Instability Monitoring](http://arxiv.org/abs/2110.01659)


  In many cyber-physical systems, imaging can be an important but expensive or
'difficult to deploy' sensing modality. One such example is detecting
combustion instability using flame images, where deep learning frameworks have
demonstrated state-of-the-art performance. The proposed frameworks are also
shown to be quite trustworthy such that domain experts can have sufficient
confidence to use these models in real systems to prevent unwanted incidents.
However, flame imaging is not a common sensing modality in engine combustors
today. Therefore, the current roadblock exists on the hardware side regarding
the acquisition and processing of high-volume flame images. On the other hand,
the acoustic pressure time series is a more feasible modality for data
collection in real combustors. To utilize acoustic time series as a sensing
modality, we propose a novel cross-modal encoder-decoder architecture that can
reconstruct cross-modal visual features from acoustic pressure time series in
combustion systems. With the "distillation" of cross-modal features, the
results demonstrate that the detection accuracy can be enhanced using the
virtual visual sensing modality. By providing the benefit of cross-modal
reconstruction, our framework can prove to be useful in different domains well
beyond the power generation and transportation industries.

    

### [[2110.01661] Rerunning OCR -- A Machine Learning Approach to Quality Assessment and Enhancement Prediction](http://arxiv.org/abs/2110.01661)


  Iterating with new and improved OCR solutions enforces decisions to be taken
when it comes to targeting the right reprocessing candidates. This especially
applies when the underlying data collection is of considerable size and rather
diverse in terms of fonts, languages, periods of publication and consequently
OCR quality. This article captures the efforts of the National Library of
Luxembourg to support those exact decisions. They are crucial in order to
guarantee low computational overhead and reduced quality degradation risks,
combined with a more quantifiable OCR improvement. In particular, this work
explains the methodology of the library with respect to text block level
quality assessment. As an extension of this technique, another contribution
comes in the form of a regression model that takes the enhancement potential of
a new OCR engine into account. They both mark promising approaches, especially
for cultural institutions dealing with historic data of lower quality.

    

### [[2110.01663] Global Convergence and Stability of Stochastic Gradient Descent](http://arxiv.org/abs/2110.01663)


  In machine learning, stochastic gradient descent (SGD) is widely deployed to
train models using highly non-convex objectives with equally complex noise
models. Unfortunately, SGD theory often makes restrictive assumptions that fail
to capture the non-convexity of real problems, and almost entirely ignore the
complex noise models that exist in practice. In this work, we make substantial
progress on this shortcoming. First, we establish that SGD's iterates will
either globally converge to a stationary point or diverge under nearly
arbitrary nonconvexity and noise models. Under a slightly more restrictive
assumption on the joint behavior of the non-convexity and noise model that
generalizes current assumptions in the literature, we show that the objective
function cannot diverge, even if the iterates diverge. As a consequence of our
results, SGD can be applied to a greater range of stochastic optimization
problems with confidence about its global convergence behavior and stability.

    

### [[2110.01664] Estimating Potential Outcome Distributions with Collaborating Causal Networks](http://arxiv.org/abs/2110.01664)


  Many causal inference approaches have focused on identifying an individual's
outcome change due to a potential treatment, or the individual treatment effect
(ITE), from observational studies. Rather than only estimating the ITE, we
propose Collaborating Causal Networks (CCN) to estimate the full potential
outcome distributions. This modification facilitates estimating the utility of
each treatment and allows for individual variation in utility functions (e.g.,
variability in risk tolerance). We show that CCN learns distributions that
asymptotically capture the correct potential outcome distributions under
standard causal inference assumptions. Furthermore, we develop a new adjustment
approach that is empirically effective in alleviating sample imbalance between
treatment groups in observational studies. We evaluate CCN by extensive
empirical experiments and demonstrate improved distribution estimates compared
to existing Bayesian and Generative Adversarial Network-based methods.
Additionally, CCN empirically improves decisions over a variety of utility
functions.

    

### [[2110.01668] Learning to shortcut and shortlist order fulfillment deciding](http://arxiv.org/abs/2110.01668)


  With the increase of order fulfillment options and business objectives taken
into consideration in the deciding process, order fulfillment deciding is
becoming more and more complex. For example, with the advent of ship from store
retailers now have many more fulfillment nodes to consider, and it is now
common to take into account many and varied business goals in making
fulfillment decisions. With increasing complexity, efficiency of the deciding
process can become a real concern. Finding the optimal fulfillment assignments
among all possible ones may be too costly to do for every order especially
during peak times. In this work, we explore the possibility of exploiting
regularity in the fulfillment decision process to reduce the burden on the
deciding system. By using data mining we aim to find patterns in past
fulfillment decisions that can be used to efficiently predict most likely
assignments for future decisions. Essentially, those assignments that can be
predicted with high confidence can be used to shortcut, or bypass, the
expensive deciding process, or else a set of most likely assignments can be
used for shortlisting -- sending a much smaller set of candidates for
consideration by the fulfillment deciding system.

    

### [[2110.01670] A manifold learning approach for gesture identification from micro-Doppler radar measurements](http://arxiv.org/abs/2110.01670)


  A recent paper (Neural Networks, {\bf 132} (2020), 253-268) introduces a
straightforward and simple kernel based approximation for manifold learning
that does not require the knowledge of anything about the manifold, except for
its dimension. In this paper, we examine the pointwise error in approximation
using least squares optimization based on this kernel, in particular, how the
error depends upon the data characteristics and deteriorates as one goes away
from the training data. The theory is presented with an abstract localized
kernel, which can utilize any prior knowledge about the data being located on
an unknown sub-manifold of a known manifold.
We demonstrate the performance of our approach using a publicly available
micro-Doppler data set investigating the use of different pre-processing
measures, kernels, and manifold dimension. Specifically, it is shown that the
Gaussian kernel introduced in the above mentioned paper leads to a
near-competitive performance to deep neural networks, and offers significant
improvements in speed and memory requirements. Similarly, a kernel based on
treating the feature space as a submanifold of the Grassman manifold
outperforms conventional hand-crafted features. To demonstrate the fact that
our methods are agnostic to the domain knowledge, we examine the classification
problem in a simple video data set.

    

### [[2110.01677] Inductive learning for product assortment graph completion](http://arxiv.org/abs/2110.01677)


  Global retailers have assortments that contain hundreds of thousands of
products that can be linked by several types of relationships like style
compatibility, "bought together", "watched together", etc. Graphs are a natural
representation for assortments, where products are nodes and relations are
edges. Relations like style compatibility are often produced by a manual
process and therefore do not cover uniformly the whole graph. We propose to use
inductive learning to enhance a graph encoding style compatibility of a fashion
assortment, leveraging rich node information comprising textual descriptions
and visual data. Then, we show how the proposed graph enhancement improves
substantially the performance on transductive tasks with a minor impact on
graph sparsity.

    

### [[2110.01698] HYPPO: A Surrogate-Based Multi-Level Parallelism Tool for Hyperparameter Optimization](http://arxiv.org/abs/2110.01698)


  We present a new software, HYPPO, that enables the automatic tuning of
hyperparameters of various deep learning (DL) models. Unlike other
hyperparameter optimization (HPO) methods, HYPPO uses adaptive surrogate models
and directly accounts for uncertainty in model predictions to find accurate and
reliable models that make robust predictions. Using asynchronous nested
parallelism, we are able to significantly alleviate the computational burden of
training complex architectures and quantifying the uncertainty. HYPPO is
implemented in Python and can be used with both TensorFlow and PyTorch
libraries. We demonstrate various software features on time-series prediction
and image classification problems as well as a scientific application in
computed tomography image reconstruction. Finally, we show that (1) we can
reduce by an order of magnitude the number of evaluations necessary to find the
most optimal region in the hyperparameter space and (2) we can reduce by two
orders of magnitude the throughput for such HPO process to complete.

    

### [[2110.01717] Molecule3D: A Benchmark for Predicting 3D Geometries from Molecular Graphs](http://arxiv.org/abs/2110.01717)


  Graph neural networks are emerging as promising methods for modeling
molecular graphs, in which nodes and edges correspond to atoms and chemical
bonds, respectively. Recent studies show that when 3D molecular geometries,
such as bond lengths and angles, are available, molecular property prediction
tasks can be made more accurate. However, computing of 3D molecular geometries
requires quantum calculations that are computationally prohibitive. For
example, accurate calculation of 3D geometries of a small molecule requires
hours of computing time using density functional theory (DFT). Here, we propose
to predict the ground-state 3D geometries from molecular graphs using machine
learning methods. To make this feasible, we develop a benchmark, known as
Molecule3D, that includes a dataset with precise ground-state geometries of
approximately 4 million molecules derived from DFT. We also provide a set of
software tools for data processing, splitting, training, and evaluation, etc.
Specifically, we propose to assess the error and validity of predicted
geometries using four metrics. We implement two baseline methods that either
predict the pairwise distance between atoms or atom coordinates in 3D space.
Experimental results show that, compared with generating 3D geometries with
RDKit, our method can achieve comparable prediction accuracy but with much
smaller computational costs. Our Molecule3D is available as a module of the
MoleculeX software library (this https URL).

    

### [[2110.01718] Randomized Projection Learning Method forDynamic Mode Decomposition](http://arxiv.org/abs/2110.01718)


  A data-driven analysis method known as dynamic mode decomposition (DMD)
approximates the linear Koopman operator on projected space. In the spirit of
Johnson-Lindenstrauss Lemma, we will use random projection to estimate the DMD
modes in reduced dimensional space. In practical applications, snapshots are in
high dimensional observable space and the DMD operator matrix is massive.
Hence, computing DMD with the full spectrum is infeasible, so our main
computational goal is estimating the eigenvalue and eigenvectors of the DMD
operator in a projected domain. We will generalize the current algorithm to
estimate a projected DMD operator. We focus on a powerful and simple random
projection algorithm that will reduce the computational and storage cost. While
clearly, a random projection simplifies the algorithmic complexity of a
detailed optimal projection, as we will show, generally the results can be
excellent nonetheless, and quality understood through a well-developed theory
of random projections. We will demonstrate that modes can be calculated for a
low cost by the projected data with sufficient dimension.
Keyword: Koopman Operator, Dynamic Mode Decomposition(DMD),
Johnson-Lindenstrauss Lemma, Random Projection, Data-driven method.

    

### [[2110.01722] Wireless Link Scheduling via Graph Representation Learning: A Comparative Study of Different Supervision Levels](http://arxiv.org/abs/2110.01722)


  We consider the problem of binary power control, or link scheduling, in
wireless interference networks, where the power control policy is trained using
graph representation learning. We leverage the interference graph of the
wireless network as an underlying topology for a graph neural network (GNN)
backbone, which converts the channel matrix to a set of node embeddings for all
transmitter-receiver pairs. We show how the node embeddings can be trained in
several ways, including via supervised, unsupervised, and self-supervised
learning, and we compare the impact of different supervision levels on the
performance of these methods in terms of the system-level throughput,
convergence behavior, sample efficiency, and generalization capability.

    

### [[2110.01729] Stochastic functional analysis with applications to robust machine learning](http://arxiv.org/abs/2110.01729)


  It is well-known that machine learning protocols typically under-utilize
information on the probability distributions of feature vectors and related
data, and instead directly compute regression or classification functions of
feature vectors. In this paper we introduce a set of novel features for
identifying underlying stochastic behavior of input data using the
Karhunen-Lo√©ve (KL) expansion, where classification is treated as detection
of anomalies from a (nominal) signal class. These features are constructed from
the recent Functional Data Analysis (FDA) theory for anomaly detection. The
related signal decomposition is an exact hierarchical tensor product expansion
with known optimality properties for approximating stochastic processes (random
fields) with finite dimensional function spaces. In principle these primary low
dimensional spaces can capture most of the stochastic behavior of `underlying
signals' in a given nominal class, and can reject signals in alternative
classes as stochastic anomalies. Using a hierarchical finite dimensional KL
expansion of the nominal class, a series of orthogonal nested subspaces is
constructed for detecting anomalous signal components. Projection coefficients
of input data in these subspaces are then used to train an ML classifier.
However, due to the split of the signal into nominal and anomalous projection
components, clearer separation surfaces of the classes arise. In fact we show
that with a sufficiently accurate estimation of the covariance structure of the
nominal class, a sharp classification can be obtained. We carefully formulate
this concept and demonstrate it on a number of high-dimensional datasets in
cancer diagnostics. This method leads to a significant increase in precision
and accuracy over the current top benchmarks for the Global Cancer Map (GCM)
gene expression network dataset.

    

### [[2110.01730] Pre-Quantized Deep Learning Models Codified in ONNX to Enable Hardware/Software Co-Design](http://arxiv.org/abs/2110.01730)


  This paper presents a methodology to separate the quantization process from
the hardware-specific model compilation stage via a pre-quantized deep learning
model description in standard ONNX format. Separating the quantization process
from the model compilation stage enables independent development. The
methodology is expressive to convey hardware-specific operations and to embed
key quantization parameters into a ONNX model which enables hardware/software
co-design. Detailed examples are given for both MLP and CNN based networks,
which can be extended to other networks in a straightforward fashion.

    

### [[2110.01736] AdjointBackMapV2: Precise Reconstruction of Arbitrary CNN Unit's Activation via Adjoint Operators](http://arxiv.org/abs/2110.01736)


  Adjoint operators have been found to be effective in the exploration of CNN's
inner workings [1]. However, the previous no-bias assumption restricted its
generalization. We overcome the restriction via embedding input images into an
extended normed space that includes bias in all CNN layers as part of the
extended input space and propose an adjoint-operator-based algorithm that maps
high-level weights back to the extended input space for reconstructing an
effective hypersurface. Such hypersurface can be computed for an arbitrary unit
in the CNN, and we prove that this reconstructed hypersurface, when multiplied
by the original input (through an inner product), will precisely replicate the
output value of each unit. We show experimental results based on the CIFAR-10
dataset that the proposed approach achieves near $0$ reconstruction error.

    

### [[2110.01742] Seizure Classification Using Parallel Genetic Naive Bayes Classifiers](http://arxiv.org/abs/2110.01742)


  Epilepsy affects 50 million people worldwide and is one of the most common
serious brain disorders. Seizure detection and classification is a valuable
tool for maintaining the condition. An automated detection algorithm will allow
for accurate diagnosis. This study proposes a method using unique features with
a novel parallel classifier trained using a genetic algorithm. Ictal states
from the EEG are segmented into 1.8 s windows, where the epochs are then
further decomposed into 13 different features from the first IMF. All of the
features are fed into a genetic algorithm (Binary Grey Wolf Optimisation Option
1) with a Naive Bayes classifier. Combining the simple-partial and
complex-partial seizures provides the highest accuracy of all the models
tested.

    

### [[2110.01746] Effects of Multi-Aspect Online Reviews with Unobserved Confounders: Estimation and Implication](http://arxiv.org/abs/2110.01746)


  Online review systems are the primary means through which many businesses
seek to build the brand and spread their messages. Prior research studying the
effects of online reviews has been mainly focused on a single numerical cause,
e.g., ratings or sentiment scores. We argue that such notions of causes entail
three key limitations: they solely consider the effects of single numerical
causes and ignore different effects of multiple aspects -- e.g., Food, Service
-- embedded in the textual reviews; they assume the absence of hidden
confounders in observational studies, e.g., consumers' personal preferences;
and they overlook the indirect effects of numerical causes that can potentially
cancel out the effect of textual reviews on business revenue. We thereby
propose an alternative perspective to this single-cause-based effect estimation
of online reviews: in the presence of hidden confounders, we consider
multi-aspect textual reviews, particularly, their total effects on business
revenue and direct effects with the numerical cause -- ratings -- being the
mediator. We draw on recent advances in machine learning and causal inference
to together estimate the hidden confounders and causal effects. We present
empirical evaluations using real-world examples to discuss the importance and
implications of differentiating the multi-aspect effects in strategizing
business operations.

    

### [[2110.01752] RASA: Efficient Register-Aware Systolic Array Matrix Engine for CPU](http://arxiv.org/abs/2110.01752)


  As AI-based applications become pervasive, CPU vendors are starting to
incorporate matrix engines within the datapath to boost efficiency. Systolic
arrays have been the premier architectural choice as matrix engines in offload
accelerators. However, we demonstrate that incorporating them inside CPUs can
introduce under-utilization and stalls due to limited register storage to
amortize the fill and drain times of the array. To address this, we propose
RASA, Register-Aware Systolic Array. We develop techniques to divide an
execution stage into several sub-stages and overlap instructions to hide
overheads and run them concurrently. RASA-based designs improve performance
significantly with negligible area and power overhead.

    

### [[2110.01756] Bottom-up Hierarchical Classification Using Confusion-based Logit Compression](http://arxiv.org/abs/2110.01756)


  In this work, we propose a method to efficiently compute label posteriors of
a base flat classifier in the presence of few validation examples within a
bottom-up hierarchical inference framework. A stand-alone validation set (not
used to train the base classifier) is preferred for posterior estimation to
avoid overfitting the base classifier, however a small validation set limits
the number of features one can effectively use. We propose a simple, yet
robust, logit vector compression approach based on generalized logits and label
confusions for the task of label posterior estimation within the context of
hierarchical classification. Extensive comparative experiments with other
compression techniques are provided across multiple sized validation sets, and
a comparison with related hierarchical classification approaches is also
conducted. The proposed approach mitigates the problem of not having enough
validation examples for reliable posterior estimation while maintaining strong
hierarchical classification performance.

    

### [[2110.01759] Controlled-Variable Selection based on Chaos Theory for the Tennessee Eastman Plant](http://arxiv.org/abs/2110.01759)


  This work explores a link between chaotic signals and the selection of
controlled variables for plantwide control system design. Some results are
shown for the Tennessee Eastman plant, which is well-known for being a
challenging process in the field of plant-wide control. This article provides a
systematic, data-driven method to select which variables should be controlled.
However, since plantwide control problems are inherently complex, this work
does not intend to provide a definite solution, but a complementary analysis to
take into account towards the final control system design. The discussion
highlights the potential hidden in the chaos theory to reduce the complexity of
the resulting control system.

    

### [[2110.01761] Proxy-bridged Image Reconstruction Network for Anomaly Detection in Medical Images](http://arxiv.org/abs/2110.01761)


  Anomaly detection in medical images refers to the identification of abnormal
images with only normal images in the training set. Most existing methods solve
this problem with a self-reconstruction framework, which tends to learn an
identity mapping and reduces the sensitivity to anomalies. To mitigate this
problem, in this paper, we propose a novel Proxy-bridged Image Reconstruction
Network (ProxyAno) for anomaly detection in medical images. Specifically, we
use an intermediate proxy to bridge the input image and the reconstructed
image. We study different proxy types, and we find that the superpixel-image
(SI) is the best one. We set all pixels' intensities within each superpixel as
their average intensity, and denote this image as SI. The proposed ProxyAno
consists of two modules, a Proxy Extraction Module and an Image Reconstruction
Module. In the Proxy Extraction Module, a memory is introduced to memorize the
feature correspondence for normal image to its corresponding SI, while the
memorized correspondence does not apply to the abnormal images, which leads to
the information loss for abnormal image and facilitates the anomaly detection.
In the Image Reconstruction Module, we map an SI to its reconstructed image.
Further, we crop a patch from the image and paste it on the normal SI to mimic
the anomalies, and enforce the network to reconstruct the normal image even
with the pseudo abnormal SI. In this way, our network enlarges the
reconstruction error for anomalies. Extensive experiments on brain MR images,
retinal OCT images and retinal fundus images verify the effectiveness of our
method for both image-level and pixel-level anomaly detection.

    

### [[2110.01765] Rapid training of deep neural networks without skip connections or normalization layers using Deep Kernel Shaping](http://arxiv.org/abs/2110.01765)


  Using an extended and formalized version of the Q/C map analysis of Poole et
al. (2016), along with Neural Tangent Kernel theory, we identify the main
pathologies present in deep networks that prevent them from training fast and
generalizing to unseen data, and show how these can be avoided by carefully
controlling the "shape" of the network's initialization-time kernel function.
We then develop a method called Deep Kernel Shaping (DKS), which accomplishes
this using a combination of precise parameter initialization, activation
function transformations, and small architectural tweaks, all of which preserve
the model class. In our experiments we show that DKS enables SGD training of
residual networks without normalization layers on Imagenet and CIFAR-10
classification tasks at speeds comparable to standard ResNetV2 and Wide-ResNet
models, with only a small decrease in generalization performance. And when
using K-FAC as the optimizer, we achieve similar results for networks without
skip connections. Our results apply for a large variety of activation
functions, including those which traditionally perform very badly, such as the
logistic sigmoid. In addition to DKS, we contribute a detailed analysis of skip
connections, normalization layers, special activation functions like RELU and
SELU, and various initialization schemes, explaining their effectiveness as
alternative (and ultimately incomplete) ways of "shaping" the network's
initialization-time kernel.

    

### [[2110.01773] Differentiable Equilibrium Computation with Decision Diagrams for Stackelberg Models of Combinatorial Congestion Games](http://arxiv.org/abs/2110.01773)


  We address Stackelberg models of combinatorial congestion games (CCGs); we
aim to optimize the parameters of CCGs so that the selfish behavior of
non-atomic players attains desirable equilibria. This model is essential for
designing such social infrastructures as traffic and communication networks.
Nevertheless, computational approaches to the model have not been thoroughly
studied due to two difficulties: (I) bilevel-programming structures and (II)
the combinatorial nature of CCGs. We tackle them by carefully combining (I) the
idea of \textit{differentiable} optimization and (II) data structures called
\textit{zero-suppressed binary decision diagrams} (ZDDs), which can compactly
represent sets of combinatorial strategies. Our algorithm numerically
approximates the equilibria of CCGs, which we can differentiate with respect to
parameters of CCGs by automatic differentiation. With the resulting
derivatives, we can apply gradient-based methods to Stackelberg models of CCGs.
Our method is tailored to induce Nesterov's acceleration and can fully utilize
the empirical compactness of ZDDs. These technical advantages enable us to deal
with CCGs with a vast number of combinatorial strategies. Experiments on
real-world network design instances demonstrate the practicality of our method.

    

### [[2110.01794] Multi-axis Attentive Prediction for Sparse EventData: An Application to Crime Prediction](http://arxiv.org/abs/2110.01794)


  Spatiotemporal prediction of event data is a challenging task with a long
history of research. While recent work in spatiotemporal prediction has
leveraged deep sequential models that substantially improve over classical
approaches, these models are prone to overfitting when the observation is
extremely sparse, as in the task of crime event prediction. To overcome these
sparsity issues, we present Multi-axis Attentive Prediction for Sparse Event
Data (MAPSED). We propose a purely attentional approach to extract both
short-term dynamics and long-term semantics of event propagation through two
observation angles. Unlike existing temporal prediction models that propagate
latent information primarily along the temporal dimension, the MAPSED
simultaneously operates over all axes (time, 2D space, event type) of the
embedded data tensor. We additionally introduce a novel Frobenius norm-based
contrastive learning objective to improve latent representational
generalization.Empirically, we validate MAPSED on two publicly accessible urban
crime datasets for spatiotemporal sparse event prediction, where MAPSED
outperforms both classical and state-of-the-art deep learning models. The
proposed contrastive learning objective significantly enhances the MAPSED's
ability to capture the semantics and dynamics of the events, resulting in
better generalization ability to combat sparse observations.

    

### [[2110.01795] Deep Subspace analysing for Semi-Supervised multi-label classification of Diabetic Foot Ulcer](http://arxiv.org/abs/2110.01795)


  Diabetes is a global raising pandemic. Diabetes patients are at risk of
developing foot ulcer that usually leads to limb amputation. In order to
develop a self monitoring mobile application, in this work, we propose a novel
deep subspace analysis pipeline for semi-supervised diabetic foot ulcer
mulit-label classification. To avoid any chance of over-fitting, unlike recent
state of the art deep semi-supervised methods, the proposed pipeline dose not
include any data augmentation. Whereas, after extracting deep features, in
order to make the representation shift invariant, we employ variety of data
augmentation methods on each image and generate an image-sets, which is then
mapped into a linear subspace. Moreover, the proposed pipeline reduces the cost
of retraining when more new unlabelled data become available. Thus, the first
stage of the pipeline employs the concept of transfer learning for feature
extraction purpose through modifying and retraining a deep convolutional
network architect known as Xception. Then, the output of a mid-layer is
extracted to generate an image set representer of any given image with help of
data augmentation methods. At this stage, each image is transferred to a linear
subspace which is a point on a Grassmann Manifold topological space. Hence, to
perform analyse them, the geometry of such manifold must be considered. As
such, each labelled image is represented as a vector of distances to number of
unlabelled images using geodesic distance on Grassmann manifold. Finally,
Random Forest is trained for multi-label classification of diabetic foot ulcer
images. The method is then evaluated on the blind test set provided by DFU2021
competition, and the result considerable improvement compared to using
classical transfer learning with data augmentation.

    

### [[2110.01799] ContractNLI: A Dataset for Document-level Natural Language Inference for Contracts](http://arxiv.org/abs/2110.01799)


  Reviewing contracts is a time-consuming procedure that incurs large expenses
to companies and social inequality to those who cannot afford it. In this work,
we propose "document-level natural language inference (NLI) for contracts", a
novel, real-world application of NLI that addresses such problems. In this
task, a system is given a set of hypotheses (such as "Some obligations of
Agreement may survive termination.") and a contract, and it is asked to
classify whether each hypothesis is "entailed by", "contradicting to" or "not
mentioned by" (neutral to) the contract as well as identifying "evidence" for
the decision as spans in the contract. We annotated and release the largest
corpus to date consisting of 607 annotated contracts. We then show that
existing models fail badly on our task and introduce a strong baseline, which
(1) models evidence identification as multi-label classification over spans
instead of trying to predict start and end tokens, and (2) employs more
sophisticated context segmentation for dealing with long documents. We also
show that linguistic characteristics of contracts, such as negations by
exceptions, are contributing to the difficulty of this task and that there is
much room for improvement.

    

### [[2110.01804] A Survey On Neural Word Embeddings](http://arxiv.org/abs/2110.01804)


  Understanding human language has been a sub-challenge on the way of
intelligent machines. The study of meaning in natural language processing (NLP)
relies on the distributional hypothesis where language elements get meaning
from the words that co-occur within contexts. The revolutionary idea of
distributed representation for a concept is close to the working of a human
mind in that the meaning of a word is spread across several neurons, and a loss
of activation will only slightly affect the memory retrieval process.
Neural word embeddings transformed the whole field of NLP by introducing
substantial improvements in all NLP tasks. In this survey, we provide a
comprehensive literature review on neural word embeddings. We give theoretical
foundations and describe existing work by an interplay between word embeddings
and language modelling. We provide broad coverage on neural word embeddings,
including early word embeddings, embeddings targeting specific semantic
relations, sense embeddings, morpheme embeddings, and finally, contextual
representations. Finally, we describe benchmark datasets in word embeddings'
performance evaluation and downstream tasks along with the performance results
of/due to word embeddings.

    

### [[2110.01810] Deep Synoptic Monte Carlo Planning in Reconnaissance Blind Chess](http://arxiv.org/abs/2110.01810)


  This paper introduces deep synoptic Monte Carlo planning (DSMCP) for large
imperfect information games. The algorithm constructs a belief state with an
unweighted particle filter and plans via playouts that start at samples drawn
from the belief state. The algorithm accounts for uncertainty by performing
inference on "synopses," a novel stochastic abstraction of information states.
DSMCP is the basis of the program Penumbra, which won the official 2020
reconnaissance blind chess competition versus 33 other programs. This paper
also evaluates algorithm variants that incorporate caution, paranoia, and a
novel bandit algorithm. Furthermore, it audits the synopsis features used in
Penumbra with per-bit saliency statistics.

    

### [[2110.01811] On the Complementarity between Pre-Training and Back-Translation for Neural Machine Translation](http://arxiv.org/abs/2110.01811)


  Pre-training (PT) and back-translation (BT) are two simple and powerful
methods to utilize monolingual data for improving the model performance of
neural machine translation (NMT). This paper takes the first step to
investigate the complementarity between PT and BT. We introduce two probing
tasks for PT and BT respectively and find that PT mainly contributes to the
encoder module while BT brings more benefits to the decoder. Experimental
results show that PT and BT are nicely complementary to each other,
establishing state-of-the-art performances on the WMT16 English-Romanian and
English-Russian benchmarks. Through extensive analyses on sentence originality
and word frequency, we also demonstrate that combining Tagged BT with PT is
more helpful to their complementarity, leading to better translation quality.
Source code is freely available at this https URL.

    

### [[2110.01813] An Efficient Anomaly Detection Approach using Cube Sampling with Streaming Data](http://arxiv.org/abs/2110.01813)


  Anomaly detection is critical in various fields, including intrusion
detection, health monitoring, fault diagnosis, and sensor network event
detection. The isolation forest (or iForest) approach is a well-known technique
for detecting anomalies. It is, however, ineffective when dealing with dynamic
streaming data, which is becoming increasingly prevalent in a wide variety of
application areas these days. In this work, we extend our previous work by
proposed an efficient iForest based approach for anomaly detection using cube
sampling that is effective on streaming data. Cube sampling is used in the
initial stage to choose nearly balanced samples, significantly reducing storage
requirements while preserving efficiency. Following that, the streaming nature
of data is addressed by a sliding window technique that generates consecutive
chunks of data for systematic processing. The novelty of this paper is in
applying Cube sampling in iForest and calculating inclusion probability. The
proposed approach is equally successful at detecting anomalies as existing
state-of-the-art approaches, requiring significantly less storage and time
complexity. We undertake empirical evaluations of the proposed approach using
standard datasets and demonstrate that it outperforms traditional approaches in
terms of Area Under the ROC Curve (AUC-ROC) and can handle high-dimensional
streaming data.

    

### [[2110.01818] Neural Network Adversarial Attack Method Based on Improved Genetic Algorithm](http://arxiv.org/abs/2110.01818)


  Deep learning algorithms are widely used in fields such as computer vision
and natural language processing, but they are vulnerable to security threats
from adversarial attacks because of their internal presence of a large number
of nonlinear functions and parameters leading to their uninterpretability. In
this paper, we propose a neural network adversarial attack method based on an
improved genetic algorithm. The improved genetic algorithm improves the
variation and crossover links based on the original genetic optimization
algorithm, which greatly improves the iteration efficiency and shortens the
running time. The method does not need the internal structure and parameter
information of the neural network model, and it can obtain the adversarial
samples with high confidence in a short time by the classification and
confidence information of the neural network. The experimental results show
that the method in this paper has a wide range of applicability and high
efficiency for the model, and provides a new idea for the adversarial attack.

    

### [[2110.01825] Attention Augmented Convolutional Transformer for Tabular Time-series](http://arxiv.org/abs/2110.01825)


  Time-series classification is one of the most frequently performed tasks in
industrial data science, and one of the most widely used data representation in
the industrial setting is tabular representation. In this work, we propose a
novel scalable architecture for learning representations from tabular
time-series data and subsequently performing downstream tasks such as
time-series classification. The representation learning framework is
end-to-end, akin to bidirectional encoder representations from transformers
(BERT) in language modeling, however, we introduce novel masking technique
suitable for pretraining of time-series data. Additionally, we also use
one-dimensional convolutions augmented with transformers and explore their
effectiveness, since the time-series datasets lend themselves naturally for
one-dimensional convolutions. We also propose a novel timestamp embedding
technique, which helps in handling both periodic cycles at different time
granularity levels, and aperiodic trends present in the time-series data. Our
proposed model is end-to-end and can handle both categorical and continuous
valued inputs, and does not require any quantization or encoding of continuous
features.

    

### [[2110.01827] When is the Convergence Time of Langevin Algorithms Dimension Independent? A Composite Optimization Viewpoint](http://arxiv.org/abs/2110.01827)


  There has been a surge of works bridging MCMC sampling and optimization, with
a specific focus on translating non-asymptotic convergence guarantees for
optimization problems into the analysis of Langevin algorithms in MCMC
sampling. A conspicuous distinction between the convergence analysis of
Langevin sampling and that of optimization is that all known convergence rates
for Langevin algorithms depend on the dimensionality of the problem, whereas
the convergence rates for optimization are dimension-free for convex problems.
Whether a dimension independent convergence rate can be achieved by Langevin
algorithm is thus a long-standing open problem. This paper provides an
affirmative answer to this problem for large classes of either Lipschitz or
smooth convex problems with normal priors. By viewing Langevin algorithm as
composite optimization, we develop a new analysis technique that leads to
dimension independent convergence rates for such problems.

    

### [[2110.01833] Attaining Interpretability in Reinforcement Learning via Hierarchical Primitive Composition](http://arxiv.org/abs/2110.01833)


  Deep reinforcement learning has shown its effectiveness in various
applications and provides a promising direction for solving tasks with high
complexity. In most reinforcement learning algorithms, however, two major
issues need to be dealt with - the sample inefficiency and the interpretability
of a policy. The former happens when the environment is sparsely rewarded
and/or has a long-term credit assignment problem, while the latter becomes a
problem when the learned policies are deployed at the customer side product. In
this paper, we propose a novel hierarchical reinforcement learning algorithm
that mitigates the aforementioned issues by decomposing the original task in a
hierarchy and by compounding pretrained primitives with intents. We show how
the proposed scheme can be employed in practice by solving a pick and place
task with a 6 DoF manipulator.

    

### [[2110.01839] Truth-Conditional Captioning of Time Series Data](http://arxiv.org/abs/2110.01839)


  In this paper, we explore the task of automatically generating natural
language descriptions of salient patterns in a time series, such as stock
prices of a company over a week. A model for this task should be able to
extract high-level patterns such as presence of a peak or a dip. While typical
contemporary neural models with attention mechanisms can generate fluent output
descriptions for this task, they often generate factually incorrect
descriptions. We propose a computational model with a truth-conditional
architecture which first runs small learned programs on the input time series,
then identifies the programs/patterns which hold true for the given input, and
finally conditions on only the chosen valid program (rather than the input time
series) to generate the output text description. A program in our model is
constructed from modules, which are small neural networks that are designed to
capture numerical patterns and temporal information. The modules are shared
across multiple programs, enabling compositionality as well as efficient
learning of module parameters. The modules, as well as the composition of the
modules, are unobserved in data, and we learn them in an end-to-end fashion
with the only training signal coming from the accompanying natural language
text descriptions. We find that the proposed model is able to generate
high-precision captions even though we consider a small and simple space of
module types.

    

### [[2110.01842] Dataset: Large-scale Urban IoT Activity Data for DDoS Attack Emulation](http://arxiv.org/abs/2110.01842)


  As IoT deployments grow in scale for applications such as smart cities, they
face increasing cyber-security threats. In particular, as evidenced by the
famous Mirai incident and other ongoing threats, large-scale IoT device
networks are particularly susceptible to being hijacked and used as botnets to
launch distributed denial of service (DDoS) attacks. Real large-scale datasets
are needed to train and evaluate the use of machine learning algorithms such as
deep neural networks to detect and defend against such DDoS attacks. We present
a dataset from an urban IoT deployment of 4060 nodes describing their
spatio-temporal activity under benign conditions. We also provide a synthetic
DDoS attack generator that injects attack activity into the dataset based on
tunable parameters such as number of nodes attacked and duration of attack. We
discuss some of the features of the dataset. We also demonstrate the utility of
the dataset as well as our synthetic DDoS attack generator by using them for
the training and evaluation of a simple multi-label feed-forward neural network
that aims to identify which nodes are under attack and when.

    

### [[2110.01843] Short-term precipitation prediction using deep learning](http://arxiv.org/abs/2110.01843)


  Accurate weather prediction is essential for many aspects of life, notably
the early warning of extreme weather events such as rainstorms. Short-term
predictions of these events rely on forecasts from numerical weather models, in
which, despite much improvement in the past decades, outstanding issues remain
concerning model uncertainties, and increasing demands for computation and
storage resources. In recent years, the advance of deep learning offers a
viable alternative approach. Here, we show that a 3D convolutional neural
network using a single frame of meteorology fields as input is capable of
predicting the precipitation spatial distribution. The network is developed
based on 39-years (1980-2018) data of meteorology and daily precipitation over
the contiguous United States. The results bring fundamental advancements in
weather prediction. First, the trained network alone outperforms the
state-of-the-art weather models in predicting daily total precipitation, and
the superiority of the network extends to forecast leads up to 5 days. Second,
combining the network predictions with the weather-model forecasts
significantly improves the accuracy of model forecasts, especially for
heavy-precipitation events. Third, the millisecond-scale inference time of the
network facilitates large ensemble predictions for further accuracy
improvement. These findings strongly support the use of deep-learning in
short-term weather predictions.

    

### [[2110.01848] Cellular Network Radio Propagation Modeling with Deep Convolutional Neural Networks](http://arxiv.org/abs/2110.01848)


  Radio propagation modeling and prediction is fundamental for modern cellular
network planning and optimization. Conventional radio propagation models fall
into two categories. Empirical models, based on coarse statistics, are simple
and computationally efficient, but are inaccurate due to oversimplification.
Deterministic models, such as ray tracing based on physical laws of wave
propagation, are more accurate and site specific. But they have higher
computational complexity and are inflexible to utilize site information other
than traditional global information system (GIS) maps.
In this article we present a novel method to model radio propagation using
deep convolutional neural networks and report significantly improved
performance compared to conventional models. We also lay down the framework for
data-driven modeling of radio propagation and enable future research to utilize
rich and unconventional information of the site, e.g. satellite photos, to
provide more accurate and flexible models.

    

### [[2110.01852] Data Augmentation Approaches in Natural Language Processing: A Survey](http://arxiv.org/abs/2110.01852)


  As an effective strategy, data augmentation (DA) alleviates data scarcity
scenarios where deep learning techniques may fail. It is widely applied in
computer vision then introduced to natural language processing and achieves
improvements in many tasks. One of the main focuses of the DA methods is to
improve the diversity of training data, thereby helping the model to better
generalize to unseen testing data. In this survey, we frame DA methods into
three categories based on the diversity of augmented data, including
paraphrasing, noising, and sampling. Our paper sets out to analyze DA methods
in detail according to the above categories. Further, we also introduce their
applications in NLP tasks as well as the challenges.

    

### [[2110.01856] Hypernetworks for Continual Semi-Supervised Learning](http://arxiv.org/abs/2110.01856)


  Learning from data sequentially arriving, possibly in a non i.i.d. way, with
changing task distribution over time is called continual learning. Much of the
work thus far in continual learning focuses on supervised learning and some
recent works on unsupervised learning. In many domains, each task contains a
mix of labelled (typically very few) and unlabelled (typically plenty) training
examples, which necessitates a semi-supervised learning approach. To address
this in a continual learning setting, we propose a framework for
semi-supervised continual learning called Meta-Consolidation for Continual
Semi-Supervised Learning (MCSSL). Our framework has a hypernetwork that learns
the meta-distribution that generates the weights of a semi-supervised auxiliary
classifier generative adversarial network $(\textit{Semi-ACGAN})$ as the base
network. We consolidate the knowledge of sequential tasks in the hypernetwork,
and the base network learns the semi-supervised learning task. Further, we
present $\textit{Semi-Split CIFAR-10}$, a new benchmark for continual
semi-supervised learning, obtained by modifying the $\textit{Split CIFAR-10}$
dataset, in which the tasks with labelled and unlabelled data arrive
sequentially. Our proposed model yields significant improvements in the
continual semi-supervised learning setting. We compare the performance of
several existing continual learning approaches on the proposed continual
semi-supervised learning benchmark of the Semi-Split CIFAR-10 dataset.

    

### [[2110.01858] KKT Conditions, First-Order and Second-Order Optimization, and Distributed Optimization: Tutorial and Survey](http://arxiv.org/abs/2110.01858)


  This is a tutorial and survey paper on Karush-Kuhn-Tucker (KKT) conditions,
first-order and second-order numerical optimization, and distributed
optimization. After a brief review of history of optimization, we start with
some preliminaries on properties of sets, norms, functions, and concepts of
optimization. Then, we introduce the optimization problem, standard
optimization problems (including linear programming, quadratic programming, and
semidefinite programming), and convex problems. We also introduce some
techniques such as eliminating inequality, equality, and set constraints,
adding slack variables, and epigraph form. We introduce Lagrangian function,
dual variables, KKT conditions (including primal feasibility, dual feasibility,
weak and strong duality, complementary slackness, and stationarity condition),
and solving optimization by method of Lagrange multipliers. Then, we cover
first-order optimization including gradient descent, line-search, convergence
of gradient methods, momentum, steepest descent, and backpropagation. Other
first-order methods are explained, such as accelerated gradient method,
stochastic gradient descent, mini-batch gradient descent, stochastic average
gradient, stochastic variance reduced gradient, AdaGrad, RMSProp, and Adam
optimizer, proximal methods (including proximal mapping, proximal point
algorithm, and proximal gradient method), and constrained gradient methods
(including projected gradient method, projection onto convex sets, and
Frank-Wolfe method). We also cover non-smooth and $\ell_1$ optimization methods
including lasso regularization, convex conjugate, Huber function,
soft-thresholding, coordinate descent, and subgradient methods. Then, we
explain second-order methods including Newton's method for unconstrained,
equality constrained, and inequality constrained problems....

    

### [[2110.01872] Permute Me Softly: Learning Soft Permutations for Graph Representations](http://arxiv.org/abs/2110.01872)


  Graph neural networks (GNNs) have recently emerged as a dominant paradigm for
machine learning with graphs. Research on GNNs has mainly focused on the family
of message passing neural networks (MPNNs). Similar to the Weisfeiler-Leman
(WL) test of isomorphism, these models follow an iterative neighborhood
aggregation procedure to update vertex representations, and they next compute
graph representations by aggregating the representations of the vertices.
Although very successful, MPNNs have been studied intensively in the past few
years. Thus, there is a need for novel architectures which will allow research
in the field to break away from MPNNs. In this paper, we propose a new graph
neural network model, so-called $\pi$-GNN which learns a "soft" permutation
(i.e., doubly stochastic) matrix for each graph, and thus projects all graphs
into a common vector space. The learned matrices impose a "soft" ordering on
the vertices of the input graphs, and based on this ordering, the adjacency
matrices are mapped into vectors. These vectors can be fed into fully-connected
or convolutional layers to deal with supervised learning tasks. In case of
large graphs, to make the model more efficient in terms of running time and
memory, we further relax the doubly stochastic matrices to row stochastic
matrices. We empirically evaluate the model on graph classification and graph
regression datasets and show that it achieves performance competitive with
state-of-the-art models.

    

### [[2110.01876] Extracting Major Topics of COVID-19 Related Tweets](http://arxiv.org/abs/2110.01876)


  With the outbreak of the Covid-19 virus, the activity of users on Twitter has
significantly increased. Some studies have investigated the hot topics of
tweets in this period; however, little attention has been paid to presenting
and analyzing the spatial and temporal trends of Covid-19 topics. In this
study, we use the topic modeling method to extract global topics during the
nationwide quarantine periods (March 23 to June 23, 2020) on Covid-19 tweets.
We implement the Latent Dirichlet Allocation (LDA) algorithm to extract the
topics and then name them with the "reopening", "death cases", "telecommuting",
"protests", "anger expression", "masking", "medication", "social distance",
"second wave", and "peak of the disease" titles. We additionally analyze
temporal trends of the topics for the whole world and four countries. By
analyzing the graphs, fascinating results are obtained from altering users'
focus on topics over time.

    

### [[2110.01889] Deep Neural Networks and Tabular Data: A Survey](http://arxiv.org/abs/2110.01889)


  Heterogeneous tabular data are the most commonly used form of data and are
essential for numerous critical and computationally demanding applications. On
homogeneous data sets, deep neural networks have repeatedly shown excellent
performance and have therefore been widely adopted. However, their application
to modeling tabular data (inference or generation) remains highly challenging.
This work provides an overview of state-of-the-art deep learning methods for
tabular data. We start by categorizing them into three groups: data
transformations, specialized architectures, and regularization models. We then
provide a comprehensive overview of the main approaches in each group. A
discussion of deep learning approaches for generating tabular data is
complemented by strategies for explaining deep models on tabular data. Our
primary contribution is to address the main research streams and existing
methodologies in this area, while highlighting relevant challenges and open
research questions. To the best of our knowledge, this is the first in-depth
look at deep learning approaches for tabular data. This work can serve as a
valuable starting point and guide for researchers and practitioners interested
in deep learning with tabular data.

    

### [[2110.01894] Combining Physics and Deep Learning to learn Continuous-Time Dynamics Models](http://arxiv.org/abs/2110.01894)


  Deep learning has been widely used within learning algorithms for robotics.
One disadvantage of deep networks is that these networks are black-box
representations. Therefore, the learned approximations ignore the existing
knowledge of physics or robotics. Especially for learning dynamics models,
these black-box models are not desirable as the underlying principles are well
understood and the standard deep networks can learn dynamics that violate these
principles. To learn dynamics models with deep networks that guarantee
physically plausible dynamics, we introduce physics-inspired deep networks that
combine first principles from physics with deep learning. We incorporate
Lagrangian mechanics within the model learning such that all approximated
models adhere to the laws of physics and conserve energy. Deep Lagrangian
Networks (DeLaN) parametrize the system energy using two networks. The
parameters are obtained by minimizing the squared residual of the
Euler-Lagrange differential equation. Therefore, the resulting model does not
require specific knowledge of the individual system, is interpretable, and can
be used as a forward, inverse, and energy model. Previously these properties
were only obtained when using system identification techniques that require
knowledge of the kinematic structure. We apply DeLaN to learning dynamics
models and apply these models to control simulated and physical rigid body
systems. The results show that the proposed approach obtains dynamics models
that can be applied to physical systems for real-time control. Compared to
standard deep networks, the physics-inspired models learn better models and
capture the underlying structure of the dynamics.

    

### [[2110.01899] Random matrices in service of ML footprint: ternary random features with no performance loss](http://arxiv.org/abs/2110.01899)


  In this article, we investigate the spectral behavior of random features
kernel matrices of the type ${\bf K} = \mathbb{E}_{\bf w}
\left[\sigma\left({\bf w}^{\sf T}{\bf x}_i\right)\sigma\left({\bf w}^{\sf
T}{\bf x}_j\right)\right]_{i,j=1}^n$, with nonlinear function $\sigma(\cdot)$,
data ${\bf x}_1, \ldots, {\bf x}_n \in \mathbb{R}^p$, and random projection
vector ${\bf w} \in \mathbb{R}^p$ having i.i.d. entries. In a high-dimensional
setting where the number of data $n$ and their dimension $p$ are both large and
comparable, we show, under a Gaussian mixture model for the data, that the
eigenspectrum of ${\bf K}$ is independent of the distribution of the
i.i.d.(zero-mean and unit-variance) entries of ${\bf w}$, and only depends on
$\sigma(\cdot)$ via its (generalized) Gaussian moments $\mathbb{E}_{z\sim
\mathcal N(0,1)}[\sigma'(z)]$ and $\mathbb{E}_{z\sim \mathcal
N(0,1)}[\sigma''(z)]$. As a result, for any kernel matrix ${\bf K}$ of the form
above, we propose a novel random features technique, called Ternary Random
Feature (TRF), that (i) asymptotically yields the same limiting kernel as the
original ${\bf K}$ in a spectral sense and (ii) can be computed and stored much
more efficiently, by wisely tuning (in a data-dependent manner) the function
$\sigma$ and the random vector ${\bf w}$, both taking values in $\{-1,0,1\}$.
The computation of the proposed random features requires no multiplication, and
a factor of $b$ times less bits for storage compared to classical random
features such as random Fourier features, with $b$ the number of bits to store
full precision values. Besides, it appears in our experiments on real data that
the substantial gains in computation and storage are accompanied with somewhat
improved performances compared to state-of-the-art random features
compression/quantization methods.

    

### [[2110.01901] Inferring Hidden Structures in Random Graphs](http://arxiv.org/abs/2110.01901)


  We study the two inference problems of detecting and recovering an isolated
community of \emph{general} structure planted in a random graph. The detection
problem is formalized as a hypothesis testing problem, where under the null
hypothesis, the graph is a realization of an Erd≈ës-R√©nyi random graph
$\mathcal{G}(n,q)$ with edge density $q\in(0,1)$; under the alternative, there
is an unknown structure $\Gamma_k$ on $k$ nodes, planted in $\mathcal{G}(n,q)$,
such that it appears as an \emph{induced subgraph}. In case of a successful
detection, we are concerned with the task of recovering the corresponding
structure. For these problems, we investigate the fundamental limits from both
the statistical and computational perspectives. Specifically, we derive lower
bounds for detecting/recovering the structure $\Gamma_k$ in terms of the
parameters $(n,k,q)$, as well as certain properties of $\Gamma_k$, and exhibit
computationally unbounded optimal algorithms that achieve these lower bounds.
We also consider the problem of testing in polynomial-time. As is customary in
many similar structured high-dimensional problems, our model undergoes an
"easy-hard-impossible" phase transition and computational constraints can
severely penalize the statistical performance. To provide an evidence for this
phenomenon, we show that the class of low-degree polynomials algorithms match
the statistical performance of the polynomial-time algorithms we develop.

    

### [[2110.01929] Data-driven Nonlinear Model Reduction to Spectral Submanifolds in Mechanical Systems](http://arxiv.org/abs/2110.01929)


  While data-driven model reduction techniques are well-established for
linearizable mechanical systems, general approaches to reducing
non-linearizable systems with multiple coexisting steady states have been
unavailable. In this paper, we review such a data-driven nonlinear model
reduction methodology based on spectral submanifolds (SSMs). As input, this
approach takes observations of unforced nonlinear oscillations to construct
normal forms of the dynamics reduced to very low dimensional invariant
manifolds. These normal forms capture amplitude-dependent properties and are
accurate enough to provide predictions for non-linearizable system response
under the additions of external forcing. We illustrate these results on
examples from structural vibrations, featuring both synthetic and experimental
data.

    

### [[2110.01939] Double Encoder-Decoder Networks for Gastrointestinal Polyp Segmentation](http://arxiv.org/abs/2110.01939)


  Polyps represent an early sign of the development of Colorectal Cancer. The
standard procedure for their detection consists of colonoscopic examination of
the gastrointestinal tract. However, the wide range of polyp shapes and visual
appearances, as well as the reduced quality of this image modality, turn their
automatic identification and segmentation with computational tools into a
challenging computer vision task. In this work, we present a new strategy for
the delineation of gastrointestinal polyps from endoscopic images based on a
direct extension of common encoder-decoder networks for semantic segmentation.
In our approach, two pretrained encoder-decoder networks are sequentially
stacked: the second network takes as input the concatenation of the original
frame and the initial prediction generated by the first network, which acts as
an attention mechanism enabling the second network to focus on interesting
areas within the image, thereby improving the quality of its predictions.
Quantitative evaluation carried out on several polyp segmentation databases
shows that double encoder-decoder networks clearly outperform their single
encoder-decoder counterparts in all cases. In addition, our best double
encoder-decoder combination attains excellent segmentation accuracy and reaches
state-of-the-art performance results in all the considered datasets, with a
remarkable boost of accuracy on images extracted from datasets not used for
training.

    

### [[2110.01950] Classification of high-dimensional data with spiked covariance matrix structure](http://arxiv.org/abs/2110.01950)


  We study the classification problem for high-dimensional data with $n$
observations on $p$ features where the $p \times p$ covariance matrix $\Sigma$
exhibits a spiked eigenvalues structure and the vector $\zeta$, given by the
difference between the whitened mean vectors, is sparse with sparsity at most
$s$. We propose an adaptive classifier (adaptive with respect to the sparsity
$s$) that first performs dimension reduction on the feature vectors prior to
classification in the dimensionally reduced space, i.e., the classifier
whitened the data, then screen the features by keeping only those corresponding
to the $s$ largest coordinates of $\zeta$ and finally apply Fisher linear
discriminant on the selected features. Leveraging recent results on entrywise
matrix perturbation bounds for covariance matrices, we show that the resulting
classifier is Bayes optimal whenever $n \rightarrow \infty$ and $s \sqrt{n^{-1}
\ln p} \rightarrow 0$. Experimental results on real and synthetic data sets
indicate that the proposed classifier is competitive with existing
state-of-the-art methods while also selecting a smaller number of features.

    

### [[2110.01951] Multi-Objective Few-shot Learning for Fair Classification](http://arxiv.org/abs/2110.01951)


  In this paper, we propose a general framework for mitigating the disparities
of the predicted classes with respect to secondary attributes within the data
(e.g., race, gender etc.). Our proposed method involves learning a
multi-objective function that in addition to learning the primary objective of
predicting the primary class labels from the data, also employs a
clustering-based heuristic to minimize the disparities of the class label
distribution with respect to the cluster memberships, with the assumption that
each cluster should ideally map to a distinct combination of attribute values.
Experiments demonstrate effective mitigation of cognitive biases on a benchmark
dataset without the use of annotations of secondary attribute values (the
zero-shot case) or with the use of a small number of attribute value
annotations (the few-shot case).

    

### [[2110.01954] Continuous-Time Fitted Value Iteration for Robust Policies](http://arxiv.org/abs/2110.01954)


  Solving the Hamilton-Jacobi-Bellman equation is important in many domains
including control, robotics and economics. Especially for continuous control,
solving this differential equation and its extension the Hamilton-Jacobi-Isaacs
equation, is important as it yields the optimal policy that achieves the
maximum reward on a give task. In the case of the Hamilton-Jacobi-Isaacs
equation, which includes an adversary controlling the environment and
minimizing the reward, the obtained policy is also robust to perturbations of
the dynamics. In this paper we propose continuous fitted value iteration (cFVI)
and robust fitted value iteration (rFVI). These algorithms leverage the
non-linear control-affine dynamics and separable state and action reward of
many continuous control problems to derive the optimal policy and optimal
adversary in closed form. This analytic expression simplifies the differential
equations and enables us to solve for the optimal value function using value
iteration for continuous actions and states as well as the adversarial case.
Notably, the resulting algorithms do not require discretization of states or
actions. We apply the resulting algorithms to the Furuta pendulum and cartpole.
We show that both algorithms obtain the optimal policy. The robustness Sim2Real
experiments on the physical systems show that the policies successfully achieve
the task in the real-world. When changing the masses of the pendulum, we
observe that robust value iteration is more robust compared to deep
reinforcement learning algorithm and the non-robust version of the algorithm.
Videos of the experiments are shown at this https URL


### [[2110.01955] Distribution Mismatch Correction for Improved Robustness in Deep Neural Networks](http://arxiv.org/abs/2110.01955)


  Deep neural networks rely heavily on normalization methods to improve their
performance and learning behavior. Although normalization methods spurred the
development of increasingly deep and efficient architectures, they also
increase the vulnerability with respect to noise and input corruptions. In most
applications, however, noise is ubiquitous and diverse; this can often lead to
complete failure of machine learning systems as they fail to cope with
mismatches between the input distribution during training- and test-time. The
most common normalization method, batch normalization, reduces the distribution
shift during training but is agnostic to changes in the input distribution
during test time. This makes batch normalization prone to performance
degradation whenever noise is present during test-time. Sample-based
normalization methods can correct linear transformations of the activation
distribution but cannot mitigate changes in the distribution shape; this makes
the network vulnerable to distribution changes that cannot be reflected in the
normalization parameters. We propose an unsupervised non-parametric
distribution correction method that adapts the activation distribution of each
layer. This reduces the mismatch between the training and test-time
distribution by minimizing the 1-D Wasserstein distance. In our experiments, we
empirically show that the proposed method effectively reduces the impact of
intense image corruptions and thus improves the classification performance
without the need for retraining or fine-tuning the model.

    

### [[2110.01960] A new harmonium for pattern recognition in survival data](http://arxiv.org/abs/2110.01960)


  Background: Survival analysis concerns the study of timeline data where the
event of interest may remain unobserved (i.e., censored). Studies commonly
record more than one type of event, but conventional survival techniques focus
on a single event type. We set out to integrate both multiple independently
censored time-to-event variables as well as missing observations.
Methods: An energy-based approach is taken with a bi-partite structure
between latent and visible states, commonly known as harmoniums (or restricted
Boltzmann machines).
Results: The present harmonium is shown, both theoretically and
experimentally, to capture non-linear patterns between distinct time
recordings. We illustrate on real world data that, for a single time-to-event
variable, our model is on par with established methods. In addition, we
demonstrate that discriminative predictions improve by leveraging an extra
time-to-event variable.
Conclusions: Multiple time-to-event variables can be successfully captured
within the harmonium paradigm.

    

### [[2110.01984] Differential Privacy of Dirichlet Posterior Sampling](http://arxiv.org/abs/2110.01984)


  Besides the Laplace distribution and the Gaussian distribution, there are
many more probability distributions which is not well-understood in terms of
privacy-preserving property of a random draw -- one of which is the Dirichlet
distribution. In this work, we study the inherent privacy of releasing a single
draw from a Dirichlet posterior distribution. As a complement to the previous
study that provides general theories on the differential privacy of posterior
sampling from exponential families, this study focuses specifically on the
Dirichlet posterior sampling and its privacy guarantees. With the notion of
truncated concentrated differential privacy (tCDP), we are able to derive a
simple privacy guarantee of the Dirichlet posterior sampling, which effectively
allows us to analyze its utility in various settings. Specifically, we prove
accuracy guarantees of private Multinomial-Dirichlet sampling, which is
prevalent in Bayesian tasks, and private release of a normalized histogram. In
addition, with our results, it is possible to make Bayesian reinforcement
learning differentially private by modifying the Dirichlet sampling for state
transition probabilities.

    

### [[2110.01999] Federating for Learning Group Fair Models](http://arxiv.org/abs/2110.01999)


  Federated learning is an increasingly popular paradigm that enables a large
number of entities to collaboratively learn better models. In this work, we
study minmax group fairness in paradigms where different participating entities
may only have access to a subset of the population groups during the training
phase. We formally analyze how this fairness objective differs from existing
federated learning fairness criteria that impose similar performance across
participants instead of demographic groups. We provide an optimization
algorithm -- FedMinMax -- for solving the proposed problem that provably enjoys
the performance guarantees of centralized learning algorithms. We
experimentally compare the proposed approach against other methods in terms of
group fairness in various federated learning setups.

    

### [[2110.02011] Sound Event Detection Transformer: An Event-based End-to-End Model for Sound Event Detection](http://arxiv.org/abs/2110.02011)


  Sound event detection (SED) has gained increasing attention with its wide
application in surveillance, video indexing, etc. Existing models in SED mainly
generate frame-level predictions, converting it into a sequence multi-label
classification problem, which inevitably brings a trade-off between event
boundary detection and audio tagging when using weakly labeled data to train
the model. Besides, it needs post-processing and cannot be trained in an
end-to-end way. This paper firstly presents the 1D Detection Transformer
(1D-DETR), inspired by Detection Transformer. Furthermore, given the
characteristics of SED, the audio query and a one-to-many matching strategy for
fine-tuning the model are added to 1D-DETR to form the model of Sound Event
Detection Transformer (SEDT), which generates event-level predictions,
end-to-end detection. Experiments are conducted on the URBAN-SED dataset and
the DCASE2019 Task4 dataset, and both experiments have achieved competitive
results compared with SOTA models. The application of SEDT on SED shows that it
can be used as a framework for one-dimensional signal detection and may be
extended to other similar tasks.

    

### [[2110.02019] FoodChem: A food-chemical relation extraction model](http://arxiv.org/abs/2110.02019)


  In this paper, we present FoodChem, a new Relation Extraction (RE) model for
identifying chemicals present in the composition of food entities, based on
textual information provided in biomedical peer-reviewed scientific literature.
The RE task is treated as a binary classification problem, aimed at identifying
whether the contains relation exists between a food-chemical entity pair. This
is accomplished by fine-tuning BERT, BioBERT and RoBERTa transformer models.
For evaluation purposes, a novel dataset with annotated contains relations in
food-chemical entity pairs is generated, in a golden and silver version. The
models are integrated into a voting scheme in order to produce the silver
version of the dataset which we use for augmenting the individual models, while
the manually annotated golden version is used for their evaluation. Out of the
three evaluated models, the BioBERT model achieves the best results, with a
macro averaged F1 score of 0.902 in the unbalanced augmentation setting.

    

### [[2110.02034] Dropout Q-Functions for Doubly Efficient Reinforcement Learning](http://arxiv.org/abs/2110.02034)


  Randomized ensemble double Q-learning (REDQ) has recently achieved
state-of-the-art sample efficiency on continuous-action reinforcement learning
benchmarks. This superior sample efficiency is possible by using a large
Q-function ensemble. However, REDQ is much less computationally efficient than
non-ensemble counterparts such as Soft Actor-Critic (SAC). To make REDQ more
computationally efficient, we propose a method of improving computational
efficiency called Dr.Q, which is a variant of REDQ that uses a small ensemble
of dropout Q-functions. Our dropout Q-functions are simple Q-functions equipped
with dropout connection and layer normalization. Despite its simplicity of
implementation, our experimental results indicate that Dr.Q is doubly (sample
and computationally) efficient. It achieved comparable sample efficiency with
REDQ and much better computational efficiency than REDQ and comparable
computational efficiency with that of SAC.

    

### [[2110.02035] FooDI-ML: a large multi-language dataset of food, drinks and groceries images and descriptions](http://arxiv.org/abs/2110.02035)


  In this paper we introduce the Food Drinks and groceries Images Multi Lingual
(FooDI-ML) dataset. This dataset contains over 1.5M unique images and over 9.5M
store names, product names descriptions, and collection sections gathered from
the Glovo application. The data made available corresponds to food, drinks and
groceries products from 37 countries in Europe, the Middle East, Africa and
Latin America. The dataset comprehends 33 languages, including 870K samples of
languages of countries from Eastern Europe and Western Asia such as Ukrainian
and Kazakh, which have been so far underrepresented in publicly available
visio-linguistic datasets. The dataset also includes widely spoken languages
such as Spanish and English. To assist further research, we include a benchmark
over the text-image retrieval task using ADAPT, a SotA existing technique.

    

### [[2110.02037] Autoregressive Diffusion Models](http://arxiv.org/abs/2110.02037)


  We introduce Autoregressive Diffusion Models (ARDMs), a model class
encompassing and generalizing order-agnostic autoregressive models (Uria et
al., 2014) and absorbing discrete diffusion (Austin et al., 2021), which we
show are special cases of ARDMs under mild assumptions. ARDMs are simple to
implement and easy to train. Unlike standard ARMs, they do not require causal
masking of model representations, and can be trained using an efficient
objective similar to modern probabilistic diffusion models that scales
favourably to highly-dimensional data. At test time, ARDMs support parallel
generation which can be adapted to fit any given generation budget. We find
that ARDMs require significantly fewer steps than discrete diffusion models to
attain the same performance. Finally, we apply ARDMs to lossless compression,
and show that they are uniquely suited to this task. Contrary to existing
approaches based on bits-back coding, ARDMs obtain compelling results not only
on complete datasets, but also on compressing single data points. Moreover,
this can be done using a modest number of network calls for (de)compression due
to the model's adaptable parallel generation.

    

### [[2110.02038] Semi-Supervised Deep Learning for Multiplex Networks](http://arxiv.org/abs/2110.02038)


  Multiplex networks are complex graph structures in which a set of entities
are connected to each other via multiple types of relations, each relation
representing a distinct layer. Such graphs are used to investigate many complex
biological, social, and technological systems. In this work, we present a novel
semi-supervised approach for structure-aware representation learning on
multiplex networks. Our approach relies on maximizing the mutual information
between local node-wise patch representations and label correlated
structure-aware global graph representations to model the nodes and cluster
structures jointly. Specifically, it leverages a novel cluster-aware,
node-contextualized global graph summary generation strategy for effective
joint-modeling of node and cluster representations across the layers of a
multiplex network. Empirically, we demonstrate that the proposed architecture
outperforms state-of-the-art methods in a range of tasks: classification,
clustering, visualization, and similarity search on seven real-world multiplex
networks for various experiment settings.

    

### [[2110.02044] Multi-Object Tracking with Deep Learning Ensemble for Unmanned Aerial System Applications](http://arxiv.org/abs/2110.02044)


  Multi-object tracking (MOT) is a crucial component of situational awareness
in military defense applications. With the growing use of unmanned aerial
systems (UASs), MOT methods for aerial surveillance is in high demand.
Application of MOT in UAS presents specific challenges such as moving sensor,
changing zoom levels, dynamic background, illumination changes, obscurations
and small objects. In this work, we present a robust object tracking
architecture aimed to accommodate for the noise in real-time situations. We
propose a kinematic prediction model, called Deep Extended Kalman Filter
(DeepEKF), in which a sequence-to-sequence architecture is used to predict
entity trajectories in latent space. DeepEKF utilizes a learned image embedding
along with an attention mechanism trained to weight the importance of areas in
an image to predict future states. For the visual scoring, we experiment with
different similarity measures to calculate distance based on entity
appearances, including a convolutional neural network (CNN) encoder,
pre-trained using Siamese networks. In initial evaluation experiments, we show
that our method, combining scoring structure of the kinematic and visual models
within a MHT framework, has improved performance especially in edge cases where
entity motion is unpredictable, or the data presents frames with significant
gaps.

    

### [[2110.02048] Graph Coloring: Comparing Cluster Graphs to Factor Graphs](http://arxiv.org/abs/2110.02048)


  We present a means of formulating and solving graph coloring problems with
probabilistic graphical models. In contrast to the prevalent literature that
uses factor graphs for this purpose, we instead approach it from a cluster
graph perspective. Since there seems to be a lack of algorithms to
automatically construct valid cluster graphs, we provide such an algorithm
(termed LTRIP). Our experiments indicate a significant advantage for preferring
cluster graphs over factor graphs, both in terms of accuracy as well as
computational efficiency.

    

### [[2110.02057] Structured Prediction in NLP -- A survey](http://arxiv.org/abs/2110.02057)


  Over the last several years, the field of Structured prediction in NLP has
had seen huge advancements with sophisticated probabilistic graphical models,
energy-based networks, and its combination with deep learning-based approaches.
This survey provides a brief of major techniques in structured prediction and
its applications in the NLP domains like parsing, sequence labeling, text
generation, and sequence to sequence tasks. We also deep-dived into
energy-based and attention-based techniques in structured prediction,
identified some relevant open issues and gaps in the current state-of-the-art
research, and have come up with some detailed ideas for future research in
these fields.

    

### [[2110.02058] Interactively Generating Explanations for Transformer-based Language Models](http://arxiv.org/abs/2110.02058)


  Transformer language models are state-of-the-art in a multitude of NLP tasks.
Despite these successes, their opaqueness remains problematic. Recent methods
aiming to provide interpretability and explainability to black-box models
primarily focus on post-hoc explanations of (sometimes spurious) input-output
correlations. Instead, we emphasize using prototype networks directly
incorporated into the model architecture and hence explain the reasoning
process behind the network's decisions. Moreover, while our architecture
performs on par with several language models, it enables one to learn from user
interactions. This not only offers a better understanding of language models,
but uses human capabilities to incorporate knowledge outside of the rigid range
of purely data-driven approaches.

    

### [[2110.02059] Multi-Relational Graph based Heterogeneous Multi-Task Learning in Community Question Answering](http://arxiv.org/abs/2110.02059)


  Various data mining tasks have been proposed to study Community Question
Answering (CQA) platforms like Stack Overflow. The relatedness between some of
these tasks provides useful learning signals to each other via Multi-Task
Learning (MTL). However, due to the high heterogeneity of these tasks, few
existing works manage to jointly solve them in a unified framework. To tackle
this challenge, we develop a multi-relational graph based MTL model called
Heterogeneous Multi-Task Graph Isomorphism Network (HMTGIN) which efficiently
solves heterogeneous CQA tasks. In each training forward pass, HMTGIN embeds
the input CQA forum graph by an extension of Graph Isomorphism Network and skip
connections. The embeddings are then shared across all task-specific output
layers to compute respective losses. Moreover, two cross-task constraints based
on the domain knowledge about tasks' relationships are used to regularize the
joint learning. In the evaluation, the embeddings are shared among different
task-specific output layers to make corresponding predictions. To the best of
our knowledge, HMTGIN is the first MTL model capable of tackling CQA tasks from
the aspect of multi-relational graphs. To evaluate HMTGIN's effectiveness, we
build a novel large-scale multi-relational graph CQA dataset with over two
million nodes from Stack Overflow. Extensive experiments show that: $(1)$
HMTGIN is superior to all baselines on five tasks; $(2)$ The proposed MTL
strategy and cross-task constraints have substantial advantages.

    

### [[2110.02063] A Critique of Strictly Batch Imitation Learning](http://arxiv.org/abs/2110.02063)


  Recent work by Jarrett et al. attempts to frame the problem of offline
imitation learning (IL) as one of learning a joint energy-based model, with the
hope of out-performing standard behavioral cloning. We suggest that notational
issues obscure how the psuedo-state visitation distribution the authors propose
to optimize might be disconnected from the policy's $\textit{true}$ state
visitation distribution. We further construct natural examples where the
parameter coupling advocated by Jarrett et al. leads to inconsistent estimates
of the expert's policy, unlike behavioral cloning.

    

### [[2110.02065] SDR: Efficient Neural Re-ranking using Succinct Document Representation](http://arxiv.org/abs/2110.02065)


  BERT based ranking models have achieved superior performance on various
information retrieval tasks. However, the large number of parameters and
complex self-attention operation come at a significant latency overhead. To
remedy this, recent works propose late-interaction architectures, which allow
pre-computation of intermediate document representations, thus reducing the
runtime latency. Nonetheless, having solved the immediate latency issue, these
methods now introduce storage costs and network fetching latency, which limits
their adoption in real-life production systems.
In this work, we propose the Succinct Document Representation (SDR) scheme
that computes \emph{highly compressed} intermediate document representations,
mitigating the storage/network issue. Our approach first reduces the dimension
of token representations by encoding them using a novel autoencoder
architecture that uses the document's textual content in both the encoding and
decoding phases. After this token encoding step, we further reduce the size of
entire document representations using a modern quantization technique.
Extensive evaluations on passage re-reranking on the MSMARCO dataset show
that compared to existing approaches using compressed document representations,
our method is highly efficient, achieving 4x-11.6x better compression rates for
the same ranking quality.

    

### [[2110.02068] Spatial Context Awareness for Unsupervised Change Detection in Optical Satellite Images](http://arxiv.org/abs/2110.02068)


  Detecting changes on the ground in multitemporal Earth observation data is
one of the key problems in remote sensing. In this paper, we introduce Sibling
Regression for Optical Change detection (SiROC), an unsupervised method for
change detection in optical satellite images with medium and high resolution.
SiROC is a spatial context-based method that models a pixel as a linear
combination of its distant neighbors. It uses this model to analyze differences
in the pixel and its spatial context-based predictions in subsequent time
periods for change detection. We combine this spatial context-based change
detection with ensembling over mutually exclusive neighborhoods and
transitioning from pixel to object-level changes with morphological operations.
SiROC achieves competitive performance for change detection with
medium-resolution Sentinel-2 and high-resolution Planetscope imagery on four
datasets. Besides accurate predictions without the need for training, SiROC
also provides a well-calibrated uncertainty of its predictions. This makes the
method especially useful in conjunction with deep-learning based methods for
applications such as pseudo-labeling.

    

### [[2110.02080] A Methodology to Identify Cognition Gaps in Visual Recognition Applications Based on Convolutional Neural Networks](http://arxiv.org/abs/2110.02080)


  Developing consistently well performing visual recognition applications based
on convolutional neural networks, e.g. for autonomous driving, is very
challenging. One of the obstacles during the development is the opaqueness of
their cognitive behaviour. A considerable amount of literature has been
published which describes irrational behaviour of trained CNNs showcasing gaps
in their cognition. In this paper, a methodology is presented that creates
worstcase images using image augmentation techniques. If the CNN's cognitive
performance on such images is weak while the augmentation techniques are
supposedly harmless, a potential gap in the cognition has been found. The
presented worst-case image generator is using adversarial search approaches to
efficiently identify the most challenging image. This is evaluated with the
well-known AlexNet CNN using images depicting a typical driving scenario.

    

### [[2110.02083] Applying Machine Learning to Study Fluid Mechanics](http://arxiv.org/abs/2110.02083)


  This paper provides a short overview of how to use machine learning to build
data-driven models in fluid mechanics. The process of machine learning is
broken down into five stages: (1) formulating a problem to model, (2)
collecting and curating training data to inform the model, (3) choosing an
architecture with which to represent the model, (4) designing a loss function
to assess the performance of the model, and (5) selecting and implementing an
optimization algorithm to train the model. At each stage, we discuss how prior
physical knowledge may be embedding into the process, with specific examples
from the field of fluid mechanics.

    

### [[2110.02085] The Potential of Machine Learning to Enhance Computational Fluid Dynamics](http://arxiv.org/abs/2110.02085)


  Machine learning is rapidly becoming a core technology for scientific
computing, with numerous opportunities to advance the field of computational
fluid dynamics. This paper highlights some of the areas of highest potential
impact, including to accelerate direct numerical simulations, to improve
turbulence closure modelling, and to develop enhanced reduced-order models. In
each of these areas, it is possible to improve machine learning capabilities by
incorporating physics into the process, and in turn, to improve the simulation
of fluids to uncover new physical understanding. Despite the promise of machine
learning described here, we also note that classical methods are often more
efficient for many tasks. We also emphasize that in order to harness the full
potential of machine learning to improve computational fluid dynamics, it is
essential for the community to continue to establish benchmark systems and best
practices for open-source software, data sharing, and reproducible research.

    

### [[2110.02095] Exploring the Limits of Large Scale Pre-training](http://arxiv.org/abs/2110.02095)


  Recent developments in large-scale machine learning suggest that by scaling
up data, model size and training time properly, one might observe that
improvements in pre-training would transfer favorably to most downstream tasks.
In this work, we systematically study this phenomena and establish that, as we
increase the upstream accuracy, the performance of downstream tasks saturates.
In particular, we investigate more than 4800 experiments on Vision
Transformers, MLP-Mixers and ResNets with number of parameters ranging from ten
million to ten billion, trained on the largest scale of available image data
(JFT, ImageNet21K) and evaluated on more than 20 downstream image recognition
tasks. We propose a model for downstream performance that reflects the
saturation phenomena and captures the nonlinear relationship in performance of
upstream and downstream tasks. Delving deeper to understand the reasons that
give rise to these phenomena, we show that the saturation behavior we observe
is closely related to the way that representations evolve through the layers of
the models. We showcase an even more extreme scenario where performance on
upstream and downstream are at odds with each other. That is, to have a better
downstream performance, we need to hurt upstream accuracy.

    

### [[2110.02096] Top-N: Equivariant set and graph generation without exchangeability](http://arxiv.org/abs/2110.02096)


  We consider one-shot probabilistic decoders that map a vector-shaped prior to
a distribution over sets or graphs. These functions can be integrated into
variational autoencoders (VAE), generative adversarial networks (GAN) or
normalizing flows, and have important applications in drug discovery. Set and
graph generation is most commonly performed by generating points (and sometimes
edge weights) i.i.d. from a normal distribution, and processing them along with
the prior vector using Transformer layers or graph neural networks. This
architecture is designed to generate exchangeable distributions (all
permutations of a set are equally likely) but it is hard to train due to the
stochasticity of i.i.d. generation. We propose a new definition of equivariance
and show that exchangeability is in fact unnecessary in VAEs and GANs. We then
introduce Top-n, a deterministic, non-exchangeable set creation mechanism which
learns to select the most relevant points from a trainable reference set. Top-n
can replace i.i.d. generation in any VAE or GAN -- it is easier to train and
better captures complex dependencies in the data. Top-n outperforms i.i.d
generation by 15% at SetMNIST reconstruction, generates sets that are 64%
closer to the true distribution on a synthetic molecule-like dataset, and is
able to generate more diverse molecules when trained on the classical QM9
dataset. With improved foundations in one-shot generation, our algorithm
contributes to the design of more effective molecule generation methods.

    

### [[2110.02102] CARL: A Benchmark for Contextual and Adaptive Reinforcement Learning](http://arxiv.org/abs/2110.02102)


  While Reinforcement Learning has made great strides towards solving ever more
complicated tasks, many algorithms are still brittle to even slight changes in
their environment. This is a limiting factor for real-world applications of RL.
Although the research community continuously aims at improving both robustness
and generalization of RL algorithms, unfortunately it still lacks an
open-source set of well-defined benchmark problems based on a consistent
theoretical framework, which allows comparing different approaches in a fair,
reliable and reproducibleway. To fill this gap, we propose CARL, a collection
of well-known RL environments extended to contextual RL problems to study
generalization. We show the urgent need of such benchmarks by demonstrating
that even simple toy environments become challenging for commonly used
approaches if different contextual instances of this task have to be
considered. Furthermore, CARL allows us to provide first evidence that
disentangling representation learning of the states from the policy learning
with the context facilitates better generalization. By providing variations of
diverse benchmarks from classic control, physical simulations, games and a
real-world application of RNA design, CARL will allow the community to derive
many more such insights on a solid empirical foundation.

    

### [[2110.02121] Optimization with Constraint Learning: A Framework and Survey](http://arxiv.org/abs/2110.02121)


  Many real-life optimization problems frequently contain one or more
constraints or objectives for which there are no explicit formulas. If data is
however available, these data can be used to learn the constraints. The
benefits of this approach are clearly seen, however there is a need for this
process to be carried out in a structured manner. This paper therefore provides
a framework for Optimization with Constraint Learning (OCL) which we believe
will help to formalize and direct the process of learning constraints from
data. This framework includes the following steps: (i) setup of the conceptual
optimization model, (ii) data gathering and preprocessing, (iii) selection and
training of predictive models, (iv) resolution of the optimization model, and
(v) verification and improvement of the optimization model. We then review the
recent OCL literature in light of this framework, and highlight current trends,
as well as areas for future research.

    

### [[2110.02124] $\textit{FacialFilmroll}$: High-resolution multi-shot video editing](http://arxiv.org/abs/2110.02124)


  We present $\textit{FacialFilmroll}$, a solution for spatially and temporally
consistent editing of faces in one or multiple shots. We build upon unwrap
mosaic [Rav-Acha et al. 2008] by specializing it to faces. We leverage recent
techniques to fit a 3D face model on monocular videos to (i) improve the
quality of the mosaic for edition and (ii) permit the automatic transfer of
edits from one shot to other shots of the same actor. We explain how
$\textit{FacialFilmroll}$ is integrated in post-production facility. Finally,
we present video editing results using $\textit{FacialFilmroll}$ on high
resolution videos.

    

### [[2110.02125] Adversarial Robustness Verification and Attack Synthesis in Stochastic Systems](http://arxiv.org/abs/2110.02125)


  Probabilistic model checking is a useful technique for specifying and
verifying properties of stochastic systems including randomized protocols and
the theoretical underpinnings of reinforcement learning models. However, these
methods rely on the assumed structure and probabilities of certain system
transitions. These assumptions may be incorrect, and may even be violated in
the event that an adversary gains control of some or all components in the
system.
In this paper, motivated by research in adversarial machine learning on
adversarial examples, we develop a formal framework for adversarial robustness
in systems defined as discrete time Markov chains (DTMCs), and extend to
include deterministic, memoryless policies acting in Markov decision processes
(MDPs). Our framework includes a flexible approach for specifying several
adversarial models with different capabilities to manipulate the system. We
outline a class of threat models under which adversaries can perturb system
transitions, constrained by an $\varepsilon$ ball around the original
transition probabilities and define four specific instances of this threat
model.
We define three main DTMC adversarial robustness problems and present two
optimization-based solutions, leveraging traditional and parametric
probabilistic model checking techniques. We then evaluate our solutions on two
stochastic protocols and a collection of GridWorld case studies, which model an
agent acting in an environment described as an MDP. We find that the parametric
solution results in fast computation for small parameter spaces. In the case of
less restrictive (stronger) adversaries, the number of parameters increases,
and directly computing property satisfaction probabilities is more scalable. We
demonstrate the usefulness of our definitions and solutions by comparing system
outcomes over various properties, threat models, and case studies.

    

### [[2110.02128] NeurWIN: Neural Whittle Index Network For Restless Bandits Via Deep RL](http://arxiv.org/abs/2110.02128)


  Whittle index policy is a powerful tool to obtain asymptotically optimal
solutions for the notoriously intractable problem of restless bandits. However,
finding the Whittle indices remains a difficult problem for many practical
restless bandits with convoluted transition kernels. This paper proposes
NeurWIN, a neural Whittle index network that seeks to learn the Whittle indices
for any restless bandits by leveraging mathematical properties of the Whittle
indices. We show that a neural network that produces the Whittle index is also
one that produces the optimal control for a set of Markov decision problems.
This property motivates using deep reinforcement learning for the training of
NeurWIN. We demonstrate the utility of NeurWIN by evaluating its performance
for three recently studied restless bandit problems. Our experiment results
show that the performance of NeurWIN is significantly better than other RL
algorithms.

    

### [[2110.02129] A study of first-passage time minimization via Q-learning in heated gridworlds](http://arxiv.org/abs/2110.02129)


  Optimization of first-passage times is required in applications ranging from
nanobots navigation to market trading. In such settings, one often encounters
unevenly distributed noise levels across the environment. We extensively study
how a learning agent fares in 1- and 2- dimensional heated gridworlds with an
uneven temperature distribution. The results show certain bias effects in
agents trained via simple tabular Q-learning, SARSA, Expected SARSA and Double
Q-learning. While high learning rate prevents exploration of regions with
higher temperature, low enough rate increases the presence of agents in such
regions. The discovered peculiarities and biases of temporal-difference-based
reinforcement learning methods should be taken into account in real-world
physical applications and agent design.

    

### [[2110.02148] NaRLE: Natural Language Models using Reinforcement Learning with Emotion Feedback](http://arxiv.org/abs/2110.02148)


  Current research in dialogue systems is focused on conversational assistants
working on short conversations in either task-oriented or open domain settings.
In this paper, we focus on improving task-based conversational assistants
online, primarily those working on document-type conversations (e.g., emails)
whose contents may or may not be completely related to the assistant's task. We
propose "NARLE" a deep reinforcement learning (RL) framework for improving the
natural language understanding (NLU) component of dialogue systems online
without the need to collect human labels for customer data. The proposed
solution associates user emotion with the assistant's action and uses that to
improve NLU models using policy gradients. For two intent classification
problems, we empirically show that using reinforcement learning to fine tune
the pre-trained supervised learning models improves performance up to 43%.
Furthermore, we demonstrate the robustness of the method to partial and noisy
implicit feedback.

    

### [[2110.02153] Inference and De-Noising of Non-Gaussian Particle Distribution Functions: A Generative Modeling Approach](http://arxiv.org/abs/2110.02153)


  The particle-in-cell numerical method of plasma physics balances a trade-off
between computational cost and intrinsic noise. Inference on data produced by
these simulations generally consists of binning the data to recover the
particle distribution function, from which physical processes may be
investigated. In addition to containing noise, the distribution function is
temporally dynamic and can be non-gaussian and multi-modal, making the task of
modeling it difficult. Here we demonstrate the use of normalizing flows to
learn a smooth, tractable approximation to the noisy particle distribution
function. We demonstrate that the resulting data driven likelihood conserves
relevant physics and may be extended to encapsulate the temporal evolution of
the distribution function.

    

### [[2110.02159] Label differential privacy via clustering](http://arxiv.org/abs/2110.02159)


  We present new mechanisms for \emph{label differential privacy}, a relaxation
of differentially private machine learning that only protects the privacy of
the labels in the training set. Our mechanisms cluster the examples in the
training set using their (non-private) feature vectors, randomly re-sample each
label from examples in the same cluster, and output a training set with noisy
labels as well as a modified version of the true loss function. We prove that
when the clusters are both large and high-quality, the model that minimizes the
modified loss on the noisy training set converges to small excess risk at a
rate that is comparable to the rate for non-private learning. We describe both
a centralized mechanism in which the entire training set is stored by a trusted
curator, and a distributed mechanism where each user stores a single labeled
example and replaces her label with the label of a randomly selected user from
the same cluster. We also describe a learning problem in which large clusters
are necessary to achieve both strong privacy and either good precision or good
recall. Our experiments show that randomizing the labels within each cluster
significantly improves the privacy vs. accuracy trade-off compared to applying
uniform randomized response to the labels, and also compared to learning a
model via DP-SGD.

    

### [[2110.02161] Optimal N-ary ECOC Matrices for Ensemble Classification](http://arxiv.org/abs/2110.02161)


  A new recursive construction of $N$-ary error-correcting output code (ECOC)
matrices for ensemble classification methods is presented, generalizing the
classic doubling construction for binary Hadamard matrices. Given any prime
integer $N$, this deterministic construction generates base-$N$ symmetric
square matrices $M$ of prime-power dimension having optimal minimum Hamming
distance between any two of its rows and columns. Experimental results for six
datasets demonstrate that using these deterministic coding matrices for $N$-ary
ECOC classification yields comparable and in many cases higher accuracy
compared to using randomly generated coding matrices. This is particular true
when $N$ is adaptively chosen so that the dimension of $M$ matches closely with
the number of classes in a dataset, which reduces the loss in minimum Hamming
distance when $M$ is truncated to fit the dataset. This is verified through a
distance formula for $M$ which shows that these adaptive matrices have
significantly higher minimum Hamming distance in comparison to randomly
generated ones.

    

### [[2110.02166] Prediction of Energy Consumption for Variable Customer Portfolios Including Aleatoric Uncertainty Estimation](http://arxiv.org/abs/2110.02166)


  Using hourly energy consumption data recorded by smart meters, retailers can
estimate the day-ahead energy consumption of their customer portfolio. Deep
neural networks are especially suited for this task as a huge amount of
historical consumption data is available from smart meter recordings to be used
for model training. Probabilistic layers further enable the estimation of the
uncertainty of the consumption forecasts. Here, we propose a method to
calculate hourly day-ahead energy consumption forecasts which include an
estimation of the aleatoric uncertainty. To consider the statistical properties
of energy consumption values, the aleatoric uncertainty is modeled using
lognormal distributions whose parameters are calculated by deep neural
networks. As a result, predictions of the hourly day-ahead energy consumption
of single customers are represented by random variables drawn from lognormal
distributions obtained as output from the neural network. We further
demonstrate, how these random variables corresponding to single customers can
be aggregated to probabilistic forecasts of customer portfolios of arbitrary
composition.

    

### [[2110.02169] SOUL: An Energy-Efficient Unsupervised Online Learning Seizure Detection Classifier](http://arxiv.org/abs/2110.02169)


  Implantable devices that record neural activity and detect seizures have been
adopted to issue warnings or trigger neurostimulation to suppress epileptic
seizures. Typical seizure detection systems rely on high-accuracy
offline-trained machine learning classifiers that require manual retraining
when seizure patterns change over long periods of time. For an implantable
seizure detection system, a low power, at-the-edge, online learning algorithm
can be employed to dynamically adapt to the neural signal drifts, thereby
maintaining high accuracy without external intervention. This work proposes
SOUL: Stochastic-gradient-descent-based Online Unsupervised Logistic regression
classifier. After an initial offline training phase, continuous online
unsupervised classifier updates are applied in situ, which improves sensitivity
in patients with drifting seizure features. SOUL was tested on two human
electroencephalography (EEG) datasets: the CHB-MIT scalp EEG dataset, and a
long (>100 hours) NeuroVista intracranial EEG dataset. It was able to achieve
an average sensitivity of 97.5% and 97.9% for the two datasets respectively, at
>95% specificity. Sensitivity improved by at most 8.2% on long-term data when
compared to a typical seizure detection classifier. SOUL was fabricated in
TSMC's 28 nm process occupying 0.1 mm2 and achieves 1.5 nJ/classification
energy efficiency, which is at least 24x more efficient than state-of-the-art.

    

### [[2110.02176] Machine learning attack on copy detection patterns: are 1x1 patterns cloneable?](http://arxiv.org/abs/2110.02176)


  Nowadays, the modern economy critically requires reliable yet cheap
protection solutions against product counterfeiting for the mass market. Copy
detection patterns (CDP) are considered as such solution in several
applications. It is assumed that being printed at the maximum achievable limit
of a printing resolution of an industrial printer with the smallest symbol size
1x1 elements, the CDP cannot be copied with sufficient accuracy and thus are
unclonable. In this paper, we challenge this hypothesis and consider a copy
attack against the CDP based on machine learning. The experimental based on
samples produced on two industrial printers demonstrate that simple detection
metrics used in the CDP authentication cannot reliably distinguish the original
CDP from their fakes. Thus, the paper calls for a need of careful
reconsideration of CDP cloneability and search for new authentication
techniques and CDP optimization because of the current attack.

    

### [[2110.02177] Secure Aggregation for Buffered Asynchronous Federated Learning](http://arxiv.org/abs/2110.02177)


  Federated learning (FL) typically relies on synchronous training, which is
slow due to stragglers. While asynchronous training handles stragglers
efficiently, it does not ensure privacy due to the incompatibility with the
secure aggregation protocols. A buffered asynchronous training protocol known
as FedBuff has been proposed recently which bridges the gap between synchronous
and asynchronous training to mitigate stragglers and to also ensure privacy
simultaneously. FedBuff allows the users to send their updates asynchronously
while ensuring privacy by storing the updates in a trusted execution
environment (TEE) enabled private buffer. TEEs, however, have limited memory
which limits the buffer size. Motivated by this limitation, we develop a
buffered asynchronous secure aggregation (BASecAgg) protocol that does not rely
on TEEs. The conventional secure aggregation protocols cannot be applied in the
buffered asynchronous setting since the buffer may have local models
corresponding to different rounds and hence the masks that the users use to
protect their models may not cancel out. BASecAgg addresses this challenge by
carefully designing the masks such that they cancel out even if they correspond
to different rounds. Our convergence analysis and experiments show that
BASecAgg almost has the same convergence guarantees as FedBuff without relying
on TEEs.

    

### [[2110.02178] MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer](http://arxiv.org/abs/2110.02178)


  Light-weight convolutional neural networks (CNNs) are the de-facto for mobile
vision tasks. Their spatial inductive biases allow them to learn
representations with fewer parameters across different vision tasks. However,
these networks are spatially local. To learn global representations,
self-attention-based vision trans-formers (ViTs) have been adopted. Unlike
CNNs, ViTs are heavy-weight. In this paper, we ask the following question: is
it possible to combine the strengths of CNNs and ViTs to build a light-weight
and low latency network for mobile vision tasks? Towards this end, we introduce
MobileViT, a light-weight and general-purpose vision transformer for mobile
devices. MobileViT presents a different perspective for the global processing
of information with transformers, i.e., transformers as convolutions. Our
results show that MobileViT significantly outperforms CNN- and ViT-based
networks across different tasks and datasets. On the ImageNet-1k dataset,
MobileViT achieves top-1 accuracy of 78.4% with about 6 million parameters,
which is 3.2% and 6.2% more accurate than MobileNetv3 (CNN-based) and DeIT
(ViT-based) for a similar number of parameters. On the MS-COCO object detection
task, MobileViT is 5.7% more accurate than Mo-bileNetv3 for a similar number of
parameters.

    

### [[2110.02180] Noisy Feature Mixup](http://arxiv.org/abs/2110.02180)


  We introduce Noisy Feature Mixup (NFM), an inexpensive yet effective method
for data augmentation that combines the best of interpolation based training
and noise injection schemes. Rather than training with convex combinations of
pairs of examples and their labels, we use noise-perturbed convex combinations
of pairs of data points in both input and feature space. This method includes
mixup and manifold mixup as special cases, but it has additional advantages,
including better smoothing of decision boundaries and enabling improved model
robustness. We provide theory to understand this as well as the implicit
regularization effects of NFM. Our theory is supported by empirical results,
demonstrating the advantage of NFM, as compared to mixup and manifold mixup. We
show that residual networks and vision transformers trained with NFM have
favorable trade-offs between predictive accuracy on clean data and robustness
with respect to various types of data perturbation across a range of computer
vision benchmark datasets.

    

### [[2110.02195] TensorPlan and the Few Actions Lower Bound for Planning in MDPs under Linear Realizability of Optimal Value Functions](http://arxiv.org/abs/2110.02195)


  We consider the minimax query complexity of online planning with a generative
model in fixed-horizon Markov decision processes (MDPs) with linear function
approximation. Following recent works, we consider broad classes of problems
where either (i) the optimal value function $v^\star$ or (ii) the optimal
action-value function $q^\star$ lie in the linear span of some features; or
(iii) both $v^\star$ and $q^\star$ lie in the linear span when restricted to
the states reachable from the starting state. Recently, Weisz et al. (2021b)
showed that under (ii) the minimax query complexity of any planning algorithm
is at least exponential in the horizon $H$ or in the feature dimension $d$ when
the size $A$ of the action set can be chosen to be exponential in $\min(d,H)$.
On the other hand, for the setting (i), Weisz et al. (2021a) introduced
TensorPlan, a planner whose query cost is polynomial in all relevant quantities
when the number of actions is fixed. Among other things, these two works left
open the question whether polynomial query complexity is possible when $A$ is
subexponential in $min(d,H)$. In this paper we answer this question in the
negative: we show that an exponentially large lower bound holds when
$A=\Omega(\min(d^{1/4},H^{1/2}))$, under either (i), (ii) or (iii). In
particular, this implies a perhaps surprising exponential separation of query
complexity compared to the work of Du et al. (2021) who prove a polynomial
upper bound when (iii) holds for all states. Furthermore, we show that the
upper bound of TensorPlan can be extended to hold under (iii) and, for MDPs
with deterministic transitions and stochastic rewards, also under (ii).

    

### [[2110.02196] Transfer Learning U-Net Deep Learning for Lung Ultrasound Segmentation](http://arxiv.org/abs/2110.02196)


  Transfer learning (TL) for medical image segmentation helps deep learning
models achieve more accurate performances when there are scarce medical images.
This study focuses on completing segmentation of the ribs from lung ultrasound
images and finding the best TL technique with U-Net, a convolutional neural
network for precise and fast image segmentation. Two approaches of TL were
used, using a pre-trained VGG16 model to build the U-Net (V-Unet) and
pre-training U-Net network with grayscale natural salient object dataset
(X-Unet). Visual results and dice coefficients (DICE) of the models were
compared. X-Unet showed more accurate and artifact-free visual performances on
the actual mask prediction, despite its lower DICE than V-Unet. A
partial-frozen network fine-tuning (FT) technique was also applied to X-Unet to
compare results between different FT strategies, which FT all layers slightly
outperformed freezing part of the network. The effect of dataset sizes was also
evaluated, showing the importance of the combination between TL and data
augmentation.

    

### [[2110.02197] $Œî$-UQ: Accurate Uncertainty Quantification via Anchor Marginalization](http://arxiv.org/abs/2110.02197)


  We present $\Delta$-UQ -- a novel, general-purpose uncertainty estimator
using the concept of anchoring in predictive models. Anchoring works by first
transforming the input into a tuple consisting of an anchor point drawn from a
prior distribution, and a combination of the input sample with the anchor using
a pretext encoding scheme. This encoding is such that the original input can be
perfectly recovered from the tuple -- regardless of the choice of the anchor.
Therefore, any predictive model should be able to predict the target response
from the tuple alone (since it implicitly represents the input). Moreover, by
varying the anchors for a fixed sample, we can estimate uncertainty in the
prediction even using only a single predictive model. We find this uncertainty
is deeply connected to improper sampling of the input data, and inherent noise,
enabling us to estimate the total uncertainty in any system. With extensive
empirical studies on a variety of use-cases, we demonstrate that $\Delta$-UQ
outperforms several competitive baselines. Specifically, we study model
fitting, sequential model optimization, model based inversion in the regression
setting and out of distribution detection, & calibration under distribution
shifts for classification.

    

### [[2110.02200] Using Psuedolabels for training Sentiment Classifiers makes the model generalize better across datasets](http://arxiv.org/abs/2110.02200)


  The problem statement addressed in this work is : For a public sentiment
classification API, how can we set up a classifier that works well on different
types of data, having limited ability to annotate data from across domains. We
show that given a large amount of unannotated data from across different
domains and pseudolabels on this dataset generated by a classifier trained on a
small annotated dataset from one domain, we can train a sentiment classifier
that generalizes better across different datasets.

    

### [[2110.02206] Predicting Credit Risk for Unsecured Lending: A Machine Learning Approach](http://arxiv.org/abs/2110.02206)


  Since the 1990s, there have been significant advances in the technology space
and the e-Commerce area, leading to an exponential increase in demand for
cashless payment solutions. This has led to increased demand for credit cards,
bringing along with it the possibility of higher credit defaults and hence
higher delinquency rates, over a period of time. The purpose of this research
paper is to build a contemporary credit scoring model to forecast credit
defaults for unsecured lending (credit cards), by employing machine learning
techniques. As much of the customer payments data available to lenders, for
forecasting Credit defaults, is imbalanced (skewed), on account of a limited
subset of default instances, this poses a challenge for predictive modelling.
In this research, this challenge is addressed by deploying Synthetic Minority
Oversampling Technique (SMOTE), a proven technique to iron out such imbalances,
from a given dataset. On running the research dataset through seven different
machine learning models, the results indicate that the Light Gradient Boosting
Machine (LGBM) Classifier model outperforms the other six classification
techniques. Thus, our research indicates that the LGBM classifier model is
better equipped to deliver higher learning speeds, better efficiencies and
manage larger data volumes. We expect that deployment of this model will enable
better and timely prediction of credit defaults for decision-makers in
commercial lending institutions and banks.

    

### [[2110.02215] Real-Time Patient-Specific ECG Classification by 1D Self-Operational Neural Networks](http://arxiv.org/abs/2110.02215)


  Despite the proliferation of numerous deep learning methods proposed for
generic ECG classification and arrhythmia detection, compact systems with the
real-time ability and high accuracy for classifying patient-specific ECG are
still few. Particularly, the scarcity of patient-specific data poses an
ultimate challenge to any classifier. Recently, compact 1D Convolutional Neural
Networks (CNNs) have achieved the state-of-the-art performance level for the
accurate classification of ventricular and supraventricular ectopic beats.
However, several studies have demonstrated the fact that the learning
performance of the conventional CNNs is limited because they are homogenous
networks with a basic (linear) neuron model. In order to address this
deficiency and further boost the patient-specific ECG classification
performance, in this study, we propose 1D Self-organized Operational Neural
Networks (1D Self-ONNs). Due to its self-organization capability, Self-ONNs
have the utmost advantage and superiority over conventional ONNs where the
prior operator search within the operator set library to find the best possible
set of operators is entirely avoided. As the first study where 1D Self-ONNs are
ever proposed for a classification task, our results over the MIT-BIH
arrhythmia benchmark database demonstrate that 1D Self-ONNs can surpass 1D CNNs
with a significant margin while having a similar computational complexity.
Under AAMI recommendations and with minimal common training data used, over the
entire MIT-BIH dataset 1D Self-ONNs have achieved 98% and 99.04% average
accuracies, 76.6% and 93.7% average F1 scores on supra-ventricular and
ventricular ectopic beat (VEB) classifications, respectively, which is the
highest performance level ever reported.

    

### [[1608.03533] Sequence Graph Transform (SGT): A Feature Embedding Function for Sequence Data Mining](http://arxiv.org/abs/1608.03533)


  Sequence feature embedding is a challenging task due to the unstructuredness
of sequence, i.e., arbitrary strings of arbitrary length. Existing methods are
efficient in extracting short-term dependencies but typically suffer from
computation issues for the long-term. Sequence Graph Transform (SGT), a feature
embedding function, that can extract a varying amount of short- to long-term
dependencies without increasing the computation is proposed. SGT's properties
are analytically proved for interpretation under normal and uniform
distribution assumptions. SGT features yield significantly superior results in
sequence clustering and classification with higher accuracy and lower
computation as compared to the existing methods, including the state-of-the-art
sequence/string Kernels and LSTM.

    

### [[2005.07567] Accelerating drug repurposing for COVID-19 via modeling drug mechanism of action with large scale gene-expression profiles](http://arxiv.org/abs/2005.07567)


  The novel coronavirus disease, named COVID-19, emerged in China in December
2019, and has rapidly spread around the world. It is clearly urgent to fight
COVID-19 at global scale. The development of methods for identifying drug uses
based on phenotypic data can improve the efficiency of drug development.
However, there are still many difficulties in identifying drug applications
based on cell picture data. This work reported one state-of-the-art machine
learning method to identify drug uses based on the cell image features of 1024
drugs generated in the LINCS program. Because the multi-dimensional features of
the image are affected by non-experimental factors, the characteristics of
similar drugs vary greatly, and the current sample number is not enough to use
deep learning and other methods are used for learning optimization. As a
consequence, this study is based on the supervised ITML algorithm to convert
the characteristics of drugs. The results show that the characteristics of ITML
conversion are more conducive to the recognition of drug functions. The
analysis of feature conversion shows that different features play important
roles in identifying different drug functions. For the current COVID-19,
Chloroquine and Hydroxychloroquine achieve antiviral effects by inhibiting
endocytosis, etc., and were classified to the same community. And Clomiphene in
the same community inibited the entry of Ebola Virus, indicated a similar MoAs
that could be reflected by cell image.

    

### [[2005.14635] Machine learning methods to detect money laundering in the Bitcoin blockchain in the presence of label scarcity](http://arxiv.org/abs/2005.14635)


  Every year, criminals launder billions of dollars acquired from serious
felonies (e.g., terrorism, drug smuggling, or human trafficking) harming
countless people and economies. Cryptocurrencies, in particular, have developed
as a haven for money laundering activity. Machine Learning can be used to
detect these illicit patterns. However, labels are so scarce that traditional
supervised algorithms are inapplicable. Here, we address money laundering
detection assuming minimal access to labels. First, we show that existing
state-of-the-art solutions using unsupervised anomaly detection methods are
inadequate to detect the illicit patterns in a real Bitcoin transaction
dataset. Then, we show that our proposed active learning solution is capable of
matching the performance of a fully supervised baseline by using just 5\% of
the labels. This solution mimics a typical real-life situation in which a
limited number of labels can be acquired through manual annotation by experts.

    

### [[2006.01043] BadNL: Backdoor Attacks against NLP Models with Semantic-preserving Improvements](http://arxiv.org/abs/2006.01043)


  Deep neural networks (DNNs) have progressed rapidly during the past decade
and have been deployed in various real-world applications. Meanwhile, DNN
models have been shown to be vulnerable to security and privacy attacks. One
such attack that has attracted a great deal of attention recently is the
backdoor attack. Specifically, the adversary poisons the target model's
training set to mislead any input with an added secret trigger to a target
class.
Previous backdoor attacks predominantly focus on computer vision (CV)
applications, such as image classification. In this paper, we perform a
systematic investigation of backdoor attack on NLP models, and propose BadNL, a
general NLP backdoor attack framework including novel attack methods.
Specifically, we propose three methods to construct triggers, namely BadChar,
BadWord, and BadSentence, including basic and semantic-preserving variants. Our
attacks achieve an almost perfect attack success rate with a negligible effect
on the original model's utility. For instance, using the BadChar, our backdoor
attack achieves a 98.9% attack success rate with yielding a utility improvement
of 1.5% on the SST-5 dataset when only poisoning 3% of the original set.
Moreover, we conduct a user study to prove that our triggers can well preserve
the semantics from humans perspective.

    

### [[2006.10571] Likelihood-Free Inference with Deep Gaussian Processes](http://arxiv.org/abs/2006.10571)


  In recent years, surrogate models have been successfully used in
likelihood-free inference to decrease the number of simulator evaluations. The
current state-of-the-art performance for this task has been achieved by
Bayesian Optimization with Gaussian Processes (GPs). While this combination
works well for unimodal target distributions, it is restricting the flexibility
and applicability of Bayesian Optimization for accelerating likelihood-free
inference more generally. We address this problem by proposing a Deep Gaussian
Process (DGP) surrogate model that can handle more irregularly behaved target
distributions. Our experiments show how DGPs can outperform GPs on objective
functions with multimodal distributions and maintain a comparable performance
in unimodal cases. This confirms that DGPs as surrogate models can extend the
applicability of Bayesian Optimization for likelihood-free inference (BOLFI),
while adding computational overhead that remains negligible for computationally
intensive simulators.

    

### [[2007.12778] CD-split and HPD-split: efficient conformal regions in high dimensions](http://arxiv.org/abs/2007.12778)


  Conformal methods create prediction bands that control average coverage
assuming solely i.i.d. data. Although the literature has mostly focused on
prediction intervals, more general regions can often better represent
uncertainty. For instance, a bimodal target is better represented by the union
of two intervals. Such prediction regions are obtained by CD-split , which
combines the split method and a data-driven partition of the feature space
which scales to high dimensions. CD-split however contains many tuning
parameters, and their role is not clear. In this paper, we provide new insights
on CD-split by exploring its theoretical properties. In particular, we show
that CD-split converges asymptotically to the oracle highest predictive density
set and satisfies local and asymptotic conditional validity. We also present
simulations that show how to tune CD-split. Finally, we introduce HPD-split, a
variation of CD-split that requires less tuning, and show that it shares the
same theoretical guarantees as CD-split. In a wide variety of our simulations,
CD-split and HPD-split have better conditional coverage and yield smaller
prediction regions than other methods.

    

### [[2007.13834] Adaptive LiDAR Sampling and Depth Completion using Ensemble Variance](http://arxiv.org/abs/2007.13834)


  This work considers the problem of depth completion, with or without image
data, where an algorithm may measure the depth of a prescribed limited number
of pixels. The algorithmic challenge is to choose pixel positions strategically
and dynamically to maximally reduce overall depth estimation error. This
setting is realized in daytime or nighttime depth completion for autonomous
vehicles with a programmable LiDAR. Our method uses an ensemble of predictors
to define a sampling probability over pixels. This probability is proportional
to the variance of the predictions of ensemble members, thus highlighting
pixels that are difficult to predict. By additionally proceeding in several
prediction phases, we effectively reduce redundant sampling of similar pixels.
Our ensemble-based method may be implemented using any depth-completion
learning algorithm, such as a state-of-the-art neural network, treated as a
black box. In particular, we also present a simple and effective Random
Forest-based algorithm, and similarly use its internal ensemble in our design.
We conduct experiments on the KITTI dataset, using the neural network algorithm
of Ma et al. and our Random Forest based learner for implementing our method.
The accuracy of both implementations exceeds the state of the art. Compared
with a random or grid sampling pattern, our method allows a reduction by a
factor of 4-10 in the number of measurements required to attain the same
accuracy.

    

### [[2008.02216] Fuzzy Jaccard Index: A robust comparison of ordered lists](http://arxiv.org/abs/2008.02216)


  We propose Fuzzy Jaccard Index (FUJI) -- a scale-invariant score for
assessment of the similarity between two ranked/ordered lists. FUJI improves
upon the Jaccard index by incorporating a membership function which takes into
account the particular ranks, thus producing both more stable and more accurate
similarity estimates. We provide theoretical insights into the properties of
the FUJI score as well as propose an efficient algorithm for computing it. We
also present empirical evidence of its performance on different synthetic
scenarios. Finally, we demonstrate its utility in a typical machine learning
setting -- comparing feature ranking lists relevant to a given machine learning
task. In real-life, and in particular high-dimensional domains, where only a
small percentage of the whole feature space might be relevant, a robust and
confident feature ranking leads to interpretable findings as well as efficient
computation and good predictive performance. In such cases, FUJI correctly
distinguishes between existing feature ranking approaches, while being more
robust and efficient than the benchmark similarity scores.

    

### [[2010.07893] MAP Propagation Algorithm: Faster Learning with a Team of Reinforcement Learning Agents](http://arxiv.org/abs/2010.07893)


  Nearly all state-of-the-art deep learning algorithms rely on error
backpropagation, which is generally regarded as biologically implausible. An
alternative way of training an artificial neural network is through treating
each unit in the network as a reinforcement learning agent, and thus the
network is considered as a team of agents. As such, all units can be trained by
REINFORCE, a local learning rule modulated by a global signal that is more
consistent with biologically observed forms of synaptic plasticity. Although
this learning rule follows the gradient of return in expectation, it suffers
from high variance and thus the low speed of learning, rendering it impractical
to train deep networks. We therefore propose a novel algorithm called MAP
propagation to reduce this variance significantly while retaining the local
property of the learning rule. Experiments demonstrated that MAP propagation
could solve common reinforcement learning tasks at a similar speed to
backpropagation when applied to an actor-critic network. Our work thus allows
for the broader application of the teams of agents in deep reinforcement
learning.

    

### [[2010.09080] Poisoned classifiers are not only backdoored, they are fundamentally broken](http://arxiv.org/abs/2010.09080)


  Under a commonly-studied backdoor poisoning attack against classification
models, an attacker adds a small trigger to a subset of the training data, such
that the presence of this trigger at test time causes the classifier to always
predict some target class. It is often implicitly assumed that the poisoned
classifier is vulnerable exclusively to the adversary who possesses the
trigger. In this paper, we show empirically that this view of backdoored
classifiers is incorrect. We describe a new threat model for poisoned
classifier, where one without knowledge of the original trigger, would want to
control the poisoned classifier. Under this threat model, we propose a
test-time, human-in-the-loop attack method to generate multiple effective
alternative triggers without access to the initial backdoor and the training
data. We construct these alternative triggers by first generating adversarial
examples for a smoothed version of the classifier, created with a procedure
called Denoised Smoothing, and then extracting colors or cropped portions of
smoothed adversarial images with human interaction. We demonstrate the
effectiveness of our attack through extensive experiments on high-resolution
datasets: ImageNet and TrojAI. We also compare our approach to previous work on
modeling trigger distributions and find that our method are more scalable and
efficient in generating effective triggers. Last, we include a user study which
demonstrates that our method allows users to easily determine the existence of
such backdoors in existing poisoned classifiers. Thus, we argue that there is
no such thing as a secret backdoor in poisoned classifiers: poisoning a
classifier invites attacks not just by the party that possesses the trigger,
but from anyone with access to the classifier.

    

### [[2010.12190] Towards Robust Neural Networks via Orthogonal Diversity](http://arxiv.org/abs/2010.12190)


  Deep Neural Networks (DNNs) are vulnerable to invisible perturbations on the
images generated by adversarial attacks, which raises researches on the
adversarial robustness of DNNs. A series of methods represented by the
\textit{adversarial training} and its variants have proved the most practical
techniques in enhancing the DNN robustness. Generally, adversarial training
focuses on enriching the training data by involving perturbed data into clean
data. Despite of the efficiency on defending specific attacks, adversarial
training essentially benefits from the data augmentation, but does not
contribute to the robustness of DNN itself, and usually suffers accuracy drop
on clean data as well as inefficiency on unknown attacks. Towards the
robustness of DNN itself, we propose a novel defense that aims at augmenting
the model in order to learn features adaptive to diverse inputs, including
adversarial examples. Specifically, we introduce multiple paths to augment the
network, and impose orthogonality constraint on these paths. In addition, a
margin-maximization loss is designed to further boost DIversity via
Orthogonality (DIO). Extensive empirical results on various data sets,
architectures, and attacks demonstrate the robustness of DIO: it does not need
any adversarial example and yet achieves greater robustness compared with
state-of-the-art adversarial training methods.

    

### [[2011.04728] Similarity-Based Clustering for Enhancing Image Classification Architectures](http://arxiv.org/abs/2011.04728)


  Convolutional networks are at the center of best in class computer vision
applications for a wide assortment of undertakings. Since 2014, profound amount
of work began to make better convolutional architectures, yielding generous
additions in different benchmarks. Albeit expanded model size and computational
cost will, in general, mean prompt quality increases for most undertakings but,
the architectures now need to have some additional information to increase the
performance. We show empirical evidence that with the amalgamation of
content-based image similarity and deep learning models, we can provide the
flow of information which can be used in making clustered learning possible. We
show how parallel training of sub-dataset clusters not only reduces the cost of
computation but also increases the benchmark accuracies by 5-11 percent.

    

### [[2011.10006] Improved rates for prediction and identification of partially observed linear dynamical systems](http://arxiv.org/abs/2011.10006)


  Identification of a linear time-invariant dynamical system from partial
observations is a fundamental problem in control theory. Particularly
challenging are systems exhibiting long-term memory. A natural question is how
learn such systems with non-asymptotic statistical rates depending on the
inherent dimensionality (order) $d$ of the system, rather than on the possibly
much larger memory length. We propose an algorithm that given a single
trajectory of length $T$ with gaussian observation noise, learns the system
with a near-optimal rate of $\widetilde O\left(\sqrt\frac{d}{T}\right)$ in
$\mathcal{H}_2$ error, with only logarithmic, rather than polynomial dependence
on memory length. We also give bounds under process noise and improved bounds
for learning a realization of the system. Our algorithm is based on multi-scale
low-rank approximation: SVD applied to Hankel matrices of geometrically
increasing sizes. Our analysis relies on careful application of concentration
bounds on the Fourier domain -- we give sharper concentration bounds for sample
covariance of correlated inputs and for $\mathcal H_\infty$ norm estimation,
which may be of independent interest.

    

### [[2012.04351] Data Dependent Randomized Smoothing](http://arxiv.org/abs/2012.04351)


  Randomized smoothing is a recent technique that achieves state-of-art
performance in training certifiably robust deep neural networks. While the
smoothing family of distributions is often connected to the choice of the norm
used for certification, the parameters of these distributions are always set as
global hyper parameters independent from the input data on which a network is
certified. In this work, we revisit Gaussian randomized smoothing and show that
the variance of the Gaussian distribution can be optimized at each input so as
to maximize the certification radius for the construction of the smooth
classifier. We also propose a simple memory-based approach to certifying the
resultant smooth classifier. This new approach is generic, parameter-free, and
easy to implement. In fact, we show that our data dependent framework can be
seamlessly incorporated into 3 randomized smoothing approaches, leading to
consistent improved certified accuracy. When this framework is used in the
training routine of these approaches followed by a data dependent
certification, we achieve 9% and 6% improvement over the certified accuracy of
the strongest baseline for a radius of 0.5 on CIFAR10 and ImageNet.

    

### [[2102.03236] Exact Optimization of Conformal Predictors via Incremental and Decremental Learning](http://arxiv.org/abs/2102.03236)


  Conformal Predictors (CP) are wrappers around ML models, providing error
guarantees under weak assumptions on the data distribution. They are suitable
for a wide range of problems, from classification and regression to anomaly
detection. Unfortunately, their very high computational complexity limits their
applicability to large datasets. In this work, we show that it is possible to
speed up a CP classifier considerably, by studying it in conjunction with the
underlying ML method, and by exploiting incremental&decremental learning. For
methods such as k-NN, KDE, and kernel LS-SVM, our approach reduces the running
time by one order of magnitude, whilst producing exact solutions. With similar
ideas, we also achieve a linear speed up for the harder case of bootstrapping.
Finally, we extend these techniques to improve upon an optimization of k-NN CP
for regression. We evaluate our findings empirically, and discuss when methods
are suitable for CP optimization.

    

### [[2102.03858] Damage detection using in-domain and cross-domain transfer learning](http://arxiv.org/abs/2102.03858)


  We investigate the capabilities of transfer learning in the area of
structural health monitoring. In particular, we are interested in damage
detection for concrete structures. Typical image datasets for such problems are
relatively small, calling for the transfer of learned representation from a
related large-scale dataset. Past efforts of damage detection using images have
mainly considered cross-domain transfer learning approaches using pre-trained
IMAGENET models that are subsequently fine-tuned for the target task. However,
there are rising concerns about the generalizability of IMAGENET
representations for specific target domains, such as for visual inspection and
medical imaging. We, therefore, evaluate a combination of in-domain and
cross-domain transfer learning strategies for damage detection in bridges. We
perform comprehensive comparisons to study the impact of cross-domain and
in-domain transfer, with various initialization strategies, using six publicly
available visual inspection datasets. The pre-trained models are also evaluated
for their ability to cope with the extremely low-data regime. We show that the
combination of cross-domain and in-domain transfer persistently shows superior
performance specially with tiny datasets. Likewise, we also provide visual
explanations of predictive models to enable algorithmic transparency and
provide insights to experts about the intrinsic decision logic of typically
black-box deep models.

    

### [[2102.05526] Dynamic $Œ≤$-VAEs for quantifying biodiversity by clustering optically recorded insect signals](http://arxiv.org/abs/2102.05526)


  While insects are the largest and most diverse group of terrestrial animals,
constituting ca. 80% of all known species, they are difficult to study due to
their small size and similarity between species. Conventional monitoring
techniques depend on time consuming trapping methods and tedious
microscope-based work by skilled experts in order to identify the caught insect
specimen at species, or even family level. Researchers and policy makers are in
urgent need of a scalable monitoring tool in order to conserve biodiversity and
secure human food production due to the rapid decline in insect numbers.
In order to improve upon existing insect clustering methods, we propose an
adaptive variant of the variational autoencoder (VAE) which is capable of
clustering data by phylogenetic groups. The proposed dynamic beta-VAE
dynamically adapts the scaling of the reconstruction and regularization loss
terms (beta value) yielding useful latent representations of the input data. We
demonstrate the usefulness of the dynamic beta-VAE on optically recorded insect
signals from regions of southern Scandinavia to cluster unlabelled targets into
possible species. We also demonstrate improved clustering performance in a
semi-supervised setting using a small subset of labelled data. These
experimental results, in both unsupervised- and semi-supervised settings, with
the dynamic beta-VAE are promising and, in the near future, can be deployed to
monitor insects and conserve the rapidly declining insect biodiversity.

    

### [[2102.06247] Sample-Optimal PAC Learning of Halfspaces with Malicious Noise](http://arxiv.org/abs/2102.06247)


  We study efficient PAC learning of homogeneous halfspaces in $\mathbb{R}^d$
in the presence of malicious noise of Valiant (1985). This is a challenging
noise model and only until recently has near-optimal noise tolerance bound been
established under the mild condition that the unlabeled data distribution is
isotropic log-concave. However, it remains unsettled how to obtain the optimal
sample complexity simultaneously. In this work, we present a new analysis for
the algorithm of Awasthi et al. (2017) and show that it essentially achieves
the near-optimal sample complexity bound of $\tilde{O}(d)$, improving the best
known result of $\tilde{O}(d^2)$. Our main ingredient is a novel incorporation
of a matrix Chernoff-type inequality to bound the spectrum of an empirical
covariance matrix for well-behaved distributions, in conjunction with a careful
exploration of the localization schemes of Awasthi et al. (2017). We further
extend the algorithm and analysis to the more general and stronger nasty noise
model of Bshouty et al. (2002), showing that it is still possible to achieve
near-optimal noise tolerance and sample complexity in polynomial time.

    

### [[2102.07325] Cross-modal Adversarial Reprogramming](http://arxiv.org/abs/2102.07325)


  With the abundance of large-scale deep learning models, it has become
possible to repurpose pre-trained networks for new tasks. Recent works on
adversarial reprogramming have shown that it is possible to repurpose neural
networks for alternate tasks without modifying the network architecture or
parameters. However these works only consider original and target tasks within
the same data domain. In this work, we broaden the scope of adversarial
reprogramming beyond the data modality of the original task. We analyze the
feasibility of adversarially repurposing image classification neural networks
for Natural Language Processing (NLP) and other sequence classification tasks.
We design an efficient adversarial program that maps a sequence of discrete
tokens into an image which can be classified to the desired class by an image
classification model. We demonstrate that by using highly efficient adversarial
programs, we can reprogram image classifiers to achieve competitive performance
on a variety of text and sequence classification benchmarks without retraining
the network.

    

### [[2102.10802] PrivateMail: Supervised Manifold Learning of Deep Features With Differential Privacy for Image Retrieval](http://arxiv.org/abs/2102.10802)


  Differential Privacy offers strong guarantees such as immutable privacy under
post processing. Thus it is often looked to as a solution to learning on
scattered and isolated data. This work focuses on supervised manifold learning,
a paradigm that can generate fine-tuned manifolds for a target use case. Our
contributions are two fold. 1) We present a novel differentially private method
\textit{PrivateMail} for supervised manifold learning, the first of its kind to
our knowledge. 2) We provide a novel private geometric embedding scheme for our
experimental use case. We experiment on private "content based image retrieval"
- embedding and querying the nearest neighbors of images in a private manner -
and show extensive privacy-utility tradeoff results, as well as the
computational efficiency and practicality of our methods.

    

### [[2103.00697] Heterogeneity for the Win: One-Shot Federated Clustering](http://arxiv.org/abs/2103.00697)


  In this work, we explore the unique challenges -- and opportunities -- of
unsupervised federated learning (FL). We develop and analyze a one-shot
federated clustering scheme, $k$-FED, based on the widely-used Lloyd's method
for $k$-means clustering. In contrast to many supervised problems, we show that
the issue of statistical heterogeneity in federated networks can in fact
benefit our analysis. We analyse $k$-FED under a center separation assumption
and compare it to the best known requirements of its centralized counterpart.
Our analysis shows that in heterogeneous regimes where the number of clusters
per device $(k')$ is smaller than the total number of clusters over the network
$k$, $(k'\le \sqrt{k})$, we can use heterogeneity to our advantage --
significantly weakening the cluster separation requirements for $k$-FED. From a
practical viewpoint, $k$-FED also has many desirable properties: it requires
only round of communication, can run asynchronously, and can handle partial
participation or node/network failures. We motivate our analysis with
experiments on common FL benchmarks, and highlight the practical utility of
one-shot clustering through use-cases in personalized FL and device sampling.

    

### [[2103.01678] Wasserstein GANs Work Because They Fail (to Approximate the Wasserstein Distance)](http://arxiv.org/abs/2103.01678)


  Wasserstein GANs are based on the idea of minimising the Wasserstein distance
between a real and a generated distribution. We provide an in-depth
mathematical analysis of differences between the theoretical setup and the
reality of training Wasserstein GANs. In this work, we gather both theoretical
and empirical evidence that the WGAN loss is not a meaningful approximation of
the Wasserstein distance. Moreover, we argue that the Wasserstein distance is
not even a desirable loss function for deep generative models, and conclude
that the success of Wasserstein GANs can in truth be attributed to a failure to
approximate the Wasserstein distance.

    

### [[2103.07162] Is BERT a Cross-Disciplinary Knowledge Learner? A Surprising Finding of Pre-trained Models' Transferability](http://arxiv.org/abs/2103.07162)


  This paper investigates whether the power of the models pre-trained on text
data, such as BERT, can be transferred to general token sequence classification
applications. To verify pre-trained models' transferability, we test the
pre-trained models on text classification tasks with meanings of tokens
mismatches, and real-world non-text token sequence classification data,
including amino acid, DNA, and music. We find that even on non-text data, the
models pre-trained on text converge faster, perform better than the randomly
initialized models, and only slightly worse than the models using task-specific
knowledge. We also find that the representations of the text and non-text
pre-trained models share non-trivial similarities.

    

### [[2104.03490] Joint Optimization of Communications and Federated Learning Over the Air](http://arxiv.org/abs/2104.03490)


  Federated learning (FL) is an attractive paradigm for making use of rich
distributed data while protecting data privacy. Nonetheless, nonideal
communication links and limited transmission resources may hinder the
implementation of fast and accurate FL. In this paper, we study joint
optimization of communications and FL based on analog aggregation transmission
in realistic wireless networks. We first derive closed-form expressions for the
expected convergence rate of FL over the air, which theoretically quantify the
impact of analog aggregation on FL. Based on the analytical results, we develop
a joint optimization model for accurate FL implementation, which allows a
parameter server to select a subset of workers and determine an appropriate
power scaling factor. Since the practical setting of FL over the air encounters
unobservable parameters, we reformulate the joint optimization of worker
selection and power allocation using controlled approximation. Finally, we
efficiently solve the resulting mixed-integer programming problem via a simple
yet optimal finite-set search method by reducing the search space. Simulation
results show that the proposed solutions developed for realistic wireless
analog channels outperform a benchmark method, and achieve comparable
performance of the ideal case where FL is implemented over noise-free wireless
channels.

    

### [[2104.04298] On Architectures and Training for Raw Waveform Feature Extraction in ASR](http://arxiv.org/abs/2104.04298)


  With the success of neural network based modeling in automatic speech
recognition (ASR), many studies investigated acoustic modeling and learning of
feature extractors directly based on the raw waveform. Recently, one line of
research has focused on unsupervised pre-training of feature extractors on
audio-only data to improve downstream ASR performance. In this work, we
investigate the usefulness of one of these front-end frameworks, namely
wav2vec, in a setting without additional untranscribed data for hybrid ASR
systems. We compare this framework both to the manually defined standard
Gammatone feature set, as well as to features extracted as part of the acoustic
model of an ASR system trained supervised. We study the benefits of using the
pre-trained feature extractor and explore how to additionally exploit an
existing acoustic model trained with different features. Finally, we
systematically examine combinations of the described features in order to
further advance the performance.

    

### [[2104.10403] Model-aided Deep Reinforcement Learning for Sample-efficient UAV Trajectory Design in IoT Networks](http://arxiv.org/abs/2104.10403)


  Deep Reinforcement Learning (DRL) is gaining attention as a potential
approach to design trajectories for autonomous unmanned aerial vehicles (UAV)
used as flying access points in the context of cellular or Internet of Things
(IoT) connectivity. DRL solutions offer the advantage of on-the-go learning
hence relying on very little prior contextual information. A corresponding
drawback however lies in the need for many learning episodes which severely
restricts the applicability of such approach in real-world time- and
energy-constrained missions. Here, we propose a model-aided deep Q-learning
approach that, in contrast to previous work, considerably reduces the need for
extensive training data samples, while still achieving the overarching goal of
DRL, i.e to guide a battery-limited UAV on an efficient data harvesting
trajectory, without prior knowledge of wireless channel characteristics and
limited knowledge of wireless node locations. The key idea consists in using a
small subset of nodes as anchors (i.e. with known location) and learning a
model of the propagation environment while implicitly estimating the positions
of regular nodes. Interaction with the model allows us to train a deep
Q-network (DQN) to approximate the optimal UAV control policy. We show that in
comparison with standard DRL approaches, the proposed model-aided approach
requires at least one order of magnitude less training data samples to reach
identical data collection performance, hence offering a first step towards
making DRL a viable solution to the problem.

    

### [[2104.13414] Traffic signal prediction on transportation networks using spatio-temporal correlations on graphs](http://arxiv.org/abs/2104.13414)


  Multivariate time series forecasting poses challenges as the variables are
intertwined in time and space, like in the case of traffic signals. Defining
signals on graphs relaxes such complexities by representing the evolution of
signals over a space using relevant graph kernels such as the heat diffusion
kernel. However, this kernel alone does not fully capture the actual dynamics
of the data as it only relies on the graph structure. The gap can be filled by
combining the graph kernel representation with data-driven models that utilize
historical data. This paper proposes a traffic propagation model that merges
multiple heat diffusion kernels into a data-driven prediction model to forecast
traffic signals. We optimize the model parameters using Bayesian inference to
minimize the prediction errors and, consequently, determine the mixing ratio of
the two approaches. Such mixing ratio strongly depends on training data size
and data anomalies, which typically correspond to the peak hours for traffic
data. The proposed model demonstrates prediction accuracy comparable to that of
the state-of-the-art deep neural networks with lower computational effort. It
notably achieves excellent performance for long-term prediction through the
inheritance of periodicity modeling in data-driven models.

    

### [[2104.13492] An Energy-Based View of Graph Neural Networks](http://arxiv.org/abs/2104.13492)


  Graph neural networks are a popular variant of neural networks that work with
graph-structured data. In this work, we consider combining graph neural
networks with the energy-based view of Grathwohl et al. (2019) with the aim of
obtaining a more robust classifier. We successfully implement this framework by
proposing a novel method to ensure generation over features as well as the
adjacency matrix and evaluate our method against the standard graph
convolutional network (GCN) architecture (Kipf & Welling (2016)). Our approach
obtains comparable discriminative performance while improving robustness,
opening promising new directions for future research for energy-based graph
neural networks.

    

### [[2105.12238] AutoMate: A Dataset and Learning Approach for Automatic Mating of CAD Assemblies](http://arxiv.org/abs/2105.12238)


  Assembly modeling is a core task of computer aided design (CAD), comprising
around one third of the work in a CAD workflow. Optimizing this process
therefore represents a huge opportunity in the design of a CAD system, but
current research of assembly based modeling is not directly applicable to
modern CAD systems because it eschews the dominant data structure of modern
CAD: parametric boundary representations (BREPs). CAD assembly modeling defines
assemblies as a system of pairwise constraints, called mates, between parts,
which are defined relative to BREP topology rather than in world coordinates
common to existing work. We propose SB-GCN, a representation learning scheme on
BREPs that retains the topological structure of parts, and use these learned
representations to predict CAD type mates. To train our system, we compiled the
first large scale dataset of BREP CAD assemblies, which we are releasing along
with benchmark mate prediction tasks. Finally, we demonstrate the compatibility
of our model with an existing commercial CAD system by building a tool that
assists users in mate creation by suggesting mate completions, with 72.2%
accuracy.

    

### [[2106.04886] Fully differentiable model discovery](http://arxiv.org/abs/2106.04886)


  Model discovery aims at autonomously discovering differential equations
underlying a dataset. Approaches based on Physics Informed Neural Networks
(PINNs) have shown great promise, but a fully-differentiable model which
explicitly learns the equation has remained elusive. In this paper we propose
such an approach by integrating neural network-based surrogates with Sparse
Bayesian Learning (SBL). This combination yields a robust model discovery
algorithm, which we showcase on various datasets. We then identify a connection
with multitask learning, and build on it to construct a Physics Informed
Normalizing Flow (PINF). We present a proof-of-concept using a PINF to directly
learn a density model from single particle data. Our work expands PINNs to
various types of neural network architectures, and connects neural
network-based surrogates to the rich field of Bayesian parameter inference.

    

### [[2106.05763] A Deep Variational Approach to Clustering Survival Data](http://arxiv.org/abs/2106.05763)


  In this work, we study the problem of clustering survival data $-$ a
challenging and so far under-explored task. We introduce a novel
semi-supervised probabilistic approach to cluster survival data by leveraging
recent advances in stochastic gradient variational inference. In contrast to
previous work, our proposed method employs a deep generative model to uncover
the underlying distribution of both the explanatory variables and censored
survival times. We compare our model to the related work on clustering and
mixture models for survival data in comprehensive experiments on a wide range
of synthetic, semi-synthetic, and real-world datasets, including medical
imaging data. Our method performs better at identifying clusters and is
competitive at predicting survival times. Relying on novel generative
assumptions, the proposed model offers a holistic perspective on clustering
survival data and holds a promise of discovering subpopulations whose survival
is regulated by different generative mechanisms.

    

### [[2106.08812] Costs and Benefits of Fair Regression](http://arxiv.org/abs/2106.08812)


  Real-world applications of machine learning tools in high-stakes domains are
often regulated to be fair, in the sense that the predicted target should
satisfy some quantitative notion of parity with respect to a protected
attribute. However, the exact tradeoff between fairness and accuracy with a
real-valued target is not entirely clear. In this paper, we characterize the
inherent tradeoff between statistical parity and accuracy in the regression
setting by providing a lower bound on the error of any fair regressor. Our
lower bound is sharp, algorithm-independent, and admits a simple
interpretation: when the moments of the target differ between groups, any fair
algorithm has to make an error on at least one of the groups. We further extend
this result to give a lower bound on the joint error of any (approximately)
fair algorithm, using the Wasserstein distance to measure the quality of the
approximation. With our novel lower bound, we also show that the price paid by
a fair regressor that does not take the protected attribute as input is less
than that of a fair regressor with explicit access to the protected attribute.
On the upside, we establish the first connection between individual fairness,
accuracy parity, and the Wasserstein distance by showing that if a regressor is
individually fair, it also approximately verifies the accuracy parity, where
the gap is given by the Wasserstein distance between the two groups. Inspired
by our theoretical results, we develop a practical algorithm for fair
regression through the lens of representation learning, and conduct experiments
on a real-world dataset to corroborate our findings.

    

### [[2106.11297] TokenLearner: What Can 8 Learned Tokens Do for Images and Videos?](http://arxiv.org/abs/2106.11297)


  In this paper, we introduce a novel visual representation learning which
relies on a handful of adaptively learned tokens, and which is applicable to
both image and video understanding tasks. Instead of relying on hand-designed
splitting strategies to obtain visual tokens and processing a large number of
densely sampled patches for attention, our approach learns to mine important
tokens in visual data. This results in efficiently and effectively finding a
few important visual tokens and enables modeling of pairwise attention between
such tokens, over a longer temporal horizon for videos, or the spatial content
in images. Our experiments demonstrate strong performance on several
challenging benchmarks for both image and video recognition tasks. Importantly,
due to our tokens being adaptive, we accomplish competitive results at
significantly reduced compute amount. We obtain comparable results to the
state-of-the-arts on ImageNet while being computationally more efficient. We
establish new state-of-the-arts on multiple video datasets, including
Kinetics-400, Kinetics-600, Charades, and AViD.

    

### [[2106.11936] Sparsistent Model Discovery](http://arxiv.org/abs/2106.11936)


  Discovering the partial differential equations underlying spatio-temporal
datasets from very limited and highly noisy observations is of paramount
interest in many scientific fields. However, it remains an open question to
know when model discovery algorithms based on sparse regression can actually
recover the underlying physical processes. In this work, we show the design
matrices used to infer the equations by sparse regression can violate the
irrepresentability condition (IRC) of the Lasso, even when derived from
analytical PDE solutions (i.e. without additional noise). Sparse regression
techniques which can recover the true underlying model under violated IRC
conditions are therefore required, leading to the introduction of the
randomised adaptive Lasso. We show once the latter is integrated within the
deep learning model discovery framework DeepMod, a wide variety of nonlinear
and chaotic canonical PDEs can be recovered: (1) up to $\mathcal{O}(2)$ higher
noise-to-sample ratios than state-of-the-art algorithms, (2) with a single set
of hyperparameters, which paves the road towards truly automated model
discovery.

    

### [[2106.12622] The Stereotyping Problem in Collaboratively Filtered Recommender Systems](http://arxiv.org/abs/2106.12622)


  Recommender systems play a crucial role in mediating our access to online
information. We show that such algorithms induce a particular kind of
stereotyping: if preferences for a set of items are anti-correlated in the
general user population, then those items may not be recommended together to a
user, regardless of that user's preferences and rating history. First, we
introduce a notion of joint accessibility, which measures the extent to which a
set of items can jointly be accessed by users. We then study joint
accessibility under the standard factorization-based collaborative filtering
framework, and provide theoretical necessary and sufficient conditions when
joint accessibility is violated. Moreover, we show that these conditions can
easily be violated when the users are represented by a single feature vector.
To improve joint accessibility, we further propose an alternative modelling
fix, which is designed to capture the diverse multiple interests of each user
using a multi-vector representation. We conduct extensive experiments on real
and simulated datasets, demonstrating the stereotyping problem with standard
single-vector matrix factorization models.

    

### [[2106.13997] The Feasibility and Inevitability of Stealth Attacks](http://arxiv.org/abs/2106.13997)


  We develop and study new adversarial perturbations that enable an attacker to
gain control over decisions in generic Artificial Intelligence (AI) systems
including deep learning neural networks. In contrast to adversarial data
modification, the attack mechanism we consider here involves alterations to the
AI system itself. Such a stealth attack could be conducted by a mischievous,
corrupt or disgruntled member of a software development team. It could also be
made by those wishing to exploit a "democratization of AI" agenda, where
network architectures and trained parameter sets are shared publicly. Building
on work by [Tyukin et al., International Joint Conference on Neural Networks,
2020], we develop a range of new implementable attack strategies with
accompanying analysis, showing that with high probability a stealth attack can
be made transparent, in the sense that system performance is unchanged on a
fixed validation set which is unknown to the attacker, while evoking any
desired output on a trigger input of interest. The attacker only needs to have
estimates of the size of the validation set and the spread of the AI's relevant
latent space. In the case of deep learning neural networks, we show that a one
neuron attack is possible - a modification to the weights and bias associated
with a single neuron - revealing a vulnerability arising from
over-parameterization. We illustrate these concepts in a realistic setting.
Guided by the theory and computational results, we also propose strategies to
guard against stealth attacks.

    

### [[2107.02173] Is Automated Topic Model Evaluation Broken?: The Incoherence of Coherence](http://arxiv.org/abs/2107.02173)


  Topic model evaluation, like evaluation of other unsupervised methods, can be
contentious. However, the field has coalesced around automated estimates of
topic coherence, which rely on the frequency of word co-occurrences in a
reference corpus. Recent models relying on neural components surpass classical
topic models according to these metrics. At the same time, unlike classical
models, the practice of neural topic model evaluation suffers from a validation
gap: automatic coherence for neural models has not been validated using human
experimentation. In addition, as we show via a meta-analysis of topic modeling
literature, there is a substantial standardization gap in the use of automated
topic modeling benchmarks. We address both the standardization gap and the
validation gap. Using two of the most widely used topic model evaluation
datasets, we assess a dominant classical model and two state-of-the-art neural
models in a systematic, clearly documented, reproducible way. We use automatic
coherence along with the two most widely accepted human judgment tasks, namely,
topic rating and word intrusion. Automated evaluation will declare one model
significantly different from another when corresponding human evaluations do
not, calling into question the validity of fully automatic evaluations
independent of human judgments.

    

### [[2107.05605] Interpretable Mammographic Image Classification using Case-Based Reasoning and Deep Learning](http://arxiv.org/abs/2107.05605)


  When we deploy machine learning models in high-stakes medical settings, we
must ensure these models make accurate predictions that are consistent with
known medical science. Inherently interpretable networks address this need by
explaining the rationale behind each decision while maintaining equal or higher
accuracy compared to black-box models. In this work, we present a novel
interpretable neural network algorithm that uses case-based reasoning for
mammography. Designed to aid a radiologist in their decisions, our network
presents both a prediction of malignancy and an explanation of that prediction
using known medical features. In order to yield helpful explanations, the
network is designed to mimic the reasoning processes of a radiologist: our
network first detects the clinically relevant semantic features of each image
by comparing each new image with a learned set of prototypical image parts from
the training images, then uses those clinical features to predict malignancy.
Compared to other methods, our model detects clinical features (mass margins)
with equal or higher accuracy, provides a more detailed explanation of its
prediction, and is better able to differentiate the classification-relevant
parts of the image.

    

### [[2110.01331] Analysis of the relation between smartphone usage changes during the COVID-19 pandemic and usage preferences on apps](http://arxiv.org/abs/2110.01331)


  Since the World Health Organization announced the COVID-19 pandemic in March
2020, curbing the spread of the virus has become an international priority. It
has greatly affected people's lifestyles. In this article, we observe and
analyze the impact of the pandemic on people's lives using changes in
smartphone application usage. First, through observing the daily usage change
trends of all users during the pandemic, we can understand and analyze the
effects of restrictive measures and policies during the pandemic on people's
lives. In addition, it is also helpful for the government and health
departments to take more appropriate restrictive measures in the case of future
pandemics. Second, we defined the usage change features and found 9 different
usage change patterns during the pandemic according to clusters of users and
show the diversity of daily usage changes. It helps to understand and analyze
the different impacts of the pandemic and restrictive measures on different
types of people in more detail. Finally, according to prediction models, we
discover the main related factors of each usage change type from user
preferences and demographic information. It helps to predict changes in
smartphone activity during future pandemics or when other restrictive measures
are implemented, which may become a new indicator to judge and manage the risks
of measures or events.

    

### [[2110.01548] Uncertainty-Based Offline Reinforcement Learning with Diversified Q-Ensemble](http://arxiv.org/abs/2110.01548)


  Offline reinforcement learning (offline RL), which aims to find an optimal
policy from a previously collected static dataset, bears algorithmic
difficulties due to function approximation errors from out-of-distribution
(OOD) data points. To this end, offline RL algorithms adopt either a constraint
or a penalty term that explicitly guides the policy to stay close to the given
dataset. However, prior methods typically require accurate estimation of the
behavior policy or sampling from OOD data points, which themselves can be a
non-trivial problem. Moreover, these methods under-utilize the generalization
ability of deep neural networks and often fall into suboptimal solutions too
close to the given dataset. In this work, we propose an uncertainty-based
offline RL method that takes into account the confidence of the Q-value
prediction and does not require any estimation or sampling of the data
distribution. We show that the clipped Q-learning, a technique widely used in
online RL, can be leveraged to successfully penalize OOD data points with high
prediction uncertainties. Surprisingly, we find that it is possible to
substantially outperform existing offline RL methods on various tasks by simply
increasing the number of Q-networks along with the clipped Q-learning. Based on
this observation, we propose an ensemble-diversified actor-critic algorithm
that reduces the number of required ensemble networks down to a tenth compared
to the naive ensemble while achieving state-of-the-art performance on most of
the D4RL benchmarks considered.

    

### [[2110.01563] A Modified Q-Learning Algorithm for Rate-Profiling of Polarization Adjusted Convolutional (PAC) Codes](http://arxiv.org/abs/2110.01563)


  In this paper, we propose a reinforcement learning based algorithm for
rate-profile construction of Arikan's Polarization Assisted Convolutional (PAC)
codes. This method can be used for any blocklength, rate, list size under
successive cancellation list (SCL) decoding and convolutional precoding
polynomial. To the best of our knowledge, we present, for the first time, a set
of new reward and update strategies which help the reinforcement learning agent
discover much better rate-profiles than those present in existing literature.
Simulation results show that PAC codes constructed with the proposed algorithm
perform better in terms of frame erasure rate (FER) compared to the PAC codes
constructed with contemporary rate profiling designs for various list lengths.
Further, by using a (64, 32) PAC code as an example, it is shown that the
choice of convolutional precoding polynomial can have a significant impact on
rate-profile construction of PAC codes.

    

### [[2110.01584] Information-theoretic generalization bounds for black-box learning algorithms](http://arxiv.org/abs/2110.01584)


  We derive information-theoretic generalization bounds for supervised learning
algorithms based on the information contained in predictions rather than in the
output of the training algorithm. These bounds improve over the existing
information-theoretic bounds, are applicable to a wider range of algorithms,
and solve two key challenges: (a) they give meaningful results for
deterministic algorithms and (b) they are significantly easier to estimate. We
show experimentally that the proposed bounds closely follow the generalization
gap in practical scenarios for deep learning.

    

### [[2110.01709] Benchmarking Memory-Centric Computing Systems: Analysis of Real Processing-in-Memory Hardware](http://arxiv.org/abs/2110.01709)


  Many modern workloads such as neural network inference and graph processing
are fundamentally memory-bound. For such workloads, data movement between
memory and CPU cores imposes a significant overhead in terms of both latency
and energy. A major reason is that this communication happens through a narrow
bus with high latency and limited bandwidth, and the low data reuse in
memory-bound workloads is insufficient to amortize the cost of memory access.
Fundamentally addressing this data movement bottleneck requires a paradigm
where the memory system assumes an active role in computing by integrating
processing capabilities. This paradigm is known as processing-in-memory (PIM).
Recent research explores different forms of PIM architectures, motivated by the
emergence of new technologies that integrate memory with a logic layer, where
processing elements can be easily placed. Past works evaluate these
architectures in simulation or, at best, with simplified hardware prototypes.
In contrast, the UPMEM company has designed and manufactured the first
publicly-available real-world PIM architecture. The UPMEM PIM architecture
combines traditional DRAM memory arrays with general-purpose in-order cores,
called DRAM Processing Units (DPUs), integrated in the same chip. This paper
presents key takeaways from the first comprehensive analysis of the first
publicly-available real-world PIM architecture. We provide four key takeaways
about the UPMEM PIM architecture, which stem from our study. More insights
about suitability of different workloads to the PIM system, programming
recommendations for software designers, and suggestions and hints for hardware
and architecture designers of future PIM systems are available in
arXiv:2105.03814


### [[2105.14295] ECMO: Peripheral Transplantation to Rehost Embedded Linux Kernels](http://arxiv.org/abs/2105.14295)


  Dynamic analysis based on the full-system emulator QEMU is widely used for
various purposes. However, it is challenging to run firmware images of embedded
devices in QEMU, especially the process to boot the Linux kernel (we call this
process rehosting the Linux kernel.) That's because embedded devices usually
use different system-on-chips (SoCs) from multiple vendors and only a limited
number of SoCs are currently supported in QEMU.
In this work, we propose a technique called peripheral transplantation. The
main idea is to transplant the device drivers of designated peripherals into
the Linux kernel. By doing so, it can replace the peripherals in the kernel
that are currently unsupported in QEMU with supported ones, thus making the
Linux kernel rehostable. After that, various applications can be built upon.
We implemented this technique inside a prototype system called ECMO and
applied it to 815 firmware images, which consist of 20 kernel versions, 37
device models, and 24 vendors. The result shows that ECMO can successfully
transplant peripherals for all the 815 Linux kernels. Among them,710 kernels
can be successfully rehosted, i.e., launching a user-space shell (87.1% success
rate). The failed cases are mainly because the root file system format (ramfs)
is not supported by the kernel. We further build three applications, i.e.,
kernel crash analysis, rootkit forensic analysis, and kernel fuzzing, based on
the rehosted kernels to demonstrate the usage scenarios of ECMO

    

### [[2110.01927] LogDP: Combining Dependency and Proximity for Log-based Anomaly Detection](http://arxiv.org/abs/2110.01927)


  Log analysis is an important technique that engineers use for troubleshooting
faults of large-scale service-oriented systems. In this study, we propose a
novel semi-supervised log-based anomaly detection approach, LogDP, which
utilizes the dependency relationships among log events and proximity among log
sequences to detect the anomalies in massive unlabeled log data. LogDP divides
log events into dependent and independent events, then learns normal patterns
of dependent events using dependency and independent events using proximity.
Events violating any normal pattern are identified as anomalies. By combining
dependency and proximity, LogDP is able to achieve high detection accuracy.
Extensive experiments have been conducted on real-world datasets, and the
results show that LogDP outperforms six state-of-the-art methods.

    

### [[2110.01936] Local certification of MSO properties for bounded treedepth graphs](http://arxiv.org/abs/2110.01936)


  The graph model checking problem consists in testing whether an input graph
satisfies a given logical formula. In this paper, we study this problem in a
distributed setting, namely local certification. The goal is to assign labels
to the nodes of a network to certify that some given property is satisfied, in
such a way that the labels can be checked locally.
We first investigate which properties can be locally certified with small
certificates. Not surprisingly, this is almost never the case, except for not
very expressive logic fragments. Following the steps of Courcelle-Grohe, we
then look for meta-theorems explaining what happens when we parameterize the
problem by some standard measures of how simple the graph classes are. In that
direction, our main result states that any MSO formula can be locally certified
on graphs with bounded treedepth with a logarithmic number of bits per node,
which is the golden standard in certification.

    

### [[2110.02140] S2 Reducer: High-Performance Sparse Communication to Accelerate Distributed Deep Learning](http://arxiv.org/abs/2110.02140)


  Distributed stochastic gradient descent (SGD) approach has been widely used
in large-scale deep learning, and the gradient collective method is vital to
ensure the training scalability of the distributed deep learning system.
Collective communication such as AllReduce has been widely adopted for the
distributed SGD process to reduce the communication time. However, AllReduce
incurs large bandwidth resources while most gradients are sparse in many cases
since many gradient values are zeros and should be efficiently compressed for
bandwidth saving. To reduce the sparse gradient communication overhead, we
propose Sparse-Sketch Reducer (S2 Reducer), a novel sketch-based sparse
gradient aggregation method with convergence guarantees. S2 Reducer reduces the
communication cost by only compressing the non-zero gradients with count-sketch
and bitmap, and enables the efficient AllReduce operators for parallel SGD
training. We perform extensive evaluation against four state-of-the-art methods
over five training models. Our results show that S2 Reducer converges to the
same accuracy, reduces 81\% sparse communication overhead, and achieves 1.8$
\times $ speedup compared to state-of-the-art approaches.

    

### [[2110.02168] A Community Roadmap for Scientific Workflows Research and Development](http://arxiv.org/abs/2110.02168)


  The landscape of workflow systems for scientific applications is notoriously
convoluted with hundreds of seemingly equivalent workflow systems, many
isolated research claims, and a steep learning curve. To address some of these
challenges and lay the groundwork for transforming workflows research and
development, the WorkflowsRI and ExaWorks projects partnered to bring the
international workflows community together. This paper reports on discussions
and findings from two virtual "Workflows Community Summits" (January and April,
2021). The overarching goals of these workshops were to develop a view of the
state of the art, identify crucial research challenges in the workflows
community, articulate a vision for potential community efforts, and discuss
technical approaches for realizing this vision. To this end, participants
identified six broad themes: FAIR computational workflows; AI workflows;
exascale challenges; APIs, interoperability, reuse, and standards; training and
education; and building a workflows community. We summarize discussions and
recommendations for each of these themes.

    

### [[1905.03448] parasweep: A template-based utility for generating, dispatching, and post-processing of parameter sweeps](http://arxiv.org/abs/1905.03448)


  We introduce parasweep, a free and open-source utility for facilitating
parallel parameter sweeps with computational models. Instead of requiring
parameters to be passed by command-line, which can be error-prone and
time-consuming, parasweep leverages the model's existing configuration files
using a template system, requiring minimal code changes. parasweep supports a
variety different sweep types, generating parameter sets accordingly and
dispatching a parallel job for each set, with support for local execution as
well as common high-performance computing (HPC) job schedulers. Post-processing
is facilitated by providing a mapping between the parameter sets and the
simulations. We demonstrate the usage of parasweep with an example.

    

### [[2007.08187] Processes, Systems and Tests: Defining Contextual Equivalences](http://arxiv.org/abs/2007.08187)


  In this position paper, we would like to offer and defend a new template to
study equivalences between programs -- in the particular framework of process
algebras for concurrent computation.We believe that our layered model of
development will clarify the distinction that is too often left implicit
between the tasks and duties of the programmer and of the tester. It will also
enlighten pre-existing issues that have been running across process algebras as
diverse as the calculus of communicating systems, the $\pi$-calculus -- also in
its distributed version -- or mobile ambients.Our distinction starts by
subdividing the notion of process itself in three conceptually separated
entities, that we call \emph{Processes}, \emph{Systems} and \emph{Tests}. While
the role of what can be observed and the subtleties in the definitions of
congruences have been intensively studied, the fact that \emph{not every
process can be tested}, and that \emph{the tester should have access to a
different set of tools than the programmer} is curiously left out, or at least
not often formally discussed.We argue that this blind spot comes from the
under-specification of contexts -- environments in which comparisons takes
place -- that play multiple distinct roles but supposedly always \enquote{stay
the same}.We illustrate our statement with a simple Java example, the
\enquote{usual} concurrent languages, but also back it up with
$\lambda$-calculus and existing implementations of concurrent languages as
well.

    

### [[2010.09086] Blockchain Based Decentralized Replay Attack Detection for Large Scale Power Systems](http://arxiv.org/abs/2010.09086)


  Large scale power systems are comprised of regional utilities with assets
that stream sensor readings in real time. In order to detect cyberattacks, the
globally acquired, real time sensor data needs to be analyzed in a centralized
fashion. However, owing to operational constraints, such a centralized sharing
mechanism turns out to be a major obstacle. In this paper, we propose a
blockchain based decentralized framework for detecting coordinated replay
attacks with full privacy of sensor data. We develop a Bayesian inference
mechanism employing locally reported attack probabilities that is tailor made
for a blockchain framework. We compare our framework to a traditional
decentralized algorithm based on the broadcast gossip framework both
theoretically as well as empirically. With the help of experiments on a private
Ethereum blockchain, we show that our approach achieves good detection quality
and significantly outperforms gossip driven approaches in terms of accuracy,
timeliness and scalability.

    

### [[2110.01412] Observing a Moving Target -- Reliable Transmission of Debug Logs from Embedded Mobile Devices](http://arxiv.org/abs/2110.01412)


  Mobile embedded devices of the Internet of Things (IoT) face tight resource
constraints and uncertain environments, including energy scarcity and unstable
connectivity. This aggravates debugging, optimization, monitoring, etc.; for
which logging information must be accessible throughout all phases of
development and product life cycles. This work compares approaches for
transmitting logs with regard to application requirements (e.g., bandwidth),
resource consumption (e.g., memory), operating constraints (e.g., power
supply), and the medium (e.g., UART, WiFi). A qualitative comparison suggests
that the adequacy of approaches depends on the concrete application and the
phase in the life cycle. We report from our case study, where the embedded
mobile device is represented by a self-driving slot car (Carrera D132). With
this target device, failure logs, new firmware, and monitoring data need to be
exchanged. The gathered experiences support our qualitative discussion:
Wireless techniques can suit the needs of many phases in the life cycle but it
is particularly evident that energy consumption is crucial. With a loaded
wireless stack and while transmitting logs, the car's operating voltage drop
within a \~ 20 ms power interruption is \~ 1.6 times higher. This limits
communication flexibility significantly.

    

### [[2110.01770] Procedure Planning in Instructional Videosvia Contextual Modeling and Model-based Policy Learning](http://arxiv.org/abs/2110.01770)


  Learning new skills by observing humans' behaviors is an essential capability
of AI. In this work, we leverage instructional videos to study humans'
decision-making processes, focusing on learning a model to plan goal-directed
actions in real-life videos. In contrast to conventional action recognition,
goal-directed actions are based on expectations of their outcomes requiring
causal knowledge of potential consequences of actions. Thus, integrating the
environment structure with goals is critical for solving this task. Previous
works learn a single world model will fail to distinguish various tasks,
resulting in an ambiguous latent space; planning through it will gradually
neglect the desired outcomes since the global information of the future goal
degrades quickly as the procedure evolves. We address these limitations with a
new formulation of procedure planning and propose novel algorithms to model
human behaviors through Bayesian Inference and model-based Imitation Learning.
Experiments conducted on real-world instructional videos show that our method
can achieve state-of-the-art performance in reaching the indicated goals.
Furthermore, the learned contextual information presents interesting features
for planning in a latent space.

    

### [[2110.01771] Feasible Architecture for Quantum Fully Convolutional Networks](http://arxiv.org/abs/2110.01771)


  Fully convolutional networks are robust in performing semantic segmentation,
with many applications from signal processing to computer vision. From the
fundamental principles of variational quantum algorithms, we propose a feasible
pure quantum architecture that can be operated on noisy intermediate-scale
quantum devices. In this work, a parameterized quantum circuit consisting of
three layers, convolutional, pooling, and upsampling, is characterized by
generative one-qubit and two-qubit gates and driven by a classical optimizer.
This architecture supplies a solution for realizing the dynamical programming
on a one-way quantum computer and maximally taking advantage of quantum
computing throughout the calculation. Moreover, our algorithm works on many
physical platforms, and particularly the upsampling layer can use either
conventional qubits or multiple-level systems. Through numerical simulations,
our study represents the successful training of a pure quantum fully
convolutional network and discusses advantages by comparing it with the hybrid
solution.

    

### [[2110.01776] An Ample Approach to Data and Modeling](http://arxiv.org/abs/2110.01776)


  In the present work, we describe a framework for modeling how models can be
built that integrates concepts and methods from a wide range of fields. The
information schism between the real-world and that which can be gathered and
considered by any individual information processing agent is characterized and
discussed, which is followed by the presentation of a series of the adopted
requisites while developing the modeling approach. The issue of mapping from
datasets into models is subsequently addressed, as well as some of the
respectively implied difficulties and limitations. Based on these
considerations, an approach to meta modeling how models are built is then
progressively developed. First, the reference M^* meta model framework is
presented, which relies critically in associating whole datasets and respective
models in terms of a strict equivalence relation. Among the interesting
features of this model are its ability to bridge the gap between data and
modeling, as well as paving the way to an algebra of both data and models which
can be employed to combine models into hierarchical manner. After illustrating
the M* model in terms of patterns derived from regular lattices, the reported
modeling approach continues by discussing how sampling issues, error and
overlooked data can be addressed, leading to the $M^{<\epsilon>}$ variant. The
situation in which the data needs to be represented in terms of respective
probability densities is treated next, yielding the $M^{<\sigma>}$ meta model,
which is then illustrated respectively to a real-world dataset (iris flowers
data). Several considerations about how the developed framework can provide
insights about data clustering, complexity, collaborative research, deep
learning, and creativity are then presented, followed by overall conclusions.

    

### [[2110.01777] MetaPix: Domain Transfer for Semantic Segmentation by Meta Pixel Weighting](http://arxiv.org/abs/2110.01777)


  Training a deep neural model for semantic segmentation requires collecting a
large amount of pixel-level labeled data. To alleviate the data scarcity
problem presented in the real world, one could utilize synthetic data whose
label is easy to obtain. Previous work has shown that the performance of a
semantic segmentation model can be improved by training jointly with real and
synthetic examples with a proper weighting on the synthetic data. Such
weighting was learned by a heuristic to maximize the similarity between
synthetic and real examples. In our work, we instead learn a pixel-level
weighting of the synthetic data by meta-learning, i.e., the learning of
weighting should only be minimizing the loss on the target task. We achieve
this by gradient-on-gradient technique to propagate the target loss back into
the parameters of the weighting model. The experiments show that our method
with only one single meta module can outperform a complicated combination of an
adversarial feature alignment, a reconstruction loss, plus a hierarchical
heuristic weighting at pixel, region and image levels.

    

### [[2110.01831] The Artificial Scientist: Logicist, Emergentist, and Universalist Approaches to Artificial General Intelligence](http://arxiv.org/abs/2110.01831)


  We attempt to define what is necessary to construct an Artificial Scientist,
explore and evaluate several approaches to artificial general intelligence
(AGI) which may facilitate this, conclude that a unified or hybrid approach is
necessary and explore two theories that satisfy this requirement to some
degree.

    

### [[2110.01834] Thinking Fast and Slow in AI: the Role of Metacognition](http://arxiv.org/abs/2110.01834)


  AI systems have seen dramatic advancement in recent years, bringing many
applications that pervade our everyday life. However, we are still mostly
seeing instances of narrow AI: many of these recent developments are typically
focused on a very limited set of competencies and goals, e.g., image
interpretation, natural language processing, classification, prediction, and
many others. Moreover, while these successes can be accredited to improved
algorithms and techniques, they are also tightly linked to the availability of
huge datasets and computational power. State-of-the-art AI still lacks many
capabilities that would naturally be included in a notion of (human)
intelligence.
We argue that a better study of the mechanisms that allow humans to have
these capabilities can help us understand how to imbue AI systems with these
competencies. We focus especially on D. Kahneman's theory of thinking fast and
slow, and we propose a multi-agent AI architecture where incoming problems are
solved by either system 1 (or "fast") agents, that react by exploiting only
past experience, or by system 2 (or "slow") agents, that are deliberately
activated when there is the need to reason and search for optimal solutions
beyond what is expected from the system 1 agent. Both kinds of agents are
supported by a model of the world, containing domain knowledge about the
environment, and a model of "self", containing information about past actions
of the system and solvers' skills.

    

### [[2110.01835] Compression, The Fermi Paradox and Artificial Super-Intelligence](http://arxiv.org/abs/2110.01835)


  The following briefly discusses possible difficulties in communication with
and control of an AGI (artificial general intelligence), building upon an
explanation of The Fermi Paradox and preceding work on symbol emergence and
artificial general intelligence. The latter suggests that to infer what someone
means, an agent constructs a rationale for the observed behaviour of others.
Communication then requires two agents labour under similar compulsions and
have similar experiences (construct similar solutions to similar tasks). Any
non-human intelligence may construct solutions such that any rationale for
their behaviour (and thus the meaning of their signals) is outside the scope of
what a human is inclined to notice or comprehend. Further, the more compressed
a signal, the closer it will appear to random noise. Another intelligence may
possess the ability to compress information to the extent that, to us, their
signals would appear indistinguishable from noise (an explanation for The Fermi
Paradox). To facilitate predictive accuracy an AGI would tend to more
compressed representations of the world, making any rationale for their
behaviour more difficult to comprehend for the same reason. Communication with
and control of an AGI may subsequently necessitate not only human-like
compulsions and experiences, but imposed cognitive impairment.

    

### [[2110.01880] Frequency Aware Face Hallucination Generative Adversarial Network with Semantic Structural Constraint](http://arxiv.org/abs/2110.01880)


  In this paper, we address the issue of face hallucination. Most current face
hallucination methods rely on two-dimensional facial priors to generate high
resolution face images from low resolution face images. These methods are only
capable of assimilating global information into the generated image. Still
there exist some inherent problems in these methods; such as, local features,
subtle structural details and missing depth information in final output image.
Present work proposes a Generative Adversarial Network (GAN) based novel
progressive Face Hallucination (FH) network to address these issues present
among current methods. The generator of the proposed model comprises of FH
network and two sub-networks, assisting FH network to generate high resolution
images. The first sub-network leverages on explicitly adding high frequency
components into the model. To explicitly encode the high frequency components,
an auto encoder is proposed to generate high resolution coefficients of
Discrete Cosine Transform (DCT). To add three dimensional parametric
information into the network, second sub-network is proposed. This network uses
a shape model of 3D Morphable Models (3DMM) to add structural constraint to the
FH network. Extensive experimentation results in the paper shows that the
proposed model outperforms the state-of-the-art methods.

    

### [[2110.01909] A Table-Based Representation for Probabilistic Logic: Preliminary Results](http://arxiv.org/abs/2110.01909)


  We present Probabilistic Decision Model and Notation (pDMN), a probabilistic
extension of Decision Model and Notation (DMN). DMN is a modeling notation for
deterministic decision logic, which intends to be user-friendly and low in
complexity. pDMN extends DMN with probabilistic reasoning, predicates,
functions, quantification, and a new hit policy. At the same time, it aims to
retain DMN's user-friendliness to allow its usage by domain experts without the
help of IT staff. pDMN models can be unambiguously translated into ProbLog
programs to answer user queries. ProbLog is a probabilistic extension of Prolog
flexibly enough to model and reason over any pDMN model.

    

### [[2110.01948] AraCOVID19-SSD: Arabic COVID-19 Sentiment and Sarcasm Detection Dataset](http://arxiv.org/abs/2110.01948)


  Coronavirus disease (COVID-19) is an infectious respiratory disease that was
first discovered in late December 2019, in Wuhan, China, and then spread
worldwide causing a lot of panic and death. Users of social networking sites
such as Facebook and Twitter have been focused on reading, publishing, and
sharing novelties, tweets, and articles regarding the newly emerging pandemic.
A lot of these users often employ sarcasm to convey their intended meaning in a
humorous, funny, and indirect way making it hard for computer-based
applications to automatically understand and identify their goal and the harm
level that they can inflect. Motivated by the emerging need for annotated
datasets that tackle these kinds of problems in the context of COVID-19, this
paper builds and releases AraCOVID19-SSD a manually annotated Arabic COVID-19
sarcasm and sentiment detection dataset containing 5,162 tweets. To confirm the
practical utility of the built dataset, it has been carefully analyzed and
tested using several classification models.

    

### [[2110.01990] SMProbLog: Stable Model Semantics in ProbLog and its Applications in Argumentation](http://arxiv.org/abs/2110.01990)


  We introduce SMProbLog, a generalization of the probabilistic logic
programming language ProbLog. A ProbLog program defines a distribution over
logic programs by specifying for each clause the probability that it belongs to
a randomly sampled program, and these probabilities are mutually independent.
The semantics of ProbLog is given by the success probability of a query, which
corresponds to the probability that the query succeeds in a randomly sampled
program. It is well-defined when each random sample uniquely determines the
truth values of all logical atoms. Argumentation problems, however, represent
an interesting practical application where this is not always the case.
SMProbLog generalizes the semantics of ProbLog to the setting where multiple
truth assignments are possible for a randomly sampled program, and implements
the corresponding algorithms for both inference and learning tasks. We then
show how this novel framework can be used to reason about probabilistic
argumentation problems. Therefore, the key contribution of this paper are: a
more general semantics for ProbLog programs, its implementation into a
probabilistic programming framework for both inference and parameter learning,
and a novel approach to probabilistic argumentation problems based on such
framework.

    

### [[2110.02007] Empowering Local Communities Using Artificial Intelligence](http://arxiv.org/abs/2110.02007)


  Many powerful Artificial Intelligence (AI) techniques have been engineered
with the goals of high performance and accuracy. Recently, AI algorithms have
been integrated into diverse and real-world applications. It has become an
important topic to explore the impact of AI on society from a people-centered
perspective. Previous works in citizen science have identified methods of using
AI to engage the public in research, such as sustaining participation,
verifying data quality, classifying and labeling objects, predicting user
interests, and explaining data patterns. These works investigated the
challenges regarding how scientists design AI systems for citizens to
participate in research projects at a large geographic scale in a generalizable
way, such as building applications for citizens globally to participate in
completing tasks. In contrast, we are interested in another area that receives
significantly less attention: how scientists co-design AI systems "with" local
communities to influence a particular geographical region, such as
community-based participatory projects. Specifically, this article discusses
the challenges of applying AI in Community Citizen Science, a framework to
create social impact through community empowerment at an intensely place-based
local scale. We provide insights in this under-explored area of focus to
connect scientific research closely to social issues and citizen needs.

    

### [[2110.02014] Solving even-parity problems using traceless genetic programming](http://arxiv.org/abs/2110.02014)


  A genetic programming (GP) variant called traceless genetic programming (TGP)
is proposed in this paper. TGP is a hybrid method combining a technique for
building individuals and a technique for representing individuals. The main
difference between TGP and other GP techniques is that TGP does not explicitly
store the evolved computer programs. Two genetic operators are used in
conjunction with TGP: crossover and insertion. TGP is applied for evolving
digital circuits for the even-parity problem. Numerical experiments show that
TGP outperforms standard GP with several orders of magnitude.

    

### [[2110.02027] Debiased Graph Contrastive Learning](http://arxiv.org/abs/2110.02027)


  Contrastive learning (CL) has emerged as a dominant technique for
unsupervised representation learning which embeds augmented versions of the
anchor close to each other (positive samples) and pushes the embeddings of
other samples (negative samples) apart. As revealed in recent works, CL can
benefit from hard negative samples (negative samples that are difficult to
distinguish from the anchor). However, we observe minor improvement or even
performance drop when we adopt existing hard negative mining techniques in
Graph Contrastive Learning (GCL). We find that many hard negative samples
similar to anchor point are false negative ones (samples from the same class as
anchor point) in GCL, which is different from CL in computer vision and will
lead to unsatisfactory performance of existing hard negative mining techniques
in GCL. To eliminate this bias, we propose Debiased Graph Contrastive Learning
(DGCL), a novel and effective method to estimate the probability whether each
negative sample is true or not. With this probability, we devise two schemes
(i.e., DGCL-weight and DGCL-mix) to boost the performance of GCL. Empirically,
DGCL outperforms or matches previous unsupervised state-of-the-art results on
several benchmarks and even exceeds the performance of supervised ones.

    

### [[2110.02042] ur-iw-hnt at GermEval 2021: An Ensembling Strategy with Multiple BERT Models](http://arxiv.org/abs/2110.02042)


  This paper describes our approach (ur-iw-hnt) for the Shared Task of
GermEval2021 to identify toxic, engaging, and fact-claiming comments. We
submitted three runs using an ensembling strategy by majority (hard) voting
with multiple different BERT models of three different types: German-based,
Twitter-based, and multilingual models. All ensemble models outperform single
models, while BERTweet is the winner of all individual models in every subtask.
Twitter-based models perform better than GermanBERT models, and multilingual
models perform worse but by a small margin.

    

### [[2110.02047] TENT: Text Classification Based on ENcoding Tree Learning](http://arxiv.org/abs/2110.02047)


  Text classification is a primary task in natural language processing (NLP).
Recently, graph neural networks (GNNs) have developed rapidly and been applied
to text classification tasks. Although more complex models tend to achieve
better performance, research highly depends on the computing power of the
device used. In this article, we propose TENT (this https URL)
to obtain better text classification performance and reduce the reliance on
computing power. Specifically, we first establish a dependency analysis graph
for each text and then convert each graph into its corresponding encoding tree.
The representation of the entire graph is obtained by updating the
representation of the non-leaf nodes in the encoding tree. Experimental results
show that our method outperforms other baselines on several datasets while
having a simple structure and few parameters.

    

### [[2110.02056] Are Training Resources Insufficient? Predict First Then Explain!](http://arxiv.org/abs/2110.02056)


  Natural language free-text explanation generation is an efficient approach to
train explainable language processing models for
commonsense-knowledge-requiring tasks. The most predominant form of these
models is the explain-then-predict (EtP) structure, which first generates
explanations and uses them for making decisions. The performance of EtP models
is highly dependent on that of the explainer by the nature of their structure.
Therefore, large-sized explanation data are required to train a good explainer
model. However, annotating explanations is expensive. Also, recent works reveal
that free-text explanations might not convey sufficient information for
decision making. These facts cast doubts on the effectiveness of EtP models. In
this paper, we argue that the predict-then-explain (PtE) architecture is a more
efficient approach in terms of the modelling perspective. Our main contribution
is twofold. First, we show that the PtE structure is the most data-efficient
approach when explanation data are lacking. Second, we reveal that the PtE
structure is always more training-efficient than the EtP structure. We also
provide experimental results that confirm the theoretical advantages.

    

### [[2110.02084] Models for Narrative Information: A Study](http://arxiv.org/abs/2110.02084)


  The major objective of this work is to study and report the existing
ontology-driven models for narrative information. The paper aims to analyze
these models across various domains. The goal of this work is to bring the
relevant literature, and ontology models under one umbrella, and perform a
parametric comparative study. A systematic literature review methodology was
adopted for an extensive literature selection. A random stratified sampling
technique was used to select the models from the literature. The findings
explicate a comparative view of the narrative models across domains. The
differences and similarities of knowledge representation across domains, in
case of narrative information models based on ontology was identified. There
are significantly fewer studies that reviewed the ontology-based narrative
models. This work goes a step further by evaluating the ontologies using the
parameters from narrative components. This paper will explore the basic
concepts and top-level concepts in the models. Besides, this study provides a
comprehensive study of the narrative theories in the context of ongoing
research. The findings of this work demonstrate the similarities and
differences among the elements of the ontology across domains. It also
identifies the state of the art literature for ontology-based narrative
information.

    

### [[2110.02182] Blockchain-based Federated Learning: A Comprehensive Survey](http://arxiv.org/abs/2110.02182)


  With the technological advances in machine learning, effective ways are
available to process the huge amount of data generated in real life. However,
issues of privacy and scalability will constrain the development of machine
learning. Federated learning (FL) can prevent privacy leakage by assigning
training tasks to multiple clients, thus separating the central server from the
local devices. However, FL still suffers from shortcomings such as
single-point-failure and malicious data. The emergence of blockchain provides a
secure and efficient solution for the deployment of FL. In this paper, we
conduct a comprehensive survey of the literature on blockchained FL (BCFL).
First, we investigate how blockchain can be applied to federal learning from
the perspective of system composition. Then, we analyze the concrete functions
of BCFL from the perspective of mechanism design and illustrate what problems
blockchain addresses specifically for FL. We also survey the applications of
BCFL in reality. Finally, we discuss some challenges and future research
directions.

    

### [[1904.07091] Deep Policies for Width-Based Planning in Pixel Domains](http://arxiv.org/abs/1904.07091)


  Width-based planning has demonstrated great success in recent years due to
its ability to scale independently of the size of the state space. For example,
Bandres et al. (2018) introduced a rollout version of the Iterated Width
algorithm whose performance compares well with humans and learning methods in
the pixel setting of the Atari games suite. In this setting, planning is done
on-line using the "screen" states and selecting actions by looking ahead into
the future. However, this algorithm is purely exploratory and does not leverage
past reward information. Furthermore, it requires the state to be factored into
features that need to be pre-defined for the particular task, e.g., the B-PROST
pixel features. In this work, we extend width-based planning by incorporating
an explicit policy in the action selection mechanism. Our method, called
$\pi$-IW, interleaves width-based planning and policy learning using the
state-actions visited by the planner. The policy estimate takes the form of a
neural network and is in turn used to guide the planning step, thus reinforcing
promising paths. Surprisingly, we observe that the representation learned by
the neural network can be used as a feature space for the width-based planner
without degrading its performance, thus removing the requirement of pre-defined
features for the planner. We compare $\pi$-IW with previous width-based methods
and with AlphaZero, a method that also interleaves planning and learning, in
simple environments, and show that $\pi$-IW has superior performance. We also
show that $\pi$-IW algorithm outperforms previous width-based methods in the
pixel setting of Atari games suite.

    

### [[2007.11073] Book Success Prediction with Pretrained Sentence Embeddings and Readability Scores](http://arxiv.org/abs/2007.11073)


  Predicting the potential success of a book in advance is vital in many
applications. This could help both publishers and readers in their
decision-making process whether or not a book is worth publishing and reading,
respectively. In this paper, we propose a model that leverages pretrained
sentence embeddings along with various readability scores for book success
prediction. Unlike previous methods, the proposed method requires no
count-based, lexical, or syntactic features. Instead, we use a convolutional
neural network over pretrained sentence embeddings and leverage different
readability scores through a simple concatenation operation. Our proposed model
outperforms strong baselines for this task by as large as 6.4\% F1-score
points. Moreover, our experiments show that according to our model, only the
first 1K sentences are good enough to predict the potential success of books.

    

### [[2007.13053] Recursive Rules with Aggregation: A Simple Unified Semantics](http://arxiv.org/abs/2007.13053)


  Complex reasoning problems are most clearly and easily specified using
logical rules, but require recursive rules with aggregation such as counts and
sums for practical applications. Unfortunately, the meaning of such rules has
been a significant challenge, leading to many disagreeing semantics. This paper
describes a unified semantics for recursive rules with aggregation, extending
the unified founded semantics and constraint semantics for recursive rules with
negation. The key idea is to support simple expression of the different
assumptions underlying different semantics, and orthogonally interpret
aggregation operations using their simple usual meaning.
We present formal definition of the semantics, prove important properties of
the semantics, and compare with prior semantics. Besides exact inference over
aggregations, we present an efficient approximation that gives precise answers
to all examples we have studied from the literature. We also apply our
semantics to a wide range of challenging examples, and show that our semantics
is simple and matches the desired results in all cases. Finally, we describe
experiments on the most challenging examples, exhibiting unexpectedly superior
performance over well-known systems when they can compute correct answers.

    

### [[2011.08446] EvoPose2D: Pushing the Boundaries of 2D Human Pose Estimation using Accelerated Neuroevolution with Weight Transfer](http://arxiv.org/abs/2011.08446)


  Neural architecture search has proven to be highly effective in the design of
efficient convolutional neural networks that are better suited for mobile
deployment than hand-designed networks. Hypothesizing that neural architecture
search holds great potential for human pose estimation, we explore the
application of neuroevolution, a form of neural architecture search inspired by
biological evolution, in the design of 2D human pose networks for the first
time. Additionally, we propose a new weight transfer scheme that enables us to
accelerate neuroevolution in a flexible manner. Our method produces network
designs that are more efficient and more accurate than state-of-the-art
hand-designed networks. In fact, the generated networks process images at
higher resolutions using less computation than previous hand-designed networks
at lower resolutions, allowing us to push the boundaries of 2D human pose
estimation. Our base network designed via neuroevolution, which we refer to as
EvoPose2D-S, achieves comparable accuracy to SimpleBaseline while being 50%
faster and 12.7x smaller in terms of file size. Our largest network,
EvoPose2D-L, achieves new state-of-the-art accuracy on the Microsoft COCO
Keypoints benchmark, is 4.3x smaller than its nearest competitor, and has
similar inference speed. The code is publicly available at
this https URL.

    

### [[2103.16704] Probabilistic Analogical Mapping with Semantic Relation Networks](http://arxiv.org/abs/2103.16704)


  The human ability to flexibly reason using analogies with domain-general
content depends on mechanisms for identifying relations between concepts, and
for mapping concepts and their relations across analogs. Building on a recent
model of how semantic relations can be learned from non-relational word
embeddings, we present a new computational model of mapping between two
analogs. The model adopts a Bayesian framework for probabilistic graph
matching, operating on semantic relation networks constructed from distributed
representations of individual concepts and of relations between concepts.
Through comparisons of model predictions with human performance in a novel
mapping task requiring integration of multiple relations, as well as in several
classic studies, we demonstrate that the model accounts for a broad range of
phenomena involving analogical mapping by both adults and children. We also
show the potential for extending the model to deal with analog retrieval. Our
approach demonstrates that human-like analogical mapping can emerge from
comparison mechanisms applied to rich semantic representations of individual
concepts and relations.

    

### [[2106.05688] AI-enabled Automation for Completeness Checking of Privacy Policies](http://arxiv.org/abs/2106.05688)


  Technological advances in information sharing have raised concerns about data
protection. Privacy policies contain privacy-related requirements about how the
personal data of individuals will be handled by an organization or a software
system (e.g., a web service or an app). In Europe, privacy policies are subject
to compliance with the General Data Protection Regulation (GDPR). A
prerequisite for GDPR compliance checking is to verify whether the content of a
privacy policy is complete according to the provisions of GDPR. Incomplete
privacy policies might result in large fines on violating organization as well
as incomplete privacy-related software specifications. Manual completeness
checking is both time-consuming and error-prone. In this paper, we propose
AI-based automation for the completeness checking of privacy policies. Through
systematic qualitative methods, we first build two artifacts to characterize
the privacy-related provisions of GDPR, namely a conceptual model and a set of
completeness criteria. Then, we develop an automated solution on top of these
artifacts by leveraging a combination of natural language processing and
supervised machine learning. Specifically, we identify the GDPR-relevant
information content in privacy policies and subsequently check them against the
completeness criteria. To evaluate our approach, we collected 234 real privacy
policies from the fund industry. Over a set of 48 unseen privacy policies, our
approach detected 300 of the total of 334 violations of some completeness
criteria correctly, while producing 23 false positives. The approach thus has a
precision of 92.9% and recall of 89.8%. Compared to a baseline that applies
keyword search only, our approach results in an improvement of 24.5% in
precision and 38% in recall.

    

### [[2109.13102] A Biologically Plausible Learning Rule for Perceptual Systems of organisms that Maximize Mutual Information](http://arxiv.org/abs/2109.13102)


  It is widely believed that the perceptual system of an organism is optimized
for the properties of the environment to which it is exposed. A specific
instance of this principle known as the Infomax principle holds that the
purpose of early perceptual processing is to maximize the mutual information
between the neural coding and the incoming sensory signal. In this article, we
present a method to implement this principle accurately with a local,
spike-based, and continuous-time learning rule.

    

### [[2110.02150] Online Application Guidance for Heterogeneous Memory Systems](http://arxiv.org/abs/2110.02150)


  Many high end and next generation computing systems to incorporated
alternative memory technologies to meet performance goals. Since these
technologies present distinct advantages and tradeoffs compared to conventional
DDR* SDRAM, such as higher bandwidth with lower capacity or vice versa, they
are typically packaged alongside conventional SDRAM in a heterogeneous memory
architecture. To utilize the different types of memory efficiently, new data
management strategies are needed to match application usage to the best
available memory technology. However, current proposals for managing
heterogeneous memories are limited because they either: 1) do not consider
high-level application behavior when assigning data to different types of
memory, or 2) require separate program execution (with a representative input)
to collect information about how the application uses memory resources.
This work presents a toolset for addressing the limitations of existing
approaches for managing complex memories. It extends the application runtime
layer with automated monitoring and management routines that assign application
data to the best tier of memory based on previous usage, without any need for
source code modification or a separate profiling run. It evaluates this
approach on a state-of-the-art server platform with both conventional DDR4
SDRAM and non-volatile Intel Optane DC memory, using both memory-intensive high
performance computing (HPC) applications as well as standard benchmarks.
Overall, the results show that this approach improves program performance
significantly compared to a standard unguided approach across a variety of
workloads and system configurations. Additionally, we show that this approach
achieves similar performance as a comparable offline profiling-based approach
after a short startup period, without requiring separate program execution or
offline analysis steps.

    

### [[2110.01964] Deductive Verification of Programs with Underspecified Semantics by Model Extraction](http://arxiv.org/abs/2110.01964)


  We present a novel and well automatable approach to formal verification of
programs with underspecified semantics, i.e., a language semantics that leaves
open the order of certain evaluations. First, we reduce this problem to
non-determinism of distributed systems, automatically extracting a distributed
Active Object model from underspecified, sequential C code. This translation
process provides a fully formal semantics for the considered C subset. In the
extracted model every non-deterministic choice corresponds to one possible
evaluation order. This step also automatically translates specifications in the
ANSI/ISO C Specification Language (ACSL) into method contracts and object
invariants for Active Objects. We then perform verification on the specified
Active Objects model. For this we have implemented a theorem prover Crowbar
based on the Behavioral Program Logic (BPL), which verifies the extracted model
with respect to the translated specification and ensures the original property
of the C code for all possible evaluation orders. By using model extraction, we
can use standard tools, without designing a new complex program logic to deal
with underspecification. The case study used is highly underspecified and
cannot be verified with existing tools for C.

    

### [<title>Error during installation R GPU win64: 'R' is not recognized as an internal or external command, operable program or batch file - XGBoost</title>](https://discuss.xgboost.ai/t/error-during-installation-r-gpu-win64-r-is-not-recognized-as-an-internal-or-external-command-operable-program-or-batch-file/2487/3)