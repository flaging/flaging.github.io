
## 2021-7-8

### [<title>GPU accelerated SHAP values crash Google Colab - XGBoost</title>](https://discuss.xgboost.ai/t/gpu-accelerated-shap-values-crash-google-colab/2361/1)

### [[2107.02932] Towards Network Behaviour Trend Evaluation in Software Defined Network (SDN) Considering the number of paths](http://arxiv.org/abs/2107.02932)


  There is a wide range of topologies to use in simulation that can make
research divergency; therefore, we propose a topology set that can be used in
research of network behaviour in Software Defined Network (SDN). This paper can
unite the trend research that is doing in different aspects of SDN. One of the
most effective items which show the behaviour of the proposed model in SDN is
the number of paths that exist between each couple of nodes; hence, we propose
three basic topologies to show this parameter. This paper is useful for those
who are working on SDN and intend to evaluate the effect of their proposal
considering the number of paths. Finally, three topologies called sparse,
partial-mesh and full-mesh will be introduced in this paper.

    

### [[2107.03046] 6G: from Densification to Diversification](http://arxiv.org/abs/2107.03046)


  The 5G system has finally begun commercialization, and now is the time to
start discussing the road map for the 6G system. While the 5G system was
designed with a focus on discovering new service types for high speed,
low-latency, and massive connective services, the evolution of the network
interface for 6G should be considered with an eye toward supporting these
complicated communication environments. As machine-driven data traffic
continues to increase exponentially, 6G must be able to support a series of
connection methods that did not previously exist. In departure from
base-station-oriented cell densification, network diversification is necessary
if we are to satisfy the comprehensive requirements of end terminals for
diverse applications. In this article, we predict what will drive 6G and look
at what key requirements should be considered in 6G. We then diversify four
types of network architectures according to link characteristics, communication
ranges, and target services. The four types of networks play complementary
roles while at the same time collaborating across the entire 6G network.
Lastly, we call attention to key technologies and challenges in the air,
network, and assistive technologies that will have to be addressed when
designing the 6G system.

    

### [[2107.03057] BBRv2+:Towards Balancing Aggressiveness and Fairness with Delay-based Bandwidth Probing](http://arxiv.org/abs/2107.03057)


  BBRv2, proposed by Google, aims at addressing BBR's shortcomings of
unfairness against loss-based congestion control algorithms (CCAs) and
excessive retransmissions in shallow-buffered networks. In this paper, we first
comprehensively study BBRv2's performance under various network conditions and
show that BBRv2 mitigates the shortcomings of BBR. Nevertheless, BBRv2's
benefits come with several costs, including the slow responsiveness to
bandwidth dynamics as well as the low resilience to random losses. We then
propose BBRv2+ to address BBRv2's performance issues without sacrificing its
advantages over BBR. To this end, BBRv2+ incorporates delay information into
its path model, which cautiously guides the aggressiveness of its bandwidth
probing to not reduce its fairness against loss-based CCAs. BBRv2+ also
integrates mechanisms for improved resilience to random losses as well as
network jitters. Extensive experiments demonstrate the effectiveness of BBRv2+.
Especially, it achieves 25% higher throughput and comparable queuing delay in
comparison with BBRv2 in high-mobility network scenarios.

    

### [[2107.03059] Cellular, Wide-Area, and Non-Terrestrial IoT: A Survey on 5G Advances and the Road Towards 6G](http://arxiv.org/abs/2107.03059)


  The next wave of wireless technologies is proliferating in connecting things
among themselves as well as to humans. In the era of the Internet of things
(IoT), billions of sensors, machines, vehicles, drones, and robots will be
connected, making the world around us smarter. The IoT will encompass devices
that must wirelessly communicate a diverse set of data gathered from the
environment for myriad new applications. The ultimate goal is to extract
insights from this data and develop solutions that improve quality of life and
generate new revenue. Providing large-scale, long-lasting, reliable, and near
real-time connectivity is the major challenge in enabling a smart connected
world. This paper provides a comprehensive survey on existing and emerging
communication solutions for serving IoT applications in the context of
cellular, wide-area, as well as non-terrestrial networks. Specifically,
wireless technology enhancements for providing IoT access in fifth-generation
(5G) and beyond cellular networks, and communication networks over the
unlicensed spectrum are presented. Aligned with the main key performance
indicators of 5G and beyond 5G networks, we investigate solutions and standards
that enable energy efficiency, reliability, low latency, and scalability
(connection density) of current and future IoT networks. The solutions include
grant-free access and channel coding for short-packet communications,
non-orthogonal multiple access, and on-device intelligence. Further, a vision
of new paradigm shifts in communication networks in the 2030s is provided, and
the integration of the associated new technologies like artificial
intelligence, non-terrestrial networks, and new spectra is elaborated. Finally,
future research directions toward beyond 5G IoT networks are pointed out.

    

### [[2107.03251] IRS-Aided WPCNs: A New Optimization Framework for Dynamic IRS Beamforming](http://arxiv.org/abs/2107.03251)


  In this paper, we propose a new dynamic IRS beamforming framework to boost
the sum throughput of an intelligent reflecting surface (IRS) aided wireless
powered communication network (WPCN). Specifically, the IRS phase-shift vectors
across time and resource allocation are jointly optimized to enhance the
efficiencies of both downlink wireless power transfer (DL WPT) and uplink
wireless information transmission (UL WIT) between a hybrid access point (HAP)
and multiple wirelessly powered devices. To this end, we first study three
special cases of the dynamic IRS beamforming,namely user-adaptive IRS
beamforming, UL-adaptive IRS beamforming, and static IRS beamforming,by
characterizing their optimal performance relationships and proposing
corresponding algorithms. Interestingly, it is rigorously proved that the
latter two cases achieve the same throughput, thus helping halve the number of
IRS phase shifts to be optimized and signalling overhead practically required
for UL-adaptive IRS beamforming. Then, we propose a general optimization
framework for dynamic IRS beamforming, which is applicable for any given number
of IRS phase-shift vectors available. Despite of the non-convexity of the
general problem with highly coupled optimization variables, we propose two
algorithms to solve it and particularly, the low-complexity algorithm exploits
the intrinsic structure of the optimal solution as well as the solutions to the
cases with user-adaptive and static IRS beamforming. Simulation results
validate our theoretical findings, illustrate the practical significance of IRS
with dynamic beamforming for spectral and energy efficient WPCNs, and
demonstrate the effectiveness of our proposed designs over various benchmark
schemes.

    

### [[2010.01370] Lyapunov-guided Deep Reinforcement Learning for Stable Online Computation Offloading in Mobile-Edge Computing Networks](http://arxiv.org/abs/2010.01370)


  Opportunistic computation offloading is an effective method to improve the
computation performance of mobile-edge computing (MEC) networks under dynamic
edge environment. In this paper, we consider a multi-user MEC network with
time-varying wireless channels and stochastic user task data arrivals in
sequential time frames. In particular, we aim to design an online computation
offloading algorithm to maximize the network data processing capability subject
to the long-term data queue stability and average power constraints. The online
algorithm is practical in the sense that the decisions for each time frame are
made without the assumption of knowing future channel conditions and data
arrivals. We formulate the problem as a multi-stage stochastic mixed integer
non-linear programming (MINLP) problem that jointly determines the binary
offloading (each user computes the task either locally or at the edge server)
and system resource allocation decisions in sequential time frames. To address
the coupling in the decisions of different time frames, we propose a novel
framework, named LyDROO, that combines the advantages of Lyapunov optimization
and deep reinforcement learning (DRL). Specifically, LyDROO first applies
Lyapunov optimization to decouple the multi-stage stochastic MINLP into
deterministic per-frame MINLP subproblems. By doing so, it guarantees to
satisfy all the long-term constraints by solving the per-frame subproblems that
are much smaller in size. Then, LyDROO integrates model-based optimization and
model-free DRL to solve the per-frame MINLP problems with low computational
complexity. Simulation results show that under various network setups, the
proposed LyDROO achieves optimal computation performance while stabilizing all
queues in the system. Besides, it induces very low execution latency that is
particularly suitable for real-time implementation in fast fading environments.

    

### [[2010.04528] P4-CoDel: Experiences on Programmable Data Plane Hardware](http://arxiv.org/abs/2010.04528)


  Fixed buffer sizing in computer networks, especially the Internet, is a
compromise between latency and bandwidth. A decision in favor of high
bandwidth, implying larger buffers, subordinates the latency as a consequence
of constantly filled buffers. This phenomenon is called Bufferbloat. Active
Queue Management (AQM) algorithms such as CoDel or PIE, designed for the use on
software based hosts, offer a flow agnostic remedy to Bufferbloat by
controlling the queue filling and hence the latency through subtle packet
drops. In previous work, we have shown that the data plane programming language
P4 is powerful enough to implement the CoDel algorithm. While legacy software
algorithms can be easily compiled onto almost any processing architecture, this
is not generally true for AQM on programmable data plane hardware, i.e.,
programmable packet processors. In this work, we highlight corresponding
challenges, demonstrate how to tackle them, and provide techniques enabling the
implementation of such AQM algorithms on different high speed P4-programmable
data plane hardware targets. In addition, we provide measurement results
created on different P4-programmable data plane targets. The resulting latency
measurements reveal the feasibility and the constraints to be considered to
perform Active Queue Management within these devices. Finally, we release the
source code and instructions to reproduce the results in this paper as open
source to the research community.

    

### [[2103.09156] 5G from Space: An Overview of 3GPP Non-Terrestrial Networks](http://arxiv.org/abs/2103.09156)


  We provide an overview of the 3rd generation partnership project (3GPP) work
on evolving the 5G wireless technology to support non-terrestrial satellite
networks. Adapting 5G to support non-terrestrial networks entails a holistic
design spanning across multiple areas from radio access network to services and
system aspects to core and terminals. In this article, we describe the main
topics of non-terrestrial networks, explain in detail the design aspects, and
share various design rationales influencing standardization.

    

### [[2106.15734] UAV-assisted Online Machine Learning over Multi-Tiered Networks: A Hierarchical Nested Personalized Federated Learning Approach](http://arxiv.org/abs/2106.15734)


  We consider distributed machine learning (ML) through unmanned aerial
vehicles (UAVs) for geo-distributed device clusters. We propose five new
technologies/techniques: (i) stratified UAV swarms with leader, worker, and
coordinator UAVs, (ii) hierarchical nested personalized federated learning
(HN-PFL): a holistic distributed ML framework for personalized model training
across the worker-leader-core network hierarchy, (iii) cooperative UAV resource
pooling for distributed ML using the UAVs' local computational capabilities,
(iv) aerial data caching and relaying for efficient data relaying to conduct
ML, and (v) concept/model drift, capturing online data variations at the
devices. We split the UAV-enabled model training problem as two parts. (a)
Network-aware HN-PFL, where we optimize a tradeoff between energy consumption
and ML model performance by configuring data offloading among devices-UAVs and
UAV-UAVs, UAVs' CPU frequencies, and mini-batch sizes subject to
communication/computation network heterogeneity. We tackle this optimization
problem via the method of posynomial condensation and propose a distributed
algorithm with a performance guarantee. (b) Macro-trajectory and learning
duration design, which we formulate as a sequential decision making problem,
tackled via deep reinforcement learning. Our simulations demonstrate the
superiority of our methodology with regards to the distributed ML performance,
the optimization of network resources, and the swarm trajectory efficiency.

    

### [[2107.02797] Generalization Error Analysis of Neural networks with Gradient Based Regularization](http://arxiv.org/abs/2107.02797)


  We study gradient-based regularization methods for neural networks. We mainly
focus on two regularization methods: the total variation and the Tikhonov
regularization. Applying these methods is equivalent to using neural networks
to solve some partial differential equations, mostly in high dimensions in
practical applications. In this work, we introduce a general framework to
analyze the generalization error of regularized networks. The error estimate
relies on two assumptions on the approximation error and the quadrature error.
Moreover, we conduct some experiments on the image classification tasks to show
that gradient-based methods can significantly improve the generalization
ability and adversarial robustness of neural networks. A graphical extension of
the gradient-based methods are also considered in the experiments.

    

### [[2107.02821] New Methods and Datasets for Group Anomaly Detection From Fundamental Physics](http://arxiv.org/abs/2107.02821)


  The identification of anomalous overdensities in data - group or collective
anomaly detection - is a rich problem with a large number of real world
applications. However, it has received relatively little attention in the
broader ML community, as compared to point anomalies or other types of single
instance outliers. One reason for this is the lack of powerful benchmark
datasets. In this paper, we first explain how, after the Nobel-prize winning
discovery of the Higgs boson, unsupervised group anomaly detection has become a
new frontier of fundamental physics (where the motivation is to find new
particles and forces). Then we propose a realistic synthetic benchmark dataset
(LHCO2020) for the development of group anomaly detection algorithms. Finally,
we compare several existing statistically-sound techniques for unsupervised
group anomaly detection, and demonstrate their performance on the LHCO2020
dataset.

    

### [[2107.02840] RAILS: A Robust Adversarial Immune-inspired Learning System](http://arxiv.org/abs/2107.02840)


  Adversarial attacks against deep neural networks (DNNs) are continuously
evolving, requiring increasingly powerful defense strategies. We develop a
novel adversarial defense framework inspired by the adaptive immune system: the
Robust Adversarial Immune-inspired Learning System (RAILS). Initializing a
population of exemplars that is balanced across classes, RAILS starts from a
uniform label distribution that encourages diversity and debiases a potentially
corrupted initial condition. RAILS implements an evolutionary optimization
process to adjust the label distribution and achieve specificity towards ground
truth. RAILS displays a tradeoff between robustness (diversity) and accuracy
(specificity), providing a new immune-inspired perspective on adversarial
learning. We empirically validate the benefits of RAILS through several
adversarial image classification experiments on MNIST, SVHN, and CIFAR-10
datasets. For the PGD attack, RAILS is found to improve the robustness over
existing methods by >= 5.62%, 12.5% and 10.32%, respectively, without
appreciable loss of standard accuracy.

    

### [[2107.02842] Immuno-mimetic Deep Neural Networks (Immuno-Net)](http://arxiv.org/abs/2107.02842)


  Biomimetics has played a key role in the evolution of artificial neural
networks. Thus far, in silico metaphors have been dominated by concepts from
neuroscience and cognitive psychology. In this paper we introduce a different
type of biomimetic model, one that borrows concepts from the immune system, for
designing robust deep neural networks. This immuno-mimetic model leads to a new
computational biology framework for robustification of deep neural networks
against adversarial attacks. Within this Immuno-Net framework we define a
robust adaptive immune-inspired learning system (Immuno-Net RAILS) that
emulates, in silico, the adaptive biological mechanisms of B-cells that are
used to defend a mammalian host against pathogenic attacks. When applied to
image classification tasks on benchmark datasets, we demonstrate that
Immuno-net RAILS results in improvement of as much as 12.5% in adversarial
accuracy of a baseline method, the DkNN-robustified CNN, without appreciable
loss of accuracy on clean data.

    

### [[2107.02845] Logit-based Uncertainty Measure in Classification](http://arxiv.org/abs/2107.02845)


  We introduce a new, reliable, and agnostic uncertainty measure for
classification tasks called logit uncertainty. It is based on logit outputs of
neural networks. We in particular show that this new uncertainty measure yields
a superior performance compared to existing uncertainty measures on different
tasks, including out of sample detection and finding erroneous predictions. We
analyze theoretical foundations of the measure and explore a relationship with
high density regions. We also demonstrate how to test uncertainty using
intermediate outputs in training of generative adversarial networks. We propose
two potential ways to utilize logit-based uncertainty in real world
applications, and show that the uncertainty measure outperforms.

    

### [[2107.02847] Transfer Learning in Information Criteria-based Feature Selection](http://arxiv.org/abs/2107.02847)


  This paper investigates the effectiveness of transfer learning based on
Mallows' Cp. We propose a procedure that combines transfer learning with
Mallows' Cp (TLCp) and prove that it outperforms the conventional Mallows' Cp
criterion in terms of accuracy and stability. Our theoretical results indicate
that, for any sample size in the target domain, the proposed TLCp estimator
performs better than the Cp estimator by the mean squared error (MSE) metric in
the case of orthogonal predictors, provided that i) the dissimilarity between
the tasks from source domain and target domain is small, and ii) the procedure
parameters (complexity penalties) are tuned according to certain explicit
rules. Moreover, we show that our transfer learning framework can be extended
to other feature selection criteria, such as the Bayesian information
criterion. By analyzing the solution of the orthogonalized Cp, we identify an
estimator that asymptotically approximates the solution of the Cp criterion in
the case of non-orthogonal predictors. Similar results are obtained for the
non-orthogonal TLCp. Finally, simulation studies and applications with real
data demonstrate the usefulness of the TLCp scheme.

    

### [[2107.02868] Principles for Evaluation of AI/ML Model Performance and Robustness](http://arxiv.org/abs/2107.02868)


  The Department of Defense (DoD) has significantly increased its investment in
the design, evaluation, and deployment of Artificial Intelligence and Machine
Learning (AI/ML) capabilities to address national security needs. While there
are numerous AI/ML successes in the academic and commercial sectors, many of
these systems have also been shown to be brittle and nonrobust. In a complex
and ever-changing national security environment, it is vital that the DoD
establish a sound and methodical process to evaluate the performance and
robustness of AI/ML models before these new capabilities are deployed to the
field. This paper reviews the AI/ML development process, highlights common best
practices for AI/ML model evaluation, and makes recommendations to DoD
evaluators to ensure the deployment of robust AI/ML capabilities for national
security needs.

    

### [[2107.02890] From Zero to The Hero: A Collaborative Market Aware Recommendation System for Crowd Workers](http://arxiv.org/abs/2107.02890)


  The success of software crowdsourcing depends on active and trustworthy pool
of worker supply. The uncertainty of crowd workers' behaviors makes it
challenging to predict workers' success and plan accordingly. In a competitive
crowdsourcing marketplace, competition for success over shared tasks adds
another layer of uncertainty in crowd workers' decision-making process.
Preliminary analysis on software worker behaviors reveals an alarming task
dropping rate of 82.9%. These factors lead to the need for an automated
recommendation system for CSD workers to improve the visibility and
predictability of their success in the competition. To that end, this paper
proposes a collaborative recommendation system for crowd workers. The proposed
recommendation system method uses five input metrics based on workers'
collaboration history in the pool, workers' preferences in taking tasks in
terms of monetary prize and duration, workers' specialty, and workers'
proficiency. The proposed method then recommends the most suitable tasks for a
worker to compete on based on workers' probability of success in the task.
Experimental results on 260 active crowd workers demonstrate that just
following the top three success probabilities of task recommendations, workers
can achieve success up to 86%

    

### [[2107.02894] Adversarial Machine Learning for Cybersecurity and Computer Vision: Current Developments and Challenges](http://arxiv.org/abs/2107.02894)


  We provide a comprehensive overview of adversarial machine learning focusing
on two application domains, i.e., cybersecurity and computer vision. Research
in adversarial machine learning addresses a significant threat to the wide
application of machine learning techniques -- they are vulnerable to carefully
crafted attacks from malicious adversaries. For example, deep neural networks
fail to correctly classify adversarial images, which are generated by adding
imperceptible perturbations to clean images.We first discuss three main
categories of attacks against machine learning techniques -- poisoning attacks,
evasion attacks, and privacy attacks. Then the corresponding defense approaches
are introduced along with the weakness and limitations of the existing defense
approaches. We notice adversarial samples in cybersecurity and computer vision
are fundamentally different. While adversarial samples in cybersecurity often
have different properties/distributions compared with training data,
adversarial images in computer vision are created with minor input
perturbations. This further complicates the development of robust learning
techniques, because a robust learning technique must withstand different types
of attacks.

    

### [[2107.02895] Bio-Inspired Adversarial Attack Against Deep Neural Networks](http://arxiv.org/abs/2107.02895)


  The paper develops a new adversarial attack against deep neural networks
(DNN), based on applying bio-inspired design to moving physical objects. To the
best of our knowledge, this is the first work to introduce physical attacks
with a moving object. Instead of following the dominating attack strategy in
the existing literature, i.e., to introduce minor perturbations to a digital
input or a stationary physical object, we show two new successful attack
strategies in this paper. We show by superimposing several patterns onto one
physical object, a DNN becomes confused and picks one of the patterns to assign
a class label. Our experiment with three flapping wing robots demonstrates the
possibility of developing an adversarial camouflage to cause a targeted mistake
by DNN. We also show certain motion can reduce the dependency among consecutive
frames in a video and make an object detector "blind", i.e., not able to detect
an object exists in the video. Hence in a successful physical attack against
DNN, targeted motion against the system should also be considered.

    

### [[2107.02896] Efficient Detection of Botnet Traffic by features selection and Decision Trees](http://arxiv.org/abs/2107.02896)


  Botnets are one of the online threats with the biggest presence, causing
billionaire losses to global economies. Nowadays, the increasing number of
devices connected to the Internet makes it necessary to analyze large amounts
of network traffic data. In this work, we focus on increasing the performance
on botnet traffic classification by selecting those features that further
increase the detection rate. For this purpose we use two feature selection
techniques, Information Gain and Gini Importance, which led to three
pre-selected subsets of five, six and seven features. Then, we evaluate the
three feature subsets along with three models, Decision Tree, Random Forest and
k-Nearest Neighbors. To test the performance of the three feature vectors and
the three models we generate two datasets based on the CTU-13 dataset, namely
QB-CTU13 and EQB-CTU13. We measure the performance as the macro averaged F1
score over the computational time required to classify a sample. The results
show that the highest performance is achieved by Decision Trees using a five
feature set which obtained a mean F1 score of 85% classifying each sample in an
average time of 0.78 microseconds.

    

### [[2107.02897] Bi-Level Poisoning Attack Model and Countermeasure for Appliance Consumption Data of Smart Homes](http://arxiv.org/abs/2107.02897)


  Accurate building energy prediction is useful in various applications
starting from building energy automation and management to optimal storage
control. However, vulnerabilities should be considered when designing building
energy prediction models, as intelligent attackers can deliberately influence
the model performance using sophisticated attack models. These may consequently
degrade the prediction accuracy, which may affect the efficiency and
performance of the building energy management systems. In this paper, we
investigate the impact of bi-level poisoning attacks on regression models of
energy usage obtained from household appliances. Furthermore, an effective
countermeasure against the poisoning attacks on the prediction model is
proposed in this paper. Attacks and defenses are evaluated on a benchmark
dataset. Experimental results show that an intelligent cyber-attacker can
poison the prediction model to manipulate the decision. However, our proposed
solution successfully ensures defense against such poisoning attacks
effectively compared to other benchmark techniques.

    

### [[2107.02908] Particle Convolution for High Energy Physics](http://arxiv.org/abs/2107.02908)


  We introduce the Particle Convolution Network (PCN), a new type of
equivariant neural network layer suitable for many tasks in jet physics. The
particle convolution layer can be viewed as an extension of Deep Sets and
Energy Flow network architectures, in which the permutation-invariant operator
is promoted to a group convolution. While the PCN can be implemented for
various kinds of symmetries, we consider the specific case of rotation about
the jet axis the $\eta - \phi$ plane. In two standard benchmark tasks, q/g
tagging and top tagging, we show that the rotational PCN (rPCN) achieves
performance comparable to graph networks such as ParticleNet. Moreover, we show
that it is possible to implement an IRC-safe rPCN, which significantly
outperforms existing IRC-safe tagging methods on both tasks. We speculate that
by generalizing the PCN to include additional convolutional symmetries relevant
to jet physics, it may outperform the current state-of-the-art set by graph
networks, while offering a new degree of control over physically-motivated
inductive biases.

    

### [[2107.02911] Scaling up Continuous-Time Markov Chains Helps Resolve Underspecification](http://arxiv.org/abs/2107.02911)


  Modeling the time evolution of discrete sets of items (e.g., genetic
mutations) is a fundamental problem in many biomedical applications. We
approach this problem through the lens of continuous-time Markov chains, and
show that the resulting learning task is generally underspecified in the usual
setting of cross-sectional data. We explore a perhaps surprising remedy:
including a number of additional independent items can help determine time
order, and hence resolve underspecification. This is in sharp contrast to the
common practice of limiting the analysis to a small subset of relevant items,
which is followed largely due to poor scaling of existing methods. To put our
theoretical insight into practice, we develop an approximate likelihood
maximization method for learning continuous-time Markov chains, which can scale
to hundreds of items and is orders of magnitude faster than previous methods.
We demonstrate the effectiveness of our approach on synthetic and real cancer
data.

    

### [[2107.02912] Supervised Bayesian Specification Inference from Demonstrations](http://arxiv.org/abs/2107.02912)


  When observing task demonstrations, human apprentices are able to identify
whether a given task is executed correctly long before they gain expertise in
actually performing that task. Prior research into learning from demonstrations
(LfD) has failed to capture this notion of the acceptability of a task's
execution; meanwhile, temporal logics provide a flexible language for
expressing task specifications. Inspired by this, we present Bayesian
specification inference, a probabilistic model for inferring task specification
as a temporal logic formula. We incorporate methods from probabilistic
programming to define our priors, along with a domain-independent likelihood
function to enable sampling-based inference. We demonstrate the efficacy of our
model for inferring specifications, with over 90% similarity observed between
the inferred specification and the ground truth, both within a synthetic domain
and during a real-world table setting task.

    

### [[2107.02919] Distributed stochastic optimization with large delays](http://arxiv.org/abs/2107.02919)


  One of the most widely used methods for solving large-scale stochastic
optimization problems is distributed asynchronous stochastic gradient descent
(DASGD), a family of algorithms that result from parallelizing stochastic
gradient descent on distributed computing architectures (possibly)
asychronously. However, a key obstacle in the efficient implementation of DASGD
is the issue of delays: when a computing node contributes a gradient update,
the global model parameter may have already been updated by other nodes several
times over, thereby rendering this gradient information stale. These delays can
quickly add up if the computational throughput of a node is saturated, so the
convergence of DASGD may be compromised in the presence of large delays. Our
first contribution is that, by carefully tuning the algorithm's step-size,
convergence to the critical set is still achieved in mean square, even if the
delays grow unbounded at a polynomial rate. We also establish finer results in
a broad class of structured optimization problems (called variationally
coherent), where we show that DASGD converges to a global optimum with
probability $1$ under the same delay assumptions. Together, these results
contribute to the broad landscape of large-scale non-convex stochastic
optimization by offering state-of-the-art theoretical guarantees and providing
insights for algorithm design.

    

### [[2107.02926] Solution of Physics-based Bayesian Inverse Problems with Deep Generative Priors](http://arxiv.org/abs/2107.02926)


  Inverse problems are notoriously difficult to solve because they can have no
solutions, multiple solutions, or have solutions that vary significantly in
response to small perturbations in measurements. Bayesian inference, which
poses an inverse problem as a stochastic inference problem, addresses these
difficulties and provides quantitative estimates of the inferred field and the
associated uncertainty. However, it is difficult to employ when inferring
vectors of large dimensions, and/or when prior information is available through
previously acquired samples. In this paper, we describe how deep generative
adversarial networks can be used to represent the prior distribution in
Bayesian inference and overcome these challenges. We apply these ideas to
inverse problems that are diverse in terms of the governing physical
principles, sources of prior knowledge, type of measurement, and the extent of
available information about measurement noise. In each case we apply the
proposed approach to infer the most likely solution and quantitative estimates
of uncertainty.

    

### [[2107.02943] Scalable Teacher Forcing Network for Semi-Supervised Large Scale Data Streams](http://arxiv.org/abs/2107.02943)


  The large-scale data stream problem refers to high-speed information flow
which cannot be processed in scalable manner under a traditional computing
platform. This problem also imposes expensive labelling cost making the
deployment of fully supervised algorithms unfeasible. On the other hand, the
problem of semi-supervised large-scale data streams is little explored in the
literature because most works are designed in the traditional single-node
computing environments while also being fully supervised approaches. This paper
offers Weakly Supervised Scalable Teacher Forcing Network (WeScatterNet) to
cope with the scarcity of labelled samples and the large-scale data streams
simultaneously. WeScatterNet is crafted under distributed computing platform of
Apache Spark with a data-free model fusion strategy for model compression after
parallel computing stage. It features an open network structure to address the
global and local drift problems while integrating a data augmentation,
annotation and auto-correction ($DA^3$) method for handling partially labelled
data streams. The performance of WeScatterNet is numerically evaluated in the
six large-scale data stream problems with only $25\%$ label proportions. It
shows highly competitive performance even if compared with fully supervised
learners with $100\%$ label proportions.

    

### [[2107.02951] Universal Approximation for Log-concave Distributions using Well-conditioned Normalizing Flows](http://arxiv.org/abs/2107.02951)


  Normalizing flows are a widely used class of latent-variable generative
models with a tractable likelihood. Affine-coupling (Dinh et al, 2014-16)
models are a particularly common type of normalizing flows, for which the
Jacobian of the latent-to-observable-variable transformation is triangular,
allowing the likelihood to be computed in linear time. Despite the widespread
usage of affine couplings, the special structure of the architecture makes
understanding their representational power challenging. The question of
universal approximation was only recently resolved by three parallel papers
(Huang et al.,2020;Zhang et al.,2020;Koehler et al.,2020) -- who showed
reasonably regular distributions can be approximated arbitrarily well using
affine couplings -- albeit with networks with a nearly-singular Jacobian. As
ill-conditioned Jacobians are an obstacle for likelihood-based training, the
fundamental question remains: which distributions can be approximated using
well-conditioned affine coupling flows?
In this paper, we show that any log-concave distribution can be approximated
using well-conditioned affine-coupling flows. In terms of proof techniques, we
uncover and leverage deep connections between affine coupling architectures,
underdamped Langevin dynamics (a stochastic differential equation often used to
sample from Gibbs measures) and Hénon maps (a structured dynamical system
that appears in the study of symplectic diffeomorphisms). Our results also
inform the practice of training affine couplings: we approximate a padded
version of the input distribution with iid Gaussians -- a strategy which
Koehler et al.(2020) empirically observed to result in better-conditioned
flows, but had hitherto no theoretical grounding. Our proof can thus be seen as
providing theoretical evidence for the benefits of Gaussian padding when
training normalizing flows.

    

### [[2107.02961] Immunization of Pruning Attack in DNN Watermarking Using Constant Weight Code](http://arxiv.org/abs/2107.02961)


  To ensure protection of the intellectual property rights of DNN models,
watermarking techniques have been investigated to insert side-information into
the models without seriously degrading the performance of original task. One of
the threats for the DNN watermarking is the pruning attack such that less
important neurons in the model are pruned to make it faster and more compact as
well as to remove the watermark. In this study, we investigate a channel coding
approach to resist the pruning attack. As the channel model is completely
different from conventional models like digital images, it has been an open
problem what kind of encoding method is suitable for DNN watermarking. A novel
encoding approach by using constant weight codes to immunize the effects of
pruning attacks is presented. To the best of our knowledge, this is the first
study that introduces an encoding technique for DNN watermarking to make it
robust against pruning attacks.

    

### [[2107.02968] Deep Extrapolation for Attribute-Enhanced Generation](http://arxiv.org/abs/2107.02968)


  Attribute extrapolation in sample generation is challenging for deep neural
networks operating beyond the training distribution. We formulate a new task
for extrapolation in sequence generation, focusing on natural language and
proteins, and propose GENhance, a generative framework that enhances attributes
through a learned latent space. Trained on movie reviews and a computed protein
stability dataset, GENhance can generate strongly-positive text reviews and
highly stable protein sequences without being exposed to similar data during
training. We release our benchmark tasks and models to contribute to the study
of generative modeling extrapolation and data-driven design in biology and
chemistry.

    

### [[2107.02970] GAN-based Data Augmentation for Chest X-ray Classification](http://arxiv.org/abs/2107.02970)


  A common problem in computer vision -- particularly in medical applications
-- is a lack of sufficiently diverse, large sets of training data. These
datasets often suffer from severe class imbalance. As a result, networks often
overfit and are unable to generalize to novel examples. Generative Adversarial
Networks (GANs) offer a novel method of synthetic data augmentation. In this
work, we evaluate the use of GAN- based data augmentation to artificially
expand the CheXpert dataset of chest radiographs. We compare performance to
traditional augmentation and find that GAN-based augmentation leads to higher
downstream performance for underrepresented classes. Furthermore, we see that
this result is pronounced in low data regimens. This suggests that GAN-based
augmentation a promising area of research to improve network performance when
data collection is prohibitively expensive.

    

### [[2107.02974] RAM-VO: Less is more in Visual Odometry](http://arxiv.org/abs/2107.02974)


  Building vehicles capable of operating without human supervision requires the
determination of the agent's pose. Visual Odometry (VO) algorithms estimate the
egomotion using only visual changes from the input images. The most recent VO
methods implement deep-learning techniques using convolutional neural networks
(CNN) extensively, which add a substantial cost when dealing with
high-resolution images. Furthermore, in VO tasks, more input data does not mean
a better prediction; on the contrary, the architecture may filter out useless
information. Therefore, the implementation of computationally efficient and
lightweight architectures is essential. In this work, we propose the RAM-VO, an
extension of the Recurrent Attention Model (RAM) for visual odometry tasks.
RAM-VO improves the visual and temporal representation of information and
implements the Proximal Policy Optimization (PPO) algorithm to learn robust
policies. The results indicate that RAM-VO can perform regressions with six
degrees of freedom from monocular input images using approximately 3 million
parameters. In addition, experiments on the KITTI dataset demonstrate that
RAM-VO achieves competitive results using only 5.7% of the available visual
information.

    

### [[2107.02990] Test for non-negligible adverse shifts](http://arxiv.org/abs/2107.02990)


  Statistical tests for dataset shift are susceptible to false alarms: they are
sensitive to minor differences where there is in fact adequate sample coverage
and predictive performance. We propose instead a robust framework for tests of
dataset shift based on outlier scores, D-SOS for short. D-SOS detects adverse
shifts and can identify false alarms caused by benign ones. It posits that a
new (test) sample is not substantively worse than an old (training) sample, and
not that the two are equal. The key idea is to reduce observations to outlier
scores and compare contamination rates. Beyond comparing distributions, users
can define what worse means in terms of predictive performance and other
relevant notions. We show how versatile and practical D-SOS is for a wide range
of real and simulated datasets. Unlike tests of equal distribution and of
goodness-of-fit, the D-SOS tests are uniquely tailored to serve as robust
performance metrics to monitor model drift and dataset shift.

    

### [[2107.02991] Keiki: Towards Realistic Danmaku Generation via Sequential GANs](http://arxiv.org/abs/2107.02991)


  Search-based procedural content generation methods have recently been
introduced for the autonomous creation of bullet hell games. Search-based
methods, however, can hardly model patterns of danmakus -- the bullet hell
shooting entity -- explicitly and the resulting levels often look
non-realistic. In this paper, we present a novel bullet hell game platform
named Keiki, which allows the representation of danmakus as a parametric
sequence which, in turn, can model the sequential behaviours of danmakus. We
employ three types of generative adversarial networks (GANs) and test Keiki
across three metrics designed to quantify the quality of the generated
danmakus. The time-series GAN and periodic spatial GAN show different yet
competitive performance in terms of the evaluation metrics adopted, their
deviation from human-designed danmakus, and the diversity of generated
danmakus. The preliminary experimental studies presented here showcase that
potential of time-series GANs for sequential content generation in games.

    

### [[2107.03003] Harnessing Heterogeneity: Learning from Decomposed Feedback in Bayesian Modeling](http://arxiv.org/abs/2107.03003)


  There is significant interest in learning and optimizing a complex system
composed of multiple sub-components, where these components may be agents or
autonomous sensors. Among the rich literature on this topic, agent-based and
domain-specific simulations can capture complex dynamics and subgroup
interaction, but optimizing over such simulations can be computationally and
algorithmically challenging. Bayesian approaches, such as Gaussian processes
(GPs), can be used to learn a computationally tractable approximation to the
underlying dynamics but typically neglect the detailed information about
subgroups in the complicated system. We attempt to find the best of both worlds
by proposing the idea of decomposed feedback, which captures group-based
heterogeneity and dynamics. We introduce a novel decomposed GP regression to
incorporate the subgroup decomposed feedback. Our modified regression has
provably lower variance -- and thus a more accurate posterior -- compared to
previous approaches; it also allows us to introduce a decomposed GP-UCB
optimization algorithm that leverages subgroup feedback. The Bayesian nature of
our method makes the optimization algorithm trackable with a theoretical
guarantee on convergence and no-regret property. To demonstrate the wide
applicability of this work, we execute our algorithm on two disparate social
problems: infectious disease control in a heterogeneous population and
allocation of distributed weather sensors. Experimental results show that our
new method provides significant improvement compared to the state-of-the-art.

    

### [[2107.03006] Structured Denoising Diffusion Models in Discrete State-Spaces](http://arxiv.org/abs/2107.03006)


  Denoising diffusion probabilistic models (DDPMs) (Ho et al. 2020) have shown
impressive results on image and waveform generation in continuous state spaces.
Here, we introduce Discrete Denoising Diffusion Probabilistic Models (D3PMs),
diffusion-like generative models for discrete data that generalize the
multinomial diffusion model of Hoogeboom et al. 2021, by going beyond
corruption processes with uniform transition probabilities. This includes
corruption with transition matrices that mimic Gaussian kernels in continuous
space, matrices based on nearest neighbors in embedding space, and matrices
that introduce absorbing states. The third allows us to draw a connection
between diffusion models and autoregressive and mask-based generative models.
We show that the choice of transition matrix is an important design decision
that leads to improved results in image and text domains. We also introduce a
new loss function that combines the variational lower bound with an auxiliary
cross entropy loss. For text, this model class achieves strong results on
character-level text generation while scaling to large vocabularies on LM1B. On
the image dataset CIFAR-10, our models approach the sample quality and exceed
the log-likelihood of the continuous-space DDPM model.

    

### [[2107.03015] Evaluating the progress of Deep Reinforcement Learning in the real world: aligning domain-agnostic and domain-specific research](http://arxiv.org/abs/2107.03015)


  Deep Reinforcement Learning (DRL) is considered a potential framework to
improve many real-world autonomous systems; it has attracted the attention of
multiple and diverse fields. Nevertheless, the successful deployment in the
real world is a test most of DRL models still need to pass. In this work we
focus on this issue by reviewing and evaluating the research efforts from both
domain-agnostic and domain-specific communities. On one hand, we offer a
comprehensive summary of DRL challenges and summarize the different proposals
to mitigate them; this helps identifying five gaps of domain-agnostic research.
On the other hand, from the domain-specific perspective, we discuss different
success stories and argue why other models might fail to be deployed. Finally,
we take up on ways to move forward accounting for both perspectives.

    

### [[2107.03018] Exact Learning Augmented Naive Bayes Classifier](http://arxiv.org/abs/2107.03018)


  Earlier studies have shown that classification accuracies of Bayesian
networks (BNs) obtained by maximizing the conditional log likelihood (CLL) of a
class variable, given the feature variables, were higher than those obtained by
maximizing the marginal likelihood (ML). However, differences between the
performances of the two scores in the earlier studies may be attributed to the
fact that they used approximate learning algorithms, not exact ones. This paper
compares the classification accuracies of BNs with approximate learning using
CLL to those with exact learning using ML. The results demonstrate that the
classification accuracies of BNs obtained by maximizing the ML are higher than
those obtained by maximizing the CLL for large data. However, the results also
demonstrate that the classification accuracies of exact learning BNs using the
ML are much worse than those of other methods when the sample size is small and
the class variable has numerous parents. To resolve the problem, we propose an
exact learning augmented naive Bayes classifier (ANB), which ensures a class
variable with no parents. The proposed method is guaranteed to asymptotically
estimate the identical class posterior to that of the exactly learned BN.
Comparison experiments demonstrated the superior performance of the proposed
method.

    

### [[2107.03019] SelfCF: A Simple Framework for Self-supervised Collaborative Filtering](http://arxiv.org/abs/2107.03019)


  Collaborative filtering (CF) is widely used to learn an informative latent
representation of a user or item from observed interactions. Existing CF-based
methods commonly adopt negative sampling to discriminate different items. That
is, observed user-item pairs are treated as positive instances; unobserved
pairs are considered as negative instances and are sampled under a defined
distribution for training. Training with negative sampling on large datasets is
computationally expensive. Further, negative items should be carefully sampled
under the defined distribution, in order to avoid selecting an observed
positive item in the training dataset. Unavoidably, some negative items sampled
from the training dataset could be positive in the test set. Recently,
self-supervised learning (SSL) has emerged as a powerful tool to learn a model
without negative samples. In this paper, we propose a self-supervised
collaborative filtering framework (SelfCF), that is specially designed for
recommender scenario with implicit feedback. The main idea of SelfCF is to
augment the output embeddings generated by backbone networks, because it is
infeasible to augment raw input of user/item ids. We propose and study three
output perturbation techniques that can be applied to different types of
backbone networks including both traditional CF models and graph-based models.
By encapsulating two popular recommendation models into the framework, our
experiments on three datasets show that the best performance of our framework
is comparable or better than the supervised counterpart. We also show that
SelfCF can boost up the performance by up to 8.93\% on average, compared with
another self-supervised framework as the baseline. Source codes are available
at: this https URL.

    

### [[2107.03022] On Codomain Separability and Label Inference from (Noisy) Loss Functions](http://arxiv.org/abs/2107.03022)


  Machine learning classifiers rely on loss functions for performance
evaluation, often on a private (hidden) dataset. Label inference was recently
introduced as the problem of reconstructing the ground truth labels of this
private dataset from just the (possibly perturbed) loss function values
evaluated at chosen prediction vectors, without any other access to the hidden
dataset. Existing results have demonstrated this inference is possible on
specific loss functions like the cross-entropy loss. In this paper, we
introduce the notion of codomain separability to formally study the necessary
and sufficient conditions under which label inference is possible from any
(noisy) loss function values. Using this notion, we show that for many commonly
used loss functions, including multiclass cross-entropy with common activation
functions and some Bregman divergence-based losses, it is possible to design
label inference attacks for arbitrary noise levels. We demonstrate that these
attacks can also be carried out through actual neural network models, and
argue, both formally and empirically, the role of finite precision arithmetic
in this setting.

    

### [[2107.03049] ADAPT : Awesome Domain Adaptation Python Toolbox](http://arxiv.org/abs/2107.03049)


  ADAPT is an open-source python library providing the implementation of
several domain adaptation methods. The library is suited for scikit-learn
estimator object (object which implement fit and predict methods) and
tensorflow models. Most of the implemented methods are developed in an
estimator agnostic fashion, offering various possibilities adapted to multiple
usage. The library offers three modules corresponding to the three principal
strategies of domain adaptation: (i) feature-based containing methods
performing feature transformation; (ii) instance-based with the implementation
of reweighting techniques and (iii) parameter-based proposing methods to adapt
pre-trained models to novel observations. A full documentation is proposed
online this https URL with gallery of examples. Besides,
the library presents an high test coverage.

    

### [[2107.03050] Controlled Caption Generation for Images Through Adversarial Attacks](http://arxiv.org/abs/2107.03050)


  Deep learning is found to be vulnerable to adversarial examples. However, its
adversarial susceptibility in image caption generation is under-explored. We
study adversarial examples for vision and language models, which typically
adopt an encoder-decoder framework consisting of two major components: a
Convolutional Neural Network (i.e., CNN) for image feature extraction and a
Recurrent Neural Network (RNN) for caption generation. In particular, we
investigate attacks on the visual encoder's hidden layer that is fed to the
subsequent recurrent network. The existing methods either attack the
classification layer of the visual encoder or they back-propagate the gradients
from the language model. In contrast, we propose a GAN-based algorithm for
crafting adversarial examples for neural image captioning that mimics the
internal representation of the CNN such that the resulting deep features of the
input image enable a controlled incorrect caption generation through the
recurrent network. Our contribution provides new insights for understanding
adversarial attacks on vision systems with language component. The proposed
method employs two strategies for a comprehensive evaluation. The first
examines if a neural image captioning system can be misled to output targeted
image captions. The second analyzes the possibility of keywords into the
predicted captions. Experiments show that our algorithm can craft effective
adversarial images based on the CNN hidden layers to fool captioning framework.
Moreover, we discover the proposed attack to be highly transferable. Our work
leads to new robustness implications for neural image captioning.

    

### [[2107.03066] Probabilistic partition of unity networks: clustering based deep approximation](http://arxiv.org/abs/2107.03066)


  Partition of unity networks (POU-Nets) have been shown capable of realizing
algebraic convergence rates for regression and solution of PDEs, but require
empirical tuning of training parameters. We enrich POU-Nets with a Gaussian
noise model to obtain a probabilistic generalization amenable to gradient-based
minimization of a maximum likelihood loss. The resulting architecture provides
spatial representations of both noiseless and noisy data as Gaussian mixtures
with closed form expressions for variance which provides an estimator of local
error. The training process yields remarkably sharp partitions of input space
based upon correlation of function values. This classification of training
points is amenable to a hierarchical refinement strategy that significantly
improves the localization of the regression, allowing for higher-order
polynomial approximation to be utilized. The framework scales more favorably to
large data sets as compared to Gaussian process regression and allows for
spatially varying uncertainty, leveraging the expressive power of deep neural
networks while bypassing expensive training associated with other probabilistic
deep learning methods. Compared to standard deep neural networks, the framework
demonstrates hp-convergence without the use of regularizers to tune the
localization of partitions. We provide benchmarks quantifying performance in
high/low-dimensions, demonstrating that convergence rates depend only on the
latent dimension of data within high-dimensional space. Finally, we introduce a
new open-source data set of PDE-based simulations of a semiconductor device and
perform unsupervised extraction of a physically interpretable reduced-order
basis.

    

### [[2107.03067] Distributed adaptive algorithm based on the asymmetric cost of error functions](http://arxiv.org/abs/2107.03067)


  In this paper, a family of novel diffusion adaptive estimation algorithm is
proposed from the asymmetric cost function perspective by combining diffusion
strategy and the linear-linear cost (LLC), quadratic-quadratic cost (QQC), and
linear-exponential cost (LEC), at all distributed network nodes, and named
diffusion LLCLMS (DLLCLMS), diffusion QQCLMS (DQQCLMS), and diffusion LECLMS
(DLECLMS), respectively. Then the stability of mean estimation error and
computational complexity of those three diffusion algorithms are analyzed
theoretically. Finally, several experiment simulation results are designed to
verify the superiority of those three proposed diffusion algorithms.
Experimental simulation results show that DLLCLMS, DQQCLMS, and DLECLMS
algorithms are more robust to the input signal and impulsive noise than the
DSELMS, DRVSSLMS, and DLLAD algorithms. In brief, theoretical analysis and
experiment results show that those proposed DLLCLMS, DQQCLMS, and DLECLMS
algorithms have superior performance when estimating the unknown linear system
under the changeable impulsive noise environments and different types of input
signals.

    

### [[2107.03080] Hub and Spoke Logistics Network Design for Urban Region with Clustering-Based Approach](http://arxiv.org/abs/2107.03080)


  This study aims to propose effective modeling and approach for designing a
logistics network in the urban area in order to offer an efficient flow
distribution network as a competitive strategy in the logistics industry where
demand is sensitive to both price and time. A multi-stage approach is
introduced to select the number of hubs and allocate spokes to the hubs for
flow distribution and hubs' location detection. Specifically, a fuzzy
clustering model with the objective function is to minimize the approximate
transportation cost is employed, in the next phase is to focus on balancing the
demand capacity among the hubs with the help of domain experts, afterward, the
facility location vehicle routing problems within the network is introduced. To
demonstrate the approach's advantages, an experiment was performed on the
designed network and its actual transportation cost for the real operational
data in which specific to the Ho Chi Minh city infrastructure conditions.
Additionally, we show the flexibility of the designed network in the flow
distribution and its computational experiments to develop the managerial
insights which contribute to the network design decision-making process.

    

### [[2107.03084] Discriminative Mutual Information Estimators for Channel Capacity Learning](http://arxiv.org/abs/2107.03084)


  Channel capacity plays a crucial role in the development of modern
communication systems as it represents the maximum rate at which information
can be reliably transmitted over a communication channel. Nevertheless, for the
majority of channels, finding a closed-form capacity expression remains an open
challenge. This is because it requires to carry out two formidable tasks a) the
computation of the mutual information between the channel input and output, and
b) its maximization with respect to the signal distribution at the channel
input. In this paper, we address both tasks. Inspired by implicit generative
models, we propose a novel cooperative framework to automatically learn the
channel capacity, for any type of memory-less channel. In particular, we
firstly develop a new methodology to estimate the mutual information directly
from a discriminator typically deployed to train adversarial networks, referred
to as discriminative mutual information estimator (DIME). Secondly, we include
the discriminator in a cooperative channel capacity learning framework,
referred to as CORTICAL, where a discriminator learns to distinguish between
dependent and independent channel input-output samples while a generator learns
to produce the optimal channel input distribution for which the discriminator
exhibits the best performance. Lastly, we prove that a particular choice of the
cooperative value function solves the channel capacity estimation problem.
Simulation results demonstrate that the proposed method offers high accuracy.

    

### [[2107.03090] RISAN: Robust Instance Specific Abstention Network](http://arxiv.org/abs/2107.03090)


  In this paper, we propose deep architectures for learning instance specific
abstain (reject option) binary classifiers. The proposed approach uses double
sigmoid loss function as described by Kulin Shah and Naresh Manwani in ("Online
Active Learning of Reject Option Classifiers", AAAI, 2020), as a performance
measure. We show that the double sigmoid loss is classification calibrated. We
also show that the excess risk of 0-d-1 loss is upper bounded by the excess
risk of double sigmoid loss. We derive the generalization error bounds for the
proposed architecture for reject option classifiers. To show the effectiveness
of the proposed approach, we experiment with several real world datasets. We
observe that the proposed approach not only performs comparable to the
state-of-the-art approaches, it is also robust against label noise. We also
provide visualizations to observe the important features learned by the network
corresponding to the abstaining decision.

    

### [[2107.03144] Neural Contextual Bandits without Regret](http://arxiv.org/abs/2107.03144)


  Contextual bandits are a rich model for sequential decision making given side
information, with important applications, e.g., in recommender systems. We
propose novel algorithms for contextual bandits harnessing neural networks to
approximate the unknown reward function. We resolve the open problem of proving
sublinear regret bounds in this setting for general context sequences,
considering both fully-connected and convolutional networks. To this end, we
first analyze NTK-UCB, a kernelized bandit optimization algorithm employing the
Neural Tangent Kernel (NTK), and bound its regret in terms of the NTK maximum
information gain $\gamma_T$, a complexity parameter capturing the difficulty of
learning. Our bounds on $\gamma_T$ for the NTK may be of independent interest.
We then introduce our neural network based algorithm NN-UCB, and show that its
regret closely tracks that of NTK-UCB. Under broad non-parametric assumptions
about the reward function, our approach converges to the optimal policy at a
$\tilde{\mathcal{O}}(T^{-1/2d})$ rate, where $d$ is the dimension of the
context.

    

### [[2107.03145] A Deep Residual Star Generative Adversarial Network for multi-domain Image Super-Resolution](http://arxiv.org/abs/2107.03145)


  Recently, most of state-of-the-art single image super-resolution (SISR)
methods have attained impressive performance by using deep convolutional neural
networks (DCNNs). The existing SR methods have limited performance due to a
fixed degradation settings, i.e. usually a bicubic downscaling of
low-resolution (LR) image. However, in real-world settings, the LR degradation
process is unknown which can be bicubic LR, bilinear LR, nearest-neighbor LR,
or real LR. Therefore, most SR methods are ineffective and inefficient in
handling more than one degradation settings within a single network. To handle
the multiple degradation, i.e. refers to multi-domain image super-resolution,
we propose a deep Super-Resolution Residual StarGAN (SR2*GAN), a novel and
scalable approach that super-resolves the LR images for the multiple LR domains
using only a single model. The proposed scheme is trained in a StarGAN like
network topology with a single generator and discriminator networks. We
demonstrate the effectiveness of our proposed approach in quantitative and
qualitative experiments compared to other state-of-the-art methods.

    

### [[2107.03176] On Training Instance Selection for Few-Shot Neural Text Generation](http://arxiv.org/abs/2107.03176)


  Large-scale pretrained language models have led to dramatic improvements in
text generation. Impressive performance can be achieved by finetuning only on a
small number of instances (few-shot setting). Nonetheless, almost all previous
work simply applies random sampling to select the few-shot training instances.
Little to no attention has been paid to the selection strategies and how they
would affect model performance. In this work, we present a study on training
instance selection in few-shot neural text generation. The selection decision
is made based only on the unlabeled data so as to identify the most worthwhile
data points that should be annotated under some budget of labeling cost. Based
on the intuition that the few-shot training instances should be diverse and
representative of the entire data distribution, we propose a simple selection
strategy with K-means clustering. We show that even with the naive
clustering-based approach, the generation models consistently outperform random
sampling on three text generation tasks: data-to-text generation, document
summarization and question generation. We hope that this work will call for
more attention on this largely unexplored area.

    

### [[2107.03182] Urban Tree Species Classification Using Aerial Imagery](http://arxiv.org/abs/2107.03182)


  Urban trees help regulate temperature, reduce energy consumption, improve
urban air quality, reduce wind speeds, and mitigating the urban heat island
effect. Urban trees also play a key role in climate change mitigation and
global warming by capturing and storing atmospheric carbon-dioxide which is the
largest contributor to greenhouse gases. Automated tree detection and species
classification using aerial imagery can be a powerful tool for sustainable
forest and urban tree management. Hence, This study first offers a pipeline for
generating labelled dataset of urban trees using Google Map's aerial images and
then investigates how state of the art deep Convolutional Neural Network models
such as VGG and ResNet handle the classification problem of urban tree aerial
images under different parameters. Experimental results show our best model
achieves an average accuracy of 60% over 6 tree species.

    

### [[2107.03183] A Closed-Form Approximation to the Conjugate Prior of the Dirichlet and Beta Distributions](http://arxiv.org/abs/2107.03183)


  We derive the conjugate prior of the Dirichlet and beta distributions and
explore it with numerical examples to gain an intuitive understanding of the
distribution itself, its hyperparameters, and conditions concerning its
convergence. Due to the prior's intractability, we proceed to define and
analyze a closed-form approximation. Finally, we provide an algorithm
implementing this approximation that enables fully tractable Bayesian conjugate
treatment of Dirichlet and beta likelihoods without the need for Monte Carlo
simulations.

    

### [[2107.03187] Intensity Prediction of Tropical Cyclones using Long Short-Term Memory Network](http://arxiv.org/abs/2107.03187)


  Tropical cyclones can be of varied intensity and cause a huge loss of lives
and property if the intensity is high enough. Therefore, the prediction of the
intensity of tropical cyclones advance in time is of utmost importance. We
propose a novel stacked bidirectional long short-term memory network (BiLSTM)
based model architecture to predict the intensity of a tropical cyclone in
terms of Maximum surface sustained wind speed (MSWS). The proposed model can
predict MSWS well advance in time (up to 72 h) with very high accuracy. We have
applied the model on tropical cyclones in the North Indian Ocean from 1982 to
2018 and checked its performance on two recent tropical cyclones, namely, Fani
and Vayu. The model predicts MSWS (in knots) for the next 3, 12, 24, 36, 48,
60, and 72 hours with a mean absolute error of 1.52, 3.66, 5.88, 7.42, 8.96,
10.15, and 11.92, respectively.

    

### [[2107.03190] Nested Counterfactual Identification from Arbitrary Surrogate Experiments](http://arxiv.org/abs/2107.03190)


  The Ladder of Causation describes three qualitatively different types of
activities an agent may be interested in engaging in, namely, seeing
(observational), doing (interventional), and imagining (counterfactual) (Pearl
and Mackenzie, 2018). The inferential challenge imposed by the causal hierarchy
is that data is collected by an agent observing or intervening in a system
(layers 1 and 2), while its goal may be to understand what would have happened
had it taken a different course of action, contrary to what factually ended up
happening (layer 3). While there exists a solid understanding of the conditions
under which cross-layer inferences are allowed from observations to
interventions, the results are somewhat scarcer when targeting counterfactual
quantities. In this paper, we study the identification of nested
counterfactuals from an arbitrary combination of observations and experiments.
Specifically, building on a more explicit definition of nested counterfactuals,
we prove the counterfactual unnesting theorem (CUT), which allows one to map
arbitrary nested counterfactuals to unnested ones. For instance, applications
in mediation and fairness analysis usually evoke notions of direct, indirect,
and spurious effects, which naturally require nesting. Second, we introduce a
sufficient and necessary graphical condition for counterfactual identification
from an arbitrary combination of observational and experimental distributions.
Lastly, we develop an efficient and complete algorithm for identifying nested
counterfactuals; failure of the algorithm returning an expression for a query
implies it is not identifiable.

    

### [[2107.03207] Bias-Tolerant Fair Classification](http://arxiv.org/abs/2107.03207)


  The label bias and selection bias are acknowledged as two reasons in data
that will hinder the fairness of machine-learning outcomes. The label bias
occurs when the labeling decision is disturbed by sensitive features, while the
selection bias occurs when subjective bias exists during the data sampling.
Even worse, models trained on such data can inherit or even intensify the
discrimination. Most algorithmic fairness approaches perform an empirical risk
minimization with predefined fairness constraints, which tends to trade-off
accuracy for fairness. However, such methods would achieve the desired fairness
level with the sacrifice of the benefits (receive positive outcomes) for
individuals affected by the bias. Therefore, we propose a
Bias-TolerantFAirRegularizedLoss (B-FARL), which tries to regain the benefits
using data affected by label bias and selection bias. B-FARL takes the biased
data as input, calls a model that approximates the one trained with fair but
latent data, and thus prevents discrimination without constraints required. In
addition, we show the effective components by decomposing B-FARL, and we
utilize the meta-learning framework for the B-FARL optimization. The
experimental results on real-world datasets show that our method is empirically
effective in improving fairness towards the direction of true but latent
labels.

    

### [[2107.03217] Combined Global and Local Search for Optimization with Gaussian Process Models](http://arxiv.org/abs/2107.03217)


  Gaussian process (GP) model based optimization is widely applied in
simulation and machine learning. In general, it first estimates a GP model
based on a few observations from the true response and then employs this model
to guide the search, aiming to quickly locate the global optimum. Despite its
successful applications, it has several limitations that may hinder its broader
usage. First, building an accurate GP model can be difficult and
computationally expensive, especially when the response function is multi-modal
or varies significantly over the design space. Second, even with an appropriate
model, the search process can be trapped in suboptimal regions before moving to
the global optimum due to the excessive effort spent around the current best
solution. In this work, we adopt the Additive Global and Local GP (AGLGP) model
in the optimization framework. The model is rooted in the inducing-points-based
GP sparse approximations and is combined with independent local models in
different regions. With these properties, the AGLGP model is suitable for
multi-modal responses with relatively large data sizes. Based on this AGLGP
model, we propose a Combined Global and Local search for Optimization (CGLO)
algorithm. It first divides the whole design space into disjoint local regions
and identifies a promising region with the global model. Next, a local model in
the selected region is fit to guide detailed search within this region. The
algorithm then switches back to the global step when a good local solution is
found. The global and local natures of CGLO enable it to enjoy the benefits of
both global and local search to efficiently locate the global optimum.

    

### [[2107.03220] Joint Embedding of Structural and Functional Brain Networks with Graph Neural Networks for Mental Illness Diagnosis](http://arxiv.org/abs/2107.03220)


  Multimodal brain networks characterize complex connectivities among different
brain regions from both structural and functional aspects and provide a new
means for mental disease analysis. Recently, Graph Neural Networks (GNNs) have
become a de facto model for analyzing graph-structured data. However, how to
employ GNNs to extract effective representations from brain networks in
multiple modalities remains rarely explored. Moreover, as brain networks
provide no initial node features, how to design informative node attributes and
leverage edge weights for GNNs to learn is left unsolved. To this end, we
develop a novel multiview GNN for multimodal brain networks. In particular, we
regard each modality as a view for brain networks and employ contrastive
learning for multimodal fusion. Then, we propose a GNN model which takes
advantage of the message passing scheme by propagating messages based on degree
statistics and brain region connectivities. Extensive experiments on two
real-world disease datasets (HIV and Bipolar) demonstrate the effectiveness of
our proposed method over state-of-the-art baselines.

    

### [[2107.03226] Graphing else matters: exploiting aspect opinions and ratings in explainable graph-based recommendations](http://arxiv.org/abs/2107.03226)


  The success of neural network embeddings has entailed a renewed interest in
using knowledge graphs for a wide variety of machine learning and information
retrieval tasks. In particular, current recommendation methods based on graph
embeddings have shown state-of-the-art performance. These methods commonly
encode latent rating patterns and content features. Different from previous
work, in this paper, we propose to exploit embeddings extracted from graphs
that combine information from ratings and aspect-based opinions expressed in
textual reviews. We then adapt and evaluate state-of-the-art graph embedding
techniques over graphs generated from Amazon and Yelp reviews on six domains,
outperforming baseline recommenders. Our approach has the advantage of
providing explanations which leverage aspect-based opinions given by users
about recommended items. Furthermore, we also provide examples of the
applicability of recommendations utilizing aspect opinions as explanations in a
visualization dashboard, which allows obtaining information about the most and
least liked aspects of similar users obtained from the embeddings of an input
graph.

    

### [[2107.03227] Scalable Data Balancing for Unlabeled Satellite Imagery](http://arxiv.org/abs/2107.03227)


  Data imbalance is a ubiquitous problem in machine learning. In large scale
collected and annotated datasets, data imbalance is either mitigated manually
by undersampling frequent classes and oversampling rare classes, or planned for
with imputation and augmentation techniques. In both cases balancing data
requires labels. In other words, only annotated data can be balanced.
Collecting fully annotated datasets is challenging, especially for large scale
satellite systems such as the unlabeled NASA's 35 PB Earth Imagery dataset.
Although the NASA Earth Imagery dataset is unlabeled, there are implicit
properties of the data source that we can rely on to hypothesize about its
imbalance, such as distribution of land and water in the case of the Earth's
imagery. We present a new iterative method to balance unlabeled data. Our
method utilizes image embeddings as a proxy for image labels that can be used
to balance data, and ultimately when trained increases overall accuracy.

    

### [[2107.03230] Coastal water quality prediction based on machine learning with feature interpretation and spatio-temporal analysis](http://arxiv.org/abs/2107.03230)


  Coastal water quality management is a public health concern, as poor coastal
water quality can harbor pathogens that are dangerous to human health.
Tourism-oriented countries need to actively monitor the condition of coastal
water at tourist popular sites during the summer season. In this study, routine
monitoring data of $Escherichia\ Coli$ and enterococci across 15 public beaches
in the city of Rijeka, Croatia, were used to build machine learning models for
predicting their levels based on environmental parameters as well as to
investigate their relationships with environmental stressors. Gradient Boosting
(Catboost, Xgboost), Random Forests, Support Vector Regression and Artificial
Neural Networks were trained with measurements from all sampling sites and used
to predict $E.\ Coli$ and enterococci values based on environmental features.
The evaluation of stability and generalizability with 10-fold cross validation
analysis of the machine learning models, showed that the Catboost algorithm
performed best with R$^2$ values of 0.71 and 0.68 for predicting $E.\ Coli$ and
enterococci, respectively, compared to other evaluated ML algorithms including
Xgboost, Random Forests, Support Vector Regression and Artificial Neural
Networks. We also use the SHapley Additive exPlanations technique to identify
and interpret which features have the most predictive power. The results show
that site salinity measured is the most important feature for forecasting both
$E.\ Coli$ and enterococci levels. Finally, the spatial and temporal accuracy
of both ML models were examined at sites with the lowest coastal water quality.
The spatial $E. Coli$ and enterococci models achieved strong R$^2$ values of
0.85 and 0.83, while the temporal models achieved R$^2$ values of 0.74 and
0.67. The temporal model also achieved moderate R$^2$ values of 0.44 and 0.46
at a site with high coastal water quality.

    

### [[2107.03248] DER Forecast using Privacy Preserving Federated Learning](http://arxiv.org/abs/2107.03248)


  With increasing penetration of Distributed Energy Resources (DERs) in grid
edge including renewable generation, flexible loads, and storage, accurate
prediction of distributed generation and consumption at the consumer level
becomes important. However, DER prediction based on the transmission of
customer level data, either repeatedly or in large amounts, is not feasible due
to privacy concerns. In this paper, a distributed machine learning approach,
Federated Learning, is proposed to carry out DER forecasting using a network of
IoT nodes, each of which transmits a model of the consumption and generation
patterns without revealing consumer data. We consider a simulation study which
includes 1000 DERs, and show that our method leads to an accurate prediction of
preserve consumer privacy, while still leading to an accurate forecast. We also
evaluate grid-specific performance metrics such as load swings and load
curtailment and show that our FL algorithm leads to satisfactory performance.
Simulations are also performed on the Pecan street dataset to demonstrate the
validity of the proposed approach on real data.

    

### [[2107.03250] Incorporating Label Uncertainty in Understanding Adversarial Robustness](http://arxiv.org/abs/2107.03250)


  A fundamental question in adversarial machine learning is whether a robust
classifier exists for a given task. A line of research has made progress
towards this goal by studying concentration of measure, but without considering
data labels. We argue that the standard concentration fails to fully
characterize the intrinsic robustness of a classification problem, since it
ignores data labels which are essential to any classification task. Building on
a novel definition of label uncertainty, we empirically demonstrate that error
regions induced by state-of-the-art models tend to have much higher label
uncertainty compared with randomly-selected subsets. This observation motivates
us to adapt a concentration estimation algorithm to account for label
uncertainty, resulting in more accurate intrinsic robustness measures for
benchmark image classification problems. We further provide empirical evidence
showing that adding an abstain option for classifiers based on label
uncertainty can help improve both the clean and robust accuracies of models.

    

### [[2107.03256] "Are you sure?": Preliminary Insights from Scaling Product Comparisons to Multiple Shops](http://arxiv.org/abs/2107.03256)


  Large eCommerce players introduced comparison tables as a new type of
recommendations. However, building comparisons at scale without pre-existing
training/taxonomy data remains an open challenge, especially within the
operational constraints of shops in the long tail. We present preliminary
results from building a comparison pipeline designed to scale in a multi-shop
scenario: we describe our design choices and run extensive benchmarks on
multiple shops to stress-test it. Finally, we run a small user study on
property selection and conclude by discussing potential improvements and
highlighting the questions that remain to be addressed.

    

### [[2107.03263] Episodic Bandits with Stochastic Experts](http://arxiv.org/abs/2107.03263)


  We study a version of the contextual bandit problem where an agent is given
soft control of a node in a graph-structured environment through a set of
stochastic expert policies. The agent interacts with the environment over
episodes, with each episode having different context distributions; this
results in the `best expert' changing across episodes. Our goal is to develop
an agent that tracks the best expert over episodes. We introduce the Empirical
Divergence-based UCB (ED-UCB) algorithm in this setting where the agent does
not have any knowledge of the expert policies or changes in context
distributions. With mild assumptions, we show that bootstrapping from
$\tilde{O}(N\log(NT^2\sqrt{E}))$ samples results in a regret of
$\tilde{O}(E(N+1) + \frac{N\sqrt{E}}{T^2})$. If the expert policies are known
to the agent a priori, then we can improve the regret to $\tilde{O}(EN)$
without requiring any bootstrapping. Our analysis also tightens pre-existing
logarithmic regret bounds to a problem-dependent constant in the non-episodic
setting when expert policies are known. We finally empirically validate our
findings through simulations.

    

### [[2107.03280] MD-split+: Practical Local Conformal Inference in High Dimensions](http://arxiv.org/abs/2107.03280)


  Quantifying uncertainty in model predictions is a common goal for
practitioners seeking more than just point predictions. One tool for
uncertainty quantification that requires minimal assumptions is conformal
inference, which can help create probabilistically valid prediction regions for
black box models. Classical conformal prediction only provides marginal
validity, whereas in many situations locally valid prediction regions are
desirable. Deciding how best to partition the feature space X when applying
localized conformal prediction is still an open question. We present MD-split+,
a practical local conformal approach that creates X partitions based on
localized model performance of conditional density estimation models. Our
method handles complex real-world data settings where such models may be
misspecified, and scales to high-dimensional inputs. We discuss how our local
partitions philosophically align with expected behavior from an unattainable
conditional conformal inference approach. We also empirically compare our
method against other local conformal approaches.

    

### [[2107.03297] Trans4E: Link Prediction on Scholarly Knowledge Graphs](http://arxiv.org/abs/2107.03297)


  The incompleteness of Knowledge Graphs (KGs) is a crucial issue affecting the
quality of AI-based services. In the scholarly domain, KGs describing research
publications typically lack important information, hindering our ability to
analyse and predict research dynamics. In recent years, link prediction
approaches based on Knowledge Graph Embedding models became the first aid for
this issue. In this work, we present Trans4E, a novel embedding model that is
particularly fit for KGs which include N to M relations with N$\gg$M. This is
typical for KGs that categorize a large number of entities (e.g., research
articles, patents, persons) according to a relatively small set of categories.
Trans4E was applied on two large-scale knowledge graphs, the Academia/Industry
DynAmics (AIDA) and Microsoft Academic Graph (MAG), for completing the
information about Fields of Study (e.g., 'neural networks', 'machine learning',
'artificial intelligence'), and affiliation types (e.g., 'education',
'company', 'government'), improving the scope and accuracy of the resulting
data. We evaluated our approach against alternative solutions on AIDA, MAG, and
four other benchmarks (FB15k, FB15k-237, WN18, and WN18RR). Trans4E outperforms
the other models when using low embedding dimensions and obtains competitive
results in high dimensions.

    

### [[2107.03299] Big Data Information and Nowcasting: Consumption and Investment from Bank Transactions in Turkey](http://arxiv.org/abs/2107.03299)


  We use the aggregate information from individual-to-firm and firm-to-firm in
Garanti BBVA Bank transactions to mimic domestic private demand. Particularly,
we replicate the quarterly national accounts aggregate consumption and
investment (gross fixed capital formation) and its bigger components (Machinery
and Equipment and Construction) in real time for the case of Turkey. In order
to validate the usefulness of the information derived from these indicators we
test the nowcasting ability of both indicators to nowcast the Turkish GDP using
different nowcasting models. The results are successful and confirm the
usefulness of Consumption and Investment Banking transactions for nowcasting
purposes. The value of the Big data information is more relevant at the
beginning of the nowcasting process, when the traditional hard data information
is scarce. This makes this information specially relevant for those countries
where statistical release lags are longer like the Emerging Markets.

    

### [[2107.03311] RoFL: Attestable Robustness for Secure Federated Learning](http://arxiv.org/abs/2107.03311)


  Federated Learning is an emerging decentralized machine learning paradigm
that allows a large number of clients to train a joint model without the need
to share their private data. Participants instead only share ephemeral updates
necessary to train the model. To ensure the confidentiality of the client
updates, Federated Learning systems employ secure aggregation; clients encrypt
their gradient updates, and only the aggregated model is revealed to the
server. Achieving this level of data protection, however, presents new
challenges to the robustness of Federated Learning, i.e., the ability to
tolerate failures and attacks. Unfortunately, in this setting, a malicious
client can now easily exert influence on the model behavior without being
detected. As Federated Learning is being deployed in practice in a range of
sensitive applications, its robustness is growing in importance. In this paper,
we take a step towards understanding and improving the robustness of secure
Federated Learning. We start this paper with a systematic study that evaluates
and analyzes existing attack vectors and discusses potential defenses and
assesses their effectiveness. We then present RoFL, a secure Federated Learning
system that improves robustness against malicious clients through input checks
on the encrypted model updates. RoFL extends Federated Learning's secure
aggregation protocol to allow expressing a variety of properties and
constraints on model updates using zero-knowledge proofs. To enable RoFL to
scale to typical Federated Learning settings, we introduce several ML and
cryptographic optimizations specific to Federated Learning. We implement and
evaluate a prototype of RoFL and show that realistic ML models can be trained
in a reasonable time while improving robustness.

    

### [[2107.03312] SoundStream: An End-to-End Neural Audio Codec](http://arxiv.org/abs/2107.03312)


  We present SoundStream, a novel neural audio codec that can efficiently
compress speech, music and general audio at bitrates normally targeted by
speech-tailored codecs. SoundStream relies on a model architecture composed by
a fully convolutional encoder/decoder network and a residual vector quantizer,
which are trained jointly end-to-end. Training leverages recent advances in
text-to-speech and speech enhancement, which combine adversarial and
reconstruction losses to allow the generation of high-quality audio content
from quantized embeddings. By training with structured dropout applied to
quantizer layers, a single model can operate across variable bitrates from
3kbps to 18kbps, with a negligible quality loss when compared with models
trained at fixed bitrates. In addition, the model is amenable to a low latency
implementation, which supports streamable inference and runs in real time on a
smartphone CPU. In subjective evaluations using audio at 24kHz sampling rate,
SoundStream at 3kbps outperforms Opus at 12kbps and approaches EVS at 9.6kbps.
Moreover, we are able to perform joint compression and enhancement either at
the encoder or at the decoder side with no additional latency, which we
demonstrate through background noise suppression for speech.

    

### [[2107.03315] Predicting with Confidence on Unseen Distributions](http://arxiv.org/abs/2107.03315)


  Recent work has shown that the performance of machine learning models can
vary substantially when models are evaluated on data drawn from a distribution
that is close to but different from the training distribution. As a result,
predicting model performance on unseen distributions is an important challenge.
Our work connects techniques from domain adaptation and predictive uncertainty
literature, and allows us to predict model accuracy on challenging unseen
distributions without access to labeled data. In the context of distribution
shift, distributional distances are often used to adapt models and improve
their performance on new domains, however accuracy estimation, or other forms
of predictive uncertainty, are often neglected in these investigations. Through
investigating a wide range of established distributional distances, such as
Frechet distance or Maximum Mean Discrepancy, we determine that they fail to
induce reliable estimates of performance under distribution shift. On the other
hand, we find that the difference of confidences (DoC) of a classifier's
predictions successfully estimates the classifier's performance change over a
variety of shifts. We specifically investigate the distinction between
synthetic and natural distribution shifts and observe that despite its
simplicity DoC consistently outperforms other quantifications of distributional
difference. $DoC$ reduces predictive error by almost half ($46\%$) on several
realistic and challenging distribution shifts, e.g., on the ImageNet-Vid-Robust
and ImageNet-Rendition datasets.

    

### [[2107.03317] Probabilistic semi-nonnegative matrix factorization: a Skellam-based framework](http://arxiv.org/abs/2107.03317)


  We present a new probabilistic model to address semi-nonnegative matrix
factorization (SNMF), called Skellam-SNMF. It is a hierarchical generative
model consisting of prior components, Skellam-distributed hidden variables and
observed data. Two inference algorithms are derived: Expectation-Maximization
(EM) algorithm for maximum \emph{a posteriori} estimation and Variational Bayes
EM (VBEM) for full Bayesian inference, including the estimation of parameters
prior distribution. From this Skellam-based model, we also introduce a new
divergence $\mathcal{D}$ between a real-valued target data $x$ and two
nonnegative parameters $\lambda_{0}$ and $\lambda_{1}$ such that
$\mathcal{D}\left(x\mid\lambda_{0},\lambda_{1}\right)=0\Leftrightarrow
x=\lambda_{0}-\lambda_{1}$, which is a generalization of the Kullback-Leibler
(KL) divergence. Finally, we conduct experimental studies on those new
algorithms in order to understand their behavior and prove that they can
outperform the classic SNMF approach on real data in a task of automatic
clustering.

    

### [[2107.03322] An algorithmic view of $\ell_2$ regularization and some path-following algorithms](http://arxiv.org/abs/2107.03322)


  We establish an equivalence between the $\ell_2$-regularized solution path
for a convex loss function, and the solution of an ordinary differentiable
equation (ODE). Importantly, this equivalence reveals that the solution path
can be viewed as the flow of a hybrid of gradient descent and Newton method
applying to the empirical loss, which is similar to a widely used optimization
technique called trust region method. This provides an interesting algorithmic
view of $\ell_2$ regularization, and is in contrast to the conventional view
that the $\ell_2$ regularization solution path is similar to the gradient flow
of the empirical loss.New path-following algorithms based on homotopy methods
and numerical ODE solvers are proposed to numerically approximate the solution
path. In particular, we consider respectively Newton method and gradient
descent method as the basis algorithm for the homotopy method, and establish
their approximation error rates over the solution path. Importantly, our theory
suggests novel schemes to choose grid points that guarantee an arbitrarily
small suboptimality for the solution path. In terms of computational cost, we
prove that in order to achieve an $\epsilon$-suboptimality for the entire
solution path, the number of Newton steps required for the Newton method is
$\mathcal O(\epsilon^{-1/2})$, while the number of gradient steps required for
the gradient descent method is $\mathcal O\left(\epsilon^{-1}
\ln(\epsilon^{-1})\right)$. Finally, we use $\ell_2$-regularized logistic
regression as an illustrating example to demonstrate the effectiveness of the
proposed path-following algorithms.

    

### [[2107.03323] AGD-Autoencoder: Attention Gated Deep Convolutional Autoencoder for Brain Tumor Segmentation](http://arxiv.org/abs/2107.03323)


  Brain tumor segmentation is a challenging problem in medical image analysis.
The endpoint is to generate the salient masks that accurately identify brain
tumor regions in an fMRI screening. In this paper, we propose a novel attention
gate (AG model) for brain tumor segmentation that utilizes both the edge
detecting unit and the attention gated network to highlight and segment the
salient regions from fMRI images. This feature enables us to eliminate the
necessity of having to explicitly point towards the damaged area(external
tissue localization) and classify(classification) as per classical computer
vision techniques. AGs can easily be integrated within the deep convolutional
neural networks(CNNs). Minimal computional overhead is required while the AGs
increase the sensitivity scores significantly. We show that the edge detector
along with an attention gated mechanism provide a sufficient enough method for
brain segmentation reaching an IOU of 0.78

    

### [[2107.03324] Enhancing an Intelligent Digital Twin with a Self-organized Reconfiguration Management based on Adaptive Process Models](http://arxiv.org/abs/2107.03324)


  Shorter product life cycles and increasing individualization of production
leads to an increased reconfiguration demand in the domain of industrial
automation systems, which will be dominated by cyber-physical production
systems in the future. In constantly changing systems, however, not all
configuration alternatives of the almost infinite state space are fully
understood. Thus, certain configurations can lead to process instability, a
reduction in quality or machine failures. Therefore, this paper presents an
approach that enhances an intelligent Digital Twin with a self-organized
reconfiguration management based on adaptive process models in order to find
optimized configurations more comprehensively.

    

### [[2107.03331] KaFiStO: A Kalman Filtering Framework for Stochastic Optimization](http://arxiv.org/abs/2107.03331)


  Optimization is often cast as a deterministic problem, where the solution is
found through some iterative procedure such as gradient descent. However, when
training neural networks the loss function changes over (iteration) time due to
the randomized selection of a subset of the samples. This randomization turns
the optimization problem into a stochastic one. We propose to consider the loss
as a noisy observation with respect to some reference optimum. This
interpretation of the loss allows us to adopt Kalman filtering as an optimizer,
as its recursive formulation is designed to estimate unknown parameters from
noisy measurements. Moreover, we show that the Kalman Filter dynamical model
for the evolution of the unknown parameters can be used to capture the gradient
dynamics of advanced methods such as Momentum and Adam. We call this stochastic
optimization method KaFiStO. KaFiStO is an easy to implement, scalable, and
efficient method to train neural networks. We show that it also yields
parameter estimates that are on par with or better than existing optimization
algorithms across several neural network architectures and machine learning
tasks, such as computer vision and language modeling.

    

### [[2107.03336] Regularization-based Continual Learning for Fault Prediction in Lithium-Ion Batteries](http://arxiv.org/abs/2107.03336)


  In recent years, the use of lithium-ion batteries has greatly expanded into
products from many industrial sectors, e.g. cars, power tools or medical
devices. An early prediction and robust understanding of battery faults could
therefore greatly increase product quality in those fields. While current
approaches for data-driven fault prediction provide good results on the exact
processes they were trained on, they often lack the ability to flexibly adapt
to changes, e.g. in operational or environmental parameters. Continual learning
promises such flexibility, allowing for an automatic adaption of previously
learnt knowledge to new tasks. Therefore, this article discusses different
continual learning approaches from the group of regularization strategies,
which are implemented, evaluated and compared based on a real battery wear
dataset. Online elastic weight consolidation delivers the best results, but, as
with all examined approaches, its performance appears to be strongly dependent
on task characteristics and task sequence.

    

### [[2107.03337] Samplets: A new paradigm for data compression](http://arxiv.org/abs/2107.03337)


  In this article, we introduce the novel concept of samplets by transferring
the construction of Tausch-White wavelets to the realm of data. This way we
obtain a multilevel representation of discrete data which directly enables data
compression, detection of singularities and adaptivity. Applying samplets to
represent kernel matrices, as they arise in kernel based learning or Gaussian
process regression, we end up with quasi-sparse matrices. By thresholding small
entries, these matrices are compressible to O(N log N) relevant entries, where
N is the number of data points. This feature allows for the use of fill-in
reducing reorderings to obtain a sparse factorization of the compressed
matrices. Besides the comprehensive introduction to samplets and their
properties, we present extensive numerical studies to benchmark the approach.
Our results demonstrate that samplets mark a considerable step in the direction
of making large data sets accessible for analysis.

    

### [[2107.03342] A Survey of Uncertainty in Deep Neural Networks](http://arxiv.org/abs/2107.03342)


  Due to their increasing spread, confidence in neural network predictions
became more and more important. However, basic neural networks do not deliver
certainty estimates or suffer from over or under confidence. Many researchers
have been working on understanding and quantifying uncertainty in a neural
network's prediction. As a result, different types and sources of uncertainty
have been identified and a variety of approaches to measure and quantify
uncertainty in neural networks have been proposed. This work gives a
comprehensive overview of uncertainty estimation in neural networks, reviews
recent advances in the field, highlights current challenges, and identifies
potential research opportunities. It is intended to give anyone interested in
uncertainty estimation in neural networks a broad overview and introduction,
without presupposing prior knowledge in this field. A comprehensive
introduction to the most crucial sources of uncertainty is given and their
separation into reducible model uncertainty and not reducible data uncertainty
is presented. The modeling of these uncertainties based on deterministic neural
networks, Bayesian neural networks, ensemble of neural networks, and test-time
data augmentation approaches is introduced and different branches of these
fields as well as the latest developments are discussed. For a practical
application, we discuss different measures of uncertainty, approaches for the
calibration of neural networks and give an overview of existing baselines and
implementations. Different examples from the wide spectrum of challenges in
different fields give an idea of the needs and challenges regarding
uncertainties in practical applications. Additionally, the practical
limitations of current methods for mission- and safety-critical real world
applications are discussed and an outlook on the next steps towards a broader
usage of such methods is given.

    

### [[2107.03354] Mitigating Performance Saturation in Neural Marked Point Processes: Architectures and Loss Functions](http://arxiv.org/abs/2107.03354)


  Attributed event sequences are commonly encountered in practice. A recent
research line focuses on incorporating neural networks with the statistical
model -- marked point processes, which is the conventional tool for dealing
with attributed event sequences. Neural marked point processes possess good
interpretability of probabilistic models as well as the representational power
of neural networks. However, we find that performance of neural marked point
processes is not always increasing as the network architecture becomes more
complicated and larger, which is what we call the performance saturation
phenomenon. This is due to the fact that the generalization error of neural
marked point processes is determined by both the network representational
ability and the model specification at the same time. Therefore we can draw two
major conclusions: first, simple network structures can perform no worse than
complicated ones for some cases; second, using a proper probabilistic
assumption is as equally, if not more, important as improving the complexity of
the network. Based on this observation, we propose a simple graph-based network
structure called GCHP, which utilizes only graph convolutional layers, thus it
can be easily accelerated by the parallel mechanism. We directly consider the
distribution of interarrival times instead of imposing a specific assumption on
the conditional intensity function, and propose to use a likelihood ratio loss
with a moment matching mechanism for optimization and model selection.
Experimental results show that GCHP can significantly reduce training time and
the likelihood ratio loss with interarrival time probability assumptions can
greatly improve the model performance.

    

### [[2107.03356] Efficient Matrix-Free Approximations of Second-Order Information, with Applications to Pruning and Optimization](http://arxiv.org/abs/2107.03356)


  Efficiently approximating local curvature information of the loss function is
a key tool for optimization and compression of deep neural networks. Yet, most
existing methods to approximate second-order information have high
computational or storage costs, which can limit their practicality. In this
work, we investigate matrix-free, linear-time approaches for estimating
Inverse-Hessian Vector Products (IHVPs) for the case when the Hessian can be
approximated as a sum of rank-one matrices, as in the classic approximation of
the Hessian by the empirical Fisher matrix. We propose two new algorithms as
part of a framework called M-FAC: the first algorithm is tailored towards
network compression and can compute the IHVP for dimension $d$, if the Hessian
is given as a sum of $m$ rank-one matrices, using $O(dm^2)$ precomputation,
$O(dm)$ cost for computing the IHVP, and query cost $O(m)$ for any single
element of the inverse Hessian. The second algorithm targets an optimization
setting, where we wish to compute the product between the inverse Hessian,
estimated over a sliding window of optimization steps, and a given gradient
direction, as required for preconditioned SGD. We give an algorithm with cost
$O(dm + m^2)$ for computing the IHVP and $O(dm + m^3)$ for adding or removing
any gradient from the sliding window. These two algorithms yield
state-of-the-art results for network pruning and optimization with lower
computational overhead relative to existing second-order methods.
Implementations are available at [10] and [18].

    

### [[2107.03361] A comparative study of various Deep Learning techniques for spatio-temporal Super-Resolution reconstruction of Forced Isotropic Turbulent flows](http://arxiv.org/abs/2107.03361)


  Super-resolution is an innovative technique that upscales the resolution of
an image or a video and thus enables us to reconstruct high-fidelity images
from low-resolution data. This study performs super-resolution analysis on
turbulent flow fields spatially and temporally using various state-of-the-art
machine learning techniques like ESPCN, ESRGAN and TecoGAN to reconstruct
high-resolution flow fields from low-resolution flow field data, especially
keeping in mind the need for low resource consumption and rapid results
production/verification. The dataset used for this study is extracted from the
'isotropic 1024 coarse' dataset which is a part of Johns Hopkins Turbulence
Databases (JHTDB). We have utilized pre-trained models and fine tuned them to
our needs, so as to minimize the computational resources and the time required
for the implementation of the super-resolution models. The advantages presented
by this method far exceed the expectations and the outcomes of regular single
structure models. The results obtained through these models are then compared
using MSE, PSNR, SAM, VIF and SCC metrics in order to evaluate the upscaled
results, find the balance between computational power and output quality, and
then identify the most accurate and efficient model for spatial and temporal
super-resolution of turbulent flow fields.

    

### [[2107.03374] Evaluating Large Language Models Trained on Code](http://arxiv.org/abs/2107.03374)


  We introduce Codex, a GPT language model fine-tuned on publicly available
code from GitHub, and study its Python code-writing capabilities. A distinct
production version of Codex powers GitHub Copilot. On HumanEval, a new
evaluation set we release to measure functional correctness for synthesizing
programs from docstrings, our model solves 28.8% of the problems, while GPT-3
solves 0% and GPT-J solves 11.4%. Furthermore, we find that repeated sampling
from the model is a surprisingly effective strategy for producing working
solutions to difficult prompts. Using this method, we solve 70.2% of our
problems with 100 samples per problem. Careful investigation of our model
reveals its limitations, including difficulty with docstrings describing long
chains of operations and with binding operations to variables. Finally, we
discuss the potential broader impacts of deploying powerful code generation
technologies, covering safety, security, and economics.

    

### [[2107.03375] Differentiable Architecture Pruning for Transfer Learning](http://arxiv.org/abs/2107.03375)


  We propose a new gradient-based approach for extracting sub-architectures
from a given large model. Contrarily to existing pruning methods, which are
unable to disentangle the network architecture and the corresponding weights,
our architecture-pruning scheme produces transferable new structures that can
be successfully retrained to solve different tasks. We focus on a
transfer-learning setup where architectures can be trained on a large data set
but very few data points are available for fine-tuning them on new tasks. We
define a new gradient-based algorithm that trains architectures of arbitrarily
low complexity independently from the attached weights. Given a search space
defined by an existing large neural model, we reformulate the architecture
search task as a complexity-penalized subset-selection problem and solve it
through a two-temperature relaxation scheme. We provide theoretical convergence
guarantees and validate the proposed transfer-learning strategy on real data.

    

### [[1905.06684] Formal derivation of Mesh Neural Networks with their Forward-Only gradient Propagation](http://arxiv.org/abs/1905.06684)


  This paper proposes the Mesh Neural Network (MNN), a novel architecture which
allows neurons to be connected in any topology, to efficiently route
information. In MNNs, information is propagated between neurons throughout a
state transition function. State and error gradients are then directly computed
from state updates without backward computation. The MNN architecture and the
error propagation schema is formalized and derived in tensor algebra. The
proposed computational model can fully supply a gradient descent process, and
is potentially suitable for very large scale sparse NNs, due to its
expressivity and training efficiency, with respect to NNs based on
back-propagation and computational graphs.

    

### [[1908.04628] L2P: An Algorithm for Estimating Heavy-tailed Outcomes](http://arxiv.org/abs/1908.04628)


  Many real-world prediction tasks have outcome variables that have
characteristic heavy-tail distributions. Examples include copies of books sold,
auction prices of art pieces, demand for commodities in warehouses, etc. By
learning heavy-tailed distributions, "big and rare" instances (e.g., the
best-sellers) will have accurate predictions. Most existing approaches are not
dedicated to learning heavy-tailed distribution; thus, they heavily
under-predict such instances. To tackle this problem, we introduce Learning to
Place (L2P), which exploits the pairwise relationships between instances for
learning. In its training phase, L2P learns a pairwise preference classifier:
is instance A > instance B? In its placing phase, L2P obtains a prediction by
placing the new instance among the known instances. Based on its placement, the
new instance is then assigned a value for its outcome variable. Experiments on
real data show that L2P outperforms competing approaches in terms of accuracy
and ability to reproduce heavy-tailed outcome distribution. In addition, L2P
provides an interpretable model by placing each predicted instance in relation
to its comparable neighbors. Interpretable models are highly desirable when
lives and treasure are at stake.

    

### [[1910.10844] Diametrical Risk Minimization: Theory and Computations](http://arxiv.org/abs/1910.10844)


  The theoretical and empirical performance of Empirical Risk Minimization
(ERM) often suffers when loss functions are poorly behaved with large Lipschitz
moduli and spurious sharp minimizers. We propose and analyze a counterpart to
ERM called Diametrical Risk Minimization (DRM), which accounts for worst-case
empirical risks within neighborhoods in parameter space. DRM has generalization
bounds that are independent of Lipschitz moduli for convex as well as nonconvex
problems and it can be implemented using a practical algorithm based on
stochastic gradient descent. Numerical results illustrate the ability of DRM to
find quality solutions with low generalization error in sharp empirical risk
landscapes from benchmark neural network classification problems with corrupted
labels.

    

### [[1911.12780] Detection and Mitigation of Rare Subclasses in Deep Neural Network Classifiers](http://arxiv.org/abs/1911.12780)


  Regions of high-dimensional input spaces that are underrepresented in
training datasets reduce machine-learnt classifier performance, and may lead to
corner cases and unwanted bias for classifiers used in decision making systems.
When these regions belong to otherwise well-represented classes, their presence
and negative impact are very hard to identify. We propose an approach for the
detection and mitigation of such rare subclasses in deep neural network
classifiers. The new approach is underpinned by an easy-to-compute commonality
metric that supports the detection of rare subclasses, and comprises methods
for reducing the impact of these subclasses during both model training and
model exploitation. We demonstrate our approach using two well-known datasets,
MNIST's handwritten digits and Kaggle's cats/dogs, identifying rare subclasses
and producing models which compensate for subclass rarity. In addition we
demonstrate how our run-time approach increases the ability of users to
identify samples likely to be misclassified at run-time.

    

### [[2002.05426] PHOTONAI -- A Python API for Rapid Machine Learning Model Development](http://arxiv.org/abs/2002.05426)


  PHOTONAI is a high-level Python API designed to simplify and accelerate
machine learning model development. It functions as a unifying framework
allowing the user to easily access and combine algorithms from different
toolboxes into custom algorithm sequences. It is especially designed to support
the iterative model development process and automates the repetitive training,
hyperparameter optimization and evaluation tasks. Importantly, the workflow
ensures unbiased performance estimates while still allowing the user to fully
customize the machine learning analysis. PHOTONAI extends existing solutions
with a novel pipeline implementation supporting more complex data streams,
feature combinations, and algorithm selection. Metrics and results can be
conveniently visualized using the PHOTONAI Explorer and predictive models are
shareable in a standardized format for further external validation or
application. A growing add-on ecosystem allows researchers to offer data
modality specific algorithms to the community and enhance machine learning in
the areas of the life sciences. Its practical utility is demonstrated on an
exemplary medical machine learning problem, achieving a state-of-the-art
solution in few lines of code. Source code is publicly available on Github,
while examples and documentation can be found at this http URL.

    

### [[2002.05582] Learning to Predict Error for MRI Reconstruction](http://arxiv.org/abs/2002.05582)


  In healthcare applications, predictive uncertainty has been used to assess
predictive accuracy. In this paper, we demonstrate that predictive uncertainty
estimated by the current methods does not highly correlate with prediction
error by decomposing the latter into random and systematic errors, and showing
that the former is equivalent to the variance of the random error. In addition,
we observe that current methods unnecessarily compromise performance by
modifying the model and training loss to estimate the target and uncertainty
jointly. We show that estimating them separately without modifications improves
performance. Following this, we propose a novel method that estimates the
target labels and magnitude of the prediction error in two steps. We
demonstrate this method on a large-scale MRI reconstruction task, and achieve
significantly better results than the state-of-the-art uncertainty estimation
methods.

    

### [[2003.06060] Your GAN is Secretly an Energy-based Model and You Should use Discriminator Driven Latent Sampling](http://arxiv.org/abs/2003.06060)


  We show that the sum of the implicit generator log-density $\log p_g$ of a
GAN with the logit score of the discriminator defines an energy function which
yields the true data density when the generator is imperfect but the
discriminator is optimal, thus making it possible to improve on the typical
generator (with implicit density $p_g$). To make that practical, we show that
sampling from this modified density can be achieved by sampling in latent space
according to an energy-based model induced by the sum of the latent prior
log-density and the discriminator output score. This can be achieved by running
a Langevin MCMC in latent space and then applying the generator function, which
we call Discriminator Driven Latent Sampling~(DDLS). We show that DDLS is
highly efficient compared to previous methods which work in the
high-dimensional pixel space and can be applied to improve on previously
trained GANs of many types. We evaluate DDLS on both synthetic and real-world
datasets qualitatively and quantitatively. On CIFAR-10, DDLS substantially
improves the Inception Score of an off-the-shelf pre-trained
SN-GAN~\citep{sngan} from $8.22$ to $9.09$ which is even comparable to the
class-conditional BigGAN~\citep{biggan} model. This achieves a new
state-of-the-art in unconditional image synthesis setting without introducing
extra parameters or additional training.

    

### [[2004.04120] Solving the scalarization issues of Advantage-based Reinforcement Learning Algorithms](http://arxiv.org/abs/2004.04120)


  In this research, some of the issues that arise from the scalarization of the
multi-objective optimization problem in the Advantage Actor Critic (A2C)
reinforcement learning algorithm are investigated. The paper shows how a naive
scalarization can lead to gradients overlapping. Furthermore, the possibility
that the entropy regularization term can be a source of uncontrolled noise is
discussed. With respect to the above issues, a technique to avoid gradient
overlapping is proposed, while keeping the same loss formulation. Moreover, a
method to avoid the uncontrolled noise, by sampling the actions from
distributions with a desired minimum entropy, is investigated. Pilot
experiments have been carried out to show how the proposed method speeds up the
training. The proposed approach can be applied to any Advantage-based
Reinforcement Learning algorithm.

    

### [[2004.07348] Learning 1-Dimensional Submanifolds for Subsequent Inference on Random Dot Product Graphs](http://arxiv.org/abs/2004.07348)


  A random dot product graph (RDPG) is a generative model for networks in which
vertices correspond to positions in a latent Euclidean space and edge
probabilities are determined by the dot products of the latent positions. We
consider RDPGs for which the latent positions are randomly sampled from an
unknown $1$-dimensional submanifold of the latent space. In principle,
restricted inference, i.e., procedures that exploit the structure of the
submanifold, should be more effective than unrestricted inference; however, it
is not clear how to conduct restricted inference when the submanifold is
unknown. We submit that techniques for manifold learning can be used to learn
the unknown submanifold well enough to realize benefit from restricted
inference. To illustrate, we test $1$- and $2$-sample hypotheses about the
Fréchet means of small communities of vertices, using the complete set of
vertices to infer latent structure. We propose test statistics that deploy the
Isomap procedure for manifold learning, using shortest path distances on
neighborhood graphs constructed from estimated latent positions to estimate arc
lengths on the unknown $1$-dimensional submanifold. Unlike conventional
applications of Isomap, the estimated latent positions do not lie on the
submanifold of interest. We extend existing convergence results for Isomap to
this setting and use them to demonstrate that, as the number of auxiliary
vertices increases, the power of our test converges to the power of the
corresponding test when the submanifold is known.

    

### [[2005.06674] On the Convergence of Overlapping Schwarz Decomposition for Nonlinear Optimal Control](http://arxiv.org/abs/2005.06674)


  We study the convergence properties of an overlapping Schwarz
decomposition~algorithm for solving nonlinear optimal control problems (OCPs).
The approach decomposes the time domain into a set of overlapping subdomains,
and solves subproblems defined over such subdomains in parallel. Convergence is
attained by updating primal-dual information at the boundaries of the
overlapping regions. We show that the algorithm exhibits local linear
convergence and that the convergence rate improves exponentially with the
overlap size. Our convergence results rely on a sensitivity result for OCPs
that we call "exponential decay of sensitivity" (EDS). Intuitively, EDS states
that the impact of parametric perturbations at the boundaries of the domain
(initial and final time) decays exponentially as one moves into the domain. We
show that EDS holds for nonlinear OCPs under a uniform second-order sufficient
condition, a controllability condition, and a uniform boundedness condition. We
conduct numerical experiments using a quadrotor motion planning problem and a
PDE control problem; and show that the approach is significantly more efficient
than ADMM and as efficient as the centralized solver Ipopt.

    

### [[2006.05535] Locally Private Graph Neural Networks](http://arxiv.org/abs/2006.05535)


  Graph Neural Networks (GNNs) have demonstrated superior performance in
learning node representations for various graph inference tasks. However,
learning over graph data can raise privacy concerns when nodes represent people
or human-related variables that involve sensitive or personal information.
While numerous techniques have been proposed for privacy-preserving deep
learning over non-relational data, there is less work addressing the privacy
issues pertained to applying deep learning algorithms on graphs. In this paper,
we study the problem of node data privacy, where graph nodes have potentially
sensitive data that is kept private, but they could be beneficial for a central
server for training a GNN over the graph. To address this problem, we develop a
privacy-preserving, architecture-agnostic GNN learning algorithm with formal
privacy guarantees based on Local Differential Privacy (LDP). Specifically, we
propose an LDP encoder and an unbiased rectifier, by which the server can
communicate with the graph nodes to privately collect their data and
approximate the GNN's first layer. To further reduce the effect of the injected
noise, we propose to prepend a simple graph convolution layer, called KProp,
which is based on the multi-hop aggregation of the nodes' features acting as a
denoising mechanism. Finally, we propose a robust training framework, in which
we benefit from KProp's denoising capability to increase the accuracy of
inference in the presence of noisy labels. Extensive experiments conducted over
real-world datasets demonstrate that our method can maintain a satisfying level
of accuracy with low privacy loss.

    

### [[2007.10316] Auxiliary Diagnosing Coronary Stenosis Using Machine Learning](http://arxiv.org/abs/2007.10316)


  How to accurately classify and diagnose whether an individual has Coronary
Stenosis (CS) without invasive physical examination? This problem has not been
solved satisfactorily. To this end, the four machine learning (ML) algorithms,
i.e., Boosted Tree (BT), Decision Tree (DT), Logistic Regression (LR) and
Random Forest (RF) are employed in this paper. First, eleven features including
basic information of an individual, symptoms and results of routine physical
examination are selected, as well as one label is specified, indicating whether
an individual suffers from different severity of coronary artery stenosis or
not. On the basis of it, a sample set is constructed. Second, each of these
four ML algorithms learns from the sample set to obtain the corresponding
optimal classified results, respectively. The experimental results show that:
RF performs better than other three algorithms, and the former algorithm
classifies whether an individual has CS with an accuracy of 95.7% (=90/94).

    

### [[2007.15823] Explainable Prediction of Text Complexity: The Missing Preliminaries for Text Simplification](http://arxiv.org/abs/2007.15823)


  Text simplification reduces the language complexity of professional content
for accessibility purposes. End-to-end neural network models have been widely
adopted to directly generate the simplified version of input text, usually
functioning as a blackbox. We show that text simplification can be decomposed
into a compact pipeline of tasks to ensure the transparency and explainability
of the process. The first two steps in this pipeline are often neglected: 1) to
predict whether a given piece of text needs to be simplified, and 2) if yes, to
identify complex parts of the text. The two tasks can be solved separately
using either lexical or deep learning methods, or solved jointly. Simply
applying explainable complexity prediction as a preliminary step, the
out-of-sample text simplification performance of the state-of-the-art,
black-box simplification models can be improved by a large margin.

    

### [[2008.02790] Decoupling Exploration and Exploitation for Meta-Reinforcement Learning without Sacrifices](http://arxiv.org/abs/2008.02790)


  The goal of meta-reinforcement learning (meta-RL) is to build agents that can
quickly learn new tasks by leveraging prior experience on related tasks.
Learning a new task often requires both exploring to gather task-relevant
information and exploiting this information to solve the task. In principle,
optimal exploration and exploitation can be learned end-to-end by simply
maximizing task performance. However, such meta-RL approaches struggle with
local optima due to a chicken-and-egg problem: learning to explore requires
good exploitation to gauge the exploration's utility, but learning to exploit
requires information gathered via exploration. Optimizing separate objectives
for exploration and exploitation can avoid this problem, but prior meta-RL
exploration objectives yield suboptimal policies that gather information
irrelevant to the task. We alleviate both concerns by constructing an
exploitation objective that automatically identifies task-relevant information
and an exploration objective to recover only this information. This avoids
local optima in end-to-end training, without sacrificing optimal exploration.
Empirically, DREAM substantially outperforms existing approaches on complex
meta-RL problems, such as sparse-reward 3D visual navigation. Videos of DREAM:
this https URL


### [[2008.04388] GRIMGEP: Learning Progress for Robust Goal Sampling in Visual Deep Reinforcement Learning](http://arxiv.org/abs/2008.04388)


  Designing agents, capable of learning autonomously a wide range of skills is
critical in order to increase the scope of reinforcement learning. It will both
increase the diversity of learned skills and reduce the burden of manually
designing reward functions for each skill. Self-supervised agents, setting
their own goals, and trying to maximize the diversity of those goals have shown
great promise towards this end. However, a currently known limitation of agents
trying to maximize the diversity of sampled goals is that they tend to get
attracted to noise or more generally to parts of the environments that cannot
be controlled (distractors). When agents have access to predefined goal
features or expert knowledge, absolute Learning Progress (ALP) provides a way
to distinguish between regions that can be controlled and those that cannot.
However, those methods often fall short when the agents are only provided with
raw sensory inputs such as images. In this work we extend those concepts to
unsupervised image-based goal exploration. We propose a framework that allows
agents to autonomously identify and ignore noisy distracting regions while
searching for novelty in the learnable regions to both improve overall
performance and avoid catastrophic forgetting. Our framework can be combined
with any state-of-the-art novelty seeking goal exploration approaches. We
construct a rich 3D image based environment with distractors. Experiments on
this environment show that agents using our framework successfully identify
interesting regions of the environment, resulting in drastically improved
performances. The source code is available at
this https URL.

    

### [[2009.07476] Path Planning using Neural A* Search](http://arxiv.org/abs/2009.07476)


  We present Neural A*, a novel data-driven search method for path planning
problems. Despite the recent increasing attention to data-driven path planning,
machine learning approaches to search-based planning are still challenging due
to the discrete nature of search algorithms. In this work, we reformulate a
canonical A* search algorithm to be differentiable and couple it with a
convolutional encoder to form an end-to-end trainable neural network planner.
Neural A* solves a path planning problem by encoding a problem instance to a
guidance map and then performing the differentiable A* search with the guidance
map. By learning to match the search results with ground-truth paths provided
by experts, Neural A* can produce a path consistent with the ground truth
accurately and efficiently. Our extensive experiments confirmed that Neural A*
outperformed state-of-the-art data-driven planners in terms of the search
optimality and efficiency trade-off. Furthermore, Neural A* successfully
predicted realistic human trajectories by directly performing search-based
planning on natural image inputs. Project page:
this https URL


### [[2010.03051] How and Why to Use Experimental Data to Evaluate Methods for Observational Causal Inference](http://arxiv.org/abs/2010.03051)


  Methods that infer causal dependence from observational data are central to
many areas of science, including medicine, economics, and the social sciences.
A variety of theoretical properties of these methods have been proven, but
empirical evaluation remains a challenge, largely due to the lack of
observational data sets for which treatment effect is known. We describe and
analyze observational sampling from randomized controlled trials (OSRCT), a
method for evaluating causal inference methods using data from randomized
controlled trials (RCTs). This method can be used to create constructed
observational data sets with corresponding unbiased estimates of treatment
effect, substantially increasing the number of data sets available for
empirical evaluation of causal inference methods. We show that, in expectation,
OSRCT creates data sets that are equivalent to those produced by randomly
sampling from empirical data sets in which all potential outcomes are
available. We then perform a large-scale evaluation of seven causal inference
methods over 37 data sets, drawn from RCTs, as well as simulators, real-world
computational systems, and observational data sets augmented with a synthetic
response variable. We find notable performance differences when comparing
across data from different sources, demonstrating the importance of using data
from a variety of sources when evaluating any causal inference method.

    

### [[2010.03131] Cognitive Learning-Aided Multi-Antenna Communications](http://arxiv.org/abs/2010.03131)


  Cognitive communications have emerged as a promising solution to enhance,
adapt, and invent new tools and capabilities that transcend conventional
wireless networks. Deep learning (DL) is critical in enabling essential
features of cognitive systems because of its fast prediction performance,
adaptive behavior, and model-free structure. These features are especially
significant for multi-antenna wireless communications systems, which generate
and handle massive data. Multiple antennas may provide multiplexing, diversity,
or antenna gains that, respectively, improve the capacity, bit error rate, or
the signal-to-interference-plus-noise ratio. In practice, multi-antenna
cognitive communications encounter challenges in terms of data complexity and
diversity, hardware complexity, and wireless channel dynamics. The DL-based
solutions tackle these problems at the various stages of communications
processing such as channel estimation, hybrid beamforming, user localization,
and sparse array design. There are research opportunities to address
significant design challenges arising from insufficient data coverage, learning
model complexity, and data transmission overheads. This article provides
synopses of various DL-based methods to impart cognitive behavior to
multi-antenna wireless communications.

    

### [[2010.06538] Modeling Atmospheric Data and Identifying Dynamics: Temporal Data-Driven Modeling of Air Pollutants](http://arxiv.org/abs/2010.06538)


  Atmospheric modeling has recently experienced a surge with the advent of deep
learning. Most of these models, however, predict concentrations of pollutants
following a data-driven approach in which the physical laws that govern their
behaviors and relationships remain hidden. With the aid of real-world air
quality data collected hourly in different stations throughout Madrid, we
present an empirical approach using data-driven techniques with the following
goals: (1) Find parsimonious systems of ordinary differential equations via
sparse identification of nonlinear dynamics (SINDy) that model the
concentration of pollutants and their changes over time; (2) assess the
performance and limitations of our models using stability analysis; (3)
reconstruct the time series of chemical pollutants not measured in certain
stations using delay coordinate embedding results. Our results show that
Akaike's Information Criterion can work well in conjunction with best subset
regression as to find an equilibrium between sparsity and goodness of fit. We
also find that, due to the complexity of the chemical system under study,
identifying the dynamics of this system over longer periods of time require
higher levels of data filtering and smoothing. Stability analysis for the
reconstructed ordinary differential equations (ODEs) reveals that more than
half of the physically relevant critical points are saddle points, suggesting
that the system is unstable even under the idealized assumption that all
environmental conditions are constant over time.

    

### [[2010.07290] XPDNet for MRI Reconstruction: an application to the 2020 fastMRI challenge](http://arxiv.org/abs/2010.07290)


  We present a new neural network, the XPDNet, for MRI reconstruction from
periodically under-sampled multi-coil data. We inform the design of this
network by taking best practices from MRI reconstruction and computer vision.
We show that this network can achieve state-of-the-art reconstruction results,
as shown by its ranking of second in the fastMRI 2020 challenge.

    

### [[2010.13118] Monocular Depth Estimation via Listwise Ranking using the Plackett-Luce Model](http://arxiv.org/abs/2010.13118)


  In many real-world applications, the relative depth of objects in an image is
crucial for scene understanding. Recent approaches mainly tackle the problem of
depth prediction in monocular images by treating the problem as a regression
task. Yet, being interested in an order relation in the first place, ranking
methods suggest themselves as a natural alternative to regression, and indeed,
ranking approaches leveraging pairwise comparisons as training information
("object A is closer to the camera than B") have shown promising performance on
this problem. In this paper, we elaborate on the use of so-called listwise
ranking as a generalization of the pairwise approach. Our method is based on
the Plackett-Luce (PL) model, a probability distribution on rankings, which we
combine with a state-of-the-art neural network architecture and a simple
sampling strategy to reduce training complexity. Moreover, taking advantage of
the representation of PL as a random utility model, the proposed predictor
offers a natural way to recover (shift-invariant) metric depth information from
ranking-only data provided at training time. An empirical evaluation on several
benchmark datasets in a "zero-shot" setting demonstrates the effectiveness of
our approach compared to existing ranking and regression methods.

    

### [[2011.14694] Improving the performance of EEG decoding using anchored-STFT in conjunction with gradient norm adversarial augmentation](http://arxiv.org/abs/2011.14694)


  Brain-computer interfaces (BCIs) enable direct communication between humans
and machines by translating brain activity into control commands. EEG is one of
the most common sources of neural signals because of its inexpensive and
non-invasive nature. However, interpretation of EEG signals is non-trivial
because EEG signals have a low spatial resolution and are often distorted with
noise and artifacts. Therefore, it is possible that meaningful patterns for
classifying EEG signals are deeply hidden. Nowadays, state-of-the-art
deep-learning algorithms have proven to be quite efficient in learning hidden,
meaningful patterns. However, the performance of the deep learning algorithms
depends upon the quality and the amount of the provided training data. Hence, a
better input formation (feature extraction) technique and a generative model to
produce high-quality data can enable the deep learning algorithms to adapt high
generalization quality. In this study, we proposed a novel input formation
(feature extraction) method in conjunction with a novel deep learning based
generative model to harness new training examples. The feature vectors are
extracted using a modified Short Time Fourier Transform (STFT) called
anchored-STFT. Anchored-STFT, inspired by wavelet transform, tries to minimize
the tradeoff between time and frequency resolution. As a result, it extracts
the inputs (feature vectors) with better time and frequency resolution compared
to the standard STFT. Secondly, we introduced a novel generative adversarial
data augmentation technique called gradient norm adversarial augmentation
(GNAA) for generating more training data. Thirdly, we investigated the
existence and significance of adversarial inputs in EEG data. Our approach
obtained the kappa value of 0.814 for BCI competition II dataset III and 0.755
for BCI competition IV dataset 2b for session-to-session transfer on test data.

    

### [[2012.00685] A data-driven approach to the forecasting of ground-level ozone concentration](http://arxiv.org/abs/2012.00685)


  The ability to forecast the concentration of air pollutants in an urban
region is crucial for decision-makers wishing to reduce the impact of pollution
on public health through active measures (e.g. temporary traffic closures). In
this study, we present a machine learning approach applied to the forecast of
the day-ahead maximum value of the ozone concentration for several geographical
locations in southern Switzerland. Due to the low density of measurement
stations and to the complex orography of the use case terrain, we adopted
feature selection methods instead of explicitly restricting relevant features
to a neighbourhood of the prediction sites, as common in spatio-temporal
forecasting methods. We then used Shapley values to assess the explainability
of the learned models in terms of feature importance and feature interactions
in relation to ozone predictions; our analysis suggests that the trained models
effectively learned explanatory cross-dependencies among atmospheric variables.
Finally, we show how weighting observations helps in increasing the accuracy of
the forecasts for specific ranges of ozone's daily peak values.

    

### [[2012.07969] A case for new neural network smoothness constraints](http://arxiv.org/abs/2012.07969)


  How sensitive should machine learning models be to input changes? We tackle
the question of model smoothness and show that it is a useful inductive bias
which aids generalization, adversarial robustness, generative modeling and
reinforcement learning. We explore current methods of imposing smoothness
constraints and observe they lack the flexibility to adapt to new tasks, they
don't account for data modalities, they interact with losses, architectures and
optimization in ways not yet fully understood. We conclude that new advances in
the field are hinging on finding ways to incorporate data, tasks and learning
into our definitions of smoothness.

    

### [[2012.13435] Identifying Training Stop Point with Noisy Labeled Data](http://arxiv.org/abs/2012.13435)


  Training deep neural networks (DNNs) with noisy labels is a challenging
problem due to over-parameterization. DNNs tend to essentially fit on clean
samples at a higher rate in the initial stages, and later fit on the noisy
samples at a relatively lower rate. Thus, with a noisy dataset, the test
accuracy increases initially and drops in the later stages. To find an early
stopping point at the maximum obtainable test accuracy (MOTA), recent studies
assume either that i) a clean validation set is available or ii) the noise
ratio is known, or, both. However, often a clean validation set is unavailable,
and the noise estimation can be inaccurate. To overcome these issues, we
provide a novel training solution, free of these conditions. We analyze the
rate of change of the training accuracy for different noise ratios under
different conditions to identify a training stop region. We further develop a
heuristic algorithm based on a small-learning assumption to find a training
stop point (TSP) at or close to MOTA. To the best of our knowledge, our method
is the first to rely solely on the \textit{training behavior}, while utilizing
the entire training set, to automatically find a TSP. We validated the
robustness of our algorithm (AutoTSP) through several experiments on CIFAR-10,
CIFAR-100, and a real-world noisy dataset for different noise ratios, noise
types, and architectures.

    

### [[2101.04869] Convolutional Neural Nets in Chemical Engineering: Foundations, Computations, and Applications](http://arxiv.org/abs/2101.04869)


  In this paper we review the mathematical foundations of convolutional neural
nets (CNNs) with the goals of: i) highlighting connections with techniques from
statistics, signal processing, linear algebra, differential equations, and
optimization, ii) demystifying underlying computations, and iii) identifying
new types of applications. CNNs are powerful machine learning models that
highlight features from grid data to make predictions (regression and
classification). The grid data object can be represented as vectors (in 1D),
matrices (in 2D), or tensors (in 3D or higher dimensions) and can incorporate
multiple channels (thus providing high flexibility in the input data
representation). CNNs highlight features from the grid data by performing
convolution operations with different types of operators. The operators
highlight different types of features (e.g., patterns, gradients, geometrical
features) and are learned by using optimization techniques. In other words,
CNNs seek to identify optimal operators that best map the input data to the
output data. A common misconception is that CNNs are only capable of processing
image or video data but their application scope is much wider; specifically,
datasets encountered in diverse applications can be expressed as grid data.
Here, we show how to apply CNNs to new types of applications such as optimal
control, flow cytometry, multivariate process monitoring, and molecular
simulations.

    

### [[2101.08699] An empirical evaluation of active inference in multi-armed bandits](http://arxiv.org/abs/2101.08699)


  A key feature of sequential decision making under uncertainty is a need to
balance between exploiting--choosing the best action according to the current
knowledge, and exploring--obtaining information about values of other actions.
The multi-armed bandit problem, a classical task that captures this trade-off,
served as a vehicle in machine learning for developing bandit algorithms that
proved to be useful in numerous industrial applications. The active inference
framework, an approach to sequential decision making recently developed in
neuroscience for understanding human and animal behaviour, is distinguished by
its sophisticated strategy for resolving the exploration-exploitation
trade-off. This makes active inference an exciting alternative to already
established bandit algorithms. Here we derive an efficient and scalable
approximate active inference algorithm and compare it to two state-of-the-art
bandit algorithms: Bayesian upper confidence bound and optimistic Thompson
sampling. This comparison is done on two types of bandit problems: a stationary
and a dynamic switching bandit. Our empirical evaluation shows that the active
inference algorithm does not produce efficient long-term behaviour in
stationary bandits. However, in the more challenging switching bandit problem
active inference performs substantially better than the two state-of-the-art
bandit algorithms. The results open exciting venues for further research in
theoretical and applied machine learning, as well as lend additional
credibility to active inference as a general framework for studying human and
animal behaviour.

    

### [[2102.00570] Impulse data models for the inverse problem of electrocardiography](http://arxiv.org/abs/2102.00570)


  The proposed method re-frames traditional inverse problems of
electrocardiography into regression problems, constraining the solution space
by decomposing signals with multidimensional Gaussian impulse basis functions.
Impulse HSPs were generated with single Gaussian basis functions at discrete
heart surface locations and projected to corresponding BSPs using a volume
conductor torso model. Both BSP (inputs) and HSP (outputs) were mapped to
regular 2D surface meshes and used to train a neural network. Predictive
capabilities of the network were tested with unseen synthetic and experimental
data. A dense full connected single hidden layer neural network was trained to
map body surface impulses to heart surface Gaussian basis functions for
reconstructing HSP. Synthetic pulses moving across the heart surface were
predicted from the neural network with root mean squared error of $9.1\pm1.4$%.
Predicted signals were robust to noise up to 20 dB and errors due to
displacement and rotation of the heart within the torso were bounded and
predictable. A shift of the heart 40 mm toward the spine resulted in a 4\%
increase in signal feature localization error. The set of training impulse
function data could be reduced and prediction error remained bounded. Recorded
HSPs from in-vitro pig hearts were reliably decomposed using space-time
Gaussian basis functions. Predicted HSPs for left-ventricular pacing had a mean
absolute error of $10.4\pm11.4$ ms. Other pacing scenarios were analyzed with
similar success. Conclusion: Impulses from Gaussian basis functions are
potentially an effective and robust way to train simple neural network data
models for reconstructing HSPs from decomposed BSPs. The HSPs predicted by the
neural network can be used to generate activation maps that non-invasively
identify features of cardiac electrical dysfunction and can guide subsequent
treatment options.

    

### [[2102.04540] Last-iterate Convergence of Decentralized Optimistic Gradient Descent/Ascent in Infinite-horizon Competitive Markov Games](http://arxiv.org/abs/2102.04540)


  We study infinite-horizon discounted two-player zero-sum Markov games, and
develop a decentralized algorithm that provably converges to the set of Nash
equilibria under self-play. Our algorithm is based on running an Optimistic
Gradient Descent Ascent algorithm on each state to learn the policies, with a
critic that slowly learns the value of each state. To the best of our
knowledge, this is the first algorithm in this setting that is simultaneously
rational (converging to the opponent's best response when it uses a stationary
policy), convergent (converging to the set of Nash equilibria under self-play),
agnostic (no need to know the actions played by the opponent), symmetric
(players taking symmetric roles in the algorithm), and enjoying a finite-time
last-iterate convergence guarantee, all of which are desirable properties of
decentralized algorithms.

    

### [[2102.05261] Simple Agent, Complex Environment: Efficient Reinforcement Learning with Agent States](http://arxiv.org/abs/2102.05261)


  We design a simple reinforcement learning (RL) agent that implements an
optimistic version of $Q$-learning and establish through regret analysis that
this agent can operate with some level of competence in any environment. While
we leverage concepts from the literature on provably efficient RL, we consider
a general agent-environment interface and provide a novel agent design and
analysis. This level of generality positions our results to inform the design
of future agents for operation in complex real environments. We establish that,
as time progresses, our agent performs competitively relative to policies that
require longer times to evaluate. The time it takes to approach asymptotic
performance is polynomial in the complexity of the agent's state representation
and the time required to evaluate the best policy that the agent can represent.
Notably, there is no dependence on the complexity of the environment. The
ultimate per-period performance loss of the agent is bounded by a constant
multiple of a measure of distortion introduced by the agent's state
representation. This work is the first to establish that an algorithm
approaches this asymptotic condition within a tractable time frame.

    

### [[2102.05836] Online Deterministic Annealing for Classification and Clustering](http://arxiv.org/abs/2102.05836)


  Inherent in virtually every iterative machine learning algorithm is the
problem of hyper-parameter tuning, which includes three major design
parameters: (a) the complexity of the model, e.g., the number of neurons in a
neural network, (b) the initial conditions, which heavily affect the behavior
of the algorithm, and (c) the dissimilarity measure used to quantify its
performance. We introduce an online prototype-based learning algorithm that can
be viewed as a progressively growing competitive-learning neural network
architecture for classification and clustering. The learning rule of the
proposed approach is formulated as an online gradient-free stochastic
approximation algorithm that solves a sequence of appropriately defined
optimization problems, simulating an annealing process. The annealing nature of
the algorithm contributes to avoiding poor local minima, offers robustness with
respect to the initial conditions, and provides a means to progressively
increase the complexity of the learning model, through an intuitive bifurcation
phenomenon. The proposed approach is interpretable, requires minimal
hyper-parameter tuning, and allows online control over the
performance-complexity trade-off. Finally, we show that Bregman divergences
appear naturally as a family of dissimilarity measures that play a central role
in both the performance and the computational complexity of the learning
algorithm. Experimental results illustrate the properties and evaluate the
performance of the proposed learning algorithm.

    

### [[2102.06966] Graph Convolution for Semi-Supervised Classification: Improved Linear Separability and Out-of-Distribution Generalization](http://arxiv.org/abs/2102.06966)


  Recently there has been increased interest in semi-supervised classification
in the presence of graphical information. A new class of learning models has
emerged that relies, at its most basic level, on classifying the data after
first applying a graph convolution. To understand the merits of this approach,
we study the classification of a mixture of Gaussians, where the data
corresponds to the node attributes of a stochastic block model. We show that
graph convolution extends the regime in which the data is linearly separable by
a factor of roughly $1/\sqrt{D}$, where $D$ is the expected degree of a node,
as compared to the mixture model data on its own. Furthermore, we find that the
linear classifier obtained by minimizing the cross-entropy loss after the graph
convolution generalizes to out-of-distribution data where the unseen data can
have different intra- and inter-class edge probabilities from the training
data.

    

### [[2102.07801] Enhancing the Spatio-temporal Observability of Grid-Edge Resources in Distribution Grids](http://arxiv.org/abs/2102.07801)


  Enhancing the spatio-temporal observability of distributed energy resources
(DERs) is crucial for achieving secure and efficient operations in distribution
grids. This paper puts forth a joint recovery framework for residential loads
by leveraging the complimentary strengths of heterogeneous types of
measurements. The proposed approaches integrate the low-resolution smart meter
data collected for every load node with the fast-sampled feeder-level
measurements provided by limited number of phasor measurement units. To address
the lack of data, we exploit two key characteristics for the loads and DERs,
namely the sparse changes due to infrequent activities of appliances and
electric vehicles (EVs) and the locational dependence of solar photovoltaic
(PV) generation. Accordingly, meaningful regularization terms are introduced to
cast a convex load recovery problem, which will be further simplified to reduce
computational complexity. The load recovery solutions can be utilized to
identify the EV charging events at each load node and to infer the total
behind-the-meter PV output. Numerical tests using real-world data have
demonstrated the effectiveness of the proposed approaches in enhancing the
visibility of these grid-edge DERs.

    

### [[2102.07804] Scaling Up Exact Neural Network Compression by ReLU Stability](http://arxiv.org/abs/2102.07804)


  We can compress a neural network while exactly preserving its underlying
functionality with respect to a given input domain if some of its neurons are
stable. However, current approaches to determine the stability of neurons with
Rectified Linear Unit (ReLU) activations require solving or finding a good
approximation to multiple discrete optimization problems. In this work, we
introduce an algorithm based on solving a single optimization problem to
identify all stable neurons. Our approach is on median 100 times faster than
the state-of-art method, which allows us to explore exact compression on deeper
(5 x 100) and wider (2 x 800) networks within minutes. For classifiers trained
under an amount of L1 regularization that does not worsen accuracy, we can
remove up to 40% of the connections

    

### [[2102.12108] The Promises and Pitfalls of Deep Kernel Learning](http://arxiv.org/abs/2102.12108)


  Deep kernel learning (DKL) and related techniques aim to combine the
representational power of neural networks with the reliable uncertainty
estimates of Gaussian processes. One crucial aspect of these models is an
expectation that, because they are treated as Gaussian process models optimized
using the marginal likelihood, they are protected from overfitting. However, we
identify situations where this is not the case. We explore this behavior,
explain its origins and consider how it applies to real datasets. Through
careful experimentation on the UCI, CIFAR-10, and the UTKFace datasets, we find
that the overfitting from overparameterized maximum marginal likelihood, in
which the model is "somewhat Bayesian", can in certain scenarios be worse than
that from not being Bayesian at all. We explain how and when DKL can still be
successful by investigating optimization dynamics. We also find that failures
of DKL can be rectified by a fully Bayesian treatment, which leads to the
desired performance improvements over standard neural networks and Gaussian
processes.

    

### [[2103.00671] Robust learning under clean-label attack](http://arxiv.org/abs/2103.00671)


  We study the problem of robust learning under clean-label data-poisoning
attacks, where the attacker injects (an arbitrary set of) correctly-labeled
examples to the training set to fool the algorithm into making mistakes on
specific test instances at test time. The learning goal is to minimize the
attackable rate (the probability mass of attackable test instances), which is
more difficult than optimal PAC learning. As we show, any robust algorithm with
diminishing attackable rate can achieve the optimal dependence on $\epsilon$ in
its PAC sample complexity, i.e., $O(1/\epsilon)$. On the other hand, the
attackable rate might be large even for some optimal PAC learners, e.g., SVM
for linear classifiers. Furthermore, we show that the class of linear
hypotheses is not robustly learnable when the data distribution has zero margin
and is robustly learnable in the case of positive margin but requires sample
complexity exponential in the dimension. For a general hypothesis class with
bounded VC dimension, if the attacker is limited to add at most $t>0$ poison
examples, the optimal robust learning sample complexity grows almost linearly
with $t$.

    

### [[2103.02718] A Modified Drake Equation for Assessing Adversarial Risk to Machine Learning Models](http://arxiv.org/abs/2103.02718)


  Machine learning models present a risk of adversarial attack when deployed in
production. Quantifying the contributing factors and uncertainties using
empirical measures could assist the industry with assessing the risk of
downloading and deploying common model types. This work proposes modifying the
traditional Drake Equation's formalism to estimate the number of potentially
successful adversarial attacks on a deployed model. The Drake Equation is
famously used for parameterizing uncertainties and it has been used in many
research fields outside of its original intentions to estimate the number of
radio-capable extra-terrestrial civilizations. While previous work has outlined
methods for discovering vulnerabilities in public model architectures, the
proposed equation seeks to provide a semi-quantitative benchmark for evaluating
and estimating the potential risk factors for adversarial attacks.

    

### [[2103.03323] Distribution-free uncertainty quantification for classification under label shift](http://arxiv.org/abs/2103.03323)


  Trustworthy deployment of ML models requires a proper measure of uncertainty,
especially in safety-critical applications. We focus on uncertainty
quantification (UQ) for classification problems via two avenues -- prediction
sets using conformal prediction and calibration of probabilistic predictors by
post-hoc binning -- since these possess distribution-free guarantees for i.i.d.
data. Two common ways of generalizing beyond the i.i.d. setting include
handling covariate and label shift. Within the context of distribution-free UQ,
the former has already received attention, but not the latter. It is known that
label shift hurts prediction, and we first argue that it also hurts UQ, by
showing degradation in coverage and calibration. Piggybacking on recent
progress in addressing label shift (for better prediction), we examine the
right way to achieve UQ by reweighting the aforementioned conformal and
calibration procedures whenever some unlabeled data from the target
distribution is available. We examine these techniques theoretically in a
distribution-free framework and demonstrate their excellent practical
performance.

    

### [[2103.03518] Anomaly detection and automatic labeling for solar cell quality inspection based on Generative Adversarial Network](http://arxiv.org/abs/2103.03518)


  Quality inspection applications in industry are required to move towards a
zero-defect manufacturing scenario, withnon-destructive inspection and
traceability of 100 % of produced parts. Developing robust fault detection and
classification modelsfrom the start-up of the lines is challenging due to the
difficulty in getting enough representative samples of the faulty patternsand
the need to manually label them. This work presents a methodology to develop a
robust inspection system, targeting thesepeculiarities, in the context of solar
cell manufacturing. The methodology is divided into two phases: In the first
phase, an anomalydetection model based on a Generative Adversarial Network
(GAN) is employed. This model enables the detection and localizationof
anomalous patterns within the solar cells from the beginning, using only
non-defective samples for training and without anymanual labeling involved. In
a second stage, as defective samples arise, the detected anomalies will be used
as automaticallygenerated annotations for the supervised training of a Fully
Convolutional Network that is capable of detecting multiple types offaults. The
experimental results using 1873 EL images of monocrystalline cells show that
(a) the anomaly detection scheme can beused to start detecting features with
very little available data, (b) the anomaly detection may serve as automatic
labeling in order totrain a supervised model, and (c) segmentation and
classification results of supervised models trained with automatic labels
arecomparable to the ones obtained from the models trained with manual labels.

    

### [[2103.08800] Predicting Opioid Use Disorder from Longitudinal Healthcare Data using Multi-stream Transformer](http://arxiv.org/abs/2103.08800)


  Opioid Use Disorder (OUD) is a public health crisis costing the US billions
of dollars annually in healthcare, lost workplace productivity, and crime.
Analyzing longitudinal healthcare data is critical in addressing many
real-world problems in healthcare. Leveraging the real-world longitudinal
healthcare data, we propose a novel multi-stream transformer model called MUPOD
for OUD identification. MUPOD is designed to simultaneously analyze multiple
types of healthcare data streams, such as medications and diagnoses, by
attending to segments within and across these data streams. Our model tested on
the data from 392,492 patients with long-term back pain problems showed
significantly better performance than the traditional models and recently
developed deep learning models.

    

### [[2103.12711] A Pseudo-Metric between Probability Distributions based on Depth-Trimmed Regions](http://arxiv.org/abs/2103.12711)


  Data depth is a non parametric statistical tool that measures centrality of
any element $x\in\mathbb{R}^d$ with respect to (w.r.t.) a probability
distribution or a data set. It is a natural median-oriented extension of the
cumulative distribution function (cdf) to the multivariate case. Consequently,
its upper level sets -- the depth-trimmed regions -- give rise to a definition
of multivariate quantiles. In this work, we propose two new pseudo-metrics
between continuous probability measures based on data depth and its associated
central regions. The first one is constructed as the Lp-distance between data
depth w.r.t. each distribution while the second one relies on the Hausdorff
distance between their quantile regions. It can further be seen as an original
way to extend the one-dimensional formulae of the Wasserstein distance, which
involves quantiles and cdfs, to the multivariate space. After discussing the
properties of these pseudo-metrics and providing conditions under which they
define a distance, we highlight similarities with the Wasserstein distance.
Interestingly, the derived non-asymptotic bounds show that in contrast to the
Wasserstein distance, the proposed pseudo-metrics do not suffer from the curse
of dimensionality. Moreover, based on the support function of a convex body, we
propose an efficient approximation possessing linear time complexity w.r.t. the
size of the data set and its dimension. The quality of this approximation as
well as the performance of the proposed approach are illustrated in
experiments. Furthermore, by construction the regions-based pseudo-metric
appears to be robust w.r.t. both outliers and heavy tails, a behavior witnessed
in the numerical experiments.

    

### [[2104.09343] Approximate Multi-Agent Fitted Q Iteration](http://arxiv.org/abs/2104.09343)


  We formulate an efficient approximation for multi-agent batch reinforcement
learning, the approximate multi-agent fitted Q iteration (AMAFQI). We present a
detailed derivation of our approach. We propose an iterative policy search and
show that it yields a greedy policy with respect to multiple approximations of
the centralized, standard Q-function. In each iteration and policy evaluation,
AMAFQI requires a number of computations that scales linearly with the number
of agents whereas the analogous number of computations increase exponentially
for the fitted Q iteration (FQI), one of the most commonly used approaches in
batch reinforcement learning. This property of AMAFQI is fundamental for the
design of a tractable multi-agent approach. We evaluate the performance of
AMAFQI and compare it to FQI in numerical simulations. Numerical examples
illustrate the significant computation time reduction when using AMAFQI instead
of FQI in multi-agent problems and corroborate the similar decision-making
performance of both approaches.

    

### [[2104.10644] A Comparative Study of Using Spatial-Temporal Graph Convolutional Networks for Predicting Availability in Bike Sharing Schemes](http://arxiv.org/abs/2104.10644)


  Accurately forecasting transportation demand is crucial for efficient urban
traffic guidance, control and management. One solution to enhance the level of
prediction accuracy is to leverage graph convolutional networks (GCN), a neural
network based modelling approach with the ability to process data contained in
graph based structures. As a powerful extension of GCN, a spatial-temporal
graph convolutional network (ST-GCN) aims to capture the relationship of data
contained in the graphical nodes across both spatial and temporal dimensions,
which presents a novel deep learning paradigm for the analysis of complex
time-series data that also involves spatial information as present in
transportation use cases. In this paper, we present an Attention-based ST-GCN
(AST-GCN) for predicting the number of available bikes in bike-sharing systems
in cities, where the attention-based mechanism is introduced to further improve
the performance of an ST-GCN. Furthermore, we also discuss the impacts of
different modelling methods of adjacency matrices on the proposed architecture.
Our experimental results are presented using two real-world datasets,
Dublinbikes and NYC-Citi Bike, to illustrate the efficacy of our proposed model
which outperforms the majority of existing approaches.

    

### [[2105.04170] AutoDebias: Learning to Debias for Recommendation](http://arxiv.org/abs/2105.04170)


  Recommender systems rely on user behavior data like ratings and clicks to
build personalization model. However, the collected data is observational
rather than experimental, causing various biases in the data which
significantly affect the learned model. Most existing work for recommendation
debiasing, such as the inverse propensity scoring and imputation approaches,
focuses on one or two specific biases, lacking the universal capacity that can
account for mixed or even unknown biases in the data.
Towards this research gap, we first analyze the origin of biases from the
perspective of \textit{risk discrepancy} that represents the difference between
the expectation empirical risk and the true risk. Remarkably, we derive a
general learning framework that well summarizes most existing debiasing
strategies by specifying some parameters of the general framework. This
provides a valuable opportunity to develop a universal solution for debiasing,
e.g., by learning the debiasing parameters from data. However, the training
data lacks important signal of how the data is biased and what the unbiased
data looks like. To move this idea forward, we propose \textit{AotoDebias} that
leverages another (small) set of uniform data to optimize the debiasing
parameters by solving the bi-level optimization problem with meta-learning.
Through theoretical analyses, we derive the generalization bound for AutoDebias
and prove its ability to acquire the appropriate debiasing strategy. Extensive
experiments on two real datasets and a simulated dataset demonstrated
effectiveness of AutoDebias. The code is available at
\url{this https URL}.

    

### [[2105.06018] Robust Dynamic Multi-Modal Data Fusion: A Model Uncertainty Perspective](http://arxiv.org/abs/2105.06018)


  This paper is concerned with multi-modal data fusion (MMDF) under unexpected
modality failures in nonlinear non-Gaussian dynamic processes. An efficient
framework to tackle this problem is proposed. In particular, a notion termed
modality "\emph{usefulness}", which takes a value of 1 or 0, is used for
indicating whether the observation of this modality is useful or not. For $n$
modalities involved, $2^n$ combinations of their "\emph{usefulness}" values
exist. Each combination defines one hypothetical model of the true data
generative process. Then the problem of concern is formalized as a task of
nonlinear non-Gaussian state filtering under model uncertainty, which is
addressed by a dynamic model averaging (DMA) based particle filter (PF)
algorithm. This DMA algorithm employs $2^n$ models, while all models share the
same state-transition function and a unique set of particle values. That makes
the computational complexity of this algorithm only slightly larger than a
single model based PF algorithm, especially for scenarios in which $n$ is
small. Experimental results show that the proposed solution outperforms
remarkably state-of-the-art methods. Code and data are available at
this https URL.

    

### [[2105.12833] Simulated Data Generation Through Algorithmic Force Coefficient Estimation for AI-Based Robotic Projectile Launch Modeling](http://arxiv.org/abs/2105.12833)


  Modeling of non-rigid object launching and manipulation is complex
considering the wide range of dynamics affecting trajectory, many of which may
be unknown. Using physics models can be inaccurate because they cannot account
for unknown factors and the effects of the deformation of the object as it is
launched; moreover, deriving force coefficients for these models is not
possible without extensive experimental testing. Recently, advancements in
data-powered artificial intelligence methods have allowed learnable models and
systems to emerge. It is desirable to train a model for launch prediction on a
robot, as deep neural networks can account for immeasurable dynamics. However,
the inability to collect large amounts of experimental data decreases
performance of deep neural networks. Through estimating force coefficients, the
accepted physics models can be leveraged to produce adequate supplemental data
to artificially increase the size of the training set, yielding improved neural
networks. In this paper, we introduce a new framework for algorithmic
estimation of force coefficients for non-rigid object launching, which can be
generalized to other domains, in order to generate large datasets. We implement
a novel training algorithm and objective for our deep neural network to
accurately model launch trajectory of non-rigid objects and predict whether
they will hit a series of targets. Our experimental results demonstrate the
effectiveness of using simulated data from force coefficient estimation and
shows the importance of simulated data for training an effective neural
network.

    

### [[2106.02295] Differentiable Dynamic Quantization with Mixed Precision and Adaptive Resolution](http://arxiv.org/abs/2106.02295)


  Model quantization is challenging due to many tedious hyper-parameters such
as precision (bitwidth), dynamic range (minimum and maximum discrete values)
and stepsize (interval between discrete values). Unlike prior arts that
carefully tune these values, we present a fully differentiable approach to
learn all of them, named Differentiable Dynamic Quantization (DDQ), which has
several benefits. (1) DDQ is able to quantize challenging lightweight
architectures like MobileNets, where different layers prefer different
quantization parameters. (2) DDQ is hardware-friendly and can be easily
implemented using low-precision matrix-vector multiplication, making it capable
in many hardware such as ARM. (3) Extensive experiments show that DDQ
outperforms prior arts on many networks and benchmarks, especially when models
are already efficient and compact. e.g., DDQ is the first approach that
achieves lossless 4-bit quantization for MobileNetV2 on ImageNet.

    

### [[2106.06356] Nonmyopic Multifidelity Active Search](http://arxiv.org/abs/2106.06356)


  Active search is a learning paradigm where we seek to identify as many
members of a rare, valuable class as possible given a labeling budget. Previous
work on active search has assumed access to a faithful (and expensive) oracle
reporting experimental results. However, some settings offer access to cheaper
surrogates such as computational simulation that may aid in the search. We
propose a model of multifidelity active search, as well as a novel,
computationally efficient policy for this setting that is motivated by
state-of-the-art classical policies. Our policy is nonmyopic and budget aware,
allowing for a dynamic tradeoff between exploration and exploitation. We
evaluate the performance of our solution on real-world datasets and demonstrate
significantly better performance than natural benchmarks.

    

### [[2106.09330] A Simple Generative Network](http://arxiv.org/abs/2106.09330)


  Generative neural networks are able to mimic intricate probability
distributions such as those of handwritten text, natural images, etc. Since
their inception several models were proposed. The most successful of these were
based on adversarial (GAN), auto-encoding (VAE) and maximum mean discrepancy
(MMD) relatively complex architectures and schemes. Surprisingly, a very simple
architecture (a single feed-forward neural network) in conjunction with an
obvious optimization goal (Kullback_Leibler divergence) was apparently
overlooked. This paper demonstrates that such a model (denoted SGN for its
simplicity) is able to generate samples visually and quantitatively competitive
as compared with the fore-mentioned state of the art methods.

    

### [[2106.10800] Lossy Compression for Lossless Prediction](http://arxiv.org/abs/2106.10800)


  Most data is automatically collected and only ever "seen" by algorithms. Yet,
data compressors preserve perceptual fidelity rather than just the information
needed by algorithms performing downstream tasks. In this paper, we
characterize the bit-rate required to ensure high performance on all predictive
tasks that are invariant under a set of transformations, such as data
augmentations. Based on our theory, we design unsupervised objectives for
training neural compressors. Using these objectives, we train a generic image
compressor that achieves substantial rate savings (more than $1000\times$ on
ImageNet) compared to JPEG on 8 datasets, without decreasing downstream
classification performance.

    

### [[2106.11232] Multi-VAE: Learning Disentangled View-common and View-peculiar Visual Representations for Multi-view Clustering](http://arxiv.org/abs/2106.11232)


  Multi-view clustering, a long-standing and important research problem,
focuses on mining complementary information from diverse views. However,
existing works often fuse multiple views' representations or handle clustering
in a common feature space, which may result in their entanglement especially
for visual representations. To address this issue, we present a novel VAE-based
multi-view clustering framework (Multi-VAE) by learning disentangled visual
representations. Concretely, we define a view-common variable and multiple
view-peculiar variables in the generative model. The prior of view-common
variable obeys approximately discrete Gumbel Softmax distribution, which is
introduced to extract the common cluster factor of multiple views. Meanwhile,
the prior of view-peculiar variable follows continuous Gaussian distribution,
which is used to represent each view's peculiar visual factors. By controlling
the mutual information capacity to disentangle the view-common and
view-peculiar representations, continuous visual information of multiple views
can be separated so that their common discrete cluster information can be
effectively mined. Experimental results demonstrate that Multi-VAE enjoys the
disentangled and explainable visual representations, while obtaining superior
clustering performance compared with state-of-the-art methods.

    

### [[2106.12798] Evaluation of Representation Models for Text Classification with AutoML Tools](http://arxiv.org/abs/2106.12798)


  Automated Machine Learning (AutoML) has gained increasing success on tabular
data in recent years. However, processing unstructured data like text is a
challenge and not widely supported by open-source AutoML tools. This work
compares three manually created text representations and text embeddings
automatically created by AutoML tools. Our benchmark includes four popular
open-source AutoML tools and eight datasets for text classification purposes.
The results show that straightforward text representations perform better than
AutoML tools with automatically created text embeddings.

    

### [[2106.13749] Jitter: Random Jittering Loss Function](http://arxiv.org/abs/2106.13749)


  Regularization plays a vital role in machine learning optimization. One novel
regularization method called flooding makes the training loss fluctuate around
the flooding level. It intends to make the model continue to random walk until
it comes to a flat loss landscape to enhance generalization. However, the
hyper-parameter flooding level of the flooding method fails to be selected
properly and uniformly. We propose a novel method called Jitter to improve it.
Jitter is essentially a kind of random loss function. Before training, we
randomly sample the Jitter Point from a specific probability distribution. The
flooding level should be replaced by Jitter point to obtain a new target
function and train the model accordingly. As Jitter point acting as a random
factor, we actually add some randomness to the loss function, which is
consistent with the fact that there exists innumerable random behaviors in the
learning process of the machine learning model and is supposed to make the
model more robust. In addition, Jitter performs random walk randomly which
divides the loss curve into small intervals and then flipping them over,
ideally making the loss curve much flatter and enhancing generalization
ability. Moreover, Jitter can be a domain-, task-, and model-independent
regularization method and train the model effectively after the training error
reduces to zero. Our experimental results show that Jitter method can improve
model performance more significantly than the previous flooding method and make
the test loss curve descend twice.

    

### [[2106.13867] POLAR: A Polynomial Arithmetic Framework for Verifying Neural-Network Controlled Systems](http://arxiv.org/abs/2106.13867)


  We propose POLAR, a \textbf{pol}ynomial \textbf{ar}ithmetic framework that
leverages polynomial overapproximations with interval remainders for
bounded-time reachability analysis of neural network-controlled systems
(NNCSs). Compared with existing arithmetic approaches that use standard Taylor
models, our framework uses a novel approach to iteratively overapproximate the
neuron output ranges layer-by-layer with a combination of Bernstein polynomial
interpolation for continuous activation functions and Taylor model arithmetic
for the other operations. This approach can overcome the main drawback in the
standard Taylor model arithmetic, i.e. its inability to handle functions that
cannot be well approximated by Taylor polynomials, and significantly improve
the accuracy and efficiency of reachable states computation for NNCSs. To
further tighten the overapproximation, our method keeps the Taylor model
remainders symbolic under the linear mappings when estimating the output range
of a neural network. We show that POLAR can be seamlessly integrated with
existing Taylor model flowpipe construction techniques, and demonstrate that
POLAR significantly outperforms the current state-of-the-art techniques on a
suite of benchmarks.

    

### [[2107.02045] Understanding the Security of Deepfake Detection](http://arxiv.org/abs/2107.02045)


  Deepfakes pose growing challenges to the trust of information on the
Internet. Thus, detecting deepfakes has attracted increasing attentions from
both academia and industry. State-of-the-art deepfake detection methods consist
of two key components, i.e., face extractor and face classifier, which extract
the face region in an image and classify it to be real/fake, respectively.
Existing studies mainly focused on improving the detection performance in
non-adversarial settings, leaving security of deepfake detection in adversarial
settings largely unexplored. In this work, we aim to bridge the gap. In
particular, we perform a systematic measurement study to understand the
security of the state-of-the-art deepfake detection methods in adversarial
settings. We use two large-scale public deepfakes data sources including
FaceForensics++ and Facebook Deepfake Detection Challenge, where the deepfakes
are fake face images; and we train state-of-the-art deepfake detection methods.
These detection methods can achieve 0.94--0.99 accuracies in non-adversarial
settings on these datasets. However, our measurement results uncover multiple
security limitations of the deepfake detection methods in adversarial settings.
First, we find that an attacker can evade a face extractor, i.e., the face
extractor fails to extract the correct face regions, via adding small Gaussian
noise to its deepfake images. Second, we find that a face classifier trained
using deepfakes generated by one method cannot detect deepfakes generated by
another method, i.e., an attacker can evade detection via generating deepfakes
using a new method. Third, we find that an attacker can leverage backdoor
attacks developed by the adversarial machine learning community to evade a face
classifier. Our results highlight that deepfake detection should consider the
adversarial nature of the problem.

    

### [[2101.00135] Faults in Deep Reinforcement Learning Programs: A Taxonomy and A Detection Approach](http://arxiv.org/abs/2101.00135)


  A growing demand is witnessed in both industry and academia for employing
Deep Learning (DL) in various domains to solve real-world problems. Deep
Reinforcement Learning (DRL) is the application of DL in the domain of
Reinforcement Learning (RL). Like any software systems, DRL applications can
fail because of faults in their programs. In this paper, we present the first
attempt to categorize faults occurring in DRL programs. We manually analyzed
761 artifacts of DRL programs (from Stack Overflow posts and GitHub issues)
developed using well-known DRL frameworks (OpenAI Gym, Dopamine, Keras-rl,
Tensorforce) and identified faults reported by developers/users. We labeled and
taxonomized the identified faults through several rounds of discussions. The
resulting taxonomy is validated using an online survey with 19
developers/researchers. To allow for the automatic detection of faults in DRL
programs, we have defined a meta-model of DRL programs and developed DRLinter,
a model-based fault detection approach that leverages static analysis and graph
transformations. The execution flow of DRLinter consists in parsing a DRL
program to generate a model conforming to our meta-model and applying detection
rules on the model to identify faults occurrences. The effectiveness of
DRLinter is evaluated using 15 synthetic DRLprograms in which we injected
faults observed in the analyzed artifacts of the taxonomy. The results show
that DRLinter can successfully detect faults in all synthetic faulty programs.

    

### [[2105.08095] Automatic Fault Detection for Deep Learning Programs Using Graph Transformations](http://arxiv.org/abs/2105.08095)


  Nowadays, we are witnessing an increasing demand in both corporates and
academia for exploiting Deep Learning (DL) to solve complex real-world
problems. A DL program encodes the network structure of a desirable DL model
and the process by which the model learns from the training dataset. Like any
software, a DL program can be faulty, which implies substantial challenges of
software quality assurance, especially in safety-critical domains. It is
therefore crucial to equip DL development teams with efficient fault detection
techniques and tools. In this paper, we propose NeuraLint, a model-based fault
detection approach for DL programs, using meta-modelling and graph
transformations. First, we design a meta-model for DL programs that includes
their base skeleton and fundamental properties. Then, we construct a
graph-based verification process that covers 23 rules defined on top of the
meta-model and implemented as graph transformations to detect faults and design
inefficiencies in the generated models (i.e., instances of the meta-model).
First, the proposed approach is evaluated by finding faults and design
inefficiencies in 28 synthesized examples built from common problems reported
in the literature. Then NeuraLint successfully finds 64 faults and design
inefficiencies in 34 real-world DL programs extracted from Stack Overflow posts
and GitHub repositories. The results show that NeuraLint effectively detects
faults and design issues in both synthesized and real-world examples with a
recall of 70.5 % and a precision of 100 %. Although the proposed meta-model is
designed for feedforward neural networks, it can be extended to support other
neural network architectures such as recurrent neural networks. Researchers can
also expand our set of verification rules to cover more types of issues in DL
programs.

    

### [[2106.08499] ICDAR 2021 Competition on Components Segmentation Task of Document Photos](http://arxiv.org/abs/2106.08499)


  This paper describes the short-term competition on Components Segmentation
Task of Document Photos that was prepared in the context of the 16th
International Conference on Document Analysis and Recognition (ICDAR 2021).
This competition aims to bring together researchers working on the filed of
identification document image processing and provides them a suitable benchmark
to compare their techniques on the component segmentation task of document
images. Three challenge tasks were proposed entailing different segmentation
assignments to be performed on a provided dataset. The collected data are from
several types of Brazilian ID documents, whose personal information was
conveniently replaced. There were 16 participants whose results obtained for
some or all the three tasks show different rates for the adopted metrics, like
Dice Similarity Coefficient ranging from 0.06 to 0.99. Different Deep Learning
models were applied by the entrants with diverse strategies to achieve the best
results in each of the tasks. Obtained results show that the current applied
methods for solving one of the proposed tasks (document boundary detection) are
already well stablished. However, for the other two challenge tasks (text zone
and handwritten sign detection) research and development of more robust
approaches are still required to achieve acceptable results.

    

### [[2107.03096] R2F: A Remote Retraining Framework for AIoT Processors with Computing Errors](http://arxiv.org/abs/2107.03096)


  AIoT processors fabricated with newer technology nodes suffer rising soft
errors due to the shrinking transistor sizes and lower power supply. Soft
errors on the AIoT processors particularly the deep learning accelerators
(DLAs) with massive computing may cause substantial computing errors. These
computing errors are difficult to be captured by the conventional training on
general purposed processors like CPUs and GPUs in a server. Applying the
offline trained neural network models to the edge accelerators with errors
directly may lead to considerable prediction accuracy loss.
To address the problem, we propose a remote retraining framework (R2F) for
remote AIoT processors with computing errors. It takes the remote AIoT
processor with soft errors in the training loop such that the on-site computing
errors can be learned with the application data on the server and the retrained
models can be resilient to the soft errors. Meanwhile, we propose an optimized
partial TMR strategy to enhance the retraining. According to our experiments,
R2F enables elastic design trade-offs between the model accuracy and the
performance penalty. The top-5 model accuracy can be improved by 1.93%-13.73%
with 0%-200% performance penalty at high fault error rate. In addition, we
notice that the retraining requires massive data transmission and even
dominates the training time, and propose a sparse increment compression
approach for the data transmission optimization, which reduces the retraining
time by 38%-88% on average with negligible accuracy loss over a straightforward
remote retraining.

    

### [[2107.02841] Toward Interlanguage Parallel Scripting for Distributed-Memory Scientific Computing](http://arxiv.org/abs/2107.02841)


  Scripting languages such as Python and R have been widely adopted as tools
for the productive development of scientific software because of the power and
expressiveness of the languages and available libraries. However, deploying
scripted applications on large-scale parallel computer systems such as the IBM
Blue Gene/Q or Cray XE6 is a challenge because of issues including operating
system limitations, interoperability challenges, parallel filesystem overheads
due to the small file system accesses common in scripted approaches, and other
issues. We present here a new approach to these problems in which the Swift
scripting system is used to integrate high-level scripts written in Python, R,
and Tcl, with native code developed in C, C++, and Fortran, by linking Swift to
the library interfaces to the script interpreters. In this approach, Swift
handles data management, movement, and marshaling among distributed-memory
processes without direct user manipulation of low-level communication libraries
such as MPI. We present a technique to efficiently launch scripted applications
on large-scale supercomputers using a hierarchical programming model.

    

### [[2107.02944] Do Small Firms Implement Enterprise Systems Differently? The Case of E-Silk Route Ventures](http://arxiv.org/abs/2107.02944)


  The cost effectiveness, ease of learning, connectedness and in-depth
analytical capabilities provided through cloud computing technologies, have
provided small firms the opportunity to implement enterprise systems (ES),
which was reserved only for the much resourceful firms. However, it is evident
that small firms are still struggling to attain purported benefits of ES and
still find it difficult to manage complexities of ES implementations. Further,
limited research has been conducted that discusses ES implementation in small
firms. This research is an attempt to further the understanding of
ES-implementation of small firms and re-evaluate the applicability of
fundamental critical success factors to small firms.

    

### [[2107.02945] The Implementation of an ES in a Small Firm: The case of Silk Cooperation](http://arxiv.org/abs/2107.02945)


  The introduction of cloud computing has provided opportunities for small
businesses to implement enterprise systems (ES) in their organizations and
thereby improve their business processes. While there have been many studies
focusing on ES implementation among medium-large sized firms, the factors that
influence the implementations of ES in such firms are different to that of
small firms. This teaching case discusses an implementation of a cloud
enterprise resource planning (ERP) system in a small firm in the Asian region.
The case illustrates factors that enabled successful implementation of a cloud
ERP system in a small firm and the lessons learnt through this successful
endeavor. The case study and the teaching notes are suitable for any
undergraduate or postgraduate cohort, following a course in management
information systems.

    

### [[2107.03078] Can Connected Autonomous Vehicles really improve mixed traffic efficiency in realistic scenarios?](http://arxiv.org/abs/2107.03078)


  Connected autonomous vehicles (CAVs) can supplement the information from
their own sensors with information from surrounding CAVs for decision making
and control. This has the potential to improve traffic efficiency. CAVs face
additional challenges in their driving, however, when they interact with
human-driven vehicles (HDVs) in mixed-traffic environments due to the
uncertainty in human's driving behavior e.g. larger reaction times, perception
errors, etc. While a lot of research has investigated the impact of CAVs on
traffic safety and efficiency at different penetration rates, all have assumed
either perfect communication or very simple scenarios with imperfect
communication. In practice, the presence of communication delays and packet
losses means that CAVs might receive only partial information from surrounding
vehicles, and this can have detrimental effects on their performance. This
paper investigates the impact of CAVs on traffic efficiency in realistic
communication and road network scenarios (i.e. imperfect communication and
large-scale road network). We analyze the effect of unreliable communication
links on CAVs operation in mixed traffic with various penetration rates and
evaluate traffic performance in congested traffic scenarios on a large-scale
road network (the M50 motorway, in Ireland). Results show that CAVs can
significantly improve traffic efficiency in congested traffic scenarios at high
penetration rates. The scale of the improvement depends on communication
reliability, with a packet drop rate of 70% leading to an increase in traffic
congestion by 28.7% and 11.88% at 40% and 70% penetration rates respectively
compared to perfect communication.

    

### [[2107.03341] Burrows Wheeler Transform on a Large Scale: Algorithms Implemented in Apache Spark](http://arxiv.org/abs/2107.03341)


  With the rapid growth of Next Generation Sequencing (NGS) technologies, large
amounts of "omics" data are daily collected and need to be processed. Indexing
and compressing large sequences datasets are some of the most important tasks
in this context. Here we propose algorithms for the computation of Burrows
Wheeler transform relying on Big Data technologies, i.e., Apache Spark and
Hadoop. Our algorithms are the first ones that distribute the index computation
and not only the input dataset, allowing to fully benefit of the available
cloud resources.

    

### [[2107.03367] Building Stable Off-chain Payment Networks](http://arxiv.org/abs/2107.03367)


  Payment channel is a protocol which allows cryptocurrency users to route
multiple transactions through network without committing them to the main
blockchain network (mainnet). This ability makes them the most prominent
solution to blockchains' scalability problem. Each modification of payment
channels requires a transaction on the mainnet and therefore, big transaction
fees. In this paper, we assume that a set of payment transactions are given
(batch or online) and we study the problem of scheduling modificiations on
payment channels to route all of the transactions with minimum modification
cost.
We investigate two cost models for aforementioned problem: the step cost
function in which every channel modification has a constant cost and the linear
cost function in which modification costs are proportional to the amount of
change. For the step cost function model, we prove impossibility results for
both batch and online case. Moreover, some heuristic methods for the batch case
are presented and compared. For the linear cost we propose a polynomial time
algorithm using linear programming for the batch case.

    

### [[2010.09007] An Efficient and Balanced Graph Partition Algorithm for the Subgraph-Centric Programming Model on Large-scale Power-law Graphs](http://arxiv.org/abs/2010.09007)


  The subgraph-centric programming model is a promising approach and has been
applied in many state-of-the-art distributed graph computing frameworks.
However, traditional graph partition algorithms have significant difficulties
in processing large-scale power-law graphs. The major problem is the
communication bottleneck found in many subgraph-centric frameworks. Detailed
analysis indicates that the communication bottleneck is caused by the huge
communication volume or the extreme message imbalance among partitioned
subgraphs. The traditional partition algorithms do not consider both factors at
the same time, especially on power-law graphs.
In this paper, we propose a novel efficient and balanced vertex-cut graph
partition algorithm (EBV) which grants appropriate weights to the overall
communication cost and communication balance. We observe that the number of
replicated vertices and the balance of edge and vertex assignment have a great
influence on communication patterns of distributed subgraph-centric frameworks,
which further affect the overall performance. Based on this insight, We design
an evaluation function that quantifies the proportion of replicated vertices
and the balance of edges and vertices assignments as important parameters.
Besides, we sort the order of edge processing by the sum of end-vertices'
degrees from small to large. Experiments show that EBV reduces replication
factor and communication by at least 21.8% and 23.7% respectively than other
self-based partition algorithms. When deployed in the subgraph-centric
framework, it reduces the running time on power-law graphs by an average of
16.8% compared with the state-of-the-art partition algorithm. Our results
indicate that EBV has a great potential in improving the performance of
subgraph-centric frameworks for the parallel large-scale power-law graph
processing.

    

### [[2107.02865] Question Answering over Knowledge Graphs with Neural Machine Translation and Entity Linking](http://arxiv.org/abs/2107.02865)


  The goal of Question Answering over Knowledge Graphs (KGQA) is to find
answers for natural language questions over a knowledge graph. Recent KGQA
approaches adopt a neural machine translation (NMT) approach, where the natural
language question is translated into a structured query language. However, NMT
suffers from the out-of-vocabulary problem, where terms in a question may not
have been seen during training, impeding their translation. This issue is
particularly problematic for the millions of entities that large knowledge
graphs describe. We rather propose a KGQA approach that delegates the
processing of entities to entity linking (EL) systems. NMT is then used to
create a query template with placeholders that are filled by entities
identified in an EL phase. Slot filling is used to decide which entity fills
which placeholder. Experiments for QA over Wikidata show that our approach
outperforms pure NMT: while there remains a strong dependence on having seen
similar query templates during training, errors relating to entities are
greatly reduced.

    

### [[2107.02905] Identification and validation of Triamcinolone and Gallopamil as treatments for early COVID-19 via an in silico repurposing pipeline](http://arxiv.org/abs/2107.02905)


  SARS-CoV-2, the causative virus of COVID-19 continues to cause an ongoing
global pandemic. Therapeutics are still needed to treat mild and severe
COVID-19. Drug repurposing provides an opportunity to deploy drugs for COVID-19
more rapidly than developing novel therapeutics. Some existing drugs have shown
promise for treating COVID-19 in clinical trials. This in silico study uses
structural similarity to clinical trial drugs to identify two drugs with
potential applications to treat early COVID-19. We apply in silico validation
to suggest a possible mechanism of action for both. Triamcinolone is a
corticosteroid structurally similar to Dexamethasone. Gallopamil is a calcium
channel blocker structurally similar to Verapamil. We propose that both these
drugs could be useful to treat early COVID-19 infection due to the proximity of
their targets within a SARS-CoV-2-induced protein-protein interaction network
to kinases active in early infection, and the APOA1 protein which is linked to
the spread of COVID-19.

    

### [[2107.02955] Quadruped Locomotion on Non-Rigid Terrain using Reinforcement Learning](http://arxiv.org/abs/2107.02955)


  Legged robots need to be capable of walking on diverse terrain conditions. In
this paper, we present a novel reinforcement learning framework for learning
locomotion on non-rigid dynamic terrains. Specifically, our framework can
generate quadruped locomotion on flat elastic terrain that consists of a matrix
of tiles moving up and down passively when pushed by the robot's feet. A
trained robot with 55cm base length can walk on terrain that can sink up to
5cm. We propose a set of observation and reward terms that enable this
locomotion; in which we found that it is crucial to include the end-effector
history and end-effector velocity terms into observation. We show the
effectiveness of our method by training the robot with various terrain
conditions.

    

### [[2107.02975] Neural Natural Language Processing for Unstructured Data in Electronic Health Records: a Review](http://arxiv.org/abs/2107.02975)


  Electronic health records (EHRs), digital collections of patient healthcare
events and observations, are ubiquitous in medicine and critical to healthcare
delivery, operations, and research. Despite this central role, EHRs are
notoriously difficult to process automatically. Well over half of the
information stored within EHRs is in the form of unstructured text (e.g.
provider notes, operation reports) and remains largely untapped for secondary
use. Recently, however, newer neural network and deep learning approaches to
Natural Language Processing (NLP) have made considerable advances,
outperforming traditional statistical and rule-based systems on a variety of
tasks. In this survey paper, we summarize current neural NLP methods for EHR
applications. We focus on a broad scope of tasks, namely, classification and
prediction, word embeddings, extraction, generation, and other topics such as
question answering, phenotyping, knowledge graphs, medical dialogue,
multilinguality, interpretability, etc.

    

### [[2107.02978] RoboCup@Home Education 2020 Best Performance: RoboBreizh, a modular approach](http://arxiv.org/abs/2107.02978)


  Every year, the Robocup@Home competition challenges teams and robots'
abilities. In 2020, the RoboCup@Home Education challenge was organized online,
altering the usual competition rules. In this paper, we present the latest
developments that lead the RoboBreizh team to win the contest. These
developments include several modules linked to each other allowing the Pepper
robot to understand, act and adapt itself to a local environment. Up-to-date
available technologies have been used for navigation and dialogue. First
contribution includes combining object detection and pose estimation techniques
to detect user's intention. Second contribution involves using Learning by
Demonstrations to easily learn new movements that improve the Pepper robot's
skills. This proposal won the best performance award of the 2020 RoboCup@Home
Education challenge.

    

### [[2107.02988] SpectralFormer: Rethinking Hyperspectral Image Classification with Transformers](http://arxiv.org/abs/2107.02988)


  Hyperspectral (HS) images are characterized by approximately contiguous
spectral information, enabling the fine identification of materials by
capturing subtle spectral discrepancies. Owing to their excellent locally
contextual modeling ability, convolutional neural networks (CNNs) have been
proven to be a powerful feature extractor in HS image classification. However,
CNNs fail to mine and represent the sequence attributes of spectral signatures
well due to the limitations of their inherent network backbone. To solve this
issue, we rethink HS image classification from a sequential perspective with
transformers, and propose a novel backbone network called \ul{SpectralFormer}.
Beyond band-wise representations in classic transformers, SpectralFormer is
capable of learning spectrally local sequence information from neighboring
bands of HS images, yielding group-wise spectral embeddings. More
significantly, to reduce the possibility of losing valuable information in the
layer-wise propagation process, we devise a cross-layer skip connection to
convey memory-like components from shallow to deep layers by adaptively
learning to fuse "soft" residuals across layers. It is worth noting that the
proposed SpectralFormer is a highly flexible backbone network, which can be
applicable to both pixel- and patch-wise inputs. We evaluate the classification
performance of the proposed SpectralFormer on three HS datasets by conducting
extensive experiments, showing the superiority over classic transformers and
achieving a significant improvement in comparison with state-of-the-art
backbone networks. The codes of this work will be available at
\url{this https URL} for the sake of
reproducibility.

    

### [[2107.03030] A convolutional neural network for teeth margin detection on 3-dimensional dental meshes](http://arxiv.org/abs/2107.03030)


  We proposed a convolutional neural network for vertex classification on
3-dimensional dental meshes, and used it to detect teeth margins. An expanding
layer was constructed to collect statistic values of neighbor vertex features
and compute new features for each vertex with convolutional neural networks. An
end-to-end neural network was proposed to take vertex features, including
coordinates, curvatures and distance, as input and output each vertex
classification label. Several network structures with different parameters of
expanding layers and a base line network without expanding layers were designed
and trained by 1156 dental meshes. The accuracy, recall and precision were
validated on 145 dental meshes to rate the best network structures, which were
finally tested on another 144 dental meshes. All networks with our expanding
layers performed better than baseline, and the best one achieved an accuracy of
0.877 both on validation dataset and test dataset.

    

### [[2107.03054] EchoEA: Echo Information between Entities and Relations for Entity Alignment](http://arxiv.org/abs/2107.03054)


  Entity alignment (EA) is to discover entities referring to the same object in
the real world from different knowledge graphs (KGs). It plays an important
role in automatically integrating KGs from multiple sources.
Existing knowledge graph embedding (KGE) methods based on Graph Neural
Networks (GNNs) have achieved promising results, which enhance entity
representation with relation information unidirectionally. Besides, more and
more methods introduce semi-supervision to ask for more labeled training data.
However, two challenges still exist in these methods: (1) Insufficient
interaction: The interaction between entities and relations is insufficiently
utilized. (2) Low-quality bootstrapping: The generated semi-supervised data is
of low quality.
In this paper, we propose a novel framework, Echo Entity Alignment (EchoEA),
which leverages self-attention mechanism to spread entity information to
relations and echo back to entities. The relation representation is dynamically
computed from entity representation. Symmetrically, the next entity
representation is dynamically calculated from relation representation, which
shows sufficient interaction.
Furthermore, we propose attribute-combined bi-directional global-filtered
strategy (ABGS) to improve bootstrapping, reduce false samples and generate
high-quality training data.
The experimental results on three real-world cross-lingual datasets are
stable at around 96\% at hits@1 on average, showing that our approach not only
significantly outperforms the state-of-the-art methods, but also is universal
and transferable for existing KGE methods.

    

### [[2107.03072] Android Security using NLP Techniques: A Review](http://arxiv.org/abs/2107.03072)


  Android is among the most targeted platform by attackers. While attackers are
improving their techniques, traditional solutions based on static and dynamic
analysis have been also evolving. In addition to the application code, Android
applications have some metadata that could be useful for security analysis of
applications. Unlike traditional application distribution mechanisms, Android
applications are distributed centrally in mobile markets. Therefore, beside
application packages, such markets contain app information provided by app
developers and app users. The availability of such useful textual data together
with the advancement in Natural Language Processing (NLP) that is used to
process and understand textual data has encouraged researchers to investigate
the use of NLP techniques in Android security. Especially, security solutions
based on NLP have accelerated in the last 5 years and proven to be useful. This
study reviews these proposals and aim to explore possible research directions
for future studies by presenting state-of-the-art in this domain. We mainly
focus on NLP-based solutions under four categories: description-to-behaviour
fidelity, description generation, privacy and malware detection.

    

### [[2107.03088] WeClick: Weakly-Supervised Video Semantic Segmentation with Click Annotations](http://arxiv.org/abs/2107.03088)


  Compared with tedious per-pixel mask annotating, it is much easier to
annotate data by clicks, which costs only several seconds for an image.
However, applying clicks to learn video semantic segmentation model has not
been explored before. In this work, we propose an effective weakly-supervised
video semantic segmentation pipeline with click annotations, called WeClick,
for saving laborious annotating effort by segmenting an instance of the
semantic class with only a single click. Since detailed semantic information is
not captured by clicks, directly training with click labels leads to poor
segmentation predictions. To mitigate this problem, we design a novel memory
flow knowledge distillation strategy to exploit temporal information (named
memory flow) in abundant unlabeled video frames, by distilling the neighboring
predictions to the target frame via estimated motion. Moreover, we adopt
vanilla knowledge distillation for model compression. In this case, WeClick
learns compact video semantic segmentation models with the low-cost click
annotations during the training phase yet achieves real-time and accurate
models during the inference period. Experimental results on Cityscapes and
Camvid show that WeClick outperforms the state-of-the-art methods, increases
performance by 10.24% mIoU than baseline, and achieves real-time execution.

    

### [[2107.03158] A Survey on Data Augmentation for Text Classification](http://arxiv.org/abs/2107.03158)


  Data augmentation, the artificial creation of training data for machine
learning by transformations, is a widely studied research field across machine
learning disciplines. While it is useful for increasing the generalization
capabilities of a model, it can also address many other challenges and
problems, from overcoming a limited amount of training data over regularizing
the objective to limiting the amount data used to protect privacy. Based on a
precise description of the goals and applications of data augmentation (C1) and
a taxonomy for existing works (C2), this survey is concerned with data
augmentation methods for textual classification and aims to achieve a concise
and comprehensive overview for researchers and practitioners (C3). Derived from
the taxonomy, we divided more than 100 methods into 12 different groupings and
provide state-of-the-art references expounding which methods are highly
promising (C4). Finally, research perspectives that may constitute a building
block for future work are given (C5).

    

### [[2107.03178] Levels of explainable artificial intelligence for human-aligned conversational explanations](http://arxiv.org/abs/2107.03178)


  Over the last few years there has been rapid research growth into eXplainable
Artificial Intelligence (XAI) and the closely aligned Interpretable Machine
Learning (IML). Drivers for this growth include recent legislative changes and
increased investments by industry and governments, along with increased concern
from the general public. People are affected by autonomous decisions every day
and the public need to understand the decision-making process to accept the
outcomes. However, the vast majority of the applications of XAI/IML are focused
on providing low-level `narrow' explanations of how an individual decision was
reached based on a particular datum. While important, these explanations rarely
provide insights into an agent's: beliefs and motivations; hypotheses of other
(human, animal or AI) agents' intentions; interpretation of external cultural
expectations; or, processes used to generate its own explanation. Yet all of
these factors, we propose, are essential to providing the explanatory depth
that people require to accept and trust the AI's decision-making. This paper
aims to define levels of explanation and describe how they can be integrated to
create a human-aligned conversational explanation system. In so doing, this
paper will survey current approaches and discuss the integration of different
technologies to achieve these levels with Broad eXplainable Artificial
Intelligence (Broad-XAI), and thereby move towards high-level `strong'
explanations.

    

### [[2107.03212] Hierarchical Semantic Segmentation using Psychometric Learning](http://arxiv.org/abs/2107.03212)


  Assigning meaning to parts of image data is the goal of semantic image
segmentation. Machine learning methods, specifically supervised learning is
commonly used in a variety of tasks formulated as semantic segmentation. One of
the major challenges in the supervised learning approaches is expressing and
collecting the rich knowledge that experts have with respect to the meaning
present in the image data. Towards this, typically a fixed set of labels is
specified and experts are tasked with annotating the pixels, patches or
segments in the images with the given labels. In general, however, the set of
classes does not fully capture the rich semantic information present in the
images. For example, in medical imaging such as histology images, the different
parts of cells could be grouped and sub-grouped based on the expertise of the
pathologist.
To achieve such a precise semantic representation of the concepts in the
image, we need access to the full depth of knowledge of the annotator. In this
work, we develop a novel approach to collect segmentation annotations from
experts based on psychometric testing. Our method consists of the psychometric
testing procedure, active query selection, query enhancement, and a deep metric
learning model to achieve a patch-level image embedding that allows for
semantic segmentation of images. We show the merits of our method with
evaluation on the synthetically generated image, aerial image and histology
image.

    

### [[2107.03265] Contrastive Explanations for Argumentation-Based Conclusions](http://arxiv.org/abs/2107.03265)


  In this paper we discuss contrastive explanations for formal argumentation -
the question why a certain argument (the fact) can be accepted, whilst another
argument (the foil) cannot be accepted under various extension-based semantics.
The recent work on explanations for argumentation-based conclusions has mostly
focused on providing minimal explanations for the (non-)acceptance of
arguments. What is still lacking, however, is a proper argumentation-based
interpretation of contrastive explanations. We show under which conditions
contrastive explanations in abstract and structured argumentation are
meaningful, and how argumentation allows us to make implicit foils explicit.

    

### [[2107.03279] Introducing the structural bases of typicality effects in deep learning](http://arxiv.org/abs/2107.03279)


  In this paper, we hypothesize that the effects of the degree of typicality in
natural semantic categories can be generated based on the structure of
artificial categories learned with deep learning models. Motivated by the human
approach to representing natural semantic categories and based on the Prototype
Theory foundations, we propose a novel Computational Prototype Model (CPM) to
represent the internal structure of semantic categories. Unlike other prototype
learning approaches, our mathematical framework proposes a first approach to
provide deep neural networks with the ability to model abstract semantic
concepts such as category central semantic meaning, typicality degree of an
object's image, and family resemblance relationship. We proposed several
methodologies based on the typicality's concept to evaluate our CPM-model in
image semantic processing tasks such as image classification, a global semantic
description, and transfer learning. Our experiments on different image
datasets, such as ImageNet and Coco, showed that our approach might be an
admissible proposition in the effort to endow machines with greater power of
abstraction for the semantic representation of objects' categories.

    

### [[2107.03287] Strategy Complexity of Mean Payoff, Total Payoff and Point Payoff Objectives in Countable MDPs](http://arxiv.org/abs/2107.03287)


  We study countably infinite Markov decision processes (MDPs) with real-valued
transition rewards. Every infinite run induces the following sequences of
payoffs: 1. Point payoff (the sequence of directly seen transition rewards), 2.
Total payoff (the sequence of the sums of all rewards so far), and 3. Mean
payoff. For each payoff type, the objective is to maximize the probability that
the $\liminf$ is non-negative. We establish the complete picture of the
strategy complexity of these objectives, i.e., how much memory is necessary and
sufficient for $\varepsilon$-optimal (resp. optimal) strategies. Some cases can
be won with memoryless deterministic strategies, while others require a step
counter, a reward counter, or both.

    

### [[2107.03288] Attribute reduction and rule acquisition of formal decision context based on two new kinds of decision rules](http://arxiv.org/abs/2107.03288)


  This paper mainly studies the rule acquisition and attribute reduction for
formal decision context based on two new kinds of decision rules, namely
I-decision rules and II-decision rules. The premises of these rules are
object-oriented concepts, and the conclusions are formal concept and
property-oriented concept respectively. The rule acquisition algorithms for
I-decision rules and II-decision rules are presented. Some comparative analysis
of these algorithms with the existing algorithms are examined which shows that
the algorithms presented in this study behave well. The attribute reduction
approaches to preserve I-decision rules and II-decision rules are presented by
using discernibility matrix.

    

### [[2107.03305] Modelling Players in Mobile Puzzle Games](http://arxiv.org/abs/2107.03305)


  Successful and accurate modelling of level difficulty is a fundamental
component of the operationalisation of player experience as difficulty is one
of the most important and commonly used signals for content design and
adaptation. In games that feature intermediate milestones, such as completable
areas or levels, difficulty is often defined by the probability of completion
or completion rate; however, this operationalisation is limited in that it does
not describe the behaviour of the player within the area.
In this research work, we formalise a model of level difficulty for puzzle
games that goes beyond the classical probability of success. We accomplish this
by describing the distribution of actions performed within a game level using a
parametric statistical model thus creating a richer descriptor of difficulty.
The model is fitted and evaluated on a dataset collected from the game Lily's
Garden by Tactile Games, and the results of the evaluation show that the it is
able to describe and explain difficulty in a vast majority of the levels.

    

### [[2003.08363] Simulated annealing based heuristic for multiple agile satellites scheduling under cloud coverage uncertainty](http://arxiv.org/abs/2003.08363)


  Agile satellites are the new generation of Earth observation satellites
(EOSs) with stronger attitude maneuvering capability. Since optical remote
sensing instruments equipped on satellites cannot see through the cloud, the
cloud coverage has a significant influence on the satellite observation
missions. We are the first to address multiple agile EOSs scheduling problem
under cloud coverage uncertainty where the objective aims to maximize the
entire observation profit. The chance constraint programming model is adopted
to describe the uncertainty initially, and the observation profit under cloud
coverage uncertainty is then calculated via sample approximation method.
Subsequently, an improved simulated annealing based heuristic combining a fast
insertion strategy is proposed for large-scale observation missions. The
experimental results show that the improved simulated annealing heuristic
outperforms other algorithms for the multiple AEOSs scheduling problem under
cloud coverage uncertainty, which verifies the efficiency and effectiveness of
the proposed algorithm.

    

### [[2012.03091] Over a Decade of Social Opinion Mining: A Systematic Review](http://arxiv.org/abs/2012.03091)


  Social media popularity and importance is on the increase due to people using
it for various types of social interaction across multiple channels. This
systematic review focuses on the evolving research area of Social Opinion
Mining, tasked with the identification of multiple opinion dimensions, such as
subjectivity, sentiment polarity, emotion, affect, sarcasm and irony, from
user-generated content represented across multiple social media platforms and
in various media formats, like text, image, video and audio. Through Social
Opinion Mining, natural language can be understood in terms of the different
opinion dimensions, as expressed by humans. This contributes towards the
evolution of Artificial Intelligence which in turn helps the advancement of
several real-world use cases, such as customer service and decision making. A
thorough systematic review was carried out on Social Opinion Mining research
which totals 485 published studies and spans a period of twelve years between
2007 and 2018. The in-depth analysis focuses on the social media platforms,
techniques, social datasets, language, modality, tools and technologies, and
other aspects derived. Social Opinion Mining can be utilised in many
application areas, ranging from marketing, advertising and sales for
product/service management, and in multiple domains and industries, such as
politics, technology, finance, healthcare, sports and government. The latest
developments in Social Opinion Mining beyond 2018 are also presented together
with future research directions, with the aim of leaving a wider academic and
societal impact in several real-world applications.

    

### [[2103.11470] NeBula: Quest for Robotic Autonomy in Challenging Environments; TEAM CoSTAR at the DARPA Subterranean Challenge](http://arxiv.org/abs/2103.11470)


  This paper presents and discusses algorithms, hardware, and software
architecture developed by the TEAM CoSTAR (Collaborative SubTerranean
Autonomous Robots), competing in the DARPA Subterranean Challenge.
Specifically, it presents the techniques utilized within the Tunnel (2019) and
Urban (2020) competitions, where CoSTAR achieved 2nd and 1st place,
respectively. We also discuss CoSTAR's demonstrations in Martian-analog surface
and subsurface (lava tubes) exploration. The paper introduces our autonomy
solution, referred to as NeBula (Networked Belief-aware Perceptual Autonomy).
NeBula is an uncertainty-aware framework that aims at enabling resilient and
modular autonomy solutions by performing reasoning and decision making in the
belief space (space of probability distributions over the robot and world
states). We discuss various components of the NeBula framework, including: (i)
geometric and semantic environment mapping; (ii) a multi-modal positioning
system; (iii) traversability analysis and local planning; (iv) global motion
planning and exploration behavior; (i) risk-aware mission planning; (vi)
networking and decentralized reasoning; and (vii) learning-enabled adaptation.
We discuss the performance of NeBula on several robot types (e.g. wheeled,
legged, flying), in various environments. We discuss the specific results and
lessons learned from fielding this solution in the challenging courses of the
DARPA Subterranean Challenge competition.

    

### [[2104.04945] Enhancing Deep Neural Network Saliency Visualizations with Gradual Extrapolation](http://arxiv.org/abs/2104.04945)


  In this paper, an enhancement technique for the class activation mapping
methods such as gradient-weighted class activation maps or excitation
backpropagation is proposed to present the visual explanations of decisions
from convolutional neural network-based models. The proposed idea, called
Gradual Extrapolation, can supplement any method that generates a heatmap
picture by sharpening the output. Instead of producing a coarse localization
map that highlights the important predictive regions in the image, the proposed
method outputs the specific shape that most contributes to the model output.
Thus, the proposed method improves the accuracy of saliency maps. The effect
has been achieved by the gradual propagation of the crude map obtained in the
deep layer through all preceding layers with respect to their activations. In
validation tests conducted on a selected set of images, the faithfulness,
interpretability, and applicability of the method are evaluated. The proposed
technique significantly improves the localization detection of the neural
networks attention at low additional computational costs. Furthermore, the
proposed method is applicable to a variety deep neural network models. The code
for the method can be found at
this https URL


### [[2104.08301] Text2App: A Framework for Creating Android Apps from Text Descriptions](http://arxiv.org/abs/2104.08301)


  We present Text2App -- a framework that allows users to create functional
Android applications from natural language specifications. The conventional
method of source code generation tries to generate source code directly, which
is impractical for creating complex software. We overcome this limitation by
transforming natural language into an abstract intermediate formal language
representing an application with a substantially smaller number of tokens. The
intermediate formal representation is then compiled into target source codes.
This abstraction of programming details allows seq2seq networks to learn
complex application structures with less overhead. In order to train sequence
models, we introduce a data synthesis method grounded in a human survey. We
demonstrate that Text2App generalizes well to unseen combination of app
components and it is capable of handling noisy natural language instructions.
We explore the possibility of creating applications from highly abstract
instructions by coupling our system with GPT-3 -- a large pretrained language
model. We perform an extensive human evaluation and identify the capabilities
and limitations of our system. The source code, a ready-to-run demo notebook,
and a demo video are publicly available at
\url{this https URL}.

    

### [[2105.06950] Plot and Rework: Modeling Storylines for Visual Storytelling](http://arxiv.org/abs/2105.06950)


  Writing a coherent and engaging story is not easy. Creative writers use their
knowledge and worldview to put disjointed elements together to form a coherent
storyline, and work and rework iteratively toward perfection. Automated visual
storytelling (VIST) models, however, make poor use of external knowledge and
iterative generation when attempting to create stories. This paper introduces
PR-VIST, a framework that represents the input image sequence as a story graph
in which it finds the best path to form a storyline. PR-VIST then takes this
path and learns to generate the final story via an iterative training process.
This framework produces stories that are superior in terms of diversity,
coherence, and humanness, per both automatic and human evaluations. An ablation
study shows that both plotting and reworking contribute to the model's
superiority.

    

### [[2106.04066] Semantically Controllable Scene Generation with Guidance of Explicit Knowledge](http://arxiv.org/abs/2106.04066)


  Deep Generative Models (DGMs) are known for their superior capability in
generating realistic data. Extending purely data-driven approaches, recent
specialized DGMs may satisfy additional controllable requirements such as
embedding a traffic sign in a driving scene, by manipulating patterns
\textit{implicitly} in the neuron or feature level. In this paper, we introduce
a novel method to incorporate domain knowledge \textit{explicitly} in the
generation process to achieve semantically controllable scene generation. We
categorize our knowledge into two types to be consistent with the composition
of natural scenes, where the first type represents the property of objects and
the second type represents the relationship among objects. We then propose a
tree-structured generative model to learn complex scene representation, whose
nodes and edges are naturally corresponding to the two types of knowledge
respectively. Knowledge can be explicitly integrated to enable semantically
controllable scene generation by imposing semantic rules on properties of nodes
and edges in the tree structure. We construct a synthetic example to illustrate
the controllability and explainability of our method in a clean setting. We
further extend the synthetic example to realistic autonomous vehicle driving
environments and conduct extensive experiments to show that our method
efficiently identifies adversarial traffic scenes against different
state-of-the-art 3D point cloud segmentation models satisfying the traffic
rules specified as the explicit knowledge.

    

### [[2107.03357] Performance Evaluation of Mixed-Precision Runge-Kutta Methods](http://arxiv.org/abs/2107.03357)


  Additive Runge-Kutta methods designed for preserving highly accurate
solutions in mixed-precision computation were proposed and analyzed in [8].
These specially designed methods use reduced precision or the implicit
computations and full precision for the explicit computations. We develop a
FORTRAN code to solve a nonlinear system of ordinary differential equations
using the mixed precision additive Runge-Kutta (MP-ARK) methods on IBM POWER9
and Intel x86\_64 chips. The convergence, accuracy, runtime, and energy
consumption of these methods is explored. We show that these MP-ARK methods
efficiently produce accurate solutions with significant reductions in runtime
(and by extension energy consumption).

    

### [[1409.7195] Pigouvian Tolls and Welfare Optimality with Parallel Servers and Heterogeneous Customers](http://arxiv.org/abs/1409.7195)


  Congestion externalities are a well-known phenomenon in transportation and
communication networks, healthcare etc. Optimization by self-interested agents
in such settings typically results in equilibria which are sub-optimal for
social welfare. Pigouvian taxes or tolls, which impose a user charge equal to
the negative externality caused by the marginal user to other users, are a
mechanism for combating this problem. In this paper, we study a non-atomic
congestion game in which heterogeneous agents choose amongst a finite set of
heterogeneous servers. The delay at a server is an increasing function of its
load. Agents differ in their sensitivity to delay. We show that, while selfish
optimisation by agents is sub-optimal for social welfare, imposing admission
charges at the servers equal to the Pigouvian tax causes the user equilibrium
to maximize social welfare. In addition, we characterize the structure of
welfare optimal and of equilibrium allocations.

    

### [[2107.03155] The Quantitative Collapse of Concurrent Games with Symmetry](http://arxiv.org/abs/2107.03155)


  We explore links between the thin concurrent games of Castellan, Clairambault
and Winskel, and the weighted relational models of linear logic studied by
Laird, Manzonetto, McCusker and Pagani. More precisely, we show that there is
an interpretationpreserving "collapse" functor from the former to the latter.
On objects, the functor defines for each game a set of possible execution
states. Defining the action on morphisms is more subtle, and this is the main
contribution of the paper. Given a strategy and an execution state, our functor
needs to count the witnesses for this state within the strategy. Strategies in
thin concurrent games describe non-linear behaviour explicitly, so in general
each witness exists in countably many symmetric copies. The challenge is to
define the right notion of witnesses, factoring out this infinity while
matching the weighted relational model. Understanding how witnesses compose is
particularly subtle and requires a delve into the combinatorics of witnesses
and their symmetries. In its basic form, this functor connects thin concurrent
games and a relational model weighted by N $\cup$ {+$\infty$}. We will
additionally consider a generalised setting where both models are weighted by
elements of an arbitrary continuous semiring; this covers the probabilistic
case, among others. Witnesses now additionally carry a value from the semiring,
and our interpretation-preserving collapse functor extends to this setting.

    