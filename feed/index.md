
## 2021-7-19

### [[2107.07604] CLEDGE: A Hybrid Cloud-Edge Computing Framework over Information Centric Networking](http://arxiv.org/abs/2107.07604)


  In today's era of Internet of Things (IoT), where massive amounts of data are
produced by IoT and other devices, edge computing has emerged as a prominent
paradigm for low-latency data processing. However, applications may have
diverse latency requirements: certain latency-sensitive processing operations
may need to be performed at the edge, while delay-tolerant operations can be
performed on the cloud, without occupying the potentially limited edge
computing resources. To achieve that, we envision an environment where
computing resources are distributed across edge and cloud offerings. In this
paper, we present the design of CLEDGE (CLoud + EDGE), an information-centric
hybrid cloud-edge framework, aiming to maximize the on-time completion of
computational tasks offloaded by applications with diverse latency
requirements. The design of CLEDGE is motivated by the networking challenges
that mixed reality researchers face. Our evaluation demonstrates that CLEDGE
can complete on-time more than 90% of offloaded tasks with modest overheads.

    

### [[2107.07869] Nearest neighbor Methods and their Applications in Design of 5G & Beyond Wireless Networks](http://arxiv.org/abs/2107.07869)


  In this paper, we present an overview of Nearest neighbor (NN) methods, which
are frequently employed for solving classification problems using supervised
learning. The article concisely introduces the theoretical background,
algorithmic, and implementation aspects along with the key applications. From
an application standpoint, this article explores the challenges related to the
5G and beyond wireless networks which can be solved using NN classification
techniques.

    

### [[2107.07930] DxHash: A Scalable Consistent Hash Based on the Pseudo-Random Sequence](http://arxiv.org/abs/2107.07930)


  Consistent hasing has played a fundamental role as a data router and a load
balancer in various fields, such as distributed database, cloud infrastructure,
and peer-to-peer network. However, the existing consistent hashing schemes
can't meet the requirements simultaneously, including full consistency,
scalability, small memory footprint, low update time and low query complexity.
Thus, We propose DxHash, a scalable consistent hashing algorithm based on the
pseudo-random sequence. For the scenario of distributed storage, there are two
optimizations based on DXHash are proposed. First, the Weighted DxHash can
adjust the workloads on arbitrary nodes. Second, the Asymmetric Replica
Strategy (ARS) is combining the replica strategy in distributed storage with
the scaleup process to improve the availability of the system and reduce the
remapping rate. The evaluation indicates that compared with the state-of-art
works, DxHash achieves significant improvements on the 5 requirements. Even
with 50% failure ratio, DxHash still can complete 16.5 million queries per
second. What's more, the two optimizations both achieve their own results.

    

### [[2107.07558] SA-GD: Improved Gradient Descent Learning Strategy with Simulated Annealing](http://arxiv.org/abs/2107.07558)


  Gradient descent algorithm is the most utilized method when optimizing
machine learning issues. However, there exists many local minimums and saddle
points in the loss function, especially for high dimensional non-convex
optimization problems like deep learning. Gradient descent may make loss
function trapped in these local intervals which impedes further optimization,
resulting in poor generalization ability. This paper proposes the SA-GD
algorithm which introduces the thought of simulated annealing algorithm to
gradient descent. SA-GD method offers model the ability of mounting hills in
probability, tending to enable the model to jump out of these local areas and
converge to a optimal state finally. We took CNN models as an example and
tested the basic CNN models on various benchmark datasets. Compared to the
baseline models with traditional gradient descent algorithm, models with SA-GD
algorithm possess better generalization ability without sacrificing the
efficiency and stability of model convergence. In addition, SA-GD can be
utilized as an effective ensemble learning approach which improves the final
performance significantly.

    

### [[2107.07564] On the Importance of Regularisation & Auxiliary Information in OOD Detection](http://arxiv.org/abs/2107.07564)


  Neural networks are often utilised in critical domain applications
(e.g.~self-driving cars, financial markets, and aerospace engineering), even
though they exhibit overconfident predictions for ambiguous inputs. This
deficiency demonstrates a fundamental flaw indicating that neural networks
often overfit on spurious correlations. To address this problem in this work we
present two novel objectives that improve the ability of a network to detect
out-of-distribution samples and therefore avoid overconfident predictions for
ambiguous inputs. We empirically demonstrate that our methods outperform the
baseline and perform better than the majority of existing approaches, while
performing competitively those that they don't outperform. Additionally, we
empirically demonstrate the robustness of our approach against common
corruptions and demonstrate the importance of regularisation and auxiliary
information in out-of-distribution detection.

    

### [[2107.07572] Globally Convergent Multilevel Training of Deep Residual Networks](http://arxiv.org/abs/2107.07572)


  We propose a globally convergent multilevel training method for deep residual
networks (ResNets). The devised method can be seen as a novel variant of the
recursive multilevel trust-region (RMTR) method, which operates in hybrid
(stochastic-deterministic) settings by adaptively adjusting mini-batch sizes
during the training. The multilevel hierarchy and the transfer operators are
constructed by exploiting a dynamical system's viewpoint, which interprets
forward propagation through the ResNet as a forward Euler discretization of an
initial value problem. In contrast to traditional training approaches, our
novel RMTR method also incorporates curvature information on all levels of the
multilevel hierarchy by means of the limited-memory SR1 method. The overall
performance and the convergence properties of our multilevel training method
are numerically investigated using examples from the field of classification
and regression.

    

### [[2107.07576] Real-Time Face Recognition System for Remote Employee Tracking](http://arxiv.org/abs/2107.07576)


  During the COVID-19 pandemic, most of the human-to-human interactions have
been stopped. To mitigate the spread of deadly coronavirus, many offices took
the initiative so that the employees can work from home. But, tracking the
employees and finding out if they are really performing what they were supposed
to turn out to be a serious challenge for all the companies and organizations
who are facilitating "Work From Home". To deal with the challenge effectively,
we came up with a solution to track the employees with face recognition. We
have been testing this system experimentally for our office. To train the face
recognition module, we used FaceNet with KNN using the Labeled Faces in the
Wild (LFW) dataset and achieved 97.8% accuracy. We integrated the trained model
into our central system, where the employees log their time. In this paper, we
discuss in brief the system we have been experimenting with and the pros and
cons of the system.

    

### [[2107.07579] A Channel Coding Benchmark for Meta-Learning](http://arxiv.org/abs/2107.07579)


  Meta-learning provides a popular and effective family of methods for
data-efficient learning of new tasks. However, several important issues in
meta-learning have proven hard to study thus far. For example, performance
degrades in real-world settings where meta-learners must learn from a wide and
potentially multi-modal distribution of training tasks; and when distribution
shift exists between meta-train and meta-test task distributions. These issues
are typically hard to study since the shape of task distributions, and shift
between them are not straightforward to measure or control in standard
benchmarks. We propose the channel coding problem as a benchmark for
meta-learning. Channel coding is an important practical application where task
distributions naturally arise, and fast adaptation to new tasks is practically
valuable. We use this benchmark to study several aspects of meta-learning,
including the impact of task distribution breadth and shift, which can be
controlled in the coding problem. Going forward, this benchmark provides a tool
for the community to study the capabilities and limitations of meta-learning,
and to drive research on practically robust and effective meta-learners.

    

### [[2107.07582] Prediction of Blood Lactate Values in Critically Ill Patients: A Retrospective Multi-center Cohort Study](http://arxiv.org/abs/2107.07582)


  Purpose. Elevations in initially obtained serum lactate levels are strong
predictors of mortality in critically ill patients. Identifying patients whose
serum lactate levels are more likely to increase can alert physicians to
intensify care and guide them in the frequency of tending the blood test. We
investigate whether machine learning models can predict subsequent serum
lactate changes.
Methods. We investigated serum lactate change prediction using the MIMIC-III
and eICU-CRD datasets in internal as well as external validation of the eICU
cohort on the MIMIC-III cohort. Three subgroups were defined based on the
initial lactate levels: i) normal group (<2 mmol/L), ii) mild group (2-4
mmol/L), and iii) severe group (>4 mmol/L). Outcomes were defined based on
increase or decrease of serum lactate levels between the groups. We also
performed sensitivity analysis by defining the outcome as lactate change of
>10% and furthermore investigated the influence of the time interval between
subsequent lactate measurements on predictive performance.
Results. The LSTM models were able to predict deterioration of serum lactate
values of MIMIC-III patients with an AUC of 0.77 (95% CI 0.762-0.771) for the
normal group, 0.77 (95% CI 0.768-0.772) for the mild group, and 0.85 (95% CI
0.840-0.851) for the severe group, with a slightly lower performance in the
external validation.
Conclusion. The LSTM demonstrated good discrimination of patients who had
deterioration in serum lactate levels. Clinical studies are needed to evaluate
whether utilization of a clinical decision support tool based on these results
could positively impact decision-making and patient outcomes.

    

### [[2107.07603] Measuring inter-cluster similarities with Alpha Shape TRIangulation in loCal Subspaces (ASTRICS) facilitates visualization and clustering of high-dimensional data](http://arxiv.org/abs/2107.07603)


  Clustering and visualizing high-dimensional (HD) data are important tasks in
a variety of fields. For example, in bioinformatics, they are crucial for
analyses of single-cell data such as mass cytometry (CyTOF) data. Some of the
most effective algorithms for clustering HD data are based on representing the
data by nodes in a graph, with edges connecting neighbouring nodes according to
some measure of similarity or distance. However, users of graph-based
algorithms are typically faced with the critical but challenging task of
choosing the value of an input parameter that sets the size of neighbourhoods
in the graph, e.g. the number of nearest neighbours to which to connect each
node or a threshold distance for connecting nodes. The burden on the user could
be alleviated by a measure of inter-node similarity that can have value 0 for
dissimilar nodes without requiring any user-defined parameters or thresholds.
This would determine the neighbourhoods automatically while still yielding a
sparse graph. To this end, I propose a new method called ASTRICS to measure
similarity between clusters of HD data points based on local dimensionality
reduction and triangulation of critical alpha shapes. I show that my ASTRICS
similarity measure can facilitate both clustering and visualization of HD data
by using it in Stage 2 of a three-stage pipeline: Stage 1 = perform an initial
clustering of the data by any method; Stage 2 = let graph nodes represent
initial clusters instead of individual data points and use ASTRICS to
automatically define edges between nodes; Stage 3 = use the graph for further
clustering and visualization. This trades the critical task of choosing a graph
neighbourhood size for the easier task of essentially choosing a resolution at
which to view the data. The graph and consequently downstream clustering and
visualization are then automatically adapted to the chosen resolution.

    

### [[2107.07617] Algorithmic insights on continual learning from fruit flies](http://arxiv.org/abs/2107.07617)


  Continual learning in computational systems is challenging due to
catastrophic forgetting. We discovered a two layer neural circuit in the fruit
fly olfactory system that addresses this challenge by uniquely combining sparse
coding and associative learning. In the first layer, odors are encoded using
sparse, high dimensional representations, which reduces memory interference by
activating non overlapping populations of neurons for different odors. In the
second layer, only the synapses between odor activated neurons and the output
neuron associated with the odor are modified during learning; the rest of the
weights are frozen to prevent unrelated memories from being overwritten. We
show empirically and analytically that this simple and lightweight algorithm
significantly boosts continual learning performance. The fly associative
learning algorithm is strikingly similar to the classic perceptron learning
algorithm, albeit two modifications, which we show are critical for reducing
catastrophic forgetting. Overall, fruit flies evolved an efficient lifelong
learning algorithm, and circuit mechanisms from neuroscience can be translated
to improve machine computation.

    

### [[2107.07618] Adversarial Attack for Uncertainty Estimation: Identifying Critical Regions in Neural Networks](http://arxiv.org/abs/2107.07618)


  We propose a novel method to capture data points near decision boundary in
neural network that are often referred to a specific type of uncertainty. In
our approach, we sought to perform uncertainty estimation based on the idea of
adversarial attack method. In this paper, uncertainty estimates are derived
from the input perturbations, unlike previous studies that provide
perturbations on the model's parameters as in Bayesian approach. We are able to
produce uncertainty with couple of perturbations on the inputs. Interestingly,
we apply the proposed method to datasets derived from blockchain. We compare
the performance of model uncertainty with the most recent uncertainty methods.
We show that the proposed method has revealed a significant outperformance over
other methods and provided less risk to capture model uncertainty in machine
learning.

    

### [[2107.07623] Correlation detection in trees for partial graph alignment](http://arxiv.org/abs/2107.07623)


  We consider alignment of sparse graphs, which consists in finding a mapping
between the nodes of two graphs which preserves most of the edges. Our approach
is to compare local structures in the two graphs, matching two nodes if their
neighborhoods are 'close enough': for correlated Erdős-Rényi random
graphs, this problem can be locally rephrased in terms of testing whether a
pair of branching trees is drawn from either a product distribution, or a
correlated distribution. We design an optimal test for this problem which gives
rise to a message-passing algorithm for graph alignment, which provably returns
in polynomial time a positive fraction of correctly matched vertices, and a
vanishing fraction of mismatches. With an average degree $\lambda = O(1)$ in
the graphs, and a correlation parameter $s \in [0,1]$, this result holds with
$\lambda s$ large enough, and $1-s$ small enough, completing the recent
state-of-the-art diagram. Tighter conditions for determining whether partial
graph alignment (or correlation detection in trees) is feasible in polynomial
time are given in terms of Kullback-Leibler divergences.

    

### [[2107.07634] Multi-task Learning with Cross Attention for Keyword Spotting](http://arxiv.org/abs/2107.07634)


  Keyword spotting (KWS) is an important technique for speech applications,
which enables users to activate devices by speaking a keyword phrase. Although
a phoneme classifier can be used for KWS, exploiting a large amount of
transcribed data for automatic speech recognition (ASR), there is a mismatch
between the training criterion (phoneme recognition) and the target task (KWS).
Recently, multi-task learning has been applied to KWS to exploit both ASR and
KWS training data. In this approach, an output of an acoustic model is split
into two branches for the two tasks, one for phoneme transcription trained with
the ASR data and one for keyword classification trained with the KWS data. In
this paper, we introduce a cross attention decoder in the multi-task learning
framework. Unlike the conventional multi-task learning approach with the simple
split of the output layer, the cross attention decoder summarizes information
from a phonetic encoder by performing cross attention between the encoder
outputs and a trainable query sequence to predict a confidence score for the
KWS task. Experimental results on KWS tasks show that the proposed approach
outperformed the conventional multi-task learning with split branches and a
bi-directional long short-team memory decoder by 12% on average.

    

### [[2107.07642] Improving application performance with biased distributions of quantum states](http://arxiv.org/abs/2107.07642)


  We consider the properties of a specific distribution of mixed quantum states
of arbitrary dimension that can be biased towards a specific mean purity. In
particular, we analyze mixtures of Haar-random pure states with
Dirichlet-distributed coefficients. We analytically derive the concentration
parameters required to match the mean purity of the Bures and Hilbert--Schmidt
distributions in any dimension. Numerical simulations suggest that this value
recovers the Hilbert--Schmidt distribution exactly, offering an alternative and
intuitive physical interpretation for ensembles of Hilbert--Schmidt-distributed
random quantum states. We then demonstrate how substituting these
Dirichlet-weighted Haar mixtures in place of the Bures and Hilbert--Schmidt
distributions results in measurable performance advantages in
machine-learning-based quantum state tomography systems and Bayesian quantum
state reconstruction. Finally, we experimentally characterize the distribution
of quantum states generated by both a cloud-accessed IBM quantum computer and
an in-house source of polarization-entangled photons. In each case, our method
can more closely match the underlying distribution than either Bures or
Hilbert--Schmidt distributed states for various experimental conditions.

    

### [[2107.07647] An Energy-Efficient Edge Computing Paradigm for Convolution-based Image Upsampling](http://arxiv.org/abs/2107.07647)


  A novel energy-efficient edge computing paradigm is proposed for real-time
deep learning-based image upsampling applications. State-of-the-art deep
learning solutions for image upsampling are currently trained using either
resize or sub-pixel convolution to learn kernels that generate high fidelity
images with minimal artifacts. However, performing inference with these learned
convolution kernels requires memory-intensive feature map transformations that
dominate time and energy costs in real-time applications. To alleviate this
pressure on memory bandwidth, we confine the use of resize or sub-pixel
convolution to training in the cloud by transforming learned convolution
kernels to deconvolution kernels before deploying them for inference as a
functionally equivalent deconvolution. These kernel transformations, intended
as a one-time cost when shifting from training to inference, enable a systems
designer to use each algorithm in their optimal context by preserving the image
fidelity learned when training in the cloud while minimizing data transfer
penalties during inference at the edge. We also explore existing variants of
deconvolution inference algorithms and introduce a novel variant for
consideration. We analyze and compare the inference properties of
convolution-based upsampling algorithms using a quantitative model of incurred
time and energy costs and show that using deconvolution for inference at the
edge improves both system latency and energy efficiency when compared to their
sub-pixel or resize convolution counterparts.

    

### [[2107.07659] Geometric Value Iteration: Dynamic Error-Aware KL Regularization for Reinforcement Learning](http://arxiv.org/abs/2107.07659)


  The recent booming of entropy-regularized literature reveals that
Kullback-Leibler (KL) regularization brings advantages to Reinforcement
Learning (RL) algorithms by canceling out errors under mild assumptions.
However, existing analyses focus on fixed regularization with a constant
weighting coefficient and have not considered the case where the coefficient is
allowed to change dynamically. In this paper, we study the dynamic coefficient
scheme and present the first asymptotic error bound. Based on the dynamic
coefficient error bound, we propose an effective scheme to tune the coefficient
according to the magnitude of error in favor of more robust learning. On top of
this development, we propose a novel algorithm: Geometric Value Iteration (GVI)
that features a dynamic error-aware KL coefficient design aiming to mitigate
the impact of errors on the performance. Our experiments demonstrate that GVI
can effectively exploit the trade-off between learning speed and robustness
over uniform averaging of constant KL coefficient. The combination of GVI and
deep networks shows stable learning behavior even in the absence of a target
network where algorithms with a constant KL coefficient would greatly oscillate
or even fail to converge.

    

### [[2107.07675] Beyond In-Place Corruption: Insertion and Deletion In Denoising Probabilistic Models](http://arxiv.org/abs/2107.07675)


  Denoising diffusion probabilistic models (DDPMs) have shown impressive
results on sequence generation by iteratively corrupting each example and then
learning to map corrupted versions back to the original. However, previous work
has largely focused on in-place corruption, adding noise to each pixel or token
individually while keeping their locations the same. In this work, we consider
a broader class of corruption processes and denoising models over sequence data
that can insert and delete elements, while still being efficient to train and
sample from. We demonstrate that these models outperform standard in-place
models on an arithmetic sequence task, and that when trained on the text8
dataset they can be used to fix spelling errors without any fine-tuning.

    

### [[2107.07677] ECG-Adv-GAN: Detecting ECG Adversarial Examples with Conditional Generative Adversarial Networks](http://arxiv.org/abs/2107.07677)


  Electrocardiogram (ECG) acquisition requires an automated system and analysis
pipeline for understanding specific rhythm irregularities. Deep neural networks
have become a popular technique for tracing ECG signals, outperforming human
experts. Despite this, convolutional neural networks are susceptible to
adversarial examples that can misclassify ECG signals and decrease the model's
precision. Moreover, they do not generalize well on the out-of-distribution
dataset. The GAN architecture has been employed in recent works to synthesize
adversarial ECG signals to increase existing training data. However, they use a
disjointed CNN-based classification architecture to detect arrhythmia. Till
now, no versatile architecture has been proposed that can detect adversarial
examples and classify arrhythmia simultaneously. To alleviate this, we propose
a novel Conditional Generative Adversarial Network to simultaneously generate
ECG signals for different categories and detect cardiac abnormalities.
Moreover, the model is conditioned on class-specific ECG signals to synthesize
realistic adversarial examples. Consequently, we compare our architecture and
show how it outperforms other classification models in normal/abnormal ECG
signal detection by benchmarking real world and adversarial signals.

    

### [[2107.07682] The Application of Active Query K-Means in Text Classification](http://arxiv.org/abs/2107.07682)


  Active learning is a state-of-art machine learning approach to deal with an
abundance of unlabeled data. In the field of Natural Language Processing,
typically it is costly and time-consuming to have all the data annotated. This
inefficiency inspires out our application of active learning in text
classification. Traditional unsupervised k-means clustering is first modified
into a semi-supervised version in this research. Then, a novel attempt is
applied to further extend the algorithm into active learning scenario with
Penalized Min-Max-selection, so as to make limited queries that yield more
stable initial centroids. This method utilizes both the interactive query
results from users and the underlying distance representation. After tested on
a Chinese news dataset, it shows a consistent increase in accuracy while
lowering the cost in training.

    

### [[2107.07684] CutDepth:Edge-aware Data Augmentation in Depth Estimation](http://arxiv.org/abs/2107.07684)


  It is difficult to collect data on a large scale in a monocular depth
estimation because the task requires the simultaneous acquisition of RGB images
and depths. Data augmentation is thus important to this task. However, there
has been little research on data augmentation for tasks such as monocular depth
estimation, where the transformation is performed pixel by pixel. In this
paper, we propose a data augmentation method, called CutDepth. In CutDepth,
part of the depth is pasted onto an input image during training. The method
extends variations data without destroying edge features. Experiments
objectively and subjectively show that the proposed method outperforms
conventional methods of data augmentation. The estimation accuracy is improved
with CutDepth even though there are few training data at long distances.

    

### [[2107.07687] Auto-differentiable Ensemble Kalman Filters](http://arxiv.org/abs/2107.07687)


  Data assimilation is concerned with sequentially estimating a
temporally-evolving state. This task, which arises in a wide range of
scientific and engineering applications, is particularly challenging when the
state is high-dimensional and the state-space dynamics are unknown. This paper
introduces a machine learning framework for learning dynamical systems in data
assimilation. Our auto-differentiable ensemble Kalman filters (AD-EnKFs) blend
ensemble Kalman filters for state recovery with machine learning tools for
learning the dynamics. In doing so, AD-EnKFs leverage the ability of ensemble
Kalman filters to scale to high-dimensional states and the power of automatic
differentiation to train high-dimensional surrogate models for the dynamics.
Numerical results using the Lorenz-96 model show that AD-EnKFs outperform
existing methods that use expectation-maximization or particle filters to merge
data assimilation and machine learning. In addition, AD-EnKFs are easy to
implement and require minimal tuning.

    

### [[2107.07691] Intersectional Bias in Causal Language Models](http://arxiv.org/abs/2107.07691)


  To examine whether intersectional bias can be observed in language
generation, we examine \emph{GPT-2} and \emph{GPT-NEO} models, ranging in size
from 124 million to ~2.7 billion parameters. We conduct an experiment combining
up to three social categories - gender, religion and disability - into
unconditional or zero-shot prompts used to generate sentences that are then
analysed for sentiment. Our results confirm earlier tests conducted with
auto-regressive causal models, including the \emph{GPT} family of models. We
also illustrate why bias may be resistant to techniques that target single
categories (e.g. gender, religion and race), as it can also manifest, in often
subtle ways, in texts prompted by concatenated social categories. To address
these difficulties, we suggest technical and community-based approaches need to
combine to acknowledge and address complex and intersectional language model
bias.

    

### [[2107.07696] Constrained Feedforward Neural Network Training via Reachability Analysis](http://arxiv.org/abs/2107.07696)


  Neural networks have recently become popular for a wide variety of uses, but
have seen limited application in safety-critical domains such as robotics near
and around humans. This is because it remains an open challenge to train a
neural network to obey safety constraints. Most existing safety-related methods
only seek to verify that already-trained networks obey constraints, requiring
alternating training and verification. Instead, this work proposes a
constrained method to simultaneously train and verify a feedforward neural
network with rectified linear unit (ReLU) nonlinearities. Constraints are
enforced by computing the network's output-space reachable set and ensuring
that it does not intersect with unsafe sets; training is achieved by
formulating a novel collision-check loss function between the reachable set and
unsafe portions of the output space. The reachable and unsafe sets are
represented by constrained zonotopes, a convex polytope representation that
enables differentiable collision checking. The proposed method is demonstrated
successfully on a network with one nonlinearity layer and approximately 50
parameters.

    

### [[2107.07702] Neural Contextual Anomaly Detection for Time Series](http://arxiv.org/abs/2107.07702)


  We introduce Neural Contextual Anomaly Detection (NCAD), a framework for
anomaly detection on time series that scales seamlessly from the unsupervised
to supervised setting, and is applicable to both univariate and multivariate
time series. This is achieved by effectively combining recent developments in
representation learning for multivariate time series, with techniques for deep
anomaly detection originally developed for computer vision that we tailor to
the time series setting. Our window-based approach facilitates learning the
boundary between normal and anomalous classes by injecting generic synthetic
anomalies into the available data. Moreover, our method can effectively take
advantage of all the available information, be it as domain knowledge, or as
training labels in the semi-supervised setting. We demonstrate empirically on
standard benchmark datasets that our approach obtains a state-of-the-art
performance in these settings.

    

### [[2107.07705] Pseudo-labelling Enhanced Media Bias Detection](http://arxiv.org/abs/2107.07705)


  Leveraging unlabelled data through weak or distant supervision is a
compelling approach to developing more effective text classification models.
This paper proposes a simple but effective data augmentation method, which
leverages the idea of pseudo-labelling to select samples from noisy distant
supervision annotation datasets. The result shows that the proposed method
improves the accuracy of biased news detection models.

    

### [[2107.07706] DANCE: DAta-Network Co-optimization for Efficient Segmentation Model Training and Inference](http://arxiv.org/abs/2107.07706)


  Semantic segmentation for scene understanding is nowadays widely demanded,
raising significant challenges for the algorithm efficiency, especially its
applications on resource-limited platforms. Current segmentation models are
trained and evaluated on massive high-resolution scene images ("data level")
and suffer from the expensive computation arising from the required multi-scale
aggregation("network level"). In both folds, the computational and energy costs
in training and inference are notable due to the often desired large input
resolutions and heavy computational burden of segmentation models. To this end,
we propose DANCE, general automated DAta-Network Co-optimization for Efficient
segmentation model training and inference. Distinct from existing efficient
segmentation approaches that focus merely on light-weight network design, DANCE
distinguishes itself as an automated simultaneous data-network co-optimization
via both input data manipulation and network architecture slimming.
Specifically, DANCE integrates automated data slimming which adaptively
downsamples/drops input images and controls their corresponding contribution to
the training loss guided by the images' spatial complexity. Such a downsampling
operation, in addition to slimming down the cost associated with the input size
directly, also shrinks the dynamic range of input object and context scales,
therefore motivating us to also adaptively slim the network to match the
downsampled data. Extensive experiments and ablating studies (on four SOTA
segmentation models with three popular segmentation datasets under two training
settings) demonstrate that DANCE can achieve "all-win" towards efficient
segmentation(reduced training cost, less expensive inference, and better mean
Intersection-over-Union (mIoU)).

    

### [[2107.07709] ScRAE: Deterministic Regularized Autoencoders with Flexible Priors for Clustering Single-cell Gene Expression Data](http://arxiv.org/abs/2107.07709)


  Clustering single-cell RNA sequence (scRNA-seq) data poses statistical and
computational challenges due to their high-dimensionality and data-sparsity,
also known as `dropout' events. Recently, Regularized Auto-Encoder (RAE) based
deep neural network models have achieved remarkable success in learning robust
low-dimensional representations. The basic idea in RAEs is to learn a
non-linear mapping from the high-dimensional data space to a low-dimensional
latent space and vice-versa, simultaneously imposing a distributional prior on
the latent space, which brings in a regularization effect. This paper argues
that RAEs suffer from the infamous problem of bias-variance trade-off in their
naive formulation. While a simple AE without a latent regularization results in
data over-fitting, a very strong prior leads to under-representation and thus
bad clustering. To address the above issues, we propose a modified RAE
framework (called the scRAE) for effective clustering of the single-cell RNA
sequencing data. scRAE consists of deterministic AE with a flexibly learnable
prior generator network, which is jointly trained with the AE. This facilitates
scRAE to trade-off better between the bias and variance in the latent space. We
demonstrate the efficacy of the proposed method through extensive
experimentation on several real-world single-cell Gene expression datasets.

    

### [[2107.07713] Towards an Interpretable Latent Space in Structured Models for Video Prediction](http://arxiv.org/abs/2107.07713)


  We focus on the task of future frame prediction in video governed by
underlying physical dynamics. We work with models which are object-centric,
i.e., explicitly work with object representations, and propagate a loss in the
latent space. Specifically, our research builds on recent work by Kipf et al.
\cite{kipf&al20}, which predicts the next state via contrastive learning of
object interactions in a latent space using a Graph Neural Network. We argue
that injecting explicit inductive bias in the model, in form of general
physical laws, can help not only make the model more interpretable, but also
improve the overall prediction of model. As a natural by-product, our model can
learn feature maps which closely resemble actual object positions in the image,
without having any explicit supervision about the object positions at the
training time. In comparison with earlier works \cite{jaques&al20}, which
assume a complete knowledge of the dynamics governing the motion in the form of
a physics engine, we rely only on the knowledge of general physical laws, such
as, world consists of objects, which have position and velocity. We propose an
additional decoder based loss in the pixel space, imposed in a curriculum
manner, to further refine the latent space predictions. Experiments in multiple
different settings demonstrate that while Kipf et al. model is effective at
capturing object interactions, our model can be significantly more effective at
localising objects, resulting in improved performance in 3 out of 4 domains
that we experiment with. Additionally, our model can learn highly intrepretable
feature maps, resembling actual object positions.

    

### [[2107.07724] Active learning for online training in imbalanced data streams under cold start](http://arxiv.org/abs/2107.07724)


  Labeled data is essential in modern systems that rely on Machine Learning
(ML) for predictive modelling. Such systems may suffer from the cold-start
problem: supervised models work well but, initially, there are no labels, which
are costly or slow to obtain. This problem is even worse in imbalanced data
scenarios. Online financial fraud detection is an example where labeling is: i)
expensive, or ii) it suffers from long delays, if relying on victims filing
complaints. The latter may not be viable if a model has to be in place
immediately, so an option is to ask analysts to label events while minimizing
the number of annotations to control costs. We propose an Active Learning (AL)
annotation system for datasets with orders of magnitude of class imbalance, in
a cold start streaming scenario. We present a computationally efficient
Outlier-based Discriminative AL approach (ODAL) and design a novel 3-stage
sequence of AL labeling policies where it is used as warm-up. Then, we perform
empirical studies in four real world datasets, with various magnitudes of class
imbalance. The results show that our method can more quickly reach a high
performance model than standard AL policies. Its observed gains over random
sampling can reach 80% and be competitive with policies with an unlimited
annotation budget or additional historical data (with 1/10 to 1/50 of the
labels).

    

### [[2107.07728] Recognizing bird species in diverse soundscapes under weak supervision](http://arxiv.org/abs/2107.07728)


  We present a robust classification approach for avian vocalization in complex
and diverse soundscapes, achieving second place in the BirdCLEF2021 challenge.
We illustrate how to make full use of pre-trained convolutional neural
networks, by using an efficient modeling and training routine supplemented by
novel augmentation methods. Thereby, we improve the generalization of weakly
labeled crowd-sourced data to productive data collected by autonomous recording
units. As such, we illustrate how to progress towards an accurate automated
assessment of avian population which would enable global biodiversity
monitoring at scale, impossible by manual annotation.

    

### [[2107.07729] Semi-supervised Learning for Marked Temporal Point Processes](http://arxiv.org/abs/2107.07729)


  Temporal Point Processes (TPPs) are often used to represent the sequence of
events ordered as per the time of occurrence. Owing to their flexible nature,
TPPs have been used to model different scenarios and have shown applicability
in various real-world applications. While TPPs focus on modeling the event
occurrence, Marked Temporal Point Process (MTPP) focuses on modeling the
category/class of the event as well (termed as the marker). Research in MTPP
has garnered substantial attention over the past few years, with an extensive
focus on supervised algorithms. Despite the research focus, limited attention
has been given to the challenging problem of developing solutions in
semi-supervised settings, where algorithms have access to a mix of labeled and
unlabeled data. This research proposes a novel algorithm for Semi-supervised
Learning for Marked Temporal Point Processes (SSL-MTPP) applicable in such
scenarios. The proposed SSL-MTPP algorithm utilizes a combination of labeled
and unlabeled data for learning a robust marker prediction model. The proposed
algorithm utilizes an RNN-based Encoder-Decoder module for learning effective
representations of the time sequence. The efficacy of the proposed algorithm
has been demonstrated via multiple protocols on the Retweet dataset, where the
proposed SSL-MTPP demonstrates improved performance in comparison to the
traditional supervised learning approach.

    

### [[2107.07732] Robust Online Control with Model Misspecification](http://arxiv.org/abs/2107.07732)


  We study online control of an unknown nonlinear dynamical system that is
approximated by a time-invariant linear system with model misspecification. Our
study focuses on robustness, which measures how much deviation from the assumed
linear approximation can be tolerated while maintaining a bounded $\ell_2$-gain
compared to the optimal control in hindsight. Some models cannot be stabilized
even with perfect knowledge of their coefficients: the robustness is limited by
the minimal distance between the assumed dynamics and the set of unstabilizable
dynamics. Therefore it is necessary to assume a lower bound on this distance.
Under this assumption, and with full observation of the $d$ dimensional state,
we describe an efficient controller that attains $\Omega(\frac{1}{\sqrt{d}})$
robustness together with an $\ell_2$-gain whose dimension dependence is near
optimal. We also give an inefficient algorithm that attains constant robustness
independent of the dimension, with a finite but sub-optimal $\ell_2$-gain.

    

### [[2107.07737] EGC2: Enhanced Graph Classification with Easy Graph Compression](http://arxiv.org/abs/2107.07737)


  Graph classification plays a significant role in network analysis. It also
faces potential security threat like adversarial attacks. Some defense methods
may sacrifice algorithm complexity for robustness like adversarial training,
while others may sacrifice the clean example performance such as
smoothing-based defense. Most of them are suffered from high-complexity or less
transferability. To address this problem, we proposed EGC$^2$, an enhanced
graph classification model with easy graph compression. EGC$^2$ captures the
relationship between features of different nodes by constructing feature graphs
and improving aggregate node-level representation. To achieve lower complexity
defense applied to various graph classification models, EGC$^2$ utilizes a
centrality-based edge importance index to compress graphs, filtering out
trivial structures and even adversarial perturbations of the input graphs, thus
improves its robustness. Experiments on seven benchmark datasets demonstrate
that the proposed feature read-out and graph compression mechanisms enhance the
robustness of various basic models, thus achieving the state-of-the-art
performance of accuracy and robustness in the threat of different adversarial
attacks.

    

### [[2107.07738] Privacy-preserving Spatiotemporal Scenario Generation of Renewable Energies: A Federated Deep Generative Learning Approach](http://arxiv.org/abs/2107.07738)


  Scenario generation is a fundamental and crucial tool for decision-making in
power systems with high-penetration renewables. Based on big historical data, a
novel federated deep generative learning framework, called Fed-LSGAN, is
proposed by integrating federated learning and least square generative
adversarial networks (LSGANs) for renewable scenario generation. Specifically,
federated learning learns a shared global model in a central server from
renewable sites at network edges, which enables the Fed-LSGAN to generate
scenarios in a privacy-preserving manner without sacrificing the generation
quality by transferring model parameters, rather than all data. Meanwhile, the
LSGANs-based deep generative model generates scenarios that conform to the
distribution of historical data through fully capturing the spatial-temporal
characteristics of renewable powers, which leverages the least squares loss
function to improve the training stability and generation quality. The
simulation results demonstrate that the proposal manages to generate
high-quality renewable scenarios and outperforms the state-of-the-art
centralized methods. Besides, an experiment with different federated learning
settings is designed and conducted to verify the robustness of our method.

    

### [[2107.07740] MS-MDA: Multisource Marginal Distribution Adaptation for Cross-subject and Cross-session EEG Emotion Recognition](http://arxiv.org/abs/2107.07740)


  As an essential element for the diagnosis and rehabilitation of psychiatric
disorders, the electroencephalogram (EEG) based emotion recognition has
achieved significant progress due to its high precision and reliability.
However, one obstacle to practicality lies in the variability between subjects
and sessions. Although several studies have adopted domain adaptation (DA)
approaches to tackle this problem, most of them treat multiple EEG data from
different subjects and sessions together as a single source domain for
transfer, which either fails to satisfy the assumption of domain adaptation
that the source has a certain marginal distribution, or increases the
difficulty of adaptation. We therefore propose the multi-source marginal
distribution adaptation (MS-MDA) for EEG emotion recognition, which takes both
domain-invariant and domain-specific features into consideration. First, we
assume that different EEG data share the same low-level features, then we
construct independent branches for multiple EEG data source domains to adopt
one-to-one domain adaptation and extract domain-specific features. Finally, the
inference is made by multiple branches. We evaluate our method on SEED and
SEED-IV for recognizing three and four emotions, respectively. Experimental
results show that the MS-MDA outperforms the comparison methods and
state-of-the-art models in cross-session and cross-subject transfer scenarios
in our settings. Codes at this https URL.

    

### [[2107.07741] When does loss-based prioritization fail?](http://arxiv.org/abs/2107.07741)


  Not all examples are created equal, but standard deep neural network training
protocols treat each training point uniformly. Each example is propagated
forward and backward through the network the same amount of times, independent
of how much the example contributes to the learning protocol. Recent work has
proposed ways to accelerate training by deviating from this uniform treatment.
Popular methods entail up-weighting examples that contribute more to the loss
with the intuition that examples with low loss have already been learned by the
model, so their marginal value to the training procedure should be lower. This
view assumes that updating the model with high loss examples will be beneficial
to the model. However, this may not hold for noisy, real world data. In this
paper, we theorize and then empirically demonstrate that loss-based
acceleration methods degrade in scenarios with noisy and corrupted data. Our
work suggests measures of example difficulty need to correctly separate out
noise from other types of challenging examples.

    

### [[2107.07752] NeXtQSM -- A complete deep learning pipeline for data-consistent quantitative susceptibility mapping trained with hybrid data](http://arxiv.org/abs/2107.07752)


  Deep learning based Quantitative Susceptibility Mapping (QSM) has shown great
potential in recent years, outperforming traditional non-learning approaches in
speed and accuracy. However, many of the current deep learning approaches are
not data consistent, require in vivo training data or do not solve all steps of
the QSM processing pipeline. Here we aim to overcome these limitations and
developed a framework to solve the QSM processing steps jointly. We developed a
new hybrid training data generation method that enables the end-to-end training
for solving background field correction and dipole inversion in a
data-consistent fashion using a variational network that combines the QSM model
term and a learned regularizer. We demonstrate that NeXtQSM overcomes the
limitations of previous model-agnostic deep learning methods and show that
NeXtQSM offers a complete deep learning based pipeline for computing robust,
fast and accurate quantitative susceptibility maps.

    

### [[2107.07754] Measuring Fairness in Generative Models](http://arxiv.org/abs/2107.07754)


  Deep generative models have made much progress in improving training
stability and quality of generated data. Recently there has been increased
interest in the fairness of deep-generated data. Fairness is important in many
applications, e.g. law enforcement, as biases will affect efficacy. Central to
fair data generation are the fairness metrics for the assessment and evaluation
of different generative models. In this paper, we first review fairness metrics
proposed in previous works and highlight potential weaknesses. We then discuss
a performance benchmark framework along with the assessment of alternative
metrics.

    

### [[2107.07757] Entropic alternatives to initialization](http://arxiv.org/abs/2107.07757)


  Local entropic loss functions provide a versatile framework to define
architecture-aware regularization procedures. Besides the possibility of being
anisotropic in the synaptic space, the local entropic smoothening of the loss
function can vary during training, thus yielding a tunable model complexity. A
scoping protocol where the regularization is strong in the early-stage of the
training and then fades progressively away constitutes an alternative to
standard initialization procedures for deep convolutional neural networks,
nonetheless, it has wider applicability. We analyze anisotropic, local entropic
smoothenings in the language of statistical physics and information theory,
providing insight into both their interpretation and workings. We comment some
aspects related to the physics of renormalization and the spacetime structure
of convolutional networks.

    

### [[2107.07788] Reinforcement Learning for Optimal Stationary Control of Linear Stochastic Systems](http://arxiv.org/abs/2107.07788)


  This paper studies the optimal stationary control of continuous-time linear
stochastic systems with both additive and multiplicative noises, using
reinforcement learning techniques. Based on policy iteration, a novel
off-policy reinforcement learning algorithm, named optimistic
least-squares-based policy iteration, is proposed which is able to iteratively
find near-optimal policies of the optimal stationary control problem directly
from input/state data without explicitly identifying any system matrices,
starting from an initial admissible control policy. The solutions given by the
proposed optimistic least-squares-based policy iteration are proved to converge
to a small neighborhood of the optimal solution with probability one, under
mild conditions. The application of the proposed algorithm to a triple inverted
pendulum example validates its feasibility and effectiveness.

    

### [[2107.07791] Graph Representation Learning for Road Type Classification](http://arxiv.org/abs/2107.07791)


  We present a novel learning-based approach to graph representations of road
networks employing state-of-the-art graph convolutional neural networks. Our
approach is applied to realistic road networks of 17 cities from Open Street
Map. While edge features are crucial to generate descriptive graph
representations of road networks, graph convolutional networks usually rely on
node features only. We show that the highly representative edge features can
still be integrated into such networks by applying a line graph transformation.
We also propose a method for neighborhood sampling based on a topological
neighborhood composed of both local and global neighbors. We compare the
performance of learning representations using different types of neighborhood
aggregation functions in transductive and inductive tasks and in supervised and
unsupervised learning. Furthermore, we propose a novel aggregation approach,
Graph Attention Isomorphism Network, GAIN. Our results show that GAIN
outperforms state-of-the-art methods on the road type classification problem.

    

### [[2107.07818] Revisiting IoT Device Identification](http://arxiv.org/abs/2107.07818)


  Internet-of-Things (IoT) devices are known to be the source of many security
problems, and as such, they would greatly benefit from automated management.
This requires robustly identifying devices so that appropriate network security
policies can be applied. We address this challenge by exploring how to
accurately identify IoT devices based on their network behavior, while
leveraging approaches previously proposed by other researchers.
We compare the accuracy of four different previously proposed machine
learning models (tree-based and neural network-based) for identifying IoT
devices. We use packet trace data collected over a period of six months from a
large IoT test-bed. We show that, while all models achieve high accuracy when
evaluated on the same dataset as they were trained on, their accuracy degrades
over time, when evaluated on data collected outside the training set. We show
that on average the models' accuracy degrades after a couple of weeks by up to
40 percentage points (on average between 12 and 21 percentage points). We argue
that, in order to keep the models' accuracy at a high level, these need to be
continuously updated.

    

### [[2107.07820] Contrastive Predictive Coding for Anomaly Detection](http://arxiv.org/abs/2107.07820)


  Reliable detection of anomalies is crucial when deploying machine learning
models in practice, but remains challenging due to the lack of labeled data. To
tackle this challenge, contrastive learning approaches are becoming
increasingly popular, given the impressive results they have achieved in
self-supervised representation learning settings. However, while most existing
contrastive anomaly detection and segmentation approaches have been applied to
images, none of them can use the contrastive losses directly for both anomaly
detection and segmentation. In this paper, we close this gap by making use of
the Contrastive Predictive Coding model (arXiv:1807.03748). We show that its
patch-wise contrastive loss can directly be interpreted as an anomaly score,
and how this allows for the creation of anomaly segmentation masks. The
resulting model achieves promising results for both anomaly detection and
segmentation on the challenging MVTec-AD dataset.

    

### [[2107.07831] Modeling User Behaviour in Research Paper Recommendation System](http://arxiv.org/abs/2107.07831)


  User intention which often changes dynamically is considered to be an
important factor for modeling users in the design of recommendation systems.
Recent studies are starting to focus on predicting user intention (what users
want) beyond user preference (what users like). In this work, a user intention
model is proposed based on deep sequential topic analysis. The model predicts a
user's intention in terms of the topic of interest. The Hybrid Topic Model
(HTM) comprising Latent Dirichlet Allocation (LDA) and Word2Vec is proposed to
derive the topic of interest of users and the history of preferences. HTM finds
the true topics of papers estimating word-topic distribution which includes
syntactic and semantic correlations among words. Next, to model user intention,
a Long Short Term Memory (LSTM) based sequential deep learning model is
proposed. This model takes into account temporal context, namely the time
difference between clicks of two consecutive papers seen by a user. Extensive
experiments with the real-world research paper dataset indicate that the
proposed approach significantly outperforms the state-of-the-art methods.
Further, the proposed approach introduces a new road map to model a user
activity suitable for the design of a research paper recommendation system.

    

### [[2107.07844] Versatile modular neural locomotion control with fast learning](http://arxiv.org/abs/2107.07844)


  Legged robots have significant potential to operate in highly unstructured
environments. The design of locomotion control is, however, still challenging.
Currently, controllers must be either manually designed for specific robots and
tasks, or automatically designed via machine learning methods that require long
training times and yield large opaque controllers. Drawing inspiration from
animal locomotion, we propose a simple yet versatile modular neural control
structure with fast learning. The key advantages of our approach are that
behavior-specific control modules can be added incrementally to obtain
increasingly complex emergent locomotion behaviors, and that neural connections
interfacing with existing modules can be quickly and automatically learned. In
a series of experiments, we show how eight modules can be quickly learned and
added to a base control module to obtain emergent adaptive behaviors allowing a
hexapod robot to navigate in complex environments. We also show that modules
can be added and removed during operation without affecting the functionality
of the remaining controller. Finally, the control approach was successfully
demonstrated on a physical hexapod robot. Taken together, our study reveals a
significant step towards fast automatic design of versatile neural locomotion
control for complex robotic systems.

    

### [[2107.07853] A Causal Perspective on Meaningful and Robust Algorithmic Recourse](http://arxiv.org/abs/2107.07853)


  Algorithmic recourse explanations inform stakeholders on how to act to revert
unfavorable predictions. However, in general ML models do not predict well in
interventional distributions. Thus, an action that changes the prediction in
the desired way may not lead to an improvement of the underlying target. Such
recourse is neither meaningful nor robust to model refits. Extending the work
of Karimi et al. (2021), we propose meaningful algorithmic recourse (MAR) that
only recommends actions that improve both prediction and target. We justify
this selection constraint by highlighting the differences between model audit
and meaningful, actionable recourse explanations. Additionally, we introduce a
relaxation of MAR called effective algorithmic recourse (EAR), which, under
certain assumptions, yields meaningful recourse by only allowing interventions
on causes of the target.

    

### [[2107.07859] Measuring and Explaining the Inter-Cluster Reliability of Multidimensional Projections](http://arxiv.org/abs/2107.07859)


  We propose Steadiness and Cohesiveness, two novel metrics to measure the
inter-cluster reliability of multidimensional projection (MDP), specifically
how well the inter-cluster structures are preserved between the original
high-dimensional space and the low-dimensional projection space. Measuring
inter-cluster reliability is crucial as it directly affects how well
inter-cluster tasks (e.g., identifying cluster relationships in the original
space from a projected view) can be conducted; however, despite the importance
of inter-cluster tasks, we found that previous metrics, such as Trustworthiness
and Continuity, fail to measure inter-cluster reliability. Our metrics consider
two aspects of the inter-cluster reliability: Steadiness measures the extent to
which clusters in the projected space form clusters in the original space, and
Cohesiveness measures the opposite. They extract random clusters with arbitrary
shapes and positions in one space and evaluate how much the clusters are
stretched or dispersed in the other space. Furthermore, our metrics can
quantify pointwise distortions, allowing for the visualization of inter-cluster
reliability in a projection, which we call a reliability map. Through
quantitative experiments, we verify that our metrics precisely capture the
distortions that harm inter-cluster reliability while previous metrics have
difficulty capturing the distortions. A case study also demonstrates that our
metrics and the reliability map 1) support users in selecting the proper
projection techniques or hyperparameters and 2) prevent misinterpretation while
performing inter-cluster tasks, thus allow an adequate identification of
inter-cluster structure.

    

### [[2107.07863] Simultaneous boundary shape estimation and velocity field de-noising in Magnetic Resonance Velocimetry using Physics-informed Neural Networks](http://arxiv.org/abs/2107.07863)


  Magnetic resonance velocimetry (MRV) is a non-invasive experimental technique
widely used in medicine and engineering to measure the velocity field of a
fluid. These measurements are dense but have a low signal-to-noise ratio (SNR).
The measurements can be de-noised by imposing physical constraints on the flow,
which are encapsulated in governing equations for mass and momentum. Previous
studies have required the shape of the boundary (for example, a blood vessel)
to be known a priori. This, however, requires a set of additional measurements,
which can be expensive to obtain. In this paper, we present a physics-informed
neural network that instead uses the noisy MRV data alone to simultaneously
infer the most likely boundary shape and de-noised velocity field. We achieve
this by training an auxiliary neural network that takes the value 1.0 within
the inferred domain of the governing PDE and 0.0 outside. This network is used
to weight the PDE residual term in the loss function accordingly and implicitly
learns the geometry of the system. We test our algorithm by assimilating both
synthetic and real MRV measurements for flows that can be well modeled by the
Poisson and Stokes equations. We find that we are able to reconstruct very
noisy (SNR = 2.5) MRV signals and recover the ground truth with low
reconstruction errors of 3.7 - 7.5%. The simplicity and flexibility of our
physics-informed neural network approach can readily scale to assimilating MRV
data with complex 3D geometries, time-varying 4D data, or unknown parameters in
the physical model.

    

### [[2107.07871] Finite Basis Physics-Informed Neural Networks (FBPINNs): a scalable domain decomposition approach for solving differential equations](http://arxiv.org/abs/2107.07871)


  Recently, physics-informed neural networks (PINNs) have offered a powerful
new paradigm for solving problems relating to differential equations. Compared
to classical numerical methods PINNs have several advantages, for example their
ability to provide mesh-free solutions of differential equations and their
ability to carry out forward and inverse modelling within the same optimisation
problem. Whilst promising, a key limitation to date is that PINNs have
struggled to accurately and efficiently solve problems with large domains
and/or multi-scale solutions, which is crucial for their real-world
application. Multiple significant and related factors contribute to this issue,
including the increasing complexity of the underlying PINN optimisation problem
as the problem size grows and the spectral bias of neural networks. In this
work we propose a new, scalable approach for solving large problems relating to
differential equations called Finite Basis PINNs (FBPINNs). FBPINNs are
inspired by classical finite element methods, where the solution of the
differential equation is expressed as the sum of a finite set of basis
functions with compact support. In FBPINNs neural networks are used to learn
these basis functions, which are defined over small, overlapping subdomains.
FBINNs are designed to address the spectral bias of neural networks by using
separate input normalisation over each subdomain, and reduce the complexity of
the underlying optimisation problem by using many smaller neural networks in a
parallel divide-and-conquer approach. Our numerical experiments show that
FBPINNs are effective in solving both small and larger, multi-scale problems,
outperforming standard PINNs in both accuracy and computational resources
required, potentially paving the way to the application of PINNs on large,
real-world problems.

    

### [[2107.07875] A Penalized Shared-parameter Algorithm for Estimating Optimal Dynamic Treatment Regimens](http://arxiv.org/abs/2107.07875)


  A dynamic treatment regimen (DTR) is a set of decision rules to personalize
treatments for an individual using their medical history. The Q-learning based
Q-shared algorithm has been used to develop DTRs that involve decision rules
shared across multiple stages of intervention. We show that the existing
Q-shared algorithm can suffer from non-convergence due to the use of linear
models in the Q-learning setup, and identify the condition in which Q-shared
fails. Leveraging properties from expansion-constrained ordinary least-squares,
we give a penalized Q-shared algorithm that not only converges in settings that
violate the condition, but can outperform the original Q-shared algorithm even
when the condition is satisfied. We give evidence for the proposed method in a
real-world application and several synthetic simulations.

    

### [[2107.07878] Ranking labs-of-origin for genetically engineered DNA using Metric Learning](http://arxiv.org/abs/2107.07878)


  With the constant advancements of genetic engineering, a common concern is to
be able to identify the lab-of-origin of genetically engineered DNA sequences.
For that reason, AltLabs has hosted the genetic Engineering Attribution
Challenge to gather many teams to propose new tools to solve this problem. Here
we show our proposed method to rank the most likely labs-of-origin and generate
embeddings for DNA sequences and labs. These embeddings can also perform
various other tasks, like clustering both DNA sequences and labs and using them
as features for Machine Learning models applied to solve other problems. This
work demonstrates that our method outperforms the classic training method for
this task while generating other helpful information.

    

### [[2107.07886] Tracing Halpha Fibrils through Bayesian Deep Learning](http://arxiv.org/abs/2107.07886)


  We present a new deep learning method, dubbed FibrilNet, for tracing
chromospheric fibrils in Halpha images of solar observations. Our method
consists of a data pre-processing component that prepares training data from a
threshold-based tool, a deep learning model implemented as a Bayesian
convolutional neural network for probabilistic image segmentation with
uncertainty quantification to predict fibrils, and a post-processing component
containing a fibril-fitting algorithm to determine fibril orientations. The
FibrilNet tool is applied to high-resolution Halpha images from an active
region (AR 12665) collected by the 1.6 m Goode Solar Telescope (GST) equipped
with high-order adaptive optics at the Big Bear Solar Observatory (BBSO). We
quantitatively assess the FibrilNet tool, comparing its image segmentation
algorithm and fibril-fitting algorithm with those employed by the
threshold-based tool. Our experimental results and major findings are
summarized as follows. First, the image segmentation results (i.e., detected
fibrils) of the two tools are quite similar, demonstrating the good learning
capability of FibrilNet. Second, FibrilNet finds more accurate and smoother
fibril orientation angles than the threshold-based tool. Third, FibrilNet is
faster than the threshold-based tool and the uncertainty maps produced by
FibrilNet not only provide a quantitative way to measure the confidence on each
detected fibril, but also help identify fibril structures that are not detected
by the threshold-based tool but are inferred through machine learning. Finally,
we apply FibrilNet to full-disk Halpha images from other solar observatories
and additional high-resolution Halpha images collected by BBSO/GST,
demonstrating the tool's usability in diverse datasets.

    

### [[2107.07889] Single Pass Entrywise-Transformed Low Rank Approximation](http://arxiv.org/abs/2107.07889)


  In applications such as natural language processing or computer vision, one
is given a large $n \times d$ matrix $A = (a_{i,j})$ and would like to compute
a matrix decomposition, e.g., a low rank approximation, of a function $f(A) =
(f(a_{i,j}))$ applied entrywise to $A$. A very important special case is the
likelihood function $f\left( A \right ) = \log{\left( \left| a_{ij}\right|
+1\right)}$. A natural way to do this would be to simply apply $f$ to each
entry of $A$, and then compute the matrix decomposition, but this requires
storing all of $A$ as well as multiple passes over its entries. Recent work of
Liang et al.\ shows how to find a rank-$k$ factorization to $f(A)$ for an $n
\times n$ matrix $A$ using only $n \cdot \operatorname{poly}(\epsilon^{-1}k\log
n)$ words of memory, with overall error $10\|f(A)-[f(A)]_k\|_F^2 +
\operatorname{poly}(\epsilon/k) \|f(A)\|_{1,2}^2$, where $[f(A)]_k$ is the best
rank-$k$ approximation to $f(A)$ and $\|f(A)\|_{1,2}^2$ is the square of the
sum of Euclidean lengths of rows of $f(A)$. Their algorithm uses three passes
over the entries of $A$. The authors pose the open question of obtaining an
algorithm with $n \cdot \operatorname{poly}(\epsilon^{-1}k\log n)$ words of
memory using only a single pass over the entries of $A$. In this paper we
resolve this open question, obtaining the first single-pass algorithm for this
problem and for the same class of functions $f$ studied by Liang et al.
Moreover, our error is $\|f(A)-[f(A)]_k\|_F^2 + \operatorname{poly}(\epsilon/k)
\|f(A)\|_F^2$, where $\|f(A)\|_F^2$ is the sum of squares of Euclidean lengths
of rows of $f(A)$. Thus our error is significantly smaller, as it removes the
factor of $10$ and also $\|f(A)\|_F^2 \leq \|f(A)\|_{1,2}^2$. We also give an
algorithm for regression, pointing out an error in previous work, and
empirically validate our results.

    

### [[2107.07977] An Uncertainty-Aware, Shareable and Transparent Neural Network Architecture for Brain-Age Modeling](http://arxiv.org/abs/2107.07977)


  The deviation between chronological age and age predicted from neuroimaging
data has been identified as a sensitive risk-marker of cross-disorder brain
changes, growing into a cornerstone of biological age-research. However,
Machine Learning models underlying the field do not consider uncertainty,
thereby confounding results with training data density and variability. Also,
existing models are commonly based on homogeneous training sets, often not
independently validated, and cannot be shared due to data protection issues.
Here, we introduce an uncertainty-aware, shareable, and transparent Monte-Carlo
Dropout Composite-Quantile-Regression (MCCQR) Neural Network trained on
N=10,691 datasets from the German National Cohort. The MCCQR model provides
robust, distribution-free uncertainty quantification in high-dimensional
neuroimaging data, achieving lower error rates compared to existing models
across ten recruitment centers and in three independent validation samples
(N=4,004). In two examples, we demonstrate that it prevents spurious
associations and increases power to detect accelerated brain-aging. We make the
pre-trained model publicly available.

    

### [[2107.07983] S2TA: Exploiting Structured Sparsity for Energy-Efficient Mobile CNN Acceleration](http://arxiv.org/abs/2107.07983)


  Exploiting sparsity is a key technique in accelerating quantized
convolutional neural network (CNN) inference on mobile devices. Prior sparse
CNN accelerators largely exploit un-structured sparsity and achieve significant
speedups. Due to the unbounded, largely unpredictable sparsity patterns,
however, exploiting unstructured sparsity requires complicated hardware design
with significant energy and area overhead, which is particularly detrimental to
mobile/IoT inference scenarios where energy and area efficiency are crucial. We
propose to exploit structured sparsity, more specifically, Density Bound Block
(DBB) sparsity for both weights and activations. DBB block tensors bound the
maximum number of non-zeros per block. DBB thus exposes statically predictable
sparsity patterns that enable lean sparsity-exploiting hardware. We propose new
hardware primitives to implement DBB sparsity for (static) weights and
(dynamic) activations, respectively, with very low overheads. Building on top
of the primitives, we describe S2TA, a systolic array-based CNN accelerator
that exploits joint weight and activation DBB sparsity and new dimensions of
data reuse unavailable on the traditional systolic array. S2TA in 16nm achieves
more than 2x speedup and energy reduction compared to a strong baseline of a
systolic array with zero-value clock gating, over five popular CNN benchmarks.
Compared to two recent non-systolic sparse accelerators, Eyeriss v2 (65nm) and
SparTen (45nm), S2TA in 65nm uses about 2.2x and 3.1x less energy per
inference, respectively.

    

### [[2107.07988] Controlled AutoEncoders to Generate Faces from Voices](http://arxiv.org/abs/2107.07988)


  Multiple studies in the past have shown that there is a strong correlation
between human vocal characteristics and facial features. However, existing
approaches generate faces simply from voice, without exploring the set of
features that contribute to these observed correlations. A computational
methodology to explore this can be devised by rephrasing the question to: "how
much would a target face have to change in order to be perceived as the
originator of a source voice?" With this in perspective, we propose a framework
to morph a target face in response to a given voice in a way that facial
features are implicitly guided by learned voice-face correlation in this paper.
Our framework includes a guided autoencoder that converts one face to another,
controlled by a unique model-conditioning component called a gating controller
which modifies the reconstructed face based on input voice recordings. We
evaluate the framework on VoxCelab and VGGFace datasets through human subjects
and face retrieval. Various experiments demonstrate the effectiveness of our
proposed model.

    

### [[2107.07994] Property-aware Adaptive Relation Networks for Molecular Property Prediction](http://arxiv.org/abs/2107.07994)


  Molecular property prediction plays a fundamental role in drug discovery to
discover candidate molecules with target properties. However, molecular
property prediction is essentially a few-shot problem which makes it hard to
obtain regular models. In this paper, we propose a property-aware adaptive
relation networks (PAR) for the few-shot molecular property prediction problem.
In comparison to existing works, we leverage the facts that both substructures
and relationships among molecules are different considering various molecular
properties. Our PAR is compatible with existing graph-based molecular encoders,
and are further equipped with the ability to obtain property-aware molecular
embedding and model molecular relation graph adaptively. The resultant relation
graph also facilitates effective label propagation within each task. Extensive
experiments on benchmark molecular property prediction datasets show that our
method consistently outperforms state-of-the-art methods and is able to obtain
property-aware molecular embedding and model molecular relation graph properly.

    

### [[2107.07997] Uncertainty Prediction for Machine Learning Models of Material Properties](http://arxiv.org/abs/2107.07997)


  Uncertainty quantification in Artificial Intelligence (AI)-based predictions
of material properties is of immense importance for the success and reliability
of AI applications in material science. While confidence intervals are commonly
reported for machine learning (ML) models, prediction intervals, i.e., the
evaluation of the uncertainty on each prediction, are seldomly available. In
this work we compare 3 different approaches to obtain such individual
uncertainty, testing them on 12 ML-physical properties. Specifically, we
investigated using the Quantile loss function, machine learning the prediction
intervals directly and using Gaussian Processes. We identify each approachs
advantages and disadvantages and end up slightly favoring the modeling of the
individual uncertainties directly, as it is the easiest to fit and, in most
cases, minimizes over-and under-estimation of the predicted errors. All data
for training and testing were taken from the publicly available JARVIS-DFT
database, and the codes developed for computing the prediction intervals are
available through JARVIS-Tools.

    

### [[2107.07999] Graph Kernel Attention Transformers](http://arxiv.org/abs/2107.07999)


  We introduce a new class of graph neural networks (GNNs), by combining
several concepts that were so far studied independently - graph kernels,
attention-based networks with structural priors and more recently, efficient
Transformers architectures applying small memory footprint implicit attention
methods via low rank decomposition techniques. The goal of the paper is
twofold. Proposed by us Graph Kernel Attention Transformers (or GKATs) are much
more expressive than SOTA GNNs as capable of modeling longer-range dependencies
within a single layer. Consequently, they can use more shallow architecture
design. Furthermore, GKAT attention layers scale linearly rather than
quadratically in the number of nodes of the input graphs, even when those
graphs are dense, requiring less compute than their regular graph attention
counterparts. They achieve it by applying new classes of graph kernels
admitting random feature map decomposition via random walks on graphs. As a
byproduct of the introduced techniques, we obtain a new class of learnable
graph sketches, called graphots, compactly encoding topological graph
properties as well as nodes' features. We conducted exhaustive empirical
comparison of our method with nine different GNN classes on tasks ranging from
motif detection through social network classification to bioinformatics
challenges, showing consistent gains coming from GKATs.

    

### [[2107.08001] Efficient Bayesian Sampling Using Normalizing Flows to Assist Markov Chain Monte Carlo Methods](http://arxiv.org/abs/2107.08001)


  Normalizing flows can generate complex target distributions and thus show
promise in many applications in Bayesian statistics as an alternative or
complement to MCMC for sampling posteriors. Since no data set from the target
posterior distribution is available beforehand, the flow is typically trained
using the reverse Kullback-Leibler (KL) divergence that only requires samples
from a base distribution. This strategy may perform poorly when the posterior
is complicated and hard to sample with an untrained normalizing flow. Here we
explore a distinct training strategy, using the direct KL divergence as loss,
in which samples from the posterior are generated by (i) assisting a local MCMC
algorithm on the posterior with a normalizing flow to accelerate its mixing
rate and (ii) using the data generated this way to train the flow. The method
only requires a limited amount of \textit{a~priori} input about the posterior,
and can be used to estimate the evidence required for model validation, as we
illustrate on examples.

    

### [[2107.08011] Adaptive first-order methods revisited: Convex optimization without Lipschitz requirements](http://arxiv.org/abs/2107.08011)


  We propose a new family of adaptive first-order methods for a class of convex
minimization problems that may fail to be Lipschitz continuous or smooth in the
standard sense. Specifically, motivated by a recent flurry of activity on
non-Lipschitz (NoLips) optimization, we consider problems that are continuous
or smooth relative to a reference Bregman function - as opposed to a global,
ambient norm (Euclidean or otherwise). These conditions encompass a wide range
of problems with singular objectives, such as Fisher markets, Poisson
tomography, D-design, and the like. In this setting, the application of
existing order-optimal adaptive methods - like UnixGrad or AcceleGrad - is not
possible, especially in the presence of randomness and uncertainty. The
proposed method - which we call adaptive mirror descent (AdaMir) - aims to
close this gap by concurrently achieving min-max optimal rates in problems that
are relatively continuous or smooth, including stochastic ones.

    

### [[2107.08013] Machine-learning Kondo physics using variational autoencoders](http://arxiv.org/abs/2107.08013)


  We employ variational autoencoders to extract physical insight from a dataset
of one-particle Anderson impurity model spectral functions. Autoencoders are
trained to find a low-dimensional, latent space representation that faithfully
characterizes each element of the training set, as measured by a reconstruction
error. Variational autoencoders, a probabilistic generalization of standard
autoencoders, further condition the learned latent space to promote highly
interpretable features. In our study, we find that the learned latent space
components strongly correlate with well known, but nontrivial, parameters that
characterize emergent behaviors in the Anderson impurity model. In particular,
one latent space component correlates with particle-hole asymmetry, while
another is in near one-to-one correspondence with the Kondo temperature, a
dynamically generated low-energy scale in the impurity model. With symbolic
regression, we model this component as a function of bare physical input
parameters and "rediscover" the non-perturbative formula for the Kondo
temperature. The machine learning pipeline we develop opens opportunities to
discover new domain knowledge in other physical systems.

    

### [[2107.08020] Online Graph Topology Learning from Matrix-valued Time Series](http://arxiv.org/abs/2107.08020)


  This paper is concerned with the statistical analysis of matrix-valued time
series. These are data collected over a network of sensors (typically a set of
spatial locations), recording, over time, observations of multiple
measurements. From such data, we propose to learn, in an online fashion, a
graph that captures two aspects of dependency: one describing the sparse
spatial relationship between sensors, and the other characterizing the
measurement relationship. To this purpose, we introduce a novel multivariate
autoregressive model to infer the graph topology encoded in the coefficient
matrix which captures the sparse Granger causality dependency structure present
in such matrix-valued time series. We decompose the graph by imposing a
Kronecker sum structure on the coefficient matrix. We develop two online
approaches to learn the graph in a recursive way. The first one uses Wald test
for the projected OLS estimation, where we derive the asymptotic distribution
for the estimator. For the second one, we formalize a Lasso-type optimization
problem. We rely on homotopy algorithms to derive updating rules for estimating
the coefficient matrix. Furthermore, we provide an adaptive tuning procedure
for the regularization parameter. Numerical experiments using both synthetic
and real data, are performed to support the effectiveness of the proposed
learning approaches.

    

### [[2107.08024] Port-Hamiltonian Neural Networks for Learning Explicit Time-Dependent Dynamical Systems](http://arxiv.org/abs/2107.08024)


  Accurately learning the temporal behavior of dynamical systems requires
models with well-chosen learning biases. Recent innovations embed the
Hamiltonian and Lagrangian formalisms into neural networks and demonstrate a
significant improvement over other approaches in predicting trajectories of
physical systems. These methods generally tackle autonomous systems that depend
implicitly on time or systems for which a control signal is known apriori.
Despite this success, many real world dynamical systems are non-autonomous,
driven by time-dependent forces and experience energy dissipation. In this
study, we address the challenge of learning from such non-autonomous systems by
embedding the port-Hamiltonian formalism into neural networks, a versatile
framework that can capture energy dissipation and time-dependent control
forces. We show that the proposed \emph{port-Hamiltonian neural network} can
efficiently learn the dynamics of nonlinear physical systems of practical
interest and accurately recover the underlying stationary Hamiltonian,
time-dependent force, and dissipative coefficient. A promising outcome of our
network is its ability to learn and predict chaotic systems such as the Duffing
equation, for which the trajectories are typically hard to learn.

    

### [[2107.08027] SOK: Seeing and Believing: Evaluating the Trustworthiness of Twitter Users](http://arxiv.org/abs/2107.08027)


  Social networking and micro-blogging services, such as Twitter, play an
important role in sharing digital information. Despite the popularity and
usefulness of social media, there have been many instances where corrupted
users found ways to abuse it, as for instance, through raising or lowering
user's credibility. As a result, while social media facilitates an
unprecedented ease of access to information, it also introduces a new challenge
- that of ascertaining the credibility of shared information. Currently, there
is no automated way of determining which news or users are credible and which
are not. Hence, establishing a system that can measure the social media user's
credibility has become an issue of great importance. Assigning a credibility
score to a user has piqued the interest of not only the research community but
also most of the big players on both sides - such as Facebook, on the side of
industry, and political parties on the societal one. In this work, we created a
model which, we hope, will ultimately facilitate and support the increase of
trust in the social network communities. Our model collected data and analysed
the behaviour of~50,000 politicians on Twitter. Influence score, based on
several chosen features, was assigned to each evaluated user. Further, we
classified the political Twitter users as either trusted or untrusted using
random forest, multilayer perceptron, and support vector machine. An active
learning model was used to classify any unlabelled ambiguous records from our
dataset. Finally, to measure the performance of the proposed model, we used
precision, recall, F1 score, and accuracy as the main evaluation metrics.

    

### [[2107.08028] Continual Learning for Automated Audio Captioning Using The Learning Without Forgetting Approach](http://arxiv.org/abs/2107.08028)


  Automated audio captioning (AAC) is the task of automatically creating
textual descriptions (i.e. captions) for the contents of a general audio
signal. Most AAC methods are using existing datasets to optimize and/or
evaluate upon. Given the limited information held by the AAC datasets, it is
very likely that AAC methods learn only the information contained in the
utilized datasets. In this paper we present a first approach for continuously
adapting an AAC method to new information, using a continual learning method.
In our scenario, a pre-optimized AAC method is used for some unseen general
audio signals and can update its parameters in order to adapt to the new
information, given a new reference caption. We evaluate our method using a
freely available, pre-optimized AAC method and two freely available AAC
datasets. We compare our proposed method with three scenarios, two of training
on one of the datasets and evaluating on the other and a third of training on
one dataset and fine-tuning on the other. Obtained results show that our method
achieves a good balance between distilling new knowledge and not forgetting the
previous one.

    

### [[2107.08031] Is attention to bounding boxes all you need for pedestrian action prediction?](http://arxiv.org/abs/2107.08031)


  The human driver is no longer the only one concerned with the complexity of
the driving scenarios. Autonomous vehicles (AV) are similarly becoming involved
in the process. Nowadays, the development of AV in urban places underpins
essential safety concerns for vulnerable road users (VRUs) such as pedestrians.
Therefore, to make the roads safer, it is critical to classify and predict
their future behavior. In this paper, we present a framework based on multiple
variations of the Transformer models to reason attentively about the dynamic
evolution of the pedestrians' past trajectory and predict its future actions of
crossing or not crossing the street. We proved that using only bounding boxes
as input to our model can outperform the previous state-of-the-art models and
reach a prediction accuracy of 91 % and an F1-score of 0.83 on the PIE dataset
up to two seconds ahead in the future. In addition, we introduced a large-size
simulated dataset (CP2A) using CARLA for action prediction. Our model has
similarly reached high accuracy (91 %) and F1-score (0.91) on this dataset.
Interestingly, we showed that pre-training our Transformer model on the
simulated dataset and then fine-tuning it on the real dataset can be very
effective for the action prediction task.

    

### [[2107.08039] Representation Consolidation for Training Expert Students](http://arxiv.org/abs/2107.08039)


  Traditionally, distillation has been used to train a student model to emulate
the input/output functionality of a teacher. A more useful goal than emulation,
yet under-explored, is for the student to learn feature representations that
transfer well to future tasks. However, we observe that standard distillation
of task-specific teachers actually *reduces* the transferability of student
representations to downstream tasks. We show that a multi-head, multi-task
distillation method using an unlabeled proxy dataset and a generalist teacher
is sufficient to consolidate representations from task-specific teacher(s) and
improve downstream performance, outperforming the teacher(s) and the strong
baseline of ImageNet pretrained features. Our method can also combine the
representational knowledge of multiple teachers trained on one or multiple
domains into a single model, whose representation is improved on all teachers'
domain(s).

    

### [[1809.09910] Generalization Properties of hyper-RKHS and its Applications](http://arxiv.org/abs/1809.09910)


  This paper generalizes regularized regression problems in a hyper-reproducing
kernel Hilbert space (hyper-RKHS), illustrates its utility for kernel learning
and out-of-sample extensions, and proves asymptotic convergence results for the
introduced regression models in an approximation theory view. Algorithmically,
we consider two regularized regression models with bivariate forms in this
space, including kernel ridge regression (KRR) and support vector regression
(SVR) endowed with hyper-RKHS, and further combine divide-and-conquer with
Nyström approximation for scalability in large sample cases. This framework
is general: the underlying kernel is learned from a broad class, and can be
positive definite or not, which adapts to various requirements in kernel
learning. Theoretically, we study the convergence behavior of regularized
regression algorithms in hyper-RKHS and derive the learning rates, which goes
beyond the classical analysis on RKHS due to the non-trivial independence of
pairwise samples and the characterisation of hyper-RKHS. Experimentally,
results on several benchmarks suggest that the employed framework is able to
learn a general kernel function form an arbitrary similarity matrix, and thus
achieves a satisfactory performance on classification tasks.

    

### [[2004.02653] Gaussian Process Boosting](http://arxiv.org/abs/2004.02653)


  We introduce a novel way to combine boosting with Gaussian process and mixed
effects models. This allows for relaxing, first, the linearity assumption for
the mean function in Gaussian process and grouped random effects models in a
flexible non-parametric way and, second, the independence assumption made in
most boosting algorithms. The former is advantageous for predictive accuracy
and for avoiding model misspecifications. The latter is important for more
efficient learning of the mean function and for obtaining probabilistic
predictions. In addition, we present an extension that scales to large data
using a Vecchia approximation for the Gaussian process model relying on novel
results for covariance parameter inference. We obtain increased predictive
accuracy compared to existing approaches on several simulated and real-world
data sets.

    

### [[2004.13688] Variational Integrator Graph Networks for Learning Energy Conserving Dynamical Systems](http://arxiv.org/abs/2004.13688)


  Recent advances show that neural networks embedded with physics-informed
priors significantly outperform vanilla neural networks in learning and
predicting the long term dynamics of complex physical systems from noisy data.
Despite this success, there has only been a limited study on how to optimally
combine physics priors to improve predictive performance. To tackle this
problem we unpack and generalize recent innovations into individual inductive
bias segments. As such, we are able to systematically investigate all possible
combinations of inductive biases of which existing methods are a natural
subset. Using this framework we introduce Variational Integrator Graph Networks
- a novel method that unifies the strengths of existing approaches by combining
an energy constraint, high-order symplectic variational integrators, and graph
neural networks. We demonstrate, across an extensive ablation, that the
proposed unifying framework outperforms existing methods, for data-efficient
learning and in predictive accuracy, across both single and many-body problems
studied in recent literature. We empirically show that the improvements arise
because high order variational integrators combined with a potential energy
constraint induce coupled learning of generalized position and momentum updates
which can be formalized via the Partitioned Runge-Kutta method.

    

### [[2005.09048] Stable and consistent density-based clustering](http://arxiv.org/abs/2005.09048)


  We present a multiscale, consistent approach to density-based clustering that
satisfies stability theorems -- in both the input data and in the parameters --
which hold without distributional assumptions. The stability in the input data
is with respect to the Gromov--Hausdorff--Prokhorov distance on metric
probability spaces and interleaving distances between (multi-parameter)
hierarchical clusterings we introduce. We prove stability results for standard
simplification procedures for hierarchical clusterings, which can be combined
with our approach to yield a stable flat clustering algorithm. We illustrate
the stability of the approach with computational examples. Our framework is
based on the concepts of persistence and interleaving distance from Topological
Data Analysis.

    

### [[2006.07886] On Disentangled Representations Learned From Correlated Data](http://arxiv.org/abs/2006.07886)


  The focus of disentanglement approaches has been on identifying independent
factors of variation in data. However, the causal variables underlying
real-world observations are often not statistically independent. In this work,
we bridge the gap to real-world scenarios by analyzing the behavior of the most
prominent disentanglement approaches on correlated data in a large-scale
empirical study (including 4260 models). We show and quantify that
systematically induced correlations in the dataset are being learned and
reflected in the latent representations, which has implications for downstream
applications of disentanglement such as fairness. We also demonstrate how to
resolve these latent correlations, either using weak supervision during
training or by post-hoc correcting a pre-trained model with a small number of
labels.

    

### [[2007.04785] Accuracy Prediction with Non-neural Model for Neural Architecture Search](http://arxiv.org/abs/2007.04785)


  Neural architecture search (NAS) with an accuracy predictor that predicts the
accuracy of candidate architectures has drawn increasing attention due to its
simplicity and effectiveness. Previous works usually employ neural
network-based predictors which require more delicate design and are easy to
overfit. Considering that most architectures are represented as sequences of
discrete symbols which are more like tabular data and preferred by non-neural
predictors, in this paper, we study an alternative approach which uses
non-neural model for accuracy prediction. Specifically, as decision tree based
models can better handle tabular data, we leverage gradient boosting decision
tree (GBDT) as the predictor for NAS. We demonstrate that the GBDT predictor
can achieve comparable (if not better) prediction accuracy than neural network
based predictors. Moreover, considering that a compact search space can ease
the search process, we propose to prune the search space gradually according to
important features derived from GBDT. In this way, NAS can be performed by
first pruning the search space and then searching a neural architecture, which
is more efficient and effective. Experiments on NASBench-101 and ImageNet
demonstrate the effectiveness of using GBDT as predictor for NAS: (1) On
NASBench-101, it is 22x, 8x, and 6x more sample efficient than random search,
regularized evolution, and Monte Carlo Tree Search (MCTS) in finding the global
optimum; (2) It achieves 24.2% top-1 error rate on ImageNet, and further
achieves 23.4% top-1 error rate on ImageNet when enhanced with search space
pruning. Code is provided in the supplementary materials.

    

### [[2009.12789] Learning Optimal Representations with the Decodable Information Bottleneck](http://arxiv.org/abs/2009.12789)


  We address the question of characterizing and finding optimal representations
for supervised learning. Traditionally, this question has been tackled using
the Information Bottleneck, which compresses the inputs while retaining
information about the targets, in a decoder-agnostic fashion. In machine
learning, however, our goal is not compression but rather generalization, which
is intimately linked to the predictive family or decoder of interest (e.g.
linear classifier). We propose the Decodable Information Bottleneck (DIB) that
considers information retention and compression from the perspective of the
desired predictive family. As a result, DIB gives rise to representations that
are optimal in terms of expected test performance and can be estimated with
guarantees. Empirically, we show that the framework can be used to enforce a
small generalization gap on downstream classifiers and to predict the
generalization ability of neural networks.

    

### [[2010.04029] RNNLogic: Learning Logic Rules for Reasoning on Knowledge Graphs](http://arxiv.org/abs/2010.04029)


  This paper studies learning logic rules for reasoning on knowledge graphs.
Logic rules provide interpretable explanations when used for prediction as well
as being able to generalize to other tasks, and hence are critical to learn.
Existing methods either suffer from the problem of searching in a large search
space (e.g., neural logic programming) or ineffective optimization due to
sparse rewards (e.g., techniques based on reinforcement learning). To address
these limitations, this paper proposes a probabilistic model called RNNLogic.
RNNLogic treats logic rules as a latent variable, and simultaneously trains a
rule generator as well as a reasoning predictor with logic rules. We develop an
EM-based algorithm for optimization. In each iteration, the reasoning predictor
is first updated to explore some generated logic rules for reasoning. Then in
the E-step, we select a set of high-quality rules from all generated rules with
both the rule generator and reasoning predictor via posterior inference; and in
the M-step, the rule generator is updated with the rules selected in the
E-step. Experiments on four datasets prove the effectiveness of RNNLogic.

    

### [[2010.08853] From Local Structures to Size Generalization in Graph Neural Networks](http://arxiv.org/abs/2010.08853)


  Graph neural networks (GNNs) can process graphs of different sizes, but their
ability to generalize across sizes, specifically from small to large graphs, is
still not well understood. In this paper, we identify an important type of data
where generalization from small to large graphs is challenging: graph
distributions for which the local structure depends on the graph size. This
effect occurs in multiple important graph learning domains, including social
and biological networks. We first prove that when there is a difference between
the local structures, GNNs are not guaranteed to generalize across sizes: there
are "bad" global minima that do well on small graphs but fail on large graphs.
We then study the size-generalization problem empirically and demonstrate that
when there is a discrepancy in local structure, GNNs tend to converge to
non-generalizing solutions. Finally, we suggest two approaches for improving
size generalization, motivated by our findings. Notably, we propose a novel
Self-Supervised Learning (SSL) task aimed at learning meaningful
representations of local structures that appear in large graphs. Our SSL task
improves classification accuracy on several popular datasets.

    

### [[2010.11524] SlimIPL: Language-Model-Free Iterative Pseudo-Labeling](http://arxiv.org/abs/2010.11524)


  Recent results in end-to-end automatic speech recognition have demonstrated
the efficacy of pseudo-labeling for semi-supervised models trained both with
Connectionist Temporal Classification (CTC) and Sequence-to-Sequence (seq2seq)
losses. Iterative Pseudo-Labeling (IPL), which continuously trains a single
model using pseudo-labels iteratively re-generated as the model learns, has
been shown to further improve performance in ASR. We improve upon the IPL
algorithm: as the model learns, we propose to iteratively re-generate
transcriptions with hard labels (the most probable tokens), that is, without a
language model. We call this approach Language-Model-Free IPL (slimIPL) and
give a resultant training setup for low-resource settings with CTC-based
models. slimIPL features a dynamic cache for pseudo-labels which reduces
sensitivity to changes in relabeling hyperparameters and results in improves
training stability. slimIPL is also highly-efficient and requires 3.5-4x fewer
computational resources to converge than other state-of-the-art
semi/self-supervised approaches. With only 10 hours of labeled audio, slimIPL
is competitive with self-supervised approaches, and is state-of-the-art with
100 hours of labeled audio without the use of a language model both at test
time and during pseudo-label generation.

    

### [[2012.03245] Capturing Delayed Feedback in Conversion Rate Prediction via Elapsed-Time Sampling](http://arxiv.org/abs/2012.03245)


  Conversion rate (CVR) prediction is one of the most critical tasks for
digital display advertising. Commercial systems often require to update models
in an online learning manner to catch up with the evolving data distribution.
However, conversions usually do not happen immediately after a user click. This
may result in inaccurate labeling, which is called delayed feedback problem. In
previous studies, delayed feedback problem is handled either by waiting
positive label for a long period of time, or by consuming the negative sample
on its arrival and then insert a positive duplicate when a conversion happens
later. Indeed, there is a trade-off between waiting for more accurate labels
and utilizing fresh data, which is not considered in existing works. To strike
a balance in this trade-off, we propose Elapsed-Time Sampling Delayed Feedback
Model (ES-DFM), which models the relationship between the observed conversion
distribution and the true conversion distribution. Then we optimize the
expectation of true conversion distribution via importance sampling under the
elapsed-time sampling distribution. We further estimate the importance weight
for each instance, which is used as the weight of loss function in CVR
prediction. To demonstrate the effectiveness of ES-DFM, we conduct extensive
experiments on a public data and a private industrial dataset. Experimental
results confirm that our method consistently outperforms the previous
state-of-the-art results.

    

### [[2012.05199] A Riemannian Block Coordinate Descent Method for Computing the Projection Robust Wasserstein Distance](http://arxiv.org/abs/2012.05199)


  The Wasserstein distance has become increasingly important in machine
learning and deep learning. Despite its popularity, the Wasserstein distance is
hard to approximate because of the curse of dimensionality. A recently proposed
approach to alleviate the curse of dimensionality is to project the sampled
data from the high dimensional probability distribution onto a
lower-dimensional subspace, and then compute the Wasserstein distance between
the projected data. However, this approach requires to solve a max-min problem
over the Stiefel manifold, which is very challenging in practice. The only
existing work that solves this problem directly is the RGAS (Riemannian
Gradient Ascent with Sinkhorn Iteration) algorithm, which requires to solve an
entropy-regularized optimal transport problem in each iteration, and thus can
be costly for large-scale problems. In this paper, we propose a Riemannian
block coordinate descent (RBCD) method to solve this problem, which is based on
a novel reformulation of the regularized max-min problem over the Stiefel
manifold. We show that the complexity of arithmetic operations for RBCD to
obtain an $\epsilon$-stationary point is $O(\epsilon^{-3})$. This significantly
improves the corresponding complexity of RGAS, which is $O(\epsilon^{-12})$.
Moreover, our RBCD has very low per-iteration complexity, and hence is suitable
for large-scale problems. Numerical results on both synthetic and real datasets
demonstrate that our method is more efficient than existing methods, especially
when the number of sampled data is very large.

    

### [[2012.05457] Neural-Swarm2: Planning and Control of Heterogeneous Multirotor Swarms using Learned Interactions](http://arxiv.org/abs/2012.05457)


  We present Neural-Swarm2, a learning-based method for motion planning and
control that allows heterogeneous multirotors in a swarm to safely fly in close
proximity. Such operation for drones is challenging due to complex aerodynamic
interaction forces, such as downwash generated by nearby drones and ground
effect. Conventional planning and control methods neglect capturing these
interaction forces, resulting in sparse swarm configuration during flight. Our
approach combines a physics-based nominal dynamics model with learned Deep
Neural Networks (DNNs) with strong Lipschitz properties. We make use of two
techniques to accurately predict the aerodynamic interactions between
heterogeneous multirotors: i) spectral normalization for stability and
generalization guarantees of unseen data and ii) heterogeneous deep sets for
supporting any number of heterogeneous neighbors in a permutation-invariant
manner without reducing expressiveness. The learned residual dynamics benefit
both the proposed interaction-aware multi-robot motion planning and the
nonlinear tracking control design because the learned interaction forces reduce
the modelling errors. Experimental results demonstrate that Neural-Swarm2 is
able to generalize to larger swarms beyond training cases and significantly
outperforms a baseline nonlinear tracking controller with up to three times
reduction in worst-case tracking errors.

    

### [[2012.07421] WILDS: A Benchmark of in-the-Wild Distribution Shifts](http://arxiv.org/abs/2012.07421)


  Distribution shifts -- where the training distribution differs from the test
distribution -- can substantially degrade the accuracy of machine learning (ML)
systems deployed in the wild. Despite their ubiquity in the real-world
deployments, these distribution shifts are under-represented in the datasets
widely used in the ML community today. To address this gap, we present WILDS, a
curated benchmark of 10 datasets reflecting a diverse range of distribution
shifts that naturally arise in real-world applications, such as shifts across
hospitals for tumor identification; across camera traps for wildlife
monitoring; and across time and location in satellite imaging and poverty
mapping. On each dataset, we show that standard training yields substantially
lower out-of-distribution than in-distribution performance. This gap remains
even with models trained by existing methods for tackling distribution shifts,
underscoring the need for new methods for training models that are more robust
to the types of distribution shifts that arise in practice. To facilitate
method development, we provide an open-source package that automates dataset
loading, contains default model architectures and hyperparameters, and
standardizes evaluations. Code and leaderboards are available at
this https URL.

    

### [[2012.11048] Bayesian Crowdsourcing with Constraints](http://arxiv.org/abs/2012.11048)


  Crowdsourcing has emerged as a powerful paradigm for efficiently labeling
large datasets and performing various learning tasks, by leveraging crowds of
human annotators. When additional information is available about the data,
semi-supervised crowdsourcing approaches that enhance the aggregation of labels
from human annotators are well motivated. This work deals with semi-supervised
crowdsourced classification, under two regimes of semi-supervision: a) label
constraints, that provide ground-truth labels for a subset of data; and b)
potentially easier to obtain instance-level constraints, that indicate
relationships between pairs of data. Bayesian algorithms based on variational
inference are developed for each regime, and their quantifiably improved
performance, compared to unsupervised crowdsourcing, is analytically and
empirically validated on several crowdsourcing datasets.

    

### [[2012.14172] Manifold learning with arbitrary norms](http://arxiv.org/abs/2012.14172)


  Manifold learning methods play a prominent role in nonlinear dimensionality
reduction and other tasks involving high-dimensional data sets with low
intrinsic dimensionality. Many of these methods are graph-based: they associate
a vertex with each data point and a weighted edge with each pair. Existing
theory shows that the Laplacian matrix of the graph converges to the
Laplace-Beltrami operator of the data manifold, under the assumption that the
pairwise affinities are based on the Euclidean norm. In this paper, we
determine the limiting differential operator for graph Laplacians constructed
using $\textit{any}$ norm. Our proof involves an interplay between the second
fundamental form of the manifold and the convex geometry of the given norm's
unit ball. To demonstrate the potential benefits of non-Euclidean norms in
manifold learning, we consider the task of mapping the motion of large
molecules with continuous variability. In a numerical simulation we show that a
modified Laplacian eigenmaps algorithm, based on the Earthmover's distance,
outperforms the classic Euclidean Laplacian eigenmaps, both in terms of
computational cost and the sample size needed to recover the intrinsic
geometry.

    

### [[2101.00926] CLeaR: An Adaptive Continual Learning Framework for Regression Tasks](http://arxiv.org/abs/2101.00926)


  Catastrophic forgetting means that a trained neural network model gradually
forgets the previously learned tasks when being retrained on new tasks.
Overcoming the forgetting problem is a major problem in machine learning.
Numerous continual learning algorithms are very successful in incremental
learning of classification tasks, where new samples with their labels appear
frequently. However, there is currently no research that addresses the
catastrophic forgetting problem in regression tasks as far as we know. This
problem has emerged as one of the primary constraints in some applications,
such as renewable energy forecasts. This article clarifies problem-related
definitions and proposes a new methodological framework that can forecast
targets and update itself by means of continual learning. The framework
consists of forecasting neural networks and buffers, which store newly
collected data from a non-stationary data stream in an application. The changed
probability distribution of the data stream, which the framework has
identified, will be learned sequentially. The framework is called CLeaR
(Continual Learning for Regression Tasks), where components can be flexibly
customized for a specific application scenario. We design two sets of
experiments to evaluate the CLeaR framework concerning fitting error
(training), prediction error (test), and forgetting ratio. The first one is
based on an artificial time series to explore how hyperparameters affect the
CLeaR framework. The second one is designed with data collected from European
wind farms to evaluate the CLeaR framework's performance in a real-world
application. The experimental results demonstrate that the CLeaR framework can
continually acquire knowledge in the data stream and improve the prediction
accuracy. The article concludes with further research issues arising from
requirements to extend the framework.

    

### [[2102.00815] Bellman Eluder Dimension: New Rich Classes of RL Problems, and Sample-Efficient Algorithms](http://arxiv.org/abs/2102.00815)


  Finding the minimal structural assumptions that empower sample-efficient
learning is one of the most important research directions in Reinforcement
Learning (RL). This paper advances our understanding of this fundamental
question by introducing a new complexity measure -- Bellman Eluder (BE)
dimension. We show that the family of RL problems of low BE dimension is
remarkably rich, which subsumes a vast majority of existing tractable RL
problems including but not limited to tabular MDPs, linear MDPs, reactive
POMDPs, low Bellman rank problems as well as low Eluder dimension problems.
This paper further designs a new optimization-based algorithm -- GOLF, and
reanalyzes a hypothesis elimination-based algorithm -- OLIVE (proposed in Jiang
et al., 2017). We prove that both algorithms learn the near-optimal policies of
low BE dimension problems in a number of samples that is polynomial in all
relevant parameters, but independent of the size of state-action space. Our
regret and sample complexity results match or improve the best existing results
for several well-known subclasses of low BE dimension problems.

    

### [[2102.03390] Projection Robust Wasserstein Barycenters](http://arxiv.org/abs/2102.03390)


  Collecting and aggregating information from several probability measures or
histograms is a fundamental task in machine learning. One of the popular
solution methods for this task is to compute the barycenter of the probability
measures under the Wasserstein metric. However, approximating the Wasserstein
barycenter is numerically challenging because of the curse of dimensionality.
This paper proposes the projection robust Wasserstein barycenter (PRWB) that
has the potential to mitigate the curse of dimensionality. Since PRWB is
numerically very challenging to solve, we further propose a relaxed PRWB
(RPRWB) model, which is more tractable. The RPRWB projects the probability
measures onto a lower-dimensional subspace that maximizes the Wasserstein
barycenter objective. The resulting problem is a max-min problem over the
Stiefel manifold. By combining the iterative Bregman projection algorithm and
Riemannian optimization, we propose two new algorithms for computing the RPRWB.
The complexity of arithmetic operations of the proposed algorithms for
obtaining an $\epsilon$-stationary solution is analyzed. We incorporate the
RPRWB into a discrete distribution clustering algorithm, and the numerical
results on real text datasets confirm that our RPRWB model helps improve the
clustering performance significantly.

    

### [[2102.04565] A Ranking Approach to Fair Classification](http://arxiv.org/abs/2102.04565)


  Algorithmic decision systems are increasingly used in areas such as hiring,
school admission, or loan approval. Typically, these systems rely on labeled
data for training a classification model. However, in many scenarios,
ground-truth labels are unavailable, and instead we have only access to
imperfect labels as the result of (potentially biased) human-made decisions.
Despite being imperfect, historical decisions often contain some useful
information on the unobserved true labels. In this paper, we focus on scenarios
where only imperfect labels are available and propose a new fair ranking-based
decision system based on monotonic relationships between legitimate features
and the outcome. Our approach is both intuitive and easy to implement, and thus
particularly suitable for adoption in real-world settings. More in detail, we
introduce a distance-based decision criterion, which incorporates useful
information from historical decisions and accounts for unwanted correlation
between protected and legitimate features. Through extensive experiments on
synthetic and real-world data, we show that our method is fair in the sense
that a) it assigns the desirable outcome to the most qualified individuals, and
b) it removes the effect of stereotypes in decision-making, thereby
outperforming traditional classification algorithms. Additionally, we are able
to show theoretically that our method is consistent with a prominent concept of
individual fairness which states that "similar individuals should be treated
similarly."

    

### [[2102.05073] Point Cloud Transformers applied to Collider Physics](http://arxiv.org/abs/2102.05073)


  Methods for processing point cloud information have seen a great success in
collider physics applications. One recent breakthrough in machine learning is
the usage of Transformer networks to learn semantic relationships between
sequences in language processing. In this work, we apply a modified Transformer
network called Point Cloud Transformer as a method to incorporate the
advantages of the Transformer architecture to an unordered set of particles
resulting from collision events. To compare the performance with other
strategies, we study jet-tagging applications for highly-boosted particles.

    

### [[2102.07402] Information flows of diverse autoencoders](http://arxiv.org/abs/2102.07402)


  The outstanding performance of deep learning in various fields has been a
fundamental query, which can be potentially examined using information theory
that interprets the learning process as the transmission and compression of
information. Information plane analyses of the mutual information between the
input-hidden-output layers demonstrated two distinct learning phases of fitting
and compression. It is debatable if the compression phase is necessary to
generalize the input-output relations extracted from training data. In this
study, we investigated this through experiments with various species of
autoencoders and evaluated their information processing phase with an accurate
kernel-based estimator of mutual information. Given sufficient training data,
vanilla autoencoders demonstrated the compression phase, which was amplified
after imposing sparsity regularization for hidden activities. However, we found
that the compression phase is not universally observed in different species of
autoencoders, including variational autoencoders, that have special constraints
on network weights or manifold of hidden space. These types of autoencoders
exhibited perfect generalization ability for test data without requiring the
compression phase. Thus, we conclude that the compression phase is not
necessary for generalization in representation learning.

    

### [[2102.12353] Nonlinear Invariant Risk Minimization: A Causal Approach](http://arxiv.org/abs/2102.12353)


  Due to spurious correlations, machine learning systems often fail to
generalize to environments whose distributions differ from the ones used at
training time. Prior work addressing this, either explicitly or implicitly,
attempted to find a data representation that has an invariant relationship with
the target. This is done by leveraging a diverse set of training environments
to reduce the effect of spurious features and build an invariant predictor.
However, these methods have generalization guarantees only when both data
representation and classifiers come from a linear model class. We propose
invariant Causal Representation Learning (iCaRL), an approach that enables
out-of-distribution (OOD) generalization in the nonlinear setting (i.e.,
nonlinear representations and nonlinear classifiers). It builds upon a
practical and general assumption: the prior over the data representation (i.e.,
a set of latent variables encoding the data) given the target and the
environment belongs to general exponential family distributions. Based on this,
we show that it is possible to identify the data representation up to simple
transformations. We also prove that all direct causes of the target can be
fully discovered, which further enables us to obtain generalization guarantees
in the nonlinear setting. Extensive experiments on both synthetic and
real-world datasets show that our approach outperforms a variety of baseline
methods. Finally, in the discussion, we further explore the aforementioned
assumption and propose a more general hypothesis, called the Agnostic
Hypothesis: there exist a set of hidden causal factors affecting both inputs
and outcomes. The Agnostic Hypothesis can provide a unifying view of machine
learning. More importantly, it can inspire a new direction to explore a general
theory for identifying hidden causal factors, which is key to enabling the OOD
generalization guarantees.

    

### [[2103.00589] Learning Symbolic Operators for Task and Motion Planning](http://arxiv.org/abs/2103.00589)


  Robotic planning problems in hybrid state and action spaces can be solved by
integrated task and motion planners (TAMP) that handle the complex interaction
between motion-level decisions and task-level plan feasibility. TAMP approaches
rely on domain-specific symbolic operators to guide the task-level search,
making planning efficient. In this work, we formalize and study the problem of
operator learning for TAMP. Central to this study is the view that operators
define a lossy abstraction of the transition model of a domain. We then propose
a bottom-up relational learning method for operator learning and show how the
learned operators can be used for planning in a TAMP system. Experimentally, we
provide results in three domains, including long-horizon robotic planning
tasks. We find our approach to substantially outperform several baselines,
including three graph neural network-based model-free approaches from the
recent literature. Video: this https URL Code:
this https URL


### [[2103.01600] Missing Value Imputation on Multidimensional Time Series](http://arxiv.org/abs/2103.01600)


  We present DeepMVI, a deep learning method for missing value imputation in
multidimensional time-series datasets. Missing values are commonplace in
decision support platforms that aggregate data over long time stretches from
disparate sources, and reliable data analytics calls for careful handling of
missing data. One strategy is imputing the missing values, and a wide variety
of algorithms exist spanning simple interpolation, matrix factorization methods
like SVD, statistical models like Kalman filters, and recent deep learning
methods. We show that often these provide worse results on aggregate analytics
compared to just excluding the missing data. DeepMVI uses a neural network to
combine fine-grained and coarse-grained patterns along a time series, and
trends from related series across categorical dimensions. After failing with
off-the-shelf neural architectures, we design our own network that includes a
temporal transformer with a novel convolutional window feature, and kernel
regression with learned embeddings. The parameters and their training are
designed carefully to generalize across different placements of missing blocks
and data characteristics. Experiments across nine real datasets, four different
missing scenarios, comparing seven existing methods show that DeepMVI is
significantly more accurate, reducing error by more than 50% in more than half
the cases, compared to the best existing method. Although slower than simpler
matrix factorization methods, we justify the increased time overheads by
showing that DeepMVI is the only option that provided overall more accurate
analytics than dropping missing values.

    

### [[2104.10785] Accurate and fast matrix factorization for low-rank learning](http://arxiv.org/abs/2104.10785)


  In this paper we tackle two important challenges related to the accurate
partial singular value decomposition (SVD) and numerical rank estimation of a
huge matrix to use in low-rank learning problems in a fast way. We use the
concepts of Krylov subspaces such as the Golub-Kahan bidiagonalization process
as well as Ritz vectors to achieve these goals. Our experiments identify
various advantages of the proposed methods compared to traditional and
randomized SVD (R-SVD) methods with respect to the accuracy of the singular
values and corresponding singular vectors computed in a similar execution time.
The proposed methods are appropriate for applications involving huge matrices
where accuracy in all spectrum of the desired singular values, and also all of
corresponding singular vectors is essential. We evaluate our method in the real
application of Riemannian similarity learning (RSL) between two various image
datasets of MNIST and USPS.

    

### [[2104.13907] Seeing All the Angles: Learning Multiview Manipulation Policies for Contact-Rich Tasks from Demonstrations](http://arxiv.org/abs/2104.13907)


  Learned visuomotor policies have shown considerable success as an alternative
to traditional, hand-crafted frameworks for robotic manipulation. Surprisingly,
an extension of these methods to the multiview domain is relatively unexplored.
A successful multiview policy could be deployed on a mobile manipulation
platform, allowing the robot to complete a task regardless of its view of the
scene. In this work, we demonstrate that a multiview policy can be found
through imitation learning by collecting data from a variety of viewpoints. We
illustrate the general applicability of the method by learning to complete
several challenging multi-stage and contact-rich tasks, from numerous
viewpoints, both in a simulated environment and on a real mobile manipulation
platform. Furthermore, we analyze our policies to determine the benefits of
learning from multiview data compared to learning with data collected from a
fixed perspective. We show that learning from multiview data results in little,
if any, penalty to performance for a fixed-view task compared to learning with
an equivalent amount of fixed-view data. Finally, we examine the visual
features learned by the multiview and fixed-view policies. Our results indicate
that multiview policies implicitly learn to identify spatially correlated
features.

    

### [[2105.05682] Multi-Scale Contrastive Siamese Networks for Self-Supervised Graph Representation Learning](http://arxiv.org/abs/2105.05682)


  Graph representation learning plays a vital role in processing
graph-structured data. However, prior arts on graph representation learning
heavily rely on labeling information. To overcome this problem, inspired by the
recent success of graph contrastive learning and Siamese networks in visual
representation learning, we propose a novel self-supervised approach in this
paper to learn node representations by enhancing Siamese self-distillation with
multi-scale contrastive learning. Specifically, we first generate two augmented
views from the input graph based on local and global perspectives. Then, we
employ two objectives called cross-view and cross-network contrastiveness to
maximize the agreement between node representations across different views and
networks. To demonstrate the effectiveness of our approach, we perform
empirical experiments on five real-world datasets. Our method not only achieves
new state-of-the-art results but also surpasses some semi-supervised
counterparts by large margins. Code is made available at
this https URL


### [[2105.14706] The Role of Entropy in Guiding a Connection Prover](http://arxiv.org/abs/2105.14706)


  In this work we study how to learn good algorithms for selecting reasoning
steps in theorem proving. We explore this in the connection tableau calculus
implemented by leanCoP where the partial tableau provides a clean and compact
notion of a state to which a limited number of inferences can be applied. We
start by incorporating a state-of-the-art learning algorithm -- a graph neural
network (GNN) -- into the plCoP theorem prover. Then we use it to observe the
system's behaviour in a reinforcement learning setting, i.e., when learning
inference guidance from successful Monte-Carlo tree searches on many problems.
Despite its better pattern matching capability, the GNN initially performs
worse than a simpler previously used learning algorithm. We observe that the
simpler algorithm is less confident, i.e., its recommendations have higher
entropy. This leads us to explore how the entropy of the inference selection
implemented via the neural network influences the proof search. This is related
to research in human decision-making under uncertainty, and in particular the
probability matching theory. Our main result shows that a proper entropy
regularisation, i.e., training the GNN not to be overconfident, greatly
improves plCoP's performance on a large mathematical corpus.

    

### [[2106.00628] Detection of preventable fetal distress during labor from scanned cardiotocogram tracings using deep learning](http://arxiv.org/abs/2106.00628)


  Despite broad application during labor and delivery, there remains
considerable debate about the value of electronic fetal monitoring (EFM). EFM
includes the surveillance of the fetal heart rate (FHR) patterns in conjunction
with the maternal uterine contractions providing a wealth of data about fetal
behavior and the threat of diminished oxygenation and perfusion. Adverse
outcomes universally associate a fetal injury with the failure to timely
respond to FHR pattern information. Historically, the EFM data, stored
digitally, are available only as rasterized pdf images for contemporary or
historical discussion and examination. In reality, however, they are rarely
reviewed systematically. Using a unique archive of EFM collected over 50 years
of practice in conjunction with adverse outcomes, we present a deep learning
framework for training and detection of incipient or past fetal injury. We
report 94% accuracy in identifying early, preventable fetal injury intrapartum.
This framework is suited for automating an early warning and decision support
system for maintaining fetal well-being during the stresses of labor.
Ultimately, such a system could enable a physician to timely respond during
labor and prevent adverse outcomes. When adverse outcomes cannot be avoided,
they can provide guidance to the early neuroprotective treatment of the
newborn.

    

### [[2106.02522] Price graphs: Utilizing the structural information of financial time series for stock prediction](http://arxiv.org/abs/2106.02522)


  Great research efforts have been devoted to exploiting deep neural networks
in stock prediction. While long-range dependencies and chaotic property are
still two major issues that lower the performance of state-of-the-art deep
learning models in forecasting future price trends. In this study, we propose a
novel framework to address both issues. Specifically, in terms of transforming
time series into complex networks, we convert market price series into graphs.
Then, structural information, referring to associations among temporal points
and the node weights, is extracted from the mapped graphs to resolve the
problems regarding long-range dependencies and the chaotic property. We take
graph embeddings to represent the associations among temporal points as the
prediction model inputs. Node weights are used as a priori knowledge to enhance
the learning of temporal attention. The effectiveness of our proposed framework
is validated using real-world stock data, and our approach obtains the best
performance among several state-of-the-art benchmarks. Moreover, in the
conducted trading simulations, our framework further obtains the highest
cumulative profits. Our results supplement the existing applications of complex
network methods in the financial realm and provide insightful implications for
investment applications regarding decision support in financial markets.

    

### [[2106.03686] Deep Unfolding of Iteratively Reweighted ADMM for Wireless RF Sensing](http://arxiv.org/abs/2106.03686)


  We address the detection of material defects, which are inside a layered
material structure using compressive sensing based multiple-input and
multiple-output (MIMO) wireless radar. Here, the strong clutter due to the
reflection of the layered structure's surface often makes the detection of the
defects challenging. Thus, sophisticated signal separation methods are required
for improved defect detection. In many scenarios, the number of defects that we
are interested in is limited and the signaling response of the layered
structure can be modeled as a low-rank structure. Therefore, we propose joint
rank and sparsity minimization for defect detection. In particular, we propose
a non-convex approach based on the iteratively reweighted nuclear and
$\ell_1-$norm (a double-reweighted approach) to obtain a higher accuracy
compared to the conventional nuclear norm and $\ell_1-$norm minimization. To
this end, an iterative algorithm is designed to estimate the low-rank and
sparse contributions. Further, we propose deep learning to learn the parameters
of the algorithm (i.e., algorithm unfolding) to improve the accuracy and the
speed of convergence of the algorithm. Our numerical results show that the
proposed approach outperforms the conventional approaches in terms of mean
square errors of the recovered low-rank and sparse components and the speed of
convergence.

    

### [[2106.11026] A data-based comparative review and AI-driven symbolic model for longitudinal dispersion coefficient in natural streams](http://arxiv.org/abs/2106.11026)


  A better understanding of dispersion in natural streams requires knowledge of
longitudinal dispersion coefficient(LDC). Various methods have been proposed
for predictions of LDC. Those studies can be grouped into three types:
analytical, statistical and ML-driven researches(Implicit and explicit).
However, a comprehensive evaluation of them is still lacking. In this paper, we
first present an in-depth analysis of those methods and find out their defects.
This is carried out on an extensive database composed of 660 samples of
hydraulic and channel properties worldwide. The reliability and
representativeness of utilized data are enhanced through the deployment of the
Subset Selection of Maximum Dissimilarity(SSMD) for testing set selection and
the Inter Quartile Range(IQR) for removal of the outlier. The evaluation
reveals the rank of those methods as: ML-driven method > the statistical method
> the analytical method. Whereas implicit ML-driven methods are black-boxes in
nature, explicit ML-driven methods have more potential in prediction of LDC.
Besides, overfitting is a universal problem in existing models. Those models
also suffer from a fixed parameter combination. To establish an interpretable
model for LDC prediction with higher performance, we then design a novel
symbolic regression method called evolutionary symbolic regression
network(ESRN). It is a combination of genetic algorithms and neural networks.
Strategies are introduced to avoid overfitting and explore more parameter
combinations. Results show that the ESRN model has superiorities over other
existing symbolic models in performance. The proposed model is suitable for
practical engineering problems due to its advantage in low requirement of
parameters (only w and U* are required). It can provide convincing solutions
for situations where the field test cannot be carried out or limited field
information can be obtained.

    

### [[2106.13805] Self-training Converts Weak Learners to Strong Learners in Mixture Models](http://arxiv.org/abs/2106.13805)


  We consider a binary classification problem when the data comes from a
mixture of two isotropic distributions satisfying concentration and
anti-concentration properties enjoyed by log-concave distributions among
others. We show that there exists a universal constant $C_{\mathrm{err}}>0$
such that if a pseudolabeler $\boldsymbol{\beta}_{\mathrm{pl}}$ can achieve
classification error at most $C_{\mathrm{err}}$, then for any $\varepsilon>0$,
an iterative self-training algorithm initialized at $\boldsymbol{\beta}_0 :=
\boldsymbol{\beta}_{\mathrm{pl}}$ using pseudolabels $\hat y =
\mathrm{sgn}(\langle \boldsymbol{\beta}_t, \mathbf{x}\rangle)$ and using at
most $\tilde O(d/\varepsilon^2)$ unlabeled examples suffices to learn the
Bayes-optimal classifier up to $\varepsilon$ error, where $d$ is the ambient
dimension. That is, self-training converts weak learners to strong learners
using only unlabeled examples. We additionally show that by running gradient
descent on the logistic loss one can obtain a pseudolabeler
$\boldsymbol{\beta}_{\mathrm{pl}}$ with classification error $C_{\mathrm{err}}$
using only $O(d)$ labeled examples (i.e., independent of $\varepsilon$).
Together our results imply that mixture models can be learned to within
$\varepsilon$ of the Bayes-optimal accuracy using at most $O(d)$ labeled
examples and $\tilde O(d/\varepsilon^2)$ unlabeled examples by way of a
semi-supervised self-training algorithm.

    

### [[2107.05445] Disentangling Transfer and Interference in Multi-Domain Learning](http://arxiv.org/abs/2107.05445)


  Humans are incredibly good at transferring knowledge from one domain to
another, enabling rapid learning of new tasks. Likewise, transfer learning has
enabled enormous success in many computer vision problems using pretraining.
However, the benefits of transfer in multi-domain learning, where a network
learns multiple tasks defined by different datasets, has not been adequately
studied. Learning multiple domains could be beneficial or these domains could
interfere with each other given limited network capacity. In this work, we
decipher the conditions where interference and knowledge transfer occur in
multi-domain learning. We propose new metrics disentangling interference and
transfer and set up experimental protocols. We further examine the roles of
network capacity, task grouping, and dynamic loss weighting in reducing
interference and facilitating transfer. We demonstrate our findings on the
CIFAR-100, MiniPlaces, and Tiny-ImageNet datasets.

    

### [[2107.07703] Estimation from Partially Sampled Distributed Traces](http://arxiv.org/abs/2107.07703)


  Sampling is often a necessary evil to reduce the processing and storage costs
of distributed tracing. In this work, we describe a scalable and adaptive
sampling approach that can preserve events of interest better than the widely
used head-based sampling approach. Sampling rates can be chosen individually
and independently for every span, allowing to take span attributes and local
resource constraints into account. The resulting traces are often only
partially and not completely sampled which complicates statistical analysis. To
exploit the given information, an unbiased estimation algorithm is presented.
Even though it does not need to know whether the traces are complete, it
reduces the estimation error in many cases compared to considering only
complete traces.

    

### [[2107.07809] A method for decompilation of AMD GCN kernels to OpenCL](http://arxiv.org/abs/2107.07809)


  Introduction: Decompilers are useful tools for software analysis and support
in the absence of source code. They are available for many hardware
architectures and programming languages. However, none of the existing
decompilers support modern AMD GPU architectures such as AMD GCN and RDNA.
Purpose: We aim at developing the first assembly decompiler tool for a modern
AMD GPU architecture that generates code in the OpenCL language, which is
widely used for programming GPGPUs. Results: We developed the algorithms for
the following operations: preprocessing assembly code, searching data accesses,
extracting system values, decompiling arithmetic operations and recovering data
types. We also developed templates for decompilation of branching operations.
Practical relevance: We implemented the presented algorithms in Python as a
tool called OpenCLDecompiler, which supports a large subset of AMD GCN
instructions. This tool automatically converts disassembled GPGPU code into the
equivalent OpenCL code, which reduces the effort required to analyze assembly
code.

    

### [[2107.07866] MD Simulation of Hundred-Billion-Metal-Atom Cascade Collision on Sunway Taihulight](http://arxiv.org/abs/2107.07866)


  Radiation damage to the steel material of reactor pressure vessels is a major
threat to the nuclear reactor safety. It is caused by the metal atom cascade
collision, initialized when the atoms are struck by a high-energy neutron. The
paper presents MISA-MD, a new implementation of molecular dynamics, to simulate
such cascade collision with EAM potential. MISA-MD realizes (1) a hash-based
data structure to efficiently store an atom and find its neighbors, and (2)
several acceleration and optimization strategies based on SW26010 processor of
Sunway Taihulight supercomputer, including an efficient potential table storage
and interpolation method, a coloring method to avoid write conflicts, and
double-buffer and data reuse strategies. The experimental results demonstrated
that MISA-MD has good accuracy and scalability, and obtains a parallel
efficiency of over 79% in an 655-billion-atom system. Compared with a
state-of-the-art MD program LAMMPS, MISA-MD requires less memory usage and
achieves better computational performance.

    

### [[2107.07972] Demo -- Zelig: Customizable Blockchain Simulator](http://arxiv.org/abs/2107.07972)


  As blockchain-based systems see wider adoption, it becomes increasingly
critical to ensure their reliability, security, and efficiency. Running
simulations is an effective method of gaining insights on the existing systems
and analyzing potential improvements. However, many of the existing blockchain
simulators have various shortcomings that yield them insufficient for a wide
range of scenarios. In this demo paper, we present Zelig: our blockchain
simulator designed with the main goals of customizability and extensibility. To
the best of our knowledge, Zelig is the only blockchain simulator that enables
simulating custom network topologies without modifying the simulator code. We
explain our simulator design, validate via experimental analysis against the
real-world Bitcoin network, and highlight potential use cases.

    

### [[2002.10047] Parallel Clique Counting and Peeling Algorithms](http://arxiv.org/abs/2002.10047)


  We present a new parallel algorithm for $k$-clique counting/listing that has
polylogarithmic span (parallel time) and is work-efficient (matches the work of
the best sequential algorithm) for sparse graphs. Our algorithm is based on
computing low out-degree orientations, which we present new linear-work and
polylogarithmic-span algorithms for computing in parallel. We also present new
parallel algorithms for producing unbiased estimations of clique counts using
graph sparsification. Finally, we design two new parallel work-efficient
algorithms for approximating the $k$-clique densest subgraph, the first of
which is a $1/k$-approximation and the second of which is a
$1/(k(1+\epsilon))$-approximation and has polylogarithmic span. Our first
algorithm does not have polylogarithmic span, but we prove that it solves a
P-complete problem.
In addition to the theoretical results, we also implement the algorithms and
propose various optimizations to improve their practical performance. On a
30-core machine with two-way hyper-threading, our algorithms achieve
13.23--38.99x and 1.19--13.76x self-relative parallel speedup for $k$-clique
counting and $k$-clique densest subgraph, respectively. Compared to the
state-of-the-art parallel $k$-clique counting algorithms, we achieve up to
9.88x speedup, and compared to existing implementations of $k$-clique densest
subgraph, we achieve up to 11.83x speedup. We are able to compute the
$4$-clique counts on the largest publicly-available graph with over two hundred
billion edges for the first time.

    

### [[2106.12485] Particle-In-Cell Simulation using Asynchronous Tasking](http://arxiv.org/abs/2106.12485)


  Recently, task-based programming models have emerged as a prominent
alternative among shared-memory parallel programming paradigms. Inherently
asynchronous, these models provide native support for dynamic load balancing
and incorporate data flow concepts to selectively synchronize the tasks.
However, tasking models are yet to be widely adopted by the HPC community and
their effective advantages when applied to non-trivial, real-world HPC
applications are still not well comprehended. In this paper, we study the
parallelization of a production electromagnetic particle-in-cell (EM-PIC) code
for kinetic plasma simulations exploring different strategies using
asynchronous task-based models. Our fully asynchronous implementation not only
significantly outperforms a conventional, synchronous approach but also
achieves near perfect scaling for 48 cores.

    

### [[2107.07566] Internet-Augmented Dialogue Generation](http://arxiv.org/abs/2107.07566)


  The largest store of continually updating knowledge on our planet can be
accessed via internet search. In this work we study giving access to this
information to conversational agents. Large language models, even though they
store an impressive amount of knowledge within their weights, are known to
hallucinate facts when generating dialogue (Shuster et al., 2021); moreover,
those facts are frozen in time at the point of model training. In contrast, we
propose an approach that learns to generate an internet search query based on
the context, and then conditions on the search results to finally generate a
response, a method that can employ up-to-the-minute relevant information. We
train and evaluate such models on a newly collected dataset of human-human
conversations whereby one of the speakers is given access to internet search
during knowledgedriven discussions in order to ground their responses. We find
that search-query based access of the internet in conversation provides
superior performance compared to existing approaches that either use no
augmentation or FAISS-based retrieval (Lewis et al., 2020).

    

### [[2107.07567] Beyond Goldfish Memory: Long-Term Open-Domain Conversation](http://arxiv.org/abs/2107.07567)


  Despite recent improvements in open-domain dialogue models, state of the art
models are trained and evaluated on short conversations with little context. In
contrast, the long-term conversation setting has hardly been studied. In this
work we collect and release a human-human dataset consisting of multiple chat
sessions whereby the speaking partners learn about each other's interests and
discuss the things they have learnt from past sessions. We show how existing
models trained on existing datasets perform poorly in this long-term
conversation setting in both automatic and human evaluations, and we study
long-context models that can perform much better. In particular, we find
retrieval-augmented methods and methods with an ability to summarize and recall
previous conversations outperform the standard encoder-decoder architectures
currently considered state of the art.

    

### [[2107.07578] Real-Time Violence Detection Using CNN-LSTM](http://arxiv.org/abs/2107.07578)


  Violence rates however have been brought down about 57% during the span of
the past 4 decades yet it doesn't change the way that the demonstration of
violence actually happens, unseen by the law. Violence can be mass controlled
sometimes by higher authorities, however, to hold everything in line one must
"Microgovern" over each movement occurring in every road of each square. To
address the butterfly effects impact in our setting, I made a unique model and
a theorized system to handle the issue utilizing deep learning. The model takes
the input of the CCTV video feeds and after drawing inference, recognizes if a
violent movement is going on. And hypothesized architecture aims towards
probability-driven computation of video feeds and reduces overhead from naively
computing for every CCTV video feeds.

    

### [[2107.07596] Depth Estimation from Monocular Images and Sparse radar using Deep Ordinal Regression Network](http://arxiv.org/abs/2107.07596)


  We integrate sparse radar data into a monocular depth estimation model and
introduce a novel preprocessing method for reducing the sparseness and limited
field of view provided by radar. We explore the intrinsic error of different
radar modalities and show our proposed method results in more data points with
reduced error. We further propose a novel method for estimating dense depth
maps from monocular 2D images and sparse radar measurements using deep learning
based on the deep ordinal regression network by Fu et al. Radar data are
integrated by first converting the sparse 2D points to a height-extended 3D
measurement and then including it into the network using a late fusion
approach. Experiments are conducted on the nuScenes dataset. Our experiments
demonstrate state-of-the-art performance in both day and night scenes.

    

### [[2107.07630] Evaluation of Human-AI Teams for Learned and Rule-Based Agents in Hanabi](http://arxiv.org/abs/2107.07630)


  Deep reinforcement learning has generated superhuman AI in competitive games
such as Go and StarCraft. Can similar learning techniques create a superior AI
teammate for human-machine collaborative games? Will humans prefer AI teammates
that improve objective team performance or those that improve subjective
metrics of trust? In this study, we perform a single-blind evaluation of teams
of humans and AI agents in the cooperative card game \emph{Hanabi}, with both
rule-based and learning-based agents. In addition to the game score, used as an
objective metric of the human-AI team performance, we also quantify subjective
measures of the human's perceived performance, teamwork, interpretability,
trust, and overall preference of AI teammate. We find that humans have a clear
preference toward a rule-based AI teammate (SmartBot) over a state-of-the-art
learning-based AI teammate (Other-Play) across nearly all subjective metrics,
and generally view the learning-based agent negatively, despite no statistical
difference in the game score. This result has implications for future AI design
and reinforcement learning benchmarking, highlighting the need to incorporate
subjective metrics of human-AI teaming rather than a singular focus on
objective task performance.

    

### [[2107.07651] Align before Fuse: Vision and Language Representation Learning with Momentum Distillation](http://arxiv.org/abs/2107.07651)


  Large-scale vision and language representation learning has shown promising
improvements on various vision-language tasks. Most existing methods employ a
transformer-based multimodal encoder to jointly model visual tokens
(region-based image features) and word tokens. Because the visual tokens and
word tokens are unaligned, it is challenging for the multimodal encoder to
learn image-text interactions. In this paper, we introduce a contrastive loss
to ALign the image and text representations BEfore Fusing (ALBEF) them through
cross-modal attention, which enables more grounded vision and language
representation learning. Unlike most existing methods, our method does not
require bounding box annotations nor high-resolution images. In order to
improve learning from noisy web data, we propose momentum distillation, a
self-training method which learns from pseudo-targets produced by a momentum
model. We provide a theoretical analysis of ALBEF from a mutual information
maximization perspective, showing that different training tasks can be
interpreted as different ways to generate views for an image-text pair. ALBEF
achieves state-of-the-art performance on multiple downstream vision-language
tasks. On image-text retrieval, ALBEF outperforms methods that are pre-trained
on orders of magnitude larger datasets. On VQA and NLVR$^2$, ALBEF achieves
absolute improvements of 2.37% and 3.84% compared to the state-of-the-art,
while enjoying faster inference speed. Code and pre-trained models are
available at this https URL.

    

### [[2107.07653] TAPEX: Table Pre-training via Learning a Neural SQL Executor](http://arxiv.org/abs/2107.07653)


  Recent years pre-trained language models hit a success on modeling natural
language sentences and (semi-)structured tables. However, existing table
pre-training techniques always suffer from low data quality and low
pre-training efficiency. In this paper, we show that table pre-training can be
realized by learning a neural SQL executor over a synthetic corpus, which is
obtained by automatically synthesizing executable SQL queries. By pre-training
on the synthetic corpus, our approach TAPEX dramatically improves the
performance on downstream tasks, boosting existing language models by at most
19.5%. Meanwhile, TAPEX has remarkably high pre-training efficiency and yields
strong results when using a small pre-trained corpus. Experimental results
demonstrate that TAPEX outperforms previous table pre-training approaches by a
large margin, and our model achieves new state-of-the-art results on four
well-known datasets, including improving the WikiSQL denotation accuracy to
89.6% (+4.9%), the WikiTableQuestions denotation accuracy to 57.5% (+4.8%), the
SQA denotation accuracy to 74.5% (+3.5%), and the TabFact accuracy to 84.6%
(+3.6%). Our work opens the way to reason over structured data by pre-training
on synthetic executable programs.

    

### [[2107.07693] Imitate TheWorld: A Search Engine Simulation Platform](http://arxiv.org/abs/2107.07693)


  Recent E-commerce applications benefit from the growth of deep learning
techniques. However, we notice that many works attempt to maximize business
objectives by closely matching offline labels which follow the supervised
learning paradigm. This results in models obtain high offline performance in
terms of Area Under Curve (AUC) and Normalized Discounted Cumulative Gain
(NDCG), but cannot consistently increase the revenue metrics such as purchases
amount of users. Towards the issues, we build a simulated search engine AESim
that can properly give feedback by a well-trained discriminator for generated
pages, as a dynamic dataset. Different from previous simulation platforms which
lose connection with the real world, ours depends on the real data in
AliExpress Search: we use adversarial learning to generate virtual users and
use Generative Adversarial Imitation Learning (GAIL) to capture behavior
patterns of users. Our experiments also show AESim can better reflect the
online performance of ranking models than classic ranking metrics, implying
AESim can play a surrogate of AliExpress Search and evaluate models without
going online.

    

### [[2107.07769] Architecture of Automated Crypto-Finance Agent](http://arxiv.org/abs/2107.07769)


  We present the cognitive architecture of an autonomous agent for active
portfolio management in decentralized finance, involving activities such as
asset selection, portfolio balancing, liquidity provision, and trading. Partial
implementation of the architecture is provided and supplied with preliminary
results and conclusions.

    

### [[2107.07771] Know Deeper: Knowledge-Conversation Cyclic Utilization Mechanism for Open-domain Dialogue Generation](http://arxiv.org/abs/2107.07771)


  End-to-End intelligent neural dialogue systems suffer from the problems of
generating inconsistent and repetitive responses. Existing dialogue models pay
attention to unilaterally incorporating personal knowledge into the dialog
while ignoring the fact that incorporating the personality-related conversation
information into personal knowledge taken as the bilateral information flow
boosts the quality of the subsequent conversation. Besides, it is indispensable
to control personal knowledge utilization over the conversation level. In this
paper, we propose a conversation-adaption multi-view persona aware response
generation model that aims at enhancing conversation consistency and
alleviating the repetition from two folds. First, we consider conversation
consistency from multiple views. From the view of the persona profile, we
design a novel interaction module that not only iteratively incorporates
personalized knowledge into each turn conversation but also captures the
personality-related information from conversation to enhance personalized
knowledge semantic representation. From the view of speaking style, we
introduce the speaking style vector and feed it into the decoder to keep the
speaking style consistency. To avoid conversation repetition, we devise a
coverage mechanism to keep track of the activation of personal knowledge
utilization. Experiments on both automatic and human evaluation verify the
superiority of our model over previous models.

    

### [[2107.07842] A Survey of Knowledge Graph Embedding and Their Applications](http://arxiv.org/abs/2107.07842)


  Knowledge Graph embedding provides a versatile technique for representing
knowledge. These techniques can be used in a variety of applications such as
completion of knowledge graph to predict missing information, recommender
systems, question answering, query expansion, etc. The information embedded in
Knowledge graph though being structured is challenging to consume in a
real-world application. Knowledge graph embedding enables the real-world
application to consume information to improve performance. Knowledge graph
embedding is an active research area. Most of the embedding methods focus on
structure-based information. Recent research has extended the boundary to
include text-based information and image-based information in entity embedding.
Efforts have been made to enhance the representation with context information.
This paper introduces growth in the field of KG embedding from simple
translation-based models to enrichment-based models. This paper includes the
utility of the Knowledge graph in real-world applications.

    

### [[2107.07843] Deep Learning Based Hybrid Precoding in Dual-Band Communication Systems](http://arxiv.org/abs/2107.07843)


  We propose a deep learning-based method that uses spatial and temporal
information extracted from the sub-6GHz band to predict/track beams in the
millimeter-wave (mmWave) band. In more detail, we consider a dual-band
communication system operating in both the sub-6GHz and mmWave bands. The
objective is to maximize the achievable mutual information in the mmWave band
with a hybrid analog/digital architecture where analog precoders (RF precoders)
are taken from a finite codebook. Finding a RF precoder using conventional
search methods incurs large signalling overhead, and the signalling scales with
the number of RF chains and the resolution of the phase shifters. To overcome
the issue of large signalling overhead in the mmWave band, the proposed method
exploits the spatiotemporal correlation between sub-6GHz and mmWave bands, and
it predicts/tracks the RF precoders in the mmWave band from sub-6GHz channel
measurements. The proposed method provides a smaller candidate set so that
performing a search over that set significantly reduces the signalling overhead
compared with conventional search heuristics. Simulations show that the
proposed method can provide reasonable achievable rates while significantly
reducing the signalling overhead.

    

### [[2107.07846] Deep Learning Beam Optimization in Millimeter-Wave Communication Systems](http://arxiv.org/abs/2107.07846)


  We propose a method that combines fixed point algorithms with a neural
network to optimize jointly discrete and continuous variables in
millimeter-wave communication systems, so that the users' rates are allocated
fairly in a well-defined sense. In more detail, the discrete variables include
user-access point assignments and the beam configurations, while the continuous
variables refer to the power allocation. The beam configuration is predicted
from user-related information using a neural network. Given the predicted beam
configuration, a fixed point algorithm allocates power and assigns users to
access points so that the users achieve the maximum fraction of their
interference-free rates. The proposed method predicts the beam configuration in
a "one-shot" manner, which significantly reduces the complexity of the beam
search procedure. Moreover, even if the predicted beam configurations are not
optimal, the fixed point algorithm still provides the optimal power allocation
and user-access point assignments for the given beam configuration.

    

### [[2107.07905] Unsupervised Discovery of Object Radiance Fields](http://arxiv.org/abs/2107.07905)


  We study the problem of inferring an object-centric scene representation from
a single image, aiming to derive a representation that explains the image
formation process, captures the scene's 3D nature, and is learned without
supervision. Most existing methods on scene decomposition lack one or more of
these characteristics, due to the fundamental challenge in integrating the
complex 3D-to-2D image formation process into powerful inference schemes like
deep networks. In this paper, we propose unsupervised discovery of Object
Radiance Fields (uORF), integrating recent progresses in neural 3D scene
representations and rendering with deep inference networks for unsupervised 3D
scene decomposition. Trained on multi-view RGB images without annotations, uORF
learns to decompose complex scenes with diverse, textured background from a
single image. We show that uORF performs well on unsupervised 3D scene
segmentation, novel view synthesis, and scene editing on three datasets.

    

### [[2107.07957] Automatic Task Requirements Writing Evaluation via Machine Reading Comprehension](http://arxiv.org/abs/2107.07957)


  Task requirements (TRs) writing is an important question type in Key English
Test and Preliminary English Test. A TR writing question may include multiple
requirements and a high-quality essay must respond to each requirement
thoroughly and accurately. However, the limited teacher resources prevent
students from getting detailed grading instantly. The majority of existing
automatic essay scoring systems focus on giving a holistic score but rarely
provide reasons to support it. In this paper, we proposed an end-to-end
framework based on machine reading comprehension (MRC) to address this problem
to some extent. The framework not only detects whether an essay responds to a
requirement question, but clearly marks where the essay answers the question.
Our framework consists of three modules: question normalization module, ELECTRA
based MRC module and response locating module. We extensively explore
state-of-the-art MRC methods. Our approach achieves 0.93 accuracy score and
0.85 F1 score on a real-world educational dataset. To encourage reproducible
results, we make our code publicly available at
\url{this https URL}.

    

### [[2107.07958] Temporal-aware Language Representation Learning From Crowdsourced Labels](http://arxiv.org/abs/2107.07958)


  Learning effective language representations from crowdsourced labels is
crucial for many real-world machine learning tasks. A challenging aspect of
this problem is that the quality of crowdsourced labels suffer high intra- and
inter-observer variability. Since the high-capacity deep neural networks can
easily memorize all disagreements among crowdsourced labels, directly applying
existing supervised language representation learning algorithms may yield
suboptimal solutions. In this paper, we propose \emph{TACMA}, a
\underline{t}emporal-\underline{a}ware language representation learning
heuristic for \underline{c}rowdsourced labels with \underline{m}ultiple
\underline{a}nnotators. The proposed approach (1) explicitly models the
intra-observer variability with attention mechanism; (2) computes and
aggregates per-sample confidence scores from multiple workers to address the
inter-observer disagreements. The proposed heuristic is extremely easy to
implement in around 5 lines of code. The proposed heuristic is evaluated on
four synthetic and four real-world data sets. The results show that our
approach outperforms a wide range of state-of-the-art baselines in terms of
prediction accuracy and AUC. To encourage the reproducible results, we make our
code publicly available at \url{this https URL}.

    

### [[2107.07961] MODRL/D-EL: Multiobjective Deep Reinforcement Learning with Evolutionary Learning for Multiobjective Optimization](http://arxiv.org/abs/2107.07961)


  Learning-based heuristics for solving combinatorial optimization problems has
recently attracted much academic attention. While most of the existing works
only consider the single objective problem with simple constraints, many
real-world problems have the multiobjective perspective and contain a rich set
of constraints. This paper proposes a multiobjective deep reinforcement
learning with evolutionary learning algorithm for a typical complex problem
called the multiobjective vehicle routing problem with time windows (MO-VRPTW).
In the proposed algorithm, the decomposition strategy is applied to generate
subproblems for a set of attention models. The comprehensive context
information is introduced to further enhance the attention models. The
evolutionary learning is also employed to fine-tune the parameters of the
models. The experimental results on MO-VRPTW instances demonstrate the
superiority of the proposed algorithm over other learning-based and
iterative-based approaches.

    

### [[2107.07964] Blockchain Technology: Bitcoins, Cryptocurrency and Applications](http://arxiv.org/abs/2107.07964)


  Blockchain is a decentralized ledger used to securely exchange digital
currency, perform deals and transactions efficient manner, each user of the
network has access to the least copy of the encrypted ledger so that they can
validate a new transaction. The blockchain ledger is a collection of all
Bitcoin transactions executed in the past. Basically, it's distributed database
that maintains continuously growing tamper-proof data structure blocks that
holds batches of individual transactions. The completed blocks are added in a
linear and chronological order. Each block contains a timestamp and information
link which points to a previous block. Bitcoin is a peer-to-peer permissionless
network that allows every user to connect to the network and send new
transactions to verify and create new blocks. Satoshi Nakamoto described the
design of Bitcoin digital currency in his research paper posted to a
cryptography listserv 2008. Nakamoto's suggestion has solved the long-pending
problem of cryptography and laid the foundation stone for digital currency.
This paper explains the concept of bitcoin, its characteristics, the need for
Blockchain, and how Bitcoin works. It attempts to highlight the role of
Blockchain in shaping the future of banking , financial services, and the
adoption of the Internet of Thinks and future Technologies.

    

### [[2107.08030] A New Robust Multivariate Mode Estimator for Eye-tracking Calibration](http://arxiv.org/abs/2107.08030)


  We propose in this work a new method for estimating the main mode of
multivariate distributions, with application to eye-tracking calibrations. When
performing eye-tracking experiments with poorly cooperative subjects, such as
infants or monkeys, the calibration data generally suffer from high
contamination. Outliers are typically organized in clusters, corresponding to
the time intervals when subjects were not looking at the calibration points. In
this type of multimodal distributions, most central tendency measures fail at
estimating the principal fixation coordinates (the first mode), resulting in
errors and inaccuracies when mapping the gaze to the screen coordinates. Here,
we developed a new algorithm to identify the first mode of multivariate
distributions, named BRIL, which rely on recursive depth-based filtering. This
novel approach was tested on artificial mixtures of Gaussian and Uniform
distributions, and compared to existing methods (conventional depth medians,
robust estimators of location and scatter, and clustering-based approaches). We
obtained outstanding performances, even for distributions containing very high
proportions of outliers, both grouped in clusters and randomly distributed.
Finally, we demonstrate the strength of our method in a real-world scenario
using experimental data from eye-tracking calibrations with Capuchin monkeys,
especially for distributions where other algorithms typically lack accuracy.

    

### [[2101.00675] Sentiment Analysis for Open Domain Conversational Agent](http://arxiv.org/abs/2101.00675)


  The applicability of common sentiment analysis models to open domain human
robot interaction is investigated within this paper. The models are used on a
dataset specific to user interaction with the Alana system (a Alexa prize
system) in order to determine which would be more appropriate for the task of
identifying sentiment when a user interacts with a non-human driven socialbot.
With the identification of a model, various improvements are attempted and
detailed prior to integration into the Alana system. The study showed that a
Random Forest Model with 25 trees trained on the dataset specific to user
interaction with the Alana system combined with the dataset present in NLTK
Vader outperforms other models. The new system (called 'Rob') matches it's
output utterance sentiment with the user's utterance sentiment. This method is
expected to improve user experience because it builds upon the overall
sentiment detection which makes it seem that new system sympathises with user
feelings. Furthermore, the results obtained from the user feedback confirms our
expectation.

    

### [[2105.14371] Fine-Tuning the Odds in Bayesian Networks](http://arxiv.org/abs/2105.14371)


  This paper proposes various new analysis techniques for Bayes networks in
which conditional probability tables (CPTs) may contain symbolic variables. The
key idea is to exploit scalable and powerful techniques for synthesis problems
in parametric Markov chains. Our techniques are applicable to arbitrarily many,
possibly dependent parameters that may occur in various CPTs. This lifts the
severe restrictions on parameters, e.g., by restricting the number of
parametrized CPTs to one or two, or by avoiding parameter dependencies between
several CPTs, in existing works for parametric Bayes networks (pBNs). We
describe how our techniques can be used for various pBN synthesis problems
studied in the literature such as computing sensitivity functions (and values),
simple and difference parameter tuning, ratio parameter tuning, and minimal
change tuning. Experiments on several benchmarks show that our prototypical
tool built on top of the probabilistic model checker Storm can handle several
hundreds of parameters.

    

### [[2106.08452] Deep Neural Networks for Approximating Stream Reasoning with C-SPARQL](http://arxiv.org/abs/2106.08452)


  The amount of information produced, whether by newspapers, blogs and social
networks, or by monitoring systems, is increasing rapidly. Processing all this
data in real-time, while taking into consideration advanced knowledge about the
problem domain, is challenging, but required in scenarios where assessing
potential risks in a timely fashion is critical. C-SPARQL, a language for
continuous queries over streams of RDF data, is one of the more prominent
approaches in stream reasoning that provides such continuous inference
capabilities over dynamic data that go beyond mere stream processing. However,
it has been shown that, in the presence of huge amounts of data, C-SPARQL may
not be able to answer queries in time, in particular when the frequency of
incoming data is higher than the time required for reasoning with that data. In
this paper, we investigate whether reasoning with C-SPARQL can be approximated
using Recurrent Neural Networks and Convolutional Neural Networks, two neural
network architectures that have been shown to be well-suited for time series
forecasting and time series classification, to leverage on their higher
processing speed once the network has been trained. We consider a variety of
different kinds of queries and obtain overall positive results with high
accuracies while improving processing time often by several orders of
magnitude.

    

### [[2107.07784] A Security Cost Modelling Framework for Cyber-Physical Systems](http://arxiv.org/abs/2107.07784)


  Cyber-Physical Systems (CPS) are formed through interconnected components
capable of computation, communication, sensing and changing the physical world.
The development of these systems poses a significant challenge since they have
to be designed in a way to ensure cyber-security without impacting their
performance. This article presents the Security Cost Modelling Framework (SCMF)
and shows supported by an experimental study how it can be used to measure,
normalise and aggregate the overall performance of a CPS. Unlike previous
studies, our approach uses different metrics to measure the overall performance
of a CPS and provides a methodology for normalising the measurement results of
different units to a common Cost Unit. Moreover, we show how the Security Costs
can be extracted from the overall performance measurements which allows to
quantify the overhead imposed by performing security-related tasks.
Furthermore, we describe the architecture of our experimental testbed and
demonstrate the applicability of SCMF in an experimental study. Our results
show that measuring the overall performance and extracting the security costs
using SCMF can serve as basis to redesign interactions to achieve the same
overall goal at less costs.

    

### [[2107.07664] SMLtoCoq: Automated Generation of Coq Specifications and Proof Obligations from SML Programs with Contracts](http://arxiv.org/abs/2107.07664)


  Formally reasoning about functional programs is supposed to be
straightforward and elegant, however, it is not typically done as a matter of
course. Reasoning in a proof assistant requires "reimplementing" the code in
those tools, which is far from trivial. SMLtoCoq provides an automatic
translation of SML programs and function contracts into Coq. Programs are
translated into Coq specifications, and function contracts into theorems, which
can then be formally proved. Using the Equations plugin and other well
established Coq libraries, SMLtoCoq is able to translate SML programs without
side-effects containing partial functions, structures, functors, records, among
others. Additionally, we provide a Coq version of many parts of SML's basis
library, so that calls to these libraries are kept almost as is.

    

### [[2107.07666] Adelfa: A System for Reasoning about LF Specifications](http://arxiv.org/abs/2107.07666)


  We present a system called Adelfa that provides mechanized support for
reasoning about specifications developed in the Edinburgh Logical Framework or
LF. Underlying Adelfa is a new logic named L_LF. Typing judgements in LF are
represented by atomic formulas in L_LF and quantification is permitted over
contexts and terms that appear in such formulas. Contexts, which constitute
type assignments to uniquely named variables that are modelled using the
technical device of nominal constants, are characterized in L_LF by context
schemas that describe their inductive structure. We present these formulas and
an associated semantics before sketching a proof system for constructing
arguments that are sound with respect to the semantics. We then outline the
realization of this proof system in Adelfa and illustrate its use through a few
example proof developments. We conclude the paper by relating Adelfa to
existing systems for reasoning about LF specifications.

    

### [[2107.07670] Touring the MetaCoq Project (Invited Paper)](http://arxiv.org/abs/2107.07670)


  Proof assistants are getting more widespread use in research and industry to
provide certified and independently checkable guarantees about theories,
designs, systems and implementations. However, proof assistant implementations
themselves are seldom verified, although they take a major share of the trusted
code base in any such certification effort. In this area, proof assistants
based on Higher-Order Logic enjoy stronger guarantees, as self-certified
implementations have been available for some years. One cause of this
difference is the inherent complexity of dependent type theories together with
their extensions with inductive types, universe polymorphism and complex sort
systems, and the gap between theory on paper and practical implementations in
efficient programming languages. MetaCoq is a collaborative project that aims
to tackle these difficulties to provide the first fully-certified realistic
implementation of a type checker for the full calculus underlying the Coq proof
assistant. To achieve this, we refined the sometimes blurry, if not incorrect,
specification and implementation of the system. We show how theoretical tools
from this community such as bidirectional type-checking,
Tait-Martin-Löf/Takahashi's confluence proof technique and monadic and
dependently-typed programming can help construct the following artefacts: a
specification of Coq's syntax and type theory, the Polymorphic Cumulative
Calculus of (Co)-Inductive Constructions (PCUIC); a monad for the manipulation
of raw syntax and interaction with the Coq system; a verification of PCUIC's
metatheory, whose main results are the confluence of reduction, type
preservation and principality of typing; a realistic, correct and complete
type-checker for PCUIC; a sound type and proof erasure procedure from PCUIC to
untyped lambda-calculus, i.e., the core of the extraction mechanism of Coq.

    

### [[2107.08038] Towards a Benchmark Set for Program Repair Based on Partial Fixes](http://arxiv.org/abs/2107.08038)


  Software bugs significantly contribute to software cost and increase the risk
of system malfunctioning. In recent years, many automated program-repair
approaches have been proposed to automatically fix undesired program behavior.
Despite of their great success, specific problems such as fixing bugs with
partial fixes still remain unresolved. A partial fix to a known software issue
is a programmer's failed attempt to fix the issue the first time. Even though
it fails, this fix attempt still conveys important information such as the
suspicious software region and the bug type. In this work we do not propose an
approach for program repair with partial fixes, but instead answer a
preliminary question: Do partial fixes occur often enough, in general, to be
relevant for the research area of automated program repair? We crawled 1500
open-source C repositories on GitHub for partial fixes. The result is a
benchmark set of 2204 benchmark tasks for automated program repair based on
partial fixes. The benchmark set is available open source and open to further
contributions and improvement.

    

### [<title>【内推】阿里云 云原生团队 2022 届秋招 - DockOne.io</title>](http://dockone.io/question/679178)