
## 2021-8-11

### [<title>[Cpp] Non-Usable interface of xgboost/predictor.h - XGBoost</title>](https://discuss.xgboost.ai/t/cpp-non-usable-interface-of-xgboost-predictor-h/2421/5)

### [<title>[Cpp] Non-Usable interface of xgboost/predictor.h - XGBoost</title>](https://discuss.xgboost.ai/t/cpp-non-usable-interface-of-xgboost-predictor-h/2421/4)

### [<title>[Cpp] Non-Usable interface of xgboost/predictor.h - XGBoost</title>](https://discuss.xgboost.ai/t/cpp-non-usable-interface-of-xgboost-predictor-h/2421/3)

### [<title>[Cpp] Non-Usable interface of xgboost/predictor.h - XGBoost</title>](https://discuss.xgboost.ai/t/cpp-non-usable-interface-of-xgboost-predictor-h/2421/2)

### [<title>[Cpp] Non-Usable interface of xgboost/predictor.h - XGBoost</title>](https://discuss.xgboost.ai/t/cpp-non-usable-interface-of-xgboost-predictor-h/2421/1)

### [<title>[Cpp] MSVC Build issues with runtime Library type MD - XGBoost</title>](https://discuss.xgboost.ai/t/cpp-msvc-build-issues-with-runtime-library-type-md/2420/1)

### [[2108.04560] Intent-driven Autonomous Network and Service Management in Future Networks: A Structured Literature Review](http://arxiv.org/abs/2108.04560)


  Intent driven networks are an essential stepping stone in the evolution of
network and service management towards a truly autonomous paradigm. User
centric intents provide an abstracted means of impacting the design,
provisioning, deployment and assurance of network infrastructure and services
with the help of service level agreements and minimum network capability
exposure. The concept of Intent-based networking (IBN) poses several challenges
in terms of the contextual definition of intents, role of different
stakeholders, and a generalized architecture. In this review, we provide a
comprehensive analysis of the state-of-the-art in IBN including the intent
description models, intent lifecycle management, significance of IBN and a
generalized architectural framework along with challenges and prospects for IBN
in future networks. An analytical study is performed on the data collected from
relevant studies primarily focusing on the inter-working of IBN with
softwarized networking based on NFV/SDN infrastructures. Critical functions
required in the IBN management and service model design are explored with
different abstract modeling techniques and a converged architectural framework
is proposed. The key findings include: 1) benefits and role of IBN in
autonomous networking, 2) improvements needed to integrate intents as
fundamental policies for service modeling and network management, 3) need for
appropriate representation models for intents in domain agnostic abstract
manner, and 4) need to include learning as a fundamental function in autonomous
networks. These observations provide the basis for in-depth investigation and
standardization efforts for IBN as a fundamental network management paradigm in
beyond 5G networks.

    

### [[2108.04577] An Open Framework for Analyzing and Modeling XR Network Traffic](http://arxiv.org/abs/2108.04577)


  Thanks to recent advancements in the technology, eXtended Reality (XR)
applications are gaining a lot of momentum, and they will surely become
increasingly popular in the next decade. These new applications, however,
require a step forward also in terms of models to simulate and analyze this
type of traffic sources in modern communication networks, in order to guarantee
to the users state of the art performance and Quality of Experience (QoE).
Recognizing this need, in this work, we present a novel open-source traffic
model, which researchers can use as a starting point both for improvements of
the model itself and for the design of optimized algorithms for the
transmission of these peculiar data flows. Along with the mathematical model
and the code, we also share with the community the traces that we gathered for
our study, collected from freely available applications such as Minecraft VR,
Google Earth VR, and Virus Popper. Finally, we propose a roadmap for the
construction of an end-to-end framework that fills this gap in the current
state of the art.

    

### [[2108.04235] Deep Transfer Learning for Identifications of Slope Surface Cracks](http://arxiv.org/abs/2108.04235)


  Geohazards such as landslides have caused great losses to the safety of
people's lives and property, which is often accompanied with surface cracks. If
such surface cracks could be identified in time, it is of great significance
for the monitoring and early warning of geohazards. Currently, the most common
method for crack identification is manual detection, which is with low
efficiency and accuracy. In this paper, a deep transfer learning framework is
proposed to effectively and efficiently identify slope surface cracks for the
sake of fast monitoring and early warning of geohazards such as landslides. The
essential idea is to employ transfer learning by training (a) the large sample
dataset of concrete cracks and (b) the small sample dataset of soil and rock
masses cracks. In the proposed framework, (1) pretrained cracks identification
models are constructed based on the large sample dataset of concrete cracks;
(2) refined cracks identification models are further constructed based on the
small sample dataset of soil and rock masses cracks. The proposed framework
could be applied to conduct UAV surveys on high-steep slopes to realize the
monitoring and early warning of landslides to ensure the safety of people's
lives and property.

    

### [[2108.04238] TDLS: A Top-Down Layer Searching Algorithm for Generating Counterfactual Visual Explanation](http://arxiv.org/abs/2108.04238)


  Explanation of AI, as well as fairness of algorithms' decisions and the
transparency of the decision model, are becoming more and more important. And
it is crucial to design effective and human-friendly techniques when opening
the black-box model. Counterfactual conforms to the human way of thinking and
provides a human-friendly explanation, and its corresponding explanation
algorithm refers to a strategic alternation of a given data point so that its
model output is "counter-facted", i.e. the prediction is reverted. In this
paper, we adapt counterfactual explanation over fine-grained image
classification problem. We demonstrated an adaptive method that could give a
counterfactual explanation by showing the composed counterfactual feature map
using top-down layer searching algorithm (TDLS). We have proved that our TDLS
algorithm could provide more flexible counterfactual visual explanation in an
efficient way using VGG-16 model on Caltech-UCSD Birds 200 dataset. At the end,
we discussed several applicable scenarios of counterfactual visual
explanations.

    

### [[2108.04240] Classification of Influenza Hemagglutinin Protein Sequences using Convolutional Neural Networks](http://arxiv.org/abs/2108.04240)


  The Influenza virus can be considered as one of the most severe viruses that
can infect multiple species with often fatal consequences to the hosts. The
Hemagglutinin (HA) gene of the virus can be a target for antiviral drug
development realised through accurate identification of its sub-types and
possible the targeted hosts. This paper focuses on accurately predicting if an
Influenza type A virus can infect specific hosts, and more specifically, Human,
Avian and Swine hosts, using only the protein sequence of the HA gene. In more
detail, we propose encoding the protein sequences into numerical signals using
the Hydrophobicity Index and subsequently utilising a Convolutional Neural
Network-based predictive model. The Influenza HA protein sequences used in the
proposed work are obtained from the Influenza Research Database (IRD).
Specifically, complete and unique HA protein sequences were used for avian,
human and swine hosts. The data obtained for this work was 17999 human-host
proteins, 17667 avian-host proteins and 9278 swine-host proteins. Given this
set of collected proteins, the proposed method yields as much as 10% higher
accuracy for an individual class (namely, Avian) and 5% higher overall accuracy
than in an earlier study. It is also observed that the accuracy for each class
in this work is more balanced than what was presented in this earlier study. As
the results show, the proposed model can distinguish HA protein sequences with
high accuracy whenever the virus under investigation can infect Human, Avian or
Swine hosts.

    

### [[2108.04267] Automated Olfactory Bulb Segmentation on High Resolutional T2-Weighted MRI](http://arxiv.org/abs/2108.04267)


  The neuroimage analysis community has neglected the automated segmentation of
the olfactory bulb (OB) despite its crucial role in olfactory function. The
lack of an automatic processing method for the OB can be explained by its
challenging properties. Nonetheless, recent advances in MRI acquisition
techniques and resolution have allowed raters to generate more reliable manual
annotations. Furthermore, the high accuracy of deep learning methods for
solving semantic segmentation problems provides us with an option to reliably
assess even small structures. In this work, we introduce a novel, fast, and
fully automated deep learning pipeline to accurately segment OB tissue on
sub-millimeter T2-weighted (T2w) whole-brain MR images. To this end, we
designed a three-stage pipeline: (1) Localization of a region containing both
OBs using FastSurferCNN, (2) Segmentation of OB tissue within the localized
region through four independent AttFastSurferCNN - a novel deep learning
architecture with a self-attention mechanism to improve modeling of contextual
information, and (3) Ensemble of the predicted label maps. The OB pipeline
exhibits high performance in terms of boundary delineation, OB localization,
and volume estimation across a wide range of ages in 203 participants of the
Rhineland Study. Moreover, it also generalizes to scans of an independent
dataset never encountered during training, the Human Connectome Project (HCP),
with different acquisition parameters and demographics, evaluated in 30 cases
at the native 0.7mm HCP resolution, and the default 0.8mm pipeline resolution.
We extensively validated our pipeline not only with respect to segmentation
accuracy but also to known OB volume effects, where it can sensitively
replicate age effects.

    

### [[2108.04289] ACE: A Novel Approach for the Statistical Analysis of Pairwise Connectivity](http://arxiv.org/abs/2108.04289)


  Analysing correlations between streams of events is an important problem. It
arises for example in Neurosciences, when the connectivity of neurons should be
inferred from spike trains that record neurons' individual spiking activity.
While recently some approaches for inferring delayed synaptic connections have
been proposed, they are limited in the types of connectivities and delays they
are able to handle, or require computation-intensive procedures. This paper
proposes a faster and more flexible approach for analysing such delayed
correlated activity: a statistical approach for the Analysis of Connectivity in
spiking Events (ACE), based on the idea of hypothesis testing. It first
computes for any pair of a source and a target neuron the inter-spike delays
between subsequent source- and target-spikes. Then, it derives a null model for
the distribution of inter-spike delays for \emph{uncorrelated}~neurons.
Finally, it compares the observed distribution of inter-spike delays to this
null model and infers pairwise connectivity based on the Pearson's Chi-squared
test statistic. Thus, ACE is capable to detect connections with a priori
unknown, non-discrete (and potentially large) inter-spike delays, which might
vary between pairs of neurons. Since ACE works incrementally, it has potential
for being used in online processing. In our experiments, we visualise the
advantages of ACE in varying experimental scenarios (except for one special
case) and in a state-of-the-art dataset which has been generated for
neuro-scientific research under most realistic conditions.

    

### [[2108.04327] Natural Numerical Networks for Natura 2000 habitats classification by satellite images](http://arxiv.org/abs/2108.04327)


  Natural numerical networks are introduced as a new classification algorithm
based on the numerical solution of nonlinear partial differential equations of
forward-backward diffusion type on complete graphs. The proposed natural
numerical network is applied to open important environmental and nature
conservation task, the automated identification of protected habitats by using
satellite images. In the natural numerical network, the forward diffusion
causes the movement of points in a feature space toward each other. The
opposite effect, keeping the points away from each other, is caused by backward
diffusion. This yields the desired classification. The natural numerical
network contains a few parameters that are optimized in the learning phase of
the method. After learning parameters and optimizing the topology of the
network graph, classification necessary for habitat identification is
performed. A relevancy map for each habitat is introduced as a tool for
validating the classification and finding new Natura 2000 habitat appearances.

    

### [[2108.04343] Towards a Generic Multimodal Architecture for Batch and Streaming Big Data Integration](http://arxiv.org/abs/2108.04343)


  Big Data are rapidly produced from various heterogeneous data sources. They
are of different types (text, image, video or audio) and have different levels
of reliability and completeness. One of the most interesting architectures that
deal with the large amount of emerging data at high velocity is called the
lambda architecture. In fact, it combines two different processing layers
namely batch and speed layers, each providing specific views of data while
ensuring robustness, fast and scalable data processing. However, most papers
dealing with the lambda architecture are focusing one single type of data
generally produced by a single data source. Besides, the layers of the
architecture are implemented independently, or, at best, are combined to
perform basic processing without assessing either the data reliability or
completeness. Therefore, inspired by the lambda architecture, we propose in
this paper a generic multimodal architecture that combines both batch and
streaming processing in order to build a complete, global and accurate insight
in near-real-time based on the knowledge extracted from multiple heterogeneous
Big Data sources. Our architecture uses batch processing to analyze the data
structures and contents, build the learning models and calculate the
reliability index of the involved sources, while the streaming processing uses
the built-in models of the batch layer to immediately process incoming data and
rapidly provide results. We validate our architecture in the context of urban
traffic management systems in order to detect congestions.

    

### [[2108.04344] A Survey of Machine Learning Techniques for Detecting and Diagnosing COVID-19 from Imaging](http://arxiv.org/abs/2108.04344)


  Due to the limited availability and high cost of the reverse
transcription-polymerase chain reaction (RT-PCR) test, many studies have
proposed machine learning techniques for detecting COVID-19 from medical
imaging. The purpose of this study is to systematically review, assess, and
synthesize research articles that have used different machine learning
techniques to detect and diagnose COVID-19 from chest X-ray and CT scan images.
A structured literature search was conducted in the relevant bibliographic
databases to ensure that the survey solely centered on reproducible and
high-quality research. We selected papers based on our inclusion criteria. In
this survey, we reviewed $98$ articles that fulfilled our inclusion criteria.
We have surveyed a complete pipeline of chest imaging analysis techniques
related to COVID-19, including data collection, pre-processing, feature
extraction, classification, and visualization. We have considered CT scans and
X-rays as both are widely used to describe the latest developments in medical
imaging to detect COVID-19. This survey provides researchers with valuable
insights into different machine learning techniques and their performance in
the detection and diagnosis of COVID-19 from chest imaging. At the end, the
challenges and limitations in detecting COVID-19 using machine learning
techniques and the future direction of research are discussed.

    

### [[2108.04345] Explainable AI and susceptibility to adversarial attacks: a case study in classification of breast ultrasound images](http://arxiv.org/abs/2108.04345)


  Ultrasound is a non-invasive imaging modality that can be conveniently used
to classify suspicious breast nodules and potentially detect the onset of
breast cancer. Recently, Convolutional Neural Networks (CNN) techniques have
shown promising results in classifying ultrasound images of the breast into
benign or malignant. However, CNN inference acts as a black-box model, and as
such, its decision-making is not interpretable. Therefore, increasing effort
has been dedicated to explaining this process, most notably through GRAD-CAM
and other techniques that provide visual explanations into inner workings of
CNNs. In addition to interpretation, these methods provide clinically important
information, such as identifying the location for biopsy or treatment. In this
work, we analyze how adversarial assaults that are practically undetectable may
be devised to alter these importance maps dramatically. Furthermore, we will
show that this change in the importance maps can come with or without altering
the classification result, rendering them even harder to detect. As such, care
must be taken when using these importance maps to shed light on the inner
workings of deep learning. Finally, we utilize Multi-Task Learning (MTL) and
propose a new network based on ResNet-50 to improve the classification
accuracies. Our sensitivity and specificity is comparable to the state of the
art results.

    

### [[2108.04349] AASeg: Attention Aware Network for Real Time Semantic Segmentation](http://arxiv.org/abs/2108.04349)


  In this paper, we present a new network named Attention Aware Network (AASeg)
for real time semantic image segmentation. Our network incorporates spatial and
channel information using Spatial Attention (SA) and Channel Attention (CA)
modules respectively. It also uses dense local multi-scale context information
using Multi Scale Context (MSC) module. The feature maps are concatenated
individually to produce the final segmentation map. We demonstrate the
effectiveness of our method using a comprehensive analysis, quantitative
experimental results and ablation study using Cityscapes, ADE20K and Camvid
datasets. Our network performs better than most previous architectures with a
74.4\% Mean IOU on Cityscapes test dataset while running at 202.7 FPS.

    

### [[2108.04351] Adversarial Open Domain Adaption Framework (AODA): Sketch-to-Photo Synthesis](http://arxiv.org/abs/2108.04351)


  This paper aims to demonstrate the efficiency of the Adversarial Open Domain
Adaption framework for sketch-to-photo synthesis. The unsupervised open domain
adaption for generating realistic photos from a hand-drawn sketch is
challenging as there is no such sketch of that class for training data. The
absence of learning supervision and the huge domain gap between both the
freehand drawing and picture domains make it hard. We present an approach that
learns both sketch-to-photo and photo-to-sketch generation to synthesise the
missing freehand drawings from pictures. Due to the domain gap between
synthetic sketches and genuine ones, the generator trained on false drawings
may produce unsatisfactory results when dealing with drawings of lacking
classes. To address this problem, we offer a simple but effective open-domain
sampling and optimization method that tricks the generator into considering
false drawings as genuine. Our approach generalises the learnt sketch-to-photo
and photo-to-sketch mappings from in-domain input to open-domain categories. On
the Scribble and SketchyCOCO datasets, we compared our technique to the most
current competing methods. For many types of open-domain drawings, our model
outperforms impressive results in synthesising accurate colour, substance, and
retaining the structural layout.

    

### [[2108.04357] MotionInput v2.0 supporting DirectX: A modular library of open-source gesture-based machine learning and computer vision methods for interacting and controlling existing software with a webcam](http://arxiv.org/abs/2108.04357)


  Touchless computer interaction has become an important consideration during
the COVID-19 pandemic period. Despite progress in machine learning and computer
vision that allows for advanced gesture recognition, an integrated collection
of such open-source methods and a user-customisable approach to utilising them
in a low-cost solution for touchless interaction in existing software is still
missing. In this paper, we introduce the MotionInput v2.0 application. This
application utilises published open-source libraries and additional gesture
definitions developed to take the video stream from a standard RGB webcam as
input. It then maps human motion gestures to input operations for existing
applications and games. The user can choose their own preferred way of
interacting from a series of motion types, including single and bi-modal hand
gesturing, full-body repetitive or extremities-based exercises, head and facial
movements, eye tracking, and combinations of the above. We also introduce a
series of bespoke gesture recognition classifications as DirectInput triggers,
including gestures for idle states, auto calibration, depth capture from a 2D
RGB webcam stream and tracking of facial motions such as mouth motions,
winking, and head direction with rotation. Three use case areas assisted the
development of the modules: creativity software, office and clinical software,
and gaming software. A collection of open-source libraries has been integrated
and provide a layer of modular gesture mapping on top of existing mouse and
keyboard controls in Windows via DirectX. With ease of access to webcams
integrated into most laptops and desktop computers, touchless computing becomes
more available with MotionInput v2.0, in a federated and locally processed
method.

    

### [[2108.04358] Convolutional Nets for Diabetic Retinopathy Screening in Bangladeshi Patients](http://arxiv.org/abs/2108.04358)


  Diabetes is one of the most prevalent chronic diseases in Bangladesh, and as
a result, Diabetic Retinopathy (DR) is widespread in the population. DR, an eye
illness caused by diabetes, can lead to blindness if it is not identified and
treated in its early stages. Unfortunately, diagnosis of DR requires medically
trained professionals, but Bangladesh has limited specialists in comparison to
its population. Moreover, the screening process is often expensive, prohibiting
many from receiving timely and proper diagnosis. To address the problem, we
introduce a deep learning algorithm which screens for different stages of DR.
We use a state-of-the-art CNN architecture to diagnose patients based on
retinal fundus imagery. This paper is an experimental evaluation of the
algorithm we developed for DR diagnosis and screening specifically for
Bangladeshi patients. We perform this validation study using separate pools of
retinal image data of real patients from a hospital and field studies in
Bangladesh. Our results show that the algorithm is effective at screening
Bangladeshi eyes even when trained on a public dataset which is out of domain,
and can accurately determine the stage of DR as well, achieving an overall
accuracy of 92.27\% and 93.02\% on two validation sets of Bangladeshi eyes. The
results confirm the ability of the algorithm to be used in real clinical
settings and applications due to its high accuracy and classwise metrics. Our
algorithm is implemented in the application Drishti, which is used to screen
for DR in patients living in rural areas in Bangladesh, where access to
professional screening is limited.

    

### [[2108.04359] Adaptable image quality assessment using meta-reinforcement learning of task amenability](http://arxiv.org/abs/2108.04359)


  The performance of many medical image analysis tasks are strongly associated
with image data quality. When developing modern deep learning algorithms,
rather than relying on subjective (human-based) image quality assessment (IQA),
task amenability potentially provides an objective measure of task-specific
image quality. To predict task amenability, an IQA agent is trained using
reinforcement learning (RL) with a simultaneously optimised task predictor,
such as a classification or segmentation neural network. In this work, we
develop transfer learning or adaptation strategies to increase the adaptability
of both the IQA agent and the task predictor so that they are less dependent on
high-quality, expert-labelled training data. The proposed transfer learning
strategy re-formulates the original RL problem for task amenability in a
meta-reinforcement learning (meta-RL) framework. The resulting algorithm
facilitates efficient adaptation of the agent to different definitions of image
quality, each with its own Markov decision process environment including
different images, labels and an adaptable task predictor. Our work demonstrates
that the IQA agents pre-trained on non-expert task labels can be adapted to
predict task amenability as defined by expert task labels, using only a small
set of expert labels. Using 6644 clinical ultrasound images from 249 prostate
cancer patients, our results for image classification and segmentation tasks
show that the proposed IQA method can be adapted using data with as few as
respective 19.7% and 29.6% expert-reviewed consensus labels and still achieve
comparable IQA and task performance, which would otherwise require a training
dataset with 100% expert labels.

    

### [[2108.04384] RaftMLP: Do MLP-based Models Dream of Winning Over Computer Vision?](http://arxiv.org/abs/2108.04384)


  For the past ten years, CNN has reigned supreme in the world of computer
vision, but recently, Transformer is on the rise. However, the quadratic
computational cost of self-attention has become a severe problem of practice.
There has been much research on architectures without CNN and self-attention in
this context. In particular, MLP-Mixer is a simple idea designed using MLPs and
hit an accuracy comparable to the Vision Transformer. However, the only
inductive bias in this architecture is the embedding of tokens. Thus, there is
still a possibility to build a non-convolutional inductive bias into the
architecture itself, and we built in an inductive bias using two simple ideas.
A way is to divide the token-mixing block vertically and horizontally. Another
way is to make spatial correlations denser among some channels of token-mixing.
With this approach, we were able to improve the accuracy of the MLP-Mixer while
reducing its parameters and computational complexity. Compared to other
MLP-based models, the proposed model, named RaftMLP has a good balance of
computational complexity, the number of parameters, and actual memory usage. In
addition, our work indicates that MLP-based models have the potential to
replace CNNs by adopting inductive bias. The source code in PyTorch version is
available at \url{this https URL}.

    

### [[2108.04389] Diversity-aware Web APIs Recommendation with Compatibility Guarantee](http://arxiv.org/abs/2108.04389)


  With the ever-increasing prevalence of web APIs (Application Programming
Interfaces) in enabling smart software developments, finding and composing a
list of existing web APIs that can corporately fulfil the software developers'
functional needs have become a promising way to develop a successful mobile
app, economically and conveniently. However, the big volume and diversity of
candidate web APIs put additional burden on the app developers' web APIs
selection decision-makings, since it is often a challenging task to
simultaneously guarantee the diversity and compatibility of the finally
selected a set of web APIs. Considering this challenge, a Diversity-aware and
Compatibility-driven web APIs Recommendation approach, namely DivCAR, is put
forward in this paper. First, to achieve diversity, DivCAR employs random walk
sampling technique on a pre-built correlation graph to generate diverse
correlation subgraphs. Afterwards, with the diverse correlation subgraphs, we
model the compatible web APIs recommendation problem to be a minimum group
Steiner tree search problem. Through solving the minimum group Steiner tree
search problem, manifold sets of compatible and diverse web APIs ranked are
returned to the app developers. At last, we design and enact a set of
experiments on a real-world dataset crawled from this http URL.
Experimental results validate the effectiveness and efficiency of our proposed
DivCAR approach in balancing the web APIs recommendation diversity and
compatibility.

    

### [[2108.04392] Rethinking Architecture Selection in Differentiable NAS](http://arxiv.org/abs/2108.04392)


  Differentiable Neural Architecture Search is one of the most popular Neural
Architecture Search (NAS) methods for its search efficiency and simplicity,
accomplished by jointly optimizing the model weight and architecture parameters
in a weight-sharing supernet via gradient-based algorithms. At the end of the
search phase, the operations with the largest architecture parameters will be
selected to form the final architecture, with the implicit assumption that the
values of architecture parameters reflect the operation strength. While much
has been discussed about the supernet's optimization, the architecture
selection process has received little attention. We provide empirical and
theoretical analysis to show that the magnitude of architecture parameters does
not necessarily indicate how much the operation contributes to the supernet's
performance. We propose an alternative perturbation-based architecture
selection that directly measures each operation's influence on the supernet. We
re-evaluate several differentiable NAS methods with the proposed architecture
selection and find that it is able to extract significantly improved
architectures from the underlying supernets consistently. Furthermore, we find
that several failure modes of DARTS can be greatly alleviated with the proposed
selection method, indicating that much of the poor generalization observed in
DARTS can be attributed to the failure of magnitude-based architecture
selection rather than entirely the optimization of its supernet.

    

### [[2108.04409] On Procedural Adversarial Noise Attack And Defense](http://arxiv.org/abs/2108.04409)


  Deep Neural Networks (DNNs) are vulnerable to adversarial examples which
would inveigle neural networks to make prediction errors with small per-
turbations on the input images. Researchers have been devoted to promoting the
research on the universal adversarial perturbations (UAPs) which are
gradient-free and have little prior knowledge on data distributions. Procedural
adversarial noise at- tack is a data-free universal perturbation generation
method. In this paper, we propose two universal adversarial perturbation (UAP)
generation methods based on procedural noise functions: Simplex noise and
Worley noise. In our framework, the shading which disturbs visual
classification is generated with rendering technology. Without changing the
semantic representations, the adversarial examples generated via our methods
show superior performance on the attack.

    

### [[2108.04417] Privacy-Preserving Machine Learning: Methods, Challenges and Directions](http://arxiv.org/abs/2108.04417)


  Machine learning (ML) is increasingly being adopted in a wide variety of
application domains. Usually, a well-performing ML model, especially, emerging
deep neural network model, relies on a large volume of training data and
high-powered computational resources. The need for a vast volume of available
data raises serious privacy concerns because of the risk of leakage of highly
privacy-sensitive information and the evolving regulatory environments that
increasingly restrict access to and use of privacy-sensitive data. Furthermore,
a trained ML model may also be vulnerable to adversarial attacks such as
membership/property inference attacks and model inversion attacks. Hence,
well-designed privacy-preserving ML (PPML) solutions are crucial and have
attracted increasing research interest from academia and industry. More and
more efforts of PPML are proposed via integrating privacy-preserving techniques
into ML algorithms, fusing privacy-preserving approaches into ML pipeline, or
designing various privacy-preserving architectures for existing ML systems. In
particular, existing PPML arts cross-cut ML, system, security, and privacy;
hence, there is a critical need to understand state-of-art studies, related
challenges, and a roadmap for future research. This paper systematically
reviews and summarizes existing privacy-preserving approaches and proposes a
PGU model to guide evaluation for various PPML solutions through elaborately
decomposing their privacy-preserving functionalities. The PGU model is designed
as the triad of Phase, Guarantee, and technical Utility. Furthermore, we also
discuss the unique characteristics and challenges of PPML and outline possible
directions of future work that benefit a wide range of research communities
among ML, distributed systems, security, and privacy areas.

    

### [[2108.04423] Semi-supervised classification of radiology images with NoTeacher: A Teacher that is not Mean](http://arxiv.org/abs/2108.04423)


  Deep learning models achieve strong performance for radiology image
classification, but their practical application is bottlenecked by the need for
large labeled training datasets. Semi-supervised learning (SSL) approaches
leverage small labeled datasets alongside larger unlabeled datasets and offer
potential for reducing labeling cost. In this work, we introduce NoTeacher, a
novel consistency-based SSL framework which incorporates probabilistic
graphical models. Unlike Mean Teacher which maintains a teacher network updated
via a temporal ensemble, NoTeacher employs two independent networks, thereby
eliminating the need for a teacher network. We demonstrate how NoTeacher can be
customized to handle a range of challenges in radiology image classification.
Specifically, we describe adaptations for scenarios with 2D and 3D inputs, uni
and multi-label classification, and class distribution mismatch between labeled
and unlabeled portions of the training data. In realistic empirical evaluations
on three public benchmark datasets spanning the workhorse modalities of
radiology (X-Ray, CT, MRI), we show that NoTeacher achieves over 90-95% of the
fully supervised AUROC with less than 5-15% labeling budget. Further, NoTeacher
outperforms established SSL methods with minimal hyperparameter tuning, and has
implications as a principled and practical option for semisupervised learning
in radiology applications.

    

### [[2108.04428] Tensor Principal Component Analysis in High Dimensional CP Models](http://arxiv.org/abs/2108.04428)


  The CP decomposition for high dimensional non-orthogonal spike tensors is an
important problem with broad applications across many disciplines. However,
previous works with theoretical guarantee typically assume restrictive
incoherence conditions on the basis vectors for the CP components. In this
paper, we propose new computationally efficient composite PCA and concurrent
orthogonalization algorithms for tensor CP decomposition with theoretical
guarantees under mild incoherence conditions. The composite PCA applies the
principal component or singular value decompositions twice, first to a matrix
unfolding of the tensor data to obtain singular vectors and then to the matrix
folding of the singular vectors obtained in the first step. It can be used as
an initialization for any iterative optimization schemes for the tensor CP
decomposition. The concurrent orthogonalization algorithm iteratively estimates
the basis vector in each mode of the tensor by simultaneously applying
projections to the orthogonal complements of the spaces generated by others CP
components in other modes. It is designed to improve the alternating least
squares estimator and other forms of the high order orthogonal iteration for
tensors with low or moderately high CP ranks. Our theoretical investigation
provides estimation accuracy and statistical convergence rates for the two
proposed algorithms. Our implementations on synthetic data demonstrate
significant practical superiority of our approach over existing methods.

    

### [[2108.04430] Enhancing Knowledge Tracing via Adversarial Training](http://arxiv.org/abs/2108.04430)


  We study the problem of knowledge tracing (KT) where the goal is to trace the
students' knowledge mastery over time so as to make predictions on their future
performance. Owing to the good representation capacity of deep neural networks
(DNNs), recent advances on KT have increasingly concentrated on exploring DNNs
to improve the performance of KT. However, we empirically reveal that the DNNs
based KT models may run the risk of overfitting, especially on small datasets,
leading to limited generalization. In this paper, by leveraging the current
advances in adversarial training (AT), we propose an efficient AT based KT
method (ATKT) to enhance KT model's generalization and thus push the limit of
KT. Specifically, we first construct adversarial perturbations and add them on
the original interaction embeddings as adversarial examples. The original and
adversarial examples are further used to jointly train the KT model, forcing it
is not only to be robust to the adversarial examples, but also to enhance the
generalization over the original ones. To better implement AT, we then present
an efficient attentive-LSTM model as KT backbone, where the key is a proposed
knowledge hidden state attention module that adaptively aggregates information
from previous knowledge hidden states while simultaneously highlighting the
importance of current knowledge hidden state to make a more accurate
prediction. Extensive experiments on four public benchmark datasets demonstrate
that our ATKT achieves new state-of-the-art performance. Code is available at:
\color{blue} {\url{this https URL}}.

    

### [[2108.04432] Revisit the Fundamental Theorem of Linear Algebra](http://arxiv.org/abs/2108.04432)


  This survey is meant to provide an introduction to the fundamental theorem of
linear algebra and the theories behind them. Our goal is to give a rigorous
introduction to the readers with prior exposure to linear algebra.
Specifically, we provide some details and proofs of some results from (Strang,
1993). We then describe the fundamental theorem of linear algebra from
different views and find the properties and relationships behind the views. The
fundamental theorem of linear algebra is essential in many fields, such as
electrical engineering, computer science, machine learning, and deep learning.
This survey is primarily a summary of purpose, significance of important
theories behind it.
The sole aim of this survey is to give a self-contained introduction to
concepts and mathematical tools in theory behind the fundamental theorem of
linear algebra and rigorous analysis in order to seamlessly introduce its
properties in four subspaces in subsequent sections. However, we clearly
realize our inability to cover all the useful and interesting results and given
the paucity of scope to present this discussion, e.g., the separated analysis
of the (orthogonal) projection matrices. We refer the reader to literature in
the field of linear algebra for a more detailed introduction to the related
fields. Some excellent examples include (Rose, 1982; Strang, 2009; Trefethen
and Bau III, 1997; Strang, 2019, 2021).

    

### [[2108.04433] Deep Learning Enhanced Dynamic Mode Decomposition](http://arxiv.org/abs/2108.04433)


  Koopman operator theory shows how nonlinear dynamical systems can be
represented as an infinite-dimensional, linear operator acting on a Hilbert
space of observables of the system. However, determining the relevant modes and
eigenvalues of this infinite-dimensional operator can be difficult. The
extended dynamic mode decomposition (EDMD) is one such method for generating
approximations to Koopman spectra and modes, but the EDMD method faces its own
set of challenges due to the need of user defined observables. To address this
issue, we explore the use of convolutional autoencoder networks to
simultaneously find optimal families of observables which also generate both
accurate embeddings of the flow into a space of observables and immersions of
the observables back into flow coordinates. This network results in a global
transformation of the flow and affords future state prediction via EDMD and the
decoder network. We call this method deep learning dynamic mode decomposition
(DLDMD). The method is tested on canonical nonlinear data sets and is shown to
produce results that outperform a standard DMD approach.

    

### [[2108.04436] A Generalizable Model-and-Data Driven Approach for Open-Set RFF Authentication](http://arxiv.org/abs/2108.04436)


  Radio-frequency fingerprints~(RFFs) are promising solutions for realizing
low-cost physical layer authentication. Machine learning-based methods have
been proposed for RFF extraction and discrimination. However, most existing
methods are designed for the closed-set scenario where the set of devices is
remains unchanged. These methods can not be generalized to the RFF
discrimination of unknown devices. To enable the discrimination of RFF from
both known and unknown devices, we propose a new end-to-end deep learning
framework for extracting RFFs from raw received signals. The proposed framework
comprises a novel preprocessing module, called neural synchronization~(NS),
which incorporates the data-driven learning with signal processing priors as an
inductive bias from communication-model based processing. Compared to
traditional carrier synchronization techniques, which are static, this module
estimates offsets by two learnable deep neural networks jointly trained by the
RFF extractor. Additionally, a hypersphere representation is proposed to
further improve the discrimination of RFF. Theoretical analysis shows that such
a data-and-model framework can better optimize the mutual information between
device identity and the RFF, which naturally leads to better performance.
Experimental results verify that the proposed RFF significantly outperforms
purely data-driven DNN-design and existing handcrafted RFF methods in terms of
both discrimination and network generalizability.

    

### [[2108.04443] AdaRNN: Adaptive Learning and Forecasting of Time Series](http://arxiv.org/abs/2108.04443)


  Time series has wide applications in the real world and is known to be
difficult to forecast. Since its statistical properties change over time, its
distribution also changes temporally, which will cause severe distribution
shift problem to existing methods. However, it remains unexplored to model the
time series in the distribution perspective. In this paper, we term this as
Temporal Covariate Shift (TCS). This paper proposes Adaptive RNNs (AdaRNN) to
tackle the TCS problem by building an adaptive model that generalizes well on
the unseen test data. AdaRNN is sequentially composed of two novel algorithms.
First, we propose Temporal Distribution Characterization to better characterize
the distribution information in the TS. Second, we propose Temporal
Distribution Matching to reduce the distribution mismatch in TS to learn the
adaptive TS model. AdaRNN is a general framework with flexible distribution
distances integrated. Experiments on human activity recognition, air quality
prediction, and financial analysis show that AdaRNN outperforms the latest
methods by a classification accuracy of 2.6% and significantly reduces the RMSE
by 9.0%. We also show that the temporal distribution matching algorithm can be
extended in Transformer structure to boost its performance.

    

### [[2108.04448] Decentralized Composite Optimization with Compression](http://arxiv.org/abs/2108.04448)


  Decentralized optimization and communication compression have exhibited their
great potential in accelerating distributed machine learning by mitigating the
communication bottleneck in practice. While existing decentralized algorithms
with communication compression mostly focus on the problems with only smooth
components, we study the decentralized stochastic composite optimization
problem with a potentially non-smooth component. A \underline{Prox}imal
gradient \underline{L}in\underline{EA}r convergent \underline{D}ecentralized
algorithm with compression, Prox-LEAD, is proposed with rigorous theoretical
analyses in the general stochastic setting and the finite-sum setting. Our
theorems indicate that Prox-LEAD works with arbitrary compression precision,
and it tremendously reduces the communication cost almost for free. The
superiorities of the proposed algorithms are demonstrated through the
comparison with state-of-the-art algorithms in terms of convergence
complexities and numerical experiments. Our algorithmic framework also
generally enlightens the compressed communication on other primal-dual
algorithms by reducing the impact of inexact iterations, which might be of
independent interest.

    

### [[2108.04449] An empirical investigation into audio pipeline approaches for classifying bird species](http://arxiv.org/abs/2108.04449)


  This paper is an investigation into aspects of an audio classification
pipeline that will be appropriate for the monitoring of bird species on edges
devices. These aspects include transfer learning, data augmentation and model
optimization. The hope is that the resulting models will be good candidates to
deploy on edge devices to monitor bird populations. Two classification
approaches will be taken into consideration, one which explores the
effectiveness of a traditional Deep Neural Network(DNN) and another that makes
use of Convolutional layers.This study aims to contribute empirical evidence of
the merits and demerits of each approach.

    

### [[2108.04452] High Quality Related Search Query Suggestions using Deep Reinforcement Learning](http://arxiv.org/abs/2108.04452)


  "High Quality Related Search Query Suggestions" task aims at recommending
search queries which are real, accurate, diverse, relevant and engaging.
Obtaining large amounts of query-quality human annotations is expensive. Prior
work on supervised query suggestion models suffered from selection and exposure
bias, and relied on sparse and noisy immediate user-feedback (e.g., clicks),
leading to low quality suggestions. Reinforcement Learning techniques employed
to reformulate a query using terms from search results, have limited
scalability to large-scale industry applications. To recommend high quality
related search queries, we train a Deep Reinforcement Learning model to predict
the query a user would enter next. The reward signal is composed of long-term
session-based user feedback, syntactic relatedness and estimated naturalness of
generated query. Over the baseline supervised model, our proposed approach
achieves a significant relative improvement in terms of recommendation
diversity (3%), down-stream user-engagement (4.2%) and per-sentence word
repetitions (82%).

    

### [[2108.04462] Deep Reinforcement Learning for Demand Driven Services in Logistics and Transportation Systems: A Survey](http://arxiv.org/abs/2108.04462)


  Recent technology development brings the booming of numerous new
Demand-Driven Services (DDS) into urban lives, including ridesharing, on-demand
delivery, express systems and warehousing. In DDS, a service loop is an
elemental structure, including its service worker, the service providers and
corresponding service targets. The service workers should transport either
humans or parcels from the providers to the target locations. Various planning
tasks within DDS can thus be classified into two individual stages: 1)
Dispatching, which is to form service loops from demand/supply distributions,
and 2)Routing, which is to decide specific serving orders within the
constructed loops. Generating high-quality strategies in both stages is
important to develop DDS but faces several challenging. Meanwhile, deep
reinforcement learning (DRL) has been developed rapidly in recent years. It is
a powerful tool to solve these problems since DRL can learn a parametric model
without relying on too many problem-based assumptions and optimize long-term
effect by learning sequential decisions. In this survey, we first define DDS,
then highlight common applications and important decision/control problems
within. For each problem, we comprehensively introduce the existing DRL
solutions, and further summarize them in
\textit{this https URL\_Survey}. We also introduce
open simulation environments for development and evaluation of DDS
applications. Finally, we analyze remaining challenges and discuss further
research opportunities in DRL solutions for DDS.

    

### [[2108.04482] Computational complexity of Inexact Proximal Point Algorithm for Convex Optimization under Holderian Growth](http://arxiv.org/abs/2108.04482)


  Several decades ago the Proximal Point Algorithm (PPA) started to gain much
attraction for both abstract operator theory and the numerical optimization
communities. Even in modern applications, researchers still use proximal
minimization theory to design scalable algorithms that overcome nonsmoothness
in high dimensional models. Several remarkable references as
\cite{Fer:91,Ber:82constrained,Ber:89parallel,Tom:11} analyzed the tight local
relations between the convergence rate of PPA and the regularity of the
objective function. However, without taking into account the concrete
computational effort paid for computing each PPA iteration, any iteration
complexity remains abstract and purely informative. In this manuscript we aim
to evaluate the computational complexity of practical PPA in terms of
(proximal) gradient/subgradient iterations, which might allow a fair
positioning of the famous PPA numerical performance in the class of first order
methods. First, we derive nonasymptotic iteration complexity estimates of exact
and inexact PPA to minimize convex functions under $\gamma-$Holderian growth:
$\BigO{\log(1/\epsilon)}$ (for $\gamma \in [1,2]$) and
$\BigO{1/\epsilon^{\gamma - 2}}$ (for $\gamma > 2$). In particular, we recover
well-known results on exact PPA: finite convergence for sharp minima and linear
convergence for quadratic growth, even under presence of inexactness. Second,
assuming that an usual (proximal) gradient/subgradient method subroutine is
employed to compute inexact PPA iteration, we show novel computational
complexity bounds on a restarted variant of the inexact PPA, available when no
information on the growth of the objective function is known. In the numerical
experiments we confirm the practical performance and implementability of our
schemes.

    

### [[2108.04496] Regularized Sequential Latent Variable Models with Adversarial Neural Networks](http://arxiv.org/abs/2108.04496)


  The recurrent neural networks (RNN) with richly distributed internal states
and flexible non-linear transition functions, have overtaken the dynamic
Bayesian networks such as the hidden Markov models (HMMs) in the task of
modeling highly structured sequential data. These data, such as from speech and
handwriting, often contain complex relationships between the underlaying
variational factors and the observed data. The standard RNN model has very
limited randomness or variability in its structure, coming from the output
conditional probability model. This paper will present different ways of using
high level latent random variables in RNN to model the variability in the
sequential data, and the training method of such RNN model under the VAE
(Variational Autoencoder) principle. We will explore possible ways of using
adversarial method to train a variational RNN model. Contrary to competing
approaches, our approach has theoretical optimum in the model training and
provides better model training stability. Our approach also improves the
posterior approximation in the variational inference network by a separated
adversarial training step. Numerical results simulated from TIMIT speech data
show that reconstruction loss and evidence lower bound converge to the same
level and adversarial training loss converges to 0.

    

### [[2108.04526] A Survey on Deep Reinforcement Learning for Data Processing and Analytics](http://arxiv.org/abs/2108.04526)


  Data processing and analytics are fundamental and pervasive. Algorithms play
a vital role in data processing and analytics where many algorithm designs have
incorporated heuristics and general rules from human knowledge and experience
to improve their effectiveness. Recently, reinforcement learning, deep
reinforcement learning (DRL) in particular, is increasingly explored and
exploited in many areas because it can learn better strategies in complicated
environments it is interacting with than statically designed algorithms.
Motivated by this trend, we provide a comprehensive review of recent works
focusing on utilizing deep reinforcement learning to improve data processing
and analytics. First, we present an introduction to key concepts, theories, and
methods in deep reinforcement learning. Next, we discuss deep reinforcement
learning deployment on database systems, facilitating data processing and
analytics in various aspects, including data organization, scheduling, tuning,
and indexing. Then, we survey the application of deep reinforcement learning in
data processing and analytics, ranging from data preparation, natural language
interface to healthcare, fintech, etc. Finally, we discuss important open
challenges and future research directions of using deep reinforcement learning
in data processing and analytics.

    

### [[2108.04536] Learning Multi-Granular Spatio-Temporal Graph Network for Skeleton-based Action Recognition](http://arxiv.org/abs/2108.04536)


  The task of skeleton-based action recognition remains a core challenge in
human-centred scene understanding due to the multiple granularities and large
variation in human motion. Existing approaches typically employ a single neural
representation for different motion patterns, which has difficulty in capturing
fine-grained action classes given limited training data. To address the
aforementioned problems, we propose a novel multi-granular spatio-temporal
graph network for skeleton-based action classification that jointly models the
coarse- and fine-grained skeleton motion patterns. To this end, we develop a
dual-head graph network consisting of two interleaved branches, which enables
us to extract features at two spatio-temporal resolutions in an effective and
efficient manner. Moreover, our network utilises a cross-head communication
strategy to mutually enhance the representations of both heads. We conducted
extensive experiments on three large-scale datasets, namely NTU RGB+D 60, NTU
RGB+D 120, and Kinetics-Skeleton, and achieves the state-of-the-art performance
on all the benchmarks, which validates the effectiveness of our method.

    

### [[2108.04543] Known Operator Learning and Hybrid Machine Learning in Medical Imaging --- A Review of the Past, the Present, and the Future](http://arxiv.org/abs/2108.04543)


  In this article, we perform a review of the state-of-the-art of hybrid
machine learning in medical imaging. We start with a short summary of the
general developments of the past in machine learning and how general and
specialized approaches have been in competition in the past decades. A
particular focus will be the theoretical and experimental evidence pro and
contra hybrid modelling. Next, we inspect several new developments regarding
hybrid machine learning with a particular focus on so-called known operator
learning and how hybrid approaches gain more and more momentum across
essentially all applications in medical imaging and medical image analysis. As
we will point out by numerous examples, hybrid models are taking over in image
reconstruction and analysis. Even domains such as physical simulation and
scanner and acquisition design are being addressed using machine learning grey
box modelling approaches. Towards the end of the article, we will investigate a
few future directions and point out relevant areas in which hybrid modelling,
meta learning, and other domains will likely be able to drive the
state-of-the-art ahead.

    

### [[2108.04551] ABC-FL: Anomalous and Benign client Classification in Federated Learning](http://arxiv.org/abs/2108.04551)


  Federated Learning is a distributed machine learning framework designed for
data privacy preservation i.e., local data remain private throughout the entire
training and testing procedure. Federated Learning is gaining popularity
because it allows one to use machine learning techniques while preserving
privacy. However, it inherits the vulnerabilities and susceptibilities raised
in deep learning techniques. For instance, Federated Learning is particularly
vulnerable to data poisoning attacks that may deteriorate its performance and
integrity due to its distributed nature and inaccessibility to the raw data. In
addition, it is extremely difficult to correctly identify malicious clients due
to the non-Independently and/or Identically Distributed (non-IID) data. The
real-world data can be complex and diverse, making them hardly distinguishable
from the malicious data without direct access to the raw data. Prior research
has focused on detecting malicious clients while treating only the clients
having IID data as benign. In this study, we propose a method that detects and
classifies anomalous clients from benign clients when benign ones have non-IID
data. Our proposed method leverages feature dimension reduction, dynamic
clustering, and cosine similarity-based clipping. The experimental results
validates that our proposed method not only classifies the malicious clients
but also alleviates their negative influences from the entire procedure. Our
findings may be used in future studies to effectively eliminate anomalous
clients when building a model with diverse data.

    

### [[2108.04552] The Benefits of Implicit Regularization from SGD in Least Squares Problems](http://arxiv.org/abs/2108.04552)


  Stochastic gradient descent (SGD) exhibits strong algorithmic regularization
effects in practice, which has been hypothesized to play an important role in
the generalization of modern machine learning approaches. In this work, we seek
to understand these issues in the simpler setting of linear regression
(including both underparameterized and overparameterized regimes), where our
goal is to make sharp instance-based comparisons of the implicit regularization
afforded by (unregularized) average SGD with the explicit regularization of
ridge regression. For a broad class of least squares problem instances (that
are natural in high-dimensional settings), we show: (1) for every problem
instance and for every ridge parameter, (unregularized) SGD, when provided with
logarithmically more samples than that provided to the ridge algorithm,
generalizes no worse than the ridge solution (provided SGD uses a tuned
constant stepsize); (2) conversely, there exist instances (in this wide problem
class) where optimally-tuned ridge regression requires quadratically more
samples than SGD in order to have the same generalization performance. Taken
together, our results show that, up to the logarithmic factors, the
generalization performance of SGD is always no worse than that of ridge
regression in a wide range of overparameterized problems, and, in fact, could
be much better for some problem instances. More generally, our results show how
algorithmic regularization has important consequences even in simpler
(overparameterized) convex settings.

    

### [[2108.04578] Data Driven VRP: A Neural Network Model to Learn Hidden Preferences for VRP](http://arxiv.org/abs/2108.04578)


  The traditional Capacitated Vehicle Routing Problem (CVRP) minimizes the
total distance of the routes under the capacity constraints of the vehicles.
But more often, the objective involves multiple criteria including not only the
total distance of the tour but also other factors such as travel costs, travel
time, and fuel consumption.Moreover, in reality, there are numerous implicit
preferences ingrained in the minds of the route planners and the drivers.
Drivers, for instance, have familiarity with certain neighborhoods and
knowledge of the state of roads, and often consider the best places for rest
and lunch breaks. This knowledge is difficult to formulate and balance when
operational routing decisions have to be made. This motivates us to learn the
implicit preferences from past solutions and to incorporate these learned
preferences in the optimization process. These preferences are in the form of
arc probabilities, i.e., the more preferred a route is, the higher is the joint
probability. The novelty of this work is the use of a neural network model to
estimate the arc probabilities, which allows for additional features and
automatic parameter estimation. This first requires identifying suitable
features, neural architectures and loss functions, taking into account that
there is typically few data available. We investigate the difference with a
prior weighted Markov counting approach, and study the applicability of neural
networks in this setting.

    

### [[2108.04584] UniNet: A Unified Scene Understanding Network and Exploring Multi-Task Relationships through the Lens of Adversarial Attacks](http://arxiv.org/abs/2108.04584)


  Scene understanding is crucial for autonomous systems which intend to operate
in the real world. Single task vision networks extract information only based
on some aspects of the scene. In multi-task learning (MTL), on the other hand,
these single tasks are jointly learned, thereby providing an opportunity for
tasks to share information and obtain a more comprehensive understanding. To
this end, we develop UniNet, a unified scene understanding network that
accurately and efficiently infers vital vision tasks including object
detection, semantic segmentation, instance segmentation, monocular depth
estimation, and monocular instance depth prediction. As these tasks look at
different semantic and geometric information, they can either complement or
conflict with each other. Therefore, understanding inter-task relationships can
provide useful cues to enable complementary information sharing. We evaluate
the task relationships in UniNet through the lens of adversarial attacks based
on the notion that they can exploit learned biases and task interactions in the
neural network. Extensive experiments on the Cityscapes dataset, using
untargeted and targeted attacks reveal that semantic tasks strongly interact
amongst themselves, and the same holds for geometric tasks. Additionally, we
show that the relationship between semantic and geometric tasks is asymmetric
and their interaction becomes weaker as we move towards higher-level
representations.

    

### [[2108.04585] Recurrent neural network-based Internal Model Control of unknown nonlinear stable systems](http://arxiv.org/abs/2108.04585)


  Owing to their superior modeling capabilities, gated Recurrent Neural
Networks (RNNs), such as Gated Recurrent Units (GRUs) and Long Short-Term
Memory networks (LSTMs), have become popular tools for learning dynamical
systems. This paper aims to discuss how these networks can be adopted for the
synthesis of Internal Model Control (IMC) architectures. To this end, a first
gated RNN is used to learn a model of the unknown input-output stable plant.
Then, another gated RNN approximating the model inverse is trained. The
proposed scheme is able to cope with the saturation of the control variables,
and it can be deployed on low-power embedded controllers since it does not
require any online computation. The approach is then tested on the Quadruple
Tank benchmark system, resulting in satisfactory closed-loop performances.

    

### [[2108.04587] On Learning and Testing Decision Tree](http://arxiv.org/abs/2108.04587)


  In this paper, we study learning and testing decision tree of size and depth
that are significantly smaller than the number of attributes $n$.
Our main result addresses the problem of poly$(n,1/\epsilon)$ time algorithms
with poly$(s,1/\epsilon)$ query complexity (independent of $n$) that
distinguish between functions that are decision trees of size $s$ from
functions that are $\epsilon$-far from any decision tree of size
$\phi(s,1/\epsilon)$, for some function $\phi > s$. The best known result is
the recent one that follows from Blank, Lange and Tan,~\cite{BlancLT20}, that
gives $\phi(s,1/\epsilon)=2^{O((\log^3s)/\epsilon^3)}$. In this paper, we give
a new algorithm that achieves $\phi(s,1/\epsilon)=2^{O(\log^2 (s/\epsilon))}$.
Moreover, we study the testability of depth-$d$ decision tree and give a {\it
distribution free} tester that distinguishes between depth-$d$ decision tree
and functions that are $\epsilon$-far from depth-$d^2$ decision tree. In
particular, for decision trees of size $s$, the above result holds in the
distribution-free model when the tree depth is $O(\log(s/\epsilon))$.
We also give other new results in learning and testing of size-$s$ decision
trees and depth-$d$ decision trees that follow from results in the literature
and some results we prove in this paper.

    

### [[2108.04595] Label-informed Graph Structure Learning for Node Classification](http://arxiv.org/abs/2108.04595)


  Graph Neural Networks (GNNs) have achieved great success among various
domains. Nevertheless, most GNN methods are sensitive to the quality of graph
structures. To tackle this problem, some studies exploit different graph
structure learning strategies to refine the original graph structure. However,
these methods only consider feature information while ignoring available label
information. In this paper, we propose a novel label-informed graph structure
learning framework which incorporates label information explicitly through a
class transition matrix. We conduct extensive experiments on seven node
classification benchmark datasets and the results show that our method
outperforms or matches the state-of-the-art baselines.

    

### [[2108.04620] A proof of convergence for the gradient descent optimization method with random initializations in the training of neural networks with ReLU activation for piecewise linear target functions](http://arxiv.org/abs/2108.04620)


  Gradient descent (GD) type optimization methods are the standard instrument
to train artificial neural networks (ANNs) with rectified linear unit (ReLU)
activation. Despite the great success of GD type optimization methods in
numerical simulations for the training of ANNs with ReLU activation, it remains
- even in the simplest situation of the plain vanilla GD optimization method
with random initializations and ANNs with one hidden layer - an open problem to
prove (or disprove) the conjecture that the risk of the GD optimization method
converges in the training of such ANNs to zero as the width of the ANNs, the
number of independent random initializations, and the number of GD steps
increase to infinity. In this article we prove this conjecture in the situation
where the probability distribution of the input data is equivalent to the
continuous uniform distribution on a compact interval, where the probability
distributions for the random initializations of the ANN parameters are standard
normal distributions, and where the target function under consideration is
continuous and piecewise affine linear. Roughly speaking, the key ingredients
in our mathematical convergence analysis are (i) to prove that suitable sets of
global minima of the risk functions are \emph{twice continuously differentiable
submanifolds of the ANN parameter spaces}, (ii) to prove that the Hessians of
the risk functions on these sets of global minima satisfy an appropriate
\emph{maximal rank condition}, and, thereafter, (iii) to apply the machinery in
[Fehrman, B., Gess, B., Jentzen, A., Convergence rates for the stochastic
gradient descent method for non-convex objective functions. J. Mach. Learn.
Res. 21(136): 1--48, 2020] to establish convergence of the GD optimization
method with random initializations.

    

### [[2108.04623] Learning to Maximize Influence](http://arxiv.org/abs/2108.04623)


  As the field of machine learning for combinatorial optimization advances,
traditional problems are resurfaced and readdressed through this new
perspective. The overwhelming majority of the literature focuses on small graph
problems, while several real-world problems are devoted to large graphs. Here,
we focus on two such problems that are related: influence estimation, a
\#P-hard counting problem, and influence maximization, an NP-hard problem. We
develop GLIE, a Graph Neural Network (GNN) that inherently parameterizes an
upper bound of influence estimation and train it on small simulated graphs.
Experiments show that GLIE can provide accurate predictions faster than the
alternatives for graphs 10 times larger than the train set. More importantly,
it can be used on arbitrary large graphs for influence maximization, as the
predictions can rank effectively seed sets even when the accuracy deteriorates.
To showcase this, we propose a version of a standard Influence Maximization
(IM) algorithm where we substitute traditional influence estimation with the
predictions of GLIE.We also transfer GLIE into a reinforcement learning model
that learns how to choose seeds to maximize influence sequentially using GLIE's
hidden representations and predictions. The final results show that the
proposed methods surpasses a previous GNN-RL approach and perform on par with a
state-of-the-art IM algorithm.

    

### [[2108.04655] Hierarchical Latent Relation Modeling for Collaborative Metric Learning](http://arxiv.org/abs/2108.04655)


  Collaborative Metric Learning (CML) recently emerged as a powerful paradigm
for recommendation based on implicit feedback collaborative filtering. However,
standard CML methods learn fixed user and item representations, which fails to
capture the complex interests of users. Existing extensions of CML also either
ignore the heterogeneity of user-item relations, i.e. that a user can
simultaneously like very different items, or the latent item-item relations,
i.e. that a user's preference for an item depends, not only on its intrinsic
characteristics, but also on items they previously interacted with. In this
paper, we present a hierarchical CML model that jointly captures latent
user-item and item-item relations from implicit data. Our approach is inspired
by translation mechanisms from knowledge graph embedding and leverages
memory-based attention networks. We empirically show the relevance of this
joint relational modeling, by outperforming existing CML models on
recommendation tasks on several real-world datasets. Our experiments also
emphasize the limits of current CML relational models on very sparse datasets.

    

### [[2108.04656] Empirical Analysis on Effectiveness of NLP Methods for Predicting Code Smell](http://arxiv.org/abs/2108.04656)


  A code smell is a surface indicator of an inherent problem in the system,
most often due to deviation from standard coding practices on the developers
part during the development phase. Studies observe that code smells made the
code more susceptible to call for modifications and corrections than code that
did not contain code smells. Restructuring the code at the early stage of
development saves the exponentially increasing amount of effort it would
require to address the issues stemming from the presence of these code smells.
Instead of using traditional features to detect code smells, we use user
comments to manually construct features to predict code smells. We use three
Extreme learning machine kernels over 629 packages to identify eight code
smells by leveraging feature engineering aspects and using sampling techniques.
Our findings indicate that the radial basis functional kernel performs best out
of the three kernel methods with a mean accuracy of 98.52.

    

### [[2108.04658] U-Net-and-a-half: Convolutional network for biomedical image segmentation using multiple expert-driven annotations](http://arxiv.org/abs/2108.04658)


  Development of deep learning systems for biomedical segmentation often
requires access to expert-driven, manually annotated datasets. If more than a
single expert is involved in the annotation of the same images, then the
inter-expert agreement is not necessarily perfect, and no single expert
annotation can precisely capture the so-called ground truth of the regions of
interest on all images. Also, it is not trivial to generate a reference
estimate using annotations from multiple experts. Here we present a deep neural
network, defined as U-Net-and-a-half, which can simultaneously learn from
annotations performed by multiple experts on the same set of images.
U-Net-and-a-half contains a convolutional encoder to generate features from the
input images, multiple decoders that allow simultaneous learning from image
masks obtained from annotations that were independently generated by multiple
experts, and a shared low-dimensional feature space. To demonstrate the
applicability of our framework, we used two distinct datasets from digital
pathology and radiology, respectively. Specifically, we trained two separate
models using pathologist-driven annotations of glomeruli on whole slide images
of human kidney biopsies (10 patients), and radiologist-driven annotations of
lumen cross-sections of human arteriovenous fistulae obtained from
intravascular ultrasound images (10 patients), respectively. The models based
on U-Net-and-a-half exceeded the performance of the traditional U-Net models
trained on single expert annotations alone, thus expanding the scope of
multitask learning in the context of biomedical image segmentation.

    

### [[2108.04659] An Empirical Study on Predictability of Software Code Smell Using Deep Learning Models](http://arxiv.org/abs/2108.04659)


  Code Smell, similar to a bad smell, is a surface indication of something
tainted but in terms of software writing practices. This metric is an
indication of a deeper problem lies within the code and is associated with an
issue which is prominent to experienced software developers with acceptable
coding practices. Recent studies have often observed that codes having code
smells are often prone to a higher probability of change in the software
development cycle. In this paper, we developed code smell prediction models
with the help of features extracted from source code to predict eight types of
code smell. Our work also presents the application of data sampling techniques
to handle class imbalance problem and feature selection techniques to find
relevant feature sets. Previous studies had made use of techniques such as
Naive - Bayes and Random forest but had not explored deep learning methods to
predict code smell. A total of 576 distinct Deep Learning models were trained
using the features and datasets mentioned above. The study concluded that the
deep learning models which used data from Synthetic Minority Oversampling
Technique gave better results in terms of accuracy, AUC with the accuracy of
some models improving from 88.47 to 96.84.

    

### [[2108.04682] ChemiRise: a data-driven retrosynthesis engine](http://arxiv.org/abs/2108.04682)


  We have developed an end-to-end, retrosynthesis system, named ChemiRise, that
can propose complete retrosynthesis routes for organic compounds rapidly and
reliably. The system was trained on a processed patent database of over 3
million organic reactions. Experimental reactions were atom-mapped, clustered,
and extracted into reaction templates. We then trained a graph convolutional
neural network-based one-step reaction proposer using template embeddings and
developed a guiding algorithm on the directed acyclic graph (DAG) of chemical
compounds to find the best candidate to explore. The atom-mapping algorithm and
the one-step reaction proposer were benchmarked against previous studies and
showed better results. The final product was demonstrated by retrosynthesis
routes reviewed and rated by human experts, showing satisfying functionality
and a potential productivity boost in real-life use cases.

    

### [[2108.04698] Active Learning for Transition State Calculation](http://arxiv.org/abs/2108.04698)


  The transition state (TS) calculation is a grand challenge for computational
intensive energy function. The traditional methods need to evaluate the
gradients of the energy function at a very large number of locations. To reduce
the number of expensive computations of the true gradients, we propose an
active learning framework consisting of a statistical surrogate model, Gaussian
process regression (GPR) for the energy function, and a single-walker dynamics
method, gentle accent dynamics (GAD), for the saddle-type transition states. TS
is detected by the GAD applied to the GPR surrogate for the gradient vector and
the Hessian matrix. Our key ingredient for efficiency improvements is an active
learning method which sequentially designs the most informative locations and
takes evaluations of the original model at these locations to train GPR. We
formulate this active learning task as the optimal experimental design problem
and propose a very efficient sample-based sub-optimal criterion to construct
the optimal locations. We show that the new method significantly decreases the
required number of energy or force evaluations of the original model.

    

### [[2108.04725] PRECODE - A Generic Model Extension to Prevent Deep Gradient Leakage](http://arxiv.org/abs/2108.04725)


  Collaborative training of neural networks leverages distributed data by
exchanging gradient information between different clients. Although training
data entirely resides with the clients, recent work shows that training data
can be reconstructed from such exchanged gradient information. To enhance
privacy, gradient perturbation techniques have been proposed. However, they
come at the cost of reduced model performance, increased convergence time, or
increased data demand. In this paper, we introduce PRECODE, a PRivacy EnhanCing
mODulE that can be used as generic extension for arbitrary model architectures.
We propose a simple yet effective realization of PRECODE using variational
modeling. The stochastic sampling induced by variational modeling effectively
prevents privacy leakage from gradients and in turn preserves privacy of data
owners. We evaluate PRECODE using state of the art gradient inversion attacks
on two different model architectures trained on three datasets. In contrast to
commonly used defense mechanisms, we find that our proposed modification
consistently reduces the attack success rate to 0% while having almost no
negative impact on model training and final performance. As a result, PRECODE
reveals a promising path towards privacy enhancing model extensions.

    

### [[2108.04727] Crowdsourced Databases and Sui Generis Rights](http://arxiv.org/abs/2108.04727)


  In this study we propose a new concept of databases (crowdsourced databases),
adding a new conceptual approach to the debate on legal protection of databases
in Europe. We also summarise the current legal framework and current indexing
and web scraping practices - it would not be prudent to suggest a new theory
without contextualising it in the legal and practical context in which it is
developed.

    

### [[2108.04729] Correlation Clustering Reconstruction in Semi-Adversarial Models](http://arxiv.org/abs/2108.04729)


  Correlation Clustering is an important clustering problem with many
applications. We study the reconstruction version of this problem in which one
is seeking to reconstruct a latent clustering that has been corrupted by random
noise and adversarial modifications.
Concerning the latter, we study a standard "post-adversarial" model, in which
adversarial modifications come after the noise, and also introduce and analyze
a "pre-adversarial" model in which adversarial modifications come before the
noise. Given an input coming from such a semi-adversarial generative model, the
goal is to reconstruct almost perfectly and with high probability the latent
clustering.
We focus on the case where the hidden clusters have equal size and show the
following. In the pre-adversarial setting, spectral algorithms are optimal, in
the sense that they reconstruct all the way to the information-theoretic
threshold beyond which no reconstruction is possible. In contrast, in the
post-adversarial setting their ability to restore the hidden clusters stops
before the threshold, but the gap is optimally filled by SDP-based algorithms.

    

### [[2108.04741] Multi-Factors Aware Dual-Attentional Knowledge Tracing](http://arxiv.org/abs/2108.04741)


  With the increasing demands of personalized learning, knowledge tracing has
become important which traces students' knowledge states based on their
historical practices. Factor analysis methods mainly use two kinds of factors
which are separately related to students and questions to model students'
knowledge states. These methods use the total number of attempts of students to
model students' learning progress and hardly highlight the impact of the most
recent relevant practices. Besides, current factor analysis methods ignore rich
information contained in questions. In this paper, we propose Multi-Factors
Aware Dual-Attentional model (MF-DAKT) which enriches question representations
and utilizes multiple factors to model students' learning progress based on a
dual-attentional mechanism. More specifically, we propose a novel
student-related factor which records the most recent attempts on relevant
concepts of students to highlight the impact of recent exercises. To enrich
questions representations, we use a pre-training method to incorporate two
kinds of question information including questions' relation and difficulty
level. We also add a regularization term about questions' difficulty level to
restrict pre-trained question representations to fine-tuning during the process
of predicting students' performance. Moreover, we apply a dual-attentional
mechanism to differentiate contributions of factors and factor interactions to
final prediction in different practice records. At last, we conduct experiments
on several real-world datasets and results show that MF-DAKT can outperform
existing knowledge tracing methods. We also conduct several studies to validate
the effects of each component of MF-DAKT.

    

### [[2108.04742] The information of attribute uncertainties: what convolutional neural networks can learn about errors in input data](http://arxiv.org/abs/2108.04742)


  Errors in measurements are key to weighting the value of data, but are often
neglected in Machine Learning (ML). We show how Convolutional Neural Networks
(CNNs) are able to learn about the context and patterns of signal and noise,
leading to improvements in the performance of classification methods. We
construct a model whereby two classes of objects follow an underlying Gaussian
distribution, and where the features (the input data) have varying, but known,
levels of noise. This model mimics the nature of scientific data sets, where
the noises arise as realizations of some random processes whose underlying
distributions are known. The classification of these objects can then be
performed using standard statistical techniques (e.g., least-squares
minimization or Markov-Chain Monte Carlo), as well as ML techniques. This
allows us to take advantage of a maximum likelihood approach to object
classification, and to measure the amount by which the ML methods are
incorporating the information in the input data uncertainties. We show that,
when each data point is subject to different levels of noise (i.e., noises with
different distribution functions), that information can be learned by the CNNs,
raising the ML performance to at least the same level of the least-squares
method -- and sometimes even surpassing it. Furthermore, we show that, with
varying noise levels, the confidence of the ML classifiers serves as a proxy
for the underlying cumulative distribution function, but only if the
information about specific input data uncertainties is provided to the CNNs.

    

### [[2108.04755] FedPAGE: A Fast Local Stochastic Gradient Method for Communication-Efficient Federated Learning](http://arxiv.org/abs/2108.04755)


  Federated Averaging (FedAvg, also known as Local-SGD) (McMahan et al., 2017)
is a classical federated learning algorithm in which clients run multiple local
SGD steps before communicating their update to an orchestrating server. We
propose a new federated learning algorithm, FedPAGE, able to further reduce the
communication complexity by utilizing the recent optimal PAGE method (Li et
al., 2021) instead of plain SGD in FedAvg. We show that FedPAGE uses much fewer
communication rounds than previous local methods for both federated convex and
nonconvex optimization. Concretely, 1) in the convex setting, the number of
communication rounds of FedPAGE is $O(\frac{N^{3/4}}{S\epsilon})$, improving
the best-known result $O(\frac{N}{S\epsilon})$ of SCAFFOLD (Karimireddy et
al.,2020) by a factor of $N^{1/4}$, where $N$ is the total number of clients
(usually is very large in federated learning), $S$ is the sampled subset of
clients in each communication round, and $\epsilon$ is the target error; 2) in
the nonconvex setting, the number of communication rounds of FedPAGE is
$O(\frac{\sqrt{N}+S}{S\epsilon^2})$, improving the best-known result
$O(\frac{N^{2/3}}{S^{2/3}\epsilon^2})$ of SCAFFOLD (Karimireddy et al.,2020) by
a factor of $N^{1/6}S^{1/3}$, if the sampled clients $S\leq \sqrt{N}$. Note
that in both settings, the communication cost for each round is the same for
both FedPAGE and SCAFFOLD. As a result, FedPAGE achieves new state-of-the-art
results in terms of communication complexity for both federated convex and
nonconvex optimization.

    

### [[2108.04763] Imitation Learning by Reinforcement Learning](http://arxiv.org/abs/2108.04763)


  Imitation Learning algorithms learn a policy from demonstrations of expert
behavior. Somewhat counterintuitively, we show that, for deterministic experts,
imitation learning can be done by reduction to reinforcement learning, which is
commonly considered more difficult. We conduct experiments which confirm that
our reduction works well in practice for a continuous control task.

    

### [[2108.04782] Bandit Algorithms for Precision Medicine](http://arxiv.org/abs/2108.04782)


  The Oxford English Dictionary defines precision medicine as "medical care
designed to optimize efficiency or therapeutic benefit for particular groups of
patients, especially by using genetic or molecular profiling." It is not an
entirely new idea: physicians from ancient times have recognized that medical
treatment needs to consider individual variations in patient characteristics.
However, the modern precision medicine movement has been enabled by a
confluence of events: scientific advances in fields such as genetics and
pharmacology, technological advances in mobile devices and wearable sensors,
and methodological advances in computing and data sciences.
This chapter is about bandit algorithms: an area of data science of special
relevance to precision medicine. With their roots in the seminal work of
Bellman, Robbins, Lai and others, bandit algorithms have come to occupy a
central place in modern data science ( Lattimore and Szepesvari, 2020). Bandit
algorithms can be used in any situation where treatment decisions need to be
made to optimize some health outcome. Since precision medicine focuses on the
use of patient characteristics to guide treatment, contextual bandit algorithms
are especially useful since they are designed to take such information into
account. The role of bandit algorithms in areas of precision medicine such as
mobile health and digital phenotyping has been reviewed before (Tewari and
Murphy, 2017; Rabbi et al., 2019). Since these reviews were published, bandit
algorithms have continued to find uses in mobile health and several new topics
have emerged in the research on bandit algorithms. This chapter is written for
quantitative researchers in fields such as statistics, machine learning, and
operations research who might be interested in knowing more about the
algorithmic and mathematical details of bandit algorithms that have been used
in mobile health.

    

### [[2108.04787] Analyzing Effects of The COVID-19 Pandemic on Road Traffic Safety: The Cases of New York City, Los Angeles, and Boston](http://arxiv.org/abs/2108.04787)


  The COVID-19 pandemic has resulted in significant social and economic impacts
throughout the world. In addition to the health consequences, the impacts on
traffic behaviors have also been sudden and dramatic. We have analyzed how the
road traffic safety of New York City, Los Angeles, and Boston in the U.S. have
been impacted by the pandemic and corresponding local government orders and
restrictions. To be specific, we have studied the accident hotspots'
distributions before and after the outbreak of the pandemic and found that
traffic accidents have shifted in both location and time compared to previous
years. In addition, we have studied the road network characteristics in those
hotspot regions with the hope to understand the underlying cause of the hotspot
shifts.

    

### [[2108.04800] Meta-repository of screening mammography classifiers](http://arxiv.org/abs/2108.04800)


  Artificial intelligence (AI) is transforming medicine and showing promise in
improving clinical diagnosis. In breast cancer screening, several recent
studies show that AI has the potential to improve radiologists' accuracy,
subsequently helping in early cancer diagnosis and reducing unnecessary workup.
As the number of proposed models and their complexity grows, it is becoming
increasingly difficult to re-implement them in order to reproduce the results
and to compare different approaches. To enable reproducibility of research in
this application area and to enable comparison between different methods, we
release a meta-repository containing deep learning models for classification of
screening mammograms. This meta-repository creates a framework that enables the
evaluation of machine learning models on any private or public screening
mammography data set. At its inception, our meta-repository contains five
state-of-the-art models with open-source implementations and cross-platform
compatibility. We compare their performance on five international data sets:
two private New York University breast cancer screening data sets as well as
three public (DDSM, INbreast and Chinese Mammography Database) data sets. Our
framework has a flexible design that can be generalized to other medical image
analysis tasks. The meta-repository is available at
this https URL.

    

### [[2108.04809] Spiderweb nanomechanical resonators via Bayesian optimization: inspired by nature and guided by machine learning](http://arxiv.org/abs/2108.04809)


  From ultra-sensitive detectors of fundamental forces to quantum networks and
sensors, mechanical resonators are enabling next-generation technologies to
operate in room temperature environments. Currently, silicon nitride
nanoresonators stand as a leading microchip platform in these advances by
allowing for mechanical resonators whose motion is remarkably isolated from
ambient thermal noise. However, to date, human intuition has remained the
driving force behind design processes. Here, inspired by nature and guided by
machine learning, a spiderweb nanomechanical resonator is developed that
exhibits vibration modes which are isolated from ambient thermal environments
via a novel "torsional soft-clamping" mechanism discovered by the data-driven
optimization algorithm. This bio-inspired resonator is then fabricated;
experimentally confirming a new paradigm in mechanics with quality factors
above 1 billion in room temperature environments. In contrast to other
state-of-the-art resonators, this milestone is achieved with a compact design
which does not require sub-micron lithographic features or complex phononic
bandgaps, making it significantly easier and cheaper to manufacture at large
scales. Here we demonstrate the ability of machine learning to work in tandem
with human intuition to augment creative possibilities and uncover new
strategies in computing and nanotechnology.

    

### [[2108.04811] Binary Complex Neural Network Acceleration on FPGA](http://arxiv.org/abs/2108.04811)


  Being able to learn from complex data with phase information is imperative
for many signal processing applications. Today' s real-valued deep neural
networks (DNNs) have shown efficiency in latent information analysis but fall
short when applied to the complex domain. Deep complex networks (DCN), in
contrast, can learn from complex data, but have high computational costs;
therefore, they cannot satisfy the instant decision-making requirements of many
deployable systems dealing with short observations or short signal bursts.
Recent, Binarized Complex Neural Network (BCNN), which integrates DCNs with
binarized neural networks (BNN), shows great potential in classifying complex
data in real-time. In this paper, we propose a structural pruning based
accelerator of BCNN, which is able to provide more than 5000 frames/s inference
throughput on edge devices. The high performance comes from both the algorithm
and hardware sides. On the algorithm side, we conduct structural pruning to the
original BCNN models and obtain 20 $\times$ pruning rates with negligible
accuracy loss; on the hardware side, we propose a novel 2D convolution
operation accelerator for the binary complex neural network. Experimental
results show that the proposed design works with over 90% utilization and is
able to achieve the inference throughput of 5882 frames/s and 4938 frames/s for
complex NIN-Net and ResNet-18 using CIFAR-10 dataset and Alveo U280 Board.

    

### [[2108.04812] Continual Learning for Grounded Instruction Generation by Observing Human Following Behavior](http://arxiv.org/abs/2108.04812)


  We study continual learning for natural language instruction generation, by
observing human users' instruction execution. We focus on a collaborative
scenario, where the system both acts and delegates tasks to human users using
natural language. We compare user execution of generated instructions to the
original system intent as an indication to the system's success communicating
its intent. We show how to use this signal to improve the system's ability to
generate instructions via contextual bandit learning. In interaction with real
users, our system demonstrates dramatic improvements in its ability to generate
language over time.

    

### [[2108.04814] R4Dyn: Exploring Radar for Self-Supervised Monocular Depth Estimation of Dynamic Scenes](http://arxiv.org/abs/2108.04814)


  While self-supervised monocular depth estimation in driving scenarios has
achieved comparable performance to supervised approaches, violations of the
static world assumption can still lead to erroneous depth predictions of
traffic participants, posing a potential safety issue. In this paper, we
present R4Dyn, a novel set of techniques to use cost-efficient radar data on
top of a self-supervised depth estimation framework. In particular, we show how
radar can be used during training as weak supervision signal, as well as an
extra input to enhance the estimation robustness at inference time. Since
automotive radars are readily available, this allows to collect training data
from a variety of existing vehicles. Moreover, by filtering and expanding the
signal to make it compatible with learning-based approaches, we address radar
inherent issues, such as noise and sparsity. With R4Dyn we are able to overcome
a major limitation of self-supervised depth estimation, i.e. the prediction of
traffic participants. We substantially improve the estimation on dynamic
objects, such as cars by 37% on the challenging nuScenes dataset, hence
demonstrating that radar is a valuable additional sensor for monocular depth
estimation in autonomous vehicles. Additionally, we plan on making the code
publicly available.

    

### [[1808.00560] Novel Compressible Adaptive Spectral Mixture Kernels for Gaussian Processes with Sparse Time and Phase Delay Structures](http://arxiv.org/abs/1808.00560)


  Spectral mixture (SM) kernels comprise a powerful class of kernels for
Gaussian processes (GPs) capable of discovering structurally complex patterns
and modeling negative covariances. Being a linear superposition of
quasi-periodical kernel components, the state-of-the-art SM kernel does not
consider component compression and dependency structures between components. In
this paper, we investigate the benefits of component compression and modeling
of both time and phase delay structures between basis components in the SM
kernel. By verifying the presence of dependencies between function components
using Gaussian conditionals and posterior covariance, we first propose a new SM
kernel variant with a time and phase delay dependency structure (SMD) and then
provide a structure adaptation (SA) algorithm for the SMD. The SMD kernel is
constructed in two steps: first, time delay and phase delay are incorporated
into each basis component; next, cross-convolution between a basis component
and the reversed complex conjugate of another basis component is performed,
which yields a complex-valued and positive definite kernel incorporating
dependency structures between basis components. The model compression and
dependency sparsity of the SMD kernel can be obtained by using automatic
pruning in SA. We perform a thorough comparative experimental analysis of the
SMD on both synthetic and real-life datasets. The results corroborate the
efficacy of the dependency structure and SA in the SMD.

    

### [[1908.04741] Tensor-based computation of metastable and coherent sets](http://arxiv.org/abs/1908.04741)


  Recent years have seen rapid advances in the data-driven analysis of
dynamical systems based on Koopman operator theory and related approaches. On
the other hand, low-rank tensor product approximations -- in particular the
tensor train (TT) format -- have become a valuable tool for the solution of
large-scale problems in a number of fields. In this work, we combine
Koopman-based models and the TT format, enabling their application to
high-dimensional problems in conjunction with a rich set of basis functions or
features. We derive efficient algorithms to obtain a reduced matrix
representation of the system's evolution operator starting from an appropriate
low-rank representation of the data. These algorithms can be applied to both
stationary and non-stationary systems. We establish the infinite-data limit of
these matrix representations, and demonstrate our methods' capabilities using
several benchmark data sets.

    

### [[1910.14442] Interactive Gibson Benchmark (iGibson 0.5): A Benchmark for Interactive Navigation in Cluttered Environments](http://arxiv.org/abs/1910.14442)


  We present Interactive Gibson Benchmark, the first comprehensive benchmark
for training and evaluating Interactive Navigation: robot navigation strategies
where physical interaction with objects is allowed and even encouraged to
accomplish a task. For example, the robot can move objects if needed in order
to clear a path leading to the goal location. Our benchmark comprises two novel
elements: 1) a new experimental setup, the Interactive Gibson Environment
(iGibson 0.5), which simulates high fidelity visuals of indoor scenes, and high
fidelity physical dynamics of the robot and common objects found in these
scenes; 2) a set of Interactive Navigation metrics which allows one to study
the interplay between navigation and physical interaction. We present and
evaluate multiple learning-based baselines in Interactive Gibson, and provide
insights into regimes of navigation with different trade-offs between
navigation path efficiency and disturbance of surrounding objects. We make our
benchmark publicly
available(this https URL) and encourage
researchers from all disciplines in robotics (e.g. planning, learning, control)
to propose, evaluate, and compare their Interactive Navigation solutions in
Interactive Gibson.

    

### [[2002.05259] Learning to Generate Levels From Nothing](http://arxiv.org/abs/2002.05259)


  Machine learning for procedural content generation has recently become an
active area of research. Levels vary in both form and function and are mostly
unrelated to each other across games. This has made it difficult to assemble
suitably large datasets to bring machine learning to level design in the same
way as it's been used for image generation. Here we propose Generative Playing
Networks which design levels for itself to play. The algorithm is built in two
parts; an agent that learns to play game levels, and a generator that learns
the distribution of playable levels. As the agent learns and improves its
ability, the space of playable levels, as defined by the agent, grows. The
generator targets the agent's playability estimates to then update its
understanding of what constitutes a playable level. We call this process of
learning the distribution of data found through self-discovery with an
environment, self-supervised inductive learning. Unlike previous approaches to
procedural content generation, Generative Playing Networks are end-to-end
differentiable and do not require human-designed examples or domain knowledge.
We demonstrate the capability of this framework by training an agent and level
generator for a 2D dungeon crawler game.

    

### [[2003.04575] GPCA: A Probabilistic Framework for Gaussian Process Embedded Channel Attention](http://arxiv.org/abs/2003.04575)


  Channel attention mechanisms have been commonly applied in many visual tasks
for effective performance improvement. It is able to reinforce the informative
channels as well as to suppress the useless channels. Recently, different
channel attention modules have been proposed and implemented in various ways.
Generally speaking, they are mainly based on convolution and pooling
operations. In this paper, we propose Gaussian process embedded channel
attention (GPCA) module and further interpret the channel attention schemes in
a probabilistic way. The GPCA module intends to model the correlations among
the channels, which are assumed to be captured by beta distributed variables.
As the beta distribution cannot be integrated into the end-to-end training of
convolutional neural networks (CNNs) with a mathematically tractable solution,
we utilize an approximation of the beta distribution to solve this problem. To
specify, we adapt a Sigmoid-Gaussian approximation, in which the Gaussian
distributed variables are transferred into the interval [0,1]. The Gaussian
process is then utilized to model the correlations among different channels. In
this case, a mathematically tractable solution is derived. The GPCA module can
be efficiently implemented and integrated into the end-to-end training of the
CNNs. Experimental results demonstrate the promising performance of the
proposed GPCA module. Codes are available at this https URL.

    

### [[2005.14408] Noise Robust Named Entity Understanding for Voice Assistants](http://arxiv.org/abs/2005.14408)


  Named Entity Recognition (NER) and Entity Linking (EL) play an essential role
in voice assistant interaction, but are challenging due to the special
difficulties associated with spoken user queries. In this paper, we propose a
novel architecture that jointly solves the NER and EL tasks by combining them
in a joint reranking module. We show that our proposed framework improves NER
accuracy by up to 3.13% and EL accuracy by up to 3.6% in F1 score. The features
used also lead to better accuracies in other natural language understanding
tasks, such as domain classification and semantic parsing.

    

### [[2006.04896] A Baseline for Shapley Values in MLPs: from Missingness to Neutrality](http://arxiv.org/abs/2006.04896)


  Deep neural networks have gained momentum based on their accuracy, but their
interpretability is often criticised. As a result, they are labelled as black
boxes. In response, several methods have been proposed in the literature to
explain their predictions. Among the explanatory methods, Shapley values is a
feature attribution method favoured for its robust theoretical foundation.
However, the analysis of feature attributions using Shapley values requires
choosing a baseline that represents the concept of missingness. An arbitrary
choice of baseline could negatively impact the explanatory power of the method
and possibly lead to incorrect interpretations. In this paper, we present a
method for choosing a baseline according to a neutrality value: as a parameter
selected by decision-makers, the point at which their choices are determined by
the model predictions being either above or below it. Hence, the proposed
baseline is set based on a parameter that depends on the actual use of the
model. This procedure stands in contrast to how other baselines are set, i.e.
without accounting for how the model is used. We empirically validate our
choice of baseline in the context of binary classification tasks, using two
datasets: a synthetic dataset and a dataset derived from the financial domain.

    

### [[2006.06731] Bandits with Partially Observable Confounded Data](http://arxiv.org/abs/2006.06731)


  We study linear contextual bandits with access to a large, confounded,
offline dataset that was sampled from some fixed policy. We show that this
problem is closely related to a variant of the bandit problem with side
information. We construct a linear bandit algorithm that takes advantage of the
projected information, and prove regret bounds. Our results demonstrate the
ability to take advantage of confounded offline data. Particularly, we prove
regret bounds that improve current bounds by a factor related to the visible
dimensionality of the contexts in the data. Our results indicate that
confounded offline data can significantly improve online learning algorithms.
Finally, we demonstrate various characteristics of our approach through
synthetic simulations.

    

### [[2006.09009] Provable Training Set Debugging for Linear Regression](http://arxiv.org/abs/2006.09009)


  We investigate problems in penalized $M$-estimation, inspired by applications
in machine learning debugging. Data are collected from two pools, one
containing data with possibly contaminated labels, and the other which is known
to contain only cleanly labeled points. We first formulate a general
statistical algorithm for identifying buggy points and provide rigorous
theoretical guarantees under the assumption that the data follow a linear
model. We then present two case studies to illustrate the results of our
general theory and the dependence of our estimator on clean versus buggy
points. We further propose an algorithm for tuning parameter selection of our
Lasso-based algorithm and provide corresponding theoretical guarantees.
Finally, we consider a two-person "game" played between a bug generator and a
debugger, where the debugger can augment the contaminated data set with cleanly
labeled versions of points in the original data pool. We establish a
theoretical result showing a sufficient condition under which the bug generator
can always fool the debugger. Nonetheless, we provide empirical results showing
that such a situation may not occur in practice, making it possible for natural
augmentation strategies combined with our Lasso debugging algorithm to succeed.

    

### [[2006.11890] Graph Backdoor](http://arxiv.org/abs/2006.11890)


  One intriguing property of deep neural networks (DNNs) is their inherent
vulnerability to backdoor attacks -- a trojan model responds to
trigger-embedded inputs in a highly predictable manner while functioning
normally otherwise. Despite the plethora of prior work on DNNs for continuous
data (e.g., images), the vulnerability of graph neural networks (GNNs) for
discrete-structured data (e.g., graphs) is largely unexplored, which is highly
concerning given their increasing use in security-sensitive domains. To bridge
this gap, we present GTA, the first backdoor attack on GNNs. Compared with
prior work, GTA departs in significant ways: graph-oriented -- it defines
triggers as specific subgraphs, including both topological structures and
descriptive features, entailing a large design spectrum for the adversary;
input-tailored -- it dynamically adapts triggers to individual graphs, thereby
optimizing both attack effectiveness and evasiveness; downstream model-agnostic
-- it can be readily launched without knowledge regarding downstream models or
fine-tuning strategies; and attack-extensible -- it can be instantiated for
both transductive (e.g., node classification) and inductive (e.g., graph
classification) tasks, constituting severe threats for a range of
security-critical applications. Through extensive evaluation using benchmark
datasets and state-of-the-art models, we demonstrate the effectiveness of GTA.
We further provide analytical justification for its effectiveness and discuss
potential countermeasures, pointing to several promising research directions.

    

### [[2008.07588] Uncertainty Quantification using Variational Inference for Biomedical Image Segmentation](http://arxiv.org/abs/2008.07588)


  Deep learning motivated by convolutional neural networks has been highly
successful in a range of medical imaging problems like image classification,
image segmentation, image synthesis etc. However for validation and
interpretability, not only do we need the predictions made by the model but
also how confident it is while making those predictions. This is important in
safety critical applications for the people to accept it. In this work, we used
an encoder decoder architecture based on variational inference techniques for
segmenting brain tumour images. We evaluate our work on the publicly available
BRATS dataset using Dice Similarity Coefficient (DSC) and Intersection Over
Union (IOU) as the evaluation metrics. Our model is able to segment brain
tumours while taking into account both aleatoric uncertainty and epistemic
uncertainty in a principled bayesian manner.

    

### [[2009.05673] Applications of Deep Neural Networks](http://arxiv.org/abs/2009.05673)


  Deep learning is a group of exciting new technologies for neural networks.
Through a combination of advanced training techniques and neural network
architectural components, it is now possible to create neural networks that can
handle tabular data, images, text, and audio as both input and output. Deep
learning allows a neural network to learn hierarchies of information in a way
that is like the function of the human brain. This course will introduce the
student to classic neural network structures, Convolution Neural Networks
(CNN), Long Short-Term Memory (LSTM), Gated Recurrent Neural Networks (GRU),
General Adversarial Networks (GAN), and reinforcement learning. Application of
these architectures to computer vision, time series, security, natural language
processing (NLP), and data generation will be covered. High-Performance
Computing (HPC) aspects will demonstrate how deep learning can be leveraged
both on graphical processing units (GPUs), as well as grids. Focus is primarily
upon the application of deep learning to problems, with some introduction to
mathematical foundations. Readers will use the Python programming language to
implement deep learning using Google TensorFlow and Keras. It is not necessary
to know Python prior to this book; however, familiarity with at least one
programming language is assumed.

    

### [[2009.06429] Into the Unknown: Active Monitoring of Neural Networks](http://arxiv.org/abs/2009.06429)


  Neural-network classifiers achieve high accuracy when predicting the class of
an input that they were trained to identify. Maintaining this accuracy in
dynamic environments, where inputs frequently fall outside the fixed set of
initially known classes, remains a challenge. The typical approach is to detect
inputs from novel classes and retrain the classifier on an augmented dataset.
However, not only the classifier but also the detection mechanism needs to
adapt in order to distinguish between newly learned and yet unknown input
classes. To address this challenge, we introduce an algorithmic framework for
active monitoring of a neural network. A monitor wrapped in our framework
operates in parallel with the neural network and interacts with a human user
via a series of interpretable labeling queries for incremental adaptation. In
addition, we propose an adaptive quantitative monitor to improve precision. An
experimental evaluation on a diverse set of benchmarks with varying numbers of
classes confirms the benefits of our active monitoring framework in dynamic
scenarios.

    

### [[2010.01184] Effective Sample Size, Dimensionality, and Generalization in Covariate Shift Adaptation](http://arxiv.org/abs/2010.01184)


  In supervised learning, training and test datasets are often sampled from
distinct distributions. Domain adaptation techniques are thus required.
Covariate shift adaptation yields good generalization performance when domains
differ only by the marginal distribution of features. Covariate shift
adaptation is usually implemented using importance weighting, which may fail,
according to common wisdom, due to small effective sample sizes (ESS). Previous
research argues this scenario is more common in high-dimensional settings.
However, how effective sample size, dimensionality, and model
performance/generalization are formally related in supervised learning,
considering the context of covariate shift adaptation, is still somewhat
obscure in the literature. Thus, a main challenge is presenting a unified
theory connecting those points. Hence, in this paper, we focus on building a
unified view connecting the ESS, data dimensionality, and generalization in the
context of covariate shift adaptation. Moreover, we also demonstrate how
dimensionality reduction or feature selection can increase the ESS, and argue
that our results support dimensionality reduction before covariate shift
adaptation as a good practice.

    

### [[2010.05244] Advanced Dropout: A Model-free Methodology for Bayesian Dropout Optimization](http://arxiv.org/abs/2010.05244)


  Due to lack of data, overfitting ubiquitously exists in real-world
applications of deep neural networks (DNNs). We propose advanced dropout, a
model-free methodology, to mitigate overfitting and improve the performance of
DNNs. The advanced dropout technique applies a model-free and easily
implemented distribution with parametric prior, and adaptively adjusts dropout
rate. Specifically, the distribution parameters are optimized by stochastic
gradient variational Bayes in order to carry out an end-to-end training. We
evaluate the effectiveness of the advanced dropout against nine dropout
techniques on seven computer vision datasets (five small-scale datasets and two
large-scale datasets) with various base models. The advanced dropout
outperforms all the referred techniques on all the datasets.We further compare
the effectiveness ratios and find that advanced dropout achieves the highest
one on most cases. Next, we conduct a set of analysis of dropout rate
characteristics, including convergence of the adaptive dropout rate, the
learned distributions of dropout masks, and a comparison with dropout rate
generation without an explicit distribution. In addition, the ability of
overfitting prevention is evaluated and confirmed. Finally, we extend the
application of the advanced dropout to uncertainty inference, network pruning,
text classification, and regression. The proposed advanced dropout is also
superior to the corresponding referred methods. Codes are available at
this https URL.

    

### [[2011.02829] Deep tree-ensembles for multi-output prediction](http://arxiv.org/abs/2011.02829)


  Recently, deep neural networks have expanded the state-of-art in various
scientific fields and provided solutions to long standing problems across
multiple application domains. Nevertheless, they also suffer from weaknesses
since their optimal performance depends on massive amounts of training data and
the tuning of an extended number of parameters. As a countermeasure, some
deep-forest methods have been recently proposed, as efficient and low-scale
solutions. Despite that, these approaches simply employ label classification
probabilities as induced features and primarily focus on traditional
classification and regression tasks, leaving multi-output prediction
under-explored. Moreover, recent work has demonstrated that tree-embeddings are
highly representative, especially in structured output prediction. In this
direction, we propose a novel deep tree-ensemble (DTE) model, where every layer
enriches the original feature set with a representation learning component
based on tree-embeddings. In this paper, we specifically focus on two
structured output prediction tasks, namely multi-label classification and
multi-target regression. We conducted experiments using multiple benchmark
datasets and the obtained results confirm that our method provides superior
results to state-of-the-art methods in both tasks.

    

### [[2011.04170] A Synthetic Over-sampling method with Minority and Majority classes for imbalance problems](http://arxiv.org/abs/2011.04170)


  Class imbalance is a substantial challenge in classifying many real-world
cases. Synthetic over-sampling methods have been effective to improve the
performance of classifiers for imbalance problems. However, most synthetic
over-sampling methods generate non-diverse synthetic instances within the
convex hull formed by the existing minority instances as they only concentrate
on the minority class and ignore the vast information provided by the majority
class. They also often do not perform well for extremely imbalanced data as the
fewer the minority instances, the less information to generate synthetic
instances. Moreover, existing methods that generate synthetic instances using
the majority class distributional information cannot perform effectively when
the majority class has a multi-modal distribution. We propose a new method to
generate diverse and adaptable synthetic instances using Synthetic
Over-sampling with Minority and Majority classes (SOMM). SOMM generates
synthetic instances diversely within the minority data space. It updates the
generated instances adaptively to the neighbourhood including both classes.
Thus, SOMM performs well for both binary and multiclass imbalance problems. We
examine the performance of SOMM for binary and multiclass problems using
benchmark data sets for different imbalance levels. The empirical results show
the superiority of SOMM compared to other existing methods.

    

### [[2011.10443] Variational Laplace for Bayesian neural networks](http://arxiv.org/abs/2011.10443)


  We develop variational Laplace for Bayesian neural networks (BNNs) which
exploits a local approximation of the curvature of the likelihood to estimate
the ELBO without the need for stochastic sampling of the neural-network
weights. The Variational Laplace objective is simple to evaluate, as it is (in
essence) the log-likelihood, plus weight-decay, plus a squared-gradient
regularizer. Variational Laplace gave better test performance and expected
calibration errors than maximum a-posteriori inference and standard
sampling-based variational inference, despite using the same variational
approximate posterior. Finally, we emphasise care needed in benchmarking
standard VI as there is a risk of stopping before the variance parameters have
converged. We show that early-stopping can be avoided by increasing the
learning rate for the variance parameters.

    

### [[2012.03728] Utilizing Concept Drift for Measuring the Effectiveness of Policy Interventions: The Case of the COVID-19 Pandemic](http://arxiv.org/abs/2012.03728)


  As a reaction to the high infectiousness and lethality of the COVID-19 virus,
countries around the world have adopted drastic policy measures to contain the
pandemic. However, it remains unclear which effect these measures, so-called
non-pharmaceutical interventions (NPIs), have on the spread of the virus. In
this article, we use machine learning and apply drift detection methods in a
novel way to predict the time lag of policy interventions with respect to the
development of daily case numbers of COVID-19 across 9 European countries and
28 US states. Our analysis shows that there are, on average, more than two
weeks between NPI enactment and a drift in the case numbers.

    

### [[2101.08490] Estimating Average Treatment Effects via Orthogonal Regularization](http://arxiv.org/abs/2101.08490)


  Decision-making often requires accurate estimation of treatment effects from
observational data. This is challenging as outcomes of alternative decisions
are not observed and have to be estimated. Previous methods estimate outcomes
based on unconfoundedness but neglect any constraints that unconfoundedness
imposes on the outcomes. In this paper, we propose a novel regularization
framework for estimating average treatment effects that exploits
unconfoundedness. To this end, we formalize unconfoundedness as an
orthogonality constraint, which ensures that the outcomes are orthogonal to the
treatment assignment. This orthogonality constraint is then included in the
loss function via a regularization. Based on our regularization framework, we
develop deep orthogonal networks for unconfounded treatments (DONUT), which
learn outcomes that are orthogonal to the treatment assignment. Using a variety
of benchmark datasets for estimating average treatment effects, we demonstrate
that DONUT outperforms the state-of-the-art substantially.

    

### [[2102.03322] CF-GNNExplainer: Counterfactual Explanations for Graph Neural Networks](http://arxiv.org/abs/2102.03322)


  Given the increasing promise of Graph Neural Networks (GNNs) in real-world
applications, several methods have been developed for explaining their
predictions. So far, these methods have primarily focused on generating
subgraphs that are especially relevant for a particular prediction. However,
such methods do not provide a clear opportunity for recourse: given a
prediction, we want to understand how the prediction can be changed in order to
achieve a more desirable outcome. In this work, we propose a method for
generating counterfactual (CF) explanations for GNNs: the minimal perturbation
to the input (graph) data such that the prediction changes. Using only edge
deletions, we find that our method, CF-GNNExplainer can generate CF
explanations for the majority of instances across three widely used datasets
for GNN explanations, while removing less than 3 edges on average, with at
least 94\% accuracy. This indicates that CF-GNNExplainer primarily removes
edges that are crucial for the original predictions, resulting in minimal CF
explanations.

    

### [[2102.06559] Infinitely Deep Bayesian Neural Networks with Stochastic Differential Equations](http://arxiv.org/abs/2102.06559)


  We perform scalable approximate inference in a continuous-depth Bayesian
neural network family. In this model class, uncertainty about separate weights
in each layer gives hidden units that follow a stochastic differential
equation. We demonstrate gradient-based stochastic variational inference in
this infinite-parameter setting, producing arbitrarily-flexible approximate
posteriors. We also derive a novel gradient estimator that approaches zero
variance as the approximate posterior over weights approaches the true
posterior. This approach brings continuous-depth Bayesian neural nets to a
competitive comparison against discrete-depth alternatives, while inheriting
the memory-efficient training and tunable precision of Neural ODEs.

    

### [[2102.10734] Provable Super-Convergence with a Large Cyclical Learning Rate](http://arxiv.org/abs/2102.10734)


  Conventional wisdom dictates that learning rate should be in the stable
regime so that gradient-based algorithms don't blow up. This letter introduces
a simple scenario where an unstably large learning rate scheme leads to a super
fast convergence, with the convergence rate depending only logarithmically on
the condition number of the problem. Our scheme uses a Cyclical Learning Rate
(CLR) where we periodically take one large unstable step and several small
stable steps to compensate for the instability. These findings also help
explain the empirical observations of [Smith and Topin, 2019] where they show
that CLR with a large maximum learning rate can dramatically accelerate
learning and lead to so-called "super-convergence". We prove that our scheme
excels in the problems where Hessian exhibits a bimodal spectrum and the
eigenvalues can be grouped into two clusters (small and large). The unstably
large step is the key to enabling fast convergence over the small
eigen-spectrum.

    

### [[2103.00793] Embedded Knowledge Distillation in Depth-Level Dynamic Neural Network](http://arxiv.org/abs/2103.00793)


  In real applications, different computation-resource devices need
different-depth networks (e.g., ResNet-18/34/50) with high-accuracy. Usually,
existing methods either design multiple networks and train them independently,
or construct depth-level/width-level dynamic neural networks which is hard to
prove the accuracy of each sub-net. In this article, we propose an elegant
Depth-Level Dynamic Neural Network (DDNN) integrated different-depth sub-nets
of similar architectures. To improve the generalization of sub-nets, we design
the Embedded-Knowledge-Distillation (EKD) training mechanism for the DDNN to
implement knowledge transfer from the teacher (full-net) to multiple students
(sub-nets). Specifically, the Kullback-Leibler (KL) divergence is introduced to
constrain the posterior class probability consistency between full-net and
sub-nets, and self-attention distillation on the same resolution feature of
different depth is addressed to drive more abundant feature representations of
sub-nets. Thus, we can obtain multiple high-accuracy sub-nets simultaneously in
a DDNN via the online knowledge distillation in each training iteration without
extra computation cost. Extensive experiments on CIFAR-10/100, and ImageNet
datasets demonstrate that sub-nets in DDNN with EKD training achieve better
performance than individually training networks while preserving the original
performance of full-nets.

    

### [[2103.05568] Select, Substitute, Search: A New Benchmark for Knowledge-Augmented Visual Question Answering](http://arxiv.org/abs/2103.05568)


  Multimodal IR, spanning text corpus, knowledge graph and images, called
outside knowledge visual question answering (OKVQA), is of much recent
interest. However, the popular data set has serious limitations. A surprisingly
large fraction of queries do not assess the ability to integrate cross-modal
information. Instead, some are independent of the image, some depend on
speculation, some require OCR or are otherwise answerable from the image alone.
To add to the above limitations, frequency-based guessing is very effective
because of (unintended) widespread answer overlaps between the train and test
folds. Overall, it is hard to determine when state-of-the-art systems exploit
these weaknesses rather than really infer the answers, because they are opaque
and their 'reasoning' process is uninterpretable. An equally important
limitation is that the dataset is designed for the quantitative assessment only
of the end-to-end answer retrieval task, with no provision for assessing the
correct(semantic) interpretation of the input query. In response, we identify a
key structural idiom in OKVQA ,viz., S3 (select, substitute and search), and
build a new data set and challenge around it. Specifically, the questioner
identifies an entity in the image and asks a question involving that entity
which can be answered only by consulting a knowledge graph or corpus passage
mentioning the entity. Our challenge consists of (i)OKVQAS3, a subset of OKVQA
annotated based on the structural idiom and (ii)S3VQA, a new dataset built from
scratch. We also present a neural but structurally transparent OKVQA system,
S3, that explicitly addresses our challenge dataset, and outperforms recent
competitive baselines.

    

### [[2103.13689] MCTSteg: A Monte Carlo Tree Search-based Reinforcement Learning Framework for Universal Non-additive Steganography](http://arxiv.org/abs/2103.13689)


  Recent research has shown that non-additive image steganographic frameworks
effectively improve security performance through adjusting distortion
distribution. However, as far as we know, all of the existing non-additive
proposals are based on handcrafted policies, and can only be applied to a
specific image domain, which heavily prevent non-additive steganography from
releasing its full potentiality. In this paper, we propose an automatic
non-additive steganographic distortion learning framework called MCTSteg to
remove the above restrictions. Guided by the reinforcement learning paradigm,
we combine Monte Carlo Tree Search (MCTS) and steganalyzer-based environmental
model to build MCTSteg. MCTS makes sequential decisions to adjust distortion
distribution without human intervention. Our proposed environmental model is
used to obtain feedbacks from each decision. Due to its self-learning
characteristic and domain-independent reward function, MCTSteg has become the
first reported universal non-additive steganographic framework which can work
in both spatial and JPEG domains. Extensive experimental results show that
MCTSteg can effectively withstand the detection of both hand-crafted
feature-based and deep-learning-based steganalyzers. In both spatial and JPEG
domains, the security performance of MCTSteg steadily outperforms the state of
the art by a clear margin under different scenarios.

    

### [[2105.01622] Poisoning the Unlabeled Dataset of Semi-Supervised Learning](http://arxiv.org/abs/2105.01622)


  Semi-supervised machine learning models learn from a (small) set of labeled
training examples, and a (large) set of unlabeled training examples.
State-of-the-art models can reach within a few percentage points of
fully-supervised training, while requiring 100x less labeled data.
We study a new class of vulnerabilities: poisoning attacks that modify the
unlabeled dataset. In order to be useful, unlabeled datasets are given strictly
less review than labeled datasets, and adversaries can therefore poison them
easily. By inserting maliciously-crafted unlabeled examples totaling just 0.1%
of the dataset size, we can manipulate a model trained on this poisoned dataset
to misclassify arbitrary examples at test time (as any desired label). Our
attacks are highly effective across datasets and semi-supervised learning
methods.
We find that more accurate methods (thus more likely to be used) are
significantly more vulnerable to poisoning attacks, and as such better training
methods are unlikely to prevent this attack. To counter this we explore the
space of defenses, and propose two methods that mitigate our attack.

    

### [[2105.06035] GIPA: General Information Propagation Algorithm for Graph Learning](http://arxiv.org/abs/2105.06035)


  Graph neural networks (GNNs) have been popularly used in analyzing
graph-structured data, showing promising results in various applications such
as node classification, link prediction and network recommendation. In this
paper, we present a new graph attention neural network, namely GIPA, for
attributed graph data learning. GIPA consists of three key components:
attention, feature propagation and aggregation. Specifically, the attention
component introduces a new multi-layer perceptron based multi-head to generate
better non-linear feature mapping and representation than conventional
implementations such as dot-product. The propagation component considers not
only node features but also edge features, which differs from existing GNNs
that merely consider node features. The aggregation component uses a residual
connection to generate the final embedding. We evaluate the performance of GIPA
using the Open Graph Benchmark proteins (ogbn-proteins for short) dataset. The
experimental results reveal that GIPA can beat the state-of-the-art models in
terms of prediction accuracy, e.g., GIPA achieves an average test ROC-AUC of
$0.8700\pm 0.0010$ and outperforms all the previous methods listed in the
ogbn-proteins leaderboard.

    

### [[2105.11686] Towards Understanding the Condensation of Two-layer Neural Networks at Initial Training](http://arxiv.org/abs/2105.11686)


  Studying the implicit regularization effect of the nonlinear training
dynamics of neural networks (NNs) is important for understanding why
over-parameterized neural networks often generalize well on real dataset.
Empirically, for two-layer NN, existing works have shown that input weights of
hidden neurons (the input weight of a hidden neuron consists of the weight from
its input layer to the hidden neuron and its bias term) condense on isolated
orientations with a small initialization. The condensation dynamics implies
that NNs can learn features from the training data with a network configuration
effectively equivalent to a much smaller network during the training. In this
work, we show that the multiple roots of activation function at origin
(referred as ``multiplicity'') is a key factor for understanding the
condensation at the initial stage of training. Our experiments of multilayer
networks suggest that the maximal number of condensed orientations is twice the
multiplicity of the activation function used. Our theoretical analysis of
two-layer networks confirms experiments for two cases, one is for the
activation function of multiplicity one, which contains many common activation
functions, and the other is for the one-dimensional input. This work makes a
step towards understanding how small initialization implicitly leads NNs to
condensation at initial training stage, which lays a foundation for the future
study of the nonlinear dynamics of NNs and its implicit regularization effect
at a later stage of training.

    

### [[2106.06895] FeSHI: Feature Map Based Stealthy Hardware Intrinsic Attack](http://arxiv.org/abs/2106.06895)


  To reduce the time-to-market and access to state-of-the-art techniques, CNN
hardware mapping and deployment on embedded accelerators are often outsourced
to untrusted third parties, which is going to be more prevalent in futuristic
artificial intelligence of things (AIoT) systems. These AIoT systems anticipate
horizontal collaboration among different resource-constrained AIoT node
devices, where CNN layers are partitioned and these devices collaboratively
compute complex CNN tasks. This horizontal collaboration opens another attack
surface to the CNN-based application, like inserting the hardware Trojans (HT)
into the embedded accelerators designed for the CNN. Therefore, there is a dire
need to explore this attack surface for designing secure embedded hardware
accelerators for CNNs. Towards this goal, in this paper, we exploited this
attack surface to propose an HT-based attack called FeSHI. Since in horizontal
collaboration of RC AIoT devices different sections of CNN architectures are
outsourced to different untrusted third parties, the attacker may not know the
input image, but it has access to the layer-by-layer output feature maps
information for the assigned sections of the CNN architecture. This attack
exploits the statistical distribution, i.e., Gaussian distribution, of the
layer-by-layer feature maps of the CNN to design two triggers for stealthy HT
with a very low probability of triggering. Also, three different novel,
stealthy and effective trigger designs are proposed.

    

### [[2106.09370] A deep generative model for probabilistic energy forecasting in power systems: normalizing flows](http://arxiv.org/abs/2106.09370)


  Greater direct electrification of end-use sectors with a higher share of
renewables is one of the pillars to power a carbon-neutral society by 2050.
However, in contrast to conventional power plants, renewable energy is subject
to uncertainty raising challenges for their interaction with power systems.
Scenario-based probabilistic forecasting models have become an important tool
to equip decision-makers. This paper proposes to present to the power systems
forecasting practitioners a recent deep learning technique, the normalizing
flows, to produce accurate scenario-based probabilistic forecasts that are
crucial to face the new challenges in power systems applications. The strength
of this technique is to directly learn the stochastic multivariate distribution
of the underlying process by maximizing the likelihood. Through comprehensive
empirical evaluations using the open data of the Global Energy Forecasting
Competition 2014, we demonstrate that this methodology is competitive with
other state-of-the-art deep learning generative models: generative adversarial
networks and variational autoencoders. The models producing weather-based wind,
solar power, and load scenarios are properly compared both in terms of forecast
value, by considering the case study of an energy retailer, and quality using
several complementary metrics. The numerical experiments are simple and easily
reproducible. Thus, we hope it will encourage other forecasting practitioners
to test and use normalizing flows in power system applications such as bidding
on electricity markets, scheduling of power systems with high renewable energy
sources penetration, energy management of virtual power plan or microgrids, and
unit commitment.

    

### [[2106.15380] Globally Optimal Hierarchical Reinforcement Learning for Linearly-Solvable Markov Decision Processes](http://arxiv.org/abs/2106.15380)


  In this work we present a novel approach to hierarchical reinforcement
learning for linearly-solvable Markov decision processes. Our approach assumes
that the state space is partitioned, and the subtasks consist in moving between
the partitions. We represent value functions on several levels of abstraction,
and use the compositionality of subtasks to estimate the optimal values of the
states in each partition. The policy is implicitly defined on these optimal
value estimates, rather than being decomposed among the subtasks. As a
consequence, our approach can learn the globally optimal policy, and does not
suffer from the non-stationarity of high-level decisions. If several partitions
have equivalent dynamics, the subtasks of those partitions can be shared. If
the set of boundary states is smaller than the entire state space, our approach
can have significantly smaller sample complexity than that of a flat learner,
and we validate this empirically in several experiments.

    

### [[2108.04212] AutoVideo: An Automated Video Action Recognition System](http://arxiv.org/abs/2108.04212)


  Action recognition is a crucial task for video understanding. In this paper,
we present AutoVideo, a Python system for automated video action recognition.
It currently supports seven action recognition algorithms and various
pre-processing modules. Unlike the existing libraries that only provide model
zoos, AutoVideo is built with the standard pipeline language. The basic
building block is primitive, which wraps a pre-processing module or an
algorithm with some hyperparameters. AutoVideo is highly modular and
extendable. It can be easily combined with AutoML searchers. The pipeline
language is quite general so that we can easily enrich AutoVideo with
algorithms for various other video-related tasks in the future. AutoVideo is
released under MIT license at this https URL


### [[1907.12975] Deep Learning architectures for generalized immunofluorescence based nuclear image segmentation](http://arxiv.org/abs/1907.12975)


  Separating and labeling each instance of a nucleus (instance-aware
segmentation) is the key challenge in segmenting single cell nuclei on
fluorescence microscopy images. Deep Neural Networks can learn the implicit
transformation of a nuclear image into a probability map indicating the class
membership of each pixel (nucleus or background), but the use of
post-processing steps to turn the probability map into a labeled object mask is
error-prone. This especially accounts for nuclear images of tissue sections and
nuclear images across varying tissue preparations. In this work, we aim to
evaluate the performance of state-of-the-art deep learning architectures to
segment nuclei in fluorescence images of various tissue origins and sample
preparation types without post-processing. We compare architectures that
operate on pixel to pixel translation and an architecture that operates on
object detection and subsequent locally applied segmentation. In addition, we
propose a novel strategy to create artificial images to extend the training
set. We evaluate the influence of ground truth annotation quality, image scale
and segmentation complexity on segmentation performance. Results show that
three out of four deep learning architectures (U-Net, U-Net with ResNet34
backbone, Mask R-CNN) can segment fluorescent nuclear images on most of the
sample preparation types and tissue origins with satisfactory segmentation
performance. Mask R-CNN, an architecture designed to address instance aware
segmentation tasks, outperforms other architectures. Equal nuclear mean size,
consistent nuclear annotations and the use of artificially generated images
result in overall acceptable precision and recall across different tissues and
sample preparation types.

    

### [[2108.02940] Evaluating Adversarial Attacks on Driving Safety in Vision-Based Autonomous Vehicles](http://arxiv.org/abs/2108.02940)


  In recent years, many deep learning models have been adopted in autonomous
driving. At the same time, these models introduce new vulnerabilities that may
compromise the safety of autonomous vehicles. Specifically, recent studies have
demonstrated that adversarial attacks can cause a significant decline in
detection precision of deep learning-based 3D object detection models. Although
driving safety is the ultimate concern for autonomous driving, there is no
comprehensive study on the linkage between the performance of deep learning
models and the driving safety of autonomous vehicles under adversarial attacks.
In this paper, we investigate the impact of two primary types of adversarial
attacks, perturbation attacks and patch attacks, on the driving safety of
vision-based autonomous vehicles rather than the detection precision of deep
learning models. In particular, we consider two state-of-the-art models in
vision-based 3D object detection, Stereo R-CNN and DSGN. To evaluate driving
safety, we propose an end-to-end evaluation framework with a set of driving
safety performance metrics. By analyzing the results of our extensive
evaluation experiments, we find that (1) the attack's impact on the driving
safety of autonomous vehicles and the attack's impact on the precision of 3D
object detectors are decoupled, and (2) the DSGN model demonstrates stronger
robustness to adversarial attacks than the Stereo R-CNN model. In addition, we
further investigate the causes behind the two findings with an ablation study.
The findings of this paper provide a new perspective to evaluate adversarial
attacks and guide the selection of deep learning models in autonomous driving.

    

### [[2108.04488] MiB: Asynchronous BFT with More Replicas](http://arxiv.org/abs/2108.04488)


  State-of-the-art asynchronous Byzantine fault-tolerant (BFT) protocols, such
as HoneyBadgerBFT, BEAT, and Dumbo, have shown a performance comparable to
partially synchronous BFT protocols. This paper studies two practical
directions in asynchronous BFT. First, while all these asynchronous BFT
protocols assume optimal resilience with 3f+1 replicas (where f is an upper
bound on the number of Byzantine replicas), it is interesting to ask whether
more efficient protocols are possible if relaxing the resilience level. Second,
these recent BFT protocols evaluate their performance under failure-free
scenarios. It is unclear if these protocols indeed perform well during failures
and attacks.
This work first studies asynchronous BFT with suboptimal resilience using
5f+1 and 7f+1 replicas. We present MiB, a novel and efficient asynchronous BFT
framework using new distributed system constructions as building blocks. MiB
consists of two main BFT instances and five other variants. As another
contribution, we systematically design experiments for asynchronous BFT
protocols with failures and evaluate their performance in various failure
scenarios. We report interesting findings, showing asynchronous BFT indeed
performs consistently well during various failure scenarios. In particular, via
a five-continent deployment on Amazon EC2 using 140 replicas, we show the MiB
instances have lower latency and much higher throughput than their asynchronous
BFT counterparts.

    

### [[2108.04520] Fast and Fair Lock-Free Locks](http://arxiv.org/abs/2108.04520)


  We present a randomized approach for lock-free locks with strong bounds on
time and fairness in a context in which any process can be arbitrarily delayed.
Our approach supports a tryLock operation that is given a set of locks, and
code to run when all the locks are acquired. Given an upper bound $\kappa$
known to the algorithm on the point contention for a tryLock it will succeed in
acquiring its locks and running the code with probability at least $1/\kappa$.
It is thus fair. If the algorithm does not know the bound $\kappa$, we present
a variant that can guarantee a probability of at least $1/\kappa\log\kappa$ of
success. Furthermore, if the maximum step complexity for the code in any lock
is $T$, and the point contentions are constant, the attempt will take $O(T)$
steps. The attempts are independent, thus if the tryLock is repeatedly retried
on failure, it will succeed in $O(T)$ expected steps, and with high probability
in not much more. Importantly, however, retrying is not mandatory, and a
process may choose to execute different code upon failure.
We assume an oblivious adversarial scheduler, which does not make decisions
based on the operations, but can predetermine any schedule for the processes,
which is unknown to our algorithm. Furthermore, to account for applications
that change their future requests based on the results of previous lock
attempts, we strengthen the adversary by allowing decisions of the start times
and lock sets of tryLock attempts to be made adaptively, given the history of
the execution so far.

    

### [[2108.04749] Evaluation of Load Prediction Techniques for Distributed Stream Processing](http://arxiv.org/abs/2108.04749)


  Distributed Stream Processing (DSP) systems enable processing large streams
of continuous data to produce results in near to real time. They are an
essential part of many data-intensive applications and analytics platforms. The
rate at which events arrive at DSP systems can vary considerably over time,
which may be due to trends, cyclic, and seasonal patterns within the data
streams. A priori knowledge of incoming workloads enables proactive approaches
to resource management and optimization tasks such as dynamic scaling, live
migration of resources, and the tuning of configuration parameters during
run-times, thus leading to a potentially better Quality of Service.
In this paper we conduct a comprehensive evaluation of different load
prediction techniques for DSP jobs. We identify three use-cases and formulate
requirements for making load predictions specific to DSP jobs. Automatically
optimized classical and Deep Learning methods are being evaluated on nine
different datasets from typical DSP domains, i.e. the IoT, Web 2.0, and cluster
monitoring. We compare model performance with respect to overall accuracy and
training duration. Our results show that the Deep Learning methods provide the
most accurate load predictions for the majority of the evaluated datasets.

    

### [[2108.04773] Survey and Benchmarking of Precision-Scalable MAC Arrays for Embedded DNN Processing](http://arxiv.org/abs/2108.04773)


  Reduced-precision and variable-precision multiply-accumulate (MAC) operations
provide opportunities to significantly improve energy efficiency and throughput
of DNN accelerators with no/limited algorithmic performance loss, paving a way
towards deploying AI applications on resource-constraint edge devices.
Accordingly, various precision-scalable MAC array (PSMA) architectures were
recently proposed. However, it is difficult to make a fair comparison between
those alternatives, as each proposed PSMA is demonstrated in different systems
with different technologies. This work aims to provide a clear view on the
design space of PSMA and offer insights for selecting the optimal architectures
based on designers' needs. First, we introduce a precision-enhanced for-loop
representation for DNN dataflows. Next, we use this new representation towards
a comprehensive PSMA taxonomy, capable to systematically cover most prominent
state-of-the-art PSMAs, as well as uncover new PSMA architectures. Following
that, we build a highly parameterized PSMA template that can be design-time
configured into a huge subset of the design space spanned by the taxonomy. This
allows to fairly and thoroughly benchmark 72 different PSMA architectures. We
perform such studies in 28nm technology targeting run-time precision
scalability from 8 to 2 bits, operating at 200 MHz and 1 GHz. Analyzing
resulting energy efficiency and area breakdowns reveals key design guidelines
for PSMA architectures.

    

### [[2108.04232] GANmapper: geographical content filling](http://arxiv.org/abs/2108.04232)


  We present a new method to create spatial data using a generative adversarial
network (GAN). Our contribution uses coarse and widely available geospatial
data to create maps of less available features at the finer scale in the built
environment, bypassing their traditional acquisition techniques (e.g. satellite
imagery or land surveying). In the work, we employ land use data and road
networks as input to generate building footprints, and conduct experiments in 9
cities around the world. The method, which we implement in a tool we release
openly, enables generating approximate maps of the urban form, and it is
generalisable to augment other types of geoinformation, enhancing the
completeness and quality of spatial data infrastructure. It may be especially
useful in locations missing detailed and high-resolution data and those that
are mapped with uncertain or heterogeneous quality, such as much of
OpenStreetMap. The quality of the results is influenced by the urban form and
scale. In most cases, experiments suggest promising performance as the method
tends to truthfully indicate the locations, amount, and shape of buildings. The
work has the potential to support several applications, such as energy,
climate, and urban morphology studies in areas previously lacking required
data.

    

### [[2108.04316] Generating Music and Generative Art from Brain activity](http://arxiv.org/abs/2108.04316)


  Nowadays, technological advances have influenced all human activities,
creating new dynamics and ways of communication. In this context, some artists
have incorporated these advances in their creative process, giving rise to
unique aesthetic expressions referred to in the literature as Generative Art,
which is characterized by assigning part of the creative process to a system
that acts with certain autonomy (Galanter, 2003).
This research work introduces a computational system for creating generative
art using a Brain-Computer Interface (BCI) which portrays the user's brain
activity in a digital artwork. In this way, the user takes an active role in
the creative process. In aims of showing that the proposed system materializes
in an artistic piece the user's mental states by means of a visual and sound
representation, several tests are carried out to ensure the reliability of the
BCI device sent data.
The generated artwork uses brain signals and concepts of geometry, color and
spatial location to give complexity to the autonomous construction. As an added
value, the visual and auditory production is accompanied by an olfactory and
kinesthetic component which complements the art pieces providing a multimodal
communication character.

    

### [[2108.04324] FairyTailor: A Multimodal Generative Framework for Storytelling](http://arxiv.org/abs/2108.04324)


  Storytelling is an open-ended task that entails creative thinking and
requires a constant flow of ideas. Natural language generation (NLG) for
storytelling is especially challenging because it requires the generated text
to follow an overall theme while remaining creative and diverse to engage the
reader. In this work, we introduce a system and a web-based demo, FairyTailor,
for human-in-the-loop visual story co-creation. Users can create a cohesive
children's fairytale by weaving generated texts and retrieved images with their
input. FairyTailor adds another modality and modifies the text generation
process to produce a coherent and creative sequence of text and images. To our
knowledge, this is the first dynamic tool for multimodal story generation that
allows interactive co-formation of both texts and images. It allows users to
give feedback on co-created stories and share their results.

    

### [[2108.04350] VirtualConductor: Music-driven Conducting Video Generation System](http://arxiv.org/abs/2108.04350)


  In this demo, we present VirtualConductor, a system that can generate
conducting video from any given music and a single user's image. First, a
large-scale conductor motion dataset is collected and constructed. Then, we
propose Audio Motion Correspondence Network (AMCNet) and adversarial-perceptual
learning to learn the cross-modal relationship and generate diverse, plausible,
music-synchronized motion. Finally, we combine 3D animation rendering and a
pose transfer model to synthesize conducting video from a single given user's
image. Therefore, any user can become a virtual conductor through the system.

    

### [[2108.04371] Extending LIME for Business Process Automation](http://arxiv.org/abs/2108.04371)


  AI business process applications automate high-stakes business decisions
where there is an increasing demand to justify or explain the rationale behind
algorithmic decisions. Business process applications have ordering or
constraints on tasks and feature values that cause lightweight, model-agnostic,
existing explanation methods like LIME to fail. In response, we propose a local
explanation framework extending LIME for explaining AI business process
applications. Empirical evaluation of our extension underscores the advantage
of our approach in the business process setting.

    

### [[2108.04376] The External Validity of Combinatorial Samples and Populations](http://arxiv.org/abs/2108.04376)


  The widely used 'Counterfactual' definition of Causal Effects was derived for
unbiasedness and accuracy - and not generalizability. We propose a simple
definition for the External Validity (EV) of Interventions, Counterfactual
statements and Samples. We use the definition to discuss several issues that
have baffled the counterfactual approach to effect estimation: out-of-sample
validity, reliance on independence assumptions or estimation, concurrent
estimation of many effects and full-models, bias-variance tradeoffs,
statistical power, omitted variables, and connections to supervised and
explaining techniques. Methodologically, the definition also allow us to
replace the parametric and generally ill-posed estimation problems that
followed the counterfactual definition by combinatorial enumeration problems on
non-experimental samples. We use over 20 contemporary methods and simulations
to demonstrate that the approach leads to accuracy gains in standard
out-of-sample prediction, intervention effect prediction and causal effect
estimation tasks. The COVID19 pandemic highlighted the need for learning
solutions to provide general predictions in small samples - many times with
missing variables. We also demonstrate applications in this pressing problem.

    

### [[2108.04378] Making Transformers Solve Compositional Tasks](http://arxiv.org/abs/2108.04378)


  Several studies have reported the inability of Transformer models to
generalize compositionally, a key type of generalization in many NLP tasks such
as semantic parsing. In this paper we explore the design space of Transformer
models showing that the inductive biases given to the model by several design
decisions significantly impact compositional generalization. Through this
exploration, we identified Transformer configurations that generalize
compositionally significantly better than previously reported in the literature
in a diverse set of compositional tasks, and that achieve state-of-the-art
results in a semantic parsing compositional generalization benchmark (COGS),
and a string edit operation composition benchmark (PCFG).

    

### [[2108.04395] StarGAN-VC+ASR: StarGAN-based Non-Parallel Voice Conversion Regularized by Automatic Speech Recognition](http://arxiv.org/abs/2108.04395)


  Preserving the linguistic content of input speech is essential during voice
conversion (VC). The star generative adversarial network-based VC method
(StarGAN-VC) is a recently developed method that allows non-parallel
many-to-many VC. Although this method is powerful, it can fail to preserve the
linguistic content of input speech when the number of available training
samples is extremely small. To overcome this problem, we propose the use of
automatic speech recognition to assist model training, to improve StarGAN-VC,
especially in low-resource scenarios.
Experimental results show that using our proposed method, StarGAN-VC can
retain more linguistic information than vanilla StarGAN-VC.

    

### [[2108.04453] Method Towards CVPR 2021 Image Matching Challenge](http://arxiv.org/abs/2108.04453)


  This report describes Megvii-3D team's approach towards CVPR 2021 Image
Matching Workshop.

    

### [[2108.04468] End-to-End User Behavior Retrieval in Click-Through RatePrediction Model](http://arxiv.org/abs/2108.04468)


  Click-Through Rate (CTR) prediction is one of the core tasks in recommender
systems (RS). It predicts a personalized click probability for each user-item
pair. Recently, researchers have found that the performance of CTR model can be
improved greatly by taking user behavior sequence into consideration,
especially long-term user behavior sequence. The report on an e-commerce
website shows that 23\% of users have more than 1000 clicks during the past 5
months. Though there are numerous works focus on modeling sequential user
behaviors, few works can handle long-term user behavior sequence due to the
strict inference time constraint in real world system. Two-stage methods are
proposed to push the limit for better performance. At the first stage, an
auxiliary task is designed to retrieve the top-$k$ similar items from long-term
user behavior sequence. At the second stage, the classical attention mechanism
is conducted between the candidate item and $k$ items selected in the first
stage. However, information gap happens between retrieval stage and the main
CTR task. This goal divergence can greatly diminishing the performance gain of
long-term user sequence. In this paper, inspired by Reformer, we propose a
locality-sensitive hashing (LSH) method called ETA (End-to-end Target
Attention) which can greatly reduce the training and inference cost and make
the end-to-end training with long-term user behavior sequence possible. Both
offline and online experiments confirm the effectiveness of our model. We
deploy ETA into a large-scale real world E-commerce system and achieve extra
3.1\% improvements on GMV (Gross Merchandise Value) compared to a two-stage
long user sequence CTR model.

    

### [[2108.04475] Localized Graph Collaborative Filtering](http://arxiv.org/abs/2108.04475)


  User-item interactions in recommendations can be naturally de-noted as a
user-item bipartite graph. Given the success of graph neural networks (GNNs) in
graph representation learning, GNN-based C methods have been proposed to
advance recommender systems. These methods often make recommendations based on
the learned user and item embeddings. However, we found that they do not
perform well wit sparse user-item graphs which are quite common in real-world
recommendations. Therefore, in this work, we introduce a novel perspective to
build GNN-based CF methods for recommendations which leads to the proposed
framework Localized Graph Collaborative Filtering (LGCF). One key advantage of
LGCF is that it does not need to learn embeddings for each user and item, which
is challenging in sparse scenarios.
Alternatively, LGCF aims at encoding useful CF information into a localized
graph and making recommendations based on such graph. Extensive experiments on
various datasets validate the effectiveness of LGCF especially in sparse
scenarios. Furthermore, empirical results demonstrate that LGCF provides
complementary information to the embedding-based CF model which can be utilized
to boost recommendation performance.

    

### [[2108.04479] Scalable Reverse Image Search Engine for NASAWorldview](http://arxiv.org/abs/2108.04479)


  Researchers often spend weeks sifting through decades of unlabeled satellite
imagery(on NASA Worldview) in order to develop datasets on which they can start
conducting research. We developed an interactive, scalable and fast image
similarity search engine (which can take one or more images as the query image)
that automatically sifts through the unlabeled dataset reducing dataset
generation time from weeks to minutes. In this work, we describe key components
of the end to end pipeline. Our similarity search system was created to be able
to identify similar images from a potentially petabyte scale database that are
similar to an input image, and for this we had to break down each query image
into its features, which were generated by a classification layer stripped CNN
trained in a supervised manner. To store and search these features efficiently,
we had to make several scalability improvements. To improve the speed, reduce
the storage, and shrink memory requirements for embedding search, we add a
fully connected layer to our CNN make all images into a 128 length vector
before entering the classification layers. This helped us compress the size of
our image features from 2048 (for ResNet, which was initially tried as our
featurizer) to 128 for our new custom model. Additionally, we utilize existing
approximate nearest neighbor search libraries to significantly speed up
embedding search. Our system currently searches over our entire database of
images at 5 seconds per query on a single virtual machine in the cloud. In the
future, we would like to incorporate a SimCLR based featurizing model which
could be trained without any labelling by a human (since the classification
aspect of the model is irrelevant to this use case).

    

### [[2108.04537] Time-Optimal Planning for Quadrotor Waypoint Flight](http://arxiv.org/abs/2108.04537)


  Quadrotors are among the most agile flying robots. However, planning
time-optimal trajectories at the actuation limit through multiple waypoints
remains an open problem. This is crucial for applications such as inspection,
delivery, search and rescue, and drone racing. Early works used polynomial
trajectory formulations, which do not exploit the full actuator potential
because of their inherent smoothness. Recent works resorted to numerical
optimization but require waypoints to be allocated as costs or constraints at
specific discrete times. However, this time allocation is a priori unknown and
renders previous works incapable of producing truly time-optimal trajectories.
To generate truly time-optimal trajectories, we propose a solution to the time
allocation problem while exploiting the full quadrotor's actuator potential. We
achieve this by introducing a formulation of progress along the trajectory,
which enables the simultaneous optimization of the time allocation and the
trajectory itself. We compare our method against related approaches and
validate it in real-world flights in one of the world's largest motion-capture
systems, where we outperform human expert drone pilots in a drone-racing task.

    

### [[2108.04541] Accelerating Evolutionary Neural Architecture Search via Multi-Fidelity Evaluation](http://arxiv.org/abs/2108.04541)


  Evolutionary neural architecture search (ENAS) has recently received
increasing attention by effectively finding high-quality neural architectures,
which however consumes high computational cost by training the architecture
encoded by each individual for complete epochs in individual evaluation.
Numerous ENAS approaches have been developed to reduce the evaluation cost, but
it is often difficult for most of these approaches to achieve high evaluation
accuracy. To address this issue, in this paper we propose an accelerated ENAS
via multifidelity evaluation termed MFENAS, where the individual evaluation
cost is significantly reduced by training the architecture encoded by each
individual for only a small number of epochs. The balance between evaluation
cost and evaluation accuracy is well maintained by suggesting a multi-fidelity
evaluation, which identifies the potentially good individuals that cannot
survive from previous generations by integrating multiple evaluations under
different numbers of training epochs. For high diversity of neural
architectures, a population initialization strategy is devised to produce
different neural architectures varying from ResNet-like architectures to
Inception-like ones. Experimental results on CIFAR-10 show that the
architecture obtained by the proposed MFENAS achieves a 2.39% test error rate
at the cost of only 0.6 GPU days on one NVIDIA 2080TI GPU, demonstrating the
superiority of the proposed MFENAS over state-of-the-art NAS approaches in
terms of both computational cost and architecture quality. The architecture
obtained by the proposed MFENAS is then transferred to CIFAR-100 and ImageNet,
which also exhibits competitive performance to the architectures obtained by
existing NAS approaches. The source code of the proposed MFENAS is available at
this https URL.

    

### [[2108.04542] TrUMAn: Trope Understanding in Movies and Animations](http://arxiv.org/abs/2108.04542)


  Understanding and comprehending video content is crucial for many real-world
applications such as search and recommendation systems. While recent progress
of deep learning has boosted performance on various tasks using visual cues,
deep cognition to reason intentions, motivation, or causality remains
challenging. Existing datasets that aim to examine video reasoning capability
focus on visual signals such as actions, objects, relations, or could be
answered utilizing text bias. Observing this, we propose a novel task, along
with a new dataset: Trope Understanding in Movies and Animations (TrUMAn),
intending to evaluate and develop learning systems beyond visual signals.
Tropes are frequently used storytelling devices for creative works. By coping
with the trope understanding task and enabling the deep cognition skills of
machines, we are optimistic that data mining applications and algorithms could
be taken to the next level. To tackle the challenging TrUMAn dataset, we
present a Trope Understanding and Storytelling (TrUSt) with a new Conceptual
Storyteller module, which guides the video encoder by performing video
storytelling on a latent space. The generated story embedding is then fed into
the trope understanding model to provide further signals. Experimental results
demonstrate that state-of-the-art learning systems on existing tasks reach only
12.01% of accuracy with raw input signals. Also, even in the oracle case with
human-annotated descriptions, BERT contextual embedding achieves at most 28% of
accuracy. Our proposed TrUSt boosts the model performance and reaches 13.94%
performance. We also provide detailed analysis topave the way for future
research. TrUMAn is publicly available
at:this https URL


### [[2108.04546] Epigenetic opportunities for Evolutionary Computation](http://arxiv.org/abs/2108.04546)


  Evolutionary Computation is a group of biologically inspired algorithms used
to solve complex optimisation problems. It can be split into Evolutionary
Algorithms, which take inspiration from genetic inheritance, and Swarm
Intelligence algorithms, that take inspiration from cultural inheritance.
However, recent developments have focused on computational or mathematical
adaptions, leaving their biological roots behind. This has left much of the
modern evolutionary literature relatively unexplored.
To understand which evolutionary mechanisms have been considered, and which
have been overlooked, this paper breaks down successful bio-inspired algorithms
under a contemporary biological framework based on the Extended Evolutionary
Synthesis, an extension of the classical, genetics focussed, Modern Synthesis.
The analysis shows that Darwinism and the Modern Synthesis have been
incorporated into Evolutionary Computation but that the Extended Evolutionary
Synthesis has been broadly ignored beyond:cultural inheritance, incorporated in
the sub-set of Swarm Intelligence algorithms, evolvability, through CMA-ES, and
multilevel selection, through Multi-Level Selection Genetic Algorithm.
The framework shows a missing gap in epigenetic inheritance for Evolutionary
Computation, despite being a key building block in modern interpretations of
how evolution occurs. Epigenetic inheritance can explain fast adaptation,
without changes in an individual's genotype, by allowing biological organisms
to self-adapt quickly to environmental cues, which, increases the speed of
convergence while maintaining stability in changing environments. This leaves a
diverse range of biologically inspired mechanisms as low hanging fruit that
should be explored further within Evolutionary Computation.

    

### [[2108.04555] Deep Joint Learning of Pathological Region Localization and Alzheimer's Disease Diagnosis](http://arxiv.org/abs/2108.04555)


  The identification of Alzheimer's disease (AD) and its early stages using
structural magnetic resonance imaging (MRI) has been attracting the attention
of researchers. Various data-driven approaches have been introduced to capture
subtle and local morphological changes of the brain accompanied by the disease
progression. One of the typical approaches for capturing subtle changes is
patch-level feature representation. However, the predetermined regions to
extract patches can limit classification performance by interrupting the
exploration of potential biomarkers. In addition, the existing patch-level
analyses have difficulty explaining their decision-making. To address these
problems, we propose the BrainBagNet with a position-based gate
(PG-BrainBagNet), a framework for jointly learning pathological region
localization and AD diagnosis in an end-to-end manner. In advance, as all scans
are aligned to a template in image processing, the position of brain images can
be represented through the 3D Cartesian space shared by the overall MRI scans.
The proposed method represents the patch-level response from whole-brain MRI
scans and discriminative brain-region from position information. Based on the
outcomes, the patch-level class evidence is calculated, and then the
image-level prediction is inferred by a transparent aggregation. The proposed
models were evaluated on the ADNI datasets. In five-fold cross-validation, the
classification performance of the proposed method outperformed that of the
state-of-the-art methods in both AD diagnosis (AD vs. normal control) and mild
cognitive impairment (MCI) conversion prediction (progressive MCI vs. stable
MCI) tasks. In addition, changes in the identified discriminant regions and
patch-level class evidence according to the patch size used for model training
are presented and analyzed.

    

### [[2108.04556] CLSEBERT: Contrastive Learning for Syntax Enhanced Code Pre-Trained Model](http://arxiv.org/abs/2108.04556)


  Pre-trained models for programming languages have proven their significant
values in various code-related tasks, such as code search, code clone
detection, and code translation. Currently, most pre-trained models treat a
code snippet as a sequence of tokens or only focus on the data flow between
code identifiers. However, rich code syntax and hierarchy are ignored which can
provide important structure information and semantic rules of codes to help
enhance code representations. In addition, although the BERT-based code
pre-trained models achieve high performance on many downstream tasks, the
native derived sequence representations of BERT are proven to be of
low-quality, it performs poorly on code matching and similarity tasks. To
address these problems, we propose CLSEBERT, a Constrastive Learning Framework
for Syntax Enhanced Code Pre-Trained Model, to deal with various code
intelligence tasks. In the pre-training stage, we consider the code syntax and
hierarchy contained in the Abstract Syntax Tree (AST) and leverage the
constrastive learning to learn noise-invariant code representations. Besides
the masked language modeling (MLM), we also introduce two novel pre-training
objectives. One is to predict the edges between nodes in the abstract syntax
tree, and the other is to predict the types of code tokens. Through extensive
experiments on four code intelligence tasks, we successfully show the
effectiveness of our proposed model.

    

### [[2108.04605] A Novel Markovian Framework for Integrating Absolute and Relative Ordinal Emotion Information](http://arxiv.org/abs/2108.04605)


  There is growing interest in affective computing for the representation and
prediction of emotions along ordinal scales. However, the term ordinal emotion
label has been used to refer to both absolute notions such as low or high
arousal, as well as relation notions such as arousal is higher at one instance
compared to another. In this paper, we introduce the terminology absolute and
relative ordinal labels to make this distinction clear and investigate both
with a view to integrate them and exploit their complementary nature. We
propose a Markovian framework referred to as Dynamic Ordinal Markov Model
(DOMM) that makes use of both absolute and relative ordinal information, to
improve speech based ordinal emotion prediction. Finally, the proposed
framework is validated on two speech corpora commonly used in affective
computing, the RECOLA and the IEMOCAP databases, across a range of system
configurations. The results consistently indicate that integrating relative
ordinal information improves absolute ordinal emotion prediction.

    

### [[2108.04751] Logical Information Cells I](http://arxiv.org/abs/2108.04751)


  In this study we explore the spontaneous apparition of visible intelligible
reasoning in simple artificial networks, and we connect this experimental
observation with a notion of semantic information. We start with the
reproduction of a DNN model of natural neurons in monkeys, studied by
Neromyliotis and Moschovakis in 2017 and 2018, to explain how "motor equivalent
neurons", coding only for the action of pointing, are supplemented by other
neurons for specifying the actor of the action, the eye E, the hand H, or the
eye and the hand together EH. There appear inner neurons performing a logical
work, making intermediary proposition, for instance E V EH. Then, we remarked
that adding a second hidden layer and choosing a symmetric metric for learning,
the activities of the neurons become almost quantized and more informative.
Using the work of Carnap and Bar-Hillel 1952, we define a measure of the
logical value for collections of such cells. The logical score growths with the
depth of the layer, i.e. the information on the output decision increases,
which confirms a kind of bottleneck principle. Then we study a bit more complex
tasks, a priori involving predicate logic. We compare the logic and the
measured weights. This shows, for groups of neurons, a neat correlation between
the logical score and the size of the weights. It exhibits a form of sparsity
between the layers. The most spectacular result concerns the triples which can
conclude for all conditions: when applying their weight matrices to their
logical matrix, we recover the classification. This shows that weights
precisely perform the proofs.

    

### [[2108.04760] Multi-Valued Cognitive Maps: Calculations with Linguistic Variables without Using Numbers](http://arxiv.org/abs/2108.04760)


  A concept of multi-valued cognitive maps is introduced in this paper. The
concept expands the fuzzy one. However, all variables and weights are not
linearly ordered in the concept, but are only partially-ordered. Such an ap-
proach allows us to operate in cognitive maps with partially-ordered linguis-
tic variables directly, without vague fuzzification/defuzzification methods.
Hence, we may consider more subtle differences in degrees of experts' uncer-
tainty, than in the fuzzy case. We prove the convergence of such cognitive maps
and give a simple computational example which demonstrates using such a
partially-ordered uncertainty degree scale.

    

### [[2108.04769] On the Foundations of Grounding in Answer Set Programming](http://arxiv.org/abs/2108.04769)


  We provide a comprehensive elaboration of the theoretical foundations of
variable instantiation, or grounding, in Answer Set Programming (ASP). Building
on the semantics of ASP's modeling language, we introduce a formal
characterization of grounding algorithms in terms of (fixed point) operators. A
major role is played by dedicated well-founded operators whose associated
models provide semantic guidance for delineating the result of grounding along
with on-the-fly simplifications. We address an expressive class of logic
programs that incorporates recursive aggregates and thus amounts to the scope
of existing ASP modeling languages. This is accompanied with a plain
algorithmic framework detailing the grounding of recursive aggregates. The
given algorithms correspond essentially to the ones used in the ASP grounder
gringo.

    

### [[2011.03974] Gaussian Processes with Skewed Laplace Spectral Mixture Kernels for Long-term Forecasting](http://arxiv.org/abs/2011.03974)


  Long-term forecasting involves predicting a horizon that is far ahead of the
last observation. It is a problem of high practical relevance, for instance for
companies in order to decide upon expensive long-term investments. Despite the
recent progress and success of Gaussian Processes (GPs) based on Spectral
Mixture kernels, long-term forecasting remains a challenging problem for these
kernels because they decay exponentially at large horizons. This is mainly due
to their use of a mixture of Gaussians to model spectral densities.
Characteristics of the signal important for long-term forecasting can be
unravelled by investigating the distribution of the Fourier coefficients of
(the training part of) the signal, which is non-smooth, heavy-tailed, sparse,
and skewed. The heavy tail and skewness characteristics of such distributions
in the spectral domain allow to capture long-range covariance of the signal in
the time domain. Motivated by these observations, we propose to model spectral
densities using a Skewed Laplace Spectral Mixture (SLSM) due to the skewness of
its peaks, sparsity, non-smoothness, and heavy tail characteristics. By
applying the inverse Fourier Transform to this spectral density we obtain a new
GP kernel for long-term forecasting. In addition, we adapt the lottery ticket
method, originally developed to prune weights of a neural network, to GPs in
order to automatically select the number of kernel components. Results of
extensive experiments, including a multivariate time series, show the
beneficial effect of the proposed SLSM kernel for long-term extrapolation and
robustness to the choice of the number of mixture components.

    

### [[2012.12007] Uncertainty and Surprisal Jointly Deliver the Punchline: Exploiting Incongruity-Based Features for Humor Recognition](http://arxiv.org/abs/2012.12007)


  Humor recognition has been widely studied as a text classification problem
using data-driven approaches. However, most existing work does not examine the
actual joke mechanism to understand humor. We break down any joke into two
distinct components: the set-up and the punchline, and further explore the
special relationship between them. Inspired by the incongruity theory of humor,
we model the set-up as the part developing semantic uncertainty, and the
punchline disrupting audience expectations. With increasingly powerful language
models, we were able to feed the set-up along with the punchline into the GPT-2
language model, and calculate the uncertainty and surprisal values of the
jokes. By conducting experiments on the SemEval 2021 Task 7 dataset, we found
that these two features have better capabilities of telling jokes from
non-jokes, compared with existing baselines.

    

### [[2104.14700] The Zero Resource Speech Challenge 2021: Spoken language modelling](http://arxiv.org/abs/2104.14700)


  We present the Zero Resource Speech Challenge 2021, which asks participants
to learn a language model directly from audio, without any text or labels. The
challenge is based on the Libri-light dataset, which provides up to 60k hours
of audio from English audio books without any associated text. We provide a
pipeline baseline system consisting on an encoder based on contrastive
predictive coding (CPC), a quantizer ($k$-means) and a standard language model
(BERT or LSTM). The metrics evaluate the learned representations at the
acoustic (ABX discrimination), lexical (spot-the-word), syntactic
(acceptability judgment) and semantic levels (similarity judgment). We present
an overview of the eight submitted systems from four groups and discuss the
main results.

    

### [[2108.04145] Long-Horizon Manipulation of Unknown Objects via Task and Motion Planning with Estimated Affordances](http://arxiv.org/abs/2108.04145)


  We present a strategy for designing and building very general robot
manipulation systems involving the integration of a general-purpose
task-and-motion planner with engineered and learned perception modules that
estimate properties and affordances of unknown objects. Such systems are
closed-loop policies that map from RGB images, depth images, and robot joint
encoder measurements to robot joint position commands. We show that following
this strategy a task-and-motion planner can be used to plan intelligent
behaviors even in the absence of a priori knowledge regarding the set of
manipulable objects, their geometries, and their affordances. We explore
several different ways of implementing such perceptual modules for
segmentation, property detection, shape estimation, and grasp generation. We
show how these modules are integrated within the PDDLStream task and motion
planning framework. Finally, we demonstrate that this strategy can enable a
single system to perform a wide variety of real-world multi-step manipulation
tasks, generalizing over a broad class of objects, object arrangements, and
goals, without any prior knowledge of the environment and without re-training.

    

### [[2108.04791] Improving MATLAB's isprime performance without arbitrary-precision arithmetic](http://arxiv.org/abs/2108.04791)


  MATLAB is a numerical computing platform used by scientists, engineers,
mathematicians, and students which contains many mathematical functions,
including isprime. MATLAB's isprime function determines which elements of an
input array are prime. This research details modular arithmetic techniques, the
Miller-Rabin primality test, vectorized operations, and division-minimizing
strategies which harness the power of MATLAB's capabilities to improve
isprime's performance. The results are typically 5 to 10 times faster for small
integers and many hundreds of times faster for large integers and long arrays.

    

### [[2108.04621] Refactoring the Whitby Intelligent Tutoring System for Clean Architecture](http://arxiv.org/abs/2108.04621)


  Whitby is the server-side of an Intelligent Tutoring System application for
learning System-Theoretic Process Analysis (STPA), a methodology used to ensure
the safety of anything that can be represented with a systems model. The
underlying logic driving the reasoning behind Whitby is Situation Calculus,
which is a many-sorted logic with situation, action, and object sorts. The
Situation Calculus is applied to Ontology Authoring and Contingent Scaffolding:
the primary activities within Whitby. Thus many fluents and actions are
aggregated in Whitby from these two sub-applications and from Whitby itself,
but all are available through a common situation query interface that does not
depend upon any of the fluents or actions. Each STPA project in Whitby is a
single situation term, which is queried for fluents that include the ontology,
and to determine what pedagogical interventions to offer.
Initially Whitby was written in Prolog using a module system. In the interest
of a cleaner architecture and implementation with improved code reuse and
extensibility, the initial application was refactored into Logtalk. This
refactoring includes decoupling the Situation Calculus reasoner, Ontology
Authoring framework, and Contingent Scaffolding framework into third-party
libraries that can be reused in other applications. This extraction was
achieved by inverting dependencies via Logtalk protocols and categories, which
are reusable interfaces and components that provide functionally cohesive sets
of predicate declarations and predicate definitions. In this paper the
architectures of two iterations of Whitby are evaluated with respect to the
motivations behind the refactor: clean architecture enabling code reuse and
extensibility.

    

### [[2108.04783] Data-Driven Abductive Inference of Library Specifications](http://arxiv.org/abs/2108.04783)


  Programmers often leverage data structure libraries that provide useful and
reusable abstractions. Modular verification of programs that make use of these
libraries naturally rely on specifications that capture important properties
about how the library expects these data structures to be accessed and
manipulated. However, these specifications are often missing or incomplete,
making it hard for clients to be confident they are using the library safely.
When library source code is also unavailable, as is often the case, the
challenge to infer meaningful specifications is further exacerbated. In this
paper, we present a novel data-driven abductive inference mechanism that infers
specifications for library methods sufficient to enable verification of the
library's clients. Our technique combines a data-driven learning-based
framework to postulate candidate specifications, along with SMT-provided
counterexamples to refine these candidates, taking special care to prevent
generating specifications that overfit to sampled tests. The resulting
specifications form a minimal set of requirements on the behavior of library
implementations that ensures safety of a particular client program. Our
solution thus provides a new multi-abduction procedure for precise
specification inference of data structure libraries guided by client-side
verification tasks. Experimental results on a wide range of realistic OCaml
data structure programs demonstrate the effectiveness of the approach.

    