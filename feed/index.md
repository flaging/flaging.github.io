
## 2021-7-13

### [[2107.04594] Convergence of 5G with Internet of Things for Enhanced Privacy](http://arxiv.org/abs/2107.04594)


  In this paper, we address the issue of privacy in 5th generation (5G) driven
Internet of Things (IoT) and related technologies while presenting a comparison
with previous technologies for communication and unaddressed issues in 5G.
Initially, an overview of 5G driven IoT is presented with details about both
technologies and eventually leading to problems that 5th generation will face.
Details about 5G are also presented while comparing them with previous
technologies. The architecture of 5G is presented hence explaining layers of 5G
and technologies like SDN, NFV and cloud computing that compose these layers.
The architecture for 5g based IoT is also presented for providing visual
understanding as well as explained based on how this addresses the issues
present in 4G. Privacy is highlighted in 5G driven IoT while providing details
about how SDN, NFV and cloud computing helps in elimination of this issue. The
issues presented will be compared with 4G based IoT and solutions are provided
about mitigation of these issues particularly bandwidth and security. Moreover,
techniques used by 4G and 5G technologies for handling the issues of privacy in
IoT are presented in a nutshell as a table. Paper also presents a detailed
overview of technologies making 5G possible meanwhile giving an explanation
about how these technologies resolve privacy issues in 5G.

    

### [[2107.04833] Attack-Aware Synchronization-Free Data Timestamping in LoRaWAN](http://arxiv.org/abs/2107.04833)


  Low-power wide-area network technologies such as LoRaWAN are promising for
collecting low-rate monitoring data from geographically distributed sensors, in
which timestamping the sensor data is a critical system function. This paper
considers a synchronization-free approach to timestamping LoRaWAN uplink data
based on signal arrival time at the gateway, which well matches LoRaWAN's
one-hop star topology and releases bandwidth from transmitting timestamps and
synchronizing end devices' clocks at all times. However, we show that this
approach is susceptible to a {\em frame delay attack} consisting of malicious
frame collision and delayed replay. Real experiments show that the attack can
affect the end devices in large areas up to about $50,000\,\text{m}^2$. In a
broader sense, the attack threatens any system functions requiring timely
deliveries of LoRaWAN frames. To address this threat, we propose a
$\mathsf{LoRaTS}$ gateway design that integrates a commodity LoRaWAN gateway
and a low-power software-defined radio receiver to track the inherent frequency
biases of the end devices. Based on an analytic model of LoRa's chirp spread
spectrum modulation, we develop signal processing algorithms to estimate the
frequency biases with high accuracy beyond that achieved by LoRa's default
demodulation. The accurate frequency bias tracking capability enables the
detection of the attack that introduces additional frequency biases. We also
investigate and implement a more crafty attack that uses advanced radio
apparatuses to eliminate the frequency biases. To address this crafty attack,
we propose a pseudorandom interval hopping scheme to enhance our frequency bias
tracking approach. Extensive experiments show the effectiveness of our approach
in deployments with real affecting factors such as temperature variations.

    

### [[2107.05000] QoS Prediction for 5G Connected and Automated Driving](http://arxiv.org/abs/2107.05000)


  5G communication system can support the demanding quality-of-service (QoS)
requirements of many advanced vehicle-to-everything (V2X) use cases. However,
the safe and efficient driving, especially of automated vehicles, may be
affected by sudden changes of the provided QoS. For that reason, the prediction
of the QoS changes and the early notification of these predicted changes to the
vehicles have been recently enabled by 5G communication systems. This solution
enables the vehicles to avoid or mitigate the effect of sudden QoS changes at
the application level. This article describes how QoS prediction could be
generated by a 5G communication system and delivered to a V2X application. The
tele-operated driving use case is used as an example to analyze the feasibility
of a QoS prediction scheme. Useful recommendations for the development of a QoS
prediction solution are provided, while open research topics are identified.

    

### [[2107.05015] Offloading Optimization with Delay Distribution in the 3-tier Federated Cloud, Edge, and Fog Systems](http://arxiv.org/abs/2107.05015)


  Mobile edge computing and fog computing are promising techniques providing
computation service closer to users to achieve lower latency. In this work, we
study the optimal offloading strategy in the three-tier federated computation
offloading system. We first present queueing models and closed-form solutions
for computing the service delay distribution and the probability of the delay
of a task exceeding a given threshold. We then propose an optimal offloading
probability algorithm based on the sub-gradient method. Our numerical results
show that our simulation results match very well with that of our closed-form
solutions, and our sub-gradient-based search algorithm can find the optimal
offloading probabilities. Specifically, for the given system parameters, our
algorithm yields the optimal QoS violating probability of 0.188 with offloading
probabilities of 0.675 and 0.37 from Fog to edge and from edge to cloud,
respectively.

    

### [[2107.05021] Some Properties of Length Rate Quotient Shapers](http://arxiv.org/abs/2107.05021)


  Length Rate Quotient (LRQ) is the first algorithm of interleaved shaping -- a
novel concept proposed to provide per-flow shaping for a flow aggregate without
per-flow queuing. This concept has been adopted by Time-Sensitive Networking
(TSN) and Deterministic Networking (DetNet). An appealing property of
interleaved shaping is that, when an interleaved shaper is appended to a FIFO
system, it does not increase the worst-case delay of the system. Based on this
"shaping-for-free" property, an approach has been introduced to deliver bounded
end-to-end latency. Specifically, at each output link of a node, class-based
aggregate scheduling is used together with one interleaved shaper per-input
link and per-class, and the interleaved shaper re-shapes every flow to its
initial traffic constraint. In this paper, we investigate other properties of
interleaved LRQ shapers, particularly as stand-alone elements. In addition,
under per-flow setting, we also investigate per-flow LRQ based flow aggregation
and derive its properties. The analysis focuses directly on the timing of
operations, such as shaping and scheduling, in the network. This timing based
method can be found in the Guaranteed Rate (GR) server model and more generally
the max-plus branch of network calculus. With the derived properties, we not
only show that an improved end-to-end latency bound can be obtained for the
current approach, but also demonstrate with two examples that new approaches
may be devised. End-to-end delay bounds for the three approaches are derived
and compared. As a highlight, the two new approaches do not require different
node architectures in allocating (shaping / scheduling) queues, which implies
that they can be readily adapted for use in TSN and DetNet. This together with
the derived properties of LRQ shed new insights on providing the TSN / DetNet
qualities of service.

    

### [[2107.05056] T-s3ra: traffic-aware scheduling for secure slicing and resource allocation in sdn/nfv enabled 5g networks](http://arxiv.org/abs/2107.05056)


  Network slicing and resource allocation play pivotal roles in
software-defined network (SDN)/network function virtualization (NFV)-assisted
5G networks. In 5G communications, the traffic rate is high, necessitating high
data rates and low latency. Deep learning is a potential solution for
overcoming these constraints. Secure slicing avoids resource wastage; however,
DDoS attackers can exploit the sliced network. Therefore, we focused on secure
slicing with resource allocation under massive network traffic. Traffic-aware
scheduling is proposed for secure slicing and resource allocation over
SDN/NFV-enabled 5G networks. In this approach (T-S3RA), user devices are
authenticated using Boolean logic with a password-based key derivation
function. The traffic is scheduled in 5G access points, and secure network
slicing and resource allocation are implemented using deep learning models such
as SliceNet and HopFieldNet, respectively. To predict DDoS attackers, we
computed the Renyi entropy for packet classification. Experiments were
conducted using a network simulator with 250 nodes in the network topology.
Performance was evaluated using metrics such as throughput, latency, packet
transmission ratio, packet loss ratio, slice capacity, bandwidth consumption,
and slice acceptance ratio. T-S3RA was implemented in three 5G use cases with
different requirements including massive machine-type communication,
ultrareliable low-latency communication, and enhanced mobile broadband.

    

### [[2107.05090] Ambrosia: Reduction in Data Transfer from Sensor to Server for Increased Lifetime of IoT Sensor Nodes](http://arxiv.org/abs/2107.05090)


  Data transmission accounts for significant energy consumption in wireless
sensor networks where streaming data is generatedby the sensors. This impedes
their use in many settings, including livestock monitoring over large pastures
(which formsour target application). We present Ambrosia, a lightweight
protocol that utilizes a window-based timeseries forecastingmechanism for data
reduction. Ambrosia employs a configurable error threshold to ensure that the
accuracy of end applicationsis unaffected by the data transfer reduction.
Experimental evaluations using LoRa and BLE on a real livestock
monitoringdeployment demonstrate 60% reduction in data transmission and a 2X
increase in battery lifetime.

    

### [[2107.05114] Spectro-Temporal RF Identification using Deep Learning](http://arxiv.org/abs/2107.05114)


  RF emissions detection, classification, and spectro-temporal localization are
crucial not only for tasks relating to understanding, managing, and protecting
the RF spectrum, but also for safety and security applications such as
detecting intruding drones or jammers. Achieving this goal for wideband
spectrum and in real-time performance is a challenging problem. We present
WRIST, a Wideband, Real-time RF Identification system with Spectro-Temporal
detection, framework and system. Our resulting deep learning model is capable
to detect, classify, and precisely locate RF emissions in time and frequency
using RF samples of 100 MHz spectrum in real-time (over 6Gbps incoming I&Q
streams). Such capabilities are made feasible by leveraging a deep-learning
based one-stage object detection framework, and transfer learning to a
multi-channel image-based RF signals representation. We also introduce an
iterative training approach which leverages synthesized and augmented RF data
to efficiently build large labelled datasets of RF emissions (SPREAD). WRIST
detector achieves 90 mean Average Precision even in extremely congested
environment in the wild. WRIST model classifies five technologies (Bluetooth,
Lightbridge, Wi-Fi, XPD, and ZigBee) and is easily extendable to others. We are
making our curated and annotated dataset available to the whole community. It
consists of nearly 1 million fully labelled RF emissions collected from various
off-the-shelf wireless radios in a range of environments and spanning the five
classes of emissions.

    

### [[2107.05181] AoI-minimizing Scheduling in UAV-relayed IoT Networks](http://arxiv.org/abs/2107.05181)


  Due to flexibility, autonomy and low operational cost, unmanned aerial
vehicles (UAVs), as fixed aerial base stations, are increasingly being used as
\textit{relays} to collect time-sensitive information (i.e., status updates)
from IoT devices and deliver it to the nearby terrestrial base station (TBS),
where the information gets processed. In order to ensure timely delivery of
information to the TBS (from all IoT devices), optimal scheduling of
time-sensitive information over two hop UAV-relayed IoT networks (i.e., IoT
device to the UAV [hop 1], and UAV to the TBS [hop 2]) becomes a critical
challenge. To address this, we propose scheduling policies for Age of
Information (AoI) minimization in such two-hop UAV-relayed IoT networks. To
this end, we present a low-complexity MAF-MAD scheduler, that employs Maximum
AoI First (MAF) policy for sampling of IoT devices at UAV (hop 1) and Maximum
AoI Difference (MAD) policy for updating sampled packets from UAV to the TBS
(hop 2). We show that MAF-MAD is the optimal scheduler under ideal conditions,
i.e., error-free channels and generate-at-will traffic generation at IoT
devices. On the contrary, for realistic conditions, we propose a
Deep-Q-Networks (DQN) based scheduler. Our simulation results show that
DQN-based scheduler outperforms MAF-MAD scheduler and three other baseline
schedulers, i.e., Maximal AoI First (MAF), Round Robin (RR) and Random,
employed at both hops under general conditions when the network is small (with
10's of IoT devices). However, it does not scale well with network size whereas
MAF-MAD outperforms all other schedulers under all considered scenarios for
larger networks.

    

### [[2107.05322] Optimally Reliable & Cheap Payment Flows on the Lightning Network](http://arxiv.org/abs/2107.05322)


  Today, payment paths in Bitcoin's Lightning Network are found by searching
for shortest paths on the fee graph. We enhance this approach in two
dimensions. Firstly, we take into account the probability of a payment actually
being possible due to the unknown balance distributions in the channels.
Secondly, we use minimum cost flows as a proper generalization of shortest
paths to multi-part payments (MPP). In particular we show that under plausible
assumptions about the balance distributions we can find the most likely MPP for
any given set of senders, recipients and amounts by solving for a (generalized)
integer minimum cost flow with a separable and convex cost function. Polynomial
time exact algorithms as well as approximations are known for this optimization
problem. We present a round-based algorithm of min-cost flow computations for
delivering large payment amounts over the Lightning Network. This algorithm
works by updating the probability distributions with the information gained
from both successful and unsuccessful paths on prior rounds. In all our
experiments a single digit number of rounds sufficed to deliver payments of
sizes that were close to the total local balance of the sender. Early
experiments indicate that our approach increases the size of payments that can
be reliably delivered by several orders of magnitude compared to the current
state of the art. We observe that finding the cheapest multi-part payments is
an NP-hard problem considering the current fee structure and propose dropping
the base fee to make it a linear min-cost flow problem. Finally, we discuss
possibilities for maximizing the probability while at the same time minimizing
the fees of a flow. While this turns out to be a hard problem in general as
well - even in the single path case - it appears to be surprisingly tractable
in practice.

    

### [[2107.05343] ETVO: Effectively Measuring Tactile Internet with Experimental Validation](http://arxiv.org/abs/2107.05343)


  The next frontier in communications is teleoperation -- manipulation and
control of remote environments with feedback. Compared to conventional
networked applications, teleoperation poses widely different requirements,
ultra-low latency (ULL) is primary. Realizing ULL communication demands
significant redesign of conventional networking techniques, and the network
infrastructure envisioned for achieving this is termed as Tactile Internet
(TI). The design of the network infrastructure and meaningful performance
metrics are crucial for seamless TI communication. However, existing
performance metrics fall severely short of comprehensively characterizing TI
performance. We take the first step towards bridging this gap. We take Dynamic
Time Warping(DTW) as the basis of our work and identify necessary changes for
characterizing TI performance. Through substantial refinements to DTW, we
design Effective Time- and Value-Offset (ETVO) -- a new method for measuring
the fine-grained performance of TI systems. Through an in-depth objective
analysis, we demonstrate the improvements of ETVO over DTW. Through
human-in-the-loop subjective experiments, we demonstrate how and why existing
QoS and QoE methods fall short of estimating the TI session performance
accurately. Using subjective experiments, we demonstrate the behavior of the
proposed metrics, their ability to match theoretically derived performance, and
finally their ability to reflect user satisfaction in a practical setting. The
results are highly encouraging.

    

### [[2102.08055] Zero-Shot Adaptation for mmWave Beam-Tracking on Overhead Messenger Wires through Robust Adversarial Reinforcement Learning](http://arxiv.org/abs/2102.08055)


  Millimeter wave (mmWave) beam-tracking based on machine learning enables the
development of accurate tracking policies while obviating the need to
periodically solve beam-optimization problems. However, its applicability is
still arguable when training-test gaps exist in terms of environmental
parameters that affect the node dynamics. From this skeptical point of view,
the contribution of this study is twofold. First, by considering an example
scenario, we confirm that the training-test gap adversely affects the
beam-tracking performance. More specifically, we consider nodes placed on
overhead messenger wires, where the node dynamics are affected by several
environmental parameters, e.g, the wire mass and tension. Although these are
particular scenarios, they yield insight into the validation of the
training-test gap problems. Second, we demonstrate the feasibility of
\textit{zero-shot adaptation} as a solution, where a learning agent adapts to
environmental parameters unseen during training. This is achieved by leveraging
a robust adversarial reinforcement learning (RARL) technique, where such
training-and-test gaps are regarded as disturbances by adversaries that are
jointly trained with a legitimate beam-tracking agent. Numerical evaluations
demonstrate that the beam-tracking policy learned via RARL can be applied to a
wide range of environmental parameters without severely degrading the received
power.

    

### [[2104.09835] WiFiMod: Transformer-based Indoor Human Mobility Modeling using Passive Sensing](http://arxiv.org/abs/2104.09835)


  Modeling human mobility has a wide range of applications from urban planning
to simulations of disease spread. It is well known that humans spend 80% of
their time indoors but modeling indoor human mobility is challenging due to
three main reasons: (i) the absence of easily acquirable, reliable, low-cost
indoor mobility datasets, (ii) high prediction space in modeling the frequent
indoor mobility, and (iii) multi-scalar periodicity and correlations in
mobility. To deal with all these challenges, we propose WiFiMod, a
Transformer-based, data-driven approach that models indoor human mobility at
multiple spatial scales using WiFi system logs. WiFiMod takes as input
enterprise WiFi system logs to extract human mobility trajectories from
smartphone digital traces. Next, for each extracted trajectory, we identify the
mobility features at multiple spatial scales, macro, and micro, to design a
multi-modal embedding Transformer that predicts user mobility for several hours
to an entire day across multiple spatial granularities. Multi-modal embedding
captures the mobility periodicity and correlations across various scales while
Transformers capture long-term mobility dependencies boosting model prediction
performance. This approach significantly reduces the prediction space by first
predicting macro mobility, then modeling indoor scale mobility, micro-mobility,
conditioned on the estimated macro mobility distribution, thereby using the
topological constraint of the macro-scale. Experimental results show that
WiFiMod achieves a prediction accuracy of at least 10% points higher than the
current state-of-art models. Additionally, we present 3 real-world applications
of WiFiMod - (i) predict high-density hot pockets for policy-making decisions
for COVID19 or ILI, (ii) generate a realistic simulation of indoor mobility,
(iii) design personal assistants.

    

### [[2105.05924] Silicon Photonics in Optical Access Networks for 5G Communications](http://arxiv.org/abs/2105.05924)


  Only radio access networks can provide connectivity across multiple antenna
sites to achieve the great leap forward in capacity targeted by 5G. Optical
fronthaul remains a sticking point in that connectivity, and we make the case
for analog radio over fiber signals and an optical access network smartedge to
achieve the potential of radio access networks. The edge of the network would
house the intelligence that coordinates wireless transmissions to minimize
interference and maximize throughput. As silicon photonics provides a hardware
platform well adapted to support optical fronthaul, it is poised to drive smart
edge adoption. We draw out the issues in adopting oursolution, propose a
strategy for network densification, and cite recent demonstrations to support
our approach.

    

### [[2105.15105] SeReMAS: Self-Resilient Mobile Autonomous Systems Through Predictive Edge Computing](http://arxiv.org/abs/2105.15105)


  Edge computing enables Mobile Autonomous Systems (MASs) to execute continuous
streams of heavy-duty mission-critical processing tasks, such as real-time
obstacle detection and navigation. However, in practical applications, erratic
patterns in channel quality, network load, and edge server load can interrupt
the task flow execution, which necessarily leads to severe disruption of the
system's key operations. Existing work has mostly tackled the problem with
reactive approaches, which cannot guarantee task-level reliability. Conversely,
in this paper we focus on learning-based predictive edge computing to achieve
self-resilient task offloading. By conducting a preliminary experimental
evaluation, we show that there is no dominant feature that can predict the
edge-MAS system reliability, which calls for an ensemble and selection of
weaker features. To tackle the complexity of the problem, we propose SeReMAS, a
data-driven optimization framework. We first mathematically formulate a
Redundant Task Offloading Problem (RTOP), where a MAS may connect to multiple
edge servers for redundancy, and needs to select which server(s) to transmit
its computing tasks in order to maximize the probability of task execution
while minimizing channel and edge resource utilization. We then create a
predictor based on Deep Reinforcement Learning (DRL), which produces the
optimum task assignment based on application-, network- and telemetry-based
features. We prototype SeReMAS on a testbed composed by a drone, mounting a
PixHawk flight controller, a Jetson Nano board, and three 802.11n WiFi
interfaces. We extensively evaluate SeReMAS by considering an application where
one drone offloads high-resolution images for real-time analysis to three edge
servers on the ground. Experimental results show that SeReMAS improves task
execution probability by $17\%$ with respect to existing reactive-based
approaches.

    

### [[2106.05086] Design and Implementation of 5G eHealth Systems, Technologies, Use Cases and Future Challenges](http://arxiv.org/abs/2106.05086)


  Fifth generation (5G) aims to connect massive devices with even higher
reliability, lower latency and even faster transmission speed, which are vital
for implementing the e-health systems. However, the current efforts on 5G
e-health systems are still not enough to accomplish its full blueprint. In this
article, we first discuss the related technologies from physical layer, upper
layer and cross layer perspectives on designing the 5G e-health systems. We
afterwards elaborate two use cases according to our implementations, i.e., 5G
e-health systems for remote health and 5G e-health systems for Covid-19
pandemic containment. We finally envision the future research trends and
challenges of 5G e-health systems.

    

### [[2107.00715] NDN4IVC: A Framework for Simulating and Testing of Applications in Vehicular Named Data Networking](http://arxiv.org/abs/2107.00715)


  This paper presents a customized framework (NDN4IVC) for simulating and
testing intelligent transportation systems and applications in vehicular
named-data networking (V-NDN). The project uses two popular simulators in the
literature for VANET simulation, a network simulator based on discrete events
(Ns-3), with ndnSIM module installed, and Sumo, a simulator for urban mobility.
NDN4IVC allows bidirectional communication between Sumo and Ns-3 and integrates
the NDN stack and the NFD (NDN Forwarding Daemon) code. The project also brings
together a comprehensive set of codes, models, functionalities, and
technologies to improve proposals and protocols in V-NDN.

    

### [[2107.04616] SITHCon: A neural network robust to variations in input scaling on the time dimension](http://arxiv.org/abs/2107.04616)


  In machine learning, convolutional neural networks (CNNs) have been extremely
influential in both computer vision and in recognizing patterns extended over
time. In computer vision, part of the flexibility arises from the use of
max-pooling operations over the convolutions to attain translation invariance.
In the mammalian brain, neural representations of time use a set of temporal
basis functions. Critically, these basis functions appear to be arranged in a
geometric series such that the basis set is evenly distributed over logarithmic
time. This paper introduces a Scale-Invariant Temporal History Convolution
network (SITHCon) that uses a logarithmically-distributed temporal memory. A
max-pool over a logarithmically-distributed temporal memory results in
scale-invariance in time. We compare performance of SITHCon to a Temporal
Convolution Network (TCN) and demonstrate that, although both networks can
learn classification and regression problems on both univariate and
multivariate time series $f(t)$, only SITHCon has the property that it
generalizes without retraining to rescaled versions of the input $f(at)$. This
property, inspired by findings from neuroscience and psychology, could lead to
large-scale networks with dramatically different capabilities, including faster
training and greater generalizability, even with significantly fewer free
parameters.

    

### [[2107.04619] Diverse Video Generation using a Gaussian Process Trigger](http://arxiv.org/abs/2107.04619)


  Generating future frames given a few context (or past) frames is a
challenging task. It requires modeling the temporal coherence of videos and
multi-modality in terms of diversity in the potential future states. Current
variational approaches for video generation tend to marginalize over
multi-modal future outcomes. Instead, we propose to explicitly model the
multi-modality in the future outcomes and leverage it to sample diverse
futures. Our approach, Diverse Video Generator, uses a Gaussian Process (GP) to
learn priors on future states given the past and maintains a probability
distribution over possible futures given a particular sample. In addition, we
leverage the changes in this distribution over time to control the sampling of
diverse future states by estimating the end of ongoing sequences. That is, we
use the variance of GP over the output function space to trigger a change in an
action sequence. We achieve state-of-the-art results on diverse future frame
generation in terms of reconstruction quality and diversity of the generated
sequences.

    

### [[2107.04633] Learning Probabilistic Reward Machines from Non-Markovian Stochastic Reward Processes](http://arxiv.org/abs/2107.04633)


  The success of reinforcement learning in typical settings is, in part,
predicated on underlying Markovian assumptions on the reward signal by which an
agent learns optimal policies. In recent years, the use of reward machines has
relaxed this assumption by enabling a structured representation of
non-Markovian rewards. In particular, such representations can be used to
augment the state space of the underlying decision process, thereby
facilitating non-Markovian reinforcement learning. However, these reward
machines cannot capture the semantics of stochastic reward signals. In this
paper, we make progress on this front by introducing probabilistic reward
machines (PRMs) as a representation of non-Markovian stochastic rewards. We
present an algorithm to learn PRMs from the underlying decision process as well
as to learn the PRM representation of a given decision-making policy.

    

### [[2107.04641] Training Over-parameterized Models with Non-decomposable Objectives](http://arxiv.org/abs/2107.04641)


  Many modern machine learning applications come with complex and nuanced
design goals such as minimizing the worst-case error, satisfying a given
precision or recall target, or enforcing group-fairness constraints. Popular
techniques for optimizing such non-decomposable objectives reduce the problem
into a sequence of cost-sensitive learning tasks, each of which is then solved
by re-weighting the training loss with example-specific costs. We point out
that the standard approach of re-weighting the loss to incorporate label costs
can produce unsatisfactory results when used to train over-parameterized
models. As a remedy, we propose new cost-sensitive losses that extend the
classical idea of logit adjustment to handle more general cost matrices. Our
losses are calibrated, and can be further improved with distilled labels from a
teacher model. Through experiments on benchmark image datasets, we showcase the
effectiveness of our approach in training ResNet models with common robust and
constrained optimization objectives.

    

### [[2107.04642] Impossibility of What? Formal and Substantive Equality in Algorithmic Fairness](http://arxiv.org/abs/2107.04642)


  In the face of compounding crises of social and economic inequality, many
have turned to algorithmic decision-making to achieve greater fairness in
society. As these efforts intensify, reasoning within the burgeoning field of
"algorithmic fairness" increasingly shapes how fairness manifests in practice.
This paper interrogates whether algorithmic fairness provides the appropriate
conceptual and practical tools for enhancing social equality. I argue that the
dominant, "formal" approach to algorithmic fairness is ill-equipped as a
framework for pursuing equality, as its narrow frame of analysis generates
restrictive approaches to reform. In light of these shortcomings, I propose an
alternative: a "substantive" approach to algorithmic fairness that centers
opposition to social hierarchies and provides a more expansive analysis of how
to address inequality. This substantive approach enables more fruitful
theorizing about the role of algorithms in combatting oppression. The
distinction between formal and substantive algorithmic fairness is exemplified
by each approach's responses to the "impossibility of fairness" (an
incompatibility between mathematical definitions of algorithmic fairness).
While the formal approach requires us to accept the "impossibility of fairness"
as a harsh limit on efforts to enhance equality, the substantive approach
allows us to escape the "impossibility of fairness" by suggesting reforms that
are not subject to this false dilemma and that are better equipped to
ameliorate conditions of social oppression.

    

### [[2107.04649] Accuracy on the Line: On the Strong Correlation Between Out-of-Distribution and In-Distribution Generalization](http://arxiv.org/abs/2107.04649)


  For machine learning systems to be reliable, we must understand their
performance in unseen, out-of-distribution environments. In this paper, we
empirically show that out-of-distribution performance is strongly correlated
with in-distribution performance for a wide range of models and distribution
shifts. Specifically, we demonstrate strong correlations between
in-distribution and out-of-distribution performance on variants of CIFAR-10 &
ImageNet, a synthetic pose estimation task derived from YCB objects, satellite
imagery classification in FMoW-WILDS, and wildlife classification in
iWildCam-WILDS. The strong correlations hold across model architectures,
hyperparameters, training set size, and training duration, and are more precise
than what is expected from existing domain adaptation theory. To complete the
picture, we also investigate cases where the correlation is weaker, for
instance some synthetic distribution shifts from CIFAR-10-C and the tissue
classification dataset Camelyon17-WILDS. Finally, we provide a candidate theory
based on a Gaussian data model that shows how changes in the data covariance
arising from distribution shift can affect the observed correlations.

    

### [[2107.04652] The Effects of Invertibility on the Representational Complexity of Encoders in Variational Autoencoders](http://arxiv.org/abs/2107.04652)


  Training and using modern neural-network based latent-variable generative
models (like Variational Autoencoders) often require simultaneously training a
generative direction along with an inferential(encoding) direction, which
approximates the posterior distribution over the latent variables. Thus, the
question arises: how complex does the inferential model need to be, in order to
be able to accurately model the posterior distribution of a given generative
model?
In this paper, we identify an important property of the generative map
impacting the required size of the encoder. We show that if the generative map
is "strongly invertible" (in a sense we suitably formalize), the inferential
model need not be much more complex. Conversely, we prove that there exist
non-invertible generative maps, for which the encoding direction needs to be
exponentially larger (under standard assumptions in computational complexity).
Importantly, we do not require the generative model to be layerwise invertible,
which a lot of the related literature assumes and isn't satisfied by many
architectures used in practice (e.g. convolution and pooling based networks).
Thus, we provide theoretical support for the empirical wisdom that learning
deep generative models is harder when data lies on a low-dimensional manifold.

    

### [[2107.04661] Hölder Bounds for Sensitivity Analysis in Causal Reasoning](http://arxiv.org/abs/2107.04661)


  We examine interval estimation of the effect of a treatment T on an outcome Y
given the existence of an unobserved confounder U. Using Hölder's inequality,
we derive a set of bounds on the confounding bias |E[Y|T=t]-E[Y|do(T=t)]| based
on the degree of unmeasured confounding (i.e., the strength of the connection
U->T, and the strength of U->Y). These bounds are tight either when U is
independent of T or when U is independent of Y given T (when there is no
unobserved confounding). We focus on a special case of this bound depending on
the total variation distance between the distributions p(U) and p(U|T=t), as
well as the maximum (over all possible values of U) deviation of the
conditional expected outcome E[Y|U=u,T=t] from the average expected outcome
E[Y|T=t]. We discuss possible calibration strategies for this bound to get
interval estimates for treatment effects, and experimentally validate the bound
using synthetic and semi-synthetic datasets.

    

### [[2107.04680] A Framework and Benchmarking Study for Counterfactual Generating Methods on Tabular Data](http://arxiv.org/abs/2107.04680)


  Counterfactual explanations are viewed as an effective way to explain machine
learning predictions. This interest is reflected by a relatively young
literature with already dozens of algorithms aiming to generate such
explanations. These algorithms are focused on finding how features can be
modified to change the output classification. However, this rather general
objective can be achieved in different ways, which brings about the need for a
methodology to test and benchmark these algorithms. The contributions of this
work are manifold: First, a large benchmarking study of 10 algorithmic
approaches on 22 tabular datasets is performed, using 9 relevant evaluation
metrics. Second, the introduction of a novel, first of its kind, framework to
test counterfactual generation algorithms. Third, a set of objective metrics to
evaluate and compare counterfactual results. And finally, insight from the
benchmarking results that indicate which approaches obtain the best performance
on what type of dataset. This benchmarking study and framework can help
practitioners in determining which technique and building blocks most suit
their context, and can help researchers in the design and evaluation of current
and future counterfactual generation algorithms. Our findings show that,
overall, there's no single best algorithm to generate counterfactual
explanations as the performance highly depends on properties related to the
dataset, model, score and factual point specificities.

    

### [[2107.04689] Lifelong Teacher-Student Network Learning](http://arxiv.org/abs/2107.04689)


  A unique cognitive capability of humans consists in their ability to acquire
new knowledge and skills from a sequence of experiences. Meanwhile, artificial
intelligence systems are good at learning only the last given task without
being able to remember the databases learnt in the past. We propose a novel
lifelong learning methodology by employing a Teacher-Student network framework.
While the Student module is trained with a new given database, the Teacher
module would remind the Student about the information learnt in the past. The
Teacher, implemented by a Generative Adversarial Network (GAN), is trained to
preserve and replay past knowledge corresponding to the probabilistic
representations of previously learn databases. Meanwhile, the Student module is
implemented by a Variational Autoencoder (VAE) which infers its latent variable
representation from both the output of the Teacher module as well as from the
newly available database. Moreover, the Student module is trained to capture
both continuous and discrete underlying data representations across different
domains. The proposed lifelong learning framework is applied in supervised,
semi-supervised and unsupervised training. The code is available~:
\url{this https URL}

    

### [[2107.04694] Lifelong Mixture of Variational Autoencoders](http://arxiv.org/abs/2107.04694)


  In this paper, we propose an end-to-end lifelong learning mixture of experts.
Each expert is implemented by a Variational Autoencoder (VAE). The experts in
the mixture system are jointly trained by maximizing a mixture of individual
component evidence lower bounds (MELBO) on the log-likelihood of the given
training samples. The mixing coefficients in the mixture, control the
contributions of each expert in the goal representation. These are sampled from
a Dirichlet distribution whose parameters are determined through non-parametric
estimation during lifelong learning. The model can learn new tasks fast when
these are similar to those previously learnt. The proposed Lifelong mixture of
VAE (L-MVAE) expands its architecture with new components when learning a
completely new task. After the training, our model can automatically determine
the relevant expert to be used when fed with new data samples. This mechanism
benefits both the memory efficiency and the required computational cost as only
one expert is used during the inference. The L-MVAE inference model is able to
perform interpolation in the joint latent space across the data domains
associated with different tasks and is shown to be efficient for disentangled
learning representation.

    

### [[2107.04695] L2M: Practical posterior Laplace approximation with optimization-driven second moment estimation](http://arxiv.org/abs/2107.04695)


  Uncertainty quantification for deep neural networks has recently evolved
through many techniques. In this work, we revisit Laplace approximation, a
classical approach for posterior approximation that is computationally
attractive. However, instead of computing the curvature matrix, we show that,
under some regularity conditions, the Laplace approximation can be easily
constructed using the gradient second moment. This quantity is already
estimated by many exponential moving average variants of Adagrad such as Adam
and RMSprop, but is traditionally discarded after training. We show that our
method (L2M) does not require changes in models or optimization, can be
implemented in a few lines of code to yield reasonable results, and it does not
require any extra computational steps besides what is already being computed by
optimizers, without introducing any new hyperparameter. We hope our method can
open new research directions on using quantities already computed by optimizers
for uncertainty estimation in deep neural networks.

    

### [[2107.04705] InfoVAEGAN : learning joint interpretable representations by information maximization and maximum likelihood](http://arxiv.org/abs/2107.04705)


  Learning disentangled and interpretable representations is an important step
towards accomplishing comprehensive data representations on the manifold. In
this paper, we propose a novel representation learning algorithm which combines
the inference abilities of Variational Autoencoders (VAE) with the
generalization capability of Generative Adversarial Networks (GAN). The
proposed model, called InfoVAEGAN, consists of three networks~: Encoder,
Generator and Discriminator. InfoVAEGAN aims to jointly learn discrete and
continuous interpretable representations in an unsupervised manner by using two
different data-free log-likelihood functions onto the variables sampled from
the generator's distribution. We propose a two-stage algorithm for optimizing
the inference network separately from the generator training. Moreover, we
enforce the learning of interpretable representations through the maximization
of the mutual information between the existing latent variables and those
created through generative and inference processes.

    

### [[2107.04713] Automated Graph Learning via Population Based Self-Tuning GCN](http://arxiv.org/abs/2107.04713)


  Owing to the remarkable capability of extracting effective graph embeddings,
graph convolutional network (GCN) and its variants have been successfully
applied to a broad range of tasks, such as node classification, link
prediction, and graph classification. Traditional GCN models suffer from the
issues of overfitting and oversmoothing, while some recent techniques like
DropEdge could alleviate these issues and thus enable the development of deep
GCN. However, training GCN models is non-trivial, as it is sensitive to the
choice of hyperparameters such as dropout rate and learning weight decay,
especially for deep GCN models. In this paper, we aim to automate the training
of GCN models through hyperparameter optimization. To be specific, we propose a
self-tuning GCN approach with an alternate training algorithm, and further
extend our approach by incorporating the population based training scheme.
Experimental results on three benchmark datasets demonstrate the effectiveness
of our approaches on optimizing multi-layer GCN, compared with several
representative baselines.

    

### [[2107.04714] A Topological-Framework to Improve Analysis of Machine Learning Model Performance](http://arxiv.org/abs/2107.04714)


  As both machine learning models and the datasets on which they are evaluated
have grown in size and complexity, the practice of using a few summary
statistics to understand model performance has become increasingly problematic.
This is particularly true in real-world scenarios where understanding model
failure on certain subpopulations of the data is of critical importance. In
this paper we propose a topological framework for evaluating machine learning
models in which a dataset is treated as a "space" on which a model operates.
This provides us with a principled way to organize information about model
performance at both the global level (over the entire test set) and also the
local level (on specific subpopulations). Finally, we describe a topological
data structure, presheaves, which offer a convenient way to store and analyze
model performance between different subpopulations.

    

### [[2107.04721] U-Net with Hierarchical Bottleneck Attention for Landmark Detection in Fundus Images of the Degenerated Retina](http://arxiv.org/abs/2107.04721)


  Fundus photography has routinely been used to document the presence and
severity of retinal degenerative diseases such as age-related macular
degeneration (AMD), glaucoma, and diabetic retinopathy (DR) in clinical
practice, for which the fovea and optic disc (OD) are important retinal
landmarks. However, the occurrence of lesions, drusen, and other retinal
abnormalities during retinal degeneration severely complicates automatic
landmark detection and segmentation. Here we propose HBA-U-Net: a U-Net
backbone enriched with hierarchical bottleneck attention. The network consists
of a novel bottleneck attention block that combines and refines self-attention,
channel attention, and relative-position attention to highlight retinal
abnormalities that may be important for fovea and OD segmentation in the
degenerated retina. HBA-U-Net achieved state-of-the-art results on fovea
detection across datasets and eye conditions (ADAM: Euclidean Distance (ED) of
25.4 pixels, REFUGE: 32.5 pixels, IDRiD: 32.1 pixels), on OD segmentation for
AMD (ADAM: Dice Coefficient (DC) of 0.947), and on OD detection for DR (IDRiD:
ED of 20.5 pixels). Our results suggest that HBA-U-Net may be well suited for
landmark detection in the presence of a variety of retinal degenerative
diseases.

    

### [[2107.04724] Longitudinal Correlation Analysis for Decoding Multi-Modal Brain Development](http://arxiv.org/abs/2107.04724)


  Starting from childhood, the human brain restructures and rewires throughout
life. Characterizing such complex brain development requires effective analysis
of longitudinal and multi-modal neuroimaging data. Here, we propose such an
analysis approach named Longitudinal Correlation Analysis (LCA). LCA couples
the data of two modalities by first reducing the input from each modality to a
latent representation based on autoencoders. A self-supervised strategy then
relates the two latent spaces by jointly disentangling two directions, one in
each space, such that the longitudinal changes in latent representations along
those directions are maximally correlated between modalities. We applied LCA to
analyze the longitudinal T1-weighted and diffusion-weighted MRIs of 679 youths
from the National Consortium on Alcohol and Neurodevelopment in Adolescence.
Unlike existing approaches that focus on either cross-sectional or single-modal
modeling, LCA successfully unraveled coupled macrostructural and
microstructural brain development from morphological and diffusivity features
extracted from the data. A retesting of LCA on raw 3D image volumes of those
subjects successfully replicated the findings from the feature-based analysis.
Lastly, the developmental effects revealed by LCA were inline with the current
understanding of maturational patterns of the adolescent brain.

    

### [[2107.04734] Layer-wise Analysis of a Self-supervised Speech Representation Model](http://arxiv.org/abs/2107.04734)


  Recently proposed self-supervised learning approaches have been successful
for pre-training speech representation models. The utility of these learned
representations has been observed empirically, but not much has been studied
about the type or extent of information encoded in the pre-trained
representations themselves. Developing such insights can help understand the
capabilities and limits of these models and enable the research community to
more efficiently develop their usage for downstream applications. In this work,
we begin to fill this gap by examining one recent and successful pre-trained
model (wav2vec 2.0), via its intermediate representation vectors, using a suite
of analysis tools. We use the metrics of canonical correlation, mutual
information, and performance on simple downstream tasks with non-parametric
probes, in order to (i) query for acoustic and linguistic information content,
(ii) characterize the evolution of information across model layers, and (iii)
understand how fine-tuning the model for automatic speech recognition (ASR)
affects these observations. Our findings motivate modifying the fine-tuning
protocol for ASR, which produces improved word error rates in a low-resource
setting.

    

### [[2107.04750] Multi-Agent Imitation Learning with Copulas](http://arxiv.org/abs/2107.04750)


  Multi-agent imitation learning aims to train multiple agents to perform tasks
from demonstrations by learning a mapping between observations and actions,
which is essential for understanding physical, social, and team-play systems.
However, most existing works on modeling multi-agent interactions typically
assume that agents make independent decisions based on their observations,
ignoring the complex dependence among agents. In this paper, we propose to use
copula, a powerful statistical tool for capturing dependence among random
variables, to explicitly model the correlation and coordination in multi-agent
systems. Our proposed model is able to separately learn marginals that capture
the local behavioral patterns of each individual agent, as well as a copula
function that solely and fully captures the dependence structure among agents.
Extensive experiments on synthetic and real-world datasets show that our model
outperforms state-of-the-art baselines across various scenarios in the action
prediction task, and is able to generate new trajectories close to expert
demonstrations.

    

### [[2107.04755] Beyond Low-pass Filtering: Graph Convolutional Networks with Automatic Filtering](http://arxiv.org/abs/2107.04755)


  Graph convolutional networks are becoming indispensable for deep learning
from graph-structured data. Most of the existing graph convolutional networks
share two big shortcomings. First, they are essentially low-pass filters, thus
the potentially useful middle and high frequency band of graph signals are
ignored. Second, the bandwidth of existing graph convolutional filters is
fixed. Parameters of a graph convolutional filter only transform the graph
inputs without changing the curvature of a graph convolutional filter function.
In reality, we are uncertain about whether we should retain or cut off the
frequency at a certain point unless we have expert domain knowledge. In this
paper, we propose Automatic Graph Convolutional Networks (AutoGCN) to capture
the full spectrum of graph signals and automatically update the bandwidth of
graph convolutional filters. While it is based on graph spectral theory, our
AutoGCN is also localized in space and has a spatial form. Experimental results
show that AutoGCN achieves significant improvement over baseline methods which
only work as low-pass filters.

    

### [[2107.04764] Hack The Box: Fooling Deep Learning Abstraction-Based Monitors](http://arxiv.org/abs/2107.04764)


  Deep learning is a type of machine learning that adapts a deep hierarchy of
concepts. Deep learning classifiers link the most basic version of concepts at
the input layer to the most abstract version of concepts at the output layer,
also known as a class or label. However, once trained over a finite set of
classes, a deep learning model does not have the power to say that a given
input does not belong to any of the classes and simply cannot be linked.
Correctly invalidating the prediction of unrelated classes is a challenging
problem that has been tackled in many ways in the literature. Novelty detection
gives deep learning the ability to output "do not know" for novel/unseen
classes. Still, no attention has been given to the security aspects of novelty
detection. In this paper, we consider the case study of abstraction-based
novelty detection and show that it is not robust against adversarial samples.
Moreover, we show the feasibility of crafting adversarial samples that fool the
deep learning classifier and bypass the novelty detection monitoring at the
same time. In other words, these monitoring boxes are hackable. We demonstrate
that novelty detection itself ends up as an attack surface.

    

### [[2107.04766] Convergence Analysis of Schr{ö}dinger-F{ö}llmer Sampler without Convexity](http://arxiv.org/abs/2107.04766)


  Schrödinger-Föllmer sampler (SFS) is a novel and efficient approach
for sampling from possibly unnormalized distributions without ergodicity. SFS
is based on the Euler-Maruyama discretization of Schrödinger-Föllmer
diffusion process $$\mathrm{d} X_{t}=-\nabla U\left(X_t, t\right) \mathrm{d}
t+\mathrm{d} B_{t}, \quad t \in[0,1],\quad X_0=0$$ on the unit interval, which
transports the degenerate distribution at time zero to the target distribution
at time one. In \cite{sfs21}, the consistency of SFS is established under a
restricted assumption that %the drift term $b(x,t)$ the potential $U(x,t)$ is
uniformly (on $t$) strongly %concave convex (on $x$). In this paper we provide
a nonasymptotic error bound of SFS in Wasserstein distance under some smooth
and bounded conditions on the density ratio of the target distribution over the
standard normal distribution, but without requiring the strongly convexity of
the potential.

    

### [[2107.04773] Is a Single Model Enough? MuCoS: A Multi-Model Ensemble Learning for Semantic Code Search](http://arxiv.org/abs/2107.04773)


  Recently, deep learning methods have become mainstream in code search since
they do better at capturing semantic correlations between code snippets and
search queries and have promising performance. However, code snippets have
diverse information from different dimensions, such as business logic, specific
algorithm, and hardware communication, so it is hard for a single code
representation module to cover all the perspectives. On the other hand, as a
specific query may focus on one or several perspectives, it is difficult for a
single query representation module to represent different user intents. In this
paper, we propose MuCoS, a multi-model ensemble learning architecture for
semantic code search. It combines several individual learners, each of which
emphasizes a specific perspective of code snippets. We train the individual
learners on different datasets which contain different perspectives of code
information, and we use a data augmentation strategy to get these different
datasets. Then we ensemble the learners to capture comprehensive features of
code snippets.

    

### [[2107.04775] LS3: Latent Space Safe Sets for Long-Horizon Visuomotor Control of Iterative Tasks](http://arxiv.org/abs/2107.04775)


  Reinforcement learning (RL) algorithms have shown impressive success in
exploring high-dimensional environments to learn complex, long-horizon tasks,
but can often exhibit unsafe behaviors and require extensive environment
interaction when exploration is unconstrained. A promising strategy for safe
learning in dynamically uncertain environments is requiring that the agent can
robustly return to states where task success (and therefore safety) can be
guaranteed. While this approach has been successful in low-dimensions,
enforcing this constraint in environments with high-dimensional state spaces,
such as images, is challenging. We present Latent Space Safe Sets (LS3), which
extends this strategy to iterative, long-horizon tasks with image observations
by using suboptimal demonstrations and a learned dynamics model to restrict
exploration to the neighborhood of a learned Safe Set where task completion is
likely. We evaluate LS3 on 4 domains, including a challenging sequential
pushing task in simulation and a physical cable routing task. We find that LS3
can use prior task successes to restrict exploration and learn more efficiently
than prior algorithms while satisfying constraints. See
this https URL for code and supplementary material.

    

### [[2107.04795] Semi-Supervised Learning with Multi-Head Co-Training](http://arxiv.org/abs/2107.04795)


  Co-training, extended from self-training, is one of the frameworks for
semi-supervised learning. It works at the cost of training extra classifiers,
where the algorithm should be delicately designed to prevent individual
classifiers from collapsing into each other. In this paper, we present a simple
and efficient co-training algorithm, named Multi-Head Co-Training, for
semi-supervised image classification. By integrating base learners into a
multi-head structure, the model is in a minimal amount of extra parameters.
Every classification head in the unified model interacts with its peers through
a "Weak and Strong Augmentation" strategy, achieving single-view co-training
without promoting diversity explicitly. The effectiveness of Multi-Head
Co-Training is demonstrated in an empirical study on standard semi-supervised
learning benchmarks.

    

### [[2107.04813] Detection of Plant Leaf Disease Directly in the JPEG Compressed Domain using Transfer Learning Technique](http://arxiv.org/abs/2107.04813)


  Plant leaf diseases pose a significant danger to food security and they cause
depletion in quality and volume of production. Therefore accurate and timely
detection of leaf disease is very important to check the loss of the crops and
meet the growing food demand of the people. Conventional techniques depend on
lab investigation and human skills which are generally costly and inaccessible.
Recently, Deep Neural Networks have been exceptionally fruitful in image
classification. In this research paper, plant leaf disease detection employing
transfer learning is explored in the JPEG compressed domain. Here, the JPEG
compressed stream consisting of DCT coefficients is, directly fed into the
Neural Network to improve the efficiency of classification. The experimental
results on JPEG compressed leaf dataset demonstrate the efficacy of the
proposed model.

    

### [[2107.04827] Identifying Layers Susceptible to Adversarial Attacks](http://arxiv.org/abs/2107.04827)


  Common neural network architectures are susceptible to attack by adversarial
samples. Neural network architectures are commonly thought of as divided into
low-level feature extraction layers and high-level classification layers;
susceptibility of networks to adversarial samples is often thought of as a
problem related to classification rather than feature extraction. We test this
idea by selectively retraining different portions of VGG and ResNet
architectures on CIFAR-10, Imagenette and ImageNet using non-adversarial and
adversarial data. Our experimental results show that susceptibility to
adversarial samples is associated with low-level feature extraction layers.
Therefore, retraining high-level layers is insufficient for achieving
robustness. This phenomenon could have two explanations: either, adversarial
attacks yield outputs from early layers that are indistinguishable from
features found in the attack classes, or adversarial attacks yield outputs from
early layers that differ statistically from features for non-adversarial
samples and do not permit consistent classification by subsequent layers. We
test this question by large-scale non-linear dimensionality reduction and
density modeling on distributions of feature vectors in hidden layers and find
that the feature distributions between non-adversarial and adversarial samples
differ substantially. Our results provide new insights into the statistical
origins of adversarial samples and possible defenses.

    

### [[2107.04831] Cluster Regularization via a Hierarchical Feature Regression](http://arxiv.org/abs/2107.04831)


  Prediction tasks with high-dimensional nonorthogonal predictor sets pose a
challenge for least squares based fitting procedures. A large and productive
literature exists, discussing various regularized approaches to improving the
out-of-sample robustness of parameter estimates. This paper proposes a novel
cluster-based regularization - the hierarchical feature regression (HFR) -,
which mobilizes insights from the domains of machine learning and graph theory
to estimate parameters along a supervised hierarchical representation of the
predictor set, shrinking parameters towards group targets. The method is
innovative in its ability to estimate optimal compositions of predictor groups,
as well as the group targets endogenously. The HFR can be viewed as a
supervised factor regression, with the strength of shrinkage governed by a
penalty on the extent of idiosyncratic variation captured in the fitting
process. The method demonstrates good predictive accuracy and versatility,
outperforming a panel of benchmark regularized estimators across a diverse set
of simulated regression tasks, including dense, sparse and grouped data
generating processes. An application to the prediction of economic growth is
used to illustrate the HFR's effectiveness in an empirical setting, with
favorable comparisons to several frequentist and Bayesian alternatives.

    

### [[2107.04846] Propagation-aware Social Recommendation by Transfer Learning](http://arxiv.org/abs/2107.04846)


  Social-aware recommendation approaches have been recognized as an effective
way to solve the data sparsity issue of traditional recommender systems. The
assumption behind is that the knowledge in social user-user connections can be
shared and transferred to the domain of user-item interactions, whereby to help
learn user preferences. However, most existing approaches merely adopt the
first-order connections among users during transfer learning, ignoring those
connections in higher orders. We argue that better recommendation performance
can also benefit from high-order social relations. In this paper, we propose a
novel Propagation-aware Transfer Learning Network (PTLN) based on the
propagation of social relations. We aim to better mine the sharing knowledge
hidden in social networks and thus further improve recommendation performance.
Specifically, we explore social influence in two aspects: (a) higher-order
friends have been taken into consideration by order bias; (b) different friends
in the same order will have distinct importance for recommendation by an
attention mechanism. Besides, we design a novel regularization to bridge the
gap between social relations and user-item interactions. We conduct extensive
experiments on two real-world datasets and beat other counterparts in terms of
ranking accuracy, especially for the cold-start users with few historical
interactions.

    

### [[2107.04855] Kernel Mean Estimation by Marginalized Corrupted Distributions](http://arxiv.org/abs/2107.04855)


  Estimating the kernel mean in a reproducing kernel Hilbert space is a
critical component in many kernel learning algorithms. Given a finite sample,
the standard estimate of the target kernel mean is the empirical average.
Previous works have shown that better estimators can be constructed by
shrinkage methods. In this work, we propose to corrupt data examples with noise
from known distributions and present a new kernel mean estimator, called the
marginalized kernel mean estimator, which estimates kernel mean under the
corrupted distribution. Theoretically, we show that the marginalized kernel
mean estimator introduces implicit regularization in kernel mean estimation.
Empirically, we show on a variety of datasets that the marginalized kernel mean
estimator obtains much lower estimation error than the existing estimators.

    

### [[2107.04857] Dense-Sparse Deep CNN Training for Image Denoising](http://arxiv.org/abs/2107.04857)


  Recently, deep learning (DL) methods such as convolutional neural networks
(CNNs) have gained prominence in the area of image denoising. This is owing to
their proven ability to surpass state-of-the-art classical image denoising
algorithms such as BM3D. Deep denoising CNNs (DnCNNs) use many feedforward
convolution layers with added regularization methods of batch normalization and
residual learning to improve denoising performance significantly. However, this
comes at the expense of a huge number of trainable parameters. In this paper,
we address this issue by reducing the number of parameters while achieving a
comparable level of performance. We derive motivation from the improved
performance obtained by training networks using the dense-sparse-dense (DSD)
training approach. We extend this training approach to a reduced DnCNN (RDnCNN)
network resulting in a faster denoising network with significantly reduced
parameters and comparable performance to the DnCNN.

    

### [[2107.04863] HOMRS: High Order Metamorphic Relations Selector for Deep Neural Networks](http://arxiv.org/abs/2107.04863)


  Deep Neural Networks (DNN) applications are increasingly becoming a part of
our everyday life, from medical applications to autonomous cars. Traditional
validation of DNN relies on accuracy measures, however, the existence of
adversarial examples has highlighted the limitations of these accuracy
measures, raising concerns especially when DNN are integrated into
safety-critical systems. In this paper, we present HOMRS, an approach to boost
metamorphic testing by automatically building a small optimized set of high
order metamorphic relations from an initial set of elementary metamorphic
relations. HOMRS' backbone is a multi-objective search; it exploits ideas drawn
from traditional systems testing such as code coverage, test case, and path
diversity. We applied HOMRS to LeNet5 DNN with MNIST dataset and we report
evidence that it builds a small but effective set of high order transformations
achieving a 95% kill ratio. Five raters manually labeled a pool of images
before and after high order transformation; Fleiss' Kappa and statistical tests
confirmed that they are metamorphic properties. HOMRS built-in relations are
also effective to confront adversarial or out-of-distribution examples; HOMRS
detected 92% of randomly sampled out-of-distribution images. HOMRS
transformations are also suitable for online real-time use.

    

### [[2107.04882] Out of Distribution Detection and Adversarial Attacks on Deep Neural Networks for Robust Medical Image Analysis](http://arxiv.org/abs/2107.04882)


  Deep learning models have become a popular choice for medical image analysis.
However, the poor generalization performance of deep learning models limits
them from being deployed in the real world as robustness is critical for
medical applications. For instance, the state-of-the-art Convolutional Neural
Networks (CNNs) fail to detect adversarial samples or samples drawn
statistically far away from the training distribution. In this work, we
experimentally evaluate the robustness of a Mahalanobis distance-based
confidence score, a simple yet effective method for detecting abnormal input
samples, in classifying malaria parasitized cells and uninfected cells. Results
indicated that the Mahalanobis confidence score detector exhibits improved
performance and robustness of deep learning models, and achieves
stateof-the-art performance on both out-of-distribution (OOD) and adversarial
samples.

    

### [[2107.04894] Improving Inductive Link Prediction Using Hyper-Relational Facts](http://arxiv.org/abs/2107.04894)


  For many years, link prediction on knowledge graphs (KGs) has been a purely
transductive task, not allowing for reasoning on unseen entities. Recently,
increasing efforts are put into exploring semi- and fully inductive scenarios,
enabling inference over unseen and emerging entities. Still, all these
approaches only consider triple-based \glspl{kg}, whereas their richer
counterparts, hyper-relational KGs (e.g., Wikidata), have not yet been properly
studied. In this work, we classify different inductive settings and study the
benefits of employing hyper-relational KGs on a wide range of semi- and fully
inductive link prediction tasks powered by recent advancements in graph neural
networks. Our experiments on a novel set of benchmarks show that qualifiers
over typed edges can lead to performance improvements of 6% of absolute gains
(for the Hits@10 metric) compared to triple-only baselines. Our code is
available at \url{this https URL}.

    

### [[2107.04895] Towards a Multimodal System for Precision Agriculture using IoT and Machine Learning](http://arxiv.org/abs/2107.04895)


  Precision agriculture system is an arising idea that refers to overseeing
farms utilizing current information and communication technologies to improve
the quantity and quality of yields while advancing the human work required. The
automation requires the assortment of information given by the sensors such as
soil, water, light, humidity, temperature for additional information to furnish
the operator with exact data to acquire excellent yield to farmers. In this
work, a study is proposed that incorporates all common state-of-the-art
approaches for precision agriculture use. Technologies like the Internet of
Things (IoT) for data collection, machine Learning for crop damage prediction,
and deep learning for crop disease detection is used. The data collection using
IoT is responsible for the measure of moisture levels for smart irrigation, n,
p, k estimations of fertilizers for best yield development. For crop damage
prediction, various algorithms like Random Forest (RF), Light gradient boosting
machine (LGBM), XGBoost (XGB), Decision Tree (DT) and K Nearest Neighbor (KNN)
are used. Subsequently, Pre-Trained Convolutional Neural Network (CNN) models
such as VGG16, Resnet50, and DenseNet121 are also trained to check if the crop
was tainted with some illness or not.

    

### [[2107.04911] Prediction of concept lengths for fast concept learning in description logics](http://arxiv.org/abs/2107.04911)


  Concept learning approaches based on refinement operators explore partially
ordered solution spaces to compute concepts, which are used as binary
classification models for individuals. However, the refinement trees spanned by
these approaches can easily grow to millions of nodes for complex learning
problems. This leads to refinement-based approaches often failing to detect
optimal concepts efficiently. In this paper, we propose a supervised machine
learning approach for learning concept lengths, which allows predicting the
length of the target concept and therefore facilitates the reduction of the
search space during concept learning. To achieve this goal, we compare four
neural architectures and evaluate them on four benchmark knowledge
graphs--Carcinogenesis, Mutagenesis, Semantic Bible, Family Benchmark. Our
evaluation results suggest that recurrent neural network architectures perform
best at concept length prediction with an F-measure of up to 92%. We show that
integrating our concept length predictor into the CELOE (Class Expression
Learner for Ontology Engineering) algorithm improves CELOE's runtime by a
factor of up to 13.4 without any significant changes to the quality of the
results it generates. For reproducibility, we provide our implementation in the
public GitHub repository at
this https URL


### [[2107.04914] Anatomy of Domain Shift Impact on U-Net Layers in MRI Segmentation](http://arxiv.org/abs/2107.04914)


  Domain Adaptation (DA) methods are widely used in medical image segmentation
tasks to tackle the problem of differently distributed train (source) and test
(target) data. We consider the supervised DA task with a limited number of
annotated samples from the target domain. It corresponds to one of the most
relevant clinical setups: building a sufficiently accurate model on the minimum
possible amount of annotated data. Existing methods mostly fine-tune specific
layers of the pretrained Convolutional Neural Network (CNN). However, there is
no consensus on which layers are better to fine-tune, e.g. the first layers for
images with low-level domain shift or the deeper layers for images with
high-level domain shift. To this end, we propose SpotTUnet - a CNN architecture
that automatically chooses the layers which should be optimally fine-tuned.
More specifically, on the target domain, our method additionally learns the
policy that indicates whether a specific layer should be fine-tuned or reused
from the pretrained network. We show that our method performs at the same level
as the best of the nonflexible fine-tuning methods even under the extreme
scarcity of annotated data. Secondly, we show that SpotTUnet policy provides a
layer-wise visualization of the domain shift impact on the network, which could
be further used to develop robust domain generalization methods. In order to
extensively evaluate SpotTUnet performance, we use a publicly available dataset
of brain MR images (CC359), characterized by explicit domain shift. We release
a reproducible experimental pipeline.

    

### [[2107.04952] Learn from Anywhere: Rethinking Generalized Zero-Shot Learning with Limited Supervision](http://arxiv.org/abs/2107.04952)


  A common problem with most zero and few-shot learning approaches is they
suffer from bias towards seen classes resulting in sub-optimal performance.
Existing efforts aim to utilize unlabeled images from unseen classes (i.e
transductive zero-shot) during training to enable generalization. However, this
limits their use in practical scenarios where data from target unseen classes
is unavailable or infeasible to collect. In this work, we present a practical
setting of inductive zero and few-shot learning, where unlabeled images from
other out-of-data classes, that do not belong to seen or unseen categories, can
be used to improve generalization in any-shot learning. We leverage a
formulation based on product-of-experts and introduce a new AUD module that
enables us to use unlabeled samples from out-of-data classes which are usually
easily available and practically entail no annotation cost. In addition, we
also demonstrate the applicability of our model to address a more practical and
challenging, Generalized Zero-shot under a limited supervision setting, where
even base seen classes do not have sufficient annotated samples.

    

### [[2107.04954] ReconVAT: A Semi-Supervised Automatic Music Transcription Framework for Low-Resource Real-World Data](http://arxiv.org/abs/2107.04954)


  Most of the current supervised automatic music transcription (AMT) models
lack the ability to generalize. This means that they have trouble transcribing
real-world music recordings from diverse musical genres that are not presented
in the labelled training data. In this paper, we propose a semi-supervised
framework, ReconVAT, which solves this issue by leveraging the huge amount of
available unlabelled music recordings. The proposed ReconVAT uses
reconstruction loss and virtual adversarial training. When combined with
existing U-net models for AMT, ReconVAT achieves competitive results on common
benchmark datasets such as MAPS and MusicNet. For example, in the few-shot
setting for the string part version of MusicNet, ReconVAT achieves F1-scores of
61.0% and 41.6% for the note-wise and note-with-offset-wise metrics
respectively, which translates into an improvement of 22.2% and 62.5% compared
to the supervised baseline model. Our proposed framework also demonstrates the
potential of continual learning on new data, which could be useful in
real-world applications whereby new data is constantly available.

    

### [[2107.04971] Self-service Data Classification Using Interactive Visualization and Interpretable Machine Learning](http://arxiv.org/abs/2107.04971)


  Machine learning algorithms often produce models considered as complex
black-box models by both end users and developers. They fail to explain the
model in terms of the domain they are designed for. The proposed Iterative
Visual Logical Classifier (IVLC) is an interpretable machine learning algorithm
that allows end users to design a model and classify data with more confidence
and without having to compromise on the accuracy. Such technique is especially
helpful when dealing with sensitive and crucial data like cancer data in the
medical domain with high cost of errors. With the help of the proposed
interactive and lossless multidimensional visualization, end users can identify
the pattern in the data based on which they can make explainable decisions.
Such options would not be possible in black box machine learning methodologies.
The interpretable IVLC algorithm is supported by the Interactive Shifted Paired
Coordinates Software System (SPCVis). It is a lossless multidimensional data
visualization system with user interactive features. The interactive approach
provides flexibility to the end user to perform data classification as
self-service without having to rely on a machine learning expert. Interactive
pattern discovery becomes challenging while dealing with large data sets with
hundreds of dimensions/features. To overcome this problem, this chapter
proposes an automated classification approach combined with new Coordinate
Order Optimizer (COO) algorithm and a Genetic algorithm. The COO algorithm
automatically generates the coordinate pair sequences that best represent the
data separation and the genetic algorithm helps optimizing the proposed IVLC
algorithm by automatically generating the areas for data classification. The
feasibility of the approach is shown by experiments on benchmark datasets
covering both interactive and automated processes used for data classification.

    

### [[2107.04973] A Deep-Bayesian Framework for Adaptive Speech Duration Modification](http://arxiv.org/abs/2107.04973)


  We propose the first method to adaptively modify the duration of a given
speech signal. Our approach uses a Bayesian framework to define a latent
attention map that links frames of the input and target utterances. We train a
masked convolutional encoder-decoder network to produce this attention map via
a stochastic version of the mean absolute error loss function; our model also
predicts the length of the target speech signal using the encoder embeddings.
The predicted length determines the number of steps for the decoder operation.
During inference, we generate the attention map as a proxy for the similarity
matrix between the given input speech and an unknown target speech signal.
Using this similarity matrix, we compute a warping path of alignment between
the two signals. Our experiments demonstrate that this adaptive framework
produces similar results to dynamic time warping, which relies on a known
target signal, on both voice conversion and emotion conversion tasks. We also
show that our technique results in a high quality of generated speech that is
on par with state-of-the-art vocoders.

    

### [[2107.04974] Non-linear Visual Knowledge Discovery with Elliptic Paired Coordinates](http://arxiv.org/abs/2107.04974)


  It is challenging for humans to enable visual knowledge discovery in data
with more than 2-3 dimensions with a naked eye. This chapter explores the
efficiency of discovering predictive machine learning models interactively
using new Elliptic Paired coordinates (EPC) visualizations. It is shown that
EPC are capable to visualize multidimensional data and support visual machine
learning with preservation of multidimensional information in 2-D. Relative to
parallel and radial coordinates, EPC visualization requires only a half of the
visual elements for each n-D point. An interactive software system EllipseVis,
which is developed in this work, processes high-dimensional datasets, creates
EPC visualizations, and produces predictive classification models by
discovering dominance rules in EPC. By using interactive and automatic
processes it discovers zones in EPC with a high dominance of a single class.
The EPC methodology has been successful in discovering non-linear predictive
models with high coverage and precision in the computational experiments. This
can benefit multiple domains by producing visually appealing dominance rules.
This chapter presents results of successful testing the EPC non-linear
methodology in experiments using real and simulated data, EPC generalized to
the Dynamic Elliptic Paired Coordinates (DEPC), incorporation of the weights of
coordinates to optimize the visual discovery, introduction of an alternative
EPC design and introduction of the concept of incompact machine learning
methodology based on EPC/DEPC.

    

### [[2107.04980] STR-GODEs: Spatial-Temporal-Ridership Graph ODEs for Metro Ridership Prediction](http://arxiv.org/abs/2107.04980)


  The metro ridership prediction has always received extensive attention from
governments and researchers. Recent works focus on designing complicated graph
convolutional recurrent network architectures to capture spatial and temporal
patterns. These works extract the information of spatial dimension well, but
the limitation of temporal dimension still exists. We extended Neural ODE
algorithms to the graph network and proposed the STR-GODEs network, which can
effectively learn spatial, temporal, and ridership correlations without the
limitation of dividing data into equal-sized intervals on the timeline. While
learning the spatial relations and the temporal correlations, we modify the
GODE-RNN cell to obtain the ridership feature and hidden states. Ridership
information and its hidden states are added to the GODESolve to reduce the
error accumulation caused by long time series in prediction. Extensive
experiments on two large-scale datasets demonstrate the efficacy and robustness
of our model.

    

### [[2107.04982] Out-of-Distribution Dynamics Detection: RL-Relevant Benchmarks and Results](http://arxiv.org/abs/2107.04982)


  We study the problem of out-of-distribution dynamics (OODD) detection, which
involves detecting when the dynamics of a temporal process change compared to
the training-distribution dynamics. This is relevant to applications in
control, reinforcement learning (RL), and multi-variate time-series, where
changes to test time dynamics can impact the performance of learning
controllers/predictors in unknown ways. This problem is particularly important
in the context of deep RL, where learned controllers often overfit to the
training environment. Currently, however, there is a lack of established OODD
benchmarks for the types of environments commonly used in RL research. Our
first contribution is to design a set of OODD benchmarks derived from common RL
environments with varying types and intensities of OODD. Our second
contribution is to design a strong OODD baseline approach based on recurrent
implicit quantile networks (RIQNs), which monitors autoregressive prediction
errors for OODD detection. Our final contribution is to evaluate the RIQN
approach on the benchmarks to provide baseline results for future comparison.

    

### [[2107.04987] Coordinate-wise Control Variates for Deep Policy Gradients](http://arxiv.org/abs/2107.04987)


  The control variates (CV) method is widely used in policy gradient estimation
to reduce the variance of the gradient estimators in practice. A control
variate is applied by subtracting a baseline function from the state-action
value estimates. Then the variance-reduced policy gradient presumably leads to
higher learning efficiency. Recent research on control variates with deep
neural net policies mainly focuses on scalar-valued baseline functions. The
effect of vector-valued baselines is under-explored. This paper investigates
variance reduction with coordinate-wise and layer-wise control variates
constructed from vector-valued baselines for neural net policies. We present
experimental evidence suggesting that lower variance can be obtained with such
baselines than with the conventional scalar-valued baseline. We demonstrate how
to equip the popular Proximal Policy Optimization (PPO) algorithm with these
new control variates. We show that the resulting algorithm with proper
regularization can achieve higher sample efficiency than scalar control
variates in continuous control benchmarks.

    

### [[2107.05001] Improving Efficiency and Accuracy of Causal Discovery Using a Hierarchical Wrapper](http://arxiv.org/abs/2107.05001)


  Causal discovery from observational data is an important tool in many
branches of science. Under certain assumptions it allows scientists to explain
phenomena, predict, and make decisions. In the large sample limit, sound and
complete causal discovery algorithms have been previously introduced, where a
directed acyclic graph (DAG), or its equivalence class, representing causal
relations is searched. However, in real-world cases, only finite training data
is available, which limits the power of statistical tests used by these
algorithms, leading to errors in the inferred causal model. This is commonly
addressed by devising a strategy for using as few as possible statistical
tests. In this paper, we introduce such a strategy in the form of a recursive
wrapper for existing constraint-based causal discovery algorithms, which
preserves soundness and completeness. It recursively clusters the observed
variables using the normalized min-cut criterion from the outset, and uses a
baseline causal discovery algorithm during backtracking for learning local
sub-graphs. It then combines them and ensures completeness. By an ablation
study, using synthetic data, and by common real-world benchmarks, we
demonstrate that our approach requires significantly fewer statistical tests,
learns more accurate graphs, and requires shorter run-times than the baseline
algorithm.

    

### [[2107.05007] Generating stable molecules using imitation and reinforcement learning](http://arxiv.org/abs/2107.05007)


  Chemical space is routinely explored by machine learning methods to discover
interesting molecules, before time-consuming experimental synthesizing is
attempted. However, these methods often rely on a graph representation,
ignoring 3D information necessary for determining the stability of the
molecules. We propose a reinforcement learning approach for generating
molecules in cartesian coordinates allowing for quantum chemical prediction of
the stability. To improve sample-efficiency we learn basic chemical rules from
imitation learning on the GDB-11 database to create an initial model applicable
for all stoichiometries. We then deploy multiple copies of the model
conditioned on a specific stoichiometry in a reinforcement learning setting.
The models correctly identify low energy molecules in the database and produce
novel isomers not found in the training set. Finally, we apply the model to
larger molecules to show how reinforcement learning further refines the
imitation learning model in domains far from the training data.

    

### [[2107.05011] Dual Optimization for Kolmogorov Model Learning Using Enhanced Gradient Descent](http://arxiv.org/abs/2107.05011)


  Data representation techniques have made a substantial contribution to
advancing data processing and machine learning (ML). Improving predictive power
was the focus of previous representation techniques, which unfortunately
perform rather poorly on the interpretability in terms of extracting underlying
insights of the data. Recently, Kolmogorov model (KM) was studied, which is an
interpretable and predictable representation approach to learning the
underlying probabilistic structure of a set of random variables. The existing
KM learning algorithms using semi-definite relaxation with randomization
(SDRwR) or discrete monotonic optimization (DMO) have, however, limited utility
to big data applications because they do not scale well computationally. In
this paper, we propose a computationally scalable KM learning algorithm, based
on the regularized dual optimization combined with enhanced gradient descent
(GD) method. To make our method more scalable to large-dimensional problems, we
propose two acceleration schemes, namely, eigenvalue decomposition (EVD)
elimination strategy and proximal EVD algorithm. Furthermore, a thresholding
technique by exploiting the approximation error analysis and leveraging the
normalized Minkowski $\ell_1$-norm and its bounds, is provided for the
selection of the number of iterations of the proximal EVD algorithm. When
applied to big data applications, it is demonstrated that the proposed method
can achieve compatible training/prediction performance with significantly
reduced computational complexity; roughly two orders of magnitude improvement
in terms of the time overhead, compared to the existing KM learning algorithms.
Furthermore, it is shown that the accuracy of logical relation mining for
interpretability by using the proposed KM learning algorithm exceeds $80\%$.

    

### [[2107.05033] Blending Pruning Criteria for Convolutional Neural Networks](http://arxiv.org/abs/2107.05033)


  The advancement of convolutional neural networks (CNNs) on various vision
applications has attracted lots of attention. Yet the majority of CNNs are
unable to satisfy the strict requirement for real-world deployment. To overcome
this, the recent popular network pruning is an effective method to reduce the
redundancy of the models. However, the ranking of filters according to their
"importance" on different pruning criteria may be inconsistent. One filter
could be important according to a certain criterion, while it is unnecessary
according to another one, which indicates that each criterion is only a partial
view of the comprehensive "importance". From this motivation, we propose a
novel framework to integrate the existing filter pruning criteria by exploring
the criteria diversity. The proposed framework contains two stages: Criteria
Clustering and Filters Importance Calibration. First, we condense the pruning
criteria via layerwise clustering based on the rank of "importance" score.
Second, within each cluster, we propose a calibration factor to adjust their
significance for each selected blending candidates and search for the optimal
blending criterion via Evolutionary Algorithm. Quantitative results on the
CIFAR-100 and ImageNet benchmarks show that our framework outperforms the
state-of-the-art baselines, regrading to the compact model performance after
pruning.

    

### [[2107.05039] Learning from Crowds with Sparse and Imbalanced Annotations](http://arxiv.org/abs/2107.05039)


  Traditional supervised learning requires ground truth labels for the training
data, whose collection can be difficult in many cases. Recently, crowdsourcing
has established itself as an efficient labeling solution through resorting to
non-expert crowds. To reduce the labeling error effects, one common practice is
to distribute each instance to multiple workers, whereas each worker only
annotates a subset of data, resulting in the {\it sparse annotation}
phenomenon. In this paper, we note that when meeting with class-imbalance,
i.e., when the ground truth labels are {\it class-imbalanced}, the sparse
annotations are prone to be skewly distributed, which thus can severely bias
the learning algorithm. To combat this issue, we propose one self-training
based approach named {\it Self-Crowd} by progressively adding confident
pseudo-annotations and rebalancing the annotation distribution. Specifically,
we propose one distribution aware confidence measure to select confident
pseudo-annotations, which adopts the resampling strategy to oversample the
minority annotations and undersample the majority annotations. On one
real-world crowdsourcing image classification task, we show that the proposed
method yields more balanced annotations throughout training than the
distribution agnostic methods and substantially improves the learning
performance at different annotation sparsity levels.

    

### [[2107.05045] Positive-Unlabeled Classification under Class-Prior Shift: A Prior-invariant Approach Based on Density Ratio Estimation](http://arxiv.org/abs/2107.05045)


  Learning from positive and unlabeled (PU) data is an important problem in
various applications. Most of the recent approaches for PU classification
assume that the class-prior (the ratio of positive samples) in the training
unlabeled dataset is identical to that of the test data, which does not hold in
many practical cases. In addition, we usually do not know the class-priors of
the training and test data, thus we have no clue on how to train a classifier
without them. To address these problems, we propose a novel PU classification
method based on density ratio estimation. A notable advantage of our proposed
method is that it does not require the class-priors in the training phase;
class-prior shift is incorporated only in the test phase. We theoretically
justify our proposed method and experimentally demonstrate its effectiveness.

    

### [[2107.05047] One Map Does Not Fit All: Evaluating Saliency Map Explanation on Multi-Modal Medical Images](http://arxiv.org/abs/2107.05047)


  Being able to explain the prediction to clinical end-users is a necessity to
leverage the power of AI models for clinical decision support. For medical
images, saliency maps are the most common form of explanation. The maps
highlight important features for AI model's prediction. Although many saliency
map methods have been proposed, it is unknown how well they perform on
explaining decisions on multi-modal medical images, where each modality/channel
carries distinct clinical meanings of the same underlying biomedical
phenomenon. Understanding such modality-dependent features is essential for
clinical users' interpretation of AI decisions. To tackle this clinically
important but technically ignored problem, we propose the MSFI
(Modality-Specific Feature Importance) metric to examine whether saliency maps
can highlight modality-specific important features. MSFI encodes the clinical
requirements on modality prioritization and modality-specific feature
localization. Our evaluations on 16 commonly used saliency map methods,
including a clinician user study, show that although most saliency map methods
captured modality importance information in general, most of them failed to
highlight modality-specific important features consistently and precisely. The
evaluation results guide the choices of saliency map methods and provide
insights to propose new ones targeting clinical applications.

    

### [[2107.05050] Neural Waveshaping Synthesis](http://arxiv.org/abs/2107.05050)


  We present the Neural Waveshaping Unit (NEWT): a novel, lightweight, fully
causal approach to neural audio synthesis which operates directly in the
waveform domain, with an accompanying optimisation (FastNEWT) for efficient CPU
inference. The NEWT uses time-distributed multilayer perceptrons with periodic
activations to implicitly learn nonlinear transfer functions that encode the
characteristics of a target timbre. Once trained, a NEWT can produce complex
timbral evolutions by simple affine transformations of its input and output
signals. We paired the NEWT with a differentiable noise synthesiser and reverb
and found it capable of generating realistic musical instrument performances
with only 260k total model parameters, conditioned on F0 and loudness features.
We compared our method to state-of-the-art benchmarks with a multi-stimulus
listening test and the Fréchet Audio Distance and found it performed
competitively across the tested timbral domains. Our method significantly
outperformed the benchmarks in terms of generation speed, and achieved
real-time performance on a consumer CPU, both with and without FastNEWT,
suggesting it is a viable basis for future creative sound design tools.

    

### [[2107.05071] Machine Learning based CVD Virtual Metrology in Mass Produced Semiconductor Process](http://arxiv.org/abs/2107.05071)


  A cross-benchmark has been done on three critical aspects, data imputing,
feature selection and regression algorithms, for machine learning based
chemical vapor deposition (CVD) virtual metrology (VM). The result reveals that
linear feature selection regression algorithm would extensively under-fit the
VM data. Data imputing is also necessary to achieve a higher prediction
accuracy as the data availability is only ~70% when optimal accuracy is
obtained. This work suggests a nonlinear feature selection and regression
algorithm combined with nearest data imputing algorithm would provide a
prediction accuracy as high as 0.7. This would lead to 70% reduced CVD
processing variation, which is believed to will lead to reduced frequency of
physical metrology as well as more reliable mass-produced wafer with improved
quality.

    

### [[2107.05074] SGD: The Role of Implicit Regularization, Batch-size and Multiple-epochs](http://arxiv.org/abs/2107.05074)


  Multi-epoch, small-batch, Stochastic Gradient Descent (SGD) has been the
method of choice for learning with large over-parameterized models. A popular
theory for explaining why SGD works well in practice is that the algorithm has
an implicit regularization that biases its output towards a good solution.
Perhaps the theoretically most well understood learning setting for SGD is that
of Stochastic Convex Optimization (SCO), where it is well known that SGD learns
at a rate of $O(1/\sqrt{n})$, where $n$ is the number of samples. In this
paper, we consider the problem of SCO and explore the role of implicit
regularization, batch size and multiple epochs for SGD. Our main contributions
are threefold:
(a) We show that for any regularizer, there is an SCO problem for which
Regularized Empirical Risk Minimzation fails to learn. This automatically rules
out any implicit regularization based explanation for the success of SGD.
(b) We provide a separation between SGD and learning via Gradient Descent on
empirical loss (GD) in terms of sample complexity. We show that there is an SCO
problem such that GD with any step size and number of iterations can only learn
at a suboptimal rate: at least $\widetilde{\Omega}(1/n^{5/12})$.
(c) We present a multi-epoch variant of SGD commonly used in practice. We
prove that this algorithm is at least as good as single pass SGD in the worst
case. However, for certain SCO problems, taking multiple passes over the
dataset can significantly outperform single pass SGD.
We extend our results to the general learning setting by showing a problem
which is learnable for any data distribution, and for this problem, SGD is
strictly better than RERM for any regularization function. We conclude by
discussing the implications of our results for deep learning, and show a
separation between SGD and ERM for two layer diagonal neural networks.

    

### [[2107.05080] Zero-Shot Scene Graph Relation Prediction through Commonsense Knowledge Integration](http://arxiv.org/abs/2107.05080)


  Relation prediction among entities in images is an important step in scene
graph generation (SGG), which further impacts various visual understanding and
reasoning tasks. Existing SGG frameworks, however, require heavy training yet
are incapable of modeling unseen (i.e.,zero-shot) triplets. In this work, we
stress that such incapability is due to the lack of commonsense reasoning,i.e.,
the ability to associate similar entities and infer similar relations based on
general understanding of the world. To fill this gap, we propose
CommOnsense-integrAted sCenegrapHrElation pRediction (COACHER), a framework to
integrate commonsense knowledge for SGG, especially for zero-shot relation
prediction. Specifically, we develop novel graph mining pipelines to model the
neighborhoods and paths around entities in an external commonsense knowledge
graph, and integrate them on top of state-of-the-art SGG frameworks. Extensive
quantitative evaluations and qualitative case studies on both original and
manipulated datasets from Visual Genome demonstrate the effectiveness of our
proposed approach.

    

### [[2107.05085] Effect of Input Size on the Classification of Lung Nodules Using Convolutional Neural Networks](http://arxiv.org/abs/2107.05085)


  Recent studies have shown that lung cancer screening using annual low-dose
computed tomography (CT) reduces lung cancer mortality by 20% compared to
traditional chest radiography. Therefore, CT lung screening has started to be
used widely all across the world. However, analyzing these images is a serious
burden for radiologists. The number of slices in a CT scan can be up to 600.
Therefore, computer-aided-detection (CAD) systems are very important for faster
and more accurate assessment of the data. In this study, we proposed a
framework that analyzes CT lung screenings using convolutional neural networks
(CNNs) to reduce false positives. We trained our model with different volume
sizes and showed that volume size plays a critical role in the performance of
the system. We also used different fusions in order to show their power and
effect on the overall accuracy. 3D CNNs were preferred over 2D CNNs because 2D
convolutional operations applied to 3D data could result in information loss.
The proposed framework has been tested on the dataset provided by the LUNA16
Challenge and resulted in a sensitivity of 0.831 at 1 false positive per scan.

    

### [[2107.05087] Remote Blood Oxygen Estimation From Videos Using Neural Networks](http://arxiv.org/abs/2107.05087)


  Blood oxygen saturation (SpO$_2$) is an essential indicator of respiratory
functionality and is receiving increasing attention during the COVID-19
pandemic. Clinical findings show that it is possible for COVID-19 patients to
have significantly low SpO$_2$ before any obvious symptoms. The prevalence of
cameras has motivated researchers to investigate methods for monitoring SpO$_2$
using videos. Most prior schemes involving smartphones are contact-based: They
require a fingertip to cover the phone's camera and the nearby light source to
capture re-emitted light from the illuminated tissue. In this paper, we propose
the first convolutional neural network based noncontact SpO$_2$ estimation
scheme using smartphone cameras. The scheme analyzes the videos of a
participant's hand for physiological sensing, which is convenient and
comfortable, and can protect their privacy and allow for keeping face masks on.
We design our neural network architectures inspired by the optophysiological
models for SpO$_2$ measurement and demonstrate the explainability by
visualizing the weights for channel combination. Our proposed models outperform
the state-of-the-art model that is designed for contact-based SpO$_2$
measurement, showing the potential of our proposed method to contribute to
public health. We also analyze the impact of skin type and the side of a hand
on SpO$_2$ estimation performance.

    

### [[2107.05093] SE-PSNet: Silhouette-based Enhancement Feature for Panoptic Segmentation Network](http://arxiv.org/abs/2107.05093)


  Recently, there has been a panoptic segmentation task combining semantic and
instance segmentation, in which the goal is to classify each pixel with the
corresponding instance ID. In this work, we propose a solution to tackle the
panoptic segmentation task. The overall structure combines the bottom-up method
and the top-down method. Therefore, not only can there be better performance,
but also the execution speed can be maintained. The network mainly pays
attention to the quality of the mask. In the previous work, we can see that the
uneven contour of the object is more likely to appear, resulting in low-quality
prediction. Accordingly, we propose enhancement features and corresponding loss
functions for the silhouette of objects and backgrounds to improve the mask.
Meanwhile, we use the new proposed confidence score to solve the occlusion
problem and make the network tend to use higher quality masks as prediction
results. To verify our research, we used the COCO dataset and CityScapes
dataset to do experiments and obtained competitive results with fast inference
time.

    

### [[2107.05097] BrainNNExplainer: An Interpretable Graph Neural Network Framework for Brain Network based Disease Analysis](http://arxiv.org/abs/2107.05097)


  Interpretable brain network models for disease prediction are of great value
for the advancement of neuroscience. GNNs are promising to model complicated
network data, but they are prone to overfitting and suffer from poor
interpretability, which prevents their usage in decision-critical scenarios
like healthcare. To bridge this gap, we propose BrainNNExplainer, an
interpretable GNN framework for brain network analysis. It is mainly composed
of two jointly learned modules: a backbone prediction model that is
specifically designed for brain networks and an explanation generator that
highlights disease-specific prominent brain network connections. Extensive
experimental results with visualizations on two challenging disease prediction
datasets demonstrate the unique interpretability and outstanding performance of
BrainNNExplainer.

    

### [[2107.05101] Machine Learning Challenges and Opportunities in the African Agricultural Sector -- A General Perspective](http://arxiv.org/abs/2107.05101)


  The improvement of computers' capacities, advancements in algorithmic
techniques, and the significant increase of available data have enabled the
recent developments of Artificial Intelligence (AI) technology. One of its
branches, called Machine Learning (ML), has shown strong capacities in
mimicking characteristics attributed to human intelligence, such as vision,
speech, and problem-solving. However, as previous technological revolutions
suggest, their most significant impacts could be mostly expected on other
sectors that were not traditional users of that technology. The agricultural
sector is vital for African economies; improving yields, mitigating losses, and
effective management of natural resources are crucial in a climate change era.
Machine Learning is a technology with an added value in making predictions,
hence the potential to reduce uncertainties and risk across sectors, in this
case, the agricultural sector. The purpose of this paper is to contextualize
and discuss barriers to ML-based solutions for African agriculture. In the
second section, we provided an overview of ML technology from a historical and
technical perspective and its main driving force. In the third section, we
provided a brief review of the current use of ML in agriculture. Finally, in
section 4, we discuss ML growing interest in Africa and the potential barriers
to creating and using ML-based solutions in the agricultural sector.

    

### [[2107.05115] Details Preserving Deep Collaborative Filtering-Based Method for Image Denoising](http://arxiv.org/abs/2107.05115)


  In spite of the improvements achieved by the several denoising algorithms
over the years, many of them still fail at preserving the fine details of the
image after denoising. This is as a result of the smooth-out effect they have
on the images. Most neural network-based algorithms have achieved better
quantitative performance than the classical denoising algorithms. However, they
also suffer from qualitative (visual) performance as a result of the smooth-out
effect. In this paper, we propose an algorithm to address this shortcoming. We
propose a deep collaborative filtering-based (Deep-CoFiB) algorithm for image
denoising. This algorithm performs collaborative denoising of image patches in
the sparse domain using a set of optimized neural network models. This results
in a fast algorithm that is able to excellently obtain a trade-off between
noise removal and details preservation. Extensive experiments show that the
DeepCoFiB performed quantitatively (in terms of PSNR and SSIM) and
qualitatively (visually) better than many of the state-of-the-art denoising
algorithms.

    

### [[2107.05124] Transformers with multi-modal features and post-fusion context for e-commerce session-based recommendation](http://arxiv.org/abs/2107.05124)


  Session-based recommendation is an important task for e-commerce services,
where a large number of users browse anonymously or may have very distinct
interests for different sessions. In this paper we present one of the winning
solutions for the Recommendation task of the SIGIR 2021 Workshop on E-commerce
Data Challenge. Our solution was inspired by NLP techniques and consists of an
ensemble of two Transformer architectures - Transformer-XL and XLNet - trained
with autoregressive and autoencoding approaches. To leverage most of the rich
dataset made available for the competition, we describe how we prepared
multi-model features by combining tabular events with textual and image
vectors. We also present a model prediction analysis to better understand the
effectiveness of our architectures for the session-based recommendation.

    

### [[2107.05127] Attack Rules: An Adversarial Approach to Generate Attacks for Industrial Control Systems using Machine Learning](http://arxiv.org/abs/2107.05127)


  Adversarial learning is used to test the robustness of machine learning
algorithms under attack and create attacks that deceive the anomaly detection
methods in Industrial Control System (ICS). Given that security assessment of
an ICS demands that an exhaustive set of possible attack patterns is studied,
in this work, we propose an association rule mining-based attack generation
technique. The technique has been implemented using data from a secure Water
Treatment plant. The proposed technique was able to generate more than 300,000
attack patterns constituting a vast majority of new attack vectors which were
not seen before. Automatically generated attacks improve our understanding of
the potential attacks and enable the design of robust attack detection
techniques.

    

### [[2107.05132] LexSubCon: Integrating Knowledge from Lexical Resources into Contextual Embeddings for Lexical Substitution](http://arxiv.org/abs/2107.05132)


  Lexical substitution is the task of generating meaningful substitutes for a
word in a given textual context. Contextual word embedding models have achieved
state-of-the-art results in the lexical substitution task by relying on
contextual information extracted from the replaced word within the sentence.
However, such models do not take into account structured knowledge that exists
in external lexical databases.
We introduce LexSubCon, an end-to-end lexical substitution framework based on
contextual embedding models that can identify highly accurate substitute
candidates. This is achieved by combining contextual information with knowledge
from structured lexical resources. Our approach involves: (i) introducing a
novel mix-up embedding strategy in the creation of the input embedding of the
target word through linearly interpolating the pair of the target input
embedding and the average embedding of its probable synonyms; (ii) considering
the similarity of the sentence-definition embeddings of the target word and its
proposed candidates; and, (iii) calculating the effect of each substitution in
the semantics of the sentence through a fine-tuned sentence similarity model.
Our experiments show that LexSubCon outperforms previous state-of-the-art
methods on LS07 and CoInCo benchmark datasets that are widely used for lexical
substitution tasks.

    

### [[2107.05134] Dual Training of Energy-Based Models with Overparametrized Shallow Neural Networks](http://arxiv.org/abs/2107.05134)


  Energy-based models (EBMs) are generative models that are usually trained via
maximum likelihood estimation. This approach becomes challenging in generic
situations where the trained energy is nonconvex, due to the need to sample the
Gibbs distribution associated with this energy. Using general Fenchel duality
results, we derive variational principles dual to maximum likelihood EBMs with
shallow overparametrized neural network energies, both in the active (aka
feature-learning) and lazy regimes. In the active regime, this dual formulation
leads to a training algorithm in which one updates concurrently the particles
in the sample space and the neurons in the parameter space of the energy. We
also consider a variant of this algorithm in which the particles are sometimes
restarted at random samples drawn from the data set, and show that performing
these restarts at every iteration step corresponds to score matching training.
Using intermediate parameter setups in our dual algorithm thereby gives a way
to interpolate between maximum likelihood and score matching training. These
results are illustrated in simple numerical experiments.

    

### [[2107.05154] MOOCRep: A Unified Pre-trained Embedding of MOOC Entities](http://arxiv.org/abs/2107.05154)


  Many machine learning models have been built to tackle information overload
issues on Massive Open Online Courses (MOOC) platforms. These models rely on
learning powerful representations of MOOC entities. However, they suffer from
the problem of scarce expert label data. To overcome this problem, we propose
to learn pre-trained representations of MOOC entities using abundant unlabeled
data from the structure of MOOCs which can directly be applied to the
downstream tasks. While existing pre-training methods have been successful in
NLP areas as they learn powerful textual representation, their models do not
leverage the richer information about MOOC entities. This richer information
includes the graph relationship between the lectures, concepts, and courses
along with the domain knowledge about the complexity of a concept. We develop
MOOCRep, a novel method based on Transformer language model trained with two
pre-training objectives : 1) graph-based objective to capture the powerful
signal of entities and relations that exist in the graph, and 2)
domain-oriented objective to effectively incorporate the complexity level of
concepts. Our experiments reveal that MOOCRep's embeddings outperform
state-of-the-art representation learning methods on two tasks important for
education community, concept pre-requisite prediction and lecture
recommendation.

    

### [[2107.05166] Stateful Detection of Model Extraction Attacks](http://arxiv.org/abs/2107.05166)


  Machine-Learning-as-a-Service providers expose machine learning (ML) models
through application programming interfaces (APIs) to developers. Recent work
has shown that attackers can exploit these APIs to extract good approximations
of such ML models, by querying them with samples of their choosing. We propose
VarDetect, a stateful monitor that tracks the distribution of queries made by
users of such a service, to detect model extraction attacks. Harnessing the
latent distributions learned by a modified variational autoencoder, VarDetect
robustly separates three types of attacker samples from benign samples, and
successfully raises an alarm for each. Further, with VarDetect deployed as an
automated defense mechanism, the extracted substitute models are found to
exhibit poor performance and transferability, as intended. Finally, we
demonstrate that even adaptive attackers with prior knowledge of the deployment
of VarDetect, are detected by it.

    

### [[2107.05180] MugRep: A Multi-Task Hierarchical Graph Representation Learning Framework for Real Estate Appraisal](http://arxiv.org/abs/2107.05180)


  Real estate appraisal refers to the process of developing an unbiased opinion
for real property's market value, which plays a vital role in decision-making
for various players in the marketplace (e.g., real estate agents, appraisers,
lenders, and buyers). However, it is a nontrivial task for accurate real estate
appraisal because of three major challenges: (1) The complicated influencing
factors for property value; (2) The asynchronously spatiotemporal dependencies
among real estate transactions; (3) The diversified correlations between
residential communities. To this end, we propose a Multi-Task Hierarchical
Graph Representation Learning (MugRep) framework for accurate real estate
appraisal. Specifically, by acquiring and integrating multi-source urban data,
we first construct a rich feature set to comprehensively profile the real
estate from multiple perspectives (e.g., geographical distribution, human
mobility distribution, and resident demographics distribution). Then, an
evolving real estate transaction graph and a corresponding event graph
convolution module are proposed to incorporate asynchronously spatiotemporal
dependencies among real estate transactions. Moreover, to further incorporate
valuable knowledge from the view of residential communities, we devise a
hierarchical heterogeneous community graph convolution module to capture
diversified correlations between residential communities. Finally, an urban
district partitioned multi-task learning module is introduced to generate
differently distributed value opinions for real estate. Extensive experiments
on two real-world datasets demonstrate the effectiveness of MugRep and its
components and features.

    

### [[2107.05187] Polynomial Time Reinforcement Learning in Correlated FMDPs with Linear Value Functions](http://arxiv.org/abs/2107.05187)


  Many reinforcement learning (RL) environments in practice feature enormous
state spaces that may be described compactly by a "factored" structure, that
may be modeled by Factored Markov Decision Processes (FMDPs). We present the
first polynomial-time algorithm for RL with FMDPs that does not rely on an
oracle planner, and instead of requiring a linear transition model, only
requires a linear value function with a suitable local basis with respect to
the factorization. With this assumption, we can solve FMDPs in polynomial time
by constructing an efficient separation oracle for convex optimization.
Importantly, and in contrast to prior work, we do not assume that the
transitions on various factors are independent.

    

### [[2107.05188] TransClaw U-Net: Claw U-Net with Transformers for Medical Image Segmentation](http://arxiv.org/abs/2107.05188)


  In recent years, computer-aided diagnosis has become an increasingly popular
topic. Methods based on convolutional neural networks have achieved good
performance in medical image segmentation and classification. Due to the
limitations of the convolution operation, the long-term spatial features are
often not accurately obtained. Hence, we propose a TransClaw U-Net network
structure, which combines the convolution operation with the transformer
operation in the encoding part. The convolution part is applied for extracting
the shallow spatial features to facilitate the recovery of the image resolution
after upsampling. The transformer part is used to encode the patches, and the
self-attention mechanism is used to obtain global information between
sequences. The decoding part retains the bottom upsampling structure for better
detail segmentation performance. The experimental results on Synapse
Multi-organ Segmentation Datasets show that the performance of TransClaw U-Net
is better than other network structures. The ablation experiments also prove
the generalization performance of TransClaw U-Net.

    

### [[2107.05201] Deep Risk Model: A Deep Learning Solution for Mining Latent Risk Factors to Improve Covariance Matrix Estimation](http://arxiv.org/abs/2107.05201)


  Modeling and managing portfolio risk is perhaps the most important step to
achieve growing and preserving investment performance. Within the modern
portfolio construction framework that built on Markowitz's theory, the
covariance matrix of stock returns is required to model the portfolio risk.
Traditional approaches to estimate the covariance matrix are based on human
designed risk factors, which often requires tremendous time and effort to
design better risk factors to improve the covariance estimation. In this work,
we formulate the quest of mining risk factors as a learning problem and propose
a deep learning solution to effectively "design" risk factors with neural
networks. The learning objective is carefully set to ensure the learned risk
factors are effective in explaining stock returns as well as have desired
orthogonality and stability. Our experiments on the stock market data
demonstrate the effectiveness of the proposed method: our method can obtain
$1.9\%$ higher explained variance measured by $R^2$ and also reduce the risk of
a global minimum variance portfolio. Incremental analysis further supports our
design of both the architecture and the learning objective.

    

### [[2107.05204] Sliding Spectrum Decomposition for Diversified Recommendation](http://arxiv.org/abs/2107.05204)


  Content feed, a type of product that recommends a sequence of items for users
to browse and engage with, has gained tremendous popularity among social media
platforms. In this paper, we propose to study the diversity problem in such a
scenario from an item sequence perspective using time series analysis
techniques. We derive a method called sliding spectrum decomposition (SSD) that
captures users' perception of diversity in browsing a long item sequence. We
also share our experiences in designing and implementing a suitable item
embedding method for accurate similarity measurement under long tail effect.
Combined together, they are now fully implemented and deployed in Xiaohongshu
App's production recommender system that serves the main Explore Feed product
for tens of millions of users every day. We demonstrate the effectiveness and
efficiency of the method through theoretical analysis, offline experiments and
online A/B tests.

    

### [[2107.05216] A Simple Reward-free Approach to Constrained Reinforcement Learning](http://arxiv.org/abs/2107.05216)


  In constrained reinforcement learning (RL), a learning agent seeks to not
only optimize the overall reward but also satisfy the additional safety,
diversity, or budget constraints. Consequently, existing constrained RL
solutions require several new algorithmic ingredients that are notably
different from standard RL. On the other hand, reward-free RL is independently
developed in the unconstrained literature, which learns the transition dynamics
without using the reward information, and thus naturally capable of addressing
RL with multiple objectives under the common dynamics. This paper bridges
reward-free RL and constrained RL. Particularly, we propose a simple
meta-algorithm such that given any reward-free RL oracle, the approachability
and constrained RL problems can be directly solved with negligible overheads in
sample complexity. Utilizing the existing reward-free RL solvers, our framework
provides sharp sample complexity results for constrained RL in the tabular MDP
setting, matching the best existing results up to a factor of horizon
dependence; our framework directly extends to a setting of tabular two-player
Markov games, and gives a new result for constrained RL with linear function
approximation.

    

### [[2107.05217] Cautious Actor-Critic](http://arxiv.org/abs/2107.05217)


  The oscillating performance of off-policy learning and persisting errors in
the actor-critic (AC) setting call for algorithms that can conservatively learn
to suit the stability-critical applications better. In this paper, we propose a
novel off-policy AC algorithm cautious actor-critic (CAC). The name cautious
comes from the doubly conservative nature that we exploit the classic policy
interpolation from conservative policy iteration for the actor and the
entropy-regularization of conservative value iteration for the critic. Our key
observation is the entropy-regularized critic facilitates and simplifies the
unwieldy interpolated actor update while still ensuring robust policy
improvement. We compare CAC to state-of-the-art AC methods on a set of
challenging continuous control problems and demonstrate that CAC achieves
comparable performance while significantly stabilizes learning.

    

### [[2107.05222] Perceptual-based deep-learning denoiser as a defense against adversarial attacks on ASR systems](http://arxiv.org/abs/2107.05222)


  In this paper we investigate speech denoising as a defense against
adversarial attacks on automatic speech recognition (ASR) systems. Adversarial
attacks attempt to force misclassification by adding small perturbations to the
original speech signal. We propose to counteract this by employing a
neural-network based denoiser as a pre-processor in the ASR pipeline. The
denoiser is independent of the downstream ASR model, and thus can be rapidly
deployed in existing systems. We found that training the denoisier using a
perceptually motivated loss function resulted in increased adversarial
robustness without compromising ASR performance on benign samples. Our defense
was evaluated (as a part of the DARPA GARD program) on the 'Kenansville' attack
strategy across a range of attack strengths and speech samples. An average
improvement in Word Error Rate (WER) of about 7.7% was observed over the
undefended model at 20 dB signal-to-noise-ratio (SNR) attack strength.

    

### [[2107.05223] MidiBERT-Piano: Large-scale Pre-training for Symbolic Music Understanding](http://arxiv.org/abs/2107.05223)


  This paper presents an attempt to employ the mask language modeling approach
of BERT to pre-train a 12-layer Transformer model over 4,166 pieces of
polyphonic piano MIDI files for tackling a number of symbolic-domain
discriminative music understanding tasks. These include two note-level
classification tasks, i.e., melody extraction and velocity prediction, as well
as two sequence-level classification tasks, i.e., composer classification and
emotion classification. We find that, given a pre-trained Transformer, our
models outperform recurrent neural network based baselines with less than 10
epochs of fine-tuning. Ablation studies show that the pre-training remains
effective even if none of the MIDI data of the downstream tasks are seen at the
pre-training stage, and that freezing the self-attention layers of the
Transformer at the fine-tuning stage slightly degrades performance. All the
five datasets employed in this work are publicly available, as well as
checkpoints of our pre-trained and fine-tuned models. As such, our research can
be taken as a benchmark for symbolic-domain music understanding.

    

### [[2107.05230] Predicting sepsis in multi-site, multi-national intensive care cohorts using deep learning](http://arxiv.org/abs/2107.05230)


  Despite decades of clinical research, sepsis remains a global public health
crisis with high mortality, and morbidity. Currently, when sepsis is detected
and the underlying pathogen is identified, organ damage may have already
progressed to irreversible stages. Effective sepsis management is therefore
highly time-sensitive. By systematically analysing trends in the plethora of
clinical data available in the intensive care unit (ICU), an early prediction
of sepsis could lead to earlier pathogen identification, resistance testing,
and effective antibiotic and supportive treatment, and thereby become a
life-saving measure. Here, we developed and validated a machine learning (ML)
system for the prediction of sepsis in the ICU. Our analysis represents the
largest multi-national, multi-centre in-ICU study for sepsis prediction using
ML to date. Our dataset contains $156,309$ unique ICU admissions, which
represent a refined and harmonised subset of five large ICU databases
originating from three countries. Using the international consensus definition
Sepsis-3, we derived hourly-resolved sepsis label annotations, amounting to
$26,734$ ($17.1\%$) septic stays. We compared our approach, a deep
self-attention model, to several clinical baselines as well as ML baselines and
performed an extensive internal and external validation within and across
databases. On average, our model was able to predict sepsis with an AUROC of
$0.847 \pm 0.050$ (internal out-of sample validation) and $0.761 \pm 0.052$
(external validation). For a harmonised prevalence of $17\%$, at $80\%$ recall
our model detects septic patients with $39\%$ precision 3.7 hours in advance.

    

### [[2107.05235] Position-enhanced and Time-aware Graph Convolutional Network for Sequential Recommendations](http://arxiv.org/abs/2107.05235)


  Most of the existing deep learning-based sequential recommendation approaches
utilize the recurrent neural network architecture or self-attention to model
the sequential patterns and temporal influence among a user's historical
behavior and learn the user's preference at a specific time. However, these
methods have two main drawbacks. First, they focus on modeling users' dynamic
states from a user-centric perspective and always neglect the dynamics of items
over time. Second, most of them deal with only the first-order user-item
interactions and do not consider the high-order connectivity between users and
items, which has recently been proved helpful for the sequential
recommendation. To address the above problems, in this article, we attempt to
model user-item interactions by a bipartite graph structure and propose a new
recommendation approach based on a Position-enhanced and Time-aware Graph
Convolutional Network (PTGCN) for the sequential recommendation. PTGCN models
the sequential patterns and temporal dynamics between user-item interactions by
defining a position-enhanced and time-aware graph convolution operation and
learning the dynamic representations of users and items simultaneously on the
bipartite graph with a self-attention aggregator. Also, it realizes the
high-order connectivity between users and items by stacking multi-layer graph
convolutions. To demonstrate the effectiveness of PTGCN, we carried out a
comprehensive evaluation of PTGCN on three real-world datasets of different
sizes compared with a few competitive baselines. Experimental results indicate
that PTGCN outperforms several state-of-the-art models in terms of two
commonly-used evaluation metrics for ranking.

    

### [[2107.05241] Prb-GAN: A Probabilistic Framework for GAN Modelling](http://arxiv.org/abs/2107.05241)


  Generative adversarial networks (GANs) are very popular to generate realistic
images, but they often suffer from the training instability issues and the
phenomenon of mode loss. In order to attain greater diversity in GAN
synthesized data, it is critical to solving the problem of mode loss. Our work
explores probabilistic approaches to GAN modelling that could allow us to
tackle these issues. We present Prb-GANs, a new variation that uses dropout to
create a distribution over the network parameters with the posterior learnt
using variational inference. We describe theoretically and validate
experimentally using simple and complex datasets the benefits of such an
approach. We look into further improvements using the concept of uncertainty
measures. Through a set of further modifications to the loss functions for each
network of the GAN, we are able to get results that show the improvement of GAN
performance. Our methods are extremely simple and require very little
modification to existing GAN architecture.

    

### [[2107.05252] OmniLytics: A Blockchain-based Secure Data Market for Decentralized Machine Learning](http://arxiv.org/abs/2107.05252)


  We propose OmniLytics, a blockchain-based secure data trading marketplace for
machine learning applications. Utilizing OmniLytics, many distributed data
owners can contribute their private data to collectively train a ML model
requested by some model owners, and get compensated for data contribution.
OmniLytics enables such model training while simultaneously providing 1) model
security against curious data owners; 2) data security against curious model
and data owners; 3) resilience to malicious data owners who provide faulty
results to poison model training; and 4) resilience to malicious model owner
who intents to evade the payment. OmniLytics is implemented as a smart contract
on the Ethereum blockchain to guarantee the atomicity of payment. In
OmniLytics, a model owner publishes encrypted initial model on the contract,
over which the participating data owners compute gradients using their private
data, and securely aggregate the gradients through the contract. Finally, the
contract reimburses the data owners, and the model owner decrypts the
aggregated model update. We implement a working prototype of OmniLytics on
Ethereum, and perform extensive experiments to measure its gas cost and
execution time under various parameter combinations, demonstrating its high
computation and cost efficiency and strong practicality.

    

### [[2107.05255] AutoFB: Automating Fetal Biometry Estimation from Standard Ultrasound Planes](http://arxiv.org/abs/2107.05255)


  During pregnancy, ultrasound examination in the second trimester can assess
fetal size according to standardized charts. To achieve a reproducible and
accurate measurement, a sonographer needs to identify three standard 2D planes
of the fetal anatomy (head, abdomen, femur) and manually mark the key
anatomical landmarks on the image for accurate biometry and fetal weight
estimation. This can be a time-consuming operator-dependent task, especially
for a trainee sonographer. Computer-assisted techniques can help in automating
the fetal biometry computation process. In this paper, we present a unified
automated framework for estimating all measurements needed for the fetal weight
assessment. The proposed framework semantically segments the key fetal
anatomies using state-of-the-art segmentation models, followed by region
fitting and scale recovery for the biometry estimation. We present an ablation
study of segmentation algorithms to show their robustness through 4-fold
cross-validation on a dataset of 349 ultrasound standard plane images from 42
pregnancies. Moreover, we show that the network with the best segmentation
performance tends to be more accurate for biometry estimation. Furthermore, we
demonstrate that the error between clinically measured and predicted fetal
biometry is lower than the permissible error during routine clinical
measurements.

    

### [[2107.05264] The Brownian motion in the transformer model](http://arxiv.org/abs/2107.05264)


  Transformer is the state of the art model for many language and visual tasks.
In this paper, we give a deep analysis of its multi-head self-attention (MHSA)
module and find that: 1) Each token is a random variable in high dimensional
feature space. 2) After layer normalization, these variables are mapped to
points on the hyper-sphere. 3) The update of these tokens is a Brownian motion.
The Brownian motion has special properties, its second order item should not be
ignored. So we present a new second-order optimizer(an iterative K-FAC
algorithm) for the MHSA module.
In some short words: All tokens are mapped to high dimension hyper-sphere.
The Scaled Dot-Product Attention
$softmax(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d}})$ is just the Markov
transition matrix for the random walking on the sphere. And the deep learning
process would learn proper kernel function to get proper positions of these
tokens. The training process in the MHSA module corresponds to a Brownian
motion worthy of further study.

    

### [[2107.05270] Learned super resolution ultrasound for improved breast lesion characterization](http://arxiv.org/abs/2107.05270)


  Breast cancer is the most common malignancy in women. Mammographic findings
such as microcalcifications and masses, as well as morphologic features of
masses in sonographic scans, are the main diagnostic targets for tumor
detection. However, improved specificity of these imaging modalities is
required. A leading alternative target is neoangiogenesis. When pathological,
it contributes to the development of numerous types of tumors, and the
formation of metastases. Hence, demonstrating neoangiogenesis by visualization
of the microvasculature may be of great importance. Super resolution ultrasound
localization microscopy enables imaging of the microvasculature at the
capillary level. Yet, challenges such as long reconstruction time, dependency
on prior knowledge of the system Point Spread Function (PSF), and separability
of the Ultrasound Contrast Agents (UCAs), need to be addressed for translation
of super-resolution US into the clinic. In this work we use a deep neural
network architecture that makes effective use of signal structure to address
these challenges. We present in vivo human results of three different breast
lesions acquired with a clinical US scanner. By leveraging our trained network,
the microvasculature structure is recovered in a short time, without prior PSF
knowledge, and without requiring separability of the UCAs. Each of the
recoveries exhibits a different structure that corresponds with the known
histological structure. This study demonstrates the feasibility of in vivo
human super resolution, based on a clinical scanner, to increase US specificity
for different breast lesions and promotes the use of US in the diagnosis of
breast pathologies.

    

### [[2107.05289] Continuous Time Bandits With Sampling Costs](http://arxiv.org/abs/2107.05289)


  We consider a continuous-time multi-arm bandit problem (CTMAB), where the
learner can sample arms any number of times in a given interval and obtain a
random reward from each sample, however, increasing the frequency of sampling
incurs an additive penalty/cost. Thus, there is a tradeoff between obtaining
large reward and incurring sampling cost as a function of the sampling
frequency. The goal is to design a learning algorithm that minimizes regret,
that is defined as the difference of the payoff of the oracle policy and that
of the learning algorithm. CTMAB is fundamentally different than the usual
multi-arm bandit problem (MAB), e.g., even the single-arm case is non-trivial
in CTMAB, since the optimal sampling frequency depends on the mean of the arm,
which needs to be estimated. We first establish lower bounds on the regret
achievable with any algorithm and then propose algorithms that achieve the
lower bound up to logarithmic factors. For the single-arm case, we show that
the lower bound on the regret is $\Omega((\log T)^2/\mu)$, where $\mu$ is the
mean of the arm, and $T$ is the time horizon. For the multiple arms case, we
show that the lower bound on the regret is $\Omega((\log T)^2 \mu/\Delta^2)$,
where $\mu$ now represents the mean of the best arm, and $\Delta$ is the
difference of the mean of the best and the second-best arm. We then propose an
algorithm that achieves the bound up to constant terms.

    

### [[2107.05295] DaCy: A Unified Framework for Danish NLP](http://arxiv.org/abs/2107.05295)


  Danish natural language processing (NLP) has in recent years obtained
considerable improvements with the addition of multiple new datasets and
models. However, at present, there is no coherent framework for applying
state-of-the-art models for Danish. We present DaCy: a unified framework for
Danish NLP built on SpaCy. DaCy uses efficient multitask models which obtain
state-of-the-art performance on named entity recognition, part-of-speech
tagging, and dependency parsing. DaCy contains tools for easy integration of
existing models such as for polarity, emotion, or subjectivity detection. In
addition, we conduct a series of tests for biases and robustness of Danish NLP
pipelines through augmentation of the test set of DaNE. DaCy large compares
favorably and is especially robust to long input lengths and spelling
variations and errors. All models except DaCy large display significant biases
related to ethnicity while only Polyglot shows a significant gender bias. We
argue that for languages with limited benchmark sets, data augmentation can be
particularly useful for obtaining more realistic and fine-grained performance
estimates. We provide a series of augmenters as a first step towards a more
thorough evaluation of language models for low and medium resource languages
and encourage further development.

    

### [[2107.05298] HEMP: High-order Entropy Minimization for neural network comPression](http://arxiv.org/abs/2107.05298)


  We formulate the entropy of a quantized artificial neural network as a
differentiable function that can be plugged as a regularization term into the
cost function minimized by gradient descent. Our formulation scales efficiently
beyond the first order and is agnostic of the quantization scheme. The network
can then be trained to minimize the entropy of the quantized parameters, so
that they can be optimally compressed via entropy coding. We experiment with
our entropy formulation at quantizing and compressing well-known network
architectures over multiple datasets. Our approach compares favorably over
similar methods, enjoying the benefits of higher order entropy estimate,
showing flexibility towards non-uniform quantization (we use Lloyd-max
quantization), scalability towards any entropy order to be minimized and
efficiency in terms of compression. We show that HEMP is able to work in
synergy with other approaches aiming at pruning or quantizing the model itself,
delivering significant benefits in terms of storage size compressibility
without harming the model's performance.

    

### [[2107.05320] Metalearning Linear Bandits by Prior Update](http://arxiv.org/abs/2107.05320)


  Fully Bayesian approaches to sequential decision-making assume that problem
parameters are generated from a known prior, while in practice, such
information is often lacking, and needs to be estimated through learning. This
problem is exacerbated in decision-making setups with partial information,
where using a misspecified prior may lead to poor exploration and inferior
performance. In this work we prove, in the context of stochastic linear bandits
and Gaussian priors, that as long as the prior estimate is sufficiently close
to the true prior, the performance of an algorithm that uses the misspecified
prior is close to that of the algorithm that uses the true prior. Next, we
address the task of learning the prior through metalearning, where a learner
updates its estimate of the prior across multiple task instances in order to
improve performance on future tasks. The estimated prior is then updated within
each task based on incoming observations, while actions are selected in order
to maximize expected reward. In this work we apply this scheme within a linear
bandit setting, and provide algorithms and regret bounds, demonstrating its
effectiveness, as compared to an algorithm that knows the correct prior. Our
results hold for a broad class of algorithms, including, for example, Thompson
Sampling and Information Directed Sampling.

    

### [[2107.05326] Learning interaction rules from multi-animal trajectories via augmented behavioral models](http://arxiv.org/abs/2107.05326)


  Extracting the interaction rules of biological agents from moving sequences
pose challenges in various domains. Granger causality is a practical framework
for analyzing the interactions from observed time-series data; however, this
framework ignores the structures of the generative process in animal behaviors,
which may lead to interpretational problems and sometimes erroneous assessments
of causality. In this paper, we propose a new framework for learning Granger
causality from multi-animal trajectories via augmented theory-based behavioral
models with interpretable data-driven models. We adopt an approach for
augmenting incomplete multi-agent behavioral models described by time-varying
dynamical systems with neural networks. For efficient and interpretable
learning, our model leverages theory-based architectures separating navigation
and motion processes, and the theory-guided regularization for reliable
behavioral modeling. This can provide interpretable signs of Granger-causal
effects over time, i.e., when specific others cause the approach or separation.
In experiments using synthetic datasets, our method achieved better performance
than various baselines. We then analyzed multi-animal datasets of mice, flies,
birds, and bats, which verified our method and obtained novel biological
insights.

    

### [[2107.05328] Structured Directional Pruning via Perturbation Orthogonal Projection](http://arxiv.org/abs/2107.05328)


  Structured pruning is an effective compression technique to reduce the
computation of neural networks, which is usually achieved by adding
perturbations to reduce network parameters at the cost of slightly increasing
training loss. A more reasonable approach is to find a sparse minimizer along
the flat minimum valley found by optimizers, i.e. stochastic gradient descent,
which keeps the training loss constant. To achieve this goal, we propose the
structured directional pruning based on orthogonal projecting the perturbations
onto the flat minimum valley. We also propose a fast solver sDprun and further
prove that it achieves directional pruning asymptotically after sufficient
training. Experiments using VGG-Net and ResNet on CIFAR-10 and CIFAR-100
datasets show that our method obtains the state-of-the-art pruned accuracy
(i.e. 93.97% on VGG16, CIFAR-10 task) without retraining. Experiments using
DNN, VGG-Net and WRN28X10 on MNIST, CIFAR-10 and CIFAR-100 datasets demonstrate
our method performs structured directional pruning, reaching the same minimum
valley as the optimizer.

    

### [[2107.05330] Personalized Federated Learning via Maximizing Correlation with Sparse and Hierarchical Extensions](http://arxiv.org/abs/2107.05330)


  Federated Learning (FL) is a collaborative machine learning technique to
train a global model without obtaining clients' private data. The main
challenges in FL are statistical diversity among clients, limited computing
capability among client equipments and the excessive communication overhead and
long latency between server and clients. To address these problems,
we propose a novel personalized federated learning via maximizing correlation
pFedMac), and further extend it to sparse and hierarchical models. By
minimizing loss functions including the properties of an approximated L1-norm
and the hierarchical correlation, the performance on statistical diversity data
is improved and the communicational and computational loads required in the
network are reduced. Theoretical proofs show that pFedMac performs better than
the L2-norm distance based personalization methods. Experimentally, we
demonstrate the benefits of this sparse hierarchical personalization
architecture compared with the state-of-the-art personalization methods and
their extensions (e.g. pFedMac achieves 99.75% accuracy on MNIST and 87.27%
accuracy on Synthetic under heterogeneous and non-i.i.d data distributions)

    

### [[2107.05341] Nonparametric Regression with Shallow Overparameterized Neural Networks Trained by GD with Early Stopping](http://arxiv.org/abs/2107.05341)


  We explore the ability of overparameterized shallow neural networks to learn
Lipschitz regression functions with and without label noise when trained by
Gradient Descent (GD). To avoid the problem that in the presence of noisy
labels, neural networks trained to nearly zero training error are inconsistent
on this class, we propose an early stopping rule that allows us to show optimal
rates. This provides an alternative to the result of Hu et al. (2021) who
studied the performance of $\ell 2$ -regularized GD for training shallow
networks in nonparametric regression which fully relied on the infinite-width
network (Neural Tangent Kernel (NTK)) approximation. Here we present a simpler
analysis which is based on a partitioning argument of the input space (as in
the case of 1-nearest-neighbor rule) coupled with the fact that trained neural
networks are smooth with respect to their inputs when trained by GD. In the
noise-free case the proof does not rely on any kernelization and can be
regarded as a finite-width result. In the case of label noise, by slightly
modifying the proof, the noise is controlled using a technique of Yao, Rosasco,
and Caponnetto (2007).

    

### [[2107.05342] EndoUDA: A modality independent segmentation approach for endoscopy imaging](http://arxiv.org/abs/2107.05342)


  Gastrointestinal (GI) cancer precursors require frequent monitoring for risk
stratification of patients. Automated segmentation methods can help to assess
risk areas more accurately, and assist in therapeutic procedures or even
removal. In clinical practice, addition to the conventional white-light imaging
(WLI), complimentary modalities such as narrow-band imaging (NBI) and
fluorescence imaging are used. While, today most segmentation approaches are
supervised and only concentrated on a single modality dataset, this work
exploits to use a target-independent unsupervised domain adaptation (UDA)
technique that is capable to generalize to an unseen target modality. In this
context, we propose a novel UDA-based segmentation method that couples the
variational autoencoder and U-Net with a common EfficientNet-B4 backbone, and
uses a joint loss for latent-space optimization for target samples. We show
that our model can generalize to unseen target NBI (target) modality when
trained using only WLI (source) modality. Our experiments on both upper and
lower GI endoscopy data show the effectiveness of our approach compared to
naive supervised approach and state-of-the-art UDA segmentation methods.

    

### [[2107.05377] A Flexible Multi-Task Model for BERT Serving](http://arxiv.org/abs/2107.05377)


  In this demonstration, we present an efficient BERT-based multi-task (MT)
framework that is particularly suitable for iterative and incremental
development of the tasks. The proposed framework is based on the idea of
partial fine-tuning, i.e. only fine-tune some top layers of BERT while keep the
other layers frozen. For each task, we train independently a single-task (ST)
model using partial fine-tuning. Then we compress the task-specific layers in
each ST model using knowledge distillation. Those compressed ST models are
finally merged into one MT model so that the frozen layers of the former are
shared across the tasks. We exemplify our approach on eight GLUE tasks,
demonstrating that it is able to achieve both strong performance and
efficiency. We have implemented our method in the utterance understanding
system of XiaoAI, a commercial AI assistant developed by Xiaomi. We estimate
that our model reduces the overall serving cost by 86%.

    

### [[2107.05380] DISCO : efficient unsupervised decoding for discrete natural language problems via convex relaxation](http://arxiv.org/abs/2107.05380)


  In this paper we study test time decoding; an ubiquitous step in almost all
sequential text generation task spanning across a wide array of natural
language processing (NLP) problems. Our main contribution is to develop a
continuous relaxation framework for the combinatorial NP-hard decoding problem
and propose Disco - an efficient algorithm based on standard first order
gradient based. We provide tight analysis and show that our proposed algorithm
linearly converges to within $\epsilon$ neighborhood of the optima. Finally, we
perform preliminary experiments on the task of adversarial text generation and
show superior performance of Disco over several popular decoding approaches.

    

### [[2107.05382] End-to-End Rich Transcription-Style Automatic Speech Recognition with Semi-Supervised Learning](http://arxiv.org/abs/2107.05382)


  We propose a semi-supervised learning method for building end-to-end rich
transcription-style automatic speech recognition (RT-ASR) systems from
small-scale rich transcription-style and large-scale common transcription-style
datasets. In spontaneous speech tasks, various speech phenomena such as
fillers, word fragments, laughter and coughs, etc. are often included. While
common transcriptions do not give special awareness to these phenomena, rich
transcriptions explicitly convert them into special phenomenon tokens as well
as textual tokens. In previous studies, the textual and phenomenon tokens were
simultaneously estimated in an end-to-end manner. However, it is difficult to
build accurate RT-ASR systems because large-scale rich transcription-style
datasets are often unavailable. To solve this problem, our training method uses
a limited rich transcription-style dataset and common transcription-style
dataset simultaneously. The Key process in our semi-supervised learning is to
convert the common transcription-style dataset into a pseudo-rich
transcription-style dataset. To this end, we introduce style tokens which
control phenomenon tokens are generated or not into transformer-based
autoregressive modeling. We use this modeling for generating the pseudo-rich
transcription-style datasets and for building RT-ASR system from the pseudo and
original datasets. Our experiments on spontaneous ASR tasks showed the
effectiveness of the proposed method.

    

### [[1805.08845] Counterfactual Mean Embeddings](http://arxiv.org/abs/1805.08845)


  Counterfactual inference has become a ubiquitous tool in online
advertisement, recommendation systems, medical diagnosis, and econometrics.
Accurate modeling of outcome distributions associated with different
interventions -- known as counterfactual distributions -- is crucial for the
success of these applications. In this work, we propose to model counterfactual
distributions using a novel Hilbert space representation called counterfactual
mean embedding (CME). The CME embeds the associated counterfactual distribution
into a reproducing kernel Hilbert space (RKHS) endowed with a positive definite
kernel, which allows us to perform causal inference over the entire landscape
of the counterfactual distribution. Based on this representation, we propose a
distributional treatment effect (DTE) that can quantify the causal effect over
entire outcome distributions. Our approach is nonparametric as the CME can be
estimated under the unconfoundedness assumption from observational data without
requiring any parametric assumption about the underlying distributions. We also
establish a rate of convergence of the proposed estimator which depends on the
smoothness of the conditional mean and the Radon-Nikodym derivative of the
underlying marginal distributions. Furthermore, our framework allows for more
complex outcomes such as images, sequences, and graphs. Our experimental
results on synthetic data and off-policy evaluation tasks demonstrate the
advantages of the proposed estimator.

    

### [[1807.11398] Preference-based Online Learning with Dueling Bandits: A Survey](http://arxiv.org/abs/1807.11398)


  In machine learning, the notion of multi-armed bandits refers to a class of
online learning problems, in which an agent is supposed to simultaneously
explore and exploit a given set of choice alternatives in the course of a
sequential decision process. In the standard setting, the agent learns from
stochastic feedback in the form of real-valued rewards. In many applications,
however, numerical reward signals are not readily available -- instead, only
weaker information is provided, in particular relative preferences in the form
of qualitative comparisons between pairs of alternatives. This observation has
motivated the study of variants of the multi-armed bandit problem, in which
more general representations are used both for the type of feedback to learn
from and the target of prediction. The aim of this paper is to provide a survey
of the state of the art in this field, referred to as preference-based
multi-armed bandits or dueling bandits. To this end, we provide an overview of
problems that have been considered in the literature as well as methods for
tackling them. Our taxonomy is mainly based on the assumptions made by these
methods about the data-generating process and, related to this, the properties
of the preference-based feedback.

    

### [[1903.12322] Implicit Langevin Algorithms for Sampling From Log-concave Densities](http://arxiv.org/abs/1903.12322)


  For sampling from a log-concave density, we study implicit integrators
resulting from $\theta$-method discretization of the overdamped Langevin
diffusion stochastic differential equation. Theoretical and algorithmic
properties of the resulting sampling methods for $ \theta \in [0,1] $ and a
range of step sizes are established. Our results generalize and extend prior
works in several directions. In particular, for $\theta\ge1/2$, we prove
geometric ergodicity and stability of the resulting methods for all step sizes.
We show that obtaining subsequent samples amounts to solving a strongly-convex
optimization problem, which is readily achievable using one of numerous
existing methods. Numerical examples supporting our theoretical analysis are
also presented.

    

### [[1904.02874] An Attentive Survey of Attention Models](http://arxiv.org/abs/1904.02874)


  Attention Model has now become an important concept in neural networks that
has been researched within diverse application domains. This survey provides a
structured and comprehensive overview of the developments in modeling
attention. In particular, we propose a taxonomy which groups existing
techniques into coherent categories. We review salient neural architectures in
which attention has been incorporated, and discuss applications in which
modeling attention has shown a significant impact. We also describe how
attention has been used to improve the interpretability of neural networks.
Finally, we discuss some future research directions in attention. We hope this
survey will provide a succinct introduction to attention models and guide
practitioners while developing approaches for their applications.

    

### [[1906.00536] Coupled VAE: Improved Accuracy and Robustness of a Variational Autoencoder](http://arxiv.org/abs/1906.00536)


  We present a coupled Variational Auto-Encoder (VAE) method that improves the
accuracy and robustness of the probabilistic inferences on represented data.
The new method models the dependency between input feature vectors (images) and
weighs the outliers with a higher penalty by generalizing the original loss
function to the coupled entropy function, using the principles of nonlinear
statistical coupling. We evaluate the performance of the coupled VAE model
using the MNIST dataset. Compared with the traditional VAE algorithm, the
output images generated by the coupled VAE method are clearer and less blurry.
The visualization of the input images embedded in 2D latent variable space
provides a deeper insight into the structure of new model with coupled loss
function: the latent variable has a smaller deviation and a more compact latent
space generates the output values. We analyze the histogram of the likelihoods
of the input images using the generalized mean, which measures the model's
accuracy as a function of the relative risk. The neutral accuracy, which is the
geometric mean and is consistent with a measure of the Shannon cross-entropy,
is improved. The robust accuracy, measured by the -2/3 generalized mean, is
also improved.

    

### [[2001.02492] Nonlinear Traffic Prediction as a Matrix Completion Problem with Ensemble Learning](http://arxiv.org/abs/2001.02492)


  This paper addresses the problem of short-term traffic prediction for
signalized traffic operations management. Specifically, we focus on predicting
sensor states in high-resolution (second-by-second). This contrasts with
traditional traffic forecasting problems, which have focused on predicting
aggregated traffic variables, typically over intervals that are no shorter than
5 minutes. Our contributions can be summarized as offering three insights:
first, we show how the prediction problem can be modeled as a matrix completion
problem. Second, we employ a block-coordinate descent algorithm and demonstrate
that the algorithm converges in sub-linear time to a block coordinate-wise
optimizer. This allows us to capitalize on the "bigness" of high-resolution
data in a computationally feasible way. Third, we develop an ensemble learning
(or adaptive boosting) approach to reduce the training error to within any
arbitrary error threshold. The latter utilizes past days so that the boosting
can be interpreted as capturing periodic patterns in the data. The performance
of the proposed method is analyzed theoretically and tested empirically using
both simulated data and a real-world high-resolution traffic dataset from Abu
Dhabi, UAE. Our experimental results show that the proposed method outperforms
other state-of-the-art algorithms.

    

### [[2001.09046] PDE-based Group Equivariant Convolutional Neural Networks](http://arxiv.org/abs/2001.09046)


  We present a PDE-based framework that generalizes Group equivariant
Convolutional Neural Networks (G-CNNs). In this framework, a network layer is
seen as a set of PDE-solvers where geometrically meaningful PDE-coefficients
become the layer's trainable weights. Formulating our PDEs on homogeneous
spaces allows these networks to be designed with built-in symmetries such as
rotation in addition to the standard translation equivariance of CNNs.
Having all the desired symmetries included in the design obviates the need to
include them by means of costly techniques such as data augmentation. We will
discuss our PDE-based G-CNNs (PDE-G-CNNs) in a general homogeneous space
setting while also going into the specifics of our primary case of interest:
roto-translation equivariance.
We solve the PDE of interest by a combination of linear group convolutions
and non-linear morphological group convolutions with analytic kernel
approximations that we underpin with formal theorems. Our kernel approximations
allow for fast GPU-implementation of the PDE-solvers, we release our
implementation with this article. Just like for linear convolution a
morphological convolution is specified by a kernel that we train in our
PDE-G-CNNs. In PDE-G-CNNs we do not use non-linearities such as max/min-pooling
and ReLUs as they are already subsumed by morphological convolutions.
We present a set of experiments to demonstrate the strength of the proposed
PDE-G-CNNs in increasing the performance of deep learning based imaging
applications with far fewer parameters than traditional CNNs.

    

### [[2002.04276] Towards explainable meta-learning](http://arxiv.org/abs/2002.04276)


  Meta-learning is a field that aims at discovering how different machine
learning algorithms perform on a wide range of predictive tasks. Such knowledge
speeds up the hyperparameter tuning or feature engineering. With the use of
surrogate models various aspects of the predictive task such as meta-features,
landmarker models e.t.c. are used to predict the expected performance. State of
the art approaches are focused on searching for the best meta-model but do not
explain how these different aspects contribute to its performance. However, to
build a new generation of meta-models we need a deeper understanding of the
importance and effect of meta-features on the model tunability. In this paper,
we propose techniques developed for eXplainable Artificial Intelligence (XAI)
to examine and extract knowledge from black-box surrogate models. To our
knowledge, this is the first paper that shows how post-hoc explainability can
be used to improve the meta-learning.

    

### [[2003.12699] Bypassing the Monster: A Faster and Simpler Optimal Algorithm for Contextual Bandits under Realizability](http://arxiv.org/abs/2003.12699)


  We consider the general (stochastic) contextual bandit problem under the
realizability assumption, i.e., the expected reward, as a function of contexts
and actions, belongs to a general function class $\mathcal{F}$. We design a
fast and simple algorithm that achieves the statistically optimal regret with
only ${O}(\log T)$ calls to an offline regression oracle across all $T$ rounds.
The number of oracle calls can be further reduced to $O(\log\log T)$ if $T$ is
known in advance. Our results provide the first universal and optimal reduction
from contextual bandits to offline regression, solving an important open
problem in the contextual bandit literature. A direct consequence of our
results is that any advances in offline regression immediately translate to
contextual bandits, statistically and computationally. This leads to faster
algorithms and improved regret guarantees for broader classes of contextual
bandit problems.

    

### [[2004.11154] Random Features for Kernel Approximation: A Survey on Algorithms, Theory, and Beyond](http://arxiv.org/abs/2004.11154)


  Random features is one of the most popular techniques to speed up kernel
methods in large-scale problems. Related works have been recognized by the
NeurIPS Test-of-Time award in 2017 and the ICML Best Paper Finalist in 2019.
The body of work on random features has grown rapidly, and hence it is
desirable to have a comprehensive overview on this topic explaining the
connections among various algorithms and theoretical results. In this survey,
we systematically review the work on random features from the past ten years.
First, the motivations, characteristics and contributions of representative
random features based algorithms are summarized according to their sampling
schemes, learning procedures, variance reduction properties and how they
exploit training data. Second, we review theoretical results that center around
the following key question: how many random features are needed to ensure a
high approximation quality or no loss in the empirical/expected risks of the
learned estimator. Third, we provide a comprehensive evaluation of popular
random features based algorithms on several large-scale benchmark datasets and
discuss their approximation quality and prediction performance for
classification. Last, we discuss the relationship between random features and
modern over-parameterized deep neural networks (DNNs), including the use of
high dimensional random features in the analysis of DNNs as well as the gaps
between current theoretical and empirical results. This survey may serve as a
gentle introduction to this topic, and as a users' guide for practitioners
interested in applying the representative algorithms and understanding
theoretical results under various technical assumptions. We hope that this
survey will facilitate discussion on the open problems in this topic, and more
importantly, shed light on future research directions.

    

### [[2005.07019] Social Media Information Sharing for Natural Disaster Response](http://arxiv.org/abs/2005.07019)


  Social media has become an essential channel for posting disaster-related
information, which provide governments and relief agencies real-time data for
better disaster management. However, research in this field has not received
sufficient attention and extracting useful information is still challenging.
This paper aims to improve disaster relief efficiency via mining and analyzing
social media data like public attitudes towards disaster response and public
demands for targeted relief supplies during different types of disasters. We
focus on different natural disasters based on properties such as types,
durations, and damages, which contains a total of 41,993 tweets. In this paper,
public perception is assessed qualitatively by manually classified tweets,
which contain information like the demand for targeted relief supplies,
satisfactions of disaster response, and public fear. Public attitudes to
natural disasters are studied via a quantitative analysis using eight machine
learning models. To better provide decision-makers with the appropriate model,
the comparison of machine learning models based on computational time and
prediction accuracy is conducted. The change of public opinion during different
natural disasters and the evolution of people's behavior of using social media
for disaster relief in the face of the identical type of natural disasters as
Twitter continues to evolve are studied. The results in this paper demonstrate
the feasibility and validation of the proposed research approach and provide
relief agencies with insights into better disaster management.

    

### [[2006.00339] Rethinking Assumptions in Deep Anomaly Detection](http://arxiv.org/abs/2006.00339)


  Though anomaly detection (AD) can be viewed as a classification problem
(nominal vs. anomalous) it is usually treated in an unsupervised manner since
one typically does not have access to, or it is infeasible to utilize, a
dataset that sufficiently characterizes what it means to be "anomalous." In
this paper we present results demonstrating that this intuition surprisingly
seems not to extend to deep AD on images. For a recent AD benchmark on
ImageNet, classifiers trained to discern between normal samples and just a few
(64) random natural images are able to outperform the current state of the art
in deep AD. Experimentally we discover that the multiscale structure of image
data makes example anomalies exceptionally informative.

    

### [[2006.03745] Re-understanding Finite-State Representations of Recurrent Policy Networks](http://arxiv.org/abs/2006.03745)


  We introduce an approach for understanding control policies represented as
recurrent neural networks. Recent work has approached this problem by
transforming such recurrent policy networks into finite-state machines (FSM)
and then analyzing the equivalent minimized FSM. While this led to interesting
insights, the minimization process can obscure a deeper understanding of a
machine's operation by merging states that are semantically distinct. To
address this issue, we introduce an analysis approach that starts with an
unminimized FSM and applies more-interpretable reductions that preserve the key
decision points of the policy. We also contribute an attention tool to attain a
deeper understanding of the role of observations in the decisions. Our case
studies on 7 Atari games and 3 control benchmarks demonstrate that the approach
can reveal insights that have not been previously noticed.

    

### [[2006.05103] The Curious Case of Convex Neural Networks](http://arxiv.org/abs/2006.05103)


  In this paper, we investigate a constrained formulation of neural networks
where the output is a convex function of the input. We show that the convexity
constraints can be enforced on both fully connected and convolutional layers,
making them applicable to most architectures. The convexity constraints include
restricting the weights (for all but the first layer) to be non-negative and
using a non-decreasing convex activation function. Albeit simple, these
constraints have profound implications on the generalization abilities of the
network. We draw three valuable insights: (a) Input Output Convex Neural
Networks (IOC-NNs) self regularize and reduce the problem of overfitting; (b)
Although heavily constrained, they outperform the base multi layer perceptrons
and achieve similar performance as compared to base convolutional architectures
and (c) IOC-NNs show robustness to noise in train labels. We demonstrate the
efficacy of the proposed idea using thorough experiments and ablation studies
on standard image classification datasets with three different neural network
architectures.

    

### [[2006.11645] Accelerating Safe Reinforcement Learning with Constraint-mismatched Policies](http://arxiv.org/abs/2006.11645)


  We consider the problem of reinforcement learning when provided with (1) a
baseline control policy and (2) a set of constraints that the learner must
satisfy. The baseline policy can arise from demonstration data or a teacher
agent and may provide useful cues for learning, but it might also be
sub-optimal for the task at hand, and is not guaranteed to satisfy the
specified constraints, which might encode safety, fairness or other
application-specific requirements. In order to safely learn from baseline
policies, we propose an iterative policy optimization algorithm that alternates
between maximizing expected return on the task, minimizing distance to the
baseline policy, and projecting the policy onto the constraint-satisfying set.
We analyze our algorithm theoretically and provide a finite-time convergence
guarantee. In our experiments on five different control tasks, our algorithm
consistently outperforms several state-of-the-art baselines, achieving 10 times
fewer constraint violations and 40% higher reward on average.

    

### [[2006.13202] Simple and Effective VAE Training with Calibrated Decoders](http://arxiv.org/abs/2006.13202)


  Variational autoencoders (VAEs) provide an effective and simple method for
modeling complex distributions. However, training VAEs often requires
considerable hyperparameter tuning to determine the optimal amount of
information retained by the latent variable. We study the impact of calibrated
decoders, which learn the uncertainty of the decoding distribution and can
determine this amount of information automatically, on the VAE performance.
While many methods for learning calibrated decoders have been proposed, many of
the recent papers that employ VAEs rely on heuristic hyperparameters and ad-hoc
modifications instead. We perform the first comprehensive comparative analysis
of calibrated decoder and provide recommendations for simple and effective VAE
training. Our analysis covers a range of image and video datasets and several
single-image and sequential VAE models. We further propose a simple but novel
modification to the commonly used Gaussian decoder, which computes the
prediction variance analytically. We observe empirically that using heuristic
modifications is not necessary with our method. Project website is at
this https URL


### [[2006.14536] Smooth Adversarial Training](http://arxiv.org/abs/2006.14536)


  It is commonly believed that networks cannot be both accurate and robust,
that gaining robustness means losing accuracy. It is also generally believed
that, unless making networks larger, network architectural elements would
otherwise matter little in improving adversarial robustness. Here we present
evidence to challenge these common beliefs by a careful study about adversarial
training. Our key observation is that the widely-used ReLU activation function
significantly weakens adversarial training due to its non-smooth nature. Hence
we propose smooth adversarial training (SAT), in which we replace ReLU with its
smooth approximations to strengthen adversarial training. The purpose of smooth
activation functions in SAT is to allow it to find harder adversarial examples
and compute better gradient updates during adversarial training.
Compared to standard adversarial training, SAT improves adversarial
robustness for "free", i.e., no drop in accuracy and no increase in
computational cost. For example, without introducing additional computations,
SAT significantly enhances ResNet-50's robustness from 33.0% to 42.3%, while
also improving accuracy by 0.9% on ImageNet. SAT also works well with larger
networks: it helps EfficientNet-L1 to achieve 82.2% accuracy and 58.6%
robustness on ImageNet, outperforming the previous state-of-the-art defense by
9.5% for accuracy and 11.6% for robustness. Models are available at
this https URL.

    

### [[2006.16912] Recovering Joint Probability of Discrete Random Variables from Pairwise Marginals](http://arxiv.org/abs/2006.16912)


  Learning the joint probability of random variables (RVs) is the cornerstone
of statistical signal processing and machine learning. However, direct
nonparametric estimation for high-dimensional joint probability is in general
impossible, due to the curse of dimensionality. Recent work has proposed to
recover the joint probability mass function (PMF) of an arbitrary number of RVs
from three-dimensional marginals, leveraging the algebraic properties of
low-rank tensor decomposition and the (unknown) dependence among the RVs.
Nonetheless, accurately estimating three-dimensional marginals can still be
costly in terms of sample complexity, affecting the performance of this line of
work in practice in the sample-starved regime. Using three-dimensional
marginals also involves challenging tensor decomposition problems whose
tractability is unclear. This work puts forth a new framework for learning the
joint PMF using only pairwise marginals, which naturally enjoys a lower sample
complexity relative to the third-order ones. A coupled nonnegative matrix
factorization (CNMF) framework is developed, and its joint PMF recovery
guarantees under various conditions are analyzed. Our method also features a
Gram--Schmidt (GS)-like algorithm that exhibits competitive runtime
performance. The algorithm is shown to provably recover the joint PMF up to
bounded error in finite iterations, under reasonable conditions. It is also
shown that a recently proposed economical expectation maximization (EM)
algorithm guarantees to improve upon the GS-like algorithm's output, thereby
further lifting up the accuracy and efficiency. Real-data experiments are
employed to showcase the effectiveness.

    

### [[2007.07314] Long-tail learning via logit adjustment](http://arxiv.org/abs/2007.07314)


  Real-world classification problems typically exhibit an imbalanced or
long-tailed label distribution, wherein many labels are associated with only a
few samples. This poses a challenge for generalisation on such labels, and also
makes naïve learning biased towards dominant labels. In this paper, we
present two simple modifications of standard softmax cross-entropy training to
cope with these challenges. Our techniques revisit the classic idea of logit
adjustment based on the label frequencies, either applied post-hoc to a trained
model, or enforced in the loss during training. Such adjustment encourages a
large relative margin between logits of rare versus dominant labels. These
techniques unify and generalise several recent proposals in the literature,
while possessing firmer statistical grounding and empirical performance.

    

### [[2007.10784] Fast Neural Models for Symbolic Regression at Scale](http://arxiv.org/abs/2007.10784)


  Deep learning owes much of its success to the astonishing expressiveness of
neural networks. However, this comes at the cost of complex, black-boxed models
that extrapolate poorly beyond the domain of the training dataset, conflicting
with goals of finding analytic expressions to describe science, engineering and
real world data. Under the hypothesis that the hierarchical modularity of such
laws can be captured by training a neural network, we introduce OccamNet, a
neural network model that finds interpretable, compact, and sparse solutions
for fitting data, à la Occam's razor. Our model defines a probability
distribution over a non-differentiable function space. We introduce a two-step
optimization method that samples functions and updates the weights with
backpropagation based on cross-entropy matching in an evolutionary strategy: we
train by biasing the probability mass toward better fitting solutions. OccamNet
is able to fit a variety of symbolic laws including simple analytic functions,
recursive programs, implicit functions, simple image classification, and can
outperform noticeably state-of-the-art symbolic regression methods on real
world regression datasets. Our method requires minimal memory footprint, does
not require AI accelerators for efficient training, fits complicated functions
in minutes of training on a single CPU, and demonstrates significant
performance gains when scaled on a GPU. Our implementation, demonstrations and
instructions for reproducing the experiments are available at
this https URL.

    

### [[2008.05030] Reliable Post hoc Explanations: Modeling Uncertainty in Explainability](http://arxiv.org/abs/2008.05030)


  As black box explanations are increasingly being employed to establish model
credibility in high stakes settings, it is important to ensure that these
explanations are accurate and reliable. However, prior work demonstrates that
explanations generated by state-of-the-art techniques are inconsistent,
unstable, and provide very little insight into their correctness and
reliability. In addition, these methods are also computationally inefficient,
and require significant hyper-parameter tuning. In this paper, we address the
aforementioned challenges by developing a novel Bayesian framework for
generating local explanations along with their associated uncertainty. We
instantiate this framework to obtain Bayesian versions of LIME and KernelSHAP
which output credible intervals for the feature importances, capturing the
associated uncertainty. The resulting explanations not only enable us to make
concrete inferences about their quality (e.g., there is a 95\% chance that the
feature importance lies within the given range), but are also highly consistent
and stable. We carry out a detailed theoretical analysis that leverages the
aforementioned uncertainty to estimate how many perturbations to sample, and
how to sample for faster convergence. This work makes the first attempt at
addressing several critical issues with popular explanation methods in one
shot, thereby generating consistent, stable, and reliable explanations with
guarantees in a computationally efficient manner. Experimental evaluation with
multiple real world datasets and user studies demonstrate that the efficacy of
the proposed framework.

    

### [[2008.09646] HRVGAN: High Resolution Video Generation using Spatio-Temporal GAN](http://arxiv.org/abs/2008.09646)


  In this paper, we present a novel network for high resolution video
generation. Our network uses ideas from Wasserstein GANs by enforcing
k-Lipschitz constraint on the loss term and Conditional GANs using class labels
for training and testing. We present Generator and Discriminator network
layerwise details along with the combined network architecture, optimization
details and algorithm used in this work. Our network uses a combination of two
loss terms: mean square pixel loss and an adversarial loss. The datasets used
for training and testing our network are UCF101, Golf and Aeroplane Datasets.
Using Inception Score and Fréchet Inception Distance as the evaluation
metrics, our network outperforms previous state of the art networks on
unsupervised video generation.

    

### [[2009.09217] A Joint introduction to Gaussian Processes and Relevance Vector Machines with Connections to Kalman filtering and other Kernel Smoothers](http://arxiv.org/abs/2009.09217)


  The expressive power of Bayesian kernel-based methods has led them to become
an important tool across many different facets of artificial intelligence, and
useful to a plethora of modern application domains, providing both power and
interpretability via uncertainty analysis. This article introduces and
discusses two methods which straddle the areas of probabilistic Bayesian
schemes and kernel methods for regression: Gaussian Processes and Relevance
Vector Machines. Our focus is on developing a common framework with which to
view these methods, via intermediate methods a probabilistic version of the
well-known kernel ridge regression, and drawing connections among them, via
dual formulations, and discussion of their application in the context of major
tasks: regression, smoothing, interpolation, and filtering. Overall, we provide
understanding of the mathematical concepts behind these models, and we
summarize and discuss in depth different interpretations and highlight the
relationship to other methods, such as linear kernel smoothers, Kalman
filtering and Fourier approximations. Throughout, we provide numerous figures
to promote understanding, and we make numerous recommendations to
practitioners. Benefits and drawbacks of the different techniques are
highlighted. To our knowledge, this is the most in-depth study of its kind to
date focused on these two methods, and will be relevant to theoretical
understanding and practitioners throughout the domains of data-science, signal
processing, machine learning, and artificial intelligence in general.

    

### [[2009.13472] Targeted VAE: Variational and Targeted Learning for Causal Inference](http://arxiv.org/abs/2009.13472)


  Undertaking causal inference with observational data is incredibly useful
across a wide range of tasks including the development of medical treatments,
advertisements and marketing, and policy making. There are two significant
challenges associated with undertaking causal inference using observational
data: treatment assignment heterogeneity (i.e., differences between the treated
and untreated groups), and an absence of counterfactual data (i.e., not knowing
what would have happened if an individual who did get treatment, were instead
to have not been treated). We address these two challenges by combining
structured inference and targeted learning. In terms of structure, we factorize
the joint distribution into risk, confounding, instrumental, and miscellaneous
factors, and in terms of targeted learning, we apply a regularizer derived from
the influence curve in order to reduce residual bias. An ablation study is
undertaken, and an evaluation on benchmark datasets demonstrates that TVAE has
competitive and state of the art performance across.

    

### [[2010.04248] Stochastically forced ensemble dynamic mode decomposition for forecasting and analysis of near-periodic systems](http://arxiv.org/abs/2010.04248)


  Time series forecasting remains a central challenge problem in almost all
scientific disciplines. We introduce a novel load forecasting method in which
observed dynamics are modeled as a forced linear system using Dynamic Mode
Decomposition (DMD) in time delay coordinates. Central to this approach is the
insight that grid load, like many observables on complex real-world systems,
has an "almost-periodic" character, i.e., a continuous Fourier spectrum
punctuated by dominant peaks, which capture regular (e.g., daily or weekly)
recurrences in the dynamics. The forecasting method presented takes advantage
of this property by (i) regressing to a deterministic linear model whose
eigenspectrum maps onto those peaks, and (ii) simultaneously learning a
stochastic Gaussian process regression (GPR) process to actuate this system.
Our forecasting algorithm is compared against state-of-the-art forecasting
techniques not using additional explanatory variables and is shown to produce
superior performance. Moreover, its use of linear intrinsic dynamics offers a
number of desirable properties in terms of interpretability and parsimony.
Results are presented for a test case using load data from an electrical grid.
Load forecasting is an essential challenge in power systems engineering, with
major implications for real-time control, pricing, maintenance, and security
decisions.

    

### [[2010.06562] Unified lower bounds for interactive high-dimensional estimation under information constraints](http://arxiv.org/abs/2010.06562)


  We consider the task of distributed parameter estimation using interactive
protocols subject to local information constraints such as bandwidth
limitations, local differential privacy, and restricted measurements. We
provide a unified framework enabling us to derive a variety of (tight) minimax
lower bounds for different parametric families of distributions, both
continuous and discrete, under any $\ell_p$ loss. Our lower bound framework is
versatile and yields "plug-and-play" bounds that are widely applicable to a
large range of estimation problems. In particular, our approach recovers bounds
obtained using data processing inequalities and Cramér--Rao bounds, two other
alternative approaches for proving lower bounds in our setting of interest.
Further, for the families considered, we complement our lower bounds with
matching upper bounds.

    

### [[2010.11415] Maximum Mean Discrepancy Test is Aware of Adversarial Attacks](http://arxiv.org/abs/2010.11415)


  The maximum mean discrepancy (MMD) test could in principle detect any
distributional discrepancy between two datasets. However, it has been shown
that the MMD test is unaware of adversarial attacks -- the MMD test failed to
detect the discrepancy between natural and adversarial data. Given this
phenomenon, we raise a question: are natural and adversarial data really from
different distributions? The answer is affirmative -- the previous use of the
MMD test on the purpose missed three key factors, and accordingly, we propose
three components. Firstly, the Gaussian kernel has limited representation
power, and we replace it with an effective deep kernel. Secondly, the test
power of the MMD test was neglected, and we maximize it following asymptotic
statistics. Finally, adversarial data may be non-independent, and we overcome
this issue with the wild bootstrap. By taking care of the three factors, we
verify that the MMD test is aware of adversarial attacks, which lights up a
novel road for adversarial data detection based on two-sample tests.

    

### [[2010.12562] On the Transformer Growth for Progressive BERT Training](http://arxiv.org/abs/2010.12562)


  Due to the excessive cost of large-scale language model pre-training,
considerable efforts have been made to train BERT progressively -- start from
an inferior but low-cost model and gradually grow the model to increase the
computational complexity. Our objective is to advance the understanding of
Transformer growth and discover principles that guide progressive training.
First, we find that similar to network architecture search, Transformer growth
also favors compound scaling. Specifically, while existing methods only conduct
network growth in a single dimension, we observe that it is beneficial to use
compound growth operators and balance multiple dimensions (e.g., depth, width,
and input length of the model). Moreover, we explore alternative growth
operators in each dimension via controlled comparison to give operator
selection practical guidance. In light of our analyses, the proposed method
speeds up BERT pre-training by 73.6% and 82.2% for the base and large models
respectively, while achieving comparable performances

    

### [[2010.12876] Electromagnetic Source Imaging via a Data-Synthesis-Based Denoising Autoencoder](http://arxiv.org/abs/2010.12876)


  Electromagnetic source imaging (ESI) is a highly ill-posed inverse problem.
To find a unique solution, traditional ESI methods impose a variety of priors
that may not reflect the actual source properties. Such limitations of
traditional ESI methods hinder their further applications. Inspired by deep
learning approaches, a novel data-synthesized spatio-temporal denoising
autoencoder method (DST-DAE) method was proposed to solve the ESI inverse
problem. Unlike the traditional methods, we utilize a neural network to
directly seek generalized mapping from the measured E/MEG signals to the
cortical sources. A novel data synthesis strategy is employed by introducing
the prior information of sources to the generated large-scale samples using the
forward model of ESI. All the generated data are used to drive the neural
network to automatically learn inverse mapping. To achieve better estimation
performance, a denoising autoencoder (DAE) architecture with spatio-temporal
feature extraction blocks is designed. Compared with the traditional methods,
we show (1) that the novel deep learning approach provides an effective and
easy-to-apply way to solve the ESI problem, that (2) compared to traditional
methods, DST-DAE with the data synthesis strategy can better consider the
characteristics of real sources than the mathematical formulation of prior
assumptions, and that (3) the specifically designed architecture of DAE can not
only provide a better estimation of source signals but also be robust to noise
pollution. Extensive numerical experiments show that the proposed method is
superior to the traditional knowledge-driven ESI methods.

    

### [[2010.16188] Bridge Composite and Real: Towards End-to-end Deep Image Matting](http://arxiv.org/abs/2010.16188)


  Extracting accurate foregrounds from natural images benefits many downstream
applications such as film production and augmented reality. However, the furry
characteristics and various appearance of the foregrounds, e.g., animal and
portrait, challenge existing matting methods, which usually require extra user
inputs such as trimap or scribbles. To resolve these problems, we study the
distinct roles of semantics and details for image matting and decompose the
task into two parallel sub-tasks: high-level semantic segmentation and
low-level details matting. Specifically, we propose a novel Glance and Focus
Matting network (GFM), which employs a shared encoder and two separate decoders
to learn both tasks in a collaborative manner for end-to-end natural image
matting. Besides, due to the limitation of available natural images in the
matting task, previous methods typically adopt composite images for training
and evaluation, which result in limited generalization ability on real-world
images. In this paper, we investigate the domain gap issue between composite
images and real-world images systematically by conducting comprehensive
analyses of various discrepancies between foreground and background images. We
find that a carefully designed composition route RSSN that aims to reduce the
discrepancies can lead to a better model with remarkable generalization
ability. Furthermore, we provide a benchmark containing 2,000 high-resolution
real-world animal images and 10,000 portrait images along with their manually
labeled alpha mattes to serve as a test bed for evaluating matting model's
generalization ability on real-world images. Comprehensive empirical studies
have demonstrated that GFM outperforms state-of-the-art methods and effectively
reduces the generalization error. The code and the dataset will be released.

    

### [[2011.08464] Exploring intermediate representation for monocular vehicle pose estimation](http://arxiv.org/abs/2011.08464)


  We present a new learning-based framework to recover vehicle pose in SO(3)
from a single RGB image. In contrast to previous works that map from local
appearance to observation angles, we explore a progressive approach by
extracting meaningful Intermediate Geometrical Representations (IGRs) to
estimate egocentric vehicle orientation. This approach features a deep model
that transforms perceived intensities to IGRs, which are mapped to a 3D
representation encoding object orientation in the camera coordinate system.
Core problems are what IGRs to use and how to learn them more effectively. We
answer the former question by designing IGRs based on an interpolated cuboid
that derives from primitive 3D annotation readily. The latter question
motivates us to incorporate geometry knowledge with a new loss function based
on a projective invariant. This loss function allows unlabeled data to be used
in the training stage to improve representation learning. Without additional
labels, our system outperforms previous monocular RGB-based methods for joint
vehicle detection and pose estimation on the KITTI benchmark, achieving
performance even comparable to stereo methods. Code and pre-trained models are
available at this https URL.

    

### [[2011.10695] Sparse sketches with small inversion bias](http://arxiv.org/abs/2011.10695)


  For a tall $n\times d$ matrix $A$ and a random $m\times n$ sketching matrix
$S$, the sketched estimate of the inverse covariance matrix $(A^\top A)^{-1}$
is typically biased: $E[(\tilde A^\top\tilde A)^{-1}]\ne(A^\top A)^{-1}$, where
$\tilde A=SA$. This phenomenon, which we call inversion bias, arises, e.g., in
statistics and distributed optimization, when averaging multiple independently
constructed estimates of quantities that depend on the inverse covariance. We
develop a framework for analyzing inversion bias, based on our proposed concept
of an $(\epsilon,\delta)$-unbiased estimator for random matrices. We show that
when the sketching matrix $S$ is dense and has i.i.d. sub-gaussian entries,
then after simple rescaling, the estimator $(\frac m{m-d}\tilde A^\top\tilde
A)^{-1}$ is $(\epsilon,\delta)$-unbiased for $(A^\top A)^{-1}$ with a sketch of
size $m=O(d+\sqrt d/\epsilon)$. This implies that for $m=O(d)$, the inversion
bias of this estimator is $O(1/\sqrt d)$, which is much smaller than the
$\Theta(1)$ approximation error obtained as a consequence of the subspace
embedding guarantee for sub-gaussian sketches. We then propose a new sketching
technique, called LEverage Score Sparsified (LESS) embeddings, which uses ideas
from both data-oblivious sparse embeddings as well as data-aware leverage-based
row sampling methods, to get $\epsilon$ inversion bias for sketch size
$m=O(d\log d+\sqrt d/\epsilon)$ in time $O(\text{nnz}(A)\log n+md^2)$, where
nnz is the number of non-zeros. The key techniques enabling our analysis
include an extension of a classical inequality of Bai and Silverstein for
random quadratic forms, which we call the Restricted Bai-Silverstein
inequality; and anti-concentration of the Binomial distribution via the
Paley-Zygmund inequality, which we use to prove a lower bound showing that
leverage score sampling sketches generally do not achieve small inversion bias.

    

### [[2011.12988] Mixed Membership Graph Clustering via Systematic Edge Query](http://arxiv.org/abs/2011.12988)


  This work considers clustering nodes of a largely incomplete graph. Under the
problem setting, only a small amount of queries about the edges can be made,
but the entire graph is not observable. This problem finds applications in
large-scale data clustering using limited annotations, community detection
under restricted survey resources, and graph topology inference under
hidden/removed node interactions. Prior works tackled this problem from various
perspectives, e.g., convex programming-based low-rank matrix completion and
active query-based clique finding. Nonetheless, many existing methods are
designed for estimating the single-cluster membership of the nodes, but nodes
may often have mixed (i.e., multi-cluster) membership in practice. Some query
and computational paradigms, e.g., the random query patterns and nuclear
norm-based optimization advocated in the convex approaches, may give rise to
scalability and implementation challenges. This work aims at learning mixed
membership of nodes using queried edges. The proposed method is developed
together with a systematic query principle that can be controlled and adjusted
by the system designers to accommodate implementation challenges -- e.g., to
avoid querying edges that are physically hard to acquire. Our framework also
features a lightweight and scalable algorithm with membership learning
guarantees. Real-data experiments on crowdclustering and community detection
are used to showcase the effectiveness of our method.

    

### [[2011.14058] Efficient Attention Network: Accelerate Attention by Searching Where to Plug](http://arxiv.org/abs/2011.14058)


  Recently, many plug-and-play self-attention modules are proposed to enhance
the model generalization by exploiting the internal information of deep
convolutional neural networks (CNNs). Previous works lay an emphasis on the
design of attention module for specific functionality, e.g., light-weighted or
task-oriented attention. However, they ignore the importance of where to plug
in the attention module since they connect the modules individually with each
block of the entire CNN backbone for granted, leading to incremental
computational cost and number of parameters with the growth of network depth.
Thus, we propose a framework called Efficient Attention Network (EAN) to
improve the efficiency for the existing attention modules. In EAN, we leverage
the sharing mechanism (Huang et al. 2020) to share the attention module within
the backbone and search where to connect the shared attention module via
reinforcement learning. Finally, we obtain the attention network with sparse
connections between the backbone and modules, while (1) maintaining accuracy
(2) reducing extra parameter increment and (3) accelerating inference.
Extensive experiments on widely-used benchmarks and popular attention networks
show the effectiveness of EAN. Furthermore, we empirically illustrate that our
EAN has the capacity of transferring to other tasks and capturing the
informative features. The code is available at
this https URL.

    

### [[2011.15056] General Invertible Transformations for Flow-based Generative Modeling](http://arxiv.org/abs/2011.15056)


  In this paper, we present a new class of invertible transformations with an
application to flow-based generative models. We indicate that many well-known
invertible transformations in reversible logic and reversible neural networks
could be derived from our proposition. Next, we propose two new coupling layers
that are important building blocks of flow-based generative models. In the
experiments on digit data, we present how these new coupling layers could be
used in Integer Discrete Flows (IDF), and that they achieve better results than
standard coupling layers used in IDF and RealNVP.

    

### [[2012.00857] StructFormer: Joint Unsupervised Induction of Dependency and Constituency Structure from Masked Language Modeling](http://arxiv.org/abs/2012.00857)


  There are two major classes of natural language grammar -- the dependency
grammar that models one-to-one correspondences between words and the
constituency grammar that models the assembly of one or several corresponded
words. While previous unsupervised parsing methods mostly focus on only
inducing one class of grammars, we introduce a novel model, StructFormer, that
can simultaneously induce dependency and constituency structure. To achieve
this, we propose a new parsing framework that can jointly generate a
constituency tree and dependency graph. Then we integrate the induced
dependency relations into the transformer, in a differentiable manner, through
a novel dependency-constrained self-attention mechanism. Experimental results
show that our model can achieve strong results on unsupervised constituency
parsing, unsupervised dependency parsing, and masked language modeling at the
same time.

    

### [[2012.01227] Message Passing Adaptive Resonance Theory for Online Active Semi-supervised Learning](http://arxiv.org/abs/2012.01227)


  Active learning is widely used to reduce labeling effort and training time by
repeatedly querying only the most beneficial samples from unlabeled data. In
real-world problems where data cannot be stored indefinitely due to limited
storage or privacy issues, the query selection and the model update should be
performed as soon as a new data sample is observed. Various online active
learning methods have been studied to deal with these challenges; however,
there are difficulties in selecting representative query samples and updating
the model efficiently without forgetting. In this study, we propose Message
Passing Adaptive Resonance Theory (MPART) that learns the distribution and
topology of input data online. Through message passing on the topological
graph, MPART actively queries informative and representative samples, and
continuously improves the classification performance using both labeled and
unlabeled data. We evaluate our model in stream-based selective sampling
scenarios with comparable query selection strategies, showing that MPART
significantly outperforms competitive models.

    

### [[2012.04595] Simulating the Time Projection Chamber responses at the MPD detector using Generative Adversarial Networks](http://arxiv.org/abs/2012.04595)


  High energy physics experiments rely heavily on the detailed detector
simulation models in many tasks. Running these detailed models typically
requires a notable amount of the computing time available to the experiments.
In this work, we demonstrate a new approach to speed up the simulation of the
Time Projection Chamber tracker of the MPD experiment at the NICA accelerator
complex. Our method is based on a Generative Adversarial Network - a deep
learning technique allowing for implicit estimation of the population
distribution for a given set of objects. This approach lets us learn and then
sample from the distribution of raw detector responses, conditioned on the
parameters of the charged particle tracks. To evaluate the quality of the
proposed model, we integrate a prototype into the MPD software stack and
demonstrate that it produces high-quality events similar to the detailed
simulator, with a speed-up of at least an order of magnitude. The prototype is
trained on the responses from the inner part of the detector and, once expanded
to the full detector, should be ready for use in physics tasks.

    

### [[2012.05433] Communication-Computation Efficient Secure Aggregation for Federated Learning](http://arxiv.org/abs/2012.05433)


  Federated learning has been spotlighted as a way to train neural networks
using distributed data with no need for individual nodes to share data.
Unfortunately, it has also been shown that adversaries may be able to extract
local data contents off model parameters transmitted during federated learning.
A recent solution based on the secure aggregation primitive enabled
privacy-preserving federated learning, but at the expense of significant extra
communication/computational resources. In this paper, we propose a
low-complexity scheme that provides data privacy using substantially reduced
communication/computational resources relative to the existing secure solution.
The key idea behind the suggested scheme is to design the topology of
secret-sharing nodes as a sparse random graph instead of the complete graph
corresponding to the existing solution. We first obtain the necessary and
sufficient condition on the graph to guarantee both reliability and privacy. We
then suggest using the Erdős-Rényi graph in particular and provide
theoretical guarantees on the reliability/privacy of the proposed scheme.
Through extensive real-world experiments, we demonstrate that our scheme, using
only $20 \sim 30\%$ of the resources required in the conventional scheme,
maintains virtually the same levels of reliability and data privacy in
practical federated learning systems.

    

### [[2012.06365] Feature Selection Based on Sparse Neural Network Layer with Normalizing Constraints](http://arxiv.org/abs/2012.06365)


  Feature selection is important step in machine learning since it has shown to
improve prediction accuracy while depressing the curse of dimensionality of
high dimensional data. The neural networks have experienced tremendous success
in solving many nonlinear learning problems. Here, we propose new
neural-network based feature selection approach that introduces two constrains,
the satisfying of which leads to sparse FS layer. We have performed extensive
experiments on synthetic and real world data to evaluate performance of the
proposed FS. In experiments we focus on the high dimension, low sample size
data since those represent the main challenge for feature selection. The
results confirm that proposed Feature Selection Based on Sparse Neural Network
Layer with Normalizing Constraints (SNEL-FS) is able to select the important
features and yields superior performance compared to other conventional FS
methods.

    

### [[2012.07816] Enabling collaborative data science development with the Ballet framework](http://arxiv.org/abs/2012.07816)


  While the open-source software development model has led to successful
large-scale collaborations in building software systems, data science projects
are frequently developed by individuals or small teams. We describe challenges
to scaling data science collaborations and present a conceptual framework and
ML programming model to address them. We instantiate these ideas in Ballet, a
lightweight framework for collaborative, open-source data science through a
focus on feature engineering, and an accompanying cloud-based development
environment. Using our framework, collaborators incrementally propose feature
definitions to a repository which are each subjected to an ML performance
evaluation and can be automatically merged into an executable feature
engineering pipeline. We leverage Ballet to conduct a case study analysis of an
income prediction problem with 27 collaborators, and discuss implications for
future designers of collaborative projects.

    

### [[2012.08743] Improving Multilingual Neural Machine Translation For Low-Resource Languages: French,English - Vietnamese](http://arxiv.org/abs/2012.08743)


  Prior works have demonstrated that a low-resource language pair can benefit
from multilingual machine translation (MT) systems, which rely on many language
pairs' joint training. This paper proposes two simple strategies to address the
rare word issue in multilingual MT systems for two low-resource language pairs:
French-Vietnamese and English-Vietnamese. The first strategy is about dynamical
learning word similarity of tokens in the shared space among source languages
while another one attempts to augment the translation ability of rare words
through updating their embeddings during the training. Besides, we leverage
monolingual data for multilingual MT systems to increase the amount of
synthetic parallel corpora while dealing with the data sparsity problem. We
have shown significant improvements of up to +1.62 and +2.54 BLEU points over
the bilingual baseline systems for both language pairs and released our
datasets for the research community.

    

### [[2012.13196] RBM-Flow and D-Flow: Invertible Flows with Discrete Energy Base Spaces](http://arxiv.org/abs/2012.13196)


  Efficient sampling of complex data distributions can be achieved using
trained invertible flows (IF), where the model distribution is generated by
pushing a simple base distribution through multiple non-linear bijective
transformations. However, the iterative nature of the transformations in IFs
can limit the approximation to the target distribution. In this paper we seek
to mitigate this by implementing RBM-Flow, an IF model whose base distribution
is a Restricted Boltzmann Machine (RBM) with a continuous smoothing applied. We
show that by using RBM-Flow we are able to improve the quality of samples
generated, quantified by the Inception Scores (IS) and Frechet Inception
Distance (FID), over baseline models with the same IF transformations, but with
less expressive base distributions. Furthermore, we also obtain D-Flow, an IF
model with uncorrelated discrete latent variables. We show that D-Flow achieves
similar likelihoods and FID/IS scores to those of a typical IF with Gaussian
base variables, but with the additional benefit that global features are
meaningfully encoded as discrete labels in the latent space.

    

### [[2101.00522] Privacy Preserving Domain Adaptation for Semantic Segmentation of Medical Images](http://arxiv.org/abs/2101.00522)


  Convolutional neural networks (CNNs) have led to significant improvements in
tasks involving semantic segmentation of images. CNNs are vulnerable in the
area of biomedical image segmentation because of distributional gap between two
source and target domains with different data modalities which leads to domain
shift. Domain shift makes data annotations in new modalities necessary because
models must be retrained from scratch. Unsupervised domain adaptation (UDA) is
proposed to adapt a model to new modalities using solely unlabeled target
domain data. Common UDA algorithms require access to data points in the source
domain which may not be feasible in medical imaging due to privacy concerns. In
this work, we develop an algorithm for UDA in a privacy-constrained setting,
where the source domain data is inaccessible. Our idea is based on encoding the
information from the source samples into a prototypical distribution that is
used as an intermediate distribution for aligning the target domain
distribution with the source domain distribution. We demonstrate the
effectiveness of our algorithm by comparing it to state-of-the-art medical
image semantic segmentation approaches on two medical image semantic
segmentation datasets.

    

### [[2102.01807] Building population models for large-scale neural recordings: opportunities and pitfalls](http://arxiv.org/abs/2102.01807)


  Modern recording technologies now enable simultaneous recording from large
numbers of neurons. This has driven the development of new statistical models
for analyzing and interpreting neural population activity. Here we provide a
broad overview of recent developments in this area. We compare and contrast
different approaches, highlight strengths and limitations, and discuss
biological and mechanistic insights that these methods provide.

    

### [[2102.03400] Confidence-Budget Matching for Sequential Budgeted Learning](http://arxiv.org/abs/2102.03400)


  A core element in decision-making under uncertainty is the feedback on the
quality of the performed actions. However, in many applications, such feedback
is restricted. For example, in recommendation systems, repeatedly asking the
user to provide feedback on the quality of recommendations will annoy them. In
this work, we formalize decision-making problems with querying budget, where
there is a (possibly time-dependent) hard limit on the number of reward queries
allowed. Specifically, we consider multi-armed bandits, linear bandits, and
reinforcement learning problems. We start by analyzing the performance of
`greedy' algorithms that query a reward whenever they can. We show that in
fully stochastic settings, doing so performs surprisingly well, but in the
presence of any adversity, this might lead to linear regret. To overcome this
issue, we propose the Confidence-Budget Matching (CBM) principle that queries
rewards when the confidence intervals are wider than the inverse square root of
the available budget. We analyze the performance of CBM based algorithms in
different settings and show that they perform well in the presence of adversity
in the contexts, initial states, and budgets.

    

### [[2102.04625] WheaCha: A Method for Explaining the Predictions of Code Summarization Models](http://arxiv.org/abs/2102.04625)


  The last decade has witnessed a rapid advance in machine learning models.
While the black-box nature of these systems allows powerful predictions, it
cannot be directly explained, posing a threat to the continuing democratization
of machine learning technology.
Tackling the challenge of model explainability, research has made significant
progress in demystifying the image classification models. In the same spirit of
these works, this paper studies code summarization models, particularly, given
an input program for which a model makes a prediction, our goal is to reveal
the key features that the model uses for predicting the label of the program.
We realize our approach in HouYi, which we use to evaluate four prominent code
summarization models: extreme summarizer, code2vec, code2seq, and sequence GNN.
Results show that all models base their predictions on syntactic and lexical
properties with little to none semantic implication. Based on this finding, we
present a novel approach to explaining the predictions of code summarization
models through the lens of training data.
Our work opens up this exciting, new direction of studying what models have
learned from source code.

    

### [[2102.05214] Task-Optimal Exploration in Linear Dynamical Systems](http://arxiv.org/abs/2102.05214)


  Exploration in unknown environments is a fundamental problem in reinforcement
learning and control. In this work, we study task-guided exploration and
determine what precisely an agent must learn about their environment in order
to complete a particular task. Formally, we study a broad class of
decision-making problems in the setting of linear dynamical systems, a class
that includes the linear quadratic regulator problem. We provide instance- and
task-dependent lower bounds which explicitly quantify the difficulty of
completing a task of interest. Motivated by our lower bound, we propose a
computationally efficient experiment-design based exploration algorithm. We
show that it optimally explores the environment, collecting precisely the
information needed to complete the task, and provide finite-time bounds
guaranteeing that it achieves the instance- and task-optimal sample complexity,
up to constant factors. Through several examples of the LQR problem, we show
that performing task-guided exploration provably improves on exploration
schemes which do not take into account the task of interest. Along the way, we
establish that certainty equivalence decision making is instance- and
task-optimal, and obtain the first algorithm for the linear quadratic regulator
problem which is instance-optimal. We conclude with several experiments
illustrating the effectiveness of our approach in practice.

    

### [[2102.06555] Online Graph Dictionary Learning](http://arxiv.org/abs/2102.06555)


  Dictionary learning is a key tool for representation learning, that explains
the data as linear combination of few basic elements. Yet, this analysis is not
amenable in the context of graph learning, as graphs usually belong to
different metric spaces. We fill this gap by proposing a new online Graph
Dictionary Learning approach, which uses the Gromov Wasserstein divergence for
the data fitting term. In our work, graphs are encoded through their nodes'
pairwise relations and modeled as convex combination of graph atoms, i.e.
dictionary elements, estimated thanks to an online stochastic algorithm, which
operates on a dataset of unregistered graphs with potentially different number
of nodes. Our approach naturally extends to labeled graphs, and is completed by
a novel upper bound that can be used as a fast approximation of Gromov
Wasserstein in the embedding space. We provide numerical evidences showing the
interest of our approach for unsupervised embedding of graph datasets and for
online graph subspace estimation and tracking.

    

### [[2102.07405] Tractable structured natural gradient descent using local parameterizations](http://arxiv.org/abs/2102.07405)


  Natural-gradient descent (NGD) on structured parameter spaces (e.g., low-rank
covariances) is computationally challenging due to difficult Fisher-matrix
computations. We address this issue by using \emph{local-parameter coordinates}
to obtain a flexible and efficient NGD method that works well for a
wide-variety of structured parameterizations. We show four applications where
our method (1) generalizes the exponential natural evolutionary strategy, (2)
recovers existing Newton-like algorithms, (3) yields new structured
second-order algorithms, and (4) gives new algorithms to learn covariances of
Gaussian and Wishart-based distributions. We show results on a range of
problems from deep learning, variational inference, and evolution strategies.
Our work opens a new direction for scalable structured geometric methods.

    

### [[2102.08747] Learning Visual Models using a Knowledge Graph as a Trainer](http://arxiv.org/abs/2102.08747)


  Traditional computer vision approaches, based on neural networks (NN), are
typically trained on a large amount of image data. By minimizing the
cross-entropy loss between a prediction and a given class label, the NN and its
visual embedding space are learned to fulfill a given task. However, due to the
sole dependence on the image data distribution of the training domain, these
models tend to fail when applied to a target domain that differs from their
source domain. To learn a more robust NN to domain shifts, we propose the
knowledge graph neural network (KG-NN), a neuro-symbolic approach that
supervises the training using image-data-invariant auxiliary knowledge. The
auxiliary knowledge is first encoded in a knowledge graph with respective
concepts and their relationships, which is then transformed into a dense vector
representation via an embedding method. Using a contrastive loss function,
KG-NN learns to adapt its visual embedding space and thus its weights according
to the image-data invariant knowledge graph embedding space. We evaluate KG-NN
on visual transfer learning tasks for classification using the mini-ImageNet
dataset and its derivatives, as well as road sign recognition datasets from
Germany and China. The results show that a visual model trained with a
knowledge graph as a trainer outperforms a model trained with cross-entropy in
all experiments, in particular when the domain gap increases. Besides better
performance and stronger robustness to domain shifts, these KG-NN adapts to
multiple datasets and classes without suffering heavily from catastrophic
forgetting.

    

### [[2102.09550] Going Full-TILT Boogie on Document Understanding with Text-Image-Layout Transformer](http://arxiv.org/abs/2102.09550)


  We address the challenging problem of Natural Language Comprehension beyond
plain-text documents by introducing the TILT neural network architecture which
simultaneously learns layout information, visual features, and textual
semantics. Contrary to previous approaches, we rely on a decoder capable of
unifying a variety of problems involving natural language. The layout is
represented as an attention bias and complemented with contextualized visual
information, while the core of our model is a pretrained encoder-decoder
Transformer. Our novel approach achieves state-of-the-art results in extracting
information from documents and answering questions which demand layout
understanding (DocVQA, CORD, SROIE). At the same time, we simplify the process
by employing an end-to-end model.

    

### [[2102.12344] Memory-based Deep Reinforcement Learning for POMDP](http://arxiv.org/abs/2102.12344)


  A promising characteristic of Deep Reinforcement Learning (DRL) is its
capability to learn optimal policy in an end-to-end manner without relying on
feature engineering. However, most approaches assume a fully observable state
space, i.e. fully observable Markov Decision Process (MDP). In real-world
robotics, this assumption is unpractical, because of the sensor issues such as
sensors' capacity limitation and sensor noise, and the lack of knowledge about
if the observation design is complete or not. These scenarios lead to Partially
Observable MDP (POMDP) and need special treatment. In this paper, we propose
Long-Short-Term-Memory-based Twin Delayed Deep Deterministic Policy Gradient
(LSTM-TD3) by introducing a memory component to TD3, and compare its
performance with other DRL algorithms in both MDPs and POMDPs. Our results
demonstrate the significant advantages of the memory component in addressing
POMDPs, including the ability to handle missing and noisy observation data.

    

### [[2103.00137] Meta-Learning with Graph Neural Networks: Methods and Applications](http://arxiv.org/abs/2103.00137)


  Graph Neural Networks (GNNs), a generalization of deep neural networks on
graph data have been widely used in various domains, ranging from drug
discovery to recommender systems. However, GNNs on such applications are
limited when there are few available samples. Meta-learning has been an
important framework to address the lack of samples in machine learning, and in
recent years, researchers have started to apply meta-learning to GNNs. In this
work, we provide a comprehensive survey of different meta-learning approaches
involving GNNs on various graph problems showing the power of using these two
approaches together. We categorize the literature based on proposed
architectures, shared representations, and applications. Finally, we discuss
several exciting future research directions and open problems.

    

### [[2103.03265] Better SGD using Second-order Momentum](http://arxiv.org/abs/2103.03265)


  We develop a new algorithm for non-convex stochastic optimization that finds
an $\epsilon$-critical point in the optimal $O(\epsilon^{-3})$ stochastic
gradient and Hessian-vector product computations. Our algorithm uses
Hessian-vector products to "correct" a bias term in the momentum of SGD with
momentum. This leads to better gradient estimates in a manner analogous to
variance reduction methods. In contrast to prior work, we do not require
excessively large batch sizes, and are able to provide an adaptive algorithm
whose convergence rate automatically improves with decreasing variance in the
gradient estimates. We validate our results on a variety of large-scale deep
learning architectures and benchmarks tasks.

    

### [[2103.04514] Nondeterminism and Instability in Neural Network Optimization](http://arxiv.org/abs/2103.04514)


  Nondeterminism in neural network optimization produces uncertainty in
performance, making small improvements difficult to discern from run-to-run
variability. While uncertainty can be reduced by training multiple model
copies, doing so is time-consuming, costly, and harms reproducibility. In this
work, we establish an experimental protocol for understanding the effect of
optimization nondeterminism on model diversity, allowing us to isolate the
effects of a variety of sources of nondeterminism. Surprisingly, we find that
all sources of nondeterminism have similar effects on measures of model
diversity. To explain this intriguing fact, we identify the instability of
model training, taken as an end-to-end procedure, as the key determinant. We
show that even one-bit changes in initial parameters result in models
converging to vastly different values. Last, we propose two approaches for
reducing the effects of instability on run-to-run variability.

    

### [[2103.04529] Self-Supervised Online Reward Shaping in Sparse-Reward Environments](http://arxiv.org/abs/2103.04529)


  We propose a novel reinforcement learning framework that performs
self-supervised online reward shaping, yielding faster, sample efficient
performance in sparse-reward environments. The proposed framework alternates
between updating a policy and inferring a reward function. While the policy
update is performed with the inferred, potentially dense reward function, the
original sparse reward is used to provide a self-supervisory signal for the
reward update by serving as an ordering over the observed trajectories. The
proposed framework is based on the theory that altering the reward function
does not affect the optimal policy of the original MDP as long as certain
relations between the altered and the original reward are maintained. We name
the proposed framework ClAssification-based Reward Shaping (CaReS), since the
altered reward is learned in a self-supervised manner using classifier-based
reward inference. Experimental results on several sparse-reward environments
demonstrate that the proposed algorithm is not only significantly more sample
efficient than the state-of-the-art reinforcement learning baseline but also
achieves a similar sample efficiency to a baseline that uses hand-designed
dense reward functions.

    

### [[2103.06671] Sample Complexity of Offline Reinforcement Learning with Deep ReLU Networks](http://arxiv.org/abs/2103.06671)


  We study the statistical theory of offline reinforcement learning (RL) with
deep ReLU network function approximation. We analyze a variant of fitted-Q
iteration (FQI) algorithm under a new dynamic condition that we call Besov
dynamic closure, which encompasses the conditions from prior analyses for deep
neural network function approximation. Under Besov dynamic closure, we prove
that the FQI-type algorithm enjoys the sample complexity of
$\tilde{\mathcal{O}}\left( \kappa^{1 + d/\alpha} \cdot \epsilon^{-2 -
2d/\alpha} \right)$ where $\kappa$ is a distribution shift measure, $d$ is the
dimensionality of the state-action space, $\alpha$ is the (possibly fractional)
smoothness parameter of the underlying MDP, and $\epsilon$ is a user-specified
precision. This is an improvement over the sample complexity of
$\tilde{\mathcal{O}}\left( K \cdot \kappa^{2 + d/\alpha} \cdot \epsilon^{-2 -
d/\alpha} \right)$ in the prior result [Yang et al., 2019] where $K$ is an
algorithmic iteration number which is arbitrarily large in practice.
Importantly, our sample complexity is obtained under the new general dynamic
condition and a data-dependent structure where the latter is either ignored in
prior algorithms or improperly handled by prior analyses. This is the first
comprehensive analysis for offline RL with deep ReLU network function
approximation under a general setting.

    

### [[2103.06828] Wasserstein Robust Classification with Fairness Constraints](http://arxiv.org/abs/2103.06828)


  We propose a distributionally robust classification model with a fairness
constraint that encourages the classifier to be fair in view of the equality of
opportunity criterion. We use a type-$\infty$ Wasserstein ambiguity set
centered at the empirical distribution to model distributional uncertainty and
derive a conservative reformulation for the worst-case equal opportunity
unfairness measure. We establish that the model is equivalent to a mixed binary
optimization problem, which can be solved by standard off-the-shelf solvers. To
improve scalability, we further propose a convex, hinge-loss-based model for
large problem instances whose reformulation does not incur any binary
variables. Moreover, we also consider the distributionally robust learning
problem with a generic ground transportation cost to hedge against the
uncertainties in the label and sensitive attribute. Finally, we numerically
demonstrate that our proposed approaches improve fairness with negligible loss
of predictive accuracy.

    

### [[2103.08659] Function approximation by deep neural networks with parameters $\{0,\pm \frac{1}{2}, \pm 1, 2\}$](http://arxiv.org/abs/2103.08659)


  In this paper it is shown that $C_\beta$-smooth functions can be approximated
by neural networks with parameters $\{0,\pm \frac{1}{2}, \pm 1, 2\}$. The
depth, width and the number of active parameters of the constructed networks
have, up to a logarithmic factor, the same dependence on the approximation
error as the networks with parameters in $[-1,1]$. In particular, this means
that the nonparametric regression estimation with the constructed networks
attains the same convergence rate as with sparse networks with parameters in
$[-1,1]$.

    

### [[2103.10897] Bilinear Classes: A Structural Framework for Provable Generalization in RL](http://arxiv.org/abs/2103.10897)


  This work introduces Bilinear Classes, a new structural framework, which
permit generalization in reinforcement learning in a wide variety of settings
through the use of function approximation. The framework incorporates nearly
all existing models in which a polynomial sample complexity is achievable, and,
notably, also includes new models, such as the Linear $Q^*/V^*$ model in which
both the optimal $Q$-function and the optimal $V$-function are linear in some
known feature space. Our main result provides an RL algorithm which has
polynomial sample complexity for Bilinear Classes; notably, this sample
complexity is stated in terms of a reduction to the generalization error of an
underlying supervised learning sub-problem. These bounds nearly match the best
known sample complexity bounds for existing models. Furthermore, this framework
also extends to the infinite dimensional (RKHS) setting: for the the Linear
$Q^*/V^*$ model, linear MDPs, and linear mixture MDPs, we provide sample
complexities that have no explicit dependence on the explicit feature dimension
(which could be infinite), but instead depends only on information theoretic
quantities.

    

### [[2103.11251] Interpretable Machine Learning: Fundamental Principles and 10 Grand Challenges](http://arxiv.org/abs/2103.11251)


  Interpretability in machine learning (ML) is crucial for high stakes
decisions and troubleshooting. In this work, we provide fundamental principles
for interpretable ML, and dispel common misunderstandings that dilute the
importance of this crucial topic. We also identify 10 technical challenge areas
in interpretable machine learning and provide history and background on each
problem. Some of these problems are classically important, and some are recent
problems that have arisen in the last few years. These problems are: (1)
Optimizing sparse logical models such as decision trees; (2) Optimization of
scoring systems; (3) Placing constraints into generalized additive models to
encourage sparsity and better interpretability; (4) Modern case-based
reasoning, including neural networks and matching for causal inference; (5)
Complete supervised disentanglement of neural networks; (6) Complete or even
partial unsupervised disentanglement of neural networks; (7) Dimensionality
reduction for data visualization; (8) Machine learning models that can
incorporate physics and other generative or causal constraints; (9)
Characterization of the "Rashomon set" of good models; and (10) Interpretable
reinforcement learning. This survey is suitable as a starting point for
statisticians and computer scientists interested in working in interpretable
machine learning.

    

### [[2103.15069] Self-supervised Discriminative Feature Learning for Deep Multi-view Clustering](http://arxiv.org/abs/2103.15069)


  Multi-view clustering is an important research topic due to its capability to
utilize complementary information from multiple views. However, there are few
methods to consider the negative impact caused by certain views with unclear
clustering structures, resulting in poor multi-view clustering performance. To
address this drawback, we propose self-supervised discriminative feature
learning for deep multi-view clustering (SDMVC). Concretely, deep autoencoders
are applied to learn embedded features for each view independently. To leverage
the multi-view complementary information, we concatenate all views' embedded
features to form the global features, which can overcome the negative impact of
some views' unclear clustering structures. In a self-supervised manner,
pseudo-labels are obtained to build a unified target distribution to perform
multi-view discriminative feature learning. During this process, global
discriminative information can be mined to supervise all views to learn more
discriminative features, which in turn are used to update the target
distribution. Besides, this unified target distribution can make SDMVC learn
consistent cluster assignments, which accomplishes the clustering consistency
of multiple views while preserving their features' diversity. Experiments on
various types of multi-view datasets show that SDMVC achieves state-of-the-art
performance.

    

### [[2104.04781] Boosted Embeddings for Time Series Forecasting](http://arxiv.org/abs/2104.04781)


  Time series forecasting is a fundamental task emerging from diverse
data-driven applications. Many advanced autoregressive methods such as ARIMA
were used to develop forecasting models. Recently, deep learning based methods
such as DeepAr, NeuralProphet, Seq2Seq have been explored for time series
forecasting problem. In this paper, we propose a novel time series forecast
model, DeepGB. We formulate and implement a variant of Gradient boosting
wherein the weak learners are DNNs whose weights are incrementally found in a
greedy manner over iterations. In particular, we develop a new embedding
architecture that improves the performance of many deep learning models on time
series using Gradient boosting variant. We demonstrate that our model
outperforms existing comparable state-of-the-art models using real-world sensor
data and public dataset.

    

### [[2104.05657] End-to-End Mandarin Tone Classification with Short Term Context Information](http://arxiv.org/abs/2104.05657)


  In this paper, we propose an end-to-end Mandarin tone classification method
from continuous speech utterances utilizing both the spectrogram and the
short-term context information as the input. Both spectrograms and context
segment features are used to train the tone classifier. We first divide the
spectrogram frames into syllable segments using force alignment results
produced by an ASR model. Then we extract the short-term segment features to
capture the context information across multiple syllables. Feeding both the
spectrogram and the short-term context segment features into an end-to-end
model could significantly improve the performance. Experiments are performed on
a large-scale open-source Mandarin speech dataset to evaluate the proposed
method. Results show that this method improves the classification accuracy from
79.5% to 92.6% on the AISHELL3 database.

    

### [[2104.07396] Node Co-occurrence based Dual Quaternion Graph Neural Networks for Knowledge Graph Link Prediction](http://arxiv.org/abs/2104.07396)


  We introduce a novel embedding model, named NoGE, which aims to integrate
co-occurrence among entities and relations into graph neural networks to
improve knowledge graph completion (i.e., link prediction). Given a knowledge
graph, NoGE constructs a single graph considering entities and relations as
individual nodes. NoGE then computes weights for edges among nodes based on the
co-occurrence of entities and relations. Next, NoGE proposes Dual Quaternion
Graph Neural Networks (Dual-QGNN) and utilizes Dual-QGNN to update vector
representations for entity and relation nodes. NoGE then adopts a score
function to produce the triple scores. Comprehensive experimental results show
that NoGE obtains state-of-the-art results on three new and difficult benchmark
datasets CoDEx for knowledge graph completion.

    

### [[2104.09792] Identifying Helpful Sentences in Product Reviews](http://arxiv.org/abs/2104.09792)


  In recent years online shopping has gained momentum and became an important
venue for customers wishing to save time and simplify their shopping process. A
key advantage of shopping online is the ability to read what other customers
are saying about products of interest. In this work, we aim to maintain this
advantage in situations where extreme brevity is needed, for example, when
shopping by voice. We suggest a novel task of extracting a single
representative helpful sentence from a set of reviews for a given product. The
selected sentence should meet two conditions: first, it should be helpful for a
purchase decision and second, the opinion it expresses should be supported by
multiple reviewers. This task is closely related to the task of Multi Document
Summarization in the product reviews domain but differs in its objective and
its level of conciseness. We collect a dataset in English of sentence
helpfulness scores via crowd-sourcing and demonstrate its reliability despite
the inherent subjectivity involved. Next, we describe a complete model that
extracts representative helpful sentences with positive and negative sentiment
towards the product and demonstrate that it outperforms several baselines.

    

### [[2104.09937] Gradient Matching for Domain Generalization](http://arxiv.org/abs/2104.09937)


  Machine learning systems typically assume that the distributions of training
and test sets match closely. However, a critical requirement of such systems in
the real world is their ability to generalize to unseen domains. Here, we
propose an inter-domain gradient matching objective that targets domain
generalization by maximizing the inner product between gradients from different
domains. Since direct optimization of the gradient inner product can be
computationally prohibitive -- requires computation of second-order derivatives
-- we derive a simpler first-order algorithm named Fish that approximates its
optimization. We demonstrate the efficacy of Fish on 6 datasets from the Wilds
benchmark, which captures distribution shift across a diverse range of
modalities. Our method produces competitive results on these datasets and
surpasses all baselines on 4 of them. We perform experiments on both the Wilds
benchmark, which captures distribution shift in the real world, as well as
datasets in DomainBed benchmark that focuses more on synthetic-to-real
transfer. Our method produces competitive results on both benchmarks,
demonstrating its effectiveness across a wide range of domain generalization
tasks.

    

### [[2104.10683] Explainable artificial intelligence for mechanics: physics-informing neural networks for constitutive models](http://arxiv.org/abs/2104.10683)


  (Artificial) neural networks have become increasingly popular in mechanics to
accelerate computations with model order reduction techniques and as universal
models for a wide variety of materials. However, the major disadvantage of
neural networks remains: their numerous parameters are challenging to interpret
and explain. Thus, neural networks are often labeled as black boxes, and their
results often elude human interpretation. In mechanics, the new and active
field of physics-informed neural networks attempts to mitigate this
disadvantage by designing deep neural networks on the basis of mechanical
knowledge. By using this a priori knowledge, deeper and more complex neural
networks became feasible, since the mechanical assumptions could be explained.
However, the internal reasoning and explanation of neural network parameters
remain mysterious.
Complementary to the physics-informed approach, we propose a first step
towards a physics-informing approach, which explains neural networks trained on
mechanical data a posteriori. This novel explainable artificial intelligence
approach aims at elucidating the black box of neural networks and their
high-dimensional representations. Therein, the principal component analysis
decorrelates the distributed representations in cell states of RNNs and allows
the comparison to known and fundamental functions. The novel approach is
supported by a systematic hyperparameter search strategy that identifies the
best neural network architectures and training parameters. The findings of
three case studies on fundamental constitutive models (hyperelasticity,
elastoplasticity, and viscoelasticity) imply that the proposed strategy can
help identify numerical and analytical closed-form solutions to characterize
new materials.

    

### [[2104.12284] Accuracy Improvement for Fully Convolutional Networks via Selective Augmentation with Applications to Electrocardiogram Data](http://arxiv.org/abs/2104.12284)


  Deep learning methods have shown suitability for time series classification
in the health and medical domain, with promising results for electrocardiogram
data classification. Successful identification of myocardial infarction holds
life saving potential and any meaningful improvement upon deep learning models
in this area is of great interest. Conventionally, data augmentation methods
are applied universally to the training set when data are limited in order to
ameliorate data resolution or sample size. In the method proposed in this
study, data augmentation was not applied in the context of data scarcity.
Instead, samples that yield low confidence predictions were selectively
augmented in order to bolster the model's sensitivity to features or patterns
less strongly associated with a given class. This approach was tested for
improving the performance of a Fully Convolutional Network. The proposed
approach achieved 90 percent accuracy for classifying myocardial infarction as
opposed to 82 percent accuracy for the baseline, a marked improvement. Further,
the accuracy of the proposed approach was optimal near a defined upper
threshold for qualifying low confidence samples and decreased as this threshold
was raised to include higher confidence samples. This suggests exclusively
selecting lower confidence samples for data augmentation comes with distinct
benefits for electrocardiogram data classification with Fully Convolutional
Networks.

    

### [[2104.14281] Leveraging Online Shopping Behaviors as a Proxy for Personal Lifestyle Choices: New Insights into Chronic Disease Prevention Literacy](http://arxiv.org/abs/2104.14281)


  Ubiquitous internet access is reshaping the way we live, but it is
accompanied by unprecedented challenges in preventing chronic diseases planted
by long exposure to unhealthy lifestyles. This paper proposes leveraging online
shopping behaviors as a proxy for personal lifestyle choices to improve chronic
disease prevention literacy targeted for times when e-commerce user experience
has been assimilated into most people's daily lives. Here, retrospective
longitudinal query logs and purchase records from millions of online shoppers
were accessed, constructing a broad spectrum of lifestyle features covering
assorted product categories and buyer personas. Using the lifestyle-related
information preceding their first purchases of prescription drugs, we could
determine associations between online shoppers' past lifestyle choices and
whether they suffered from a particular chronic disease or not. Novel lifestyle
risk factors were discovered in two exemplars -- depression and diabetes, most
of which showed cognitive congruence with existing healthcare knowledge.
Further, such empirical findings could be adopted to locate online shoppers at
high risk of these chronic diseases with fair accuracy, closely matching the
performance of screening surveys benchmarked against medical diagnosis.
Unobtrusive chronic disease surveillance via e-commerce sites may soon meet
consenting individuals in the digital space they already inhabit.

    

### [[2104.14435] Customizable Reference Runtime Monitoring of Neural Networks using Resolution Boxes](http://arxiv.org/abs/2104.14435)


  Classification neural networks fail to detect inputs that do not fall inside
the classes they have been trained for. Runtime monitoring techniques on the
neuron activation pattern can be used to detect such inputs. We present an
approach for monitoring classification systems via data abstraction. Data
abstraction relies on the notion of box with a resolution. Box-based
abstraction consists in representing a set of values by its minimal and maximal
values in each dimension. We augment boxes with a notion of resolution and
define their clustering coverage, which is intuitively a quantitative metric
that indicates the abstraction quality. This allows studying the effect of
different clustering parameters on the constructed boxes and estimating an
interval of sub-optimal parameters. Moreover, we automatically construct
monitors that leverage both the correct and incorrect behaviors of a system.
This allows checking the size of the monitor abstractions and analyzing the
separability of the network. Monitors are obtained by combining the
sub-monitors of each class of the system placed at some selected layers. Our
experiments demonstrate the effectiveness of our clustering coverage estimation
and show how to assess the effectiveness and precision of monitors according to
the selected clustering parameter and monitored layers.

    

### [[2105.05155] TAG: Task-based Accumulated Gradients for Lifelong learning](http://arxiv.org/abs/2105.05155)


  When an agent encounters a continual stream of new tasks in the lifelong
learning setting, it leverages the knowledge it gained from the earlier tasks
to help learn the new tasks better. In such a scenario, identifying an
efficient knowledge representation becomes a challenging problem. Most research
works propose to either store a subset of examples from the past tasks in a
replay buffer, dedicate a separate set of parameters to each task or penalize
excessive updates over parameters by introducing a regularization term. While
existing methods employ the general task-agnostic stochastic gradient descent
update rule, we propose a task-aware optimizer that adapts the learning rate
based on the relatedness among tasks. We utilize the directions taken by the
parameters during the updates by accumulating the gradients specific to each
task. These task-based accumulated gradients act as a knowledge base that is
maintained and updated throughout the stream. We empirically show that our
proposed adaptive learning rate not only accounts for catastrophic forgetting
but also allows positive backward transfer. We also show that our method
performs better than several state-of-the-art methods in lifelong learning on
complex datasets with a large number of tasks.

    

### [[2105.09540] Fed-EINI: An Efficient and Interpretable Inference Framework for Decision Tree Ensembles in Federated Learning](http://arxiv.org/abs/2105.09540)


  The increasing concerns about data privacy and security drive an emerging
field of studying privacy-preserving machine learning from isolated data
sources, i.e., federated learning. A class of federated learning, vertical
federated learning, where different parties hold different features for common
users, has a great potential of driving a more variety of business cooperation
among enterprises in many fields. In machine learning, decision tree ensembles
such as gradient boosting decision tree (GBDT) and random forest are widely
applied powerful models with high interpretability and modeling efficiency.
However, the interpretability is compromised in state-of-the-art vertical
federated learning frameworks such as SecureBoost with anonymous features to
avoid possible data breaches. To address this issue in the inference process,
in this paper, we propose Fed-EINI to protect data privacy and allow the
disclosure of feature meaning by concealing decision paths with a
communication-efficient secure computation method for inference outputs. The
advantages of Fed-EINI will be demonstrated through both theoretical analysis
and extensive numerical results.

    

### [[2105.11043] SleepTransformer: Automatic Sleep Staging with Interpretability and Uncertainty Quantification](http://arxiv.org/abs/2105.11043)


  Black-box skepticism is one of the main hindrances impeding
deep-learning-based automatic sleep scoring from being used in clinical
environments. Towards interpretability, this work proposes a
sequence-to-sequence sleep-staging model, namely SleepTransformer. It is based
on the transformer backbone whose self-attention scores offer interpretability
of the model's decisions at both the epoch and sequence level. At the epoch
level, the attention scores can be encoded as a heat map to highlight
sleep-relevant features captured from the input EEG signal. At the sequence
level, the attention scores are visualized as the influence of different
neighboring epochs in an input sequence (i.e. the context) to recognition of a
target epoch, mimicking the way manual scoring is done by human experts. We
further propose a simple yet efficient method to quantify uncertainty in the
model's decisions. The method, which is based on entropy, can serve as a metric
for deferring low-confidence epochs to a human expert for further inspection.
Additionally, we demonstrate that the proposed SleepTransformer outperforms
existing methods at a lower computational cost and achieves state-of-the-art
performance on two experimental databases of different sizes.

    

### [[2105.11781] A unified framework based on graph consensus term for multi-view learning](http://arxiv.org/abs/2105.11781)


  In recent years, multi-view learning technologies for various applications
have attracted a surge of interest. Due to more compatible and complementary
information from multiple views, existing multi-view methods could achieve more
promising performance than conventional single-view methods in most situations.
However, there are still no sufficient researches on the unified framework in
existing multi-view works. Meanwhile, how to efficiently integrate multi-view
information is still full of challenges. In this paper, we propose a novel
multi-view learning framework, which aims to leverage most existing graph
embedding works into a unified formula via introducing the graph consensus
term. In particular, our method explores the graph structure in each view
independently to preserve the diversity property of graph embedding methods.
Meanwhile, we choose heterogeneous graphs to construct the graph consensus term
to explore the correlations among multiple views jointly. To this end, the
diversity and complementary information among different views could be
simultaneously considered. Furthermore, the proposed framework is utilized to
implement the multi-view extension of Locality Linear Embedding, named
Multi-view Locality Linear Embedding (MvLLE), which could be efficiently solved
by applying the alternating optimization strategy. Empirical validations
conducted on six benchmark datasets can show the effectiveness of our proposed
method.

    

### [[2105.14409] A Matrix Autoencoder Framework to Align the Functional and Structural Connectivity Manifolds as Guided by Behavioral Phenotypes](http://arxiv.org/abs/2105.14409)


  We propose a novel matrix autoencoder to map functional connectomes from
resting state fMRI (rs-fMRI) to structural connectomes from Diffusion Tensor
Imaging (DTI), as guided by subject-level phenotypic measures. Our specialized
autoencoder infers a low dimensional manifold embedding for the rs-fMRI
correlation matrices that mimics a canonical outer-product decomposition. The
embedding is simultaneously used to reconstruct DTI tractography matrices via a
second manifold alignment decoder and to predict inter-subject phenotypic
variability via an artificial neural network. We validate our framework on a
dataset of 275 healthy individuals from the Human Connectome Project database
and on a second clinical dataset consisting of 57 subjects with Autism Spectrum
Disorder. We demonstrate that the model reliably recovers structural
connectivity patterns across individuals, while robustly extracting predictive
and interpretable brain biomarkers in a cross-validated setting. Finally, our
framework outperforms several baselines at predicting behavioral phenotypes in
both real-world datasets.

    

### [[2106.06845] Harmonization with Flow-based Causal Inference](http://arxiv.org/abs/2106.06845)


  Heterogeneity in medical data, e.g., from data collected at different sites
and with different protocols in a clinical study, is a fundamental hurdle for
accurate prediction using machine learning models, as such models often fail to
generalize well. This paper leverages a recently proposed
normalizing-flow-based method to perform counterfactual inference upon a
structural causal model (SCM), in order to achieve harmonization of such data.
A causal model is used to model observed effects (brain magnetic resonance
imaging data) that result from known confounders (site, gender and age) and
exogenous noise variables. Our formulation exploits the bijection induced by
flow for the purpose of harmonization. We infer the posterior of exogenous
variables, intervene on observations, and draw samples from the resultant SCM
to obtain counterfactuals. This approach is evaluated extensively on multiple,
large, real-world medical datasets and displayed better cross-domain
generalization compared to state-of-the-art algorithms. Further experiments
that evaluate the quality of confounder-independent data generated by our model
using regression and classification tasks are provided.

    

### [[2106.06885] Online Learning with Optimism and Delay](http://arxiv.org/abs/2106.06885)


  Inspired by the demands of real-time climate and weather forecasting, we
develop optimistic online learning algorithms that require no parameter tuning
and have optimal regret guarantees under delayed feedback. Our algorithms --
DORM, DORM+, and AdaHedgeD -- arise from a novel reduction of delayed online
learning to optimistic online learning that reveals how optimistic hints can
mitigate the regret penalty caused by delay. We pair this delay-as-optimism
perspective with a new analysis of optimistic learning that exposes its
robustness to hinting errors and a new meta-algorithm for learning effective
hinting strategies in the presence of delay. We conclude by benchmarking our
algorithms on four subseasonal climate forecasting tasks, demonstrating low
regret relative to state-of-the-art forecasting models.

    

### [[2106.06896] Hyperspectral and Multispectral Classification for Coastal Wetland Using Depthwise Feature Interaction Network](http://arxiv.org/abs/2106.06896)


  The monitoring of coastal wetlands is of great importance to the protection
of marine and terrestrial ecosystems. However, due to the complex environment,
severe vegetation mixture, and difficulty of access, it is impossible to
accurately classify coastal wetlands and identify their species with
traditional classifiers. Despite the integration of multisource remote sensing
data for performance enhancement, there are still challenges with acquiring and
exploiting the complementary merits from multisource data. In this paper, the
Deepwise Feature Interaction Network (DFINet) is proposed for wetland
classification. A depthwise cross attention module is designed to extract
self-correlation and cross-correlation from multisource feature pairs. In this
way, meaningful complementary information is emphasized for classification.
DFINet is optimized by coordinating consistency loss, discrimination loss, and
classification loss. Accordingly, DFINet reaches the standard solution-space
under the regularity of loss functions, while the spatial consistency and
feature discrimination are preserved. Comprehensive experimental results on two
hyperspectral and multispectral wetland datasets demonstrate that the proposed
DFINet outperforms other competitive methods in terms of overall accuracy.

    

### [[2106.06989] The DEformer: An Order-Agnostic Distribution Estimating Transformer](http://arxiv.org/abs/2106.06989)


  Order-agnostic autoregressive distribution (density) estimation (OADE), i.e.,
autoregressive distribution estimation where the features can occur in an
arbitrary order, is a challenging problem in generative machine learning. Prior
work on OADE has encoded feature identity by assigning each feature to a
distinct fixed position in an input vector. As a result, architectures built
for these inputs must strategically mask either the input or model weights to
learn the various conditional distributions necessary for inferring the full
joint distribution of the dataset in an order-agnostic way. In this paper, we
propose an alternative approach for encoding feature identities, where each
feature's identity is included alongside its value in the input. This feature
identity encoding strategy allows neural architectures designed for sequential
data to be applied to the OADE task without modification. As a proof of
concept, we show that a Transformer trained on this input (which we refer to as
"the DEformer", i.e., the distribution estimating Transformer) can effectively
model binarized-MNIST, approaching the performance of fixed-order
autoregressive distribution estimating algorithms while still being entirely
order-agnostic. Additionally, we find that the DEformer surpasses the
performance of recent flow-based architectures when modeling a tabular dataset.

    

### [[2106.07533] Cold Posteriors Improve Bayesian Medical Image Post-Processing](http://arxiv.org/abs/2106.07533)


  Cold posteriors have been reported to perform better in practice in the
context of Bayesian deep learning (Wenzel et al., 2020). In variational
inference, it is common to employ only a partially tempered posterior by
scaling the complexity term in the log-evidence lower bound (ELBO). In this
work, we optimize the ELBO for a fully tempered posterior in mean-field
variational inference and use Bayesian optimization to automatically find the
optimal posterior temperature and prior scale. Choosing an appropriate
posterior temperature leads to better predictive performance and improved
uncertainty calibration, which we demonstrate for the task of denoising medical
X-ray images.

    

### [[2106.08318] Gradient Forward-Propagation for Large-Scale Temporal Video Modelling](http://arxiv.org/abs/2106.08318)


  How can neural networks be trained on large-volume temporal data efficiently?
To compute the gradients required to update parameters, backpropagation blocks
computations until the forward and backward passes are completed. For temporal
signals, this introduces high latency and hinders real-time learning. It also
creates a coupling between consecutive layers, which limits model parallelism
and increases memory consumption. In this paper, we build upon Sideways, which
avoids blocking by propagating approximate gradients forward in time, and we
propose mechanisms for temporal integration of information based on different
variants of skip connections. We also show how to decouple computation and
delegate individual neural modules to different devices, allowing distributed
and parallel training. The proposed Skip-Sideways achieves low latency
training, model parallelism, and, importantly, is capable of extracting
temporal features, leading to more stable training and improved performance on
real-world action recognition video datasets such as HMDB51, UCF101, and the
large-scale Kinetics-600. Finally, we also show that models trained with
Skip-Sideways generate better future frames than Sideways models, and hence
they can better utilize motion cues.

    

### [[2106.13008] Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting](http://arxiv.org/abs/2106.13008)


  Extending the forecasting time is a critical demand for real applications,
such as extreme weather early warning and long-term energy consumption
planning. This paper studies the \textit{long-term forecasting} problem of time
series. Prior Transformer-based models adopt various self-attention mechanisms
to discover the long-range dependencies. However, intricate temporal patterns
of the long-term future prohibit the model from finding reliable dependencies.
Also, Transformers have to adopt the sparse versions of point-wise
self-attentions for long series efficiency, resulting in the information
utilization bottleneck. Towards these challenges, we propose Autoformer as a
novel decomposition architecture with an Auto-Correlation mechanism. We go
beyond the pre-processing convention of series decomposition and renovate it as
a basic inner block of deep models. This design empowers Autoformer with
progressive decomposition capacities for complex time series. Further, inspired
by the stochastic process theory, we design the Auto-Correlation mechanism
based on the series periodicity, which conducts the dependencies discovery and
representation aggregation at the sub-series level. Auto-Correlation
outperforms self-attention in both efficiency and accuracy. In long-term
forecasting, Autoformer yields state-of-the-art accuracy, with a 38% relative
improvement on six benchmarks, covering five practical applications: energy,
traffic, economics, weather and disease.

    

### [[2106.13238] Hate Speech Detection in Clubhouse](http://arxiv.org/abs/2106.13238)


  With the rise of voice chat rooms, a gigantic resource of data can be exposed
to the research community for natural language processing tasks. Moderators in
voice chat rooms actively monitor the discussions and remove the participants
with offensive language. However, it makes the hate speech detection even more
difficult since some participants try to find creative ways to articulate hate
speech. This makes the hate speech detection challenging in new social media
like Clubhouse. To the best of our knowledge all the hate speech datasets have
been collected from text resources like Twitter. In this paper, we take the
first step to collect a significant dataset from Clubhouse as the rising star
in social media industry. We analyze the collected instances from statistical
point of view using the Google Perspective Scores. Our experiments show that,
the Perspective Scores can outperform Bag of Words and Word2Vec as high level
text features.

    

### [[2106.13786] Data efficiency in graph networks through equivariance](http://arxiv.org/abs/2106.13786)


  We introduce a novel architecture for graph networks which is equivariant to
any transformation in the coordinate embeddings that preserves the distance
between neighbouring nodes. In particular, it is equivariant to the Euclidean
and conformal orthogonal groups in $n$-dimensions. Thanks to its equivariance
properties, the proposed model is extremely more data efficient with respect to
classical graph architectures and also intrinsically equipped with a better
inductive bias. We show that, learning on a minimal amount of data, the
architecture we propose can perfectly generalise to unseen data in a synthetic
problem, while much more training data are required from a standard model to
reach comparable performance.

    

### [[2106.13924] Self-Attentive Ensemble Transformer: Representing Ensemble Interactions in Neural Networks for Earth System Models](http://arxiv.org/abs/2106.13924)


  Ensemble data from Earth system models has to be calibrated and
post-processed. I propose a novel member-by-member post-processing approach
with neural networks. I bridge ideas from ensemble data assimilation with
self-attention, resulting into the self-attentive ensemble transformer. Here,
interactions between ensemble members are represented as additive and dynamic
self-attentive part. As proof-of-concept, I regress global ECMWF ensemble
forecasts to 2-metre-temperature fields from the ERA5 reanalysis. I demonstrate
that the ensemble transformer can calibrate the ensemble spread and extract
additional information from the ensemble. As it is a member-by-member approach,
the ensemble transformer directly outputs multivariate and spatially-coherent
ensemble members. Therefore, self-attention and the transformer technique can
be a missing piece for a non-parametric post-processing of ensemble data with
neural networks.

    

### [[2106.14251] Pairing Conceptual Modeling with Machine Learning](http://arxiv.org/abs/2106.14251)


  Both conceptual modeling and machine learning have long been recognized as
important areas of research. With the increasing emphasis on digitizing and
processing large amounts of data for business and other applications, it would
be helpful to consider how these areas of research can complement each other.
To understand how they can be paired, we provide an overview of machine
learning foundations and development cycle. We then examine how conceptual
modeling can be applied to machine learning and propose a framework for
incorporating conceptual modeling into data science projects. The framework is
illustrated by applying it to a healthcare application. For the inverse
pairing, machine learning can impact conceptual modeling through text and rule
mining, as well as knowledge graphs. The pairing of conceptual modeling and
machine learning in this this way should help lay the foundations for future
research.

    

### [[2106.14323] Use of Variational Inference in Music Emotion Recognition](http://arxiv.org/abs/2106.14323)


  This work was developed aiming to employ Statistical techniques to the field
of Music Emotion Recognition, a well-recognized area within the Signal
Processing world, but hardly explored from the statistical point of view. Here,
we opened several possibilities within the field, applying modern Bayesian
Statistics techniques and developing efficient algorithms, focusing on the
applicability of the results obtained. Although the motivation for this project
was the development of a emotion-based music recommendation system, its main
contribution is a highly adaptable multivariate model that can be useful
interpreting any database where there is an interest in applying regularization
in an efficient manner. Broadly speaking, we will explore what role a sound
theoretical statistical analysis can play in the modeling of an algorithm that
is able to understand a well-known database and what can be gained with this
kind of approach.

    

### [[2106.14473] Error analysis for physics informed neural networks (PINNs) approximating Kolmogorov PDEs](http://arxiv.org/abs/2106.14473)


  Physics informed neural networks approximate solutions of PDEs by minimizing
pointwise residuals. We derive rigorous bounds on the error, incurred by PINNs
in approximating the solutions of a large class of linear parabolic PDEs,
namely Kolmogorov equations that include the heat equation and Black-Scholes
equation of option pricing, as examples. We construct neural networks, whose
PINN residual (generalization error) can be made as small as desired. We also
prove that the total $L^2$-error can be bounded by the generalization error,
which in turn is bounded in terms of the training error, provided that a
sufficient number of randomly chosen training (collocation) points is used.
Moreover, we prove that the size of the PINNs and the number of training
samples only grow polynomially with the underlying dimension, enabling PINNs to
overcome the curse of dimensionality in this context. These results enable us
to provide a comprehensive error analysis for PINNs in approximating Kolmogorov
PDEs.

    

### [[2006.16535] A Case for Reversible Coherence Protocol](http://arxiv.org/abs/2006.16535)


  We propose the first Reversible Coherence Protocol (RCP), a new protocol
designed from ground up that enables invisible speculative load. RCP takes a
bold approach by including the speculative loads and merge/purge operation in
the interface between processor and cache coherence, and allowing them to
participate in the coherence protocol. It means, speculative load, ordinary
load/store, and merge/purge can all affect the state of a given cache line. RCP
is the first coherence protocol that enables the commit and squash of the
speculative load among distributed cache components in a general memory
hierarchy. RCP incurs an average slowdown of (3.0%,8.3%,7.4%) on
(SPEC2006,SPEC2017,PARSEC), which is lower compared to (26.5%,12%,18.3%) in
InvisiSpec and (3.2%,9.4%,24.2%) in CleanupSpec. The coherence traffic overhead
is on average 46%, compared to 40% and 27% of InvisiSpec and CleanupSpec,
respectively. Even with higher traffic overhead (~46%), the performance
overhead of RCP is lower than InvisiSpec and comparable to CleanupSpec. It
reveals a key advantage of RCP: the coherence actions triggered by the merge
and purge operations are not in the critical path of the execution and can be
performed in the cache hierarchy concurrently with processor execution

    

### [[2107.04648] Efficient Real-Time Image Recognition Using Collaborative Swarm of UAVs and Convolutional Networks](http://arxiv.org/abs/2107.04648)


  Unmanned Aerial Vehicles (UAVs) have recently attracted significant attention
due to their outstanding ability to be used in different sectors and serve in
difficult and dangerous areas. Moreover, the advancements in computer vision
and artificial intelligence have increased the use of UAVs in various
applications and solutions, such as forest fires detection and borders
monitoring. However, using deep neural networks (DNNs) with UAVs introduces
several challenges of processing deeper networks and complex models, which
restricts their on-board computation. In this work, we present a strategy
aiming at distributing inference requests to a swarm of resource-constrained
UAVs that classifies captured images on-board and finds the minimum
decision-making latency. We formulate the model as an optimization problem that
minimizes the latency between acquiring images and making the final decisions.
The formulated optimization solution is an NP-hard problem. Hence it is not
adequate for online resource allocation. Therefore, we introduce an online
heuristic solution, namely DistInference, to find the layers placement strategy
that gives the best latency among the available UAVs. The proposed approach is
general enough to be used for different low decision-latency applications as
well as for all CNN types organized into the pipeline of layers (e.g., VGG) or
based on residual blocks (e.g., ResNet).

    

### [[2107.04748] Resilient Edge Service Placement and Workload Allocation under Uncertainty](http://arxiv.org/abs/2107.04748)


  In this paper, we study an optimal service placement and workload allocation
problem for a service provider (SP), who can procure resources from numerous
edge nodes to serve its users.The SP aims to improve the user experience while
minimizing its cost, considering various system uncertainties. To tackle this
challenging problem, we propose a novel resilience-aware edge service placement
and workload allocation model that jointly captures the uncertainties of
resource demand and node failures. The first-stage decisions include the
optimal service placement and resource procurement, while the optimal workload
reallocation is determined in the second stage after the uncertainties are
disclosed. The salient feature of the proposed model is that it produces a
placement and procurement solution that is robust against any possible
realization of the uncertainties. By leveraging the column-and-constraint
generation method, we introduce two iterative algorithms that can converge to
an exact optimal solution within a finite number of iterations. We further
suggest an affine decision rule approximation approach for solving large-scale
problem instances in a reasonable time. Extensive numerical results are shown
to demonstrate the advantages of the proposed model and solutions.

    

### [[2107.04885] Filling MIS Vertices by Myopic Luminous Robots](http://arxiv.org/abs/2107.04885)


  We present the problem of finding a maximal independent set (MIS) (named as
\emph{MIS Filling problem}) of an arbitrary connected graph having $n$ vertices
with luminous myopic mobile robots. The robots enter the graph one after
another from a particular vertex called the \emph{Door} and disperse along the
edges of the graph without collision to occupy vertices such that the set of
vertices occupied by the robots is a maximal independent set. We assume the
robots have knowledge only about the maximum degree of the graph, denoted by
$\Delta$.
In this paper, we explore two versions of the problem: the solution to the
first version, named as \emph{MIS Filling with Single Door}, works under an
asynchronous scheduler using robots with 3 hops of visibility range, $\Delta +
6$ number of colors and $O(\log \Delta)$ bits of persistent storage. The time
complexity is measured in terms of epochs and it can be solved in $O(n^2)$
epochs. An epoch is the smallest time interval in which each participating
robot gets activated and executes the algorithm at least once. For the second
version with $k~ ( > 1)$ \textit{Doors}, named as \emph{MIS Filling with
Multiple Doors}, the solution works under a semi-synchronous scheduler using
robots with 5 hops of visibility range, $\Delta + k + 6$ number of colors and
$O(\log (\Delta + k))$ bits of persistent storage. The problem with multiple
Doors can be solved in $O(n^2)$ epochs.

    

### [[2107.04904] Blockumulus: A Scalable Framework for Smart Contracts on the Cloud](http://arxiv.org/abs/2107.04904)


  Public blockchains have spurred the growing popularity of decentralized
transactions and smart contracts, but they exhibit limitations on the
transaction throughput, storage, and computation. To avoid transaction
gridlock, public blockchains impose large fees and per-block resource limits,
making it difficult to accommodate the ever-growing transaction demand.
Previous research endeavors to improve the scalability of blockchain through
various technologies, such as side-chaining, sharding, secured off-chain
computation, communication network optimizations, and efficient consensus
protocols. However, these approaches have not attained a widespread adoption
due to their inability in delivering a cloud-like performance, in terms of the
scalability in transaction throughput, storage, and compute capacity. In this
work, we determine that the major obstacle to public blockchain scalability is
their underlying unstructured P2P networks. We further show that a centralized
network can support the deployment of decentralized smart contracts. We propose
a novel approach for achieving scalable decentralization: instead of trying to
make blockchain scalable, we deliver decentralization to already scalable cloud
by using an Ethereum smart contract. We introduce Blockumulus, a framework that
can deploy decentralized cloud smart contract environments using a novel
technique called overlay consensus. Through experiments, we demonstrate that
Blockumulus is scalable in all three dimensions: computation, data storage, and
transaction throughput. Besides eliminating the current code execution and
storage restrictions, Blockumulus delivers a transaction latency between 2 and
5 seconds under normal load. Moreover, the stress test of our prototype reveals
the ability to execute 20,000 simultaneous transactions under 26 seconds, which
is on par with the average throughput of worldwide credit card transactions.

    

### [[2107.04947] On the Performance of Pipelined HotStuff](http://arxiv.org/abs/2107.04947)


  HotStuff is a state-of-the-art Byzantine fault-tolerant consensus protocol.
It can be pipelined to build large-scale blockchains. One of its variants
called LibraBFT is adopted in Facebook's Libra blockchain. Although it is well
known that pipelined HotStuff is secure against up to $1/3$ of Byzantine nodes,
its performance in terms of throughput and delay is still under-explored. In
this paper, we develop a multi-metric evaluation framework to quantitatively
analyze pipelined \mbox{HotStuff's performance} with respect to its chain
growth rate, chain quality, and latency. We then propose two attack strategies
and evaluate their effects on the performance of pipelined HotStuff. Our
analysis shows that the chain growth rate (resp, chain quality) of pipelined
HotStuff under our attacks can drop to as low as 4/9 (resp, 12/17) of that
without attacks when $1/3$ nodes are Byzantine. As another application, we use
our framework to evaluate certain engineering optimizations adopted by
LibraBFT. We find that these optimizations make the system more vulnerable to
our attacks than the original pipelined HotStuff. Finally, we provide two
countermeasures to thwart these attacks. We hope that our studies can shed
light on the rigorous understanding of the state-of-the-art pipelined HotStuff
protocol as well as its variants.

    

### [[1901.04620] Selfish Mining in Ethereum](http://arxiv.org/abs/1901.04620)


  As the second largest cryptocurrency by market capitalization and today's
biggest decentralized platform that runs smart contracts, Ethereum has received
much attention from both industry and academia. Nevertheless, there exist very
few studies about the security of its mining strategies, especially from the
selfish mining perspective. In this paper, we aim to fill this research gap by
analyzing selfish mining in Ethereum and understanding its potential threat.
First, we introduce a 2-dimensional Markov process to model the behavior of a
selfish mining strategy inspired by a Bitcoin mining strategy proposed by Eyal
and Sirer. Second, we derive the stationary distribution of our Markov model
and compute long-term average mining rewards. This allows us to determine the
threshold of computational power that makes selfish mining profitable in
Ethereum. We find that this threshold is lower than that in Bitcoin mining
(which is 25% as discovered by Eyal and Sirer), suggesting that Ethereum is
more vulnerable to selfish mining than Bitcoin.

    

### [[2010.08009] Study of Automatic Offloading Method in Mixed Offloading Destination Environment](http://arxiv.org/abs/2010.08009)


  In recent years, utilization of heterogeneous hardware other than small core
CPU such as GPU, FPGA or many core CPU is increasing. However, when using
heterogeneous hardware, barriers of technical skills such as OpenMP, CUDA and
OpenCL are high. Based on that, I have proposed environment-adaptive software
that enables automatic conversion, configuration, and high performance
operation of once written code, according to the hardware to be placed.
However, including existing technologies, there has been no research to
properly and automatically offload the mixed offloading destination environment
such as GPU, FPGA and many core CPU. In this paper, as a new element of
environment-adaptive software, I study a method for offloading applications
properly and automatically in the environment where the offloading destination
is mixed with GPU, FPGA and many core CPU. I evaluate the effectiveness of the
proposed method in multiple applications.

    

### [[2012.08545] Accelerated, Scalable and Reproducible AI-driven Gravitational Wave Detection](http://arxiv.org/abs/2012.08545)


  The development of reusable artificial intelligence (AI) models for wider use
and rigorous validation by the community promises to unlock new opportunities
in multi-messenger astrophysics. Here we develop a workflow that connects the
Data and Learning Hub for Science, a repository for publishing AI models, with
the Hardware Accelerated Learning (HAL) cluster, using funcX as a universal
distributed computing service. Using this workflow, an ensemble of four openly
available AI models can be run on HAL to process an entire month's worth
(August 2017) of advanced Laser Interferometer Gravitational-Wave Observatory
data in just seven minutes, identifying all four all four binary black hole
mergers previously identified in this dataset and reporting no
misclassifications. This approach combines advances in AI, distributed
computing, and scientific data infrastructure to open new pathways to conduct
reproducible, accelerated, data-driven discovery.

    

### [[2105.12739] Task inefficiency patterns for a wave equation solver](http://arxiv.org/abs/2105.12739)


  The orchestration of complex algorithms demands high levels of automation to
use modern hardware efficiently. Task-based programming with OpenMP 5.0 is a
prominent candidate to accomplish this goal. We study OpenMP 5.0's tasking in
the context of a wave equation solver (ExaHyPE) using three different
architectures and runtimes. We describe several task-scheduling flaws present
in currently available runtimes, demonstrate how they impact performance and
show how to work around them. Finally, we propose extensions to the OpenMP
standard.

    

### [[2107.04631] Ill-posed Surface Emissivity Retrieval from Multi-Geometry HyperspectralImages using a Hybrid Deep Neural Network](http://arxiv.org/abs/2107.04631)


  Atmospheric correction is a fundamental task in remote sensing because
observations are taken either of the atmosphere or looking through the
atmosphere. Atmospheric correction errors can significantly alter the spectral
signature of the observations, and lead to invalid classifications or target
detection. This is even more crucial when working with hyperspectral data,
where a precise measurement of spectral properties is required.
State-of-the-art physics-based atmospheric correction approaches require
extensive prior knowledge about sensor characteristics, collection geometry,
and environmental characteristics of the scene being collected. These
approaches are computationally expensive, prone to inaccuracy due to lack of
sufficient environmental and collection information, and often impossible for
real-time applications. In this paper, a geometry-dependent hybrid neural
network is proposed for automatic atmospheric correction using multi-scan
hyperspectral data collected from different geometries. The proposed network
can characterize the atmosphere without any additional meteorological data. A
grid-search method is also proposed to solve the temperature emissivity
separation problem. Results show that the proposed network has the capacity to
accurately characterize the atmosphere and estimate target emissivity spectra
with a Mean Absolute Error (MAE) under 0.02 for 29 different materials. This
solution can lead to accurate atmospheric correction to improve target
detection for real time applications.

    

### [[2107.04632] Algorithmic Causal Effect Identification with causaleffect](http://arxiv.org/abs/2107.04632)


  Our evolution as a species made a huge step forward when we understood the
relationships between causes and effects. These associations may be trivial for
some events, but they are not in complex scenarios. To rigorously prove that
some occurrences are caused by others, causal theory and causal inference were
formalized, introducing the $do$-operator and its associated rules. The main
goal of this report is to review and implement in Python some algorithms to
compute conditional and non-conditional causal queries from observational data.
To this end, we first present some basic background knowledge on probability
and graph theory, before introducing important results on causal theory, used
in the construction of the algorithms. We then thoroughly study the
identification algorithms presented by Shpitser and Pearl in 2006, explaining
our implementation in Python alongside. The main identification algorithm can
be seen as a repeated application of the rules of $do$-calculus, and it
eventually either returns an expression for the causal query from experimental
probabilities or fails to identify the causal effect, in which case the effect
is non-identifiable. We introduce our newly developed Python library and give
some usage examples.

    

### [[2107.04635] Playing Angry Birds with a Domain-Independent PDDL+ Planner](http://arxiv.org/abs/2107.04635)


  This demo paper presents the first system for playing the popular Angry Birds
game using a domain-independent planner. Our system models Angry Birds levels
using PDDL+, a planning language for mixed discrete/continuous domains. It uses
a domain-independent PDDL+ planner to generate plans and executes them. In this
demo paper, we present the system's PDDL+ model for this domain, identify key
design decisions that reduce the problem complexity, and compare the
performance of our system to model-specific methods for this domain. The
results show that our system's performance is on par with other domain-specific
systems for Angry Birds, suggesting the applicability of domain-independent
planning to this benchmark AI challenge.

    

### [[2107.04708] Lifelong Twin Generative Adversarial Networks](http://arxiv.org/abs/2107.04708)


  In this paper, we propose a new continuously learning generative model,
called the Lifelong Twin Generative Adversarial Networks (LT-GANs). LT-GANs
learns a sequence of tasks from several databases and its architecture consists
of three components: two identical generators, namely the Teacher and
Assistant, and one Discriminator. In order to allow for the LT-GANs to learn
new concepts without forgetting, we introduce a new lifelong training approach,
namely Lifelong Adversarial Knowledge Distillation (LAKD), which encourages the
Teacher and Assistant to alternately teach each other, while learning a new
database. This training approach favours transferring knowledge from a more
knowledgeable player to another player which knows less information about a
previously given task.

    

### [[2107.04767] Anomaly Detection in Residential Video Surveillance on Edge Devices in IoT Framework](http://arxiv.org/abs/2107.04767)


  Intelligent resident surveillance is one of the most essential smart
community services. The increasing demand for security needs surveillance
systems to be able to detect anomalies in surveillance scenes. Employing
high-capacity computational devices for intelligent surveillance in residential
societies is costly and not feasible. Therefore, we propose anomaly detection
for intelligent surveillance using CPU-only edge devices. A modular framework
to capture object-level inferences and tracking is developed. To cope with
partial occlusions, posture deformations, and complex scenes we employed
feature encoding and trajectory associations. Elements of the anomaly detection
framework are optimized to run on CPU-only edge devices with sufficient FPS.
The experimental results indicate the proposed method is feasible and achieves
satisfactory results in real-life scenarios.

    

### [[2107.04768] DualVGR: A Dual-Visual Graph Reasoning Unit for Video Question Answering](http://arxiv.org/abs/2107.04768)


  Video question answering is a challenging task, which requires agents to be
able to understand rich video contents and perform spatial-temporal reasoning.
However, existing graph-based methods fail to perform multi-step reasoning
well, neglecting two properties of VideoQA: (1) Even for the same video,
different questions may require different amount of video clips or objects to
infer the answer with relational reasoning; (2) During reasoning, appearance
and motion features have complicated interdependence which are correlated and
complementary to each other. Based on these observations, we propose a
Dual-Visual Graph Reasoning Unit (DualVGR) which reasons over videos in an
end-to-end fashion. The first contribution of our DualVGR is the design of an
explainable Query Punishment Module, which can filter out irrelevant visual
features through multiple cycles of reasoning. The second contribution is the
proposed Video-based Multi-view Graph Attention Network, which captures the
relations between appearance and motion features. Our DualVGR network achieves
state-of-the-art performance on the benchmark MSVD-QA and SVQA datasets, and
demonstrates competitive results on benchmark MSRVTT-QA datasets. Our code is
available at this https URL.

    

### [[2107.04771] Similar Cases Recommendation using Legal Knowledge Graphs](http://arxiv.org/abs/2107.04771)


  A legal knowledge graph constructed from court cases, judgments, laws and
other legal documents can enable a number of applications like question
answering, document similarity, and search. While the use of knowledge graphs
for distant supervision in NLP tasks is well researched, using knowledge graphs
for downstream graph tasks like node similarity presents challenges in
selecting node types and their features. In this demo, we describe our solution
for predicting similar nodes in a case graph derived from our legal knowledge
graph.

    

### [[2107.04781] Formal context reduction in deriving concept hierarchies from corpora using adaptive evolutionary clustering algorithm star](http://arxiv.org/abs/2107.04781)


  It is beneficial to automate the process of deriving concept hierarchies from
corpora since a manual construction of concept hierarchies is typically a
time-consuming and resource-intensive process. As such, the overall process of
learning concept hierarchies from corpora encompasses a set of steps: parsing
the text into sentences, splitting the sentences and then tokenising it. After
the lemmatisation step, the pairs are extracted using FCA. However, there might
be some uninteresting and erroneous pairs in the formal context. Generating
formal context may lead to a time-consuming process, so formal context size
reduction is required to remove uninterested and erroneous pairs, taking less
time to extract the concept lattice and concept hierarchies accordingly. In
this premise, this study aims to propose two frameworks: (1) A framework to
review the current process of deriving concept hierarchies from corpus
utilising FCA; (2) A framework to decrease the formal contexts ambiguity of the
first framework using an adaptive version of ECA*. Experiments are conducted by
applying 385 sample corpora from Wikipedia on the two frameworks to examine the
reducing size of formal context, which leads to yield concept lattice and
concept hierarchy. The resulting lattice of formal context is evaluated to the
standard one using concept lattice-invariants. Accordingly, the homomorphic
between the two lattices preserves the quality of resulting concept hierarchies
by 89% in contrast to the basic ones, and the reduced concept lattice inherits
the structural relation of the standard one. The adaptive ECA* is examined
against its four counterpart baseline algorithms to measure the execution time
on random datasets with different densities (fill ratios). The results show
that adaptive ECA* performs concept lattice faster than other mentioned
competitive techniques in different fill ratios.

    

### [[2107.04806] Speech2Video: Cross-Modal Distillation for Speech to Video Generation](http://arxiv.org/abs/2107.04806)


  This paper investigates a novel task of talking face video generation solely
from speeches. The speech-to-video generation technique can spark interesting
applications in entertainment, customer service, and human-computer-interaction
industries. Indeed, the timbre, accent and speed in speeches could contain rich
information relevant to speakers' appearance. The challenge mainly lies in
disentangling the distinct visual attributes from audio signals. In this
article, we propose a light-weight, cross-modal distillation method to extract
disentangled emotional and identity information from unlabelled video inputs.
The extracted features are then integrated by a generative adversarial network
into talking face video clips. With carefully crafted discriminators, the
proposed framework achieves realistic generation results. Experiments with
observed individuals demonstrated that the proposed framework captures the
emotional expressions solely from speeches, and produces spontaneous facial
motion in the video output. Compared to the baseline method where speeches are
combined with a static image of the speaker, the results of the proposed
framework is almost indistinguishable. User studies also show that the proposed
method outperforms the existing algorithms in terms of emotion expression in
the generated videos.

    

### [[2107.04810] Not End-to-End: Explore Multi-Stage Architecture for Online Surgical Phase Recognition](http://arxiv.org/abs/2107.04810)


  Surgical phase recognition is of particular interest to computer assisted
surgery systems, in which the goal is to predict what phase is occurring at
each frame for a surgery video. Networks with multi-stage architecture have
been widely applied in many computer vision tasks with rich patterns, where a
predictor stage first outputs initial predictions and an additional refinement
stage operates on the initial predictions to perform further refinement.
Existing works show that surgical video contents are well ordered and contain
rich temporal patterns, making the multi-stage architecture well suited for the
surgical phase recognition task. However, we observe that when simply applying
the multi-stage architecture to the surgical phase recognition task, the
end-to-end training manner will make the refinement ability fall short of its
wishes. To address the problem, we propose a new non end-to-end training
strategy and explore different designs of multi-stage architecture for surgical
phase recognition task. For the non end-to-end training strategy, the
refinement stage is trained separately with proposed two types of disturbed
sequences. Meanwhile, we evaluate three different choices of refinement models
to show that our analysis and solution are robust to the choices of specific
multi-stage models. We conduct experiments on two public benchmarks, the
M2CAI16 Workflow Challenge, and the Cholec80 dataset. Results show that
multi-stage architecture trained with our strategy largely boosts the
performance of the current state-of-the-art single-stage model. Code is
available at \url{this https URL}.

    

### [[2107.04851] Machine Learning for Financial Forecasting, Planning and Analysis: Recent Developments and Pitfalls](http://arxiv.org/abs/2107.04851)


  This article is an introduction to machine learning for financial
forecasting, planning and analysis (FP\&A). Machine learning appears well
suited to support FP\&A with the highly automated extraction of information
from large amounts of data. However, because most traditional machine learning
techniques focus on forecasting (prediction), we discuss the particular care
that must be taken to avoid the pitfalls of using them for planning and
resource allocation (causal inference). While the naive application of machine
learning usually fails in this context, the recently developed double machine
learning framework can address causal questions of interest. We review the
current literature on machine learning in FP\&A and illustrate in a simulation
study how machine learning can be used for both forecasting and planning. We
also investigate how forecasting and planning improve as the number of data
points increases.

    

### [[2107.04870] From Common Sense Reasoning to Neural Network Models through Multiple Preferences: an overview](http://arxiv.org/abs/2107.04870)


  In this paper we discuss the relationships between conditional and
preferential logics and neural network models, based on a multi-preferential
semantics. We propose a concept-wise multipreference semantics, recently
introduced for defeasible description logics to take into account preferences
with respect to different concepts, as a tool for providing a semantic
interpretation to neural network models. This approach has been explored both
for unsupervised neural network models (Self-Organising Maps) and for
supervised ones (Multilayer Perceptrons), and we expect that the same approach
might be extended to other neural network models. It allows for logical
properties of the network to be checked (by model checking) over an
interpretation capturing the input-output behavior of the network. For
Multilayer Perceptrons, the deep network itself can be regarded as a
conditional knowledge base, in which synaptic connections correspond to
weighted conditionals. The paper describes the general approach, through the
cases of Self-Organising Maps and Multilayer Perceptrons, and discusses some
open issues and perspectives.

    

### [[2107.04924] Distributed Deep Reinforcement Learning for Intelligent Traffic Monitoring with a Team of Aerial Robots](http://arxiv.org/abs/2107.04924)


  This paper studies the traffic monitoring problem in a road network using a
team of aerial robots. The problem is challenging due to two main reasons.
First, the traffic events are stochastic, both temporally and spatially.
Second, the problem has a non-homogeneous structure as the traffic events
arrive at different locations of the road network at different rates.
Accordingly, some locations require more visits by the robots compared to other
locations. To address these issues, we define an uncertainty metric for each
location of the road network and formulate a path planning problem for the
aerial robots to minimize the network's average uncertainty. We express this
problem as a partially observable Markov decision process (POMDP) and propose a
distributed and scalable algorithm based on deep reinforcement learning to
solve it. We consider two different scenarios depending on the communication
mode between the agents (aerial robots) and the traffic management center
(TMC). The first scenario assumes that the agents continuously communicate with
the TMC to send/receive real-time information about the traffic events. Hence,
the agents have global and real-time knowledge of the environment. However, in
the second scenario, we consider a challenging setting where the observation of
the aerial robots is partial and limited to their sensing ranges. Moreover, in
contrast to the first scenario, the information exchange between the aerial
robots and the TMC is restricted to specific time instances. We evaluate the
performance of our proposed algorithm in both scenarios for a real road network
topology and demonstrate its functionality in a traffic monitoring system.

    

### [[2107.04932] Aligning Correlation Information for Domain Adaptation in Action Recognition](http://arxiv.org/abs/2107.04932)


  Domain adaptation (DA) approaches address domain shift and enable networks to
be applied to different scenarios. Although various image DA approaches have
been proposed in recent years, there is limited research towards video DA. This
is partly due to the complexity in adapting the different modalities of
features in videos, which includes the correlation features extracted as
long-term dependencies of pixels across spatiotemporal dimensions. The
correlation features are highly associated with action classes and proven their
effectiveness in accurate video feature extraction through the supervised
action recognition task. Yet correlation features of the same action would
differ across domains due to domain shift. Therefore we propose a novel
Adversarial Correlation Adaptation Network (ACAN) to align action videos by
aligning pixel correlations. ACAN aims to minimize the distribution of
correlation information, termed as Pixel Correlation Discrepancy (PCD).
Additionally, video DA research is also limited by the lack of cross-domain
video datasets with larger domain shifts. We, therefore, introduce a novel
HMDB-ARID dataset with a larger domain shift caused by a larger statistical
difference between domains. This dataset is built in an effort to leverage
current datasets for dark video classification. Empirical results demonstrate
the state-of-the-art performance of our proposed ACAN for both existing and the
new video DA datasets.

    

### [[2107.04941] Partial Video Domain Adaptation with Partial Adversarial Temporal Attentive Network](http://arxiv.org/abs/2107.04941)


  Partial Domain Adaptation (PDA) is a practical and general domain adaptation
scenario, which relaxes the fully shared label space assumption such that the
source label space subsumes the target one. The key challenge of PDA is the
issue of negative transfer caused by source-only classes. For videos, such
negative transfer could be triggered by both spatial and temporal features,
which leads to a more challenging Partial Video Domain Adaptation (PVDA)
problem. In this paper, we propose a novel Partial Adversarial Temporal
Attentive Network (PATAN) to address the PVDA problem by utilizing both spatial
and temporal features for filtering source-only classes. Besides, PATAN
constructs effective overall temporal features by attending to local temporal
features that contribute more toward the class filtration process. We further
introduce new benchmarks to facilitate research on PVDA problems, covering a
wide range of PVDA scenarios. Empirical results demonstrate the
state-of-the-art performance of our proposed PATAN across the multiple PVDA
benchmarks.

    

### [[2107.04983] Leveraging Domain Adaptation for Low-Resource Geospatial Machine Learning](http://arxiv.org/abs/2107.04983)


  Machine learning in remote sensing has matured alongside a proliferation in
availability and resolution of geospatial imagery, but its utility is
bottlenecked by the need for labeled data. What's more, many labeled geospatial
datasets are specific to certain regions, instruments, or extreme weather
events. We investigate the application of modern domain-adaptation to multiple
proposed geospatial benchmarks, uncovering unique challenges and proposing
solutions to them.

    

### [[2107.04991] Prediction Surface Uncertainty Quantification in Object Detection Models for Autonomous Driving](http://arxiv.org/abs/2107.04991)


  Object detection in autonomous cars is commonly based on camera images and
Lidar inputs, which are often used to train prediction models such as deep
artificial neural networks for decision making for object recognition,
adjusting speed, etc. A mistake in such decision making can be damaging; thus,
it is vital to measure the reliability of decisions made by such prediction
models via uncertainty measurement. Uncertainty, in deep learning models, is
often measured for classification problems. However, deep learning models in
autonomous driving are often multi-output regression models. Hence, we propose
a novel method called PURE (Prediction sURface uncErtainty) for measuring
prediction uncertainty of such regression models. We formulate the object
recognition problem as a regression model with more than one outputs for
finding object locations in a 2-dimensional camera view. For evaluation, we
modified three widely-applied object recognition models (i.e., YoLo, SSD300 and
SSD512) and used the KITTI, Stanford Cars, Berkeley DeepDrive, and NEXET
datasets. Results showed the statistically significant negative correlation
between prediction surface uncertainty and prediction accuracy suggesting that
uncertainty significantly impacts the decisions made by autonomous driving.

    

### [[2107.05002] Improving Low-resource Reading Comprehension via Cross-lingual Transposition Rethinking](http://arxiv.org/abs/2107.05002)


  Extractive Reading Comprehension (ERC) has made tremendous advances enabled
by the availability of large-scale high-quality ERC training data. Despite of
such rapid progress and widespread application, the datasets in languages other
than high-resource languages such as English remain scarce. To address this
issue, we propose a Cross-Lingual Transposition ReThinking (XLTT) model by
modelling existing high-quality extractive reading comprehension datasets in a
multilingual environment. To be specific, we present multilingual adaptive
attention (MAA) to combine intra-attention and inter-attention to learn more
general generalizable semantic and lexical knowledge from each pair of language
families. Furthermore, to make full use of existing datasets, we adopt a new
training framework to train our model by calculating task-level similarities
between each existing dataset and target dataset. The experimental results show
that our XLTT model surpasses six baselines on two multilingual ERC benchmarks,
especially more effective for low-resource languages with 3.9 and 4.1 average
improvement in F1 and EM, respectively.

    

### [[2107.05031] Semi-Supervised Object Detection with Adaptive Class-Rebalancing Self-Training](http://arxiv.org/abs/2107.05031)


  This study delves into semi-supervised object detection (SSOD) to improve
detector performance with additional unlabeled data. State-of-the-art SSOD
performance has been achieved recently by self-training, in which training
supervision consists of ground truths and pseudo-labels. In current studies, we
observe that class imbalance in SSOD severely impedes the effectiveness of
self-training. To address the class imbalance, we propose adaptive
class-rebalancing self-training (ACRST) with a novel memory module called
CropBank. ACRST adaptively rebalances the training data with foreground
instances extracted from the CropBank, thereby alleviating the class imbalance.
Owing to the high complexity of detection tasks, we observe that both
self-training and data-rebalancing suffer from noisy pseudo-labels in SSOD.
Therefore, we propose a novel two-stage filtering algorithm to generate
accurate pseudo-labels. Our method achieves satisfactory improvements on
MS-COCO and VOC benchmarks. When using only 1\% labeled data in MS-COCO, our
method achieves 17.02 mAP improvement over supervised baselines, and 5.32 mAP
improvement compared with state-of-the-art methods.

    

### [[2107.05037] BCNet: A Deep Convolutional Neural Network for Breast Cancer Grading](http://arxiv.org/abs/2107.05037)


  Breast cancer has become one of the most prevalent cancers by which people
all over the world are affected and is posed serious threats to human beings,
in a particular woman. In order to provide effective treatment or prevention of
this cancer, disease diagnosis in the early stages would be of high importance.
There have been various methods to detect this disorder in which using images
have to play a dominant role. Deep learning has been recently adopted widely in
different areas of science, especially medicine. In breast cancer detection
problems, some diverse deep learning techniques have been developed on
different datasets and resulted in good accuracy. In this article, we aimed to
present a deep neural network model to classify histopathological images from
the Databiox image dataset as the first application on this image database. Our
proposed model named BCNet has taken advantage of the transfer learning
approach in which VGG16 is selected from available pertained models as a
feature extractor. Furthermore, to address the problem of insufficient data, we
employed the data augmentation technique to expand the input dataset. All
implementations in this research, ranging from pre-processing actions to
depicting the diagram of the model architecture, have been carried out using
tf.keras API. As a consequence of the proposed model execution, the significant
validation accuracy of 88% and evaluation accuracy of 72% obtained.

    

### [[2107.05073] Locality Relationship Constrained Multi-view Clustering Framework](http://arxiv.org/abs/2107.05073)


  In most practical applications, it's common to utilize multiple features from
different views to represent one object. Among these works, multi-view
subspace-based clustering has gained extensive attention from many researchers,
which aims to provide clustering solutions to multi-view data. However, most
existing methods fail to take full use of the locality geometric structure and
similarity relationship among samples under the multi-view scenario. To solve
these issues, we propose a novel multi-view learning method with locality
relationship constraint to explore the problem of multi-view clustering, called
Locality Relationship Constrained Multi-view Clustering Framework (LRC-MCF).
LRC-MCF aims to explore the diversity, geometric, consensus and complementary
information among different views, by capturing the locality relationship
information and the common similarity relationships among multiple views.
Moreover, LRC-MCF takes sufficient consideration to weights of different views
in finding the common-view locality structure and straightforwardly produce the
final clusters. To effectually reduce the redundancy of the learned
representations, the low-rank constraint on the common similarity matrix is
considered additionally. To solve the minimization problem of LRC-MCF, an
Alternating Direction Minimization (ADM) method is provided to iteratively
calculate all variables LRC-MCF. Extensive experimental results on seven
benchmark multi-view datasets validate the effectiveness of the LRC-MCF method.

    

### [[2107.05088] Fairer Software Made Easier (using "Keys")](http://arxiv.org/abs/2107.05088)


  Can we simplify explanations for software analytics? Maybe. Recent results
show that systems often exhibit a "keys effect"; i.e. a few key features
control the rest. Just to say the obvious, for systems controlled by a few
keys, explanation and control is just a matter of running a handful of
"what-if" queries across the keys. By exploiting the keys effect, it should be
possible to dramatically simplify even complex explanations, such as those
required for ethical AI systems.

    

### [[2107.05112] Repo2Vec: A Comprehensive Embedding Approach for Determining Repository Similarity](http://arxiv.org/abs/2107.05112)


  How can we identify similar repositories and clusters among a large online
archive, such as GitHub? Determiningrepository similarity is an essential
building block in studying the dynamics and the evolution of such software
ecosystems. The key challenge is to determine the right representation for the
diverse repository features in a way that: (a) it captures all aspects of the
available information, and (b) it is readily usable by MLalgorithms. We propose
Repo2Vec, a comprehensive embedding approach to represent a repository as a
distributed vector by combining features from three types of information
sources. As our key novelty, we consider three types of information:
(a)metadata, (b) the structure of the repository, and (c) the source code. We
also introduce a series of embedding approaches to represent and combine these
information types into a single embedding. We evaluate our method with two real
datasets from GitHub for a combined 1013 repositories. First, we show that our
method outperforms previous methods in terms of precision (93%vs 78%), with
nearly twice as many Strongly Similar repositories and 30% fewer False
Positives. Second, we show how Repo2Vecprovides a solid basis for: (a)
distinguishing between malware and benign repositories, and (b) identifying a
meaningful hierarchical clustering. For example, we achieve 98% precision and
96%recall in distinguishing malware and benign repositories. Overall, our work
is a fundamental building block for enabling many repository analysis functions
such as repository categorization by target platform or intention, detecting
code-reuse and clones, and identifying lineage and evolution.

    

### [[2107.05151] Document Embedding for Scientific Articles: Efficacy of Word Embeddings vs TFIDF](http://arxiv.org/abs/2107.05151)


  Over the last few years, neural network derived word embeddings became
popular in the natural language processing literature. Studies conducted have
mostly focused on the quality and application of word embeddings trained on
public available corpuses such as Wikipedia or other news and social media
sources. However, these studies are limited to generic text and thus lack
technical and scientific nuances such as domain specific vocabulary,
abbreviations, or scientific formulas which are commonly used in academic
context. This research focuses on the performance of word embeddings applied to
a large scale academic corpus. More specifically, we compare quality and
efficiency of trained word embeddings to TFIDF representations in modeling
content of scientific articles. We use a word2vec skip-gram model trained on
titles and abstracts of about 70 million scientific articles. Furthermore, we
have developed a benchmark to evaluate content models in a scientific context.
The benchmark is based on a categorization task that matches articles to
journals for about 1.3 million articles published in 2017. Our results show
that content models based on word embeddings are better for titles (short text)
while TFIDF works better for abstracts (longer text). However, the slight
improvement of TFIDF for larger text comes at the expense of 3.7 times more
memory requirement as well as up to 184 times higher computation times which
may make it inefficient for online applications. In addition, we have created a
2-dimensional visualization of the journals modeled via embeddings to
qualitatively inspect embedding model. This graph shows useful insights and can
be used to find competitive journals or gaps to propose new journals.

    

### [[2107.05172] Deep Transfer Learning Based Intrusion Detection System for Electric Vehicular Networks](http://arxiv.org/abs/2107.05172)


  The Controller Area Network (CAN) bus works as an important protocol in the
real-time In-Vehicle Network (IVN) systems for its simple, suitable, and robust
architecture. The risk of IVN devices has still been insecure and vulnerable
due to the complex data-intensive architectures which greatly increase the
accessibility to unauthorized networks and the possibility of various types of
cyberattacks. Therefore, the detection of cyberattacks in IVN devices has
become a growing interest. With the rapid development of IVNs and evolving
threat types, the traditional machine learning-based IDS has to update to cope
with the security requirements of the current environment. Nowadays, the
progression of deep learning, deep transfer learning, and its impactful outcome
in several areas has guided as an effective solution for network intrusion
detection. This manuscript proposes a deep transfer learning-based IDS model
for IVN along with improved performance in comparison to several other existing
models. The unique contributions include effective attribute selection which is
best suited to identify malicious CAN messages and accurately detect the normal
and abnormal activities, designing a deep transfer learning-based LeNet model,
and evaluating considering real-world data. To this end, an extensive
experimental performance evaluation has been conducted. The architecture along
with empirical analyses shows that the proposed IDS greatly improves the
detection accuracy over the mainstream machine learning, deep learning, and
benchmark deep transfer learning models and has demonstrated better performance
for real-time IVN security.

    

### [[2107.05249] Impact of Energy Efficiency on the Morphology and Behaviour of Evolved Robots](http://arxiv.org/abs/2107.05249)


  Most evolutionary robotics studies focus on evolving some targeted behavior
without taking the energy usage into account. This limits the practical value
of such systems because energy efficiency is an important property for
real-world autonomous robots. In this paper, we mitigate this problem by
extending our simulator with a battery model and taking energy consumption into
account during fitness evaluations. Using this system we investigate how energy
awareness affects the evolution of robots. Since our system is to evolve
morphologies as well as controllers, the main research question is twofold: (i)
what is the impact on the morphologies of the evolved robots, and (ii) what is
the impact on the behavior of the evolved robots if energy consumption is
included in the fitness evaluation? The results show that including the energy
consumption in the fitness in a multi-objective fashion (by NSGA-II) reduces
the average size of robot bodies while at the same time reducing their speed.
However, robots generated without size reduction can achieve speeds comparable
to robots from the baseline set.

    

### [[2107.05278] Constrained Sampling from a Kernel Density Estimator to Generate Scenarios for the Assessment of Automated Vehicles](http://arxiv.org/abs/2107.05278)


  The safety assessment of automated vehicles (AVs) is an important aspect of
the development cycle of AVs. A scenario-based assessment approach is accepted
by many players in the field as part of the complete safety assessment. A
scenario is a representation of a situation on the road to which the AV needs
to respond appropriately. One way to generate the required scenario-based test
descriptions is to parameterize the scenarios and to draw these parameters from
a probability density function (pdf). Because the shape of the pdf is unknown
beforehand, assuming a functional form of the pdf and fitting the parameters to
the data may lead to inaccurate fits. As an alternative, Kernel Density
Estimation (KDE) is a promising candidate for estimating the underlying pdf,
because it is flexible with the underlying distribution of the parameters.
Drawing random samples from a pdf estimated with KDE is possible without the
need of evaluating the actual pdf, which makes it suitable for drawing random
samples for, e.g., Monte Carlo methods. Sampling from a KDE while the samples
satisfy a linear equality constraint, however, has not been described in the
literature, as far as the authors know.
In this paper, we propose a method to sample from a pdf estimated using KDE,
such that the samples satisfy a linear equality constraint. We also present an
algorithm of our method in pseudo-code. The method can be used to generating
scenarios that have, e.g., a predetermined starting speed or to generate
different types of scenarios. This paper also shows that the method for
sampling scenarios can be used in case a Singular Value Decomposition (SVD) is
used to reduce the dimension of the parameter vectors.

    

### [[2107.05307] Real-Time Super-Resolution System of 4K-Video Based on Deep Learning](http://arxiv.org/abs/2107.05307)


  Video super-resolution (VSR) technology excels in reconstructing low-quality
video, avoiding unpleasant blur effect caused by interpolation-based
algorithms. However, vast computation complexity and memory occupation hampers
the edge of deplorability and the runtime inference in real-life applications,
especially for large-scale VSR task. This paper explores the possibility of
real-time VSR system and designs an efficient and generic VSR network, termed
EGVSR. The proposed EGVSR is based on spatio-temporal adversarial learning for
temporal coherence. In order to pursue faster VSR processing ability up to 4K
resolution, this paper tries to choose lightweight network structure and
efficient upsampling method to reduce the computation required by EGVSR network
under the guarantee of high visual quality. Besides, we implement the batch
normalization computation fusion, convolutional acceleration algorithm and
other neural network acceleration techniques on the actual hardware platform to
optimize the inference process of EGVSR network. Finally, our EGVSR achieves
the real-time processing capacity of 4K@29.61FPS. Compared with TecoGAN, the
most advanced VSR network at present, we achieve 85.04% reduction of
computation density and 7.92x performance speedups. In terms of visual quality,
the proposed EGVSR tops the list of most metrics (such as LPIPS, tOF, tLP,
etc.) on the public test dataset Vid4 and surpasses other state-of-the-art
methods in overall performance score. The source code of this project can be
found on this https URL.

    

### [[2107.05344] Post Triangular Rewiring Method for Shorter RRT Robot Path Planning](http://arxiv.org/abs/2107.05344)


  This paper proposed the 'Post Triangular Rewiring' method that minimizes the
sacrifice of planning time and overcomes the limit of Optimality of
sampling-based algorithm such as Rapidly-exploring Random Tree (RRT) algorithm.
The proposed 'Post Triangular Rewiring' method creates a closer to the optimal
path than RRT algorithm before application through the triangular inequality
principle. The experiments were conducted to verify a performance of the
proposed method. When the method proposed in this paper are applied to the RRT
algorithm, the Optimality efficiency increase compared to the planning time.

    

### [[2107.05346] SimDem A Multi-agent Simulation Environment to Model Persons with Dementia and their Assistance](http://arxiv.org/abs/2107.05346)


  Developing artificial intelligence based assistive systems to aid Persons
with Dementia (PwD) requires large amounts of training data. However, data
collection poses ethical, legal, economic, and logistic issues. Synthetic data
generation tools, in this regard, provide a potential solution. However, we
believe that already available such tools do not adequately reflect cognitive
deficiencies in behavior simulation. To counter these issues we propose a
simulation model (SimDem ) that primarily focuses on cognitive impairments
suffered by PwD and can be easily configured and adapted by the users to model
and evaluate assistive solutions.

    

### [[2107.05348] Zero-shot Visual Question Answering using Knowledge Graph](http://arxiv.org/abs/2107.05348)


  Incorporating external knowledge to Visual Question Answering (VQA) has
become a vital practical need. Existing methods mostly adopt pipeline
approaches with different components for knowledge matching and extraction,
feature learning, etc.However, such pipeline approaches suffer when some
component does not perform well, which leads to error propagation and poor
overall performance. Furthermore, the majority of existing approaches ignore
the answer bias issue -- many answers may have never appeared during training
(i.e., unseen answers) in real-word this http URL bridge these gaps, in this
paper, we propose a Zero-shot VQA algorithm using knowledge graphs and a
mask-based learning mechanism for better incorporating external knowledge, and
present new answer-based Zero-shot VQA splits for the F-VQA dataset.
Experiments show that our method can achieve state-of-the-art performance in
Zero-shot VQA with unseen answers, meanwhile dramatically augment existing
end-to-end models on the normal VQA task.

    

### [[2107.05363] Towards solving the 7-in-a-row game](http://arxiv.org/abs/2107.05363)


  Our paper explores the game theoretic value of the 7-in-a-row game. We reduce
the problem to solving a finite board game, which we target using Proof Number
Search. We present a number of heuristic improvements to Proof Number Search
and examine their effect within the context of this particular game. Although
our paper does not solve the 7-in-a-row game, our experiments indicate that we
have made significant progress towards it.

    

### [[2107.05366] HCGR: Hyperbolic Contrastive Graph Representation Learning for Session-based Recommendation](http://arxiv.org/abs/2107.05366)


  Session-based recommendation (SBR) learns users' preferences by capturing the
short-term and sequential patterns from the evolution of user behaviors. Among
the studies in the SBR field, graph-based approaches are a relatively powerful
kind of way, which generally extract item information by message aggregation
under Euclidean space. However, such methods can't effectively extract the
hierarchical information contained among consecutive items in a session, which
is critical to represent users' preferences. In this paper, we present a
hyperbolic contrastive graph recommender (HCGR), a principled session-based
recommendation framework involving Lorentz hyperbolic space to adequately
capture the coherence and hierarchical representations of the items. Within
this framework, we design a novel adaptive hyperbolic attention computation to
aggregate the graph message of each user's preference in a session-based
behavior sequence. In addition, contrastive learning is leveraged to optimize
the item representation by considering the geodesic distance between positive
and negative samples in hyperbolic space. Extensive experiments on four
real-world datasets demonstrate that HCGR consistently outperforms
state-of-the-art baselines by 0.43$\%$-28.84$\%$ in terms of $HitRate$, $NDCG$
and $MRR$.

    

### [[2107.05368] A Three Phase Semantic Web Matchmaker](http://arxiv.org/abs/2107.05368)


  Since using environments that are made according to the service oriented
architecture, we have more effective and dynamic applications. Semantic
matchmaking process is finding valuable service candidates for substitution. It
is a very important aspect of using semantic Web Services. Our proposed
matchmaker algorithm performs semantic matching of Web Services on the basis of
input and output descriptions of semantic Web Services matching. This technique
takes advantages from a graph structure and flow networks. Our novel approach
is assigning matchmaking scores to semantics of the inputs and outputs
parameters and their types. It makes a flow network in which the weights of the
edges are these scores, using FordFulkerson algorithm, we find matching rate of
two web services. So, all services should be described in the same Ontology Web
Language. Among these candidates, best one is chosen for substitution in the
case of an execution failure. Our approach uses the algorithm that has the
least running time among all others that can be used for bipartite matching.
The importance of problem is that in real systems, many fundamental problems
will occur by late answering. So system`s service should always be on and if
one of them crashes, it would be replaced fast. Semantic web matchmaker eases
this process.

    

### [[2107.05369] How to Approximate Ontology-Mediated Queries](http://arxiv.org/abs/2107.05369)


  We introduce and study several notions of approximation for ontology-mediated
queries based on the description logics ALC and ALCI. Our approximations are of
two kinds: we may (1) replace the ontology with one formulated in a tractable
ontology language such as ELI or certain TGDs and (2) replace the database with
one from a tractable class such as the class of databases whose treewidth is
bounded by a constant. We determine the computational complexity and the
relative completeness of the resulting approximations. (Almost) all of them
reduce the data complexity from coNP-complete to PTime, in some cases even to
fixed-parameter tractable and to linear time. While approximations of kind (1)
also reduce the combined complexity, this tends to not be the case for
approximations of kind (2). In some cases, the combined complexity even
increases.

    

### [[2107.05373] On the Evaluation of Commit Message Generation Models: An Experimental Study](http://arxiv.org/abs/2107.05373)


  Commit messages are natural language descriptions of code changes, which are
important for program understanding and maintenance. However, writing commit
messages manually is time-consuming and laborious, especially when the code is
updated frequently. Various approaches utilizing generation or retrieval
techniques have been proposed to automatically generate commit messages. To
achieve a better understanding of how the existing approaches perform in
solving this problem, this paper conducts a systematic and in-depth analysis of
the state-of-the-art models and datasets. We find that: (1) Different variants
of the BLEU metric are used in previous works, which affects the evaluation and
understanding of existing methods. (2) Most existing datasets are crawled only
from Java repositories while repositories in other programming languages are
not sufficiently explored. (3) Dataset splitting strategies can influence the
performance of existing models by a large margin. Some models show better
performance when the datasets are split by commit, while other models perform
better when the datasets are split by timestamp or by project. Based on our
findings, we conduct a human evaluation and find the BLEU metric that best
correlates with the human scores for the task. We also collect a large-scale,
information-rich, and multi-language commit message dataset MCMD and evaluate
existing models on this dataset. Furthermore, we conduct extensive experiments
under different dataset splitting strategies and suggest the suitable models
under different scenarios. Based on the experimental results and findings, we
provide feasible suggestions for comprehensively evaluating commit message
generation models and discuss possible future research directions. We believe
this work can help practitioners and researchers better evaluate and select
models for automatic commit message generation.

    

### [[2107.05383] Not Quite 'Ask a Librarian': AI on the Nature, Value, and Future of LIS](http://arxiv.org/abs/2107.05383)


  AI language models trained on Web data generate prose that reflects human
knowledge and public sentiments, but can also contain novel insights and
predictions. We asked the world's best language model, GPT-3, fifteen difficult
questions about the nature, value, and future of library and information
science (LIS), topics that receive perennial attention from LIS scholars. We
present highlights from its 45 different responses, which range from platitudes
and caricatures to interesting perspectives and worrisome visions of the
future, thus providing an LIS-tailored demonstration of the current performance
of AI language models. We also reflect on the viability of using AI to forecast
or generate research ideas in this way today. Finally, we have shared the full
response log online for readers to consider and evaluate for themselves.

    

### [[2006.00483] Real-World Scenario Mining for the Assessment of Automated Vehicles](http://arxiv.org/abs/2006.00483)


  Scenario-based methods for the assessment of Automated Vehicles (AVs) are
widely supported by many players in the automotive field. Scenarios captured
from real-world data can be used to define the scenarios for the assessment and
to estimate their relevance. Therefore, different techniques are proposed for
capturing scenarios from real-world data. In this paper, we propose a new
method to capture scenarios from real-world data using a two-step approach. The
first step consists in automatically labeling the data with tags. Second, we
mine the scenarios, represented by a combination of tags, based on the labeled
tags. One of the benefits of our approach is that the tags can be used to
identify characteristics of a scenario that are shared among different type of
scenarios. In this way, these characteristics need to be identified only once.
Furthermore, the method is not specific for one type of scenario and,
therefore, it can be applied to a large variety of scenarios. We provide two
examples to illustrate the method. This paper is concluded with some promising
future possibilities for our approach, such as automatic generation of
scenarios for the assessment of automated vehicles.

    

### [[2007.15203] Algorithmic Stability in Fair Allocation of Indivisible Goods Among Two Agents](http://arxiv.org/abs/2007.15203)


  Many allocation problems in multiagent systems rely on agents specifying
cardinal preferences. However, allocation mechanisms can be sensitive to small
perturbations in cardinal preferences, thus causing agents who make ``small" or
``innocuous" mistakes while reporting their preferences to experience a large
change in their utility for the final outcome. To address this, we introduce a
notion of algorithmic stability and study it in the context of fair and
efficient allocations of indivisible goods among two agents. We show that it is
impossible to achieve exact stability along with even a weak notion of fairness
and even approximate efficiency. As a result, we propose two relaxations to
stability, namely, approximate-stability and weak-approximate-stability, and
show how existing algorithms in the fair division literature that guarantee
fair and efficient outcomes perform poorly with respect to these relaxations.
This leads us to explore the possibility of designing new algorithms that are
more stable. Towards this end, we present a general characterization result for
pairwise maximin share allocations, and in turn use it to design an algorithm
that is approximately-stable and guarantees a pairwise maximin share and Pareto
optimal allocation for two agents. Finally, we present a simple framework that
can be used to modify existing fair and efficient algorithms in order to ensure
that they also achieve weak-approximate-stability.

    

### [[2011.13977] Improving Welfare in One-sided Matching using Simple Threshold Queries](http://arxiv.org/abs/2011.13977)


  We study one-sided matching problems where $n$ agents have preferences over
$m$ objects and each of them need to be assigned to at most one object. Most
work on such problems assume that the agents only have ordinal preferences and
usually the goal in them is to compute a matching that satisfies some notion of
economic efficiency. However, in reality, agents may have some preference
intensities or cardinal utilities that, e.g., indicate that they like an object
much more than another object, and not taking these into account can result in
a loss in welfare. While one way to potentially account for these is to
directly ask the agents for this information, such an elicitation process is
cognitively demanding. Therefore, we focus on learning more about their
cardinal preferences using simple threshold queries which ask an agent if they
value an object greater than a certain value, and use this in turn to come up
with algorithms that produce a matching that, for a particular economic notion
$X$, satisfies $X$ and also achieves a good approximation to the optimal
welfare among all matchings that satisfy $X$. We focus on several notions of
economic efficiency, and look at both adaptive and non-adaptive algorithms.
Overall, our results show how one can improve welfare by even non-adaptively
asking the agents for just one bit of extra information per object.

    

### [[2102.01353] Subdimensional Expansion for Multi-objective Multi-agent Path Finding](http://arxiv.org/abs/2102.01353)


  Conventional multi-agent path planners typically determine a path that
optimizes a single objective, such as path length. Many applications, however,
may require multiple objectives, say time-to-completion and fuel use, to be
simultaneously optimized in the planning process. Often, these criteria may not
be readily compared and sometimes lie in competition with each other. Simply
applying standard multi-objective search algorithms to multi-agent path finding
may prove to be inefficient because the size of the space of possible
solutions, i.e., the Pareto-optimal set, can grow exponentially with the number
of agents (the dimension of the search space). This paper presents an approach
that bypasses this so-called curse of dimensionality by leveraging our prior
multi-agent work with a framework called subdimensional expansion. One example
of subdimensional expansion, when applied to A*, is called M* and M* was
limited to a single objective function. We combine principles of dominance and
subdimensional expansion to create a new algorithm named multi-objective M*
(MOM*), which dynamically couples agents for planning only when those agents
have to "interact" with each other. MOM* computes the complete Pareto-optimal
set for multiple agents efficiently and naturally trades off sub-optimal
approximations of the Pareto-optimal set and computational efficiency. Our
approach is able to find the complete Pareto-optimal set for problem instances
with hundreds of solutions which the standard multi-objective A* algorithms
could not find within a bounded time.

    

### [[2102.08005] TransFuse: Fusing Transformers and CNNs for Medical Image Segmentation](http://arxiv.org/abs/2102.08005)


  Medical image segmentation - the prerequisite of numerous clinical needs -
has been significantly prospered by recent advances in convolutional neural
networks (CNNs). However, it exhibits general limitations on modeling explicit
long-range relation, and existing cures, resorting to building deep encoders
along with aggressive downsampling operations, leads to redundant deepened
networks and loss of localized details. Hence, the segmentation task awaits a
better solution to improve the efficiency of modeling global contexts while
maintaining a strong grasp of low-level details. In this paper, we propose a
novel parallel-in-branch architecture, TransFuse, to address this challenge.
TransFuse combines Transformers and CNNs in a parallel style, where both global
dependency and low-level spatial details can be efficiently captured in a much
shallower manner. Besides, a novel fusion technique - BiFusion module is
created to efficiently fuse the multi-level features from both branches.
Extensive experiments demonstrate that TransFuse achieves the newest
state-of-the-art results on both 2D and 3D medical image sets including polyp,
skin lesion, hip, and prostate segmentation, with significant parameter
decrease and inference speed improvement.

    

### [[2103.09712] Trans-SVNet: Accurate Phase Recognition from Surgical Videos via Hybrid Embedding Aggregation Transformer](http://arxiv.org/abs/2103.09712)


  Real-time surgical phase recognition is a fundamental task in modern
operating rooms. Previous works tackle this task relying on architectures
arranged in spatio-temporal order, however, the supportive benefits of
intermediate spatial features are not considered. In this paper, we introduce,
for the first time in surgical workflow analysis, Transformer to reconsider the
ignored complementary effects of spatial and temporal features for accurate
surgical phase recognition. Our hybrid embedding aggregation Transformer fuses
cleverly designed spatial and temporal embeddings by allowing for active
queries based on spatial information from temporal embedding sequences. More
importantly, our framework processes the hybrid embeddings in parallel to
achieve a high inference speed. Our method is thoroughly validated on two large
surgical video datasets, i.e., Cholec80 and M2CAI16 Challenge datasets, and
outperforms the state-of-the-art approaches at a processing speed of 91 fps.

    

### [[2103.15552] Energy Decay Network (EDeN)](http://arxiv.org/abs/2103.15552)


  This paper and accompanying Python and C++ Framework is the product of the
authors perceived problems with narrow (Discrimination based) AI. (Artificial
Intelligence) The Framework attempts to develop a genetic transfer of
experience through potential structural expressions using a common
regulation/exchange value (energy) to create a model whereby neural
architecture and all unit processes are co-dependently developed by genetic and
real time signal processing influences; successful routes are defined by
stability of the spike distribution per epoch which is influenced by
genetically encoded morphological development biases.These principles are aimed
towards creating a diverse and robust network that is capable of adapting to
general tasks by training within a simulation designed for transfer learning to
other mediums at scale.

    

### [[2104.01266] Designing for human-AI complementarity in K-12 education](http://arxiv.org/abs/2104.01266)


  Recent work has explored how complementary strengths of humans and artificial
intelligence (AI) systems might be productively combined. However, successful
forms of human-AI partnership have rarely been demonstrated in real-world
settings. We present the iterative design and evaluation of Lumilo, smart
glasses that help teachers help their students in AI-supported classrooms by
presenting real-time analytics about students' learning, metacognition, and
behavior. Results from a field study conducted in K-12 classrooms indicate that
students learn more when teachers and AI tutors work together during class. We
discuss implications of this research for the design of human-AI partnerships.
We argue for more participatory approaches to research and design in this area,
in which practitioners and other stakeholders are deeply, meaningfully involved
throughout the process. Furthermore, we advocate for theory-building and for
principled approaches to the study of human-AI decision-making in real-world
contexts.

    

### [[2106.14587] Topos and Stacks of Deep Neural Networks](http://arxiv.org/abs/2106.14587)


  Every known artificial deep neural network (DNN) corresponds to an object in
a canonical Grothendieck's topos; its learning dynamic corresponds to a flow of
morphisms in this topos. Invariance structures in the layers (like CNNs or
LSTMs) correspond to Giraud's stacks. This invariance is supposed to be
responsible of the generalization property, that is extrapolation from learning
data under constraints. The fibers represent pre-semantic categories (Culioli,
Thom), over which artificial languages are defined, with internal logics,
intuitionist, classical or linear (Girard). Semantic functioning of a network
is its ability to express theories in such a language for answering questions
in output about input data. Quantities and spaces of semantic information are
defined by analogy with the homological interpretation of Shannon's entropy
(P.Baudot and D.B. 2015). They generalize the measures found by Carnap and
Bar-Hillel (1952). Amazingly, the above semantical structures are classified by
geometric fibrant objects in a closed model category of Quillen, then they give
rise to homotopical invariants of DNNs and of their semantic functioning.
Intentional type theories (Martin-Loef) organize these objects and fibrations
between them. Information contents and exchanges are analyzed by Grothendieck's
derivators.

    

### [[2103.00738] FPS-Net: A Convolutional Fusion Network for Large-Scale LiDAR Point Cloud Segmentation](http://arxiv.org/abs/2103.00738)


  Scene understanding based on LiDAR point cloud is an essential task for
autonomous cars to drive safely, which often employs spherical projection to
map 3D point cloud into multi-channel 2D images for semantic segmentation. Most
existing methods simply stack different point attributes/modalities (e.g.
coordinates, intensity, depth, etc.) as image channels to increase information
capacity, but ignore distinct characteristics of point attributes in different
image channels. We design FPS-Net, a convolutional fusion network that exploits
the uniqueness and discrepancy among the projected image channels for optimal
point cloud segmentation. FPS-Net adopts an encoder-decoder structure. Instead
of simply stacking multiple channel images as a single input, we group them
into different modalities to first learn modality-specific features separately
and then map the learned features into a common high-dimensional feature space
for pixel-level fusion and learning. Specifically, we design a residual dense
block with multiple receptive fields as a building block in the encoder which
preserves detailed information in each modality and learns hierarchical
modality-specific and fused features effectively. In the FPS-Net decoder, we
use a recurrent convolution block likewise to hierarchically decode fused
features into output space for pixel-level classification. Extensive
experiments conducted on two widely adopted point cloud datasets show that
FPS-Net achieves superior semantic segmentation as compared with
state-of-the-art projection-based methods. In addition, the proposed modality
fusion idea is compatible with typical projection-based methods and can be
incorporated into them with consistent performance improvements.

    

### [[2107.04663] A cost-aware logical framework](http://arxiv.org/abs/2107.04663)


  We present \textbf{calf}, a \textbf{c}ost-\textbf{a}ware \textbf{l}ogical
\textbf{f}ramework for studying quantitative aspects of functional programs.
Taking inspiration from recent work that reconstructs traditional aspects of
programming languages in terms of a modal account of \emph{phase distinctions},
we argue that the cost structure of programs motivates a phase distinction
between \emph{intension} and \emph{extension}. Armed with this technology, we
contribute a synthetic account of cost structure as a computational effect in
which cost-aware programs enjoy an internal noninterference property:
input/output behavior cannot depend on cost.
As a full-spectrum dependent type theory, \textbf{calf} presents a unified
language for programming and specification of both cost and behavior that can
be integrated smoothly with existing mathematical libraries available in type
theoretic proof assistants. We evaluate \textbf{calf} as a general framework
for cost analysis by implementing two fundamental techniques for algorithm
analysis: the \emph{method of recurrence relations} and \emph{physicist's
method for amortized analysis}. We deploy these techniques on a variety of case
studies: we prove a tight, closed bound for Euclid's algorithm, verify the
amortized complexity of batched queues, and derive tight, closed bounds for the
sequential and \emph{parallel} complexity of merge sort, all fully mechanized
in the Agda proof assistant. Lastly we substantiate the soundness of
quantitative reasoning in \textbf{calf} by means of a model construction.

    

### [[2107.04859] Approximate Normalization and Eager Equality Checking for Gradual Inductive Families](http://arxiv.org/abs/2107.04859)


  Harnessing the power of dependently typed languages can be difficult.
Programmers must manually construct proofs to produce well-typed programs,
which is not an easy task. In particular, migrating code to these languages is
challenging. Gradual typing can make dependently-typed languages easier to use
by mixing static and dynamic checking in a principled way. With gradual types,
programmers can incrementally migrate code to a dependently typed language.
However, adding gradual types to dependent types creates a new challenge:
mixing decidable type-checking and incremental migration in a full-featured
language is a precarious balance. Programmers expect type-checking to
terminate, but dependent type-checkers evaluate terms at compile time, which is
problematic because gradual types can introduce non-termination into an
otherwise terminating language. Steps taken to mitigate this non-termination
must not jeopardize the smooth transitions between dynamic and static.
We present a gradual dependently-typed language that supports inductive type
families, has decidable type-checking, and provably supports smooth migration
between static and dynamic, as codified by the refined criteria for gradual
typing proposed by Siek et al. (2015). Like Eremondi et al. (2019), we use
approximate normalization for terminating compile-time evaluation. Unlike
Eremondi et al., our normalization does not require comparison of variables,
allowing us to show termination with a syntactic model that accommodates
inductive types. Moreover, we design a novel a technique for tracking
constraints on type indices, so that dynamic constraint violations signal
run-time errors eagerly. To facilitate these checks, we define an algebraic
notion of gradual precision, axiomatizing certain semantic properties of
gradual terms.

    

### [[2107.05225] Incremental Vulnerability Detection via Back-Propagating Symbolic Execution of Insecurity Separation Logic](http://arxiv.org/abs/2107.05225)


  We present the first compositional, incremental static analysis for detecting
memory-safety and information leakage vulnerabilities in C-like programs. To do
so, we develop the first under-approximate relational program logics, including
Insecurity Separation Logic (InsecSL). We show how InsecSL can be automated via
back-propagating symbolic execution (BPSE) to build a bottom-up,
inter-procedural and incremental analysis for detecting vulnerabilities. We
prove our approach sound in Isabelle/HOL and implement it in a proof-of-concept
tool, Underflow, for analysing C programs, which we apply to various case
studies.

    

### [[1707.02894] Kleene Algebra Modulo Theories](http://arxiv.org/abs/1707.02894)


  Kleene algebras with tests (KATs) offer sound, complete, and decidable
equational reasoning about regularly structured programs. Since NetKAT
demonstrated how well various extensions of KATs apply to computer networks,
interest in KATs has increased greatly. Unfortunately, extending a KAT to a
particular domain by adding custom primitives, proving its equational theory
sound and complete, and coming up with efficient automata-theoretic
implementations is still an expert's task.
We present a general framework for deriving KATs we call Kleene algebra
modulo theories: given primitives and notions of state, we can automatically
derive a corresponding KAT's semantics, prove its equational theory sound and
complete, and generate an automata-based implementation of equivalence
checking. Our framework is based on pushback, a way of specifying how
predicates and actions interact, first used in Temporal NetKAT. We offer
several case studies, including theories for bitvectors, increasing natural
numbers, unbounded sets and maps, temporal logic, and network protocols.
Finally, we provide an OCaml implementation that closely matches the theory:
with only a few declarations, users can automatically derive an
automata-theoretic decision procedure for a KAT.

    

### [[2105.14769] Gillian: A Multi-Language Platform for Unified Symbolic Analysis](http://arxiv.org/abs/2105.14769)


  This is an evolving document describing the meta-theory, the implementation,
and the instantiations of Gillian, a multi-language symbolic analysis platform.

    