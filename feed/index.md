
## 2021-8-13

### [[2108.05567] Combinatorial Resources Auction in Decentralized Edge-Thing Systems Using Blockchain and Differential Privacy](http://arxiv.org/abs/2108.05567)


  With the continuous expansion of Internet of Things (IoT) devices, edge
computing mode has emerged in recent years to overcome the shortcomings of
traditional cloud computing mode, such as high delay, network congestion, and
large resource consumption. Thus, edge-thing systems will replace the classic
cloud-thing/cloud-edge-thing systems and become mainstream gradually, where IoT
devices can offload their tasks to neighboring edge nodes. A common problem is
how to utilize edge computing resources. For the sake of fairness, double
auction can be used in the edge-thing system to achieve an effective resource
allocation and pricing mechanism. Due to the lack of third-party management
agencies and mutual distrust between nodes, in our edge-thing systems, we
introduce blockchains to prevent malicious nodes from tampering with
transaction records and smart contracts to act as an auctioneer to realize
resources auction. Since the auction results stored in this blockchain-based
system are transparent, they are threatened with inference attacks. Thus in
this paper, we design a differentially private combinatorial double auction
mechanism by exploring the exponential mechanism such that maximizing the
revenue of edge computing platform, in which each IoT device requests a
resource bundle and edge nodes compete with each other to provide resources. It
can not only guarantee approximate truthfulness and high revenue, but also
ensure privacy security. Through necessary theoretical analysis and numerical
simulations, the effectiveness of our proposed mechanisms can be validated.

    

### [[2108.05690] Going Deeper in Frequency Convolutional Neural Network: A Theoretical Perspective](http://arxiv.org/abs/2108.05690)


  Convolutional neural network (CNN) is one of the most widely-used successful
architectures in the era of deep learning. However, the high-computational cost
of CNN still hampers more universal uses to light devices. Fortunately, the
Fourier transform on convolution gives an elegant and promising solution to
dramatically reduce the computation cost. Recently, some studies devote to such
a challenging problem and pursue the complete frequency computation without any
switching between spatial domain and frequent domain. In this work, we revisit
the Fourier transform theory to derive feed-forward and back-propagation
frequency operations of typical network modules such as convolution, activation
and pooling. Due to the calculation limitation of complex numbers on most
computation tools, we especially extend the Fourier transform to the Laplace
transform for CNN, which can run in the real domain with more relaxed
constraints. This work more focus on a theoretical extension and discussion
about frequency CNN, and lay some theoretical ground for real application.

    

### [[2108.05781] Networked Twins and Twins of Networks: an Overview on the Relationship Between Digital Twins and 6G](http://arxiv.org/abs/2108.05781)


  Digital Twin (DT) is a promising technology for the new immersive digital
life with a variety of applications in areas such as Industry 4.0, aviation,
and healthcare. Proliferation of this technology requires higher data rates,
reliability, resilience, and lower latency beyond what is currently offered by
5G. Thus, DT can become a major driver for 6G research and development.
Alternatively, 6G network development can benefit from Digital Twin technology
and its powerful features such as modularity and remote intelligence. Using DT,
a 6G network (or some of its components) will have the opportunity to use
Artificial Intelligence more proactively in order to enhance its resilience.
DT's application in telecommunications is still in its infancy. In this article
we highlight some of the most promising research and development directions for
this technology.

    

### [[2102.00588] Stochastic Geometry Analysis of Spatial-Temporal Performance in Wireless Networks: A Tutorial](http://arxiv.org/abs/2102.00588)


  The performance of wireless networks is fundamentally limited by the
aggregate interference, which depends on the spatial distributions of the
interferers, channel conditions, and user traffic patterns (or queueing
dynamics). These factors usually exhibit spatial and temporal correlations and
thus make the performance of large-scale networks environment-dependent (i.e.,
dependent on network topology, locations of the blockages, etc.). The
correlation can be exploited in protocol designs (e.g., spectrum-, load-,
location-, energy-aware resource allocations) to provide efficient wireless
services. For this, accurate system-level performance characterization and
evaluation with spatio-temporal correlation are required. In this context,
stochastic geometry models and random graph techniques have been used to
develop analytical frameworks to capture the spatio-temporal interference
correlation in large-scale wireless networks. The objective of this article is
to provide a tutorial on the stochastic geometry analysis of large-scale
wireless networks that captures the spatio-temporal interference correlation
(and hence the signal-to-interference ratio (SIR) correlation). We first
discuss the importance of spatio-temporal performance analysis, different
parameters affecting the spatio-temporal correlation in the SIR, and the
different performance metrics for spatio-temporal analysis. Then we describe
the methodologies to characterize spatio-temporal SIR correlations for
different network configurations (independent, attractive, repulsive
configurations), shadowing scenarios, user locations, queueing behavior,
relaying, retransmission, and mobility. We conclude by outlining future
research directions in the context of spatio-temporal analysis of emerging
wireless communications scenarios.

    

### [[2108.05382] Skill Preferences: Learning to Extract and Execute Robotic Skills from Human Feedback](http://arxiv.org/abs/2108.05382)


  A promising approach to solving challenging long-horizon tasks has been to
extract behavior priors (skills) by fitting generative models to large offline
datasets of demonstrations. However, such generative models inherit the biases
of the underlying data and result in poor and unusable skills when trained on
imperfect demonstration data. To better align skill extraction with human
intent we present Skill Preferences (SkiP), an algorithm that learns a model
over human preferences and uses it to extract human-aligned skills from offline
data. After extracting human-preferred skills, SkiP also utilizes human
feedback to solve down-stream tasks with RL. We show that SkiP enables a
simulated kitchen robot to solve complex multi-step manipulation tasks and
substantially outperforms prior leading RL algorithms with human preferences as
well as leading skill extraction algorithms without human preferences.

    

### [[2108.05385] Physics-Coupled Spatio-Temporal Active Learning for Dynamical Systems](http://arxiv.org/abs/2108.05385)


  Spatio-temporal forecasting is of great importance in a wide range of
dynamical systems applications from atmospheric science, to recent COVID-19
spread modeling. These applications rely on accurate predictions of
spatio-temporal structured data reflecting real-world phenomena. A stunning
characteristic is that the dynamical system is not only driven by some physics
laws but also impacted by the localized factor in spatial and temporal regions.
One of the major challenges is to infer the underlying causes, which generate
the perceived data stream and propagate the involved causal dynamics through
the distributed observing units. Another challenge is that the success of
machine learning based predictive models requires massive annotated data for
model training. However, the acquisition of high-quality annotated data is
objectively manual and tedious as it needs a considerable amount of human
intervention, making it infeasible in fields that require high levels of
expertise. To tackle these challenges, we advocate a spatio-temporal
physics-coupled neural networks (ST-PCNN) model to learn the underlying physics
of the dynamical system and further couple the learned physics to assist the
learning of the recurring dynamics. To deal with data-acquisition constraints,
an active learning mechanism with Kriging for actively acquiring the most
informative data is proposed for ST-PCNN training in a partially observable
environment. Our experiments on both synthetic and real-world datasets exhibit
that the proposed ST-PCNN with active learning converges to near optimal
accuracy with substantially fewer instances.

    

### [[2108.05390] Seven challenges for harmonizing explainability requirements](http://arxiv.org/abs/2108.05390)


  Regulators have signalled an interest in adopting explainable AI(XAI)
techniques to handle the diverse needs for model governance, operational
servicing, and compliance in the financial services industry. In this short
overview, we review the recent technical literature in XAI and argue that based
on our current understanding of the field, the use of XAI techniques in
practice necessitate a highly contextualized approach considering the specific
needs of stakeholders for particular business applications.

    

### [[2108.05401] Ontology drift is a challenge for explainable data governance](http://arxiv.org/abs/2108.05401)


  We introduce the needs for explainable AI that arise from Standard No. 239
from the Basel Committee on Banking Standards (BCBS 239), which outlines 11
principles for effective risk data aggregation and risk reporting for financial
institutions. Of these, explainableAI is necessary for compliance in two key
aspects: data quality, and appropriate reporting for multiple stakeholders. We
describe the implementation challenges for one specific regulatory
requirement:that of having a complete data taxonomy that is appropriate for
firmwide use. The constantly evolving nature of financial ontologies
necessitate a continuous updating process to ensure ongoing compliance.

    

### [[2108.05421] Seismic wave propagation and inversion with Neural Operators](http://arxiv.org/abs/2108.05421)


  Seismic wave propagation forms the basis for most aspects of seismological
research, yet solving the wave equation is a major computational burden that
inhibits the progress of research. This is exaspirated by the fact that new
simulations must be performed when the velocity structure or source location is
perturbed. Here, we explore a prototype framework for learning general
solutions using a recently developed machine learning paradigm called Neural
Operator. A trained Neural Operator can compute a solution in negligible time
for any velocity structure or source location. We develop a scheme to train
Neural Operators on an ensemble of simulations performed with random velocity
models and source locations. As Neural Operators are grid-free, it is possible
to evaluate solutions on higher resolution velocity models than trained on,
providing additional computational efficiency. We illustrate the method with
the 2D acoustic wave equation and demonstrate the method's applicability to
seismic tomography, using reverse mode automatic differentiation to compute
gradients of the wavefield with respect to the velocity structure. The
developed procedure is nearly an order of magnitude faster than using
conventional numerical methods for full waveform inversion.

    

### [[2108.05433] Learning to Hash Robustly, with Guarantees](http://arxiv.org/abs/2108.05433)


  The indexing algorithms for the high-dimensional nearest neighbor search
(NNS) with the best worst-case guarantees are based on the randomized Locality
Sensitive Hashing (LSH), and its derivatives. In practice, many heuristic
approaches exist to "learn" the best indexing method in order to speed-up NNS,
crucially adapting to the structure of the given dataset.
Oftentimes, these heuristics outperform the LSH-based algorithms on real
datasets, but, almost always, come at the cost of losing the guarantees of
either correctness or robust performance on adversarial queries, or apply to
datasets with an assumed extra structure/model. In this paper, we design an NNS
algorithm for the Hamming space that has worst-case guarantees essentially
matching that of theoretical algorithms, while optimizing the hashing to the
structure of the dataset (think instance-optimal algorithms) for performance on
the minimum-performing query. We evaluate the algorithm's ability to optimize
for a given dataset both theoretically and practically. On the theoretical
side, we exhibit a natural setting (dataset model) where our algorithm is much
better than the standard theoretical one. On the practical side, we run
experiments that show that our algorithm has a 1.8x and 2.1x better recall on
the worst-performing queries to the MNIST and ImageNet datasets.

    

### [[2108.05439] Gap-Dependent Unsupervised Exploration for Reinforcement Learning](http://arxiv.org/abs/2108.05439)


  For the problem of task-agnostic reinforcement learning (RL), an agent first
collects samples from an unknown environment without the supervision of reward
signals, then is revealed with a reward and is asked to compute a corresponding
near-optimal policy. Existing approaches mainly concern the worst-case
scenarios, in which no structural information of the reward/transition-dynamics
is utilized. Therefore the best sample upper bound is
$\propto\widetilde{\mathcal{O}}(1/\epsilon^2)$, where $\epsilon>0$ is the
target accuracy of the obtained policy, and can be overly pessimistic. To
tackle this issue, we provide an efficient algorithm that utilizes a gap
parameter, $\rho>0$, to reduce the amount of exploration. In particular, for an
unknown finite-horizon Markov decision process, the algorithm takes only
$\widetilde{\mathcal{O}} (1/\epsilon \cdot (H^3SA / \rho + H^4 S^2 A) )$
episodes of exploration, and is able to obtain an $\epsilon$-optimal policy for
a post-revealed reward with sub-optimality gap at least $\rho$, where $S$ is
the number of states, $A$ is the number of actions, and $H$ is the length of
the horizon, obtaining a nearly \emph{quadratic saving} in terms of $\epsilon$.
We show that, information-theoretically, this bound is nearly tight for $\rho <
\Theta(1/(HS))$ and $H>1$. We further show that
$\propto\widetilde{\mathcal{O}}(1)$ sample bound is possible for $H=1$ (i.e.,
multi-armed bandit) or with a sampling simulator, establishing a stark
separation between those settings and the RL setting.

    

### [[2108.05449] Learning Bias-Invariant Representation by Cross-Sample Mutual Information Minimization](http://arxiv.org/abs/2108.05449)


  Deep learning algorithms mine knowledge from the training data and thus would
likely inherit the dataset's bias information. As a result, the obtained model
would generalize poorly and even mislead the decision process in real-life
applications. We propose to remove the bias information misused by the target
task with a cross-sample adversarial debiasing (CSAD) method. CSAD explicitly
extracts target and bias features disentangled from the latent representation
generated by a feature extractor and then learns to discover and remove the
correlation between the target and bias features. The correlation measurement
plays a critical role in adversarial debiasing and is conducted by a
cross-sample neural mutual information estimator. Moreover, we propose joint
content and local structural representation learning to boost mutual
information estimation for better performance. We conduct thorough experiments
on publicly available datasets to validate the advantages of the proposed
method over state-of-the-art approaches.

    

### [[2108.05476] Weakly Supervised Medical Image Segmentation](http://arxiv.org/abs/2108.05476)


  In this paper, we propose a novel approach for few-shot semantic segmentation
with sparse labeled images. We investigate the effectiveness of our method,
which is based on the Model-Agnostic Meta-Learning (MAML) algorithm, in the
medical scenario, where the use of sparse labeling and few-shot can alleviate
the cost of producing new annotated datasets. Our method uses sparse labels in
the meta-training and dense labels in the meta-test, thus making the model
learn to predict dense labels from sparse ones. We conducted experiments with
four Chest X-Ray datasets to evaluate two types of annotations (grid and
points). The results show that our method is the most suitable when the target
domain highly differs from source domains, achieving Jaccard scores comparable
to dense labels, using less than 2% of the pixels of an image with labels in
few-shot scenarios.

    

### [[2108.05484] Self-supervised Contrastive Learning for Irrigation Detection in Satellite Imagery](http://arxiv.org/abs/2108.05484)


  Climate change has caused reductions in river runoffs and aquifer recharge
resulting in an increasingly unsustainable crop water demand from reduced
freshwater availability. Achieving food security while deploying water in a
sustainable manner will continue to be a major challenge necessitating careful
monitoring and tracking of agricultural water usage. Historically, monitoring
water usage has been a slow and expensive manual process with many
imperfections and abuses. Ma-chine learning and remote sensing developments
have increased the ability to automatically monitor irrigation patterns, but
existing techniques often require curated and labelled irrigation data, which
are expensive and time consuming to obtain and may not exist for impactful
areas such as developing countries. In this paper, we explore an end-to-end
real world application of irrigation detection with uncurated and unlabeled
satellite imagery. We apply state-of-the-art self-supervised deep learning
techniques to optical remote sensing data, and find that we are able to detect
irrigation with up to nine times better precision, 90% better recall and 40%
more generalization ability than the traditional supervised learning methods.

    

### [[2108.05490] Attacks against Ranking Algorithms with Text Embeddings: a Case Study on Recruitment Algorithms](http://arxiv.org/abs/2108.05490)


  Recently, some studies have shown that text classification tasks are
vulnerable to poisoning and evasion attacks. However, little work has
investigated attacks against decision making algorithms that use text
embeddings, and their output is a ranking. In this paper, we focus on ranking
algorithms for recruitment process, that employ text embeddings for ranking
applicants resumes when compared to a job description. We demonstrate both
white box and black box attacks that identify text items, that based on their
location in embedding space, have significant contribution in increasing the
similarity score between a resume and a job description. The adversary then
uses these text items to improve the ranking of their resume among others. We
tested recruitment algorithms that use the similarity scores obtained from
Universal Sentence Encoder (USE) and Term Frequency Inverse Document Frequency
(TF IDF) vectors. Our results show that in both adversarial settings, on
average the attacker is successful. We also found that attacks against TF IDF
is more successful compared to USE.

    

### [[2108.05509] DOI: Divergence-based Out-of-Distribution Indicators via Deep Generative Models](http://arxiv.org/abs/2108.05509)


  To ensure robust and reliable classification results, OoD
(out-of-distribution) indicators based on deep generative models are proposed
recently and are shown to work well on small datasets. In this paper, we
conduct the first large collection of benchmarks (containing 92 dataset pairs,
which is 1 order of magnitude larger than previous ones) for existing OoD
indicators and observe that none perform well. We thus advocate that a large
collection of benchmarks is mandatory for evaluating OoD indicators. We propose
a novel theoretical framework, DOI, for divergence-based Out-of-Distribution
indicators (instead of traditional likelihood-based) in deep generative models.
Following this framework, we further propose a simple and effective OoD
detection algorithm: Single-shot Fine-tune. It significantly outperforms past
works by 5~8 in AUROC, and its performance is close to optimal. In recent, the
likelihood criterion is shown to be ineffective in detecting OoD. Single-shot
Fine-tune proposes a novel fine-tune criterion to detect OoD, by whether the
likelihood of the testing sample is improved after fine-tuning a well-trained
model on it. Fine-tune criterion is a clear and easy-following criterion, which
will lead the OoD domain into a new stage.

    

### [[2108.05523] Fair Decision-Making for Food Inspections](http://arxiv.org/abs/2108.05523)


  We revisit the application of predictive models by the Chicago Department of
Public Health to schedule restaurant inspections and prioritize the detection
of critical violations of the food code. Performing the first analysis from the
perspective of fairness to the population served by the restaurants, we find
that the model treats inspections unequally based on the sanitarian who
conducted the inspection and that in turn there are both geographic and
demographic disparities in the benefits of the model. We examine both
approaches to use the original model in a fairer way and ways to train the
model to achieve fairness and find more success with the former class of
approaches. The challenges from this application point to important directions
for future work around fairness with collective entities rather than
individuals, the use of critical violations as a proxy, and the disconnect
between fair classification and fairness in the dynamic scheduling system.

    

### [[2108.05529] Robotic Testbed for Rendezvous and Optical Navigation: Multi-Source Calibration and Machine Learning Use Cases](http://arxiv.org/abs/2108.05529)


  This work presents the most recent advances of the Robotic Testbed for
Rendezvous and Optical Navigation (TRON) at Stanford University - the first
robotic testbed capable of validating machine learning algorithms for
spaceborne optical navigation. The TRON facility consists of two 6
degrees-of-freedom KUKA robot arms and a set of Vicon motion track cameras to
reconfigure an arbitrary relative pose between a camera and a target mockup
model. The facility includes multiple Earth albedo light boxes and a sun lamp
to recreate the high-fidelity spaceborne illumination conditions. After the
overview of the facility, this work details the multi-source calibration
procedure which enables the estimation of the relative pose between the object
and the camera with millimeter-level position and millidegree-level orientation
accuracies. Finally, a comparative analysis of the synthetic and TRON simulated
imageries is performed using a Convolutional Neural Network (CNN) pre-trained
on the synthetic images. The result shows a considerable gap in the CNN's
performance, suggesting the TRON simulated images can be used to validate the
robustness of any machine learning algorithms trained on more easily accessible
synthetic imagery from computer graphics.

    

### [[2108.05531] The Contextual Appointment Scheduling Problem](http://arxiv.org/abs/2108.05531)


  This study is concerned with the determination of optimal appointment times
for a sequence of jobs with uncertain duration. We investigate the data-driven
Appointment Scheduling Problem (ASP) when one has $n$ observations of $p$
features (covariates) related to the jobs as well as historical data. We
formulate ASP as an Integrated Estimation and Optimization problem using a
task-based loss function. We justify the use of contexts by showing that not
including the them yields to inconsistent decisions, which translates to
sub-optimal appointments. We validate our approach through two numerical
experiments.

    

### [[2108.05533] Efficient Local Planning with Linear Function Approximation](http://arxiv.org/abs/2108.05533)


  We study query and computationally efficient planning algorithms with linear
function approximation and a simulator. We assume that the agent only has local
access to the simulator, meaning that the agent can only query the simulator at
states that have been visited before. This setting is more practical than many
prior works on reinforcement learning with a generative model. We propose an
algorithm named confident Monte Carlo least square policy iteration (Confident
MC-LSPI) for this setting. Under the assumption that the Q-functions of all
deterministic policies are linear in known features of the state-action pairs,
we show that our algorithm has polynomial query and computational complexities
in the dimension of the features, the effective planning horizon and the
targeted sub-optimality, while these complexities are independent of the size
of the state space. One technical contribution of our work is the introduction
of a novel proof technique that makes use of a virtual policy iteration
algorithm. We use this method to leverage existing results on
$\ell_\infty$-bounded approximate policy iteration to show that our algorithm
can learn the optimal policy for the given initial state even only with local
access to the simulator. We believe that this technique can be extended to
broader settings beyond this work.

    

### [[2108.05552] Graph Trend Networks for Recommendations](http://arxiv.org/abs/2108.05552)


  Recommender systems aim to provide personalized services to users and are
playing an increasingly important role in our daily lives. The key of
recommender systems is to predict how likely users will interact with items
based on their historical online behaviors, e.g., clicks, add-to-cart,
purchases, etc. To exploit these user-item interactions, there are increasing
efforts on considering the user-item interactions as a user-item bipartite
graph and then performing information propagation in the graph via Graph Neural
Networks (GNNs). Given the power of GNNs in graph representation learning,
these GNN-based recommendation methods have remarkably boosted the
recommendation performance. Despite their success, most existing GNN-based
recommender systems overlook the existence of interactions caused by unreliable
behaviors (e.g., random/bait clicks) and uniformly treat all the interactions,
which can lead to sub-optimal and unstable performance. In this paper, we
investigate the drawbacks (e.g., non-adaptive propagation and non-robustness)
of existing GNN-based recommendation methods. To address these drawbacks, we
propose the Graph Trend Networks for recommendations (GTN) with principled
designs that can capture the adaptive reliability of the interactions.
Comprehensive experiments and ablation studies are presented to verify and
understand the effectiveness of the proposed framework. Our implementation and
datasets can be released after publication.

    

### [[2108.05568] A Contract Theory based Incentive Mechanism for Federated Learning](http://arxiv.org/abs/2108.05568)


  Federated learning (FL) serves as a data privacy-preserved machine learning
paradigm, and realizes the collaborative model trained by distributed clients.
To accomplish an FL task, the task publisher needs to pay financial incentives
to the FL server and FL server offloads the task to the contributing FL
clients. It is challenging to design proper incentives for the FL clients due
to the fact that the task is privately trained by the clients. This paper aims
to propose a contract theory based FL task training model towards minimizing
incentive budget subject to clients being individually rational (IR) and
incentive compatible (IC) in each FL training round. We design a
two-dimensional contract model by formally defining two private types of
clients, namely data quality and computation effort. To effectively aggregate
the trained models, a contract-based aggregator is proposed. We analyze the
feasible and optimal contract solutions to the proposed contract model.
%Experimental results demonstrate that the proposed framework and contract
model can effective improve the generation accuracy of FL tasks. Experimental
results show that the generalization accuracy of the FL tasks can be improved
by the proposed incentive mechanism where contract-based aggregation is
applied.

    

### [[2108.05569] Agnostic Online Learning and Excellent Sets](http://arxiv.org/abs/2108.05569)


  We revisit a key idea from the interaction of model theory and combinatorics,
the existence of large ``indivisible'' sets, called ``$\epsilon$-excellent,''
in $k$-edge stable graphs (equivalently, Littlestone classes). Translating to
the language of probability, we find a quite different existence proof for
$\epsilon$-excellent sets in Littlestone classes, using regret bounds in online
learning. This proof applies to any $\epsilon < {1}/{2}$, compared to $<
{1}/{2^{2^k}}$ or so in the original proof. We include a second proof using
closure properties and the VC theorem, with other advantages but weaker bounds.
As a simple corollary, the Littlestone dimension remains finite under some
natural modifications to the definition. A theme in these proofs is the
interaction of two abstract notions of majority, arising from measure, and from
rank or dimension; we prove that these densely often coincide and that this is
characteristic of Littlestone (stable) classes. The last section lists several
open problems.

    

### [[2108.05574] Implicit Sparse Regularization: The Impact of Depth and Early Stopping](http://arxiv.org/abs/2108.05574)


  In this paper, we study the implicit bias of gradient descent for sparse
regression. We extend results on regression with quadratic parametrization,
which amounts to depth-2 diagonal linear networks, to more general depth-N
networks, under more realistic settings of noise and correlated designs. We
show that early stopping is crucial for gradient descent to converge to a
sparse model, a phenomenon that we call implicit sparse regularization. This
result is in sharp contrast to known results for noiseless and
uncorrelated-design cases. We characterize the impact of depth and early
stopping and show that for a general depth parameter N, gradient descent with
early stopping achieves minimax optimal sparse recovery with sufficiently small
initialization and step size. In particular, we show that increasing depth
enlarges the scale of working initialization and the early-stopping window,
which leads to more stable gradient paths for sparse recovery.

    

### [[2108.05580] perf4sight: A toolflow to model CNN training performance on Edge GPUs](http://arxiv.org/abs/2108.05580)


  The increased memory and processing capabilities of today's edge devices
create opportunities for greater edge intelligence. In the domain of vision,
the ability to adapt a Convolutional Neural Network's (CNN) structure and
parameters to the input data distribution leads to systems with lower memory
footprint, latency and power consumption. However, due to the limited compute
resources and memory budget on edge devices, it is necessary for the system to
be able to predict the latency and memory footprint of the training process in
order to identify favourable training configurations of the network topology
and device combination for efficient network adaptation. This work proposes
perf4sight, an automated methodology for developing accurate models that
predict CNN training memory footprint and latency given a target device and
network. This enables rapid identification of network topologies that can be
retrained on the edge device with low resource consumption. With PyTorch as the
framework and NVIDIA Jetson TX2 as the target device, the developed models
predict training memory footprint and latency with 95% and 91% accuracy
respectively for a wide range of networks, opening the path towards efficient
network adaptation on edge GPUs.

    

### [[2108.05595] Reinforcement Learning Approach to Active Learning for Image Classification](http://arxiv.org/abs/2108.05595)


  Machine Learning requires large amounts of labeled data to fit a model. Many
datasets are already publicly available, nevertheless forcing application
possibilities of machine learning to the domains of those public datasets. The
ever-growing penetration of machine learning algorithms in new application
areas requires solutions for the need for data in those new domains. This
thesis works on active learning as one possible solution to reduce the amount
of data that needs to be processed by hand, by processing only those datapoints
that specifically benefit the training of a strong model for the task. A newly
proposed framework for framing the active learning workflow as a reinforcement
learning problem is adapted for image classification and a series of three
experiments is conducted. Each experiment is evaluated and potential issues
with the approach are outlined. Each following experiment then proposes
improvements to the framework and evaluates their impact. After the last
experiment, a final conclusion is drawn, unfortunately rejecting this work's
hypothesis and outlining that the proposed framework at the moment is not
capable of improving active learning for image classification with a trained
reinforcement learning agent.

    

### [[2108.05598] AffRankNet+: Ranking Affect Using Privileged Information](http://arxiv.org/abs/2108.05598)


  Many of the affect modelling tasks present an asymmetric distribution of
information between training and test time; additional information is given
about the training data, which is not available at test time. Learning under
this setting is called Learning Under Privileged Information (LUPI). At the
same time, due to the ordinal nature of affect annotations, formulating affect
modelling tasks as supervised learning ranking problems is gaining ground
within the Affective Computing research community. Motivated by the two facts
above, in this study, we introduce a ranking model that treats additional
information about the training data as privileged information to accurately
rank affect states. Our ranking model extends the well-known RankNet model to
the LUPI paradigm, hence its name AffRankNet+. To the best of our knowledge, it
is the first time that a ranking model based on neural networks exploits
privileged information. We evaluate the performance of the proposed model on
the public available Afew-VA dataset and compare it against the RankNet model,
which does not use privileged information. Experimental evaluation indicates
that the AffRankNet+ model can yield significantly better performance.

    

### [[2108.05618] Conditional Sequential Slate Optimization](http://arxiv.org/abs/2108.05618)


  The top search results matching a user query that are displayed on the first
page are critical to the effectiveness and perception of a search system. A
search ranking system typically orders the results by independent
query-document scores to produce a slate of search results. However, such
unilateral scoring methods may fail to capture inter-document dependencies that
users are sensitive to, thus producing a sub-optimal slate. Further, in
practice, many real-world applications such as e-commerce search require
enforcing certain distributional criteria at the slate-level, due to business
objectives or long term user retention goals. Unilateral scoring of results
does not explicitly support optimizing for such objectives with respect to a
slate. Hence, solutions to the slate optimization problem must consider the
optimal selection and order of the documents, along with adherence to
slate-level distributional criteria. To that end, we propose a hybrid framework
extended from traditional slate optimization to solve the conditional slate
optimization problem. We introduce conditional sequential slate optimization
(CSSO), which jointly learns to optimize for traditional ranking metrics as
well as prescribed distribution criteria of documents within the slate. The
proposed method can be applied to practical real world problems such as
enforcing diversity in e-commerce search results, mitigating bias in top
results and personalization of results. Experiments on public datasets and
real-world data from e-commerce datasets show that CSSO outperforms popular
comparable ranking methods in terms of adherence to distributional criteria
while producing comparable or better relevance metrics.

    

### [[2108.05620] FreaAI: Automated extraction of data slices to test machine learning models](http://arxiv.org/abs/2108.05620)


  Machine learning (ML) solutions are prevalent. However, many challenges exist
in making these solutions business-grade. One major challenge is to ensure that
the ML solution provides its expected business value. In order to do that, one
has to bridge the gap between the way ML model performance is measured and the
solution requirements. In previous work (Barash et al, "Bridging the gap...")
we demonstrated the effectiveness of utilizing feature models in bridging this
gap. Whereas ML performance metrics, such as the accuracy or F1-score of a
classifier, typically measure the average ML performance, feature models shed
light on explainable data slices that are too far from that average, and
therefore might indicate unsatisfied requirements. For example, the overall
accuracy of a bank text terms classifier may be very high, say $98\% \pm 2\%$,
yet it might perform poorly for terms that include short descriptions and
originate from commercial accounts. A business requirement, which may be
implicit in the training data, may be to perform well regardless of the type of
account and length of the description. Therefore, the under-performing data
slice that includes short descriptions and commercial accounts suggests
poorly-met requirements. In this paper we show the feasibility of automatically
extracting feature models that result in explainable data slices over which the
ML solution under-performs. Our novel technique, IBM FreaAI aka FreaAI,
extracts such slices from structured ML test data or any other labeled data. We
demonstrate that FreaAI can automatically produce explainable and
statistically-significant data slices over seven open datasets.

    

### [[2108.05643] On minimal representations of shallow ReLU networks](http://arxiv.org/abs/2108.05643)


  The realization function of a shallow ReLU network is a continuous and
piecewise affine function $f:\mathbb R^d\to \mathbb R$, where the domain
$\mathbb R^{d}$ is partitioned by a set of $n$ hyperplanes into cells on which
$f$ is affine. We show that the minimal representation for $f$ uses either $n$,
$n+1$ or $n+2$ neurons and we characterize each of the three cases. In the
particular case, where the input layer is one-dimensional, minimal
representations always use at most $n+1$ neurons but in all higher dimensional
settings there are functions for which $n+2$ neurons are needed. Then we show
that the set of minimal networks representing $f$ forms a
$C^\infty$-submanifold $M$ and we derive the dimension and the number of
connected components of $M$. Additionally, we give a criterion for the
hyperplanes that guarantees that all continuous, piecewise affine functions are
realization functions of appropriate ReLU networks.

    

### [[2108.05647] DARTS for Inverse Problems: a Study on Hyperparameter Sensitivity](http://arxiv.org/abs/2108.05647)


  Differentiable architecture search (DARTS) is a widely researched tool for
neural architecture search, due to its promising results for image
classification. The main benefit of DARTS is the effectiveness achieved through
the weight-sharing one-shot paradigm, which allows efficient architecture
search. In this work, we investigate DARTS in a systematic case study of
inverse problems, which allows us to analyze these potential benefits in a
controlled manner. Although we demonstrate that the success of DARTS can be
extended from image classification to reconstruction, our experiments yield
three fundamental difficulties in the evaluation of DARTS-based methods: First,
the results show a large variance in all test cases. Second, the final
performance is highly dependent on the hyperparameters of the optimizer. And
third, the performance of the weight-sharing architecture used during training
does not reflect the final performance of the found architecture well. Thus, we
conclude the necessity to 1) report the results of any DARTS-based methods from
several runs along with its underlying performance statistics, 2) show the
correlation of the training and final architecture performance, and 3)
carefully consider if the computational efficiency of DARTS outweighs the costs
of hyperparameter optimization and multiple runs.

    

### [[2108.05649] Resetting the baseline: CT-based COVID-19 diagnosis with Deep Transfer Learning is not as accurate as widely thought](http://arxiv.org/abs/2108.05649)


  Deep learning is gaining instant popularity in computer aided diagnosis of
COVID-19. Due to the high sensitivity of Computed Tomography (CT) to this
disease, CT-based COVID-19 detection with visual models is currently at the
forefront of medical imaging research. Outcomes published in this direction are
frequently claiming highly accurate detection under deep transfer learning.
This is leading medical technologists to believe that deep transfer learning is
the mainstream solution for the problem. However, our critical analysis of the
literature reveals an alarming performance disparity between different
published results. Hence, we conduct a systematic thorough investigation to
analyze the effectiveness of deep transfer learning for COVID-19 detection with
CT images. Exploring 14 state-of-the-art visual models with over 200 model
training sessions, we conclusively establish that the published literature is
frequently overestimating transfer learning performance for the problem, even
in the prestigious scientific sources. The roots of overestimation trace back
to inappropriate data curation. We also provide case studies that consider more
realistic scenarios, and establish transparent baselines for the problem. We
hope that our reproducible investigation will help in curbing hype-driven
claims for the critical problem of COVID-19 diagnosis, and pave the way for a
more transparent performance evaluation of techniques for CT-based COVID-19
detection.

    

### [[2108.05660] Development of Risk-Free COVID-19 Screening Algorithm from Routine Blood Test using Ensemble Machine Learning](http://arxiv.org/abs/2108.05660)


  The Reverse Transcription Polymerase Chain Reaction (RTPCR) test is the
silver bullet diagnostic test to discern COVID infection. Rapid antigen
detection is a screening test to identify COVID positive patients in little as
15 minutes, but has a lower sensitivity than the PCR tests. Besides having
multiple standardized test kits, many people are getting infected & either
recovering or dying even before the test due to the shortage and cost of kits,
lack of indispensable specialists and labs, time-consuming result compared to
bulk population especially in developing and underdeveloped countries.
Intrigued by the parametric deviations in immunological & hematological profile
of a COVID patient, this research work leveraged the concept of COVID-19
detection by proposing a risk-free and highly accurate Stacked Ensemble Machine
Learning model to identify a COVID patient from communally
available-widespread-cheap routine blood tests which gives a promising
accuracy, precision, recall & F1-score of 100%. Analysis from R-curve also
shows the preciseness of the risk-free model to be implemented. The proposed
method has the potential for large scale ubiquitous low-cost screening
application. This can add an extra layer of protection in keeping the number of
infected cases to a minimum and control the pandemic by identifying
asymptomatic or pre-symptomatic people early.

    

### [[2108.05670] Communication Optimization in Large Scale Federated Learning using Autoencoder Compressed Weight Updates](http://arxiv.org/abs/2108.05670)


  Federated Learning (FL) solves many of this decade's concerns regarding data
privacy and computation challenges. FL ensures no data leaves its source as the
model is trained at where the data resides. However, FL comes with its own set
of challenges. The communication of model weight updates in this distributed
environment comes with significant network bandwidth costs. In this context, we
propose a mechanism of compressing the weight updates using Autoencoders (AE),
which learn the data features of the weight updates and subsequently perform
compression. The encoder is set up on each of the nodes where the training is
performed while the decoder is set up on the node where the weights are
aggregated. This setup achieves compression through the encoder and recreates
the weights at the end of every communication round using the decoder. This
paper shows that the dynamic and orthogonal AE based weight compression
technique could serve as an advantageous alternative (or an add-on) in a large
scale FL, as it not only achieves compression ratios ranging from 500x to 1720x
and beyond, but can also be modified based on the accuracy requirements,
computational capacity, and other requirements of the given FL setup.

    

### [[2108.05677] How Nonconformity Functions and Difficulty of Datasets Impact the Efficiency of Conformal Classifiers](http://arxiv.org/abs/2108.05677)


  The property of conformal predictors to guarantee the required accuracy rate
makes this framework attractive in various practical applications. However,
this property is achieved at a price of reduction in precision. In the case of
conformal classification, the systems can output multiple class labels instead
of one. It is also known from the literature, that the choice of nonconformity
function has a major impact on the efficiency of conformal classifiers.
Recently, it was shown that different model-agnostic nonconformity functions
result in conformal classifiers with different characteristics. For a Neural
Network-based conformal classifier, the inverse probability (or hinge loss)
allows minimizing the average number of predicted labels, and margin results in
a larger fraction of singleton predictions. In this work, we aim to further
extend this study. We perform an experimental evaluation using 8 different
classification algorithms and discuss when the previously observed relationship
holds or not. Additionally, we propose a successful method to combine the
properties of these two nonconformity functions. The experimental evaluation is
done using 11 real and 5 synthetic datasets.

    

### [[2108.05681] Semantics-Native Communication with Contextual Reasoning](http://arxiv.org/abs/2108.05681)


  Spurred by a huge interest in the post-Shannon communication, it has recently
been shown that leveraging semantics can significantly improve the
communication effectiveness across many tasks. In this article, inspired by
human communication, we propose a novel stochastic model of System 1
semantics-native communication (SNC) for generic tasks, where a speaker has an
intention of referring to an entity, extracts the semantics, and communicates
its symbolic representation to a target listener. To further reach its full
potential, we additionally infuse contextual reasoning into SNC such that the
speaker locally and iteratively self-communicates with a virtual agent built on
the physical listener's unique way of coding its semantics, i.e., communication
context. The resultant System 2 SNC allows the speaker to extract the most
effective semantics for its listener. Leveraging the proposed stochastic model,
we show that the reliability of System 2 SNC increases with the number of
meaningful concepts, and derive the expected semantic representation (SR) bit
length which quantifies the extracted effective semantics. It is also shown
that System 2 SNC significantly reduces the SR length without compromising
communication reliability.

    

### [[2108.05684] RW-Resnet: A Novel Speech Anti-Spoofing Model Using Raw Waveform](http://arxiv.org/abs/2108.05684)


  In recent years, synthetic speech generated by advanced text-to-speech (TTS)
and voice conversion (VC) systems has caused great harms to automatic speaker
verification (ASV) systems, urging us to design a synthetic speech detection
system to protect ASV systems. In this paper, we propose a new speech
anti-spoofing model named ResWavegram-Resnet (RW-Resnet). The model contains
two parts, Conv1D Resblocks and backbone Resnet34. The Conv1D Resblock is based
on the Conv1D block with a residual connection. For the first part, we use the
raw waveform as input and feed it to the stacked Conv1D Resblocks to get the
ResWavegram. Compared with traditional methods, ResWavegram keeps all the
information from the audio signal and has a stronger ability in extracting
features. For the second part, the extracted features are fed to the backbone
Resnet34 for the spoofed or bonafide decision. The ASVspoof2019 logical access
(LA) corpus is used to evaluate our proposed RW-Resnet. Experimental results
show that the RW-Resnet achieves better performance than other state-of-the-art
anti-spoofing models, which illustrates its effectiveness in detecting
synthetic speech attacks.

    

### [[2108.05696] Correlation Clustering with Asymmetric Classification Errors](http://arxiv.org/abs/2108.05696)


  In the Correlation Clustering problem, we are given a weighted graph $G$ with
its edges labeled as "similar" or "dissimilar" by a binary classifier. The goal
is to produce a clustering that minimizes the weight of "disagreements": the
sum of the weights of "similar" edges across clusters and "dissimilar" edges
within clusters. We study the correlation clustering problem under the
following assumption: Every "similar" edge $e$ has weight
$\mathbf{w}_e\in[\alpha \mathbf{w}, \mathbf{w}]$ and every "dissimilar" edge
$e$ has weight $\mathbf{w}_e\geq \alpha \mathbf{w}$ (where $\alpha\leq 1$ and
$\mathbf{w}>0$ is a scaling parameter). We give a $(3 + 2 \log_e (1/\alpha))$
approximation algorithm for this problem. This assumption captures well the
scenario when classification errors are asymmetric. Additionally, we show an
asymptotically matching Linear Programming integrality gap of $\Omega(\log
1/\alpha)$.

    

### [[2108.05697] Local Correlation Clustering with Asymmetric Classification Errors](http://arxiv.org/abs/2108.05697)


  In the Correlation Clustering problem, we are given a complete weighted graph
$G$ with its edges labeled as "similar" and "dissimilar" by a noisy binary
classifier. For a clustering $\mathcal{C}$ of graph $G$, a similar edge is in
disagreement with $\mathcal{C}$, if its endpoints belong to distinct clusters;
and a dissimilar edge is in disagreement with $\mathcal{C}$ if its endpoints
belong to the same cluster. The disagreements vector, $\text{dis}$, is a vector
indexed by the vertices of $G$ such that the $v$-th coordinate $\text{dis}_v$
equals the weight of all disagreeing edges incident on $v$. The goal is to
produce a clustering that minimizes the $\ell_p$ norm of the disagreements
vector for $p\geq 1$. We study the $\ell_p$ objective in Correlation Clustering
under the following assumption: Every similar edge has weight in the range of
$[\alpha\mathbf{w},\mathbf{w}]$ and every dissimilar edge has weight at least
$\alpha\mathbf{w}$ (where $\alpha \leq 1$ and $\mathbf{w}>0$ is a scaling
parameter). We give an
$O\left((\frac{1}{\alpha})^{\frac{1}{2}-\frac{1}{2p}}\cdot
\log\frac{1}{\alpha}\right)$ approximation algorithm for this problem.
Furthermore, we show an almost matching convex programming integrality gap.

    

### [[2108.05698] Preventing Catastrophic Forgetting and Distribution Mismatch in Knowledge Distillation via Synthetic Data](http://arxiv.org/abs/2108.05698)


  With the increasing popularity of deep learning on edge devices, compressing
large neural networks to meet the hardware requirements of resource-constrained
devices became a significant research direction. Numerous compression
methodologies are currently being used to reduce the memory sizes and energy
consumption of neural networks. Knowledge distillation (KD) is among such
methodologies and it functions by using data samples to transfer the knowledge
captured by a large model (teacher) to a smaller one(student). However, due to
various reasons, the original training data might not be accessible at the
compression stage. Therefore, data-free model compression is an ongoing
research problem that has been addressed by various works. In this paper, we
point out that catastrophic forgetting is a problem that can potentially be
observed in existing data-free distillation methods. Moreover, the sample
generation strategies in some of these methods could result in a mismatch
between the synthetic and real data distributions. To prevent such problems, we
propose a data-free KD framework that maintains a dynamic collection of
generated samples over time. Additionally, we add the constraint of matching
the real data distribution in sample generation strategies that target maximum
information gain. Our experiments demonstrate that we can improve the accuracy
of the student models obtained via KD when compared with state-of-the-art
approaches on the SVHN, Fashion MNIST and CIFAR100 datasets.

    

### [[2108.05701] An Approach to Partial Observability in Games: Learning to Both Act and Observe](http://arxiv.org/abs/2108.05701)


  Reinforcement learning (RL) is successful at learning to play games where the
entire environment is visible. However, RL approaches are challenged in complex
games like Starcraft II and in real-world environments where the entire
environment is not visible. In these more complex games with more limited
visual information, agents must choose where to look and how to optimally use
their limited visual information in order to succeed at the game. We verify
that with a relatively simple model the agent can learn where to look in
scenarios with a limited visual bandwidth. We develop a method for masking part
of the environment in Atari games to force the RL agent to learn both where to
look and how to play the game in order to study where the RL agent learns to
look. In addition, we develop a neural network architecture and method for
allowing the agent to choose where to look and what action to take in the Pong
game. Further, we analyze the strategies the agent learns to better understand
how the RL agent learns to play the game.

    

### [[2108.05713] Towards real-world navigation with deep differentiable planners](http://arxiv.org/abs/2108.05713)


  We train embodied neural networks to plan and navigate unseen complex 3D
environments, emphasising real-world deployment. Rather than requiring prior
knowledge of the agent or environment, the planner learns to model the state
transitions and rewards. To avoid the potentially hazardous trial-and-error of
reinforcement learning, we focus on differentiable planners such as Value
Iteration Networks (VIN), which are trained offline from safe expert
demonstrations. Although they work well in small simulations, we address two
major limitations that hinder their deployment. First, we observed that current
differentiable planners struggle to plan long-term in environments with a high
branching complexity. While they should ideally learn to assign low rewards to
obstacles to avoid collisions, we posit that the constraints imposed on the
network are not strong enough to guarantee the network to learn sufficiently
large penalties for every possible collision. We thus impose a structural
constraint on the value iteration, which explicitly learns to model any
impossible actions. Secondly, we extend the model to work with a limited
perspective camera under translation and rotation, which is crucial for real
robot deployment. Many VIN-like planners assume a 360 degrees or overhead view
without rotation. In contrast, our method uses a memory-efficient lattice map
to aggregate CNN embeddings of partial observations, and models the rotational
dynamics explicitly using a 3D state-space grid (translation and rotation). Our
proposals significantly improve semantic navigation and exploration on several
2D and 3D environments, succeeding in settings that are otherwise challenging
for this class of methods. As far as we know, we are the first to successfully
perform differentiable planning on the difficult Active Vision Dataset,
consisting of real images captured from a robot.

    

### [[2108.05717] Engineering an Efficient Boolean Functional Synthesis Engine](http://arxiv.org/abs/2108.05717)


  Given a Boolean specification between a set of inputs and outputs, the
problem of Boolean functional synthesis is to synthesise each output as a
function of inputs such that the specification is met. Although the past few
years have witnessed intense algorithmic development, accomplishing scalability
remains the holy grail. The state-of-the-art approach combines machine learning
and automated reasoning to efficiently synthesise Boolean functions. In this
paper, we propose four algorithmic improvements for a data-driven framework for
functional synthesis: using a dependency-driven multi-classifier to learn
candidate function, extracting uniquely defined functions by interpolation,
variables retention, and using lexicographic MaxSAT to repair candidates. We
implement these improvements in the state-of-the-art framework, called Manthan.
The proposed framework is called Manthan2. Manthan2 shows significantly
improved runtime performance compared to Manthan. In an extensive experimental
evaluation on 609 benchmarks, Manthan2 is able to synthesise a Boolean function
vector for 509 instances compared to 356 instances solved by Manthan--- an
increment of 153 instances over the state-of-the-art. To put this into
perspective, Manthan improved on the prior state-of-the-art by only 76
instances.

    

### [[2108.05722] MT-ORL: Multi-Task Occlusion Relationship Learning](http://arxiv.org/abs/2108.05722)


  Retrieving occlusion relation among objects in a single image is challenging
due to sparsity of boundaries in image. We observe two key issues in existing
works: firstly, lack of an architecture which can exploit the limited amount of
coupling in the decoder stage between the two subtasks, namely occlusion
boundary extraction and occlusion orientation prediction, and secondly,
improper representation of occlusion orientation. In this paper, we propose a
novel architecture called Occlusion-shared and Path-separated Network (OPNet),
which solves the first issue by exploiting rich occlusion cues in shared
high-level features and structured spatial information in task-specific
low-level features. We then design a simple but effective orthogonal occlusion
representation (OOR) to tackle the second issue. Our method surpasses the
state-of-the-art methods by 6.1%/8.3% Boundary-AP and 6.5%/10% Orientation-AP
on standard PIOD/BSDS ownership datasets. Code is available at
this https URL.

    

### [[2108.05732] Deep Microlocal Reconstruction for Limited-Angle Tomography](http://arxiv.org/abs/2108.05732)


  We present a deep learning-based algorithm to jointly solve a reconstruction
problem and a wavefront set extraction problem in tomographic imaging. The
algorithm is based on a recently developed digital wavefront set extractor as
well as the well-known microlocal canonical relation for the Radon transform.
We use the wavefront set information about x-ray data to improve the
reconstruction by requiring that the underlying neural networks simultaneously
extract the correct ground truth wavefront set and ground truth image. As a
necessary theoretical step, we identify the digital microlocal canonical
relations for deep convolutional residual neural networks. We find strong
numerical evidence for the effectiveness of this approach.

    

### [[2108.05761] Analyzing hierarchical multi-view MRI data with StaPLR: An application to Alzheimer's disease classification](http://arxiv.org/abs/2108.05761)


  Multi-view data refers to a setting where features are divided into feature
sets, for example because they correspond to different sources. Stacked
penalized logistic regression (StaPLR) is a recently introduced method that can
be used for classification and automatically selecting the views that are most
important for prediction. We show how this method can easily be extended to a
setting where the data has a hierarchical multi-view structure. We apply StaPLR
to Alzheimer's disease classification where different MRI measures have been
calculated from three scan types: structural MRI, diffusion-weighted MRI, and
resting-state fMRI. StaPLR can identify which scan types and which MRI measures
are most important for classification, and it outperforms elastic net
regression in classification performance.

    

### [[2108.05762] Multimodal analysis of the predictability of hand-gesture properties](http://arxiv.org/abs/2108.05762)


  Embodied conversational agents benefit from being able to accompany their
speech with gestures. Although many data-driven approaches to gesture
generation have been proposed in recent years, it is still unclear whether such
systems can consistently generate gestures that convey meaning. We investigate
which gesture properties (phase, category, and semantics) can be predicted from
speech text and/or audio using contemporary deep learning. In extensive
experiments, we show that gesture properties related to gesture meaning
(semantics and category) are predictable from text features (time-aligned BERT
embeddings) alone, but not from prosodic audio features, while rhythm-related
gesture properties (phase) on the other hand can be predicted from either
audio, text (with word-level timing information), or both. These results are
encouraging as they indicate that it is possible to equip an embodied agent
with content-wise meaningful co-speech gestures using a machine-learning model.

    

### [[2108.05765] Dynamic Attention-based Communication-Efficient Federated Learning](http://arxiv.org/abs/2108.05765)


  Federated learning (FL) offers a solution to train a global machine learning
model while still maintaining data privacy, without needing access to data
stored locally at the clients. However, FL suffers performance degradation when
client data distribution is non-IID, and a longer training duration to combat
this degradation may not necessarily be feasible due to communication
limitations. To address this challenge, we propose a new adaptive training
algorithm $\texttt{AdaFL}$, which comprises two components: (i) an
attention-based client selection mechanism for a fairer training scheme among
the clients; and (ii) a dynamic fraction method to balance the trade-off
between performance stability and communication efficiency. Experimental
results show that our $\texttt{AdaFL}$ algorithm outperforms the usual
$\texttt{FedAvg}$ algorithm, and can be incorporated to further improve various
state-of-the-art FL algorithms, with respect to three aspects: model accuracy,
performance stability, and communication efficiency.

    

### [[2108.05773] Correlate-and-Excite: Real-Time Stereo Matching via Guided Cost Volume Excitation](http://arxiv.org/abs/2108.05773)


  Volumetric deep learning approach towards stereo matching aggregates a cost
volume computed from input left and right images using 3D convolutions. Recent
works showed that utilization of extracted image features and a spatially
varying cost volume aggregation complements 3D convolutions. However, existing
methods with spatially varying operations are complex, cost considerable
computation time, and cause memory consumption to increase. In this work, we
construct Guided Cost volume Excitation (GCE) and show that simple channel
excitation of cost volume guided by image can improve performance considerably.
Moreover, we propose a novel method of using top-k selection prior to
soft-argmin disparity regression for computing the final disparity estimate.
Combining our novel contributions, we present an end-to-end network that we
call Correlate-and-Excite (CoEx). Extensive experiments of our model on the
SceneFlow, KITTI 2012, and KITTI 2015 datasets demonstrate the effectiveness
and efficiency of our model and show that our model outperforms other
speed-based algorithms while also being competitive to other state-of-the-art
algorithms. Codes will be made available at this https URL.

    

### [[2108.05774] HopfE: Knowledge Graph Representation Learning using Inverse Hopf Fibrations](http://arxiv.org/abs/2108.05774)


  Recently, several Knowledge Graph Embedding (KGE) approaches have been
devised to represent entities and relations in dense vector space and employed
in downstream tasks such as link prediction. A few KGE techniques address
interpretability, i.e., mapping the connectivity patterns of the relations
(i.e., symmetric/asymmetric, inverse, and composition) to a geometric
interpretation such as rotations. Other approaches model the representations in
higher dimensional space such as four-dimensional space (4D) to enhance the
ability to infer the connectivity patterns (i.e., expressiveness). However,
modeling relation and entity in a 4D space often comes at the cost of
interpretability. This paper proposes HopfE, a novel KGE approach aiming to
achieve the interpretability of inferred relations in the four-dimensional
space. We first model the structural embeddings in 3D Euclidean space and view
the relation operator as an SO(3) rotation. Next, we map the entity embedding
vector from a 3D space to a 4D hypersphere using the inverse Hopf Fibration, in
which we embed the semantic information from the KG ontology. Thus, HopfE
considers the structural and semantic properties of the entities without losing
expressivity and interpretability. Our empirical results on four well-known
benchmarks achieve state-of-the-art performance for the KG completion task.

    

### [[2108.05776] Learning from Matured Dumb Teacher for Fine Generalization](http://arxiv.org/abs/2108.05776)


  The flexibility of decision boundaries in neural networks that are unguided
by training data is a well-known problem typically resolved with generalization
methods. A surprising result from recent knowledge distillation (KD) literature
is that random, untrained, and equally structured teacher networks can also
vastly improve generalization performance. It raises the possibility of
existence of undiscovered assumptions useful for generalization on an uncertain
region. In this paper, we shed light on the assumptions by analyzing decision
boundaries and confidence distributions of both simple and KD-based
generalization methods. Assuming that a decision boundary exists to represent
the most general tendency of distinction on an input sample space (i.e., the
simplest hypothesis), we show the various limitations of methods when using the
hypothesis. To resolve these limitations, we propose matured dumb teacher based
KD, conservatively transferring the hypothesis for generalization of the
student without massive destruction of trained information. In practical
experiments on feed-forward and convolution neural networks for image
classification tasks on MNIST, CIFAR-10, and CIFAR-100 datasets, the proposed
method shows stable improvement to the best test performance in the grid search
of hyperparameters. The analysis and results imply that the proposed method can
provide finer generalization than existing methods.

    

### [[2108.05796] Goal scoring in Premier League with Poisson regression](http://arxiv.org/abs/2108.05796)


  Premier League is known as one of the most competitive football league in the
world, hence there are many goals are scored here every match. Which are the
factors that affect to the number of goal scored in each match? We use Poisson
regression to find out the relation between many factors as shots on target,
corners, red cards, to the goals home team can score in their match.

    

### [[2108.05801] A Hybrid Learning Approach to Detecting Regime Switches in Financial Markets](http://arxiv.org/abs/2108.05801)


  Financial markets are of much interest to researchers due to their dynamic
and stochastic nature. With their relations to world populations, global
economies and asset valuations, understanding, identifying and forecasting
trends and regimes are highly important. Attempts have been made to forecast
market trends by employing machine learning methodologies, while statistical
techniques have been the primary methods used in developing market regime
switching models used for trading and hedging. In this paper we present a novel
framework for the detection of regime switches within the US financial markets.
Principal component analysis is applied for dimensionality reduction and the
k-means algorithm is used as a clustering technique. Using a combination of
cluster analysis and classification, we identify regimes in financial markets
based on publicly available economic data. We display the efficacy of the
framework by constructing and assessing the performance of two trading
strategies based on detected regimes.

    

### [[2108.05805] Reimagining an autonomous vehicle](http://arxiv.org/abs/2108.05805)


  The self driving challenge in 2021 is this century's technological equivalent
of the space race, and is now entering the second major decade of development.
Solving the technology will create social change which parallels the invention
of the automobile itself. Today's autonomous driving technology is laudable,
though rooted in decisions made a decade ago. We argue that a rethink is
required, reconsidering the autonomous vehicle (AV) problem in the light of the
body of knowledge that has been gained since the DARPA challenges which seeded
the industry. What does AV2.0 look like? We present an alternative vision: a
recipe for driving with machine learning, and grand challenges for research in
driving.

    

### [[2108.05814] Decoder Fusion RNN: Context and Interaction Aware Decoders for Trajectory Prediction](http://arxiv.org/abs/2108.05814)


  Forecasting the future behavior of all traffic agents in the vicinity is a
key task to achieve safe and reliable autonomous driving systems. It is a
challenging problem as agents adjust their behavior depending on their
intentions, the others' actions, and the road layout. In this paper, we propose
Decoder Fusion RNN (DF-RNN), a recurrent, attention-based approach for motion
forecasting. Our network is composed of a recurrent behavior encoder, an
inter-agent multi-headed attention module, and a context-aware decoder. We
design a map encoder that embeds polyline segments, combines them to create a
graph structure, and merges their relevant parts with the agents' embeddings.
We fuse the encoded map information with further inter-agent interactions only
inside the decoder and propose to use explicit training as a method to
effectively utilize the information available. We demonstrate the efficacy of
our method by testing it on the Argoverse motion forecasting dataset and show
its state-of-the-art performance on the public benchmark.

    

### [[2108.05818] PatrickStar: Parallel Training of Pre-trained Models via a Chunk-based Memory Management](http://arxiv.org/abs/2108.05818)


  The pre-trained model (PTM) is revolutionizing Artificial intelligence (AI)
technology. It learns a model with general language features on the vast text
and then fine-tunes the model using a task-specific dataset. Unfortunately, PTM
training requires prohibitively expensive computing devices, especially
fine-tuning, which is still a game for a small proportion of people in the AI
community. Enabling PTMs training on low-quality devices, PatrickStar now makes
PTM accessible to everyone.
PatrickStar reduces memory requirements of computing platforms by using the
CPU-GPU heterogeneous memory space to store model data, consisting of
parameters, gradients, and optimizer states. We observe that the GPU memory
available for model data changes regularly, in a tide-like pattern, decreasing
and increasing iteratively. However, the existing heterogeneous training works
do not take advantage of this pattern. Instead, they statically partition the
model data among CPU and GPU, leading to both memory waste and memory abuse. In
contrast, PatrickStar manages model data in chunks, which are dynamically
distributed in heterogeneous memory spaces. Chunks consist of stateful tensors
which run as finite state machines during training. Guided by the runtime
memory statistics collected in a warm-up iteration, chunks are orchestrated
efficiently in heterogeneous memory and generate lower CPU-GPU data
transmission volume. Symbiosis with the Zero Redundancy Optimizer, PatrickStar
scales to multiple GPUs using data parallelism, with the lowest communication
bandwidth requirements and more efficient bandwidth utilization. Experimental
results show PatrickStar trains a 12 billion parameters GPT model, 2x larger
than the STOA work, on an 8-V100 and 240GB CPU memory node, and is also more
efficient on the same model size.

    

### [[2108.05828] A functional mirror ascent view of policy gradient methods with function approximation](http://arxiv.org/abs/2108.05828)


  We use functional mirror ascent to propose a general framework (referred to
as FMA-PG) for designing policy gradient methods. The functional perspective
distinguishes between a policy's functional representation (what are its
sufficient statistics) and its parameterization (how are these statistics
represented) and naturally results in computationally efficient off-policy
updates. For simple policy parameterizations, the FMA-PG framework ensures that
the optimal policy is a fixed point of the updates. It also allows us to handle
complex policy parameterizations (e.g., neural networks) while guaranteeing
policy improvement. Our framework unifies several PG methods and opens the way
for designing sample-efficient variants of existing methods. Moreover, it
recovers important implementation heuristics (e.g., using forward vs reverse KL
divergence) in a principled way. With a softmax functional representation,
FMA-PG results in a variant of TRPO with additional desirable properties. It
also suggests an improved variant of PPO, whose robustness and efficiency we
empirically demonstrate on MuJoCo. Via experiments on simple reinforcement
learning problems, we evaluate algorithms instantiated by FMA-PG.

    

### [[2108.05839] Logit Attenuating Weight Normalization](http://arxiv.org/abs/2108.05839)


  Over-parameterized deep networks trained using gradient-based optimizers are
a popular choice for solving classification and ranking problems. Without
appropriately tuned $\ell_2$ regularization or weight decay, such networks have
the tendency to make output scores (logits) and network weights large, causing
training loss to become too small and the network to lose its adaptivity
(ability to move around) in the parameter space. Although regularization is
typically understood from an overfitting perspective, we highlight its role in
making the network more adaptive and enabling it to escape more easily from
weights that generalize poorly. To provide such a capability, we propose a
method called Logit Attenuating Weight Normalization (LAWN), that can be
stacked onto any gradient-based optimizer. LAWN controls the logits by
constraining the weight norms of layers in the final homogeneous sub-network.
Empirically, we show that the resulting LAWN variant of the optimizer makes a
deep network more adaptive to finding minimas with superior generalization
performance on large-scale image classification and recommender systems. While
LAWN is particularly impressive in improving Adam, it greatly improves all
optimizers when used with large batch sizes

    

### [[2108.05862] m-RevNet: Deep Reversible Neural Networks with Momentum](http://arxiv.org/abs/2108.05862)


  In recent years, the connections between deep residual networks and
first-order Ordinary Differential Equations (ODEs) have been disclosed. In this
work, we further bridge the deep neural architecture design with the
second-order ODEs and propose a novel reversible neural network, termed as
m-RevNet, that is characterized by inserting momentum update to residual
blocks. The reversible property allows us to perform backward pass without
access to activation values of the forward pass, greatly relieving the storage
burden during training. Furthermore, the theoretical foundation based on
second-order ODEs grants m-RevNet with stronger representational power than
vanilla residual networks, which potentially explains its performance gains.
For certain learning scenarios, we analytically and empirically reveal that our
m-RevNet succeeds while standard ResNet fails. Comprehensive experiments on
various image classification and semantic segmentation benchmarks demonstrate
the superiority of our m-RevNet over ResNet, concerning both memory efficiency
and recognition performance.

    

### [[2108.05875] Distributional Depth-Based Estimation of Object Articulation Models](http://arxiv.org/abs/2108.05875)


  We propose a method that efficiently learns distributions over articulation
model parameters directly from depth images without the need to know
articulation model categories a priori. By contrast, existing methods that
learn articulation models from raw observations typically only predict point
estimates of the model parameters, which are insufficient to guarantee the safe
manipulation of articulated objects. Our core contributions include a novel
representation for distributions over rigid body transformations and
articulation model parameters based on screw theory, von Mises-Fisher
distributions, and Stiefel manifolds. Combining these concepts allows for an
efficient, mathematically sound representation that implicitly satisfies the
constraints that rigid body transformations and articulations must adhere to.
Leveraging this representation, we introduce a novel deep learning based
approach, DUST-net, that performs category-independent articulation model
estimation while also providing model uncertainties. We evaluate our approach
on several benchmarking datasets and real-world objects and compare its
performance with two current state-of-the-art methods. Our results demonstrate
that DUST-net can successfully learn distributions over articulation models for
novel objects across articulation model categories, which generate point
estimates with better accuracy than state-of-the-art methods and effectively
capture the uncertainty over predicted model parameters due to noisy inputs.

    

### [[2108.05877] DexMV: Imitation Learning for Dexterous Manipulation from Human Videos](http://arxiv.org/abs/2108.05877)


  While we have made significant progress on understanding hand-object
interactions in computer vision, it is still very challenging for robots to
perform complex dexterous manipulation. In this paper, we propose a new
platform and pipeline, DexMV (Dex Manipulation from Videos), for imitation
learning to bridge the gap between computer vision and robot learning. We
design a platform with: (i) a simulation system for complex dexterous
manipulation tasks with a multi-finger robot hand and (ii) a computer vision
system to record large-scale demonstrations of a human hand conducting the same
tasks. In our new pipeline, we extract 3D hand and object poses from the
videos, and convert them to robot demonstrations via motion retargeting. We
then apply and compare multiple imitation learning algorithms with the
demonstrations. We show that the demonstrations can indeed improve robot
learning by a large margin and solve the complex tasks which reinforcement
learning alone cannot solve. Project page with video:
this https URL


### [[2108.05879] Feature Engineering with Regularity Structures](http://arxiv.org/abs/2108.05879)


  We investigate the use of models from the theory of regularity structure as
features in machine learning tasks. A model is a multi-linear function of a
space-time signal designed to well-approximate solutions to partial
differential equations (PDEs), even in low regularity regimes. Models can be
seen as natural multi-dimensional generalisations of signatures of paths; our
work therefore aims to extend the recent use of signatures in data science
beyond the context of time-ordered data. We provide a flexible definition of a
model feature vector associated to a space-time signal, along with two
algorithms which illustrate ways in which these features can be combined with
linear regression. We apply these algorithms in several numerical experiments
designed to learn solutions to PDEs with a given forcing and boundary data. Our
experiments include semi-linear parabolic and wave equations with forcing, and
Burgers' equation with no forcing. We find an advantage in favour of our
algorithms when compared to several alternative methods. Additionally, in the
experiment with Burgers' equation, we noticed stability in the prediction power
when noise is added to the observations.

    

### [[2108.05885] The paradox of the compositionality of natural language: a neural machine translation case study](http://arxiv.org/abs/2108.05885)


  Moving towards human-like linguistic performance is often argued to require
compositional generalisation. Whether neural networks exhibit this ability is
typically studied using artificial languages, for which the compositionality of
input fragments can be guaranteed and their meanings algebraically composed.
However, compositionality in natural language is vastly more complex than this
rigid, arithmetics-like version of compositionality, and as such artificial
compositionality tests do not allow us to draw conclusions about how neural
models deal with compositionality in more realistic scenarios. In this work, we
re-instantiate three compositionality tests from the literature and reformulate
them for neural machine translation (NMT). The results highlight two main
issues: the inconsistent behaviour of NMT models and their inability to
(correctly) modulate between local and global processing. Aside from an
empirical study, our work is a call to action: we should rethink the evaluation
of compositionality in neural networks of natural language, where composing
meaning is not as straightforward as doing the math.

    

### [[2108.05887] Billion-Scale Pretraining with Vision Transformers for Multi-Task Visual Representations](http://arxiv.org/abs/2108.05887)


  Large-scale pretraining of visual representations has led to state-of-the-art
performance on a range of benchmark computer vision tasks, yet the benefits of
these techniques at extreme scale in complex production systems has been
relatively unexplored. We consider the case of a popular visual discovery
product, where these representations are trained with multi-task learning, from
use-case specific visual understanding (e.g. skin tone classification) to
general representation learning for all visual content (e.g. embeddings for
retrieval). In this work, we describe how we (1) generate a dataset with over a
billion images via large weakly-supervised pretraining to improve the
performance of these visual representations, and (2) leverage Transformers to
replace the traditional convolutional backbone, with insights into both system
and performance improvements, especially at 1B+ image scale. To support this
backbone model, we detail a systematic approach to deriving weakly-supervised
image annotations from heterogenous text signals, demonstrating the benefits of
clustering techniques to handle the long-tail distribution of image labels.
Through a comprehensive study of offline and online evaluation, we show that
large-scale Transformer-based pretraining provides significant benefits to
industry computer vision applications. The model is deployed in a production
visual shopping system, with 36% improvement in top-1 relevance and 23%
improvement in click-through volume. We conduct extensive experiments to better
understand the empirical relationships between Transformer-based architectures,
dataset scale, and the performance of production vision systems.

    

### [[2108.05889] Towards Interpretable Deep Metric Learning with Structural Matching](http://arxiv.org/abs/2108.05889)


  How do the neural networks distinguish two images? It is of critical
importance to understand the matching mechanism of deep models for developing
reliable intelligent systems for many risky visual applications such as
surveillance and access control. However, most existing deep metric learning
methods match the images by comparing feature vectors, which ignores the
spatial structure of images and thus lacks interpretability. In this paper, we
present a deep interpretable metric learning (DIML) method for more transparent
embedding learning. Unlike conventional metric learning methods based on
feature vector comparison, we propose a structural matching strategy that
explicitly aligns the spatial embeddings by computing an optimal matching flow
between feature maps of the two images. Our method enables deep models to learn
metrics in a more human-friendly way, where the similarity of two images can be
decomposed to several part-wise similarities and their contributions to the
overall similarity. Our method is model-agnostic, which can be applied to
off-the-shelf backbone networks and metric learning methods. We evaluate our
method on three major benchmarks of deep metric learning including CUB200-2011,
Cars196, and Stanford Online Products, and achieve substantial improvements
over popular metric learning methods with better interpretability. Code is
available at this https URL


### [[2108.05891] Page-level Optimization of e-Commerce Item Recommendations](http://arxiv.org/abs/2108.05891)


  The item details page (IDP) is a web page on an e-commerce website that
provides information on a specific product or item listing. Just below the
details of the item on this page, the buyer can usually find recommendations
for other relevant items. These are typically in the form of a series of
modules or carousels, with each module containing a set of recommended items.
The selection and ordering of these item recommendation modules are intended to
increase discover-ability of relevant items and encourage greater user
engagement, while simultaneously showcasing diversity of inventory and
satisfying other business objectives. Item recommendation modules on the IDP
are often curated and statically configured for all customers, ignoring
opportunities for personalization. In this paper, we present a scalable
end-to-end production system to optimize the personalized selection and
ordering of item recommendation modules on the IDP in real-time by utilizing
deep neural networks. Through extensive offline experimentation and online A/B
testing, we show that our proposed system achieves significantly higher
click-through and conversion rates compared to other existing methods. In our
online A/B test, our framework improved click-through rate by 2.48% and
purchase-through rate by 7.34% over a static configuration.

    

### [[2108.05894] MicroNet: Improving Image Recognition with Extremely Low FLOPs](http://arxiv.org/abs/2108.05894)


  This paper aims at addressing the problem of substantial performance
degradation at extremely low computational cost (e.g. 5M FLOPs on ImageNet
classification). We found that two factors, sparse connectivity and dynamic
activation function, are effective to improve the accuracy. The former avoids
the significant reduction of network width, while the latter mitigates the
detriment of reduction in network depth. Technically, we propose
micro-factorized convolution, which factorizes a convolution matrix into low
rank matrices, to integrate sparse connectivity into convolution. We also
present a new dynamic activation function, named Dynamic Shift Max, to improve
the non-linearity via maxing out multiple dynamic fusions between an input
feature map and its circular channel shift. Building upon these two new
operators, we arrive at a family of networks, named MicroNet, that achieves
significant performance gains over the state of the art in the low FLOP regime.
For instance, under the constraint of 12M FLOPs, MicroNet achieves 59.4\% top-1
accuracy on ImageNet classification, outperforming MobileNetV3 by 9.6\%. Source
code is at
\href{this https URL}{this https URL}.

    

### [[2108.05895] Mobile-Former: Bridging MobileNet and Transformer](http://arxiv.org/abs/2108.05895)


  We present Mobile-Former, a parallel design of MobileNet and Transformer with
a two-way bridge in between. This structure leverages the advantage of
MobileNet at local processing and transformer at global interaction. And the
bridge enables bidirectional fusion of local and global features. Different
with recent works on vision transformer, the transformer in Mobile-Former
contains very few tokens (e.g. less than 6 tokens) that are randomly
initialized, resulting in low computational cost. Combining with the proposed
light-weight cross attention to model the bridge, Mobile-Former is not only
computationally efficient, but also has more representation power,
outperforming MobileNetV3 at low FLOP regime from 25M to 500M FLOPs on ImageNet
classification. For instance, it achieves 77.9\% top-1 accuracy at 294M FLOPs,
gaining 1.3\% over MobileNetV3 but saving 17\% of computations. When
transferring to object detection, Mobile-Former outperforms MobileNetV3 by 8.6
AP.

    

### [[1901.03450] Ubiquitous Acoustic Sensing on Commodity IoT Devices: A Survey](http://arxiv.org/abs/1901.03450)


  With the proliferation of Internet-of-Things devices, acoustic sensing
attracts much attention in recent years. It exploits acoustic transceivers such
as microphones and speakers beyond their primary functions, namely recording
and playing, to enable novel applications and new user experiences. In this
paper, we present the first systematic survey of recent advances in active
acoustic sensing using commodity hardware with a frequency range below
24~\!kHz. We propose a general framework that categorizes main building blocks
of acoustic sensing systems. This framework encompasses three layers, i.e.,
physical layer, core technique layer, and application layer. The physical layer
includes basic hardware components, acoustic platforms as well as the air-borne
and structure-borne channel characteristics. The core technique layer
encompasses key mechanisms to generate acoustic signals (waveforms) and to
extract useful temporal, spatial and spectral information from received
signals. The application layer builds upon the functions offered by the core
techniques to realize different acoustic sensing applications. We highlight
unique challenges due to the limitations of physical devices and acoustic
channels and how they are mitigated or overcame by core processing techniques
and application-specific solutions. Finally, research opportunities and future
directions are discussed to spawn further in-depth investigation on acoustic
sensing.

    

### [[1904.05330] Hierarchical Stochastic Block Model for Community Detection in Multiplex Networks](http://arxiv.org/abs/1904.05330)


  Multiplex networks have become increasingly more prevalent in many fields,
and have emerged as a powerful tool for modeling the complexity of real
networks. There is a critical need for developing inference models for
multiplex networks that can take into account potential dependencies across
different layers, particularly when the aim is community detection. We add to a
limited literature by proposing a novel and efficient Bayesian model for
community detection in multiplex networks. A key feature of our approach is the
ability to model varying communities at different network layers. In contrast,
many existing models assume the same communities for all layers. Moreover, our
model automatically picks up the necessary number of communities at each layer
(as validated by real data examples). This is appealing, since deciding the
number of communities is a challenging aspect of community detection, and
especially so in the multiplex setting, if one allows the communities to change
across layers. Borrowing ideas from hierarchical Bayesian modeling, we use a
hierarchical Dirichlet prior to model community labels across layers, allowing
dependency in their structure. Given the community labels, a stochastic block
model (SBM) is assumed for each layer. We develop an efficient slice sampler
for sampling the posterior distribution of the community labels as well as the
link probabilities between communities. In doing so, we address some unique
challenges posed by coupling the complex likelihood of SBM with the
hierarchical nature of the prior on the labels. An extensive empirical
validation is performed on simulated and real data, demonstrating the superior
performance of the model over single-layer alternatives, as well as the ability
to uncover interesting structures in real networks.

    

### [[1905.10917] Temporal-difference learning with nonlinear function approximation: lazy training and mean field regimes](http://arxiv.org/abs/1905.10917)


  We discuss the approximation of the value function for infinite-horizon
discounted Markov Reward Processes (MRP) with nonlinear functions trained with
the Temporal-Difference (TD) learning algorithm. We first consider this problem
under a certain scaling of the approximating function, leading to a regime
called lazy training. In this regime, the parameters of the model vary only
slightly during the learning process, a feature that has recently been observed
in the training of neural networks, where the scaling we study arises
naturally, implicit in the initialization of their parameters. Both in the
under- and over-parametrized frameworks, we prove exponential convergence to
local, respectively global minimizers of the above algorithm in the lazy
training regime. We then compare this scaling of the parameters to the
mean-field regime, where the approximately linear behavior of the model is
lost. Under this alternative scaling we prove that all fixed points of the
dynamics in parameter space are global minimizers. We finally give examples of
our convergence results in the case of models that diverge if trained with
non-lazy TD learning, and in the case of neural networks.

    

### [[1908.11057] A Deep Neural Information Fusion Architecture for Textual Network Embeddings](http://arxiv.org/abs/1908.11057)


  Textual network embeddings aim to learn a low-dimensional representation for
every node in the network so that both the structural and textual information
from the networks can be well preserved in the representations. Traditionally,
the structural and textual embeddings were learned by models that rarely take
the mutual influences between them into account. In this paper, a deep neural
architecture is proposed to effectively fuse the two kinds of informations into
one representation. The novelties of the proposed architecture are manifested
in the aspects of a newly defined objective function, the complementary
information fusion method for structural and textual features, and the mutual
gate mechanism for textual feature extraction. Experimental results show that
the proposed model outperforms the comparing methods on all three datasets.

    

### [[1909.02688] AutoGMM: Automatic and Hierarchical Gaussian Mixture Modeling in Python](http://arxiv.org/abs/1909.02688)


  Background: Gaussian mixture modeling is a fundamental tool in clustering, as
well as discriminant analysis and semiparametric density estimation. However,
estimating the optimal model for any given number of components is an NP-hard
problem, and estimating the number of components is in some respects an even
harder problem. Findings: In R, a popular package called mclust addresses both
of these problems. However, Python has lacked such a package. We therefore
introduce AutoGMM, a Python algorithm for automatic Gaussian mixture modeling,
and its hierarchical version, HGMM. AutoGMM builds upon scikit-learn's
AgglomerativeClustering and GaussianMixture classes, with certain modifications
to make the results more stable. Empirically, on several different
applications, AutoGMM performs approximately as well as mclust, and sometimes
better. Conclusions: AutoMM, a freely available Python package, enables
efficient Gaussian mixture modeling by automatically selecting the
initialization, number of clusters and covariance constraints.

    

### [[1911.00472] Progressive Compressed Records: Taking a Byte out of Deep Learning Data](http://arxiv.org/abs/1911.00472)


  Deep learning accelerators efficiently train over vast and growing amounts of
data, placing a newfound burden on commodity networks and storage devices. A
common approach to conserve bandwidth involves resizing or compressing data
prior to training. We introduce Progressive Compressed Records (PCRs), a data
format that uses compression to reduce the overhead of fetching and
transporting data, effectively reducing the training time required to achieve a
target accuracy. PCRs deviate from previous storage formats by combining
progressive compression with an efficient storage layout to view a single
dataset at multiple fidelities---all without adding to the total dataset size.
We implement PCRs and evaluate them on a range of datasets, training tasks, and
hardware architectures. Our work shows that: (i) the amount of compression a
dataset can tolerate exceeds 50% of the original encoding for many DL training
tasks; (ii) it is possible to automatically and efficiently select appropriate
compression levels for a given task; and (iii) PCRs enable tasks to readily
access compressed data at runtime---utilizing as little as half the training
bandwidth and thus potentially doubling training speed.

    

### [[1911.01533] Speaker-invariant Affective Representation Learning via Adversarial Training](http://arxiv.org/abs/1911.01533)


  Representation learning for speech emotion recognition is challenging due to
labeled data sparsity issue and lack of gold standard references. In addition,
there is much variability from input speech signals, human subjective
perception of the signals and emotion label ambiguity. In this paper, we
propose a machine learning framework to obtain speech emotion representations
by limiting the effect of speaker variability in the speech signals.
Specifically, we propose to disentangle the speaker characteristics from
emotion through an adversarial training network in order to better represent
emotion. Our method combines the gradient reversal technique with an entropy
loss function to remove such speaker information. Our approach is evaluated on
both IEMOCAP and CMU-MOSEI datasets. We show that our method improves speech
emotion classification and increases generalization to unseen speakers.

    

### [[1912.00215] Probing the State of the Art: A Critical Look at Visual Representation Evaluation](http://arxiv.org/abs/1912.00215)


  Self-supervised research improved greatly over the past half decade, with
much of the growth being driven by objectives that are hard to quantitatively
compare. These techniques include colorization, cyclical consistency, and
noise-contrastive estimation from image patches. Consequently, the field has
settled on a handful of measurements that depend on linear probes to adjudicate
which approaches are the best. Our first contribution is to show that this test
is insufficient and that models which perform poorly (strongly) on linear
classification can perform strongly (weakly) on more involved tasks like
temporal activity localization. Our second contribution is to analyze the
capabilities of five different representations. And our third contribution is a
much needed new dataset for temporal activity localization.

    

### [[1912.02572] Dynamic Pricing on E-commerce Platform with Deep Reinforcement Learning: A Field Experiment](http://arxiv.org/abs/1912.02572)


  In this paper we present an end-to-end framework for addressing the problem
of dynamic pricing (DP) on E-commerce platform using methods based on deep
reinforcement learning (DRL). By using four groups of different business data
to represent the states of each time period, we model the dynamic pricing
problem as a Markov Decision Process (MDP). Compared with the state-of-the-art
DRL-based dynamic pricing algorithms, our approaches make the following three
contributions. First, we extend the discrete set problem to the continuous
price set. Second, instead of using revenue as the reward function directly, we
define a new function named difference of revenue conversion rates (DRCR).
Third, the cold-start problem of MDP is tackled by pre-training and evaluation
using some carefully chosen historical sales data. Our approaches are evaluated
by both offline evaluation method using real dataset of Alibaba Inc., and
online field experiments starting from July 2018 with thousands of items,
lasting for months on this http URL. To our knowledge, there is no other DP field
experiment using DRL before. Field experiment results suggest that DRCR is a
more appropriate reward function than revenue, which is widely used by current
literature. Also, continuous price sets have better performance than discrete
sets and our approaches significantly outperformed the manual pricing by
operation experts.

    

### [[2002.04131] Mean-Field Controls with Q-learning for Cooperative MARL: Convergence and Complexity Analysis](http://arxiv.org/abs/2002.04131)


  Multi-agent reinforcement learning (MARL), despite its popularity and
empirical success, suffers from the curse of dimensionality. This paper builds
the mathematical framework to approximate cooperative MARL by a mean-field
control (MFC) approach, and shows that the approximation error is of
$\mathcal{O}(\frac{1}{\sqrt{N}})$. By establishing an appropriate form of the
dynamic programming principle for both the value function and the Q function,
it proposes a model-free kernel-based Q-learning algorithm (MFC-K-Q), which is
shown to have a linear convergence rate for the MFC problem, the first of its
kind in the MARL literature. It further establishes that the convergence rate
and the sample complexity of MFC-K-Q are independent of the number of agents
$N$, which provides an $\mathcal{O}(\frac{1}{\sqrt{N}})$ approximation to the
MARL problem with $N$ agents in the learning environment. Empirical studies for
the network traffic congestion problem demonstrate that MFC-K-Q outperforms
existing MARL algorithms when $N$ is large, for instance when $N>50$.

    

### [[2002.10069] Robust Learning-Based Control via Bootstrapped Multiplicative Noise](http://arxiv.org/abs/2002.10069)


  Despite decades of research and recent progress in adaptive control and
reinforcement learning, there remains a fundamental lack of understanding in
designing controllers that provide robustness to inherent non-asymptotic
uncertainties arising from models estimated with finite, noisy data. We propose
a robust adaptive control algorithm that explicitly incorporates such
non-asymptotic uncertainties into the control design. The algorithm has three
components: (1) a least-squares nominal model estimator; (2) a bootstrap
resampling method that quantifies non-asymptotic variance of the nominal model
estimate; and (3) a non-conventional robust control design method using an
optimal linear quadratic regulator (LQR) with multiplicative noise. A key
advantage of the proposed approach is that the system identification and robust
control design procedures both use stochastic uncertainty representations, so
that the actual inherent statistical estimation uncertainty directly aligns
with the uncertainty the robust controller is being designed against. We show
through numerical experiments that the proposed robust adaptive controller can
significantly outperform the certainty equivalent controller on both expected
regret and measures of regret risk.

    

### [[2006.12367] Adaptive Discretization for Adversarial Lipschitz Bandits](http://arxiv.org/abs/2006.12367)


  Lipschitz bandits is a prominent version of multi-armed bandits that studies
large, structured action spaces such as the [0,1] interval, where similar
actions are guaranteed to have similar rewards. A central theme here is the
adaptive discretization of the action space, which gradually ``zooms in'' on
the more promising regions thereof. The goal is to take advantage of ``nicer''
problem instances, while retaining near-optimal worst-case performance. While
the stochastic version of the problem is well-understood, the general version
with adversarial rewards is not. We provide the first algorithm for adaptive
discretization in the adversarial version, and derive instance-dependent regret
bounds. In particular, we recover the worst-case optimal regret bound for the
adversarial version, and the instance-dependent regret bound for the stochastic
version. Further, an application of our algorithm to dynamic pricing (where a
seller repeatedly adjusts prices for a product) enjoys these regret bounds
without any smoothness assumptions.

    

### [[2008.02397] DANA: Dimension-Adaptive Neural Architecture for Multivariate Sensor Data](http://arxiv.org/abs/2008.02397)


  Motion sensors embedded in wearable and mobile devices allow for dynamic
selection of sensor streams and sampling rates, enabling several applications,
such as power management and data-sharing control. While deep neural networks
(DNNs) achieve competitive accuracy in sensor data classification, DNNs
generally process incoming data from a fixed set of sensors with a fixed
sampling rate, and changes in the dimensions of their inputs cause considerable
accuracy loss, unnecessary computations, or failure in operation. We introduce
a dimension-adaptive pooling (DAP) layer that makes DNNs flexible and more
robust to changes in sensor availability and in sampling rate. DAP operates on
convolutional filter maps of variable dimensions and produces an input of fixed
dimensions suitable for feedforward and recurrent layers. We also propose a
dimension-adaptive training (DAT) procedure for enabling DNNs that use DAP to
better generalize over the set of feasible data dimensions at inference time.
DAT comprises the random selection of dimensions during the forward passes and
optimization with accumulated gradients of several backward passes. Combining
DAP and DAT, we show how to transform non-adaptive DNNs into a
Dimension-Adaptive Neural Architecture (DANA), while keeping the same number of
parameters. Compared to existing approaches, our solution provides better
classification accuracy over the range of possible data dimensions at inference
time and does not require up-sampling or imputation, thus reducing unnecessary
computations. Experiments on seven datasets (four benchmark real-world datasets
for human activity recognition and three synthetic datasets) show that DANA
prevents significant losses in classification accuracy of the state-of-the-art
DNNs and, compared to baselines, it better captures correlated patterns in
sensor data under dynamic sensor availability and varying sampling rates.

    

### [[2010.05903] PANDA: Adapting Pretrained Features for Anomaly Detection and Segmentation](http://arxiv.org/abs/2010.05903)


  Anomaly detection methods require high-quality features. In recent years, the
anomaly detection community has attempted to obtain better features using
advances in deep self-supervised feature learning. Surprisingly, a very
promising direction, using pretrained deep features, has been mostly
overlooked. In this paper, we first empirically establish the perhaps expected,
but unreported result, that combining pretrained features with simple anomaly
detection and segmentation methods convincingly outperforms, much more complex,
state-of-the-art methods.
In order to obtain further performance gains in anomaly detection, we adapt
pretrained features to the target distribution. Although transfer learning
methods are well established in multi-class classification problems, the
one-class classification (OCC) setting is not as well explored. It turns out
that naive adaptation methods, which typically work well in supervised
learning, often result in catastrophic collapse (feature deterioration) and
reduce performance in OCC settings. A popular OCC method, DeepSVDD, advocates
using specialized architectures, but this limits the adaptation performance
gain. We propose two methods for combating collapse: i) a variant of early
stopping that dynamically learns the stopping iteration ii) elastic
regularization inspired by continual learning. Our method, PANDA, outperforms
the state-of-the-art in the OCC, outlier exposure and anomaly segmentation
settings by large margins.

    

### [[2010.09393] Locality Sensitive Hashing with Extended Differential Privacy](http://arxiv.org/abs/2010.09393)


  Extended differential privacy, a generalization of standard differential
privacy (DP) using a general metric, has been widely studied to provide
rigorous privacy guarantees while keeping high utility. However, existing works
on extended DP are limited to few metrics, such as the Euclidean metric.
Consequently, they have only a small number of applications, such as
location-based services and document processing. In this paper, we propose a
couple of mechanisms providing extended DP with a different metric: angular
distance (or cosine distance). Our mechanisms are based on locality sensitive
hashing (LSH), which can be applied to the angular distance and work well for
personal data in a high-dimensional space. We theoretically analyze the privacy
properties of our mechanisms, and prove extended DP for input data by taking
into account that LSH preserves the original metric only approximately. We
apply our mechanisms to friend matching based on high-dimensional personal data
with angular distance in the local model, and evaluate our mechanisms using two
real datasets. We show that LDP requires a very large privacy budget and that
RAPPOR does not work in this application. Then we show that our mechanisms
enable friend matching with high utility and rigorous privacy guarantees based
on extended DP.

    

### [[2011.06294] RIFE: Real-Time Intermediate Flow Estimation for Video Frame Interpolation](http://arxiv.org/abs/2011.06294)


  We propose RIFE, a Real-time Intermediate Flow Estimation algorithm for Video
Frame Interpolation (VFI). Many recent flow-based VFI methods first estimate
the bi-directional optical flows, then scale and reverse them to approximate
intermediate flows, leading to artifacts on motion boundaries. RIFE uses a
neural network named IFNet that can directly estimate the intermediate flows
from coarse-to-fine with much better speed. We design a privileged distillation
scheme for training intermediate flow model, which leads to a large performance
improvement. Experiments demonstrate that RIFE is flexible and can achieve
state-of-the-art performance on several public benchmarks. The code is
available at \url{this https URL}

    

### [[2012.00451] Just Ask: Learning to Answer Questions from Millions of Narrated Videos](http://arxiv.org/abs/2012.00451)


  Recent methods for visual question answering rely on large-scale annotated
datasets. Manual annotation of questions and answers for videos, however, is
tedious, expensive and prevents scalability. In this work, we propose to avoid
manual annotation and generate a large-scale training dataset for video
question answering making use of automatic cross-modal supervision. We leverage
a question generation transformer trained on text data and use it to generate
question-answer pairs from transcribed video narrations. Given narrated videos,
we then automatically generate the HowToVQA69M dataset with 69M
video-question-answer triplets. To handle the open vocabulary of diverse
answers in this dataset, we propose a training procedure based on a contrastive
loss between a video-question multi-modal transformer and an answer
transformer. We introduce the zero-shot VideoQA task and show excellent
results, in particular for rare answers. Furthermore, we demonstrate our method
to significantly outperform the state of the art on MSRVTT-QA, MSVD-QA,
ActivityNet-QA and How2QA. Finally, for a detailed evaluation we introduce
iVQA, a new VideoQA dataset with reduced language biases and high-quality
redundant manual annotations. Our code, datasets and trained models are
available at this https URL.

    

### [[2012.12351] Is the brain macroscopically linear? A system identification of resting state dynamics](http://arxiv.org/abs/2012.12351)


  A central challenge in the computational modeling of neural dynamics is the
trade-off between accuracy and simplicity. At the level of individual neurons,
nonlinear dynamics are both experimentally established and essential for
neuronal functioning. An implicit assumption has thus formed that an accurate
computational model of whole-brain dynamics must also be highly nonlinear,
whereas linear models may provide a first-order approximation. Here, we provide
a rigorous and data-driven investigation of this hypothesis at the level of
whole-brain blood-oxygen-level-dependent (BOLD) and macroscopic field potential
dynamics by leveraging the theory of system identification. Using functional
MRI (fMRI) and intracranial EEG (iEEG), we model the resting state activity of
700 subjects in the Human Connectome Project (HCP) and 122 subjects from the
Restoring Active Memory (RAM) project using state-of-the-art linear and
nonlinear model families. We assess relative model fit using predictive power,
computational complexity, and the extent of residual dynamics unexplained by
the model. Contrary to our expectations, linear auto-regressive models achieve
the best measures across all three metrics, eliminating the trade-off between
accuracy and simplicity. To understand and explain this linearity, we highlight
four properties of macroscopic neurodynamics which can counteract or mask
microscopic nonlinear dynamics: averaging over space, averaging over time,
observation noise, and limited data samples. Whereas the latter two are
technological limitations and can improve in the future, the former two are
inherent to aggregated macroscopic brain activity. Our results, together with
the unparalleled interpretability of linear models, can greatly facilitate our
understanding of macroscopic neural dynamics and the principled design of
model-based interventions for the treatment of neuropsychiatric disorders.

    

### [[2102.00369] Spectral Roll-off Points Variations: Exploring Useful Information in Feature Maps by Its Variations](http://arxiv.org/abs/2102.00369)


  Useful information (UI) is an elusive concept in neural networks. A
quantitative measurement of UI is absent, despite the variations of UI can be
recognized by prior knowledge. The communication bandwidth of feature maps
decreases after downscaling operations, but UI flows smoothly after training
due to lower Nyquist frequency. Inspired by the low-Nyqusit-frequency nature of
UI, we propose the use of spectral roll-off points (SROPs) to estimate UI on
variations. The computation of an SROP is extended from a 1-D signal to a 2-D
image by the required rotation invariance in image classification tasks. SROP
statistics across feature maps are implemented as layer-wise useful information
estimates. We design sanity checks to explore SROP variations when UI
variations are produced by variations in model input, model architecture and
training stages. The variations of SROP is synchronizes with UI variations in
various randomized and sufficiently trained model structures. Therefore, SROP
variations is an accurate and convenient sign of UI variations, which promotes
the explainability of data representations with respect to frequency-domain
knowledge.

    

### [[2102.08184] Constructing Multiclass Classifiers using Binary Classifiers Under Log-Loss](http://arxiv.org/abs/2102.08184)


  The construction of multiclass classifiers from binary elements is studied in
this paper, and performance is quantified by the regret, defined with respect
to the Bayes optimal log-loss. We discuss two known methods. The first is one
vs. all (OVA), for which we prove that the multiclass regret is upper bounded
by the sum of binary regrets of the constituent classifiers. The second is
hierarchical classification, based on a binary tree. For this method we prove
that the multiclass regret is exactly a weighted sum of constituent binary
regrets where the weighing is determined by the tree structure.
We also introduce a leverage-hierarchical classification method, which
potentially yields smaller log-loss and regret. The advantages of these
classification methods are demonstrated by simulation on both synthetic and
real-life datasets.

    

### [[2103.00241] Neural Architecture Search From Task Similarity Measure](http://arxiv.org/abs/2103.00241)


  In this paper, we propose a neural architecture search framework based on a
similarity measure between the baseline tasks and the incoming target task. We
first define the notion of task similarity based on the log-determinant of the
Fisher Information Matrices. Next, we compute the task similarity from each of
the baseline tasks to the incoming target task. By utilizing the relation
between a target and a set of learned baseline tasks, the search space of
architectures for the incoming target task can be significantly reduced, making
the discovery of the best candidates in the set of possible architectures
tractable and efficient, in terms of GPU days. This method eliminates the
requirement for training the networks from scratch for the incoming target task
as well as introducing the bias in the initialization of the search space from
the human domain. Experimental results with 8 classification tasks in MNIST and
CIFAR-10 datasets illustrate the efficacy of our proposed approach and its
competitiveness with other state-of-art methods in terms of the classification
performance, the number of parameters, and the search time.

    

### [[2103.04578] Testing Autonomous Systems with Believed Equivalence Refinement](http://arxiv.org/abs/2103.04578)


  Continuous engineering of autonomous driving functions commonly requires
deploying vehicles in road testing to obtain inputs that cause problematic
decisions. Although the discovery leads to producing an improved system, it
also challenges the foundation of testing using equivalence classes and the
associated relative test coverage criterion. In this paper, we propose believed
equivalence, where the establishment of an equivalence class is initially based
on expert belief and is subject to a set of available test cases having a
consistent valuation. Upon a newly encountered test case that breaks the
consistency, one may need to refine the established categorization in order to
split the originally believed equivalence into two. Finally, we focus on
modules implemented using deep neural networks where every category partitions
an input over the real domain. We present both analytical and lazy methods to
suggest the refinement. The concept is demonstrated in analyzing multiple
autonomous driving modules, indicating the potential of our proposed approach.

    

### [[2103.04783] DDGC: Generative Deep Dexterous Grasping in Clutter](http://arxiv.org/abs/2103.04783)


  Recent advances in multi-fingered robotic grasping have enabled fast
6-Degrees-Of-Freedom (DOF) single object grasping. Multi-finger grasping in
cluttered scenes, on the other hand, remains mostly unexplored due to the added
difficulty of reasoning over obstacles which greatly increases the
computational time to generate high-quality collision-free grasps. In this work
we address such limitations by introducing DDGC, a fast generative multi-finger
grasp sampling method that can generate high quality grasps in cluttered scenes
from a single RGB-D image. DDGC is built as a network that encodes scene
information to produce coarse-to-fine collision-free grasp poses and
configurations. We experimentally benchmark DDGC against the
simulated-annealing planner in GraspIt! on 1200 simulated cluttered scenes and
7 real world scenes. The results show that DDGC outperforms the baseline on
synthesizing high-quality grasps and removing clutter while being 5 times
faster. This, in turn, opens up the door for using multi-finger grasps in
practical applications which has so far been limited due to the excessive
computation time needed by other methods.

    

### [[2103.12323] Anomaly detection using principles of human perception](http://arxiv.org/abs/2103.12323)


  In the fields of statistics and unsupervised machine learning a fundamental
and well-studied problem is anomaly detection. Although anomalies are difficult
to define, many algorithms have been proposed. Underlying the approaches is the
nebulous understanding that anomalies are rare, unusual or inconsistent with
the majority of data. The present work gives a philosophical approach to
clearly define anomalies and to develop an algorithm for their efficient
detection with minimal user intervention. Inspired by the Gestalt School of
Psychology and the Helmholtz principle of human perception, the idea is to
assume anomalies are observations that are unexpected to occur with respect to
certain groupings made by the majority of the data. Thus, under appropriate
random variable modelling anomalies are directly found in a set of data under a
uniform and independent random assumption of the distribution of constituent
elements of the observations; anomalies correspond to those observations where
the expectation of occurrence of the elements in a given view is $<1$. Starting
from fundamental principles of human perception an unsupervised anomaly
detection algorithm is developed that is simple, real-time and parameter-free.
Experiments suggest it as the prime choice for univariate data and it shows
promising performance on the detection of global anomalies in multivariate
data.

    

### [[2104.14121] Real Negatives Matter: Continuous Training with Real Negatives for Delayed Feedback Modeling](http://arxiv.org/abs/2104.14121)


  One of the difficulties of conversion rate (CVR) prediction is that the
conversions can delay and take place long after the clicks. The delayed
feedback poses a challenge: fresh data are beneficial to continuous training
but may not have complete label information at the time they are ingested into
the training pipeline. To balance model freshness and label certainty, previous
methods set a short waiting window or even do not wait for the conversion
signal. If conversion happens outside the waiting window, this sample will be
duplicated and ingested into the training pipeline with a positive label.
However, these methods have some issues. First, they assume the observed
feature distribution remains the same as the actual distribution. But this
assumption does not hold due to the ingestion of duplicated samples. Second,
the certainty of the conversion action only comes from the positives. But the
positives are scarce as conversions are sparse in commercial systems. These
issues induce bias during the modeling of delayed feedback. In this paper, we
propose DElayed FEedback modeling with Real negatives (DEFER) method to address
these issues. The proposed method ingests real negative samples into the
training pipeline. The ingestion of real negatives ensures the observed feature
distribution is equivalent to the actual distribution, thus reducing the bias.
The ingestion of real negatives also brings more certainty information of the
conversion. To correct the distribution shift, DEFER employs importance
sampling to weigh the loss function. Experimental results on industrial
datasets validate the superiority of DEFER. DEFER have been deployed in the
display advertising system of Alibaba, obtaining over 6.0% improvement on CVR
in several scenarios. The code and data in this paper are now open-sourced
{this https URL}.

    

### [[2105.05498] Improving Lexically Constrained Neural Machine Translation with Source-Conditioned Masked Span Prediction](http://arxiv.org/abs/2105.05498)


  Accurate terminology translation is crucial for ensuring the practicality and
reliability of neural machine translation (NMT) systems. To address this,
lexically constrained NMT explores various methods to ensure pre-specified
words and phrases appear in the translation output. However, in many cases,
those methods are studied on general domain corpora, where the terms are mostly
uni- and bi-grams (>98%). In this paper, we instead tackle a more challenging
setup consisting of domain-specific corpora with much longer n-gram and highly
specialized terms. Inspired by the recent success of masked span prediction
models, we propose a simple and effective training strategy that achieves
consistent improvements on both terminology and sentence-level translation for
three domain-specific corpora in two language pairs.

    

### [[2105.13649] Pruning and Slicing Neural Networks using Formal Verification](http://arxiv.org/abs/2105.13649)


  Deep neural networks (DNNs) play an increasingly important role in various
computer systems. In order to create these networks, engineers typically
specify a desired topology, and then use an automated training algorithm to
select the network's weights. While training algorithms have been studied
extensively and are well understood, the selection of topology remains a form
of art, and can often result in networks that are unnecessarily large - and
consequently are incompatible with end devices that have limited memory,
battery or computational power. Here, we propose to address this challenge by
harnessing recent advances in DNN verification. We present a framework and a
methodology for discovering redundancies in DNNs - i.e., for finding neurons
that are not needed, and can be removed in order to reduce the size of the DNN.
By using sound verification techniques, we can formally guarantee that our
simplified network is equivalent to the original, either completely, or up to a
prescribed tolerance. Further, we show how to combine our technique with
slicing, which results in a family of very small DNNs, which are together
equivalent to the original. Our approach can produce DNNs that are
significantly smaller than the original, rendering them suitable for deployment
on additional kinds of systems, and even more amenable to subsequent formal
verification. We provide a proof-of-concept implementation of our approach, and
use it to evaluate our techniques on several real-world DNNs.

    

### [[2106.03761] Bias Mitigation of Face Recognition Models Through Calibration](http://arxiv.org/abs/2106.03761)


  Face recognition models suffer from bias: for example, the probability of a
false positive (incorrect face match) strongly depends on sensitive attributes
like ethnicity. As a result, these models may disproportionately and negatively
impact minority groups when used in law enforcement. In this work, we introduce
the Bias Mitigation Calibration (BMC) method, which (i) increases model
accuracy (improving the state-of-the-art), (ii) produces fairly-calibrated
probabilities, (iii) significantly reduces the gap in the false positive rates,
and (iv) does not require knowledge of the sensitive attribute.

    

### [[2106.04096] Linear Convergence of Entropy-Regularized Natural Policy Gradient with Linear Function Approximation](http://arxiv.org/abs/2106.04096)


  Natural policy gradient (NPG) methods with function approximation achieve
impressive empirical success in reinforcement learning problems with large
state-action spaces. However, theoretical understanding of their convergence
behaviors remains limited in the function approximation setting. In this paper,
we perform a finite-time analysis of NPG with linear function approximation and
softmax parameterization, and prove for the first time that widely used entropy
regularization method, which encourages exploration, leads to linear
convergence rate. Under considerably weaker regularity conditions, we prove
that entropy-regularized Q-NPG variant with linear function approximation
achieves $\tilde{O}(1/T)$ convergence rate. We adopt a Lyapunov drift analysis
to prove the convergence results and explain the effectiveness of entropy
regularization in improving the convergence rates.

    

### [[2106.04993] Initialization Matters: Regularizing Manifold-informed Initialization for Neural Recommendation Systems](http://arxiv.org/abs/2106.04993)


  Proper initialization is crucial to the optimization and the generalization
of neural networks. However, most existing neural recommendation systems
initialize the user and item embeddings randomly. In this work, we propose a
new initialization scheme for user and item embeddings called Laplacian
Eigenmaps with Popularity-based Regularization for Isolated Data (LEPORID).
LEPORID endows the embeddings with information regarding multi-scale
neighborhood structures on the data manifold and performs adaptive
regularization to compensate for high embedding variance on the tail of the
data distribution. Exploiting matrix sparsity, LEPORID embeddings can be
computed efficiently. We evaluate LEPORID in a wide range of neural
recommendation models. In contrast to the recent surprising finding that the
simple K-nearest-neighbor (KNN) method often outperforms neural recommendation
systems, we show that existing neural systems initialized with LEPORID often
perform on par or better than KNN. To maximize the effects of the
initialization, we propose the Dual-Loss Residual Recommendation (DLR2)
network, which, when initialized with LEPORID, substantially outperforms both
traditional and state-of-the-art neural recommender systems.

    

### [[2106.10662] FedXGBoost: Privacy-Preserving XGBoost for Federated Learning](http://arxiv.org/abs/2106.10662)


  Federated learning is the distributed machine learning framework that enables
collaborative training across multiple parties while ensuring data privacy.
Practical adaptation of XGBoost, the state-of-the-art tree boosting framework,
to federated learning remains limited due to high cost incurred by conventional
privacy-preserving methods. To address the problem, we propose two variants of
federated XGBoost with privacy guarantee: FedXGBoost-SMM and FedXGBoost-LDP.
Our first protocol FedXGBoost-SMM deploys enhanced secure matrix multiplication
method to preserve privacy with lossless accuracy and lower overhead than
encryption-based techniques. Developed independently, the second protocol
FedXGBoost-LDP is heuristically designed with noise perturbation for local
differential privacy, and empirically evaluated on real-world and synthetic
datasets.

    

### [[2108.04135] Manifold-aware Synthesis of High-resolution Diffusion from Structural Imaging](http://arxiv.org/abs/2108.04135)


  The physical and clinical constraints surrounding diffusion-weighted imaging
(DWI) often limit the spatial resolution of the produced images to voxels up to
8 times larger than those of T1w images. Thus, the detailed information
contained in T1w imagescould help in the synthesis of diffusion images in
higher resolution. However, the non-Euclidean nature of diffusion imaging
hinders current deep generative models from synthesizing physically plausible
images. In this work, we propose the first Riemannian network architecture for
the direct generation of diffusion tensors (DT) and diffusion orientation
distribution functions (dODFs) from high-resolution T1w images. Our integration
of the Log-Euclidean Metric into a learning objective guarantees, unlike
standard Euclidean networks, the mathematically-valid synthesis of diffusion.
Furthermore, our approach improves the fractional anisotropy mean squared error
(FA MSE) between the synthesized diffusion and the ground-truth by more than
23% and the cosine similarity between principal directions by almost 5% when
compared to our baselines. We validate our generated diffusion by comparing the
resulting tractograms to our expected real data. We observe similar fiber
bundles with streamlines having less than 3% difference in length, less than 1%
difference in volume, and a visually close shape. While our method is able to
generate high-resolution diffusion images from structural inputs in less than
15 seconds, we acknowledge and discuss the limits of diffusion inference solely
relying on T1w images. Our results nonetheless suggest a relationship between
the high-level geometry of the brain and the overall white matter architecture.

    

### [[2108.05475] SAFE: Secure Aggregation with Failover and Encryption](http://arxiv.org/abs/2108.05475)


  We propose and experimentally evaluate a novel secure aggregation algorithm
targeted at cross-organizational federated learning applications with a fixed
set of participating learners. Our solution organizes learners in a chain and
encrypts all traffic to reduce the controller of the aggregation to a mere
message broker. We show that our algorithm scales better and is less resource
demanding than existing solutions, while being easy to implement on constrained
platforms.
With 36 nodes our method outperforms state-of-the-art secure aggregation by
70x, and 56x with and without failover, respectively.

    

### [[2108.05742] Secure Private and Adaptive Matrix Multiplication Beyond the Singleton Bound](http://arxiv.org/abs/2108.05742)


  Consider the problem of designing secure and private codes for distributed
matrix-matrix multiplication. A master server owns two private matrices $\mA$
and $\mB$ and hires worker nodes to help computing their multiplication. The
matrices should remain information-theoretically private from the workers. Some
of the workers are malicious and return corrupted results to the master. This
work is motivated by the literature on myopic adversaries in network coding and
distributed storage. Security beyond the Singleton bound is possible when the
adversary has limited knowledge about the master's data and probabilistic
decoding is acceptable. The key observation in this setting is that the master
is the sender and the receiver. Therefore, the master enjoys a plethora of
advantages that enable coding for security beyond the Singleton bound.
We design a framework for security against malicious adversaries in private
matrix-matrix multiplication. Our main goal is to apply this security framework
to schemes with adaptive rates previously introduced by a subset of the
authors. Adaptive schemes divide the workers into clusters and thus provide
flexibility in trading decoding complexity for efficiency. Checking the
integrity of the computation per cluster has low complexity but costs deleting
the results of a whole cluster with at least one malicious worker. Checking the
integrity of the results per worker is more complex but allows an efficient use
of the non-malicious workers. Our scheme, called SRPM3, provides a
computationally efficient security check that detects malicious workers with
high probability and can tolerate the presence of an arbitrary number of
malicious workers. We provide simulation results that validate our theoretical
findings.

    

### [[2108.05815] Communication-free and Parallel Simulation of Neutral Biodiversity Models](http://arxiv.org/abs/2108.05815)


  We present a novel communication-free algorithm for individual-based
probabilistic neutral biodiversity simulations. The algorithm transforms a
neutral Moran ecosystem model into an embarrassingly parallel problem by
trading off inter-process communication at the cost of some redundant
computation.
Specifically, by careful design of the random number generator that drives
the simulation, we arrange for evolutionary parent-child interactions to be
modelled without requiring knowledge of the interaction, its participants, or
which processor is performing the computation. Critically, this means that
every individual can be simulated entirely independently. The simulation is
thus fully reproducible irrespective of the number of processors it is
distributed over. With our novel algorithm, a simulation can be (1) split up
into independent batch jobs and (2) simulated across any number of
heterogeneous machines - all without affecting the simulation result.
We use the Rust programming language to build the extensible and statically
checked simulation package $\texttt{necsim-rust}$. We evaluate our
parallelisation approach by comparing three traditional simulation algorithms
against a CPU and GPU implementation of our Independent algorithm. These
experiments show that as long as some local state is maintained to cull
redundant individuals, our Independent algorithm is as efficient as existing
sequential solutions. The GPU implementation further outperforms all algorithms
on the CPU by a factor ranging from $\sim 2$ to $\sim 80$, depending on the
model parameterisation and the analysis that is performed. Amongst the parallel
algorithms we have investigated, our Independent algorithm provides the only
non-approximate parallelisation strategy that can scale to large simulation
domains.

    

### [[2103.09389] PythonFOAM: In-situ data analyses with OpenFOAM and Python](http://arxiv.org/abs/2103.09389)


  We outline the development of a general-purpose Python-based data analysis
tool for OpenFOAM. Our implementation relies on the construction of OpenFOAM
applications that have bindings to data analysis libraries in Python. Double
precision data in OpenFOAM is cast to a NumPy array using the NumPy C-API and
Python modules may then be used for arbitrary data analysis and manipulation on
flow-field information. We highlight how the proposed wrapper may be used for
an in-situ online singular value decomposition (SVD) implemented in Python and
accessed from the OpenFOAM solver PimpleFOAM. Here, `in-situ' refers to a
programming paradigm that allows for a concurrent computation of the data
analysis on the same computational resources utilized for the partial
differential equation solver. In addition, to demonstrate data-parallel
analyses, we deploy a distributed SVD, which collects snapshot data across the
ranks of a distributed simulation to compute the global left singular vectors.
Crucially, both OpenFOAM and Python share the same message passing interface
(MPI) communicator for this deployment which allows Python objects and
functions to exchange NumPy arrays across ranks. Subsequently, we provide
scaling assessments of this distributed SVD on multiple nodes of Intel
Broadwell and KNL architectures for canonical test cases such as the large eddy
simulations of a backward facing step and a channel flow at friction Reynolds
number of 395. Finally, we demonstrate the deployment of a deep neural network
for compressing the flow-field information using an autoencoder to demonstrate
an ability to use state-of-the-art machine learning tools in the Python
ecosystem.

    

### [[2108.05402] Composition Machines: Programming Self-Organising Software Models for the Emergence of Sequential Program Spaces](http://arxiv.org/abs/2108.05402)


  We are entering a new era in which software systems are becoming more and
more complex and larger. So, the composition of such systems is becoming
infeasible by manual means. To address this challenge, self-organising software
models represent a promising direction since they allow the (bottom-up)
emergence of complex computational structures from simple rules. In this paper,
we propose an abstract machine, called the composition machine, which allows
the definition and the execution of such models. Unlike typical abstract
machines, our proposal does not compute individual programs but enables the
emergence of multiple programs at once. We particularly present the machine's
semantics and provide examples to demonstrate its operation with well-known
rules from the realm of Boolean logic and elementary cellular automata.

    

### [[2108.05410] User-friendly Comparison of Similarity Algorithms on Wikidata](http://arxiv.org/abs/2108.05410)


  While the similarity between two concept words has been evaluated and studied
for decades, much less attention has been devoted to algorithms that can
compute the similarity of nodes in very large knowledge graphs, like Wikidata.
To facilitate investigations and head-to-head comparisons of similarity
algorithms on Wikidata, we present a user-friendly interface that allows
flexible computation of similarity between Qnodes in Wikidata. At present, the
similarity interface supports four algorithms, based on: graph embeddings
(TransE, ComplEx), text embeddings (BERT), and class-based similarity. We
demonstrate the behavior of the algorithms on representative examples about
semantically similar, related, and entirely unrelated entity pairs. To support
anticipated applications that require efficient similarity computations, like
entity linking and recommendation, we also provide a REST API that can compute
most similar neighbors for any Qnode in Wikidata.

    

### [[2108.05412] Analyzing Race and Country of Citizenship Bias in Wikidata](http://arxiv.org/abs/2108.05412)


  As an open and collaborative knowledge graph created by users and bots, it is
possible that the knowledge in Wikidata is biased in regards to multiple
factors such as gender, race, and country of citizenship. Previous work has
mostly studied the representativeness of Wikidata knowledge in terms of genders
of people. In this paper, we examine the race and citizenship bias in general
and in regards to STEM representation for scientists, software developers, and
engineers. By comparing Wikidata queries to real-world datasets, we identify
the differences in representation to characterize the biases present in
Wikidata. Through this analysis, we discovered that there is an
overrepresentation of white individuals and those with citizenship in Europe
and North America; the rest of the groups are generally underrepresented. Based
on these findings, we have found and linked to Wikidata additional data about
STEM scientists from the minorities. This data is ready to be inserted into
Wikidata with a bot. Increasing representation of minority race and country of
citizenship groups can create a more accurate portrayal of individuals in STEM.

    

### [[2108.05428] Determining ActionReversibility in STRIPS Using Answer Set and Epistemic Logic Programming](http://arxiv.org/abs/2108.05428)


  In the context of planning and reasoning about actions and change, we call an
action reversible when its effects can be reverted by applying other actions,
returning to the original state. Renewed interest in this area has led to
several results in the context of the PDDL language, widely used for describing
planning tasks.
In this paper, we propose several solutions to the computational problem of
deciding the reversibility of an action. In particular, we leverage an existing
translation from PDDL to Answer Set Programming (ASP), and then use several
different encodings to tackle the problem of action reversibility for the
STRIPS fragment of PDDL. For these, we use ASP, as well as Epistemic Logic
Programming (ELP), an extension of ASP with epistemic operators, and compare
and contrast their strengths and weaknesses.
Under consideration for acceptance in TPLP.

    

### [[2108.05436] Friddy multiagent price stabilization model](http://arxiv.org/abs/2108.05436)


  In a multiagent network model consisting of nodes, each network node has an
agent and priced Friddy coins, and the agent can buy or sell Friddy coins in
the marketplace. Though every node may not effectively have an equal price
during the transaction time, the prices have to reach equilibrium by iterating
buy and sell transactions on a macro level.

    

### [[2108.05454] Extracting Semantics from Maintenance Records](http://arxiv.org/abs/2108.05454)


  Rapid progress in natural language processing has led to its utilization in a
variety of industrial and enterprise settings, including in its use for
information extraction, specifically named entity recognition and relation
extraction, from documents such as engineering manuals and field maintenance
reports. While named entity recognition is a well-studied problem, existing
state-of-the-art approaches require large labelled datasets which are hard to
acquire for sensitive data such as maintenance records. Further, industrial
domain experts tend to distrust results from black box machine learning models,
especially when the extracted information is used in downstream predictive
maintenance analytics. We overcome these challenges by developing three
approaches built on the foundation of domain expert knowledge captured in
dictionaries and ontologies. We develop a syntactic and semantic rules-based
approach and an approach leveraging a pre-trained language model, fine-tuned
for a question-answering task on top of our base dictionary lookup to extract
entities of interest from maintenance records. We also develop a preliminary
ontology to represent and capture the semantics of maintenance records. Our
evaluations on a real-world aviation maintenance records dataset show promising
results and help identify challenges specific to named entity recognition in
the context of noisy industrial data.

    

### [[2108.05457] Low-level Pose Control of Tilting Multirotor for Wall Perching Tasks Using Reinforcement Learning](http://arxiv.org/abs/2108.05457)


  Recently, needs for unmanned aerial vehicles (UAVs) that are attachable to
the wall have been highlighted. As one of the ways to address the need,
researches on various tilting multirotors that can increase maneuverability has
been employed. Unfortunately, existing studies on the tilting multirotors
require considerable amounts of prior information on the complex dynamic model.
Meanwhile, reinforcement learning on quadrotors has been studied to mitigate
this issue. Yet, these are only been applied to standard quadrotors, whose
systems are less complex than those of tilting multirotors. In this paper, a
novel reinforcement learning-based method is proposed to control a tilting
multirotor on real-world applications, which is the first attempt to apply
reinforcement learning to a tilting multirotor. To do so, we propose a novel
reward function for a neural network model that takes power efficiency into
account. The model is initially trained over a simulated environment and then
fine-tuned using real-world data in order to overcome the sim-to-real gap
issue. Furthermore, a novel, efficient state representation with respect to the
goal frame that helps the network learn optimal policy better is proposed. As
verified on real-world experiments, our proposed method shows robust
controllability by overcoming the complex dynamics of tilting multirotors.

    

### [[2108.05494] A Mathematical Approach to Constraining Neural Abstraction and the Mechanisms Needed to Scale to Higher-Order Cognition](http://arxiv.org/abs/2108.05494)


  Artificial intelligence has made great strides in the last decade but still
falls short of the human brain, the best-known example of intelligence. Not
much is known of the neural processes that allow the brain to make the leap to
achieve so much from so little beyond its ability to create knowledge
structures that can be flexibly and dynamically combined, recombined, and
applied in new and novel ways. This paper proposes a mathematical approach
using graph theory and spectral graph theory, to hypothesize how to constrain
these neural clusters of information based on eigen-relationships. This same
hypothesis is hierarchically applied to scale up from the smallest to the
largest clusters of knowledge that eventually lead to model building and
reasoning.

    

### [[2108.05516] Text Anchor Based Metric Learning for Small-footprint Keyword Spotting](http://arxiv.org/abs/2108.05516)


  Keyword Spotting (KWS) remains challenging to achieve the trade-off between
small footprint and high accuracy. Recently proposed metric learning approaches
improved the generalizability of models for the KWS task, and 1D-CNN based KWS
models have achieved the state-of-the-arts (SOTA) in terms of model size.
However, for metric learning, due to data limitations, the speech anchor is
highly susceptible to the acoustic environment and speakers. Also, we note that
the 1D-CNN models have limited capability to capture long-term temporal
acoustic features. To address the above problems, we propose to utilize text
anchors to improve the stability of anchors. Furthermore, a new type of model
(LG-Net) is exquisitely designed to promote long-short term acoustic feature
modeling based on 1D-CNN and self-attention. Experiments are conducted on
Google Speech Commands Dataset version 1 (GSCDv1) and 2 (GSCDv2). The results
demonstrate that the proposed text anchor based metric learning method shows
consistent improvements over speech anchor on representative CNN-based models.
Moreover, our LG-Net model achieves SOTA accuracy of 97.67% and 96.79% on two
datasets, respectively. It is encouraged to see that our lighter LG-Net with
only 74k parameters obtains 96.82% KWS accuracy on the GSCDv1 and 95.77% KWS
accuracy on the GSCDv2.

    

### [[2108.05525] Clustering with UMAP: Why and How Connectivity Matters](http://arxiv.org/abs/2108.05525)


  Topology based dimensionality reduction methods such as t-SNE and UMAP have
seen increasing success and popularity in high-dimensional data. These methods
have strong mathematical foundations and are based on the intuition that the
topology in low dimensions should be close to that of high dimensions. Given
that the initial topological structure is a precursor to the success of the
algorithm, this naturally raises the question: What makes a "good" topological
structure for dimensionality reduction? %Insight into this will enable us to
design better algorithms which take into account both local and global
structure. In this paper which focuses on UMAP, we study the effects of node
connectivity (k-Nearest Neighbors vs \textit{mutual} k-Nearest Neighbors) and
relative neighborhood (Adjacent via Path Neighbors) on dimensionality
reduction. We explore these concepts through extensive ablation studies on 4
standard image and text datasets; MNIST, FMNIST, 20NG, AG, reducing to 2 and 64
dimensions. Our findings indicate that a more refined notion of connectivity
(\textit{mutual} k-Nearest Neighbors with minimum spanning tree) together with
a flexible method of constructing the local neighborhood (Path Neighbors), can
achieve a much better representation than default UMAP, as measured by
downstream clustering performance.

    

### [[2108.05536] Intelligent computational model for the classification of Covid-19 with chest radiography compared to other respiratory diseases](http://arxiv.org/abs/2108.05536)


  Lung X-ray images, if processed using statistical and computational methods,
can distinguish pneumonia from COVID-19. The present work shows that it is
possible to extract lung X-ray characteristics to improve the methods of
examining and diagnosing patients with suspected COVID-19, distinguishing them
from malaria, dengue, H1N1, tuberculosis, and Streptococcus pneumonia. More
precisely, an intelligent computational model was developed to process lung
X-ray images and classify whether the image is of a patient with COVID-19. The
images were processed and extracted their characteristics. These
characteristics were the input data for an unsupervised statistical learning
method, PCA, and clustering, which identified specific attributes of X-ray
images with Covid-19. The introduction of statistical models allowed a fast
algorithm, which used the X-means clustering method associated with the
Bayesian Information Criterion (CIB). The developed algorithm efficiently
distinguished each pulmonary pathology from X-ray images. The method exhibited
excellent sensitivity. The average recognition accuracy of COVID-19 was 0.93
and 0.051.

    

### [[2108.05539] Put the Bear on the Chair! Intelligent Robot Interaction with Previously Unseen Objects via Robot Imagination](http://arxiv.org/abs/2108.05539)


  In this letter, we study the problem of autonomously placing a teddy bear on
a previously unseen chair for sitting. To achieve this goal, we present a novel
method for robots to imagine the sitting pose of the bear by physically
simulating a virtual humanoid agent sitting on the chair. We also develop a
robotic system which leverages motion planning to plan SE(2) motions for a
humanoid robot to walk to the chair and whole-body motions to put the bear on
it, respectively. Furthermore, to cope with the cases where the chair is not in
an accessible pose for placing the bear, a human-robot interaction (HRI)
framework is introduced in which a human follows language instructions given by
the robot to rotate the chair and help make the chair accessible. We implement
our method with a robot arm and a humanoid robot. We calibrate the proposed
system with 3 chairs and test on 12 previously unseen chairs in both accessible
and inaccessible poses extensively. Results show that our method enables the
robot to autonomously put the teddy bear on the 12 unseen chairs with a very
high success rate. The HRI framework is also shown to be very effective in
changing the accessibility of the chair. Source code will be available. Video
demos are available at this https URL.

    

### [[2108.05641] Session-based Recommendation with Heterogeneous Graph Neural Network](http://arxiv.org/abs/2108.05641)


  The purpose of the Session-Based Recommendation System is to predict the
user's next click according to the previous session sequence. The current
studies generally learn user preferences according to the transitions of items
in the user's session sequence. However, other effective information in the
session sequence, such as user profiles, are largely ignored which may lead to
the model unable to learn the user's specific preferences. In this paper, we
propose a heterogeneous graph neural network-based session recommendation
method, named SR-HetGNN, which can learn session embeddings by heterogeneous
graph neural network (HetGNN), and capture the specific preferences of
anonymous users. Specifically, SR-HetGNN first constructs heterogeneous graphs
containing various types of nodes according to the session sequence, which can
capture the dependencies among items, users, and sessions. Second, HetGNN
captures the complex transitions between items and learns the item embeddings
containing user information. Finally, to consider the influence of users' long
and short-term preferences, local and global session embeddings are combined
with the attentional network to obtain the final session embedding. SR-HetGNN
is shown to be superior to the existing state-of-the-art session-based
recommendation methods through extensive experiments over two real large
datasets Diginetica and Tmall.

    

### [[2108.05680] Lutz's Spoiler Technique Revisited: A Unified Approach to Worst-Case Optimal Entailment of Unions of Conjunctive Queries in Locally-Forward Description Logics](http://arxiv.org/abs/2108.05680)


  We present a unified approach to (both finite and unrestricted) worst-case
optimal entailment of (unions of) conjunctive queries (U)CQs in the wide class
of "locally-forward" description logics. The main technique that we employ is a
generalisation of Lutz's spoiler technique, originally developed for CQ
entailment in ALCHQ. Our result closes numerous gaps present in the literature,
most notably implying ExpTime-completeness of (U)CQ-querying for any superlogic
of ALC contained in ALCHbregQ, and, as we believe, is abstract enough to be
employed as a black-box in many new scenarios.

    

### [[2108.05693] MISS GAN: A Multi-IlluStrator Style Generative Adversarial Network for image to illustration translation](http://arxiv.org/abs/2108.05693)


  Unsupervised style transfer that supports diverse input styles using only one
trained generator is a challenging and interesting task in computer vision.
This paper proposes a Multi-IlluStrator Style Generative Adversarial Network
(MISS GAN) that is a multi-style framework for unsupervised
image-to-illustration translation, which can generate styled yet content
preserving images. The illustrations dataset is a challenging one since it is
comprised of illustrations of seven different illustrators, hence contains
diverse styles. Existing methods require to train several generators (as the
number of illustrators) to handle the different illustrators' styles, which
limits their practical usage, or require to train an image specific network,
which ignores the style information provided in other images of the
illustrator. MISS GAN is both input image specific and uses the information of
other images using only one trained model.

    

### [[2108.05708] Reply to arXiv:2102.11963, An experimental demonstration of the memristor test, Y. V. Pershin, J. Kim, T. Datta, M. Di Ventra, 23 Feb 2021. Does an ideal memristor truly exist?](http://arxiv.org/abs/2108.05708)


  After a decade of research, we developed a prototype device and
experimentally demonstrated that the direct phi q interaction could be
memristive, as predicted by Chua in 1971. With a constant input current to
avoid any parasitic inductor effect, our device meets three criteria for an
ideal memristor: a single valued, nonlinear, continuously differentiable, and
strictly monotonically increasing constitutive phi q curve, a pinched v i
hysteresis loop, and a charge only dependent resistance. Our work represents a
step forward in terms of experimentally verifying the memristive flux charge
interaction but we have not reached the final because this prototype still
suffers from two serious limitations: 1, a superficial but dominant inductor
effect (behind which the above memristive fingerprints hide) due to its
inductor-like core structure, and 2. bistability and dynamic sweep of a
continuous resistance range. In this article, we also discuss how to make a
fully functioning ideal memristor with multiple or an infinite number of stable
states and no parasitic inductance, and give a number of suggestions, such as
open structure, nanoscale size, magnetic materials with cubic anisotropy (or
even isotropy), and sequential switching of the magnetic domains. Additionally,
we respond to a recent challenge from arXiv.org that claims that our device is
simply an inductor with memory since our device did not pass their designed
capacitor-memristor circuit test. Contrary to their conjecture that an ideal
memristor may not exist or may be a purely mathematical concept, we remain
optimistic that researchers will discover an ideal memristor in nature or make
one in the laboratory based on our current work.

    

### [[2108.05789] Presenting an extensive lab- and field-image dataset of crops and weeds for computer vision tasks in agriculture](http://arxiv.org/abs/2108.05789)


  We present two large datasets of labelled plant-images that are suited
towards the training of machine learning and computer vision models. The first
dataset encompasses as the day of writing over 1.2 million images of
indoor-grown crops and weeds common to the Canadian Prairies and many US
states. The second dataset consists of over 540,000 images of plants imaged in
farmland. All indoor plant images are labelled by species and we provide rich
etadata on the level of individual images. This comprehensive database allows
to filter the datasets under user-defined specifications such as for example
the crop-type or the age of the plant. Furthermore, the indoor dataset contains
images of plants taken from a wide variety of angles, including profile shots,
top-down shots, and angled perspectives. The images taken from plants in fields
are all from a top-down perspective and contain usually multiple plants per
image. For these images metadata is also available. In this paper we describe
both datasets' characteristics with respect to plant variety, plant age, and
number of images. We further introduce an open-access sample of the
indoor-dataset that contains 1,000 images of each species covered in our
dataset. These, in total 14,000 images, had been selected, such that they form
a representative sample with respect to plant age and ndividual plants per
species. This sample serves as a quick entry point for new users to the
dataset, allowing them to explore the data on a small scale and find the
parameters of data most useful for their application without having to deal
with hundreds of thousands of individual images.

    

### [[2108.05799] Scalable pragmatic communication via self-supervision](http://arxiv.org/abs/2108.05799)


  Models of context-sensitive communication often use the Rational Speech Act
framework (RSA; Frank & Goodman, 2012), which formulates listeners and speakers
in a cooperative reasoning process. However, the standard RSA formulation can
only be applied to small domains, and large-scale applications have relied on
imitating human behavior. Here, we propose a new approach to scalable
pragmatics, building upon recent theoretical results (Zaslavsky et al., 2020)
that characterize pragmatic reasoning in terms of general information-theoretic
principles. Specifically, we propose an architecture and learning process in
which agents acquire pragmatic policies via self-supervision instead of
imitating human data. This work suggests a new principled approach for
equipping artificial agents with pragmatic skills via self-supervision, which
is grounded both in pragmatic theory and in information theory.

    

### [[2108.05800] On Liquidity Mining for Uniswap v3](http://arxiv.org/abs/2108.05800)


  The recently proposed Uniswap v3 replaces the fungible liquidity provider
token (LP token) into non-fungible ones, making the design for liquidity mining
more difficult. In this paper, we propose a flexible liquidity mining scheme
that realizes the overall liquidity distribution through the fine control of
local rewards. From the liquidity provider's point of view, the liquidity
provision strategy forms a multiplayer zero-sum game. We analyze the Nash
Equilibrium and the corresponding strategy, approximately, deploying the
liquidity proportional to the reward distribution, in some special cases and
use it to guide the general situations. Based on the strategic response above,
such a scheme allows the mining rewards provider to optimize the distribution
of liquidity for the purpose such as low slippage and price stabilization.

    

### [[2108.05809] Competency Model Approach to AI Literacy: Research-based Path from Initial Framework to Model](http://arxiv.org/abs/2108.05809)


  The recent developments in Artificial Intelligence (AI) technologies
challenge educators and educational institutions to respond with curriculum and
resources that prepare students of all ages with the foundational knowledge and
skills for success in the AI workplace. Research on AI Literacy could lead to
an effective and practical platform for developing these skills. We propose and
advocate for a pathway for developing AI Literacy as a pragmatic and useful
tool for AI education. Such a discipline requires moving beyond a conceptual
framework to a multi-level competency model with associated competency
assessments. This approach to an AI Literacy could guide future development of
instructional content as we prepare a range of groups (i.e., consumers,
co-workers, collaborators, and creators). We propose here a research matrix as
an initial step in the development of a roadmap for AI Literacy research, which
requires a systematic and coordinated effort with the support of publication
outlets and research funding, to expand the areas of competency and
assessments.

    

### [[2108.05872] HAC Explore: Accelerating Exploration with Hierarchical Reinforcement Learning](http://arxiv.org/abs/2108.05872)


  Sparse rewards and long time horizons remain challenging for reinforcement
learning algorithms. Exploration bonuses can help in sparse reward settings by
encouraging agents to explore the state space, while hierarchical approaches
can assist with long-horizon tasks by decomposing lengthy tasks into shorter
subtasks. We propose HAC Explore (HACx), a new method that combines these
approaches by integrating the exploration bonus method Random Network
Distillation (RND) into the hierarchical approach Hierarchical Actor-Critic
(HAC). HACx outperforms either component method on its own, as well as an
existing approach to combining hierarchy and exploration, in a set of difficult
simulated robotics tasks. HACx is the first RL method to solve a sparse reward,
continuous-control task that requires over 1,000 actions.

    

### [[2012.12556] A Survey on Vision Transformer](http://arxiv.org/abs/2012.12556)


  Transformer, first applied to the field of natural language processing, is a
type of deep neural network mainly based on the self-attention mechanism.
Thanks to its strong representation capabilities, researchers are looking at
ways to apply transformer to computer vision tasks. In a variety of visual
benchmarks, transformer-based models perform similar to or better than other
types of networks such as convolutional and recurrent networks. Given its high
performance and less need for vision-specific inductive bias, transformer is
receiving more and more attention from the computer vision community. In this
paper, we review these vision transformer models by categorizing them in
different tasks and analyzing their advantages and disadvantages. The main
categories we explore include the backbone network, high/mid-level vision,
low-level vision, and video processing. We also include efficient transformer
methods for pushing transformer into real device-based applications.
Furthermore, we also take a brief look at the self-attention mechanism in
computer vision, as it is the base component in transformer. Toward the end of
this paper, we discuss the challenges and provide several further research
directions for vision transformers.

    

### [[2103.15551] Toward Building Science Discovery Machines](http://arxiv.org/abs/2103.15551)


  The dream of building machines that can do science has inspired scientists
for decades. Remarkable advances have been made recently; however, we are still
far from achieving this goal. In this paper, we focus on the scientific
discovery process where a high level of reasoning and remarkable
problem-solving ability are required. We review different machine learning
techniques used in scientific discovery with their limitations. We survey and
discuss the main principles driving the scientific discovery process. These
principles are used in different fields and by different scientists to solve
problems and discover new knowledge. We provide many examples of the use of
these principles in different fields such as physics, mathematics, and biology.
We also review AI systems that attempt to implement some of these principles.
We argue that building science discovery machines should be guided by these
principles as an alternative to the dominant approach of current AI systems
that focuses on narrow objectives. Building machines that fully incorporate
these principles in an automated way might open the doors for many
advancements.

    

### [[2108.05691] Can We Spot Energy Regressions using Developers Tests?](http://arxiv.org/abs/2108.05691)


  Software Energy Consumption(SEC) is gaining more and more attention. In this
paper, we tackle the problem of hinting developers about the SEC of their
programs in the context of software developments based on Continuous
Integration(CI). In this study, we investigate if the CI can leverage
developers' tests to perform a new class of tests: the energy regression
testing. Energy regression is similar to performance regression but focused on
the energy consumption of the program instead of standard performance
indicators, like execution time or memory consumption. We propose to perform an
exploratory study of the usage of developers' tests for energy regression
testing. We propose to first investigate if developers' tests can be used to
obtain stable SEC indicators. Then, to consider if comparing the SEC of
developers' tests between two versions can accurately spot energy regressions
introduced by automated program mutations. Finally, to assess if it can
successfully pinpoint the source code lines guilty of energy regressions. Our
study will pave the way for automated SEC regression tools that can be readily
deployed inside an existing CI infrastructure to raise awareness of SEC issues
among practitioners.

    

### [[2108.05710] Exploration of lane-changing duration for heavy vehicles and passenger cars: a survival analysis approach](http://arxiv.org/abs/2108.05710)


  Lane-changing (LC) behavior describes the lateral movement of the vehicle
from the current-lane to the target-lane while proceeding forward. Among the
many research directions, LC duration (LCD) measures the total time it takes
for a vehicle to travel from the current lane to the target lane, which is an
indispensable indicator to characterize the LC behavior. Although existing
research has made some achievements, less attention has been paid to the
research of heavy vehicles' LCD. Therefore, this paper aims to further explore
the LCD between heavy vehicles and passenger cars. LC trajectories are
extracted from the newly-released HighD dataset, which contains of 16.5 hours
of measurement and over 11,000 vehicles. The survival function of LCD has been
estimated, and the characteristic has been analyzed. Thereafter, the
Accelerated Failure Time model is introduced to explore the influencing
factors. Results demonstrate that the MST value of passenger cars and heavy
vehicles is about 5.51s and 6.08s. The heavy vehicles would maintain a longer
time-headway and distance-headway with preceding vehicle when performing LC.
Nevertheless, these two factors do not significantly affect the LCD of heavy
vehicles. Finally, the results and the modeling implications have been
discussed. We hope this paper could contribute to our further understanding of
the LC behaviors for heavy vehicles and passenger cars.

    

### [[1803.10688] Dispatching to Parallel Servers: Solutions of Poisson's Equation for First-Policy Improvement](http://arxiv.org/abs/1803.10688)


  Policy iteration techniques for multiple-server dispatching rely on the
computation of value functions. In this context, we consider the
continuous-space M/G/1-FCFS queue endowed with an arbitrarily-designed cost
function for the waiting times of the incoming jobs. The associated relative
value function is a solution of Poisson's equation for Markov chains, which in
this work we solve in the Laplace transform domain by considering an ancillary,
underlying stochastic process extended to (imaginary) negative backlog states.
This construction enables us to issue closed-form relative value functions for
polynomial and exponential cost functions and for piecewise compositions of the
latter, in turn permitting the derivation of interval bounds for the relative
value function in the form of power series or trigonometric sums. We review
various cost approximation schemes and assess the convergence of the interval
bounds these induce on the relative value function. Namely: Taylor expansions
(divergent, except for a narrow class of entire functions with low orders of
growth), and uniform approximation schemes (polynomials, trigonometric), which
achieve optimal convergence rates over finite intervals. This study addresses
all the steps to implementing dispatching policies for systems of parallel
servers, from the specification of general cost functions towards the
computation of interval bounds for the relative value functions and the exact
implementation of the first-policy improvement step.

    