
## 2021-7-9

### [<title>GPU accelerated SHAP values crash Google Colab - XGBoost</title>](https://discuss.xgboost.ai/t/gpu-accelerated-shap-values-crash-google-colab/2361/3)

### [<title>GPU accelerated SHAP values crash Google Colab - XGBoost</title>](https://discuss.xgboost.ai/t/gpu-accelerated-shap-values-crash-google-colab/2361/2)

### [[2107.02992] A Dual-Port 8-T CAM-Based Network Intrusion Detection Engine for IoT](http://arxiv.org/abs/2107.02992)


  This letter presents an energy- and memory-efficient pattern-matching engine
for a network intrusion detection system (NIDS) in the Internet of Things.
Tightly coupled architecture and circuit co-designs are proposed to fully
exploit the statistical behaviors of NIDS pattern matching. The proposed engine
performs pattern matching in three phases, where the phase-1 prefix matching
employs reconfigurable pipelined automata processing to minimize memory
footprint without loss of throughput and efficiency. The processing elements
utilize 8-T content-addressable memory (CAM) cells for dual-port search by
leveraging proposed fixed-1s encoding. A 65-nm prototype demonstrates
best-in-class 1.54-fJ energy per search per pattern byte and 0.9-byte memory
usage per pattern byte.

    

### [<title>Early_stopping_rounds, max_bin, and overfitting - XGBoost</title>](https://discuss.xgboost.ai/t/early-stopping-rounds-max-bin-and-overfitting/2362/1)

### [[2107.03428] Management of Resource at the Network Edge for Federated Learning](http://arxiv.org/abs/2107.03428)


  Federated learning has been explored as a promising solution for training at
the edge, where end devices collaborate to train models without sharing data
with other entities. Since the execution of these learning models occurs at the
edge, where resources are limited, new solutions must be developed. In this
paper, we describe the recent work on resource management at the edge, and
explore the challenges and future directions to allow the execution of
federated learning at the edge. Some of the problems of this management, such
as discovery of resources, deployment, load balancing, migration, and energy
efficiency will be discussed in the paper.

    

### [[2107.03484] An Overview of Low latency for Wireless Communications: an Evolutionary Perspective](http://arxiv.org/abs/2107.03484)


  Ultra-low latency supported by the fifth generation (5G) give impetus to the
prosperity of many wireless network applications, such as autonomous driving,
robotics, telepresence, virtual reality and so on. Ultra-low latency is not
achieved in a moment, but requires long-term evolution of network structure and
key enabling communication technologies. In this paper, we provide an
evolutionary overview of low latency in mobile communication systems, including
two different evolutionary perspectives: 1) network architecture; 2) physical
layer air interface technologies. We firstly describe in detail the evolution
of communication network architecture from the second generation (2G) to 5G,
highlighting the key points reducing latency. Moreover, we review the evolution
of key enabling technologies in the physical layer from 2G to 5G, which is also
aimed at reducing latency. We also discussed the challenges and future research
directions for low latency in network architecture and physical layer.

    

### [[2107.03541] Tuning Channel Access to Enable Real-Time Applications in Wi-Fi 7](http://arxiv.org/abs/2107.03541)


  Real-Time Applications (RTA) are among the most important use cases for
future Wi-Fi 7, defined by the IEEE 802.11be standard. This paper studies two
backward-compatible channel access approaches to satisfy the strict quality of
service (QoS) requirements of RTA on the transmission latency and packet loss
rate that have been considered in the 802.11be Task Group. The first approach
is based on limiting the transmission duration of non-RTA frames in the
network. The second approach is based on preliminary channel access to ensure
the timely delivery of RTA frames. With the developed mathematical model of
these approaches, it is shown that both of them can satisfy the RTA QoS
requirements. At the same time, the preliminary channel access provides up to
60% higher efficiency of the channel usage by the non-RTA traffic in scenarios
with very strict RTA QoS requirements or with low intensity of the RTA traffic.

    

### [[2107.03543] Resource Allocation Strategies for Real-Time Applications in Wi-Fi 7](http://arxiv.org/abs/2107.03543)


  In 2019 IEEE 802 LAN/MAN Standards Committee started the development of the
next major amendment of the Wi-Fi standard: the IEEE 802.11be, also known as
Wi-Fi 7. This new amendment will introduce many new functions and will improve
the existing ones that will make Wi-Fi more efficient in many new scenarios.
One of the scenarios is the service of Real-Time Applications with strict
requirements on latency and reliability of communications. Providing low
latencies can be challenging in Wi-Fi because of the unlicensed spectrum and
related interference from neighboring devices. In this paper, we consider the
usage of OFDMA transmissions for Real-Time Applications and design resource
allocation algorithms that can provide the required latency and reliability in
the presence of interference.

    

### [[2107.03924] Smart Healthcare in the Age of AI: Recent Advances, Challenges, and Future Prospects](http://arxiv.org/abs/2107.03924)


  The significant increase in the number of individuals with chronic ailments
(including the elderly and disabled) has dictated an urgent need for an
innovative model for healthcare systems. The evolved model will be more
personalized and less reliant on traditional brick-and-mortar healthcare
institutions such as hospitals, nursing homes, and long-term healthcare
centers. The smart healthcare system is a topic of recently growing interest
and has become increasingly required due to major developments in modern
technologies, especially in artificial intelligence (AI) and machine learning
(ML). This paper is aimed to discuss the current state-of-the-art smart
healthcare systems highlighting major areas like wearable and smartphone
devices for health monitoring, machine learning for disease diagnosis, and the
assistive frameworks, including social robots developed for the ambient
assisted living environment. Additionally, the paper demonstrates software
integration architectures that are very significant to create smart healthcare
systems, integrating seamlessly the benefit of data analytics and other tools
of AI. The explained developed systems focus on several facets: the
contribution of each developed framework, the detailed working procedure, the
performance as outcomes, and the comparative merits and limitations. The
current research challenges with potential future directions are addressed to
highlight the drawbacks of existing systems and the possible methods to
introduce novel frameworks, respectively. This review aims at providing
comprehensive insights into the recent developments of smart healthcare systems
to equip experts to contribute to the field.

    

### [[2107.03933] A Federated Semi-Supervised Learning Approach for Network Traffic Classification](http://arxiv.org/abs/2107.03933)


  Network traffic classification, a task to classify network traffic and
identify its type, is the most fundamental step to improve network services and
manage modern networks. Classical machine learning and deep learning method
have developed well in the field of network traffic classification. However,
there are still two major challenges. One is how to protect the privacy of
users' traffic data, and the other is that it is difficult to obtain labeled
data in reality. In this paper, we propose a novel approach using federated
semi-supervised learning for network traffic classification. In our approach,
the federated servers and several clients work together to train a global
classification model. Among them, unlabeled data is used on the client, and
labeled data is used on the server. Moreover, we use two traffic subflow
sampling methods: simple sampling and incremental sampling for data
preprocessing. The experimental results in the QUIC dataset show that the
accuracy of our federated semi-supervised approach can reach 91.08% and 97.81%
when using the simple sampling method and incremental sampling method
respectively. The experimental results also show that the accuracy gap between
our method and the centralized training method is minimal, and it can
effectively protect users' privacy and does not require a large amount of
labeled data.

    

### [[2107.03988] Longitudinal Study of an IP Geolocation Database](http://arxiv.org/abs/2107.03988)


  IP geolocation - the process of mapping network identifiers to physical
locations - has myriad applications. We examine a large collection of snapshots
from a popular geolocation database and take a first look at its longitudinal
properties. We define metrics of IP geo-persistence, prevalence, coverage, and
movement, and analyse 10 years of geolocation data at different location
granularities. Across different classes of IP addresses, we find that
significant location differences can exist even between successive instances of
the database - a previously underappreciated source of potential error when
using geolocation data: 47% of end users IP addresses move by more than 40 km
in 2019. To assess the sensitivity of research results to the instance of the
geo database, we reproduce prior research that depended on geolocation lookups.
In this case study, which analyses geolocation database performance on routers,
we demonstrate impact of these temporal effects: median distance from ground
truth shifted from 167 km to 40 km when using a two months apart snapshot.
Based on our findings, we make recommendations for best practices when using
geolocation databases in order to best encourage reproducibility and sound
measurement.

    

### [[2007.12963] Minimum Overhead Beamforming and Resource Allocation in D2D Edge Networks](http://arxiv.org/abs/2007.12963)


  Device-to-device (D2D) communications is expected to be a critical enabler of
distributed computing in edge networks at scale. A key challenge in providing
this capability is the requirement for judicious management of the
heterogeneous communication and computation resources that exist at the edge to
meet processing needs. In this paper, we develop an optimization methodology
that considers the network topology jointly with device and network resource
allocation to minimize total D2D overhead, which we quantify in terms of time
and energy required for task processing. Variables in our model include task
assignment, CPU allocation, subchannel selection, and beamforming design for
multiple-input multiple-output (MIMO) wireless devices. We propose two methods
to solve the resulting non-convex mixed integer program: semi-exhaustive search
optimization, which represents a "best-effort" at obtaining the optimal
solution, and efficient alternate optimization, which is more computationally
efficient. As a component of these two methods, we develop a novel coordinated
beamforming algorithm which we show obtains the optimal beamformer for a common
receiver characteristic. Through numerical experiments, we find that our
methodology yields substantial improvements in network overhead compared with
local computation and partially optimized methods, which validates our joint
optimization approach. Further, we find that the efficient alternate
optimization scales well with the number of nodes, and thus can be a practical
solution for D2D computing in large networks.

    

### [[2106.06949] How Crucial is it for 6G Networks to be Autonomous?](http://arxiv.org/abs/2106.06949)


  The sixth generation (6G), unlike any of the previous generations, is
envisioned by 2030 to connect everything. Moreover, in addition to the new use
cases, 6G is expected to support, it will need to provide a superior
performance over 5G. The global connectivity, large network dimensions, users
heterogeneity, extremely low-power consumption, high throughput, ultrahigh
reliability, efficient network operation and maintenance, and low-latency
requirements to be met by future networks inevitably necessitate the autonomy
of 6G. Intelligence, facilitated mainly by the advancement of artificial
intelligence (AI) techniques, is a key to achieve autonomy. In this paper, we
provide a bird's-eye view of 6G, its vision, progress, and objectives.
Furthermore, we present some technologies that would be mainly enabling
intelligent globally connected world. In addition to discussing the role of AI
for future wireless communications, we, unlike any other review papers, provide
our original results which give early evidence for the viability of achieving
6G networks autonomy through leveraging AI advances. Furthermore, we, very
importantly, identify 6G implementation challenges and key innovative
techniques that promise to solve them. This article serves as a starting point
for learners to acquire more knowledge about 6G and also for researchers to
promote more development to the field.

    

### [[2107.01001] Feeling of Presence Maximization: mmWave-Enabled Virtual Reality Meets Deep Reinforcement Learning](http://arxiv.org/abs/2107.01001)


  This paper investigates the problem of providing ultra-reliable and
energy-efficient virtual reality (VR) experiences for wireless mobile users. To
ensure reliable ultra-high-definition (UHD) video frame delivery to mobile
users and enhance their immersive visual experiences, a coordinated multipoint
(CoMP) transmission technique and millimeter wave (mmWave) communications are
exploited. Owing to user movement and time-varying wireless channels, the
wireless VR experience enhancement problem is formulated as a
sequence-dependent and mixed-integer problem with a goal of maximizing users'
feeling of presence (FoP) in the virtual world, subject to power consumption
constraints on access points (APs) and users' head-mounted displays (HMDs). The
problem, however, is hard to be directly solved due to the lack of users'
accurate tracking information and the sequence-dependent and mixed-integer
characteristics. To overcome this challenge, we develop a parallel echo state
network (ESN) learning method to predict users' tracking information by
training fresh and historical tracking samples separately collected by APs.
With the learnt results, we propose a deep reinforcement learning (DRL) based
optimization algorithm to solve the formulated problem. In this algorithm, we
implement deep neural networks (DNNs) as a scalable solution to produce integer
decision variables and solving a continuous power control problem to criticize
the integer decision variables. Finally, the performance of the proposed
algorithm is compared with various benchmark algorithms, and the impact of
different design parameters is also discussed. Simulation results demonstrate
that the proposed algorithm is more 4.14% energy-efficient than the benchmark
algorithms.

    

### [[2107.03383] Assessing putative bias in prediction of anti-microbial resistance from real-world genotyping data under explicit causal assumptions](http://arxiv.org/abs/2107.03383)


  Whole genome sequencing (WGS) is quickly becoming the customary means for
identification of antimicrobial resistance (AMR) due to its ability to obtain
high resolution information about the genes and mechanisms that are causing
resistance and driving pathogen mobility. By contrast, traditional phenotypic
(antibiogram) testing cannot easily elucidate such information. Yet development
of AMR prediction tools from genotype-phenotype data can be biased, since
sampling is non-randomized. Sample provenience, period of collection, and
species representation can confound the association of genetic traits with AMR.
Thus, prediction models can perform poorly on new data with sampling
distribution shifts. In this work -- under an explicit set of causal
assumptions -- we evaluate the effectiveness of propensity-based rebalancing
and confounding adjustment on AMR prediction using genotype-phenotype AMR data
from the Pathosystems Resource Integration Center (PATRIC). We select bacterial
genotypes (encoded as k-mer signatures, i.e. DNA fragments of length k),
country, year, species, and AMR phenotypes for the tetracycline drug class,
preparing test data with recent genomes coming from a single country. We test
boosted logistic regression (BLR) and random forests (RF) with/without
bias-handling. On 10,936 instances, we find evidence of species, location and
year imbalance with respect to the AMR phenotype. The crude versus
bias-adjusted change in effect of genetic signatures on AMR varies but only
moderately (selecting the top 20,000 out of 40+ million k-mers). The area under
the receiver operating characteristic (AUROC) of the RF (0.95) is comparable to
that of BLR (0.94) on both out-of-bag samples from bootstrap and the external
test (n=1,085), where AUROCs do not decrease. We observe a 1%-5% gain in AUROC
with bias-handling compared to the sole use of genetic signatures. ...

    

### [[2107.03385] Rating and aspect-based opinion graph embeddings for explainable recommendations](http://arxiv.org/abs/2107.03385)


  The success of neural network embeddings has entailed a renewed interest in
using knowledge graphs for a wide variety of machine learning and information
retrieval tasks. In particular, recent recommendation methods based on graph
embeddings have shown state-of-the-art performance. In general, these methods
encode latent rating patterns and content features. Differently from previous
work, in this paper, we propose to exploit embeddings extracted from graphs
that combine information from ratings and aspect-based opinions expressed in
textual reviews. We then adapt and evaluate state-of-the-art graph embedding
techniques over graphs generated from Amazon and Yelp reviews on six domains,
outperforming baseline recommenders. Additionally, our method has the advantage
of providing explanations that involve the coverage of aspect-based opinions
given by users about recommended items.

    

### [[2107.03387] Sleep syndromes onset detection based on automatic sleep staging algorithm](http://arxiv.org/abs/2107.03387)


  In this paper, we propose a novel method and a practical approach to
predicting early onsets of sleep syndromes, including restless leg syndrome,
insomnia, based on an algorithm that is comprised of two modules. A Fast
Fourier Transform is applied to 30 seconds long epochs of EEG recordings to
provide localized time-frequency information, and a deep convolutional LSTM
neural network is trained for sleep stage classification. Automating sleep
stages detection from EEG data offers great potential to tackling sleep
irregularities on a daily basis. Thereby, a novel approach for sleep stage
classification is proposed which combines the best of signal processing and
statistics. In this study, we used the PhysioNet Sleep European Data Format
(EDF) Database. The code evaluation showed impressive results, reaching an
accuracy of 86.43, precision of 77.76, recall of 93,32, F1-score of 89.12 with
the final mean false error loss of 0.09.

    

### [[2107.03402] Self-organized criticality in neural networks](http://arxiv.org/abs/2107.03402)


  We demonstrate, both analytically and numerically, that learning dynamics of
neural networks is generically attracted towards a self-organized critical
state. The effect can be modeled with quartic interactions between
non-trainable variables (e.g. states of neurons) and trainable variables (e.g.
weight matrix). Non-trainable variables are rapidly driven towards stochastic
equilibrium and trainable variables are slowly driven towards learning
equilibrium described by a scale-invariant distribution on a wide range of
scales. Our results suggest that the scale invariance observed in many physical
and biological systems might be due to some kind of learning dynamics and
support the claim that the universe might be a neural network.

    

### [[2107.03423] Recurrence-Aware Long-Term Cognitive Network for Explainable Pattern Classification](http://arxiv.org/abs/2107.03423)


  Machine learning solutions for pattern classification problems are nowadays
widely deployed in society and industry. However, the lack of transparency and
accountability of most accurate models often hinders their meaningful and safe
use. Thus, there is a clear need for developing explainable artificial
intelligence mechanisms. There exist model-agnostic methods that summarize
feature contributions, but their interpretability is limited to specific
predictions made by black-box models. An open challenge is to develop models
that have intrinsic interpretability and produce their own explanations, even
for classes of models that are traditionally considered black boxes like
(recurrent) neural networks. In this paper, we propose an LTCN-based model for
interpretable pattern classification of structured data. Our method brings its
own mechanism for providing explanations by quantifying the relevance of each
feature in the decision process. For supporting the interpretability without
affecting the performance, the model incorporates more flexibility through a
quasi-nonlinear reasoning rule that allows controlling nonlinearity. Besides,
we propose a recurrence-aware decision model that evades the issues posed by
unique fixed points while introducing a deterministic learning method to
compute the learnable parameters. The simulations show that our interpretable
model obtains competitive performance when compared to the state-of-the-art
white and black boxes.

    

### [[2107.03427] Deep Learning for Two-Sided Matching](http://arxiv.org/abs/2107.03427)


  We initiate the use of a multi-layer neural network to model two-sided
matching and to explore the design space between strategy-proofness and
stability. It is well known that both properties cannot be achieved
simultaneously but the efficient frontier in this design space is not
understood. We show empirically that it is possible to achieve a good
compromise between stability and strategy-proofness-substantially better than
that achievable through a convex combination of deferred acceptance (stable and
strategy-proof for only one side of the market) and randomized serial
dictatorship (strategy-proof but not stable).

    

### [[2107.03432] IowaRain: A Statewide Rain Event Dataset Based on Weather Radars and Quantitative Precipitation Estimation](http://arxiv.org/abs/2107.03432)


  Effective environmental planning and management to address climate change
could be achieved through extensive environmental modeling with machine
learning and conventional physical models. In order to develop and improve
these models, practitioners and researchers need comprehensive benchmark
datasets that are prepared and processed with environmental expertise that they
can rely on. This study presents an extensive dataset of rainfall events for
the state of Iowa (2016-2019) acquired from the National Weather Service Next
Generation Weather Radar (NEXRAD) system and processed by a quantitative
precipitation estimation system. The dataset presented in this study could be
used for better disaster monitoring, response and recovery by paving the way
for both predictive and prescriptive modeling.

    

### [[2107.03433] In-Network Learning: Distributed Training and Inference in Networks](http://arxiv.org/abs/2107.03433)


  It is widely perceived that leveraging the success of modern machine learning
techniques to mobile devices and wireless networks has the potential of
enabling important new services. This, however, poses significant challenges,
essentially due to that both data and processing power are highly distributed
in a wireless network. In this paper, we develop a learning algorithm and an
architecture that make use of multiple data streams and processing units, not
only during the training phase but also during the inference phase. In
particular, the analysis reveals how inference propagates and fuses across a
network. We study the design criterion of our proposed method and its bandwidth
requirements. Also, we discuss implementation aspects using neural networks in
typical wireless radio access; and provide experiments that illustrate benefits
over state-of-the-art techniques.

    

### [[2107.03442] Modality Completion via Gaussian Process Prior Variational Autoencoders for Multi-Modal Glioma Segmentation](http://arxiv.org/abs/2107.03442)


  In large studies involving multi protocol Magnetic Resonance Imaging (MRI),
it can occur to miss one or more sub-modalities for a given patient owing to
poor quality (e.g. imaging artifacts), failed acquisitions, or hallway
interrupted imaging examinations. In some cases, certain protocols are
unavailable due to limited scan time or to retrospectively harmonise the
imaging protocols of two independent studies. Missing image modalities pose a
challenge to segmentation frameworks as complementary information contributed
by the missing scans is then lost. In this paper, we propose a novel model,
Multi-modal Gaussian Process Prior Variational Autoencoder (MGP-VAE), to impute
one or more missing sub-modalities for a patient scan. MGP-VAE can leverage the
Gaussian Process (GP) prior on the Variational Autoencoder (VAE) to utilize the
subjects/patients and sub-modalities correlations. Instead of designing one
network for each possible subset of present sub-modalities or using frameworks
to mix feature maps, missing data can be generated from a single model based on
all the available samples. We show the applicability of MGP-VAE on brain tumor
segmentation where either, two, or three of four sub-modalities may be missing.
Our experiments against competitive segmentation baselines with missing
sub-modality on BraTS'19 dataset indicate the effectiveness of the MGP-VAE
model for segmentation tasks.

    

### [[2107.03443] BumbleBee: A Transformer for Music](http://arxiv.org/abs/2107.03443)


  We will introduce BumbleBee, a transformer model that will generate MIDI
music data . We will tackle the issue of transformers applied to long sequences
by implementing a longformer generative model that uses dilating sliding
windows to compute the attention layers. We will compare our results to that of
the music transformer and Long-Short term memory (LSTM) to benchmark our
results. This analysis will be performed using piano MIDI files, in particular
, the JSB Chorales dataset that has already been used for other research works
(Huang et al., 2018)

    

### [[2107.03453] $S^3$: Sign-Sparse-Shift Reparametrization for Effective Training of Low-bit Shift Networks](http://arxiv.org/abs/2107.03453)


  Shift neural networks reduce computation complexity by removing expensive
multiplication operations and quantizing continuous weights into low-bit
discrete values, which are fast and energy efficient compared to conventional
neural networks. However, existing shift networks are sensitive to the weight
initialization, and also yield a degraded performance caused by vanishing
gradient and weight sign freezing problem. To address these issues, we propose
S low-bit re-parameterization, a novel technique for training low-bit shift
networks. Our method decomposes a discrete parameter in a sign-sparse-shift
3-fold manner. In this way, it efficiently learns a low-bit network with a
weight dynamics similar to full-precision networks and insensitive to weight
initialization. Our proposed training method pushes the boundaries of shift
neural networks and shows 3-bit shift networks out-performs their
full-precision counterparts in terms of top-1 accuracy on ImageNet.

    

### [[2107.03455] Model Selection for Generic Contextual Bandits](http://arxiv.org/abs/2107.03455)


  We consider the problem of model selection for the general stochastic
contextual bandits under the realizability assumption. We propose a successive
refinement based algorithm called Adaptive Contextual Bandit ({\ttfamily ACB}),
that works in phases and successively eliminates model classes that are too
simple to fit the given instance. We prove that this algorithm is adaptive,
i.e., the regret rate order-wise matches that of {\ttfamily FALCON}, the
state-of-art contextual bandit algorithm of Levi et. al '20, that needs
knowledge of the true model class. The price of not knowing the correct model
class is only an additive term contributing to the second order term in the
regret bound. This cost possess the intuitive property that it becomes smaller
as the model class becomes easier to identify, and vice-versa. We then show
that a much simpler explore-then-commit (ETC) style algorithm also obtains a
regret rate of matching that of {\ttfamily FALCON}, despite not knowing the
true model class. However, the cost of model selection is higher in ETC as
opposed to in {\ttfamily ACB}, as expected. Furthermore, {\ttfamily ACB}
applied to the linear bandit setting with unknown sparsity, order-wise recovers
the model selection guarantees previously established by algorithms tailored to
the linear setting.

    

### [[2107.03463] CHASE: Robust Visual Tracking via Cell-Level Differentiable Neural Architecture Search](http://arxiv.org/abs/2107.03463)


  A strong visual object tracker nowadays relies on its well-crafted modules,
which typically consist of manually-designed network architectures to deliver
high-quality tracking results. Not surprisingly, the manual design process
becomes a particularly challenging barrier, as it demands sufficient prior
experience, enormous effort, intuition and perhaps some good luck. Meanwhile,
neural architecture search has gaining grounds in practical applications such
as image segmentation, as a promising method in tackling the issue of automated
search of feasible network structures. In this work, we propose a novel
cell-level differentiable architecture search mechanism to automate the network
design of the tracking module, aiming to adapt backbone features to the
objective of a tracking network during offline training. The proposed approach
is simple, efficient, and with no need to stack a series of modules to
construct a network. Our approach is easy to be incorporated into existing
trackers, which is empirically validated using different differentiable
architecture search-based methods and tracking objectives. Extensive
experimental evaluations demonstrate the superior performance of our approach
over five commonly-used benchmarks. Meanwhile, our automated searching process
takes 41 (18) hours for the second (first) order DARTS method on the
TrackingNet dataset.

    

### [[2107.03474] Differentiable Random Access Memory using Lattices](http://arxiv.org/abs/2107.03474)


  We introduce a differentiable random access memory module with $O(1)$
performance regardless of size, scaling to billions of entries. The design
stores entries on points of a chosen lattice to calculate nearest neighbours of
arbitrary points efficiently by exploiting symmetries. Augmenting a standard
neural network architecture with a single memory layer based on this, we can
scale the parameter count up to memory limits with negligible computational
overhead, giving better accuracy at similar cost. On large language modelling
tasks, these enhanced models with larger capacity significantly outperform the
unmodified transformer baseline. We found continued scaling with memory size up
to the limits tested.

    

### [[2107.03483] Impossibility results for fair representations](http://arxiv.org/abs/2107.03483)


  With the growing awareness to fairness in machine learning and the
realization of the central role that data representation has in data processing
tasks, there is an obvious interest in notions of fair data representations.
The goal of such representations is that a model trained on data under the
representation (e.g., a classifier) will be guaranteed to respect some fairness
constraints.
Such representations are useful when they can be fixed for training models on
various different tasks and also when they serve as data filtering between the
raw data (known to the representation designer) and potentially malicious
agents that use the data under the representation to learn predictive models
and make decisions.
A long list of recent research papers strive to provide tools for achieving
these goals.
However, we prove that this is basically a futile effort. Roughly stated, we
prove that no representation can guarantee the fairness of classifiers for
different tasks trained using it; even the basic goal of achieving
label-independent Demographic Parity fairness fails once the marginal data
distribution shifts. More refined notions of fairness, like Odds Equality,
cannot be guaranteed by a representation that does not take into account the
task specific labeling rule with respect to which such fairness will be
evaluated (even if the marginal data distribution is known a priory).
Furthermore, except for trivial cases, no representation can guarantee Odds
Equality fairness for any two different tasks, while allowing accurate label
predictions for both.
While some of our conclusions are intuitive, we formulate (and prove) crisp
statements of such impossibilities, often contrasting impressions conveyed by
many recent works on fair representations.

    

### [[2107.03502] CSDI: Conditional Score-based Diffusion Models for Probabilistic Time Series Imputation](http://arxiv.org/abs/2107.03502)


  The imputation of missing values in time series has many applications in
healthcare and finance. While autoregressive models are natural candidates for
time series imputation, score-based diffusion models have recently outperformed
existing counterparts including autoregressive models in many tasks such as
image generation and audio synthesis, and would be promising for time series
imputation. In this paper, we propose Conditional Score-based Diffusion models
for Imputation (CSDI), a novel time series imputation method that utilizes
score-based diffusion models conditioned on observed data. Unlike existing
score-based approaches, the conditional diffusion model is explicitly trained
for imputation and can exploit correlations between observed values. On
healthcare and environmental data, CSDI improves by 40-70% over existing
probabilistic imputation methods on popular performance metrics. In addition,
deterministic imputation by CSDI reduces the error by 5-20% compared to the
state-of-the-art deterministic imputation methods. Furthermore, CSDI can also
be applied to time series interpolation and probabilistic forecasting, and is
competitive with existing baselines.

    

### [[2107.03520] Energy Efficient Federated Learning in Integrated Fog-Cloud Computing Enabled Internet-of-Things Networks](http://arxiv.org/abs/2107.03520)


  We investigate resource allocation scheme to reduce the energy consumption of
federated learning (FL) in the integrated fog-cloud computing enabled
Internet-of-things (IoT) networks. In the envisioned system, IoT devices are
connected with the centralized cloud server (CS) via multiple fog access points
(F-APs). We consider two different scenarios for training the local models. In
the first scenario, local models are trained at the IoT devices and the F-APs
upload the local model parameters to the CS. In the second scenario, local
models are trained at the F-APs based on the collected data from the IoT
devices and the F-APs collaborate with the CS for updating the model
parameters. Our objective is to minimize the overall energy-consumption of both
scenarios subject to FL time constraint. Towards this goal, we devise a joint
optimization of scheduling of IoT devices with the F-APs, transmit power
allocation, computation frequency allocation at the devices and F-APs and
decouple it into two subproblems. In the first subproblem, we optimize the IoT
device scheduling and power allocation, while in the second subproblem, we
optimize the computation frequency allocation. For each scenario, we develop a
conflict graph based solution to iteratively solve the two subproblems.
Simulation results show that the proposed two schemes achieve a considerable
performance gain in terms of the energy consumption minimization. The presented
simulation results interestingly reveal that for a large number of IoT devices
and large data sizes, it is more energy efficient to train the local models at
the IoT devices instead of the F-APs.

    

### [[2107.03577] Adaptive Stress Testing for Adversarial Learning in a Financial Environment](http://arxiv.org/abs/2107.03577)


  We demonstrate the use of Adaptive Stress Testing to detect and address
potential vulnerabilities in a financial environment. We develop a simplified
model for credit card fraud detection that utilizes a linear regression
classifier based on historical payment transaction data coupled with business
rules. We then apply the reinforcement learning model known as Adaptive Stress
Testing to train an agent, that can be thought of as a potential fraudster, to
find the most likely path to system failure -- successfully defrauding the
system. We show the connection between this most likely failure path and the
limits of the classifier and discuss how the fraud detection system's business
rules can be further augmented to mitigate these failure modes.

    

### [[2107.03588] Identification and Adaptation with Binary-Valued Observations under Non-Persistent Excitation Condition](http://arxiv.org/abs/2107.03588)


  Dynamical systems with binary-valued observations are widely used in
information industry, technology of biological pharmacy and other fields.
Though there have been much efforts devoted to the identification of such
systems, most of the previous investigations are based on first-order gradient
algorithm which usually has much slower convergence rate than the Quasi-Newton
algorithm. Moreover, persistence of excitation(PE) conditions are usually
required to guarantee consistent parameter estimates in the existing
literature, which are hard to be verified or guaranteed for feedback control
systems. In this paper, we propose an online projected Quasi-Newton type
algorithm for parameter estimation of stochastic regression models with
binary-valued observations and varying thresholds. By using both the stochastic
Lyapunov function and martingale estimation methods, we establish the strong
consistency of the estimation algorithm and provide the convergence rate, under
a signal condition which is considerably weaker than the traditional PE
condition and coincides with the weakest possible excitation known for the
classical least square algorithm of stochastic regression models. Convergence
of adaptive predictors and their applications in adaptive control are also
discussed.

    

### [[2107.03607] SpecGrav -- Detection of Gravitational Waves using Deep Learning](http://arxiv.org/abs/2107.03607)


  Gravitational waves are ripples in the fabric of space-time that travel at
the speed of light. The detection of gravitational waves by LIGO is a major
breakthrough in the field of astronomy. Deep Learning has revolutionized many
industries including health care, finance and education. Deep Learning
techniques have also been explored for detection of gravitational waves to
overcome the drawbacks of traditional matched filtering method. However, in
several researches, the training phase of neural network is very time consuming
and hardware devices with large memory are required for the task. In order to
reduce the extensive amount of hardware resources and time required in training
a neural network for detecting gravitational waves, we made SpecGrav. We use 2D
Convolutional Neural Network and spectrograms of gravitational waves embedded
in noise to detect gravitational waves from binary black hole merger and binary
neutron star merger. The training phase of our neural network was of about just
19 minutes on a 2GB GPU.

    

### [[2107.03620] Predicting Disease Progress with Imprecise Lab Test Results](http://arxiv.org/abs/2107.03620)


  In existing deep learning methods, almost all loss functions assume that
sample data values used to be predicted are the only correct ones. This
assumption does not hold for laboratory test data. Test results are often
within tolerable or imprecision ranges, with all values in the ranges
acceptable. By considering imprecision samples, we propose an imprecision range
loss (IR loss) method and incorporate it into Long Short Term Memory (LSTM)
model for disease progress prediction. In this method, each sample in
imprecision range space has a certain probability to be the real value,
participating in the loss calculation. The loss is defined as the integral of
the error of each point in the impression range space. A sampling method for
imprecision space is formulated. The continuous imprecision space is
discretized, and a sequence of imprecise data sets are obtained, which is
convenient for gradient descent learning. A heuristic learning algorithm is
developed to learn the model parameters based on the imprecise data sets.
Experimental results on real data show that the prediction method based on IR
loss can provide more stable and consistent prediction result when test samples
are generated from imprecision range.

    

### [[2107.03633] Generalization Error of GAN from the Discriminator's Perspective](http://arxiv.org/abs/2107.03633)


  The generative adversarial network (GAN) is a well-known model for learning
high-dimensional distributions, but the mechanism for its generalization
ability is not understood. In particular, GAN is vulnerable to the memorization
phenomenon, the eventual convergence to the empirical distribution. We consider
a simplified GAN model with the generator replaced by a density, and analyze
how the discriminator contributes to generalization. We show that with early
stopping, the generalization error measured by Wasserstein metric escapes from
the curse of dimensionality, despite that in the long term, memorization is
inevitable. In addition, we present a hardness of learning result for WGAN.

    

### [[2107.03635] Sublinear Regret for Learning POMDPs](http://arxiv.org/abs/2107.03635)


  We study the model-based undiscounted reinforcement learning for partially
observable Markov decision processes (POMDPs). The oracle we consider is the
optimal policy of the POMDP with a known environment in terms of the average
reward over an infinite horizon. We propose a learning algorithm for this
problem, building on spectral method-of-moments estimations for hidden Markov
models, the belief error control in POMDPs and upper-confidence-bound methods
for online learning. We establish a regret bound of $O(T^{2/3}\sqrt{\log T})$
for the proposed learning algorithm where $T$ is the learning horizon. This is,
to the best of our knowledge, the first algorithm achieving sublinear regret
with respect to our oracle for learning general POMDPs.

    

### [[2107.03645] A hybrid virtual sensing approach for approximating non-linear dynamic system behavior using LSTM networks](http://arxiv.org/abs/2107.03645)


  Modern Internet of Things solutions are used in a variety of different areas,
ranging from connected vehicles and healthcare to industrial applications. They
rely on a large amount of interconnected sensors, which can lead to both
technical and economical challenges. Virtual sensing techniques aim to reduce
the number of physical sensors in a system by using data from available
measurements to estimate additional unknown quantities of interest. Successful
model-based solutions include Kalman filters or the combination of finite
element models and modal analysis, while many data-driven methods rely on
machine learning algorithms. The presented hybrid virtual sensing approach
combines Long Short-Term Memory networks with frequency response function
models in order to estimate the behavior of non-linear dynamic systems with
multiple input and output channels. Network training and prediction make use of
short signal subsequences, which are later recombined by applying a windowing
technique. The frequency response function model acts as a baseline estimate
which perfectly captures linear dynamic systems and is augmented by the
non-linear Long Short-Term Memory network following two different hybrid
modeling strategies. The approach is tested using a non-linear experimental
dataset, which results from measurements of a three-component servo-hydraulic
fatigue test bench. A variety of metrics in time and frequency domains, as well
as fatigue strength under variable amplitudes are used to evaluate the
approximation quality of the proposed method. In addition to virtual sensing,
the algorithm is also applied to a forward prediction task. Synthetic data are
used in a separate study to estimate the prediction quality on datasets of
different size.

    

### [[2107.03651] Elastic deformation of optical coherence tomography images of diabetic macular edema for deep-learning models training: how far to go?](http://arxiv.org/abs/2107.03651)


  To explore the clinical validity of elastic deformation of optical coherence
tomography (OCT) images for data augmentation in the development of
deep-learning model for detection of diabetic macular edema (DME).

    

### [[2107.03653] MAFIA: Machine Learning Acceleration on FPGAs for IoT Applications](http://arxiv.org/abs/2107.03653)


  Recent breakthroughs in ML have produced new classes of models that allow ML
inference to run directly on milliwatt-powered IoT devices. On one hand,
existing ML-to-FPGA compilers are designed for deep neural-networks on large
FPGAs. On the other hand, general-purpose HLS tools fail to exploit properties
specific to ML inference, thereby resulting in suboptimal performance. We
propose MAFIA, a tool to compile ML inference on small form-factor FPGAs for
IoT applications. MAFIA provides native support for linear algebra operations
and can express a variety of ML algorithms, including state-of-the-art models.
We show that MAFIA-generated programs outperform best-performing variant of a
commercial HLS compiler by 2.5x on average.

    

### [[2107.03673] MOD-Net: A Machine Learning Approach via Model-Operator-Data Network for Solving PDEs](http://arxiv.org/abs/2107.03673)


  In this paper, we propose a model-operator-data network (MOD-Net) for solving
PDEs. A MOD-Net is driven by a model to solve PDEs based on operator
representation with regularization from data. In this work, we use a deep
neural network to parameterize the Green's function. The empirical risk
consists of the mean square of the governing equation, boundary conditions, and
a few labels, which are numerically computed by traditional schemes on coarse
grid points with cheap computation cost. With only the labeled dataset or only
the model constraints, it is insufficient to accurately train a MOD-Net for
complicate problems. Intuitively, the labeled dataset works as a regularization
in addition to the model constraints. The MOD-Net is much efficient than
original neural operator because the MOD-Net also uses the information of
governing equation and the boundary conditions of the PDE rather than purely
the expensive labels. Since the MOD-Net learns the Green's function of a PDE,
it solves a type of PDEs but not a specific case. We numerically show MOD-Net
is very efficient in solving Poisson equation and one-dimensional Boltzmann
equation. For non-linear PDEs, where the concept of the Green's function does
not apply, the non-linear MOD-Net can be similarly used as an ansatz for
solving non-linear PDEs.

    

### [[2107.03690] Proceedings of the First Workshop on Weakly Supervised Learning (WeaSuL)](http://arxiv.org/abs/2107.03690)


  Welcome to WeaSuL 2021, the First Workshop on Weakly Supervised Learning,
co-located with ICLR 2021. In this workshop, we want to advance theory, methods
and tools for allowing experts to express prior coded knowledge for automatic
data annotations that can be used to train arbitrary deep neural networks for
prediction. The ICLR 2021 Workshop on Weak Supervision aims at advancing
methods that help modern machine-learning methods to generalize from knowledge
provided by experts, in interaction with observable (unlabeled) data. In total,
15 papers were accepted. All the accepted contributions are listed in these
Proceedings.

    

### [[2107.03704] Digitizing Handwriting with a Sensor Pen: A Writer-Independent Recognizer](http://arxiv.org/abs/2107.03704)


  Online handwriting recognition has been studied for a long time with only few
practicable results when writing on normal paper. Previous approaches using
sensor-based devices encountered problems that limited the usage of the
developed systems in real-world applications. This paper presents a
writer-independent system that recognizes characters written on plain paper
with the use of a sensor-equipped pen. This system is applicable in real-world
applications and requires no user-specific training for recognition. The pen
provides linear acceleration, angular velocity, magnetic field, and force
applied by the user, and acts as a digitizer that transforms the analogue
signals of the sensors into timeseries data while writing on regular paper. The
dataset we collected with this pen consists of Latin lower-case and upper-case
alphabets. We present the results of a convolutional neural network model for
letter classification and show that this approach is practical and achieves
promising results for writer-independent character recognition. This work aims
at providing a realtime handwriting recognition system to be used for writing
on normal paper.

    

### [[2107.03719] Bag of Tricks for Neural Architecture Search](http://arxiv.org/abs/2107.03719)


  While neural architecture search methods have been successful in previous
years and led to new state-of-the-art performance on various problems, they
have also been criticized for being unstable, being highly sensitive with
respect to their hyperparameters, and often not performing better than random
search. To shed some light on this issue, we discuss some practical
considerations that help improve the stability, efficiency and overall
performance.

    

### [[2107.03729] The Three Ensemble Clustering (3EC) Algorithm for Pattern Discovery in Unsupervised Learning](http://arxiv.org/abs/2107.03729)


  This paper presents a multiple learner algorithm called the 'Three Ensemble
Clustering 3EC' algorithm that classifies unlabeled data into quality clusters
as a part of unsupervised learning. It offers the flexibility to explore the
context of new clusters formed by an ensemble of algorithms based on internal
validation indices.
It is worth mentioning that the input data set is considered to be a cluster
of clusters. An anomaly can possibly manifest as a cluster as well. Each
partitioned cluster is considered to be a new data set and is a candidate to
explore the most optimal algorithm and its number of partition splits until a
predefined stopping criteria is met. The algorithms independently partition the
data set into clusters and the quality of the partitioning is assessed by an
ensemble of internal cluster validation indices. The 3EC algorithm presents the
validation index scores from a choice of algorithms and its configuration of
partitions and it is called the Tau Grid. 3EC chooses the most optimal score.
The 3EC algorithm owes its name to the two input ensembles of algorithms and
internal validation indices and an output ensemble of final clusters.
Quality plays an important role in this clustering approach and it also acts
as a stopping criteria from further partitioning. Quality is determined based
on the quality of the clusters provided by an algorithm and its optimal number
of splits. The 3EC algorithm determines this from the score of the ensemble of
validation indices. The user can configure the stopping criteria by providing
quality thresholds for the score range of each of the validation indices and
the optimal size of the output cluster. The users can experiment with different
sets of stopping criteria and choose the most 'sensible group' of quality
clusters

    

### [[2107.03730] Encoding Domain Information with Sparse Priors for Inferring Explainable Latent Variables](http://arxiv.org/abs/2107.03730)


  Latent variable models are powerful statistical tools that can uncover
relevant variation between patients or cells, by inferring unobserved hidden
states from observable high-dimensional data. A major shortcoming of current
methods, however, is their inability to learn sparse and interpretable hidden
states. Additionally, in settings where partial knowledge on the latent
structure of the data is readily available, a statistically sound integration
of prior information into current methods is challenging. To address these
issues, we propose spex-LVM, a factorial latent variable model with sparse
priors to encourage the inference of explainable factors driven by
domain-relevant information. spex-LVM utilizes existing knowledge of curated
biomedical pathways to automatically assign annotated attributes to latent
factors, yielding interpretable results tailored to the corresponding domain of
interest. Evaluations on simulated and real single-cell RNA-seq datasets
demonstrate that our model robustly identifies relevant structure in an
inherently explainable manner, distinguishes technical noise from sources of
biomedical variation, and provides dataset-specific adaptations of existing
pathway annotations. Implementation is available at
this https URL.

    

### [[2107.03738] Direct detection of plasticity onset through total-strain profile evolution](http://arxiv.org/abs/2107.03738)


  Plastic yielding in solids strongly depends on various conditions, such as
temperature and loading rate and indeed, sample-dependent knowledge of yield
points in structural materials promotes reliability in mechanical behavior.
Commonly, yielding is measured through controlled mechanical testing at small
or large scales, in ways that either distinguish elastic (stress) from total
deformation measurements, or by identifying plastic slip contributions. In this
paper we argue that instead of separate elastic/plastic measurements, yielding
can be unraveled through statistical analysis of total strain fluctuations
during the evolution sequence of profiles measured in-situ, through digital
image correlation. We demonstrate two distinct ways of precisely quantifying
yield locations in widely applicable crystal plasticity models, that apply in
polycrystalline solids, either by using principal component analysis or
discrete wavelet transforms. We test and compare these approaches in synthetic
data of polycrystal simulations and a variety of yielding responses, through
changes of the applied loading rates and the strain-rate sensitivity exponents.

    

### [[2107.03741] Adaptation of Quadruped Robot Locomotion with Meta-Learning](http://arxiv.org/abs/2107.03741)


  Animals have remarkable abilities to adapt locomotion to different terrains
and tasks. However, robots trained by means of reinforcement learning are
typically able to solve only a single task and a transferred policy is usually
inferior to that trained from scratch. In this work, we demonstrate that
meta-reinforcement learning can be used to successfully train a robot capable
to solve a wide range of locomotion tasks. The performance of the meta-trained
robot is similar to that of a robot that is trained on a single task.

    

### [[2107.03742] Grid Partitioned Attention: Efficient TransformerApproximation with Inductive Bias for High Resolution Detail Generation](http://arxiv.org/abs/2107.03742)


  Attention is a general reasoning mechanism than can flexibly deal with image
information, but its memory requirements had made it so far impractical for
high resolution image generation. We present Grid Partitioned Attention (GPA),
a new approximate attention algorithm that leverages a sparse inductive bias
for higher computational and memory efficiency in image domains: queries attend
only to few keys, spatially close queries attend to close keys due to
correlations. Our paper introduces the new attention layer, analyzes its
complexity and how the trade-off between memory usage and model power can be
tuned by the hyper-parameters.We will show how such attention enables novel
deep learning architectures with copying modules that are especially useful for
conditional image generation tasks like pose morphing. Our contributions are
(i) algorithm and code1of the novel GPA layer, (ii) a novel deep
attention-copying architecture, and (iii) new state-of-the art experimental
results in human pose morphing generation benchmarks.

    

### [[2107.03743] Probabilistic Time Series Forecasting with Implicit Quantile Networks](http://arxiv.org/abs/2107.03743)


  Here, we propose a general method for probabilistic time series forecasting.
We combine an autoregressive recurrent neural network to model temporal
dynamics with Implicit Quantile Networks to learn a large class of
distributions over a time-series target. When compared to other probabilistic
neural forecasting models on real- and simulated data, our approach is
favorable in terms of point-wise prediction accuracy as well as on estimating
the underlying temporal distribution.

    

### [[2107.03759] Analytically Tractable Hidden-States Inference in Bayesian Neural Networks](http://arxiv.org/abs/2107.03759)


  With few exceptions, neural networks have been relying on backpropagation and
gradient descent as the inference engine in order to learn the model
parameters, because the closed-form Bayesian inference for neural networks has
been considered to be intractable. In this paper, we show how we can leverage
the tractable approximate Gaussian inference's (TAGI) capabilities to infer
hidden states, rather than only using it for inferring the network's
parameters. One novel aspect it allows is to infer hidden states through the
imposition of constraints designed to achieve specific objectives, as
illustrated through three examples: (1) the generation of adversarial-attack
examples, (2) the usage of a neural network as a black-box optimization method,
and (3) the application of inference on continuous-action reinforcement
learning. These applications showcase how tasks that were previously reserved
to gradient-based optimization approaches can now be approached with
analytically tractable inference

    

### [[2107.03769] Image Resolution Susceptibility of Face Recognition Models](http://arxiv.org/abs/2107.03769)


  Face recognition approaches often rely on equal image resolution for
verification faces on two images. However, in practical applications, those
image resolutions are usually not in the same range due to different image
capture mechanisms or sources. In this work, we first analyze the impact of
image resolutions on the face verification performance with a state-of-the-art
face recognition model. For images, synthetically reduced to $5\, \times 5\,
\mathrm{px}$ resolution, the verification performance drops from $99.23\%$
increasingly down to almost $55\%$. Especially, for cross-resolution image
pairs (one high- and one low-resolution image), the verification accuracy
decreases even further. We investigate this behavior more in-depth by looking
at the feature distances for every 2-image test pair. To tackle this problem,
we propose the following two methods: 1) Train a state-of-the-art
face-recognition model straightforward with $50\%$ low-resolution images
directly within each batch. \\ 2) Train a siamese-network structure and adding
a cosine distance feature loss between high- and low-resolution features. Both
methods show an improvement for cross-resolution scenarios and can increase the
accuracy at very low resolution to approximately $70\%$. However, a
disadvantage is that a specific model needs to be trained for every
resolution-pair ...

    

### [[2107.03770] Federated Learning as a Mean-Field Game](http://arxiv.org/abs/2107.03770)


  We establish a connection between federated learning, a concept from machine
learning, and mean-field games, a concept from game theory and control theory.
In this analogy, the local federated learners are considered as the players and
the aggregation of the gradients in a central server is the mean-field effect.
We present federated learning as a differential game and discuss the properties
of the equilibrium of this game. We hope this novel view to federated learning
brings together researchers from these two distinct areas to work on
fundamental problems of large-scale distributed and privacy-preserving learning
algorithms.

    

### [[2107.03774] Optimizing Data Processing in Space for Object Detection in Satellite Imagery](http://arxiv.org/abs/2107.03774)


  There is a proliferation in the number of satellites launched each year,
resulting in downlinking of terabytes of data each day. The data received by
ground stations is often unprocessed, making this an expensive process
considering the large data sizes and that not all of the data is useful. This,
coupled with the increasing demand for real-time data processing, has led to a
growing need for on-orbit processing solutions. In this work, we investigate
the performance of CNN-based object detectors on constrained devices by
applying different image compression techniques to satellite data. We examine
the capabilities of the NVIDIA Jetson Nano and NVIDIA Jetson AGX Xavier;
low-power, high-performance computers, with integrated GPUs, small enough to
fit on-board a nanosatellite. We take a closer look at object detection
networks, including the Single Shot MultiBox Detector (SSD) and Region-based
Fully Convolutional Network (R-FCN) models that are pre-trained on DOTA - a
Large Scale Dataset for Object Detection in Aerial Images. The performance is
measured in terms of execution time, memory consumption, and accuracy, and are
compared against a baseline containing a server with two powerful GPUs. The
results show that by applying image compression techniques, we are able to
improve the execution time and memory consumption, achieving a fully runnable
dataset. A lossless compression technique achieves roughly a 10% reduction in
execution time and about a 3% reduction in memory consumption, with no impact
on the accuracy. While a lossy compression technique improves the execution
time by up to 144% and the memory consumption is reduced by as much as 97%.
However, it has a significant impact on accuracy, varying depending on the
compression ratio. Thus the application and ratio of these compression
techniques may differ depending on the required level of accuracy for a
particular task.

    

### [[2107.03786] Quadruplet Deep Metric Learning Model for Imbalanced Time-series Fault Diagnosis](http://arxiv.org/abs/2107.03786)


  Intelligent diagnosis method based on data-driven and deep learning is an
attractive and meaningful field in recent years. However, in practical
application scenarios, the imbalance of time-series fault is an urgent problem
to be solved. From the perspective of Bayesian probability, this paper analyzes
how to improve the performance of imbalanced classification by adjusting the
distance between classes and the distribution within a class and proposes a
time-series fault diagnosis model based on deep metric learning. As a core of
deep metric learning, a novel quadruplet data pair design considering imbalance
class is proposed with reference to traditional deep metric learning. Based on
such data pair, this paper proposes a quadruplet loss function which takes into
account the inter-class distance and the intra-class data distribution, and
pays special attention to imbalanced sample pairs. The reasonable combination
of quadruplet loss and softmax loss function can reduce the impact of
imbalance. Experiments on two open datasets are carried out to verify the
effectiveness and robustness of the model. Experimental results show that the
proposed method can effectively improve the performance of imbalanced
classification.

    

### [[2107.03806] Output Randomization: A Novel Defense for both White-box and Black-box Adversarial Models](http://arxiv.org/abs/2107.03806)


  Adversarial examples pose a threat to deep neural network models in a variety
of scenarios, from settings where the adversary has complete knowledge of the
model in a "white box" setting and to the opposite in a "black box" setting. In
this paper, we explore the use of output randomization as a defense against
attacks in both the black box and white box models and propose two defenses. In
the first defense, we propose output randomization at test time to thwart
finite difference attacks in black box settings. Since this type of attack
relies on repeated queries to the model to estimate gradients, we investigate
the use of randomization to thwart such adversaries from successfully creating
adversarial examples. We empirically show that this defense can limit the
success rate of a black box adversary using the Zeroth Order Optimization
attack to 0%. Secondly, we propose output randomization training as a defense
against white box adversaries. Unlike prior approaches that use randomization,
our defense does not require its use at test time, eliminating the Backward
Pass Differentiable Approximation attack, which was shown to be effective
against other randomization defenses. Additionally, this defense has low
overhead and is easily implemented, allowing it to be used together with other
defenses across various model architectures. We evaluate output randomization
training against the Projected Gradient Descent attacker and show that the
defense can reduce the PGD attack's success rate down to 12% when using
cross-entropy loss.

    

### [[2107.03815] Collaboration of Experts: Achieving 80% Top-1 Accuracy on ImageNet with 100M FLOPs](http://arxiv.org/abs/2107.03815)


  In this paper, we propose a Collaboration of Experts (CoE) framework to pool
together the expertise of multiple networks towards a common aim. Each expert
is an individual network with expertise on a unique portion of the dataset,
which enhances the collective capacity. Given a sample, an expert is selected
by the delegator, which simultaneously outputs a rough prediction to support
early termination. To fulfill this framework, we propose three modules to impel
each model to play its role, namely weight generation module (WGM), label
generation module (LGM) and variance calculation module (VCM). Our method
achieves the state-of-the-art performance on ImageNet, 80.7% top-1 accuracy
with 194M FLOPs. Combined with PWLU activation function and CondConv, CoE
further achieves the accuracy of 80.0% with only 100M FLOPs for the first time.
More importantly, our method is hardware friendly and achieves a 3-6x speedup
compared with some existing conditional computation approaches.

    

### [[2107.03825] Short-term Renewable Energy Forecasting in Greece using Prophet Decomposition and Tree-based Ensembles](http://arxiv.org/abs/2107.03825)


  Energy production using renewable sources exhibits inherent uncertainties due
to their intermittent nature. Nevertheless, the unified European energy market
promotes the increasing penetration of renewable energy sources (RES) by the
regional energy system operators. Consequently, RES forecasting can assist in
the integration of these volatile energy sources, since it leads to higher
reliability and reduced ancillary operational costs for power systems. This
paper presents a new dataset for solar and wind energy generation forecast in
Greece and introduces a feature engineering pipeline that enriches the
dimensional space of the dataset. In addition, we propose a novel method that
utilizes the innovative Prophet model, an end-to-end forecasting tool that
considers several kinds of nonlinear trends in decomposing the energy time
series before a tree-based ensemble provides short-term predictions. The
performance of the system is measured through representative evaluation
metrics, and by estimating the model's generalization under an industryprovided
scheme of absolute error thresholds. The proposed hybrid model competes with
baseline persistence models, tree-based regression ensembles, and the Prophet
model, managing to outperform them, presenting both lower error rates and more
favorable error distribution.

    

### [[2107.03836] Consistency of the Maximal Information Coefficient Estimator](http://arxiv.org/abs/2107.03836)


  The Maximal Information Coefficient (MIC) of Reshef et al. (Science, 2011) is
a statistic for measuring dependence between variable pairs in large datasets.
In this note, we prove that MIC is a consistent estimator of the corresponding
population statistic MIC$_*$. This corrects an error in an argument of Reshef
et al. (JMLR, 2016), which we describe.

    

### [[2107.03844] A Review of Bangla Natural Language Processing Tasks and the Utility of Transformer Models](http://arxiv.org/abs/2107.03844)


  Bangla -- ranked as the 6th most widely spoken language across the world
(this https URL), with 230 million native
speakers -- is still considered as a low-resource language in the natural
language processing (NLP) community. With three decades of research, Bangla NLP
(BNLP) is still lagging behind mainly due to the scarcity of resources and the
challenges that come with it. There is sparse work in different areas of BNLP;
however, a thorough survey reporting previous work and recent advances is yet
to be done. In this study, we first provide a review of Bangla NLP tasks,
resources, and tools available to the research community; we benchmark datasets
collected from various platforms for nine NLP tasks using current
state-of-the-art algorithms (i.e., transformer-based models). We provide
comparative results for the studied NLP tasks by comparing monolingual vs.
multilingual models of varying sizes. We report our results using both
individual and consolidated datasets and provide data splits for future
research. We reviewed a total of 108 papers and conducted 175 sets of
experiments. Our results show promising performance using transformer-based
models while highlighting the trade-off with computational costs. We hope that
such a comprehensive survey will motivate the community to build on and further
advance the research on Bangla NLP.

    

### [[2107.03846] Label-set Loss Functions for Partial Supervision: Application to Fetal Brain 3D MRI Parcellation](http://arxiv.org/abs/2107.03846)


  Deep neural networks have increased the accuracy of automatic segmentation,
however, their accuracy depends on the availability of a large number of fully
segmented images. Methods to train deep neural networks using images for which
some, but not all, regions of interest are segmented are necessary to make
better use of partially annotated datasets. In this paper, we propose the first
axiomatic definition of label-set loss functions that are the loss functions
that can handle partially segmented images. We prove that there is one and only
one method to convert a classical loss function for fully segmented images into
a proper label-set loss function. Our theory also allows us to define the
leaf-Dice loss, a label-set generalization of the Dice loss particularly suited
for partial supervision with only missing labels. Using the leaf-Dice loss, we
set a new state of the art in partially supervised learning for fetal brain 3D
MRI segmentation. We achieve a deep neural network able to segment white
matter, ventricles, cerebellum, extra-ventricular CSF, cortical gray matter,
deep gray matter, brainstem, and corpus callosum based on fetal brain 3D MRI of
anatomically normal fetuses or with open spina bifida. Our implementation of
the proposed label-set loss functions is available at
this https URL


### [[2107.03851] Imitation by Predicting Observations](http://arxiv.org/abs/2107.03851)


  Imitation learning enables agents to reuse and adapt the hard-won expertise
of others, offering a solution to several key challenges in learning behavior.
Although it is easy to observe behavior in the real-world, the underlying
actions may not be accessible. We present a new method for imitation solely
from observations that achieves comparable performance to experts on
challenging continuous control tasks while also exhibiting robustness in the
presence of observations unrelated to the task. Our method, which we call FORM
(for "Future Observation Reward Model") is derived from an inverse RL objective
and imitates using a model of expert behavior learned by generative modelling
of the expert's observations, without needing ground truth actions. We show
that FORM performs comparably to a strong baseline IRL method (GAIL) on the
DeepMind Control Suite benchmark, while outperforming GAIL in the presence of
task-irrelevant features.

    

### [[2107.03852] Augmented Data as an Auxiliary Plug-in Towards Categorization of Crowdsourced Heritage Data](http://arxiv.org/abs/2107.03852)


  In this paper, we propose a strategy to mitigate the problem of inefficient
clustering performance by introducing data augmentation as an auxiliary
plug-in. Classical clustering techniques such as K-means, Gaussian mixture
model and spectral clustering are central to many data-driven applications.
However, recently unsupervised simultaneous feature learning and clustering
using neural networks also known as Deep Embedded Clustering (DEC) has gained
prominence. Pioneering works on deep feature clustering focus on defining
relevant clustering loss function and choosing the right neural network for
extracting features. A central problem in all these cases is data sparsity
accompanied by high intra-class and low inter-class variance, which
subsequently leads to poor clustering performance and erroneous candidate
assignments. Towards this, we employ data augmentation techniques to improve
the density of the clusters, thus improving the overall performance. We train a
variant of Convolutional Autoencoder (CAE) with augmented data to construct the
initial feature space as a novel model for deep clustering. We demonstrate the
results of proposed strategy on crowdsourced Indian Heritage dataset. Extensive
experiments show consistent improvements over existing works.

    

### [[2107.03860] SSSE: Efficiently Erasing Samples from Trained Machine Learning Models](http://arxiv.org/abs/2107.03860)


  The availability of large amounts of user-provided data has been key to the
success of machine learning for many real-world tasks. Recently, an increasing
awareness has emerged that users should be given more control about how their
data is used. In particular, users should have the right to prohibit the use of
their data for training machine learning systems, and to have it erased from
already trained systems. While several sample erasure methods have been
proposed, all of them have drawbacks which have prevented them from gaining
widespread adoption. Most methods are either only applicable to very specific
families of models, sacrifice too much of the original model's accuracy, or
they have prohibitive memory or computational requirements. In this paper, we
propose an efficient and effective algorithm, SSSE, for samples erasure, that
is applicable to a wide class of machine learning models. From a second-order
analysis of the model's loss landscape we derive a closed-form update step of
the model parameters that only requires access to the data to be erased, not to
the original training set. Experiments on three datasets, CelebFaces attributes
(CelebA), Animals with Attributes 2 (AwA2) and CIFAR10, show that in certain
cases SSSE can erase samples almost as well as the optimal, yet impractical,
gold standard of training a new model from scratch with only the permitted
data.

    

### [[2107.03863] Benchpress: a scalable and platform-independent workflow for benchmarking structure learning algorithms for graphical models](http://arxiv.org/abs/2107.03863)


  Describing the relationship between the variables in a study domain and
modelling the data generating mechanism is a fundamental problem in many
empirical sciences. Probabilistic graphical models are one common approach to
tackle the problem. Learning the graphical structure is computationally
challenging and a fervent area of current research with a plethora of
algorithms being developed. To facilitate the benchmarking of different
methods, we present a novel automated workflow, called benchpress for producing
scalable, reproducible, and platform-independent benchmarks of structure
learning algorithms for probabilistic graphical models. Benchpress is
interfaced via a simple JSON-file, which makes it accessible for all users,
while the code is designed in a fully modular fashion to enable researchers to
contribute additional methodologies. Benchpress currently provides an interface
to a large number of state-of-the-art algorithms from libraries such as BiDAG,
bnlearn, GOBNILP, pcalg, r.blip, scikit-learn, TETRAD, and trilearn as well as
a variety of methods for data generating models and performance evaluation.
Alongside user-defined models and randomly generated datasets, the software
tool also includes a number of standard datasets and graphical models from the
literature, which may be included in a benchmarking workflow. We demonstrate
the applicability of this workflow for learning Bayesian networks in four
typical data scenarios. The source code and documentation is publicly available
from this http URL.

    

### [[2107.03869] Explainable AI (XAI) for PHM of Industrial Asset: A State-of-The-Art, PRISMA-Compliant Systematic Review](http://arxiv.org/abs/2107.03869)


  A state-of-the-art systematic review on XAI applied to Prognostic and Health
Management (PHM) of industrial asset is presented. The work attempts to provide
an overview of the general trend of XAI in PHM, answers the question of
accuracy versus explainability, investigates the extent of human role,
explainability evaluation and uncertainty management in PHM XAI. Research
articles linked to PHM XAI, in English language, from 2015 to 2021 are selected
from IEEE Xplore, ScienceDirect, SpringerLink, ACM Digital Library and Scopus
databases using PRISMA guidelines. Data was extracted from 35 selected articles
and examined using MS. Excel. Several findings were synthesized. Firstly, while
the discipline is still young, the analysis indicates the growing acceptance of
XAI in PHM domain. Secondly, XAI functions as a double edge sword, where it is
assimilated as a tool to execute PHM tasks as well as a mean of explanation, in
particular in diagnostic and anomaly detection. There is thus a need for XAI in
PHM. Thirdly, the review shows that PHM XAI papers produce either good or
excellent results in general, suggesting that PHM performance is unaffected by
XAI. Fourthly, human role, explainability metrics and uncertainty management
are areas requiring further attention by the PHM community. Adequate
explainability metrics to cater for PHM need are urgently needed. Finally, most
case study featured on the accepted articles are based on real, indicating that
available AI and XAI approaches are equipped to solve complex real-world
challenges, increasing the confidence of AI model adoption in the industry.
This work is funded by the Universiti Teknologi Petronas Foundation.

    

### [[2107.03876] Bootstrapping Generalization of Process Models Discovered From Event Data](http://arxiv.org/abs/2107.03876)


  Process mining studies ways to derive value from process executions recorded
in event logs of IT-systems, with process discovery the task of inferring a
process model for an event log emitted by some unknown system. One quality
criterion for discovered process models is generalization. Generalization seeks
to quantify how well the discovered model describes future executions of the
system, and is perhaps the least understood quality criterion in process
mining. The lack of understanding is primarily a consequence of generalization
seeking to measure properties over the entire future behavior of the system,
when the only available sample of behavior is that provided by the event log
itself. In this paper, we draw inspiration from computational statistics, and
employ a bootstrap approach to estimate properties of a population based on a
sample. Specifically, we define an estimator of the model's generalization
based on the event log it was discovered from, and then use bootstrapping to
measure the generalization of the model with respect to the system, and its
statistical significance. Experiments demonstrate the feasibility of the
approach in industrial settings.

    

### [[2107.03900] The Price of Diversity](http://arxiv.org/abs/2107.03900)


  Systemic bias with respect to gender, race and ethnicity, often unconscious,
is prevalent in datasets involving choices among individuals. Consequently,
society has found it challenging to alleviate bias and achieve diversity in a
way that maintains meritocracy in such settings. We propose (a) a novel
optimization approach based on optimally flipping outcome labels and training
classification models simultaneously to discover changes to be made in the
selection process so as to achieve diversity without significantly affecting
meritocracy, and (b) a novel implementation tool employing optimal
classification trees to provide insights on which attributes of individuals
lead to flipping of their labels, and to help make changes in the current
selection processes in a manner understandable by human decision makers. We
present case studies on three real-world datasets consisting of parole,
admissions to the bar and lending decisions, and demonstrate that the price of
diversity is low and sometimes negative, that is we can modify our selection
processes in a way that enhances diversity without affecting meritocracy
significantly, and sometimes improving it.

    

### [[2107.03901] Federated Learning for Multi-Center Imaging Diagnostics: A Study in Cardiovascular Disease](http://arxiv.org/abs/2107.03901)


  Deep learning models can enable accurate and efficient disease diagnosis, but
have thus far been hampered by the data scarcity present in the medical world.
Automated diagnosis studies have been constrained by underpowered single-center
datasets, and although some results have shown promise, their generalizability
to other institutions remains questionable as the data heterogeneity between
institutions is not taken into account. By allowing models to be trained in a
distributed manner that preserves patients' privacy, federated learning
promises to alleviate these issues, by enabling diligent multi-center studies.
We present the first federated learning study on the modality of cardiovascular
magnetic resonance (CMR) and use four centers derived from subsets of the M\&M
and ACDC datasets, focusing on the diagnosis of hypertrophic cardiomyopathy
(HCM). We adapt a 3D-CNN network pretrained on action recognition and explore
two different ways of incorporating shape prior information to the model, and
four different data augmentation set-ups, systematically analyzing their impact
on the different collaborative learning choices. We show that despite the small
size of data (180 subjects derived from four centers), the privacy preserving
federated learning achieves promising results that are competitive with
traditional centralized learning. We further find that federatively trained
models exhibit increased robustness and are more sensitive to domain shift
effects.

    

### [[2107.03903] Manifold Hypothesis in Data Analysis: Double Geometrically-Probabilistic Approach to Manifold Dimension Estimation](http://arxiv.org/abs/2107.03903)


  Manifold hypothesis states that data points in high-dimensional space
actually lie in close vicinity of a manifold of much lower dimension. In many
cases this hypothesis was empirically verified and used to enhance unsupervised
and semi-supervised learning. Here we present new approach to manifold
hypothesis checking and underlying manifold dimension estimation. In order to
do it we use two very different methods simultaneously - one geometric, another
probabilistic - and check whether they give the same result. Our geometrical
method is a modification for sparse data of a well-known box-counting algorithm
for Minkowski dimension calculation. The probabilistic method is new. Although
it exploits standard nearest neighborhood distance, it is different from
methods which were previously used in such situations. This method is robust,
fast and includes special preliminary data transformation. Experiments on real
datasets show that the suggested approach based on two methods combination is
powerful and effective.

    

### [[2107.03913] Patient Embeddings in Healthcare and Insurance Applications](http://arxiv.org/abs/2107.03913)


  The paper researches the problem of concept and patient representations in
the medical domain. We present the patient histories from Electronic Health
Records (EHRs) as temporal sequences of ICD concepts for which embeddings are
learned in an unsupervised setup with a transformer-based neural network model.
The model training was performed on the collection of one million patients'
histories in 6 years. The predictive power of such a model is assessed in
comparison with several baseline methods. A series of experiments on the
MIMIC-III data show the advantage of the presented model compared to a similar
system. Further, we analyze the obtained embedding space with regards to
concept relations and show how knowledge from the medical domain can be
successfully transferred to the practical task of insurance scoring in the form
of patient embeddings.

    

### [[2107.03919] Understanding the Limits of Unsupervised Domain Adaptation via Data Poisoning](http://arxiv.org/abs/2107.03919)


  Unsupervised domain adaptation (UDA) enables cross-domain learning without
target domain labels by transferring knowledge from a labeled source domain
whose distribution differs from the target. However, UDA is not always
successful and several accounts of "negative transfer" have been reported in
the literature. In this work, we prove a simple lower bound on the target
domain error that complements the existing upper bound. Our bound shows the
insufficiency of minimizing source domain error and marginal distribution
mismatch for a guaranteed reduction in the target domain error, due to the
possible increase of induced labeling function mismatch. This insufficiency is
further illustrated through simple distributions for which the same UDA
approach succeeds, fails, and may succeed or fail with an equal chance.
Motivated from this, we propose novel data poisoning attacks to fool UDA
methods into learning representations that produce large target domain errors.
We evaluate the effect of these attacks on popular UDA methods using benchmark
datasets where they have been previously shown to be successful. Our results
show that poisoning can significantly decrease the target domain accuracy,
dropping it to almost 0\% in some cases, with the addition of only 10\%
poisoned data in the source domain. The failure of UDA methods demonstrates the
limitations of UDA at guaranteeing cross-domain generalization consistent with
the lower bound. Thus, evaluation of UDA methods in adversarial settings such
as data poisoning can provide a better sense of their robustness in scenarios
unfavorable for UDA.

    

### [[2107.03920] Likelihood-Free Frequentist Inference: Bridging Classical Statistics and Machine Learning in Simulation and Uncertainty Quantification](http://arxiv.org/abs/2107.03920)


  Many areas of science make extensive use of computer simulators that
implicitly encode likelihood functions for complex systems. Classical
statistical methods are poorly suited for these so-called likelihood-free
inference (LFI) settings, outside the asymptotic and low-dimensional regimes.
Although new machine learning methods, such as normalizing flows, have
revolutionized the sample efficiency and capacity of LFI methods, it remains an
open question whether they produce reliable measures of uncertainty. In this
paper, we present a statistical framework for LFI that unifies classical
statistics with modern machine learning to: (1) construct frequentist
confidence sets and hypothesis tests with finite-sample guarantees of nominal
coverage (type I error control) and power, and (2) provide rigorous diagnostics
for assessing empirical coverage over the entire parameter space. We refer to
our framework as likelihood-free frequentist inference (LF2I). Any method that
estimates a test statistic, such as the likelihood ratio, can be plugged into
our framework to create powerful tests and confidence sets with correct
coverage. In this work, we specifically study two test statistics (ACORE and
BFF), which, respectively, maximize versus integrate an odds function over the
parameter space. Our theoretical and empirical results offer multifaceted
perspectives on error sources and challenges in likelihood-free frequentist
inference.

    

### [[2107.03926] Measuring Financial Time Series Similarity With a View to Identifying Profitable Stock Market Opportunities](http://arxiv.org/abs/2107.03926)


  Forecasting stock returns is a challenging problem due to the highly
stochastic nature of the market and the vast array of factors and events that
can influence trading volume and prices. Nevertheless it has proven to be an
attractive target for machine learning research because of the potential for
even modest levels of prediction accuracy to deliver significant benefits. In
this paper, we describe a case-based reasoning approach to predicting stock
market returns using only historical pricing data. We argue that one of the
impediments for case-based stock prediction has been the lack of a suitable
similarity metric when it comes to identifying similar pricing histories as the
basis for a future prediction -- traditional Euclidean and correlation based
approaches are not effective for a variety of reasons -- and in this regard, a
key contribution of this work is the development of a novel similarity metric
for comparing historical pricing data. We demonstrate the benefits of this
metric and the case-based approach in a real-world application in comparison to
a variety of conventional benchmarks.

    

### [[2107.03940] Locally differentially private estimation of nonlinear functionals of discrete distributions](http://arxiv.org/abs/2107.03940)


  We study the problem of estimating non-linear functionals of discrete
distributions in the context of local differential privacy. The initial data
$x_1,\ldots,x_n \in [K]$ are supposed i.i.d. and distributed according to an
unknown discrete distribution $p = (p_1,\ldots,p_K)$. Only $\alpha$-locally
differentially private (LDP) samples $z_1,...,z_n$ are publicly available,
where the term 'local' means that each $z_i$ is produced using one individual
attribute $x_i$. We exhibit privacy mechanisms (PM) that are interactive (i.e.
they are allowed to use already published confidential data) or
non-interactive. We describe the behavior of the quadratic risk for estimating
the power sum functional $F_{\gamma} = \sum_{k=1}^K p_k^{\gamma}$, $\gamma >0$
as a function of $K, \, n$ and $\alpha$. In the non-interactive case, we study
two plug-in type estimators of $F_{\gamma}$, for all $\gamma >0$, that are
similar to the MLE analyzed by Jiao et al. (2017) in the multinomial model.
However, due to the privacy constraint the rates we attain are slower and
similar to those obtained in the Gaussian model by Collier et al. (2020). In
the interactive case, we introduce for all $\gamma >1$ a two-step procedure
which attains the faster parametric rate $(n \alpha^2)^{-1/2}$ when $\gamma
\geq 2$. We give lower bounds results over all $\alpha$-LDP mechanisms and all
estimators using the private samples.

    

### [[2107.03955] On Margins and Derandomisation in PAC-Bayes](http://arxiv.org/abs/2107.03955)


  We develop a framework for derandomising PAC-Bayesian generalisation bounds
achieving a margin on training data, relating this process to the
concentration-of-measure phenomenon. We apply these tools to linear prediction,
single-hidden-layer neural networks with an unusual erf activation function,
and deep ReLU networks, obtaining new bounds. The approach is also extended to
the idea of "partial-derandomisation" where only some layers are derandomised
and the others are stochastic. This allows empirical evaluation of
single-hidden-layer networks on more complex datasets, and helps bridge the gap
between generalisation bounds for non-stochastic deep networks and those for
randomised deep networks as generally examined in PAC-Bayes.

    

### [[2107.03964] CamTuner: Reinforcement-Learning based System for Camera Parameter Tuning to enhance Analytics](http://arxiv.org/abs/2107.03964)


  Complex sensors like video cameras include tens of configurable parameters,
which can be set by end-users to customize the sensors to specific application
scenarios. Although parameter settings significantly affect the quality of the
sensor output and the accuracy of insights derived from sensor data, most
end-users use a fixed parameter setting because they lack the skill or
understanding to appropriately configure these parameters. We propose CamTuner,
which is a system to automatically, and dynamically adapt the complex sensor to
changing environments. CamTuner includes two key components. First, a bespoke
analytics quality estimator, which is a deep-learning model to automatically
and continuously estimate the quality of insights from an analytics unit as the
environment around a sensor change. Second, a reinforcement learning (RL)
module, which reacts to the changes in quality, and automatically adjusts the
camera parameters to enhance the accuracy of insights. We improve the training
time of the RL module by an order of magnitude by designing virtual models to
mimic essential behavior of the camera: we design virtual knobs that can be set
to different values to mimic the effects of assigning different values to the
camera's configurable parameters, and we design a virtual camera model that
mimics the output from a video camera at different times of the day. These
virtual models significantly accelerate training because (a) frame rates from a
real camera are limited to 25-30 fps while the virtual models enable processing
at 300 fps, (b) we do not have to wait until the real camera sees different
environments, which could take weeks or months, and (c) virtual knobs can be
updated instantly, while it can take 200-500 ms to change the camera parameter
settings. Our dynamic tuning approach results in up to 12% improvement in the
accuracy of insights from several video analytics tasks.

    

### [[2107.03974] Offline Meta-Reinforcement Learning with Online Self-Supervision](http://arxiv.org/abs/2107.03974)


  Meta-reinforcement learning (RL) can be used to train policies that quickly
adapt to new tasks with orders of magnitude less data than standard RL, but
this fast adaptation often comes at the cost of greatly increasing the amount
of reward supervision during meta-training time. Offline meta-RL removes the
need to continuously provide reward supervision because rewards must only be
provided once when the offline dataset is generated. In addition to the
challenges of offline RL, a unique distribution shift is present in meta RL:
agents learn exploration strategies that can gather the experience needed to
learn a new task, and also learn adaptation strategies that work well when
presented with the trajectories in the dataset, but the adaptation strategies
are not adapted to the data distribution that the learned exploration
strategies collect. Unlike the online setting, the adaptation and exploration
strategies cannot effectively adapt to each other, resulting in poor
performance. In this paper, we propose a hybrid offline meta-RL algorithm,
which uses offline data with rewards to meta-train an adaptive policy, and then
collects additional unsupervised online data, without any ground truth reward
labels, to bridge this distribution shift problem. Our method uses the offline
data to learn the distribution of reward functions, which is then sampled to
self-supervise reward labels for the additional online data. By removing the
need to provide reward labels for the online experience, our approach can be
more practical to use in settings where reward supervision would otherwise be
provided manually. We compare our method to prior work on offline meta-RL on
simulated robot locomotion and manipulation tasks and find that using
additional data and self-generated rewards significantly improves an agent's
ability to generalize.

    

### [[2107.03985] Comparing Supervised Models And Learned Speech Representations For Classifying Intelligibility Of Disordered Speech On Selected Phrases](http://arxiv.org/abs/2107.03985)


  Automatic classification of disordered speech can provide an objective tool
for identifying the presence and severity of speech impairment. Classification
approaches can also help identify hard-to-recognize speech samples to teach ASR
systems about the variable manifestations of impaired speech. Here, we develop
and compare different deep learning techniques to classify the intelligibility
of disordered speech on selected phrases. We collected samples from a diverse
set of 661 speakers with a variety of self-reported disorders speaking 29 words
or phrases, which were rated by speech-language pathologists for their overall
intelligibility using a five-point Likert scale. We then evaluated classifiers
developed using 3 approaches: (1) a convolutional neural network (CNN) trained
for the task, (2) classifiers trained on non-semantic speech representations
from CNNs that used an unsupervised objective [1], and (3) classifiers trained
on the acoustic (encoder) embeddings from an ASR system trained on typical
speech [2]. We found that the ASR encoder's embeddings considerably outperform
the other two on detecting and classifying disordered speech. Further analysis
shows that the ASR embeddings cluster speech by the spoken phrase, while the
non-semantic embeddings cluster speech by speaker. Also, longer phrases are
more indicative of intelligibility deficits than single words.

    

### [[2107.03992] A Long Short-Term Memory for AI Applications in Spike-based Neuromorphic Hardware](http://arxiv.org/abs/2107.03992)


  In spite of intensive efforts it has remained an open problem to what extent
current Artificial Intelligence (AI) methods that employ Deep Neural Networks
(DNNs) can be implemented more energy-efficiently on spike-based neuromorphic
hardware. This holds in particular for AI methods that solve sequence
processing tasks, a primary application target for spike-based neuromorphic
hardware. One difficulty is that DNNs for such tasks typically employ Long
Short-Term Memory (LSTM) units. Yet an efficient emulation of these units in
spike-based hardware has been missing. We present a biologically inspired
solution that solves this problem. This solution enables us to implement a
major class of DNNs for sequence processing tasks such as time series
classification and question answering with substantial energy savings on
neuromorphic hardware. In fact, the Relational Network for reasoning about
relations between objects that we use for question answering is the first
example of a large DNN that carries out a sequence processing task with
substantial energy-saving on neuromorphic hardware.

    

### [[2107.03996] Learning Vision-Guided Quadrupedal Locomotion End-to-End with Cross-Modal Transformers](http://arxiv.org/abs/2107.03996)


  We propose to address quadrupedal locomotion tasks using Reinforcement
Learning (RL) with a Transformer-based model that learns to combine
proprioceptive information and high-dimensional depth sensor inputs. While
learning-based locomotion has made great advances using RL, most methods still
rely on domain randomization for training blind agents that generalize to
challenging terrains. Our key insight is that proprioceptive states only offer
contact measurements for immediate reaction, whereas an agent equipped with
visual sensory observations can learn to proactively maneuver environments with
obstacles and uneven terrain by anticipating changes in the environment many
steps ahead. In this paper, we introduce LocoTransformer, an end-to-end RL
method for quadrupedal locomotion that leverages a Transformer-based model for
fusing proprioceptive states and visual observations. We evaluate our method in
challenging simulated environments with different obstacles and uneven terrain.
We show that our method obtains significant improvements over policies with
only proprioceptive state inputs, and that Transformer-based models further
improve generalization across environments. Our project page with videos is at
this https URL .

    

### [[2107.04000] Active Safety Envelopes using Light Curtains with Probabilistic Guarantees](http://arxiv.org/abs/2107.04000)


  To safely navigate unknown environments, robots must accurately perceive
dynamic obstacles. Instead of directly measuring the scene depth with a LiDAR
sensor, we explore the use of a much cheaper and higher resolution sensor:
programmable light curtains. Light curtains are controllable depth sensors that
sense only along a surface that a user selects. We use light curtains to
estimate the safety envelope of a scene: a hypothetical surface that separates
the robot from all obstacles. We show that generating light curtains that sense
random locations (from a particular distribution) can quickly discover the
safety envelope for scenes with unknown objects. Importantly, we produce
theoretical safety guarantees on the probability of detecting an obstacle using
random curtains. We combine random curtains with a machine learning based model
that forecasts and tracks the motion of the safety envelope efficiently. Our
method accurately estimates safety envelopes while providing probabilistic
safety guarantees that can be used to certify the efficacy of a robot
perception system to detect and avoid dynamic obstacles. We evaluate our
approach in a simulated urban driving environment and a real-world environment
with moving pedestrians using a light curtain device and show that we can
estimate safety envelopes efficiently and effectively. Project website:
this https URL


### [[2107.04004] 3D Neural Scene Representations for Visuomotor Control](http://arxiv.org/abs/2107.04004)


  Humans have a strong intuitive understanding of the 3D environment around us.
The mental model of the physics in our brain applies to objects of different
materials and enables us to perform a wide range of manipulation tasks that are
far beyond the reach of current robots. In this work, we desire to learn models
for dynamic 3D scenes purely from 2D visual observations. Our model combines
Neural Radiance Fields (NeRF) and time contrastive learning with an
autoencoding framework, which learns viewpoint-invariant 3D-aware scene
representations. We show that a dynamics model, constructed over the learned
representation space, enables visuomotor control for challenging manipulation
tasks involving both rigid bodies and fluids, where the target is specified in
a viewpoint different from what the robot operates on. When coupled with an
auto-decoding framework, it can even support goal specification from camera
viewpoints that are outside the training distribution. We further demonstrate
the richness of the learned 3D dynamics model by performing future prediction
and novel view synthesis. Finally, we provide detailed ablation studies
regarding different system designs and qualitative analysis of the learned
representations.

    

### [[2107.04008] Malware Classification Using Deep Boosted Learning](http://arxiv.org/abs/2107.04008)


  Malicious activities in cyberspace have gone further than simply hacking
machines and spreading viruses. It has become a challenge for a nations
survival and hence has evolved to cyber warfare. Malware is a key component of
cyber-crime, and its analysis is the first line of defence against attack. This
work proposes a novel deep boosted hybrid learning-based malware classification
framework and named as Deep boosted Feature Space-based Malware classification
(DFS-MC). In the proposed framework, the discrimination power is enhanced by
fusing the feature spaces of the best performing customized CNN architectures
models and its discrimination by an SVM for classification. The discrimination
capacity of the proposed classification framework is assessed by comparing it
against the standard customized CNNs. The customized CNN models are implemented
in two ways: softmax classifier and deep hybrid learning-based malware
classification. In the hybrid learning, Deep features are extracted from
customized CNN architectures and fed into the conventional machine learning
classifier to improve the classification performance. We also introduced the
concept of transfer learning in a customized CNN architecture based malware
classification framework through fine-tuning. The performance of the proposed
malware classification approaches are validated on the MalImg malware dataset
using the hold-out cross-validation technique. Experimental comparisons were
conducted by employing innovative, customized CNN, trained from scratch and
fine-tuning the customized CNN using transfer learning. The proposed
classification framework DFS-MC showed improved results, Accuracy: 98.61%,
F-score: 0.96, Precision: 0.96, and Recall: 0.96.

    

### [[2107.04009] Knowledge Transfer by Discriminative Pre-training for Academic Performance Prediction](http://arxiv.org/abs/2107.04009)


  The needs for precisely estimating a student's academic performance have been
emphasized with an increasing amount of attention paid to Intelligent Tutoring
System (ITS). However, since labels for academic performance, such as test
scores, are collected from outside of ITS, obtaining the labels is costly,
leading to label-scarcity problem which brings challenge in taking machine
learning approaches for academic performance prediction. To this end, inspired
by the recent advancement of pre-training method in natural language processing
community, we propose DPA, a transfer learning framework with Discriminative
Pre-training tasks for Academic performance prediction. DPA pre-trains two
models, a generator and a discriminator, and fine-tunes the discriminator on
academic performance prediction. In DPA's pre-training phase, a sequence of
interactions where some tokens are masked is provided to the generator which is
trained to reconstruct the original sequence. Then, the discriminator takes an
interaction sequence where the masked tokens are replaced by the generator's
outputs, and is trained to predict the originalities of all tokens in the
sequence. Compared to the previous state-of-the-art generative pre-training
method, DPA is more sample efficient, leading to fast convergence to lower
academic performance prediction error. We conduct extensive experimental
studies on a real-world dataset obtained from a multi-platform ITS application
and show that DPA outperforms the previous state-of-the-art generative
pre-training method with a reduction of 4.05% in mean absolute error and more
robust to increased label-scarcity.

    

### [[2107.04010] A Machine Learning Approach to Safer Airplane Landings: Predicting Runway Conditions using Weather and Flight Data](http://arxiv.org/abs/2107.04010)


  The presence of snow and ice on runway surfaces reduces the available
tire-pavement friction needed for retardation and directional control and
causes potential economic and safety threats for the aviation industry during
the winter seasons. To activate appropriate safety procedures, pilots need
accurate and timely information on the actual runway surface conditions. In
this study, XGBoost is used to create a combined runway assessment system,
which includes a classifcation model to predict slippery conditions and a
regression model to predict the level of slipperiness. The models are trained
on weather data and data from runway reports. The runway surface conditions are
represented by the tire-pavement friction coefficient, which is estimated from
flight sensor data from landing aircrafts. To evaluate the performance of the
models, they are compared to several state-of-the-art runway assessment
methods. The XGBoost models identify slippery runway conditions with a ROC AUC
of 0.95, predict the friction coefficient with a MAE of 0.0254, and outperforms
all the previous methods. The results show the strong abilities of machine
learning methods to model complex, physical phenomena with a good accuracy when
domain knowledge is used in the variable extraction. The XGBoost models are
combined with SHAP (SHapley Additive exPlanations) approximations to provide a
comprehensible decision support system for airport operators and pilots, which
can contribute to safer and more economic operations of airport runways.

    

### [[2107.04013] Multi-Modality Task Cascade for 3D Object Detection](http://arxiv.org/abs/2107.04013)


  Point clouds and RGB images are naturally complementary modalities for 3D
visual understanding - the former provides sparse but accurate locations of
points on objects, while the latter contains dense color and texture
information. Despite this potential for close sensor fusion, many methods train
two models in isolation and use simple feature concatenation to represent 3D
sensor data. This separated training scheme results in potentially sub-optimal
performance and prevents 3D tasks from being used to benefit 2D tasks that are
often useful on their own. To provide a more integrated approach, we propose a
novel Multi-Modality Task Cascade network (MTC-RCNN) that leverages 3D box
proposals to improve 2D segmentation predictions, which are then used to
further refine the 3D boxes. We show that including a 2D network between two
stages of 3D modules significantly improves both 2D and 3D task performance.
Moreover, to prevent the 3D module from over-relying on the overfitted 2D
predictions, we propose a dual-head 2D segmentation training and inference
scheme, allowing the 2nd 3D module to learn to interpret imperfect 2D
segmentation predictions. Evaluating our model on the challenging SUN RGB-D
dataset, we improve upon state-of-the-art results of both single modality and
fusion networks by a large margin ($\textbf{+3.8}$ mAP@0.5). Code will be
released $\href{this https URL}{\text{here.}}$

    

### [[2107.04034] RMA: Rapid Motor Adaptation for Legged Robots](http://arxiv.org/abs/2107.04034)


  Successful real-world deployment of legged robots would require them to adapt
in real-time to unseen scenarios like changing terrains, changing payloads,
wear and tear. This paper presents Rapid Motor Adaptation (RMA) algorithm to
solve this problem of real-time online adaptation in quadruped robots. RMA
consists of two components: a base policy and an adaptation module. The
combination of these components enables the robot to adapt to novel situations
in fractions of a second. RMA is trained completely in simulation without using
any domain knowledge like reference trajectories or predefined foot trajectory
generators and is deployed on the A1 robot without any fine-tuning. We train
RMA on a varied terrain generator using bioenergetics-inspired rewards and
deploy it on a variety of difficult terrains including rocky, slippery,
deformable surfaces in environments with grass, long vegetation, concrete,
pebbles, stairs, sand, etc. RMA shows state-of-the-art performance across
diverse real-world as well as simulation experiments. Video results at
this https URL


### [[1705.03439] Frequentist Consistency of Variational Bayes](http://arxiv.org/abs/1705.03439)


  A key challenge for modern Bayesian statistics is how to perform scalable
inference of posterior distributions. To address this challenge, variational
Bayes (VB) methods have emerged as a popular alternative to the classical
Markov chain Monte Carlo (MCMC) methods. VB methods tend to be faster while
achieving comparable predictive performance. However, there are few theoretical
results around VB. In this paper, we establish frequentist consistency and
asymptotic normality of VB methods. Specifically, we connect VB methods to
point estimates based on variational approximations, called frequentist
variational approximations, and we use the connection to prove a variational
Bernstein-von Mises theorem. The theorem leverages the theoretical
characterizations of frequentist variational approximations to understand
asymptotic properties of VB. In summary, we prove that (1) the VB posterior
converges to the Kullback-Leibler (KL) minimizer of a normal distribution,
centered at the truth and (2) the corresponding variational expectation of the
parameter is consistent and asymptotically normal. As applications of the
theorem, we derive asymptotic properties of VB posteriors in Bayesian mixture
models, Bayesian generalized linear mixed models, and Bayesian stochastic block
models. We conduct a simulation study to illustrate these theoretical results.

    

### [[1710.09064] End-to-End Optimized Speech Coding with Deep Neural Networks](http://arxiv.org/abs/1710.09064)


  Modern compression algorithms are often the result of laborious
domain-specific research; industry standards such as MP3, JPEG, and AMR-WB took
years to develop and were largely hand-designed. We present a deep neural
network model which optimizes all the steps of a wideband speech coding
pipeline (compression, quantization, entropy coding, and decompression)
end-to-end directly from raw speech data -- no manual feature engineering
necessary, and it trains in hours. In testing, our DNN-based coder performs on
par with the AMR-WB standard at a variety of bitrates (~9kbps up to ~24kbps).
It also runs in realtime on a 3.8GhZ Intel CPU.

    

### [[1904.05981] Community detection in the sparse hypergraph stochastic block model](http://arxiv.org/abs/1904.05981)


  We consider the community detection problem in sparse random hypergraphs.
Angelini et al. (2015) conjectured the existence of a sharp threshold on model
parameters for community detection in sparse hypergraphs generated by a
hypergraph stochastic block model. We solve the positive part of the conjecture
for the case of two blocks: above the threshold, there is a spectral algorithm
which asymptotically almost surely constructs a partition of the hypergraph
correlated with the true partition. Our method is a generalization to random
hypergraphs of the method developed by Massouli (2014) for sparse random
graphs.

    

### [[1905.10629] Flexibly Regularized Mixture Models and Application to Image Segmentation](http://arxiv.org/abs/1905.10629)


  Probabilistic finite mixture models are widely used for unsupervised
clustering. These models can often be improved by adapting them to the topology
of the data. For instance, in order to classify spatially adjacent data points
similarly, it is common to introduce a Laplacian constraint on the posterior
probability that each data point belongs to a class. Alternatively, the mixing
probabilities can be treated as free parameters, while assuming Gauss-Markov or
more complex priors to regularize those mixing probabilities. However, these
approaches are constrained by the shape of the prior and often lead to
complicated or intractable inference. Here, we propose a new parametrization of
the Dirichlet distribution to flexibly regularize the mixing probabilities of
over-parametrized mixture distributions. Using the Expectation-Maximization
algorithm, we show that our approach allows us to define any linear update rule
for the mixing probabilities, including spatial smoothing regularization as a
special case. We then show that this flexible design can be extended to share
class information between multiple mixture models. We apply our algorithm to
artificial and natural image segmentation tasks, and we provide quantitative
and qualitative comparison of the performance of Gaussian and Student-t
mixtures on the Berkeley Segmentation Dataset. We also demonstrate how to
propagate class information across the layers of deep convolutional neural
networks in a probabilistically optimal way, suggesting a new interpretation
for feedback signals in biological visual systems. Our flexible approach can be
easily generalized to adapt probabilistic mixture models to arbitrary data
topologies.

    

### [[1905.13298] DeepShift: Towards Multiplication-Less Neural Networks](http://arxiv.org/abs/1905.13298)


  The high computation, memory, and power budgets of inferring convolutional
neural networks (CNNs) are major bottlenecks of model deployment to edge
computing platforms, e.g., mobile devices and IoT. Moreover, training CNNs is
time and energy-intensive even on high-grade servers. Convolution layers and
fully connected layers, because of their intense use of multiplications, are
the dominant contributor to this computation budget.
We propose to alleviate this problem by introducing two new operations:
convolutional shifts and fully-connected shifts which replace multiplications
with bitwise shift and sign flipping during both training and inference. During
inference, both approaches require only 5 bits (or less) to represent the
weights. This family of neural network architectures (that use convolutional
shifts and fully connected shifts) is referred to as DeepShift models. We
propose two methods to train DeepShift models: DeepShift-Q which trains regular
weights constrained to powers of 2, and DeepShift-PS that trains the values of
the shifts and sign flips directly.
Very close accuracy, and in some cases higher accuracy, to baselines are
achieved. Converting pre-trained 32-bit floating-point baseline models of
ResNet18, ResNet50, VGG16, and GoogleNet to DeepShift and training them for 15
to 30 epochs, resulted in Top-1/Top-5 accuracies higher than that of the
original model.
Last but not least, we implemented the convolutional shifts and fully
connected shift GPU kernels and showed a reduction in latency time of 25% when
inferring ResNet18 compared to unoptimized multiplication-based GPU kernels.
The code can be found at this https URL.

    

### [[1906.09744] Improving the Effectiveness and Efficiency of Stochastic Neighbour Embedding with Isolation Kernel](http://arxiv.org/abs/1906.09744)


  This paper presents a new insight into improving the performance of
Stochastic Neighbour Embedding (t-SNE) by using Isolation kernel instead of
Gaussian kernel. Isolation kernel outperforms Gaussian kernel in two aspects.
First, the use of Isolation kernel in t-SNE overcomes the drawback of
misrepresenting some structures in the data, which often occurs when Gaussian
kernel is applied in t-SNE. This is because Gaussian kernel determines each
local bandwidth based on one local point only, while Isolation kernel is
derived directly from the data based on space partitioning. Second, the use of
Isolation kernel yields a more efficient similarity computation because
data-dependent Isolation kernel has only one parameter that needs to be tuned.
In contrast, the use of data-independent Gaussian kernel increases the
computational cost by determining n bandwidths for a dataset of n points. As
the root cause of these deficiencies in t-SNE is Gaussian kernel, we show that
simply replacing Gaussian kernel with Isolation kernel in t-SNE significantly
improves the quality of the final visualisation output (without creating
misrepresented structures) and removes one key obstacle that prevents t-SNE
from processing large datasets. Moreover, Isolation kernel enables t-SNE to
deal with large-scale datasets in less runtime without trading off accuracy,
unlike existing methods in speeding up t-SNE.

    

### [[1909.11201] Matrix Sketching for Secure Collaborative Machine Learning](http://arxiv.org/abs/1909.11201)


  Collaborative learning allows participants to jointly train a model without
data sharing. To update the model parameters, the central server broadcasts
model parameters to the clients, and the clients send updating directions such
as gradients to the server. While data do not leave a client device, the
communicated gradients and parameters will leak a client's privacy. Attacks
that infer clients' privacy from gradients and parameters have been developed
by prior work. Simple defenses such as dropout and differential privacy either
fail to defend the attacks or seriously hurt test accuracy.
We propose a practical defense which we call Double-Blind Collaborative
Learning (DBCL). The high-level idea is to apply random matrix sketching to the
parameters (aka weights) and re-generate random sketching after each iteration.
DBCL prevents clients from conducting gradient-based privacy inferences which
are the most effective attacks. DBCL works because from the attacker's
perspective, sketching is effectively random noise that outweighs the signal.
Notably, DBCL does not much increase computation and communication costs and
does not hurt test accuracy at all.

    

### [[2003.08964] The value of text for small business default prediction: A deep learning approach](http://arxiv.org/abs/2003.08964)


  Compared to consumer lending, Micro, Small and Medium Enterprise (mSME)
credit risk modelling is particularly challenging, as, often, the same sources
of information are not available. Therefore, it is standard policy for a loan
officer to provide a textual loan assessment to mitigate limited data
availability. In turn, this statement is analysed by a credit expert alongside
any available standard credit data. In our paper, we exploit recent advances
from the field of Deep Learning and Natural Language Processing (NLP),
including the BERT (Bidirectional Encoder Representations from Transformers)
model, to extract information from 60 000 textual assessments provided by a
lender. We consider the performance in terms of the AUC (Area Under the
receiver operating characteristic Curve) and Brier Score metrics and find that
the text alone is surprisingly effective for predicting default. However, when
combined with traditional data, it yields no additional predictive capability,
with performance dependent on the text's length. Our proposed deep learning
model does, however, appear to be robust to the quality of the text and
therefore suitable for partly automating the mSME lending process. We also
demonstrate how the content of loan assessments influences performance, leading
us to a series of recommendations on a new strategy for collecting future mSME
loan assessments.

    

### [[2004.08100] Recommendation system using a deep learning and graph analysis approach](http://arxiv.org/abs/2004.08100)


  When a user connects to the Internet to fulfill his needs, he often
encounters a huge amount of related information. Recommender systems are the
techniques for massively filtering information and offering the items that
users find them satisfying and interesting. The advances in machine learning
methods, especially deep learning, have led to great achievements in
recommender systems, although these systems still suffer from challenges such
as cold-start and sparsity problems. To solve these problems, context
information such as user communication network is usually used. In this paper,
we have proposed a novel recommendation method based on Matrix Factorization
and graph analysis methods. In addition, we leverage deep Autoencoders to
initialize users and items latent factors, and deep embedding method gathers
users' latent factors from the user trust graph. The proposed method is
implemented on two standard datasets. The experimental results and comparisons
demonstrate that the proposed approach is superior to the existing
state-of-the-art recommendation methods. Our approach outperforms other
comparative methods and achieves great improvements. This work has been
submitted to the IEEE for possible publication. Copyright may be transferred
without notice, after which this version may no longer be accessible

    

### [[2005.08307] AC-VRNN: Attentive Conditional-VRNN for Multi-Future Trajectory Prediction](http://arxiv.org/abs/2005.08307)


  Anticipating human motion in crowded scenarios is essential for developing
intelligent transportation systems, social-aware robots and advanced video
surveillance applications. A key component of this task is represented by the
inherently multi-modal nature of human paths which makes socially acceptable
multiple futures when human interactions are involved. To this end, we propose
a generative architecture for multi-future trajectory predictions based on
Conditional Variational Recurrent Neural Networks (C-VRNNs). Conditioning
mainly relies on prior belief maps, representing most likely moving directions
and forcing the model to consider past observed dynamics in generating future
positions. Human interactions are modeled with a graph-based attention
mechanism enabling an online attentive hidden state refinement of the recurrent
estimation. To corroborate our model, we perform extensive experiments on
publicly-available datasets (e.g., ETH/UCY, Stanford Drone Dataset, STATS
SportVU NBA, Intersection Drone Dataset and TrajNet++) and demonstrate its
effectiveness in crowded scenes compared to several state-of-the-art methods.

    

### [[2006.07340] Fourier Sparse Leverage Scores and Approximate Kernel Learning](http://arxiv.org/abs/2006.07340)


  We prove new explicit upper bounds on the leverage scores of Fourier sparse
functions under both the Gaussian and Laplace measures. In particular, we study
$s$-sparse functions of the form $f(x) = \sum_{j=1}^s a_j e^{i \lambda_j x}$
for coefficients $a_j \in \mathbb{C}$ and frequencies $\lambda_j \in
\mathbb{R}$. Bounding Fourier sparse leverage scores under various measures is
of pure mathematical interest in approximation theory, and our work extends
existing results for the uniform measure [Erd17,CP19a]. Practically, our bounds
are motivated by two important applications in machine learning:
1. Kernel Approximation. They yield a new random Fourier features algorithm
for approximating Gaussian and Cauchy (rational quadratic) kernel matrices. For
low-dimensional data, our method uses a near optimal number of features, and
its runtime is polynomial in the $statistical\ dimension$ of the approximated
kernel matrix. It is the first "oblivious sketching method" with this property
for any kernel besides the polynomial kernel, resolving an open question of
[AKM+17,AKK+20b].
2. Active Learning. They can be used as non-uniform sampling distributions
for robust active learning when data follows a Gaussian or Laplace
distribution. Using the framework of [AKM+19], we provide essentially optimal
results for bandlimited and multiband interpolation, and Gaussian process
regression. These results generalize existing work that only applies to
uniformly distributed data.

    

### [[2006.11355] Certifying clusters from sum-of-norms clustering](http://arxiv.org/abs/2006.11355)


  Sum-of-norms clustering is a clustering formulation based on convex
optimization that automatically induces hierarchy. Multiple algorithms have
been proposed to solve the optimization problem: subgradient descent by Hocking
et al., ADMM and ADA by Chi and Lange, stochastic incremental algorithm by
Panahi et al. and semismooth Newton-CG augmented Lagrangian method by Sun et
al. All algorithms yield approximate solutions, even though an exact solution
is demanded to determine the correct cluster assignment. The purpose of this
paper is to close the gap between the output from existing algorithms and the
exact solution to the optimization problem. We present a clustering test that
identifies and certifies the correct cluster assignment from an approximate
solution yielded by any primal-dual algorithm. Our certification validates
clustering for both unit and multiplicative weights. The test may not succeed
if the approximation is inaccurate. However, we show the correct cluster
assignment is guaranteed to be certified by a primal-dual path following
algorithm after sufficiently many iterations, provided that the model parameter
$\lambda$ avoids a finite number of bad values. Numerical experiments are
conducted on Gaussian mixture and half-moon data, which indicate that carefully
chosen multiplicative weights increase the recovery power of sum-of-norms
clustering.

    

### [[2007.11612] Convergence of Langevin Monte Carlo in Chi-Squared and Renyi Divergence](http://arxiv.org/abs/2007.11612)


  We study sampling from a target distribution $\nu_* = e^{-f}$ using the
unadjusted Langevin Monte Carlo (LMC) algorithm when the potential $f$
satisfies a strong dissipativity condition and it is first-order smooth with a
Lipschitz gradient. We prove that, initialized with a Gaussian random vector
that has sufficiently small variance, iterating the LMC algorithm for
$\widetilde{\mathcal{O}}(\lambda^2 d\epsilon^{-1})$ steps is sufficient to
reach $\epsilon$-neighborhood of the target in both Chi-squared and Renyi
divergence, where $\lambda$ is the logarithmic Sobolev constant of $\nu_*$. Our
results do not require warm-start to deal with the exponential dimension
dependency in Chi-squared divergence at initialization. In particular, for
strongly convex and first-order smooth potentials, we show that the LMC
algorithm achieves the rate estimate $\widetilde{\mathcal{O}}(d\epsilon^{-1})$
which improves the previously known rates in both of these metrics, under the
same assumptions. Translating this rate to other metrics, our results also
recover the state-of-the-art rate estimates in KL divergence, total variation
and $2$-Wasserstein distance in the same setup. Finally, as we rely on the
logarithmic Sobolev inequality, our framework covers a range of non-convex
potentials that are first-order smooth and exhibit strong convexity outside of
a compact region.

    

### [[2008.00025] Rethinking Default Values: a Low Cost and Efficient Strategy to Define Hyperparameters](http://arxiv.org/abs/2008.00025)


  Machine Learning (ML) algorithms have been increasingly applied to problems
from several different areas. Despite their growing popularity, their
predictive performance is usually affected by the values assigned to their
hyperparameters (HPs). As consequence, researchers and practitioners face the
challenge of how to set these values. Many users have limited knowledge about
ML algorithms and the effect of their HP values and, therefore, do not take
advantage of suitable settings. They usually define the HP values by trial and
error, which is very subjective, not guaranteed to find good values and
dependent on the user experience. Tuning techniques search for HP values able
to maximize the predictive performance of induced models for a given dataset,
but have the drawback of a high computational cost. Thus, practitioners use
default values suggested by the algorithm developer or by tools implementing
the algorithm. Although default values usually result in models with acceptable
predictive performance, different implementations of the same algorithm can
suggest distinct default values. To maintain a balance between tuning and using
default values, we propose a strategy to generate new optimized default values.
Our approach is grounded on a small set of optimized values able to obtain
predictive performance values better than default settings provided by popular
tools. After performing a large experiment and a careful analysis of the
results, we concluded that our approach delivers better default values.
Besides, it leads to competitive solutions when compared to tuned values,
making it easier to use and having a lower cost. We also extracted simple rules
to guide practitioners in deciding whether to use our new methodology or a HP
tuning approach.

    

### [[2008.08733] Optimal Network Compression](http://arxiv.org/abs/2008.08733)


  This paper introduces a formulation of the optimal network compression
problem for financial systems. This general formulation is presented for
different levels of network compression or rerouting allowed from the initial
interbank network. We prove that this problem is, generically, NP-hard. We
focus on objective functions generated by systemic risk measures under
systematic shocks to the financial network. We conclude by studying the optimal
compression problem for specific networks; this permits us to study the
so-called robust fragility of certain network topologies more generally as well
as the potential benefits and costs of network compression. In particular,
under systematic shocks and heterogeneous financial networks the typical
heuristics of robust fragility no longer hold generally.

    

### [[2009.05244] Defending Against Multiple and Unforeseen Adversarial Videos](http://arxiv.org/abs/2009.05244)


  Adversarial robustness of deep neural networks has been actively
investigated. However, most existing defense approaches are limited to a
specific type of adversarial perturbations. Specifically, they often fail to
offer resistance to multiple attack types simultaneously, i.e., they lack
multi-perturbation robustness. Furthermore, compared to image recognition
problems, the adversarial robustness of video recognition models is relatively
unexplored. While several studies have proposed how to generate adversarial
videos, only a handful of approaches about the defense strategies have been
published in the literature. In this paper, we propose one of the first defense
strategies against multiple types of adversarial videos for video recognition.
The proposed method, referred to as MultiBN, performs adversarial training on
multiple adversarial video types using multiple independent batch normalization
(BN) layers with a learning-based BN selection module. With a multiple BN
structure, each BN brach is responsible for learning the distribution of a
single perturbation type and thus provides more precise distribution
estimations. This mechanism benefits dealing with multiple perturbation types.
The BN selection module detects the attack type of an input video and sends it
to the corresponding BN branch, making MultiBN fully automatic and allow
end-to-end training. Compared to present adversarial training approaches, the
proposed MultiBN exhibits stronger multi-perturbation robustness against
different and even unforeseen adversarial video types, ranging from Lp-bounded
attacks and physically realizable attacks. This holds true on different
datasets and target models. Moreover, we conduct an extensive analysis to study
the properties of the multiple BN structure.

    

### [[2009.09706] Deep Reinforcement Learning Methods for Structure-Guided Processing Path Optimization](http://arxiv.org/abs/2009.09706)


  A major goal of materials design is to find material structures with desired
properties and in a second step to find a processing path to reach one of these
structures. In this paper, we propose and investigate a deep reinforcement
learning approach for the optimization of processing paths. The goal is to find
optimal processing paths in the material structure space that lead to
target-structures, which have been identified beforehand to result in desired
material properties. There exists a target set containing one or multiple
different structures. Our proposed methods can find an optimal path from a
start structure to a single target structure, or optimize the processing paths
to one of the equivalent target-structures in the set. In the latter case, the
algorithm learns during processing to simultaneously identify the best
reachable target structure and the optimal path to it. The proposed methods
belong to the family of model-free deep reinforcement learning algorithms. They
are guided by structure representations as features of the process state and by
a reward signal, which is formulated based on a distance function in the
structure space. Model-free reinforcement learning algorithms learn through
trial and error while interacting with the process. Thereby, they are not
restricted to information from a priori sampled processing data and are able to
adapt to the specific process. The optimization itself is model-free and does
not require any prior knowledge about the process itself. We instantiate and
evaluate the proposed methods by optimizing paths of a generic metal forming
process. We show the ability of both methods to find processing paths leading
close to target structures and the ability of the extended method to identify
target-structures that can be reached effectively and efficiently and to focus
on these targets for sample efficient processing path optimization.

    

### [[2009.13303] Sim-to-Real Transfer in Deep Reinforcement Learning for Robotics: a Survey](http://arxiv.org/abs/2009.13303)


  Deep reinforcement learning has recently seen huge success across multiple
areas in the robotics domain. Owing to the limitations of gathering real-world
data, i.e., sample inefficiency and the cost of collecting it, simulation
environments are utilized for training the different agents. This not only aids
in providing a potentially infinite data source, but also alleviates safety
concerns with real robots. Nonetheless, the gap between the simulated and real
worlds degrades the performance of the policies once the models are transferred
into real robots. Multiple research efforts are therefore now being directed
towards closing this sim-to-real gap and accomplish more efficient policy
transfer. Recent years have seen the emergence of multiple methods applicable
to different domains, but there is a lack, to the best of our knowledge, of a
comprehensive review summarizing and putting into context the different
methods. In this survey paper, we cover the fundamental background behind
sim-to-real transfer in deep reinforcement learning and overview the main
methods being utilized at the moment: domain randomization, domain adaptation,
imitation learning, meta-learning and knowledge distillation. We categorize
some of the most relevant recent works, and outline the main application
scenarios. Finally, we discuss the main opportunities and challenges of the
different approaches and point to the most promising directions.

    

### [[2010.01245] Consensus Clustering With Unsupervised Representation Learning](http://arxiv.org/abs/2010.01245)


  Recent advances in deep clustering and unsupervised representation learning
are based on the idea that different views of an input image (generated through
data augmentation techniques) must either be closer in the representation
space, or have a similar cluster assignment. Bootstrap Your Own Latent (BYOL)
is one such representation learning algorithm that has achieved
state-of-the-art results in self-supervised image classification on ImageNet
under the linear evaluation protocol. However, the utility of the learnt
features of BYOL to perform clustering is not explored. In this work, we study
the clustering ability of BYOL and observe that features learnt using BYOL may
not be optimal for clustering. We propose a novel consensus clustering based
loss function, and train BYOL with the proposed loss in an end-to-end way that
improves the clustering ability and outperforms similar clustering based
methods on some popular computer vision datasets.

    

### [[2011.07018] Synthetic Data -- Anonymisation Groundhog Day](http://arxiv.org/abs/2011.07018)


  Synthetic data has been advertised as a silver-bullet solution to
privacy-preserving data publishing that addresses the shortcomings of
traditional anonymisation techniques. The promise is that synthetic data drawn
from generative models preserves the statistical properties of the original
dataset but, at the same time, provides perfect protection against privacy
attacks. In this work, we present the first quantitative evaluation of the
privacy gain of synthetic data publishing and compare it to that of previous
anonymisation techniques.
Our evaluation of a wide range of state-of-the-art generative models
demonstrates that synthetic data either does not prevent inference attacks or
does not retain data utility. In other words, we empirically show that
synthetic data suffers from the same limitations as traditional anonymisation
techniques.
Furthermore, we find that, in contrast to traditional anonymisation, the
privacy-utility tradeoff of synthetic data publishing is hard to predict.
Because it is impossible to predict what signals a synthetic dataset will
preserve and what information will be lost, synthetic data leads to a highly
variable privacy gain and unpredictable utility loss. In summary, we find that
synthetic data is far from the holy grail of privacy-preserving data
publishing.

    

### [[2101.00828] Transformer-based Conditional Variational Autoencoder for Controllable Story Generation](http://arxiv.org/abs/2101.00828)


  We investigate large-scale latent variable models (LVMs) for neural story
generation -- an under-explored application for open-domain long text -- with
objectives in two threads: generation effectiveness and controllability. LVMs,
especially the variational autoencoder (VAE), have achieved both effective and
controllable generation through exploiting flexible distributional latent
representations. Recently, Transformers and its variants have achieved
remarkable effectiveness without explicit latent representation learning, thus
lack satisfying controllability in generation. In this paper, we advocate to
revive latent variable modeling, essentially the power of representation
learning, in the era of Transformers to enhance controllability without hurting
state-of-the-art generation effectiveness. Specifically, we integrate latent
representation vectors with a Transformer-based pre-trained architecture to
build conditional variational autoencoder (CVAE). Model components such as
encoder, decoder and the variational posterior are all built on top of
pre-trained language models -- GPT2 specifically in this paper. Experiments
demonstrate state-of-the-art conditional generation ability of our model, as
well as its excellent representation learning capability and controllability.

    

### [[2101.02287] COVID19-HPSMP: COVID-19 Adopted Hybrid and Parallel Deep Information Fusion Framework for Stock Price Movement Prediction](http://arxiv.org/abs/2101.02287)


  The novel of coronavirus (COVID-19) has suddenly and abruptly changed the
world as we knew at the start of the 3rd decade of the 21st century.
Particularly, COVID-19 pandemic has negatively affected financial econometrics
and stock markets across the globe. Artificial Intelligence (AI) and Machine
Learning (ML)-based prediction models, especially Deep Neural Network (DNN)
architectures, have the potential to act as a key enabling factor to reduce the
adverse effects of the COVID-19 pandemic and future possible ones on financial
markets. In this regard, first, a unique COVID-19 related PRIce MOvement
prediction (COVID19 PRIMO) dataset is introduced in this paper, which
incorporates effects of social media trends related to COVID-19 on stock market
price movements. Afterwards, a novel hybrid and parallel DNN-based framework is
proposed that integrates different and diversified learning architectures.
Referred to as the COVID-19 adopted Hybrid and Parallel deep fusion framework
for Stock price Movement Prediction (COVID19-HPSMP), innovative fusion
strategies are used to combine scattered social media news related to COVID-19
with historical mark data. The proposed COVID19-HPSMP consists of two parallel
paths (hence hybrid), one based on Convolutional Neural Network (CNN) with
Local/Global Attention modules, and one integrated CNN and Bi-directional Long
Short term Memory (BLSTM) path. The two parallel paths are followed by a
multilayer fusion layer acting as a fusion centre that combines localized
features. Performance evaluations are performed based on the introduced COVID19
PRIMO dataset illustrating superior performance of the proposed framework.

    

### [[2102.05406] Non-stationary Reinforcement Learning without Prior Knowledge: An Optimal Black-box Approach](http://arxiv.org/abs/2102.05406)


  We propose a black-box reduction that turns a certain reinforcement learning
algorithm with optimal regret in a (near-)stationary environment into another
algorithm with optimal dynamic regret in a non-stationary environment,
importantly without any prior knowledge on the degree of non-stationarity. By
plugging different algorithms into our black-box, we provide a list of examples
showing that our approach not only recovers recent results for (contextual)
multi-armed bandits achieved by very specialized algorithms, but also
significantly improves the state of the art for (generalized) linear bandits,
episodic MDPs, and infinite-horizon MDPs in various ways. Specifically, in most
cases our algorithm achieves the optimal dynamic regret
$\widetilde{\mathcal{O}}(\min\{\sqrt{LT}, \Delta^{1/3}T^{2/3}\})$ where $T$ is
the number of rounds and $L$ and $\Delta$ are the number and amount of changes
of the world respectively, while previous works only obtain suboptimal bounds
and/or require the knowledge of $L$ and $\Delta$.

    

### [[2102.06794] Extending Lagrangian and Hamiltonian Neural Networks with Differentiable Contact Models](http://arxiv.org/abs/2102.06794)


  The incorporation of appropriate inductive bias plays a critical role in
learning dynamics from data. A growing body of work has been exploring ways to
enforce energy conservation in the learned dynamics by encoding Lagrangian or
Hamiltonian dynamics into the neural network architecture. These existing
approaches are based on differential equations, which do not allow
discontinuity in the states and thereby limit the class of systems one can
learn. However, in reality, most physical systems, such as legged robots and
robotic manipulators, involve contacts and collisions, which introduce
discontinuities in the states. In this paper, we introduce a differentiable
contact model, which can capture contact mechanics: frictionless/frictional, as
well as elastic/inelastic. This model can also accommodate inequality
constraints, such as limits on the joint angles. The proposed contact model
extends the scope of Lagrangian and Hamiltonian neural networks by allowing
simultaneous learning of contact and system properties. We demonstrate this
framework on a series of challenging 2D and 3D physical systems with different
coefficients of restitution and friction. The learned dynamics can be used as a
differentiable physics simulator for downstream gradient-based optimization
tasks, such as planning and control.

    

### [[2102.13176] A Probabilistic Interpretation of Self-Paced Learning with Applications to Reinforcement Learning](http://arxiv.org/abs/2102.13176)


  Across machine learning, the use of curricula has shown strong empirical
potential to improve learning from data by avoiding local optima of training
objectives. For reinforcement learning (RL), curricula are especially
interesting, as the underlying optimization has a strong tendency to get stuck
in local optima due to the exploration-exploitation trade-off. Recently, a
number of approaches for an automatic generation of curricula for RL have been
shown to increase performance while requiring less expert knowledge compared to
manually designed curricula. However, these approaches are seldomly
investigated from a theoretical perspective, preventing a deeper understanding
of their mechanics. In this paper, we present an approach for automated
curriculum generation in RL with a clear theoretical underpinning. More
precisely, we formalize the well-known self-paced learning paradigm as inducing
a distribution over training tasks, which trades off between task complexity
and the objective to match a desired task distribution. Experiments show that
training on this induced distribution helps to avoid poor local optima across
RL algorithms in different tasks with uninformative rewards and challenging
exploration requirements.

    

### [[2103.04555] Real-world Ride-hailing Vehicle Repositioning using Deep Reinforcement Learning](http://arxiv.org/abs/2103.04555)


  We present a new practical framework based on deep reinforcement learning and
decision-time planning for real-world vehicle repositioning on ride-hailing (a
type of mobility-on-demand, MoD) platforms. Our approach learns the
spatiotemporal state-value function using a batch training algorithm with deep
value networks. The optimal repositioning action is generated on-demand through
value-based policy search, which combines planning and bootstrapping with the
value networks. For the large-fleet problems, we develop several algorithmic
features that we incorporate into our framework and that we demonstrate to
induce coordination among the algorithmically-guided vehicles. We benchmark our
algorithm with baselines in a ride-hailing simulation environment to
demonstrate its superiority in improving income efficiency meausred by
income-per-hour. We have also designed and run a real-world experiment program
with regular drivers on a major ride-hailing platform. We have observed
significantly positive results on key metrics comparing our method with
experienced drivers who performed idle-time repositioning based on their own
expertise.

    

### [[2103.11820] GPNAS: A Neural Network Architecture Search Framework Based on Graphical Predictor](http://arxiv.org/abs/2103.11820)


  In practice, the problems encountered in Neural Architecture Search (NAS)
training are not simple problems, but often a series of difficult combinations
(wrong compensation estimation, curse of dimension, overfitting, high
complexity, etc.). In this paper, we propose a framework to decouple network
structure from operator search space, and use two BOHBs to search
alternatively. Considering that activation function and initialization are also
important parts of neural network, the generalization ability of the model will
be affected. We introduce an activation function and an initialization method
domain, and add them into the operator search space to form a generalized
search space, so as to improve the generalization ability of the child model.
We then trained a GCN-based predictor using feedback from the child model. This
can not only improve the search efficiency, but also solve the problem of
dimension curse. Next, unlike other NAS studies, we used predictors to analyze
the stability of different network structures. Finally, we applied our
framework to neural structure search and achieved significant improvements on
multiple datasets.

    

### [[2103.17171] Spectral decoupling allows training transferable neural networks in medical imaging](http://arxiv.org/abs/2103.17171)


  Many current neural networks for medical imaging generalise poorly to data
unseen during training. Such behaviour can be caused by networks overfitting
easy-to-learn, or statistically dominant, features while disregarding other
potentially informative features. For example, indistinguishable differences in
the sharpness of the images from two different scanners can degrade the
performance of the network significantly. All neural networks intended for
clinical practice need to be robust to variation in data caused by differences
in imaging equipment, sample preparation and patient populations.
To address these challenges, we evaluate the utility of spectral decoupling
as an implicit bias mitigation method. Spectral decoupling encourages the
neural network to learn more features by simply regularising the networks'
unnormalised prediction scores with an L2 penalty, thus having no added
computational costs.
We show that spectral decoupling allows training neural networks on datasets
with strong spurious correlations. Networks trained without spectral decoupling
do not learn the original task and appear to make false predictions based on
the spurious correlations. Spectral decoupling also increases networks'
robustness for data distribution shifts. To validate our findings, we train
networks with and without spectral decoupling to detect prostate cancer tissue
slides and COVID-19 in chest radiographs. Networks trained with spectral
decoupling achieve substantially higher performance on all evaluation datasets.
Our results show that spectral decoupling helps with generalisation issues
associated with neural networks. We recommend using spectral decoupling as an
implicit bias mitigation method in any neural network intended for clinical
use.

    

### [[2105.00774] Fast Multi-Step Critiquing for VAE-based Recommender Systems](http://arxiv.org/abs/2105.00774)


  Recent studies have shown that providing personalized explanations alongside
recommendations increases trust and perceived quality. Furthermore, it gives
users an opportunity to refine the recommendations by critiquing parts of the
explanations. On one hand, current recommender systems model the
recommendation, explanation, and critiquing objectives jointly, but this
creates an inherent trade-off between their respective performance. On the
other hand, although recent latent linear critiquing approaches are built upon
an existing recommender system, they suffer from computational inefficiency at
inference due to the objective optimized at each conversation's turn. We
address these deficiencies with M&Ms-VAE, a novel variational autoencoder for
recommendation and explanation that is based on multimodal modeling
assumptions. We train the model under a weak supervision scheme to simulate
both fully and partially observed variables. Then, we leverage the
generalization ability of a trained M&Ms-VAE model to embed the user preference
and the critique separately. Our work's most important innovation is our
critiquing module, which is built upon and trained in a self-supervised manner
with a simple ranking objective. Experiments on four real-world datasets
demonstrate that among state-of-the-art models, our system is the first to
dominate or match the performance in terms of recommendation, explanation, and
multi-step critiquing. Moreover, M&Ms-VAE processes the critiques up to 25.6x
faster than the best baselines. Finally, we show that our model infers coherent
joint and cross generation, even under weak supervision, thanks to our
multimodal-based modeling and training scheme.

    

### [[2105.09908] Classification of Urban Morphology with Deep Learning: Application on Urban Vitality](http://arxiv.org/abs/2105.09908)


  There is a prevailing trend to study urban morphology quantitatively thanks
to the growing accessibility to various forms of spatial big data, increasing
computing power, and use cases benefiting from such information. The methods
developed up to now measure urban morphology with numerical indices describing
density, proportion, and mixture, but they do not directly represent
morphological features from the human's visual and intuitive perspective. We
take the first step to bridge the gap by proposing a deep learning-based
technique to automatically classify road networks into four classes on a visual
basis. The method is implemented by generating an image of the street network
(Colored Road Hierarchy Diagram), which we introduce in this paper, and
classifying it using a deep convolutional neural network (ResNet-34). The model
achieves an overall classification accuracy of 0.875. Nine cities around the
world are selected as the study areas with their road networks acquired from
OpenStreetMap. Latent subgroups among the cities are uncovered through
clustering on the percentage of each road network category. In the subsequent
part of the paper, we focus on the usability of such classification: we apply
our method in a case study of urban vitality prediction. An advanced tree-based
regression model (LightGBM) is for the first time designated to establish the
relationship between morphological indices and vitality indicators. The effect
of road network classification is found to be small but positively associated
with urban vitality. This work expands the toolkit of quantitative urban
morphology study with new techniques, supporting further studies in the future.

    

### [[2105.13921] TensorFlow RiemOpt: a library for optimization on Riemannian manifolds](http://arxiv.org/abs/2105.13921)


  The adoption of neural networks and deep learning in non-Euclidean domains
has been hindered until recently by the lack of scalable and efficient learning
frameworks. Existing toolboxes in this space were mainly motivated by research
and education use cases, whereas practical aspects, such as deploying and
maintaining machine learning models, were often overlooked.
We attempt to bridge this gap by proposing TensorFlow RiemOpt, a Python
library for optimization on Riemannian manifolds in TensorFlow. The library is
designed with the aim for a seamless integration with the TensorFlow ecosystem,
targeting not only research, but also streamlining production machine learning
pipelines.

    

### [[2106.05956] Beyond BatchNorm: Towards a General Understanding of Normalization in Deep Learning](http://arxiv.org/abs/2106.05956)


  Inspired by BatchNorm, there has been an explosion of normalization layers
for deep neural networks (DNNs). However, these alternative normalization
layers have seen minimal use, partially due to a lack of guiding principles
that can help identify when these layers can serve as a replacement for
BatchNorm. To address this problem, we take a theoretical approach,
generalizing the known beneficial mechanisms of BatchNorm to several recently
proposed normalization techniques. Our generalized theory leads to the
following set of principles: (i) similar to BatchNorm, activations-based
normalization layers can prevent exponential growth of activations in ResNets,
but parametric layers require explicit remedies; (ii) use of GroupNorm can
ensure informative forward propagation, with different samples being assigned
dissimilar activations, but increasing group size results in increasingly
indistinguishable activations for different samples, explaining slow
convergence speed in models with LayerNorm; (iii) small group sizes result in
large gradient norm in earlier layers, hence explaining training instability
issues in Instance Normalization and illustrating a speed-stability tradeoff in
GroupNorm. Overall, our analysis reveals a unified set of mechanisms that
underpin the success of normalization methods in deep learning, providing us
with a compass to systematically explore the vast design space of DNN
normalization layers.

    

### [[2106.06054] Fair Preprocessing: Towards Understanding Compositional Fairness of Data Transformers in Machine Learning Pipeline](http://arxiv.org/abs/2106.06054)


  In recent years, many incidents have been reported where machine learning
models exhibited discrimination among people based on race, sex, age, etc.
Research has been conducted to measure and mitigate unfairness in machine
learning models. For a machine learning task, it is a common practice to build
a pipeline that includes an ordered set of data preprocessing stages followed
by a classifier. However, most of the research on fairness has considered a
single classifier based prediction task. What are the fairness impacts of the
preprocessing stages in machine learning pipeline? Furthermore, studies showed
that often the root cause of unfairness is ingrained in the data itself, rather
than the model. But no research has been conducted to measure the unfairness
caused by a specific transformation made in the data preprocessing stage. In
this paper, we introduced the causal method of fairness to reason about the
fairness impact of data preprocessing stages in ML pipeline. We leveraged
existing metrics to define the fairness measures of the stages. Then we
conducted a detailed fairness evaluation of the preprocessing stages in 37
pipelines collected from three different sources. Our results show that certain
data transformers are causing the model to exhibit unfairness. We identified a
number of fairness patterns in several categories of data transformers.
Finally, we showed how the local fairness of a preprocessing stage composes in
the global fairness of the pipeline. We used the fairness composition to choose
appropriate downstream transformer that mitigates unfairness in the machine
learning pipeline.

    

### [[2106.06130] ChemRL-GEM: Geometry Enhanced Molecular Representation Learning for Property Prediction](http://arxiv.org/abs/2106.06130)


  Effective molecular representation learning is of great importance to
facilitate molecular property prediction, which is a fundamental task for the
drug and material industry. Recent advances in graph neural networks (GNNs)
have shown great promise in applying GNNs for molecular representation
learning. Moreover, a few recent studies have also demonstrated successful
applications of self-supervised learning methods to pre-train the GNNs to
overcome the problem of insufficient labeled molecules. However, existing GNNs
and pre-training strategies usually treat molecules as topological graph data
without fully utilizing the molecular geometry information. Whereas, the
three-dimensional (3D) spatial structure of a molecule, a.k.a molecular
geometry, is one of the most critical factors for determining molecular
physical, chemical, and biological properties. To this end, we propose a novel
Geometry Enhanced Molecular representation learning method (GEM) for Chemical
Representation Learning (ChemRL). At first, we design a geometry-based GNN
architecture that simultaneously models atoms, bonds, and bond angles in a
molecule. To be specific, we devised double graphs for a molecule: The first
one encodes the atom-bond relations; The second one encodes bond-angle
relations. Moreover, on top of the devised GNN architecture, we propose several
novel geometry-level self-supervised learning strategies to learn spatial
knowledge by utilizing the local and global molecular 3D structures. We compare
ChemRL-GEM with various state-of-the-art (SOTA) baselines on different
molecular benchmarks and exhibit that ChemRL-GEM can significantly outperform
all baselines in both regression and classification tasks. For example, the
experimental results show an overall improvement of 8.8% on average compared to
SOTA baselines on the regression tasks, demonstrating the superiority of the
proposed method.

    

### [[2106.10494] Teacher's pet: understanding and mitigating biases in distillation](http://arxiv.org/abs/2106.10494)


  Knowledge distillation is widely used as a means of improving the performance
of a relatively simple student model using the predictions from a complex
teacher model. Several works have shown that distillation significantly boosts
the student's overall performance; however, are these gains uniform across all
data subgroups? In this paper, we show that distillation can harm performance
on certain subgroups, e.g., classes with few associated samples. We trace this
behaviour to errors made by the teacher distribution being transferred to and
amplified by the student model. To mitigate this problem, we present techniques
which soften the teacher influence for subgroups where it is less reliable.
Experiments on several image classification benchmarks show that these
modifications of distillation maintain boost in overall accuracy, while
additionally ensuring improvement in subgroup performance.

    

### [[2106.13681] Robust Matrix Factorization with Grouping Effect](http://arxiv.org/abs/2106.13681)


  Although many techniques have been applied to matrix factorization (MF), they
may not fully exploit the feature structure. In this paper, we incorporate the
grouping effect into MF and propose a novel method called Robust Matrix
Factorization with Grouping effect (GRMF). The grouping effect is a
generalization of the sparsity effect, which conducts denoising by clustering
similar values around multiple centers instead of just around 0. Compared with
existing algorithms, the proposed GRMF can automatically learn the grouping
structure and sparsity in MF without prior knowledge, by introducing a
naturally adjustable non-convex regularization to achieve simultaneous sparsity
and grouping effect. Specifically, GRMF uses an efficient alternating
minimization framework to perform MF, in which the original non-convex problem
is first converted into a convex problem through Difference-of-Convex (DC)
programming, and then solved by Alternating Direction Method of Multipliers
(ADMM). In addition, GRMF can be easily extended to the Non-negative Matrix
Factorization (NMF) settings. Extensive experiments have been conducted using
real-world data sets with outliers and contaminated noise, where the
experimental results show that GRMF has promoted performance and robustness,
compared to five benchmark algorithms.

    

### [[2106.14257] Use of Machine Learning Technique to maximize the signal over background for $H \rightarrow $](http://arxiv.org/abs/2106.14257)


  In recent years, artificial neural networks (ANNs) have won numerous contests
in pattern recognition and machine learning. ANNS have been applied to problems
ranging from speech recognition to prediction of protein secondary structure,
classification of cancers, and gene prediction. Here, we intend to maximize the
chances of finding the Higgs boson decays to two $\tau$ leptons in the pseudo
dataset using a Machine Learning technique to classify the recorded events as
signal or background.

    

### [[2106.15282] Cascaded Diffusion Models for High Fidelity Image Generation](http://arxiv.org/abs/2106.15282)


  We show that cascaded diffusion models are capable of generating high
fidelity images on the class-conditional ImageNet generation challenge, without
any assistance from auxiliary image classifiers to boost sample quality. A
cascaded diffusion model comprises a pipeline of multiple diffusion models that
generate images of increasing resolution, beginning with a standard diffusion
model at the lowest resolution, followed by one or more super-resolution
diffusion models that successively upsample the image and add higher resolution
details. We find that the sample quality of a cascading pipeline relies
crucially on conditioning augmentation, our proposed method of data
augmentation of the lower resolution conditioning inputs to the
super-resolution models. Our experiments show that conditioning augmentation
prevents compounding error during sampling in a cascaded model, helping us to
train cascading pipelines achieving FID scores of 1.48 at 64x64, 3.52 at
128x128 and 4.88 at 256x256 resolutions, outperforming BigGAN-deep, and
classification accuracy scores of 63.02% (top-1) and 84.06% (top-5) at 256x256,
outperforming VQ-VAE-2.

    

### [[2107.02279] Design Smells in Deep Learning Programs: An Empirical Study](http://arxiv.org/abs/2107.02279)


  Nowadays, we are witnessing an increasing adoption of Deep Learning (DL)
based software systems in many industries. Designing a DL program requires
constructing a deep neural network (DNN) and then training it on a dataset.
This process requires that developers make multiple architectural (e.g., type,
size, number, and order of layers) and configuration (e.g., optimizer,
regularization methods, and activation functions) choices that affect the
quality of the DL models, and consequently software quality. An under-specified
or poorly-designed DL model may train successfully but is likely to perform
poorly when deployed in production. Design smells in DL programs are poor
design and-or configuration decisions taken during the development of DL
components, that are likely to have a negative impact on the performance (i.e.,
prediction accuracy) and then quality of DL based software systems. In this
paper, we present a catalogue of 8 design smells for a popular DL architecture,
namely deep Feedforward Neural Networks which is widely employed in industrial
applications. The design smells were identified through a review of the
existing literature on DL design and a manual inspection of 659 DL programs
with performance issues and design inefficiencies. The smells are specified by
describing their context, consequences, and recommended refactorings. To
provide empirical evidence on the relevance and perceived impact of the
proposed design smells, we conducted a survey with 81 DL developers. In
general, the developers perceived the proposed design smells as reflective of
design or implementation problems, with agreement levels varying between 47\%
and 68\%.

    

### [[2107.03781] Towards a Trusted Execution Environment via Reconfigurable FPGA](http://arxiv.org/abs/2107.03781)


  Trusted Execution Environments (TEEs) are used to protect sensitive data and
run secure execution for security-critical applications, by providing an
environment isolated from the rest of the system. However, over the last few
years, TEEs have been proven weak, as either TEEs built upon security-oriented
hardware extensions (e.g., Arm TrustZone) or resorting to dedicated secure
elements were exploited multiple times. In this project, we introduce Trusted
Execution Environments On-Demand (TEEOD), a novel TEE design that leverages the
programmable logic (PL) in the heterogeneous system on chips (SoC) as the
secure execution environment. Unlike other TEE designs, TEEOD can provide
high-bandwidth connections and physical on-chip isolation. We implemented a
proof-of-concept (PoC) implementation targeting an Ultra96-V2 platform. The
conducted evaluation demonstrated TEEOD can host up to 6 simultaneous enclaves
with a resource usage per enclave of 7.0%, 3.8%, and 15.3% of the total LUTs,
FFs, and BRAMS, respectively. To demonstrate the practicability of TEEOD in
real-world applications, we successfully run a legacy open-source Bitcoin
wallet.

    

### [[2107.03467] An Empirical Analysis of VM Startup Times in Public IaaS Clouds: An Extended Report](http://arxiv.org/abs/2107.03467)


  VM startup time is an essential factor in designing elastic cloud
applications. For example, a cloud application with autoscaling can reduce
under- and over-provisioning of VM instances with a precise estimation of VM
startup time, and in turn, it is likely to guarantee the application's
performance and improve the cost efficiency. However, VM startup time has been
little studied, and available measurement results performed previously did not
consider various configurations of VMs for modern cloud applications. In this
work, we perform comprehensive measurements and analysis of VM startup time
from two major cloud providers, namely Amazon Web Services (AWS) and Google
Cloud Platform (GCP). With three months of measurements, we collected more than
300,000 data points from each provider by applying a set of configurations,
including 11+ VM types, four different data center locations, four VM image
sizes, two OS types, and two purchase models (e.g., spot/preemptible VMs vs.
on-demand VMs). With extensive analysis, we found that VM startup time can vary
significantly because of several important factors, such as VM image sizes,
data center locations, VM types, and OS types. Moreover, by comparing with
previous measurement results, we confirm that cloud providers (specifically
AWS) made significant improvements for the VM startup times and currently have
much quicker VM startup times than in the past.

    

### [[2107.03492] Persistent Software Combining](http://arxiv.org/abs/2107.03492)


  We study the performance power of software combining in designing persistent
algorithms and data structures. We present Bcomb, a new blocking
highly-efficient combining protocol, and built upon it to get PBcomb, a
persistent version of it that performs a small number of persistence
instructions and exhibits low synchronization cost. We built fundamental
recoverable data structures, such as stacks and queues based on PBcomb, as well
as on PWFcomb, a wait-free universal construction we present. Our experiments
show that PBcomb and PWFcomb outperform by far state-of-the-art recoverable
universal constructions and transactional memory systems, many of which ensure
weaker consistency properties than our algorithms. We built recoverable queues
and stacks, based on PBcomb and PWFcomb, and present experiments to show that
they have much better performance than previous recoverable implementations of
stacks and queues. We build the first recoverable implementation of a
concurrent heap and present experiments to show that it has good performance
when the size of the heap is not very large.

    

### [[2107.03882] A Multi-Protocol, Secure, and Dynamic Data Storage Integration Frameworkfor Multi-tenanted Science Gateway Middleware](http://arxiv.org/abs/2107.03882)


  Science gateways are user-centric, end-to-end cyberinfrastructure for
managing scientific data and executions of computational software on
distributed resources. In order to simplify the creation and management of
science gateways, we have pursued a multi-tenanted, platform-as-a-service
approach that allows multiple gateway front-ends (portals) to be integrated
with a consolidated middleware that manages the movement of data and the
execution of workflows on multiple back-end scientific computing resources. An
important challenge for this approach is to provide an end-to-end data movement
and management solution that allows gateway users to integrate their own data
stores with the gateway platform. These user-provided data stores may include
commercial cloud-based object store systems, third-party data stores accessed
through APIs such as REST endpoints, and users' own local storage resources. In
this paper, we present a solution design and implementation based on the
integration of a managed file transfer (MFT) service (Airavata MFT) into the
platform.

    

### [[2107.03947] HTCondor data movement at 100 Gbps](http://arxiv.org/abs/2107.03947)


  HTCondor is a major workload management system used in distributed high
throughput computing (dHTC) environments, e.g., the Open Science Grid. One of
the distinguishing features of HTCondor is the native support for data
movement, allowing it to operate without a shared filesystem. Coupling data
handling and compute scheduling is both convenient for users and allows for
significant infrastructure flexibility but does introduce some limitations. The
default HTCondor data transfer mechanism routes both the input and output data
through the submission node, making it a potential bottleneck. In this document
we show that by using a node equipped with a 100 Gbps network interface (NIC)
HTCondor can serve data at up to 90 Gbps, which is sufficient for most current
use cases, as it would saturate the border network links of most research
universities at the time of writing.

    

### [[2107.03963] Expanding IceCube GPU computing into the Clouds](http://arxiv.org/abs/2107.03963)


  The IceCube collaboration relies on GPU compute for many of its needs,
including ray tracing simulation and machine learning activities. GPUs are
however still a relatively scarce commodity in the scientific resource provider
community, so we expanded the available resource pool with GPUs provisioned
from the commercial Cloud providers. The provisioned resources were fully
integrated into the normal IceCube workload management system through the Open
Science Grid (OSG) infrastructure and used CloudBank for budget management. The
result was an approximate doubling of GPU wall hours used by IceCube over a
period of 2 weeks, adding over 3.1 fp32 EFLOP hours for a price tag of about
$58k. This paper describes the setup used and the operational experience.

    

### [[2107.03415] A Graph-based Approach for Mitigating Multi-sided Exposure Bias in Recommender Systems](http://arxiv.org/abs/2107.03415)


  Fairness is a critical system-level objective in recommender systems that has
been the subject of extensive recent research. A specific form of fairness is
supplier exposure fairness where the objective is to ensure equitable coverage
of items across all suppliers in recommendations provided to users. This is
especially important in multistakeholder recommendation scenarios where it may
be important to optimize utilities not just for the end-user, but also for
other stakeholders such as item sellers or producers who desire a fair
representation of their items. This type of supplier fairness is sometimes
accomplished by attempting to increasing aggregate diversity in order to
mitigate popularity bias and to improve the coverage of long-tail items in
recommendations. In this paper, we introduce FairMatch, a general graph-based
algorithm that works as a post processing approach after recommendation
generation to improve exposure fairness for items and suppliers. The algorithm
iteratively adds high quality items that have low visibility or items from
suppliers with low exposure to the users' final recommendation lists. A
comprehensive set of experiments on two datasets and comparison with
state-of-the-art baselines show that FairMatch, while significantly improves
exposure fairness and aggregate diversity, maintains an acceptable level of
relevance of the recommendations.

    

### [[2107.03451] Anticipating Safety Issues in E2E Conversational AI: Framework and Tooling](http://arxiv.org/abs/2107.03451)


  Over the last several years, end-to-end neural conversational agents have
vastly improved in their ability to carry a chit-chat conversation with humans.
However, these models are often trained on large datasets from the internet,
and as a result, may learn undesirable behaviors from this data, such as toxic
or otherwise harmful language. Researchers must thus wrestle with the issue of
how and when to release these models. In this paper, we survey the problem
landscape for safety for end-to-end conversational AI and discuss recent and
related work. We highlight tensions between values, potential positive impact
and potential harms, and provide a framework for making decisions about whether
and how to release these models, following the tenets of value-sensitive
design. We additionally provide a suite of tools to enable researchers to make
better-informed decisions about training and releasing end-to-end
conversational AI models.

    

### [[2107.03510] Federated Learning with Downlink Device Selection](http://arxiv.org/abs/2107.03510)


  We study federated edge learning, where a global model is trained
collaboratively using privacy-sensitive data at the edge of a wireless network.
A parameter server (PS) keeps track of the global model and shares it with the
wireless edge devices for training using their private local data. The devices
then transmit their local model updates, which are used to update the global
model, to the PS. The algorithm, which involves transmission over PS-to-device
and device-to-PS links, continues until the convergence of the global model or
lack of any participating devices. In this study, we consider device selection
based on downlink channels over which the PS shares the global model with the
devices. Performing digital downlink transmission, we design a partial device
participation framework where a subset of the devices is selected for training
at each iteration. Therefore, the participating devices can have a better
estimate of the global model compared to the full device participation case
which is due to the shared nature of the broadcast channel with the price of
updating the global model with respect to a smaller set of data. At each
iteration, the PS broadcasts different quantized global model updates to
different participating devices based on the last global model estimates
available at the devices. We investigate the best number of participating
devices through experimental results for image classification using the MNIST
dataset with biased distribution.

    

### [[2107.03564] Unsupervised Proxy Selection for Session-based Recommender Systems](http://arxiv.org/abs/2107.03564)


  Session-based Recommender Systems (SRSs) have been actively developed to
recommend the next item of an anonymous short item sequence (i.e., session).
Unlike sequence-aware recommender systems where the whole interaction sequence
of each user can be used to model both the short-term interest and the general
interest of the user, the absence of user-dependent information in SRSs makes
it difficult to directly derive the user's general interest from data.
Therefore, existing SRSs have focused on how to effectively model the
information about short-term interest within the sessions, but they are
insufficient to capture the general interest of users. To this end, we propose
a novel framework to overcome the limitation of SRSs, named ProxySR, which
imitates the missing information in SRSs (i.e., general interest of users) by
modeling proxies of sessions. ProxySR selects a proxy for the input session in
an unsupervised manner, and combines it with the encoded short-term interest of
the session. As a proxy is jointly learned with the short-term interest and
selected by multiple sessions, a proxy learns to play the role of the general
interest of a user and ProxySR learns how to select a suitable proxy for an
input session. Moreover, we propose another real-world situation of SRSs where
a few users are logged-in and leave their identifiers in sessions, and a
revision of ProxySR for the situation. Our experiments on real-world datasets
show that ProxySR considerably outperforms the state-of-the-art competitors,
and the proxies successfully imitate the general interest of the users without
any user-dependent information.

    

### [[2107.03573] Deep Structural Point Process for Learning Temporal Interaction Networks](http://arxiv.org/abs/2107.03573)


  This work investigates the problem of learning temporal interaction networks.
A temporal interaction network consists of a series of chronological
interactions between users and items. Previous methods tackle this problem by
using different variants of recurrent neural networks to model sequential
interactions, which fail to consider the structural information of temporal
interaction networks and inevitably lead to sub-optimal results. To this end,
we propose a novel Deep Structural Point Process termed as DSPP for learning
temporal interaction networks. DSPP simultaneously incorporates the topological
structure and long-range dependency structure into our intensity function to
enhance model expressiveness. To be specific, by using the topological
structure as a strong prior, we first design a topological fusion encoder to
obtain node embeddings. An attentive shift encoder is then developed to learn
the long-range dependency structure between users and items in continuous time.
The proposed two modules enable our model to capture the user-item correlation
and dynamic influence in temporal interaction networks. DSPP is evaluated on
three real-world datasets for both tasks of item prediction and time
prediction. Extensive experiments demonstrate that our model achieves
consistent and significant improvements over state-of-the-art baselines.

    

### [[2107.03603] CLAIM: Curriculum Learning Policy for Influence Maximization in Unknown Social Networks](http://arxiv.org/abs/2107.03603)


  Influence maximization is the problem of finding a small subset of nodes in a
network that can maximize the diffusion of information. Recently, it has also
found application in HIV prevention, substance abuse prevention, micro-finance
adoption, etc., where the goal is to identify the set of peer leaders in a
real-world physical social network who can disseminate information to a large
group of people. Unlike online social networks, real-world networks are not
completely known, and collecting information about the network is costly as it
involves surveying multiple people. In this paper, we focus on this problem of
network discovery for influence maximization. The existing work in this
direction proposes a reinforcement learning framework. As the environment
interactions in real-world settings are costly, so it is important for the
reinforcement learning algorithms to have minimum possible environment
interactions, i.e, to be sample efficient. In this work, we propose CLAIM -
Curriculum LeArning Policy for Influence Maximization to improve the sample
efficiency of RL methods. We conduct experiments on real-world datasets and
show that our approach can outperform the current best approach.

    

### [[2107.03619] Validation and Inference of Agent Based Models](http://arxiv.org/abs/2107.03619)


  Agent Based Modelling (ABM) is a computational framework for simulating the
behaviours and interactions of autonomous agents. As Agent Based Models are
usually representative of complex systems, obtaining a likelihood function of
the model parameters is nearly always intractable. There is a necessity to
conduct inference in a likelihood free context in order to understand the model
output. Approximate Bayesian Computation is a suitable approach for this
inference. It can be applied to an Agent Based Model to both validate the
simulation and infer a set of parameters to describe the model. Recent research
in ABC has yielded increasingly efficient algorithms for calculating the
approximate likelihood. These are investigated and compared using a pedestrian
model in the Hamilton CBD.

    

### [[2107.03640] A Dataset and Method for Hallux Valgus Angle Estimation Based on Deep Learing](http://arxiv.org/abs/2107.03640)


  Angular measurements is essential to make a resonable treatment for Hallux
valgus (HV), a common forefoot deformity. However, it still depends on manual
labeling and measurement, which is time-consuming and sometimes unreliable.
Automating this process is a thing of concern. However, it lack of dataset and
the keypoints based method which made a great success in pose estimation is not
suitable for this this http URL solve the problems, we made a dataset and developed
an algorithm based on deep learning and linear regression. It shows great
fitting ability to the ground truth.

    

### [[2107.03685] Towards Autonomous Pipeline Inspection with Hierarchical Reinforcement Learning](http://arxiv.org/abs/2107.03685)


  Inspection and maintenance are two crucial aspects of industrial pipeline
plants. While robotics has made tremendous progress in the mechanic design of
in-pipe inspection robots, the autonomous control of such robots is still a big
open challenge due to the high number of actuators and the complex manoeuvres
required. To address this problem, we investigate the usage of Deep
Reinforcement Learning for achieving autonomous navigation of in-pipe robots in
pipeline networks with complex topologies. Moreover, we introduce a
hierarchical policy decomposition based on Hierarchical Reinforcement Learning
to learn robust high-level navigation skills. We show that the hierarchical
structure introduced in the policy is fundamental for solving the navigation
task through pipes and necessary for achieving navigation performances superior
to human-level control.

    

### [[2107.03700] Complete Scanning Application Using OpenCv](http://arxiv.org/abs/2107.03700)


  In the following paper, we have combined the various basic functionalities
provided by the NumPy library and OpenCv library, which is an open source for
Computer Vision applications, like conversion of colored images to grayscale,
calculating threshold, finding contours and using those contour points to take
perspective transform of the image inputted by the user, using Python version
3.7. Additional features include cropping, rotating and saving as well. All
these functions and features, when implemented step by step, results in a
complete scanning application. The applied procedure involves the following
steps: Finding contours, applying Perspective transform and brightening the
image, Adaptive Thresholding and applying filters for noise cancellation, and
Rotation features and perspective transform for a special cropping algorithm.
The described technique is implemented on various samples.

    

### [[2107.03721] Demystifying the Draft EU Artificial Intelligence Act](http://arxiv.org/abs/2107.03721)


  In April 2021, the European Commission proposed a Regulation on Artificial
Intelligence, known as the AI Act. We present an overview of the Act and
analyse its implications, drawing on scholarship ranging from the study of
contemporary AI practices to the structure of EU product safety regimes over
the last four decades. Aspects of the AI Act, such as different rules for
different risk-levels of AI, make sense. But we also find that some provisions
of the draft AI Act have surprising legal implications, whilst others may be
largely ineffective at achieving their stated goals. Several overarching
aspects, including the enforcement regime and the effect of maximum
harmonisation on the space for AI policy more generally, engender significant
concern. These issues should be addressed as a priority in the legislative
process.

    

### [[2107.03751] Exploiting the relationship between visual and textual features in social networks for image classification with zero-shot deep learning](http://arxiv.org/abs/2107.03751)


  One of the main issues related to unsupervised machine learning is the cost
of processing and extracting useful information from large datasets. In this
work, we propose a classifier ensemble based on the transferable learning
capabilities of the CLIP neural network architecture in multimodal environments
(image and text) from social media. For this purpose, we used the InstaNY100K
dataset and proposed a validation approach based on sampling techniques. Our
experiments, based on image classification tasks according to the labels of the
Places dataset, are performed by first considering only the visual part, and
then adding the associated texts as support. The results obtained demonstrated
that trained neural networks such as CLIP can be successfully applied to image
classification with little fine-tuning, and considering the associated texts to
the images can help to improve the accuracy depending on the goal. The results
demonstrated what seems to be a promising research direction.

    

### [[2107.03767] Discrete-time Temporal Network Embedding via Implicit Hierarchical Learning in Hyperbolic Space](http://arxiv.org/abs/2107.03767)


  Representation learning over temporal networks has drawn considerable
attention in recent years. Efforts are mainly focused on modeling structural
dependencies and temporal evolving regularities in Euclidean space which,
however, underestimates the inherent complex and hierarchical properties in
many real-world temporal networks, leading to sub-optimal embeddings. To
explore these properties of a complex temporal network, we propose a hyperbolic
temporal graph network (HTGN) that fully takes advantage of the exponential
capacity and hierarchical awareness of hyperbolic geometry. More specially,
HTGN maps the temporal graph into hyperbolic space, and incorporates hyperbolic
graph neural network and hyperbolic gated recurrent neural network, to capture
the evolving behaviors and implicitly preserve hierarchical information
simultaneously. Furthermore, in the hyperbolic space, we propose two important
modules that enable HTGN to successfully model temporal networks: (1)
hyperbolic temporal contextual self-attention (HTA) module to attend to
historical states and (2) hyperbolic temporal consistency (HTC) module to
ensure stability and generalization. Experimental results on multiple
real-world datasets demonstrate the superiority of HTGN for temporal graph
embedding, as it consistently outperforms competing methods by significant
margins in various temporal link prediction tasks. Specifically, HTGN achieves
AUC improvement up to 9.98% for link prediction and 11.4% for new link
prediction. Moreover, the ablation study further validates the representational
ability of hyperbolic geometry and the effectiveness of the proposed HTA and
HTC modules.

    

### [[2107.03772] Degrees of riskiness, falsifiability, and truthlikeness. A neo-Popperian account applicable to probabilistic theories](http://arxiv.org/abs/2107.03772)


  In this paper, we take a fresh look at three Popperian concepts: riskiness,
falsifiability, and truthlikeness (or verisimilitude) of scientific hypotheses
or theories. First, we make explicit the dimensions that underlie the notion of
riskiness. Secondly, we examine if and how degrees of falsifiability can be
defined, and how they are related to various dimensions of the concept of
riskiness as well as the experimental context. Thirdly, we consider the
relation of riskiness to (expected degrees of) truthlikeness. Throughout, we
pay special attention to probabilistic theories and we offer a tentative,
quantitative account of verisimilitude for probabilistic theories.

    

### [[2107.03813] Heterogeneous Global Graph Neural Networks for Personalized Session-based Recommendation](http://arxiv.org/abs/2107.03813)


  Predicting the next interaction of a short-term interaction session is a
challenging task in session-based recommendation. Almost all existing works
rely on item transition patterns, and neglect the impact of user historical
sessions while modeling user preference, which often leads to non-personalized
recommendation. Additionally, existing personalized session-based recommenders
capture user preference only based on the sessions of the current user, but
ignore the useful item-transition patterns from other user's historical
sessions. To address these issues, we propose a novel Heterogeneous Global
Graph Neural Networks (HG-GNN) to exploit the item transitions over all
sessions in a subtle manner for better inferring user preference from the
current and historical sessions. To effectively exploit the item transitions
over all sessions from users, we propose a novel heterogeneous global graph
that contains item transitions of sessions, user-item interactions and global
co-occurrence items. Moreover, to capture user preference from sessions
comprehensively, we propose to learn two levels of user representations from
the global graph via two graph augmented preference encoders. Specifically, we
design a novel heterogeneous graph neural network (HGNN) on the heterogeneous
global graph to learn the long-term user preference and item representations
with rich semantics. Based on the HGNN, we propose the Current Preference
Encoder and the Historical Preference Encoder to capture the different levels
of user preference from the current and historical sessions, respectively. To
achieve personalized recommendation, we integrate the representations of the
user current preference and historical interests to generate the final user
preference representation. Extensive experimental results on three real-world
datasets show that our model outperforms other state-of-the-art methods.

    

### [[2107.03884] CANDLE: Decomposing Conditional and Conjunctive Queries for Task-Oriented Dialogue Systems](http://arxiv.org/abs/2107.03884)


  Domain-specific dialogue systems generally determine user intents by relying
on sentence-level classifiers which mainly focus on single action sentences.
Such classifiers are not designed to effectively handle complex queries
composed of conditional and sequential clauses that represent multiple actions.
We attempt to decompose such queries into smaller single-action sub-queries
that are reasonable for intent classifiers to understand in a dialogue
pipeline. We release CANDLE (Conditional & AND type Expressions), a dataset
consisting of 3124 utterances manually tagged with conditional and sequential
labels and demonstrates this decomposition by training two baseline taggers.

    

### [[2107.03930] Quantum belief function](http://arxiv.org/abs/2107.03930)


  The belief function in Dempster Shafer evidence theory can express more
information than the traditional Bayesian distribution. It is widely used in
approximate reasoning, decision-making and information fusion. However, its
power exponential explosion characteristics leads to the extremely high
computational complexity when handling large amounts of elements in classic
computers. In order to solve the problem, we encode the basic belief assignment
(BBA) into quantum states, which makes each qubit correspond to control an
element. Besides the high efficiency, this quantum expression is very conducive
to measure the similarity between two BBAs, and the measuring quantum algorithm
we come up with has exponential acceleration theoretically compared to the
corresponding classical algorithm. In addition, we simulate our quantum version
of BBA on Qiskit platform, which ensures the rationality of our algorithm
experimentally. We believe our results will shed some light on utilizing the
characteristic of quantum computation to handle belief function more
conveniently.

    

### [[2107.03959] Privacy Concerns in Chatbot Interactions: When to Trust and When to Worry](http://arxiv.org/abs/2107.03959)


  Through advances in their conversational abilities, chatbots have started to
request and process an increasing variety of sensitive personal information.
The accurate disclosure of sensitive information is essential where it is used
to provide advice and support to users in the healthcare and finance sectors.
In this study, we explore users' concerns regarding factors associated with the
use of sensitive data by chatbot providers. We surveyed a representative sample
of 491 British citizens. Our results show that the user concerns focus on
deleting personal information and concerns about their data's inappropriate
use. We also identified that individuals were concerned about losing control
over their data after a conversation with conversational agents. We found no
effect from a user's gender or education but did find an effect from the user's
age, with those over 45 being more concerned than those under 45. We also
considered the factors that engender trust in a chatbot. Our respondents'
primary focus was on the chatbot's technical elements, with factors such as the
response quality being identified as the most critical factor. We again found
no effect from the user's gender or education level; however, when we
considered some social factors (e.g. avatars or perceived 'friendliness'), we
found those under 45 years old rated these as more important than those over
45. The paper concludes with a discussion of these results within the context
of designing inclusive, digital systems that support a wide range of users.

    

### [[2107.03961] Computational Benefits of Intermediate Rewards for Hierarchical Planning](http://arxiv.org/abs/2107.03961)


  Many hierarchical reinforcement learning (RL) applications have empirically
verified that incorporating prior knowledge in reward design improves
convergence speed and practical performance. We attempt to quantify the
computational benefits of hierarchical RL from a planning perspective under
assumptions about the intermediate state and intermediate rewards frequently
(but often implicitly) adopted in practice. Our approach reveals a trade-off
between computational complexity and the pursuit of the shortest path in
hierarchical planning: using intermediate rewards significantly reduces the
computational complexity in finding a successful policy but does not guarantee
to find the shortest path, whereas using sparse terminal rewards finds the
shortest path at a significantly higher computational cost. We also corroborate
our theoretical results with extensive experiments on the MiniGrid environments
using Q-learning and other popular deep RL algorithms.

    

### [[2107.04007] Inspiration through Observation: Demonstrating the Influence of Automatically Generated Text on Creative Writing](http://arxiv.org/abs/2107.04007)


  Getting machines to generate text perceived as creative is a long-pursued
goal. A growing body of research directs this goal towards augmenting the
creative writing abilities of human authors. In this paper, we pursue this
objective by analyzing how observing examples of automatically generated text
influences writing. In particular, we examine a task referred to as sentence
infilling, which involves transforming a list of words into a complete
sentence. We emphasize "storiability" as a desirable feature of the resulting
sentences, where "storiable" sentences are those that suggest a story a reader
would be curious to hear about. Both humans and an automated system (based on a
neural language model) performed this sentence infilling task. In one setting,
people wrote sentences on their own; in a different setting, people observed
the sentences produced by the model while writing their own sentences. Readers
then assigned storiability preferences to the resulting sentences in a
subsequent evaluation. We find that human-authored sentences were judged as
more storiable when authors observed the generated examples, and that
storiability increased as authors derived more semantic content from the
examples. This result gives evidence of an "inspiration through observation"
paradigm for human-computer collaborative writing, through which human writing
can be enhanced by text generation models without directly copying their
output.

    

### [[2107.04026] CVEH: A Dynamic Framework To Profile Vehicle Movements To Mitigate Hit And Run Cases Using Crowdsourcing](http://arxiv.org/abs/2107.04026)


  In developed countries like the USA, Germany, and the UK, the security forces
used highly sophisticated equipment, fast vehicles, drones, and helicopters to
catch offenders' vehicles. Whereas, in developing countries with limited
resources such schemes cannot be utilized due to management cost and other
constraints. In this paper, we proposed a framework called CVEH that enables
developing countries to profile the offender vehicle movements through
crowdsourcing technique and act as an early warning system to the law forcing
agencies. It also engages citizens to play their role in improving security
conditions. The proposed CVEH framework allows Vehicle-to-Infrastructure (V2I)
communication to monitor the movement of the offender's vehicle and shared its
information with the Command and Control (CC) centre. The CC centre projects
the path and engages nearly located law enforcement agencies. CVEH is developed
and evaluated on android smartphones. Simulations conducted for this study
exhibit the effectiveness of our framework.

    

### [[2002.09811] Learning Interpretable Error Functions for Combinatorial Optimization Problem Modeling](http://arxiv.org/abs/2002.09811)


  In Constraint Programming, constraints are usually represented as predicates
allowing or forbidding combinations of values. However, some algorithms exploit
a finer representation: error functions. Their usage comes with a price though:
it makes problem modeling significantly harder. Here, we propose a method to
automatically learn an error function corresponding to a constraint, given a
function deciding if assignments are valid or not. This is, to the best of our
knowledge, the first attempt to automatically learn error functions for hard
constraints. Our method uses a variant of neural networks we named
Interpretable Compositional Networks, allowing us to get interpretable results,
unlike regular artificial neural networks. Experiments on 5 different
constraints show that our system can learn functions that scale to high
dimensions, and can learn fairly good functions over incomplete spaces.

    

### [[2005.07648] Language Conditioned Imitation Learning over Unstructured Data](http://arxiv.org/abs/2005.07648)


  Natural language is perhaps the most flexible and intuitive way for humans to
communicate tasks to a robot. Prior work in imitation learning typically
requires each task be specified with a task id or goal image -- something that
is often impractical in open-world environments. On the other hand, previous
approaches in instruction following allow agent behavior to be guided by
language, but typically assume structure in the observations, actuators, or
language that limit their applicability to complex settings like robotics. In
this work, we present a method for incorporating free-form natural language
conditioning into imitation learning. Our approach learns perception from
pixels, natural language understanding, and multitask continuous control
end-to-end as a single neural network. Unlike prior work in imitation learning,
our method is able to incorporate unlabeled and unstructured demonstration data
(i.e. no task or language labels). We show this dramatically improves language
conditioned performance, while reducing the cost of language annotation to less
than 1% of total data. At test time, a single language conditioned visuomotor
policy trained with our method can perform a wide variety of robotic
manipulation skills in a 3D environment, specified only with natural language
descriptions of each task (e.g. "open the drawer...now pick up the block...now
press the green button..."). To scale up the number of instructions an agent
can follow, we propose combining text conditioned policies with large
pretrained neural language models. We find this allows a policy to be robust to
many out-of-distribution synonym instructions, without requiring new
demonstrations. See videos of a human typing live text commands to our agent at
this http URL


### [[2006.16709] A Survey on Recent Progress in the Theory of Evolutionary Algorithms for Discrete Optimization](http://arxiv.org/abs/2006.16709)


  The theory of evolutionary computation for discrete search spaces has made
significant progress in the last ten years. This survey summarizes some of the
most important recent results in this research area. It discusses fine-grained
models of runtime analysis of evolutionary algorithms, highlights recent
theoretical insights on parameter tuning and parameter control, and summarizes
the latest advances for stochastic and dynamic problems. We regard how
evolutionary algorithms optimize submodular functions and we give an overview
over the large body of recent results on estimation of distribution algorithms.
Finally, we present the state of the art of drift analysis, one of the most
powerful analysis technique developed in this field.

    

### [[2102.07277] Anomaly Detection for Scenario-based Insider Activities using CGAN Augmented Data](http://arxiv.org/abs/2102.07277)


  Insider threats are the cyber attacks from within the trusted entities of an
organization. Lack of real-world data and issue of data imbalance leave insider
threat analysis an understudied research area. To mitigate the effect of skewed
class distribution and prove the potential of multinomial classification
algorithms for insider threat detection, we propose an approach that combines
generative model with supervised learning to perform multi-class classification
using deep learning. The generative adversarial network (GAN) based insider
detection model introduces Conditional Generative Adversarial Network (CGAN) to
enrich minority class samples to provide data for multi-class anomaly
detection. The comprehensive experiments performed on the benchmark dataset
demonstrates the effectiveness of introducing GAN derived synthetic data and
the capability of multi-class anomaly detection in insider activity analysis.
Moreover, the method is compared with other existing methods against different
parameters and performance metrics.

    

### [[2103.00923] Anticipation Next -- System-sensitive technology development and integration in work contexts](http://arxiv.org/abs/2103.00923)


  When discussing future concerns within socio-technical systems in work
contexts, we often find descriptions of missed technology development and
integration. The experience of technology that fails whilst being integrated is
often rooted in dysfunctional epistemological approaches within the research
and development process. Thus, ultimately leading to sustainable
technology-distrust in work contexts. This is true for organizations that
integrate new technologies and for organizations that invent them.
Organizations in which we find failed technology development and integrations
are, in their very nature, social systems. Nowadays, those complex social
systems act within an even more complex environment. This urges the development
of new anticipation methods for technology development and integration.
Gathering of and dealing with complex information in the described context is
what we call Anticipation Next. This explorative work uses existing literature
from the adjoining research fields of system theory, organizational theory, and
socio-technical research to combine various concepts. We deliberately aim at a
networked way of thinking in scientific contexts and thus combine
multidisciplinary subject areas in one paper to present an innovative way to
deal with multi-faceted problems in a human-centred way. We end with suggesting
a conceptual framework that should be used in the very early stages of
technology development and integration in work contexts.

    

### [[2105.04250] Expressing and Exploiting the Common Subgoal Structure of Classical Planning Domains Using Sketches: Extended Version](http://arxiv.org/abs/2105.04250)


  Width-based planning methods deal with conjunctive goals by decomposing
problems into subproblems of low width. Algorithms like SIW thus fail when the
goal is not easily serializable in this way or when some of the subproblems
have a high width. In this work, we address these limitations by using a simple
but powerful language for expressing finer problem decompositions introduced
recently by Bonet and Geffner, called policy sketches. A policy sketch over a
set of Boolean and numerical features is a set of sketch rules that express how
the values of these features are supposed to change. Like general policies,
policy sketches are domain general, but unlike policies, the changes captured
by sketch rules do not need to be achieved in a single step. We show that many
planning domains that cannot be solved by SIW are provably solvable in low
polynomial time with the SIW_R algorithm, the version of SIW that employs
user-provided policy sketches. Policy sketches are thus shown to be a powerful
language for expressing domain-specific knowledge in a simple and compact way
and a convenient alternative to languages such as HTNs or temporal logics.
Furthermore, they make it easy to express general problem decompositions and
prove key properties of them like their width and complexity.

    

### [[2105.11071] Alternating Fixpoint Operator for Hybrid MKNF Knowledge Bases as an Approximator of AFT](http://arxiv.org/abs/2105.11071)


  Approximation fixpoint theory (AFT) provides an algebraic framework for the
study of fixpoints of operators on bilattices and has found its applications in
characterizing semantics for various classes of logic programs and nonmonotonic
languages. In this paper, we show one more application of this kind: the
alternating fixpoint operator by Knorr et al. for the study of the well-founded
semantics for hybrid MKNF knowledge bases is in fact an approximator of AFT in
disguise, which, thanks to the power of abstraction of AFT, characterizes not
only the well-founded semantics but also two-valued as well as three-valued
semantics for hybrid MKNF knowledge bases. Furthermore, we show an improved
approximator for these knowledge bases, of which the least stable fixpoint is
information richer than the one formulated from Knorr et al.'s construction.
This leads to an improved computation for the well-founded semantics. This work
is built on an extension of AFT that supports consistent as well as
inconsistent pairs in the induced product bilattice, to deal with
inconsistencies that arise in the context of hybrid MKNF knowledge bases. This
part of the work can be considered generalizing the original AFT from symmetric
approximators to arbitrary approximators.

    

### [[2106.09212] Long-Short Temporal Contrastive Learning of Video Transformers](http://arxiv.org/abs/2106.09212)


  Video transformers have recently emerged as a competitive alternative to 3D
CNNs for video understanding. However, due to their large number of parameters
and reduced inductive biases, these models require supervised pretraining on
large-scale image datasets to achieve top performance. In this paper, we
empirically demonstrate that self-supervised pretraining of video transformers
on video-only datasets can lead to action recognition results that are on par
or better than those obtained with supervised pretraining on large-scale image
datasets, even massive ones such as ImageNet-21K. Since transformer-based
models are effective at capturing dependencies over extended temporal spans, we
propose a simple learning procedure that forces the model to match a long-term
view to a short-term view of the same video. Our approach, named Long-Short
Temporal Contrastive Learning (LSTCL), enables video transformers to learn an
effective clip-level representation by predicting temporal context captured
from a longer temporal extent. To demonstrate the generality of our findings,
we implement and validate our approach under three different self-supervised
contrastive learning frameworks (MoCo v3, BYOL, SimSiam) using two distinct
video-transformer architectures, including an improved variant of the Swin
Transformer augmented with space-time attention. We conduct a thorough ablation
study and show that LSTCL achieves competitive performance on multiple video
benchmarks and represents a convincing alternative to supervised image-based
pretraining.

    

### [[2107.03569] Dynamic Data-Race Detection through the Fine-Grained Lens](http://arxiv.org/abs/2107.03569)


  Data races are among the most common bugs in concurrency. The standard
approach to data-race detection is via dynamic analyses, which work over
executions of concurrent programs, instead of the program source code. The rich
literature on the topic has created various notions of dynamic data races,
which are known to be detected efficiently when certain parameters (e.g.,
number of threads) are small. However, the \emph{fine-grained} complexity of
all these notions of races has remained elusive, making it impossible to
characterize their trade-offs between precision and efficiency.
In this work we establish several fine-grained separations between many
popular notions of dynamic data races. The input is an execution trace with $N$
events, $T$ threads and $L$ locks. Our main results are as follows. First, we
show that happens-before (HB) races can be detected in $O(N\cdot \min(T, L))$
time, improving over the standard $O(N\cdot T)$ bound when $L=o(T)$. Moreover,
we show that even reporting an HB race that involves a read access is hard for
2-orthogonal vectors (2-OV). This is the first rigorous proof of the
conjectured quadratic lower-bound in detecting HB races. Second, we show that
the recently introduced synchronization-preserving races are hard to detect for
OV-3 and thus have a cubic lower bound, when $T=\Omega(N)$. This establishes a
complexity separation from HB races which are known to be less expressive.
Third, we show that lock-cover races are hard for 2-OV, and thus have a
quadratic lower-bound, even when $T=2$ and $L = \omega(\log N)$. The similar
notion of lock-set races is known to be detectable in $O(N\cdot L)$ time, and
thus we achieve a complexity separation between the two. Moreover, we show that
lock-set races become hitting-set (HS)-hard when $L=\Theta(N)$, and thus also
have a quadratic lower bound, when the input is sufficiently complex.

    

### [[2107.03660] Duplicate-sensitivity Guided Transformation Synthesis for DBMS Correctness Bug Detection](http://arxiv.org/abs/2107.03660)


  Database Management System (DBMS) plays a core role in modern software from
mobile apps to online banking. It is critical that DBMS should provide correct
data to all applications. When the DBMS returns incorrect data, a correctness
bug is triggered. Current production-level DBMSs still suffer from insufficient
testing due to the limited hand-written test cases. Recently several works
proposed to automatically generate many test cases with query transformation, a
process of generating an equivalent query pair and testing a DBMS by checking
whether the system returns the same result set for both queries. However, all
of them still heavily rely on manual work to provide a transformation which
largely confines their exploration of the valid input query space.
This paper introduces duplicate-sensitivity guided transformation synthesis
which automatically finds new transformations by first synthesizing many
candidates then filtering the nonequivalent ones. Our automated synthesis is
achieved by mutating a query while keeping its duplicate sensitivity, which is
a necessary condition for query equivalence. After candidate synthesis, we keep
the mutant query which is equivalent to the given one by using a query
equivalent checker. Furthermore, we have implemented our idea in a tool Eqsql
and used it to test the production-level DBMSs. In two months, we detected in
total 30 newly confirmed and unique bugs in MySQL, TiDB and CynosDB.

    

### [[2107.03984] Generalising Projection in Asynchronous Multiparty Session Types](http://arxiv.org/abs/2107.03984)


  Multiparty session types (MSTs) provide an efficient methodology for
specifying and verifying message passing software systems. In the theory of
MSTs, a global type specifies the interaction among the roles at the global
level. A local specification for each role is generated by projecting from the
global type on to the message exchanges it participates in. Whenever a global
type can be projected on to each role, the composition of the projections is
deadlock free and has exactly the behaviours specified by the global type. The
key to the usability of MSTs is the projection operation: a more expressive
projection allows more systems to be type-checked but requires a more difficult
soundness argument. In this paper, we generalise the standard projection
operation in MSTs. This allows us to model and type-check many design patterns
in distributed systems, such as load balancing, that are rejected by the
standard projection. The key to the new projection is an analysis that tracks
causality between messages. Our soundness proof uses novel graph-theoretic
techniques from the theory of message-sequence charts. We demonstrate the
efficacy of the new projection operation by showing many global types for
common patterns that can be projected under our projection but not under the
standard projection operation.

    