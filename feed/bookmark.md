
## 2021-9-6

### [<title>Save and Load model in XGBoost4j with Databricks DBFS - XGBoost</title>](https://discuss.xgboost.ai/t/save-and-load-model-in-xgboost4j-with-databricks-dbfs/1643/14)

### [[2109.01187] Hosting Industry Centralization and Consolidation](http://arxiv.org/abs/2109.01187)


  There have been growing concerns about the concentration and centralization
of Internet infrastructure. In this work, we scrutinize the hosting industry on
the Internet by using active measurements covering 19 Top-Level Domains~(TLDs).
We show how the market is heavily concentrated: 1/3 of the domains are hosted
by only 5 hosting providers, all US based companies. For the country-code
TLDs~(ccTLDs), however, hosting is primarily done by local, national hosting
providers and not by the large American cloud and content providers. We show
how a shared language (and borders) shapes the hosting market -- German hosting
companies have a notable presence in Austria and Switzerland markets, given
they all share German as official language. While hosting concentration has
been relatively high and stable over the past four years, we see that American
hosting companies have been continuously increase their presence in the market
related with high traffic, popular domains within ccTLDs -- except for Russia,
notably.

    

### [[2109.01290] Recursive Periodicity Shifting for Semi-Persistent Scheduling of Time-Sensitive Communication in 5G](http://arxiv.org/abs/2109.01290)


  Various legacy and emerging industrial control applications create the
requirement of periodic and time-sensitive communication (TSC) for 5G/6G
networks. State-of-the-art semi-persistent scheduling (SPS) techniques fall
short of meeting the requirements of this type of critical traffic due to
periodicity misalignment between assignments and arriving packets that lead to
significant waiting delays. To tackle this challenge, we develop a novel
recursive periodicity shifting (RPS)-SPS scheme that provides an optimal
scheduling policy by recursively aligning the period of assignments until the
timing mismatch is minimized. RPS can be realized in 5G wireless networks with
minimal modifications to the scheduling framework. Performance evaluation shows
the effectiveness of the proposed scheme in terms of minimizing misalignment
delay with arbitrary traffic periodicity.

    

### [[2109.01379] Enabling Reproducible Analysis of Complex Workflows on the Edge-to-Cloud Continuum](http://arxiv.org/abs/2109.01379)


  Distributed digital infrastructures for computation and analytics are now
evolving towards an interconnected ecosystem allowing complex applications to
be executed from IoT Edge devices to the HPC Cloud (aka the Computing
Continuum, the Digital Continuum, or the Transcontinuum). Understanding
end-to-end performance in such a complex continuum is challenging. This breaks
down to reconciling many, typically contradicting application requirements and
constraints with low-level infrastructure design choices. One important
challenge is to accurately reproduce relevant behaviors of a given application
workflow and representative settings of the physical infrastructure underlying
this complex continuum. We introduce a rigorous methodology for such a process
and validate it through E2Clab. It is the first platform to support the
complete experimental cycle across the Computing Continuum: deployment,
analysis, optimization. Preliminary results with real-life use cases show that
E2Clab allows one to understand and improve performance, by correlating it to
the parameter settings, the resource usage and the specifics of the underlying
infrastructure.

    

### [[2109.01445] Is Machine Learning Ready for Traffic Engineering Optimization?](http://arxiv.org/abs/2109.01445)


  Traffic Engineering (TE) is a basic building block of the Internet. In this
paper, we analyze whether modern Machine Learning (ML) methods are ready to be
used for TE optimization. We address this open question through a comparative
analysis between the state of the art in ML and the state of the art in TE. To
this end, we first present a novel distributed system for TE that leverages the
latest advancements in ML. Our system implements a novel architecture that
combines Multi-Agent Reinforcement Learning (MARL) and Graph Neural Networks
(GNN) to minimize network congestion. In our evaluation, we compare our
MARL+GNN system with DEFO, a network optimizer based on Constraint Programming
that represents the state of the art in TE. Our experimental results show that
the proposed MARL+GNN solution achieves equivalent performance to DEFO in a
wide variety of network scenarios including three real-world network
topologies. At the same time, we show that MARL+GNN can achieve significant
reductions in execution time (from the scale of minutes with DEFO to a few
seconds with our solution).

    

### [[2109.01465] Challenge: A Cost and Power Feasibility Analysis of Quantum Annealing for NextG Cellular Wireless Networks](http://arxiv.org/abs/2109.01465)


  In order to meet mobile cellular users' ever-increasing network usage,
today's 4G and 5G networks are designed mainly with the goal of maximizing
spectral efficiency. While they have made progress in this regard, controlling
the carbon footprint and operational costs of such networks remains a
long-standing problem among network designers. This Challenge paper takes a
long view on this problem, envisioning a NextG scenario where the network
leverages quantum annealing computation for cellular baseband processing. We
gather and synthesize insights on power consumption, computational throughput
and latency, spectral efficiency, and operational cost, and deployment
timelines surrounding quantum technology. Armed with these data, we analyze and
project the quantitative performance targets future quantum hardware must meet
in order to provide a computational and power advantage over silicon hardware,
while matching its whole-network spectral efficiency. Our quantitative analysis
predicts that with quantum hardware operating at a 140 $\mu$s problem latency
and 4.3M qubits, quantum computation will achieve a spectral efficiency equal
to silicon while reducing power consumption by 40.8 kW (45% lower) in a
representative 5G base station scenario with 400 MHz bandwidth and 64 antennas,
and an 8 kW power reduction (16% lower) using 2.2M qubits in a 200
MHz-bandwidth 5G scenario.

    

### [[2109.01480] FedApp: a Research Sandbox for Application Orchestration in Federated Clouds using OpenStack](http://arxiv.org/abs/2109.01480)


  Multi-cluster federation is envisioned to be the next-generation cloud
infrastructure, where it will play a vital part in the realization of concepts
such as edge and fog computing. Orchestrating applications in federated
environments poses new challenges to well-known research problems in various
fields, such as load-balancing, auto-scaling, resource allocation and service
migration. However, as access to real multi-cluster infrastructure is limited,
a test-bed that provides similar characteristics to a real system is in demand.
To enable researchers in associated fields to quickly setup experiments in a
federated cloud environment, we have created the open-source sandbox FedApp
that simplifies the process of deploying multiple virtual clusters in an
OpenStack environment with the possibility of adding realistic network
characteristics between sites. Each cluster comes deployed with the open-source
and production-grade container orchestrator Kubernetes, complete with
federation-wide monitoring using Prometheus/Grafana and simplified
inter-cluster microservice communication using Istio.

    

### [[2109.01608] The Promise and Challenges of Computation Deduplication and Reuse at the Network Edge](http://arxiv.org/abs/2109.01608)


  In edge computing environments, where devices may be in close proximity to
each other, these devices may offload similar computational tasks (i.e., tasks
with similar input data for the same edge computing service or for services of
the same nature). This results in the execution of duplicate (redundant)
computation, which may become a pressing issue for future edge computing
deployments, since such deployments are envisioned to consist of small-scale
data-centers at the edge. To tackle this issue, in this paper, we highlight the
importance of paradigms for the deduplication and reuse of computation at the
network edge. Such paradigms have the potential to significantly reduce the
completion times for offloaded tasks, accommodating more users, devices, and
tasks with the same volume of deployed edge computing resources, however, they
come with their own technical challenges. Finally, we present a multi-layer
architecture to enable computation deduplication and reuse at the network edge
and discuss open challenges and future research directions.

    

### [[2109.01164] Scalable Data Annotation Pipeline for High-Quality Large Speech Datasets Development](http://arxiv.org/abs/2109.01164)


  This paper introduces a human-in-the-loop (HITL) data annotation pipeline to
generate high-quality, large-scale speech datasets. The pipeline combines human
and machine advantages to more quickly, accurately, and cost-effectively
annotate datasets with machine pre-labeling and fully manual auditing. Quality
control mechanisms such as blind testing, behavior monitoring, and data
validation have been adopted in the annotation pipeline to mitigate potential
bias introduced by machine-generated labels. Our A/B testing and pilot results
demonstrated the HITL pipeline can improve annotation speed and capacity by at
least 80% and quality is comparable to or higher than manual double pass
annotation. We are leveraging this scalable pipeline to create and continuously
grow ultra-high volume off-the-shelf (UHV-OTS) speech corpora for multiple
languages, with the capability to expand to 10,000+ hours per language
annually. Customized datasets can be produced from the UHV-OTS corpora using
dynamic packaging. UHV-OTS is a long-term Appen project to support commercial
and academic research data needs in speech processing. Appen will donate a
number of free speech datasets from the UHV-OTS each year to support academic
and open source community research under the CC-BY-SA license. We are also
releasing the code of the data pre-processing and pre-tagging pipeline under
the Apache 2.0 license to allow reproduction of the results reported in the
paper.

    

### [[2109.01178] Multi-Agent Inverse Reinforcement Learning: Suboptimal Demonstrations and Alternative Solution Concepts](http://arxiv.org/abs/2109.01178)


  Multi-agent inverse reinforcement learning (MIRL) can be used to learn reward
functions from agents in social environments. To model realistic social
dynamics, MIRL methods must account for suboptimal human reasoning and
behavior. Traditional formalisms of game theory provide computationally
tractable behavioral models, but assume agents have unrealistic cognitive
capabilities. This research identifies and compares mechanisms in MIRL methods
which a) handle noise, biases and heuristics in agent decision making and b)
model realistic equilibrium solution concepts. MIRL research is systematically
reviewed to identify solutions for these challenges. The methods and results of
these studies are analyzed and compared based on factors including performance
accuracy, efficiency, and descriptive quality. We found that the primary
methods for handling noise, biases and heuristics in MIRL were extensions of
Maximum Entropy (MaxEnt) IRL to multi-agent settings. We also found that many
successful solution concepts are generalizations of the traditional Nash
Equilibrium (NE). These solutions include the correlated equilibrium, logistic
stochastic best response equilibrium and entropy regularized mean field NE.
Methods which use recursive reasoning or updating also perform well, including
the feedback NE and archive multi-agent adversarial IRL. Success in modeling
specific biases and heuristics in single-agent IRL and promising results using
a Theory of Mind approach in MIRL imply that modeling specific biases and
heuristics may be useful. Flexibility and unbiased inference in the identified
alternative solution concepts suggest that a solution concept which has both
recursive and generalized characteristics may perform well at modeling
realistic social interactions.

    

### [[2109.01226] So Cloze yet so Far: N400 Amplitude is Better Predicted by Distributional Information than Human Predictability Judgements](http://arxiv.org/abs/2109.01226)


  More predictable words are easier to process - they are read faster and
elicit smaller neural signals associated with processing difficulty, most
notably, the N400 component of the event-related brain potential. Thus, it has
been argued that prediction of upcoming words is a key component of language
comprehension, and that studying the amplitude of the N400 is a valuable way to
investigate the predictions that we make. In this study, we investigate whether
the linguistic predictions of computational language models or humans better
reflect the way in which natural language stimuli modulate the amplitude of the
N400. One important difference in the linguistic predictions of humans versus
computational language models is that while language models base their
predictions exclusively on the preceding linguistic context, humans may rely on
other factors. We find that the predictions of three top-of-the-line
contemporary language models - GPT-3, RoBERTa, and ALBERT - match the N400 more
closely than human predictions. This suggests that the predictive processes
underlying the N400 may be more sensitive to the surface-level statistics of
language than previously thought.

    

### [[2109.01229] Multimodal Conditionality for Natural Language Generation](http://arxiv.org/abs/2109.01229)


  Large scale pretrained language models have demonstrated state-of-the-art
performance in language understanding tasks. Their application has recently
expanded into multimodality learning, leading to improved representations
combining vision and language. However, progress in adapting language models
towards conditional Natural Language Generation (NLG) has been limited to a
single modality, generally text. We propose MAnTiS, Multimodal Adaptation for
Text Synthesis, a general approach for multimodal conditionality in
transformer-based NLG models. In this method, we pass inputs from each modality
through modality-specific encoders, project to textual token space, and finally
join to form a conditionality prefix. We fine-tune the pretrained language
model and encoders with the conditionality prefix guiding the generation. We
apply MAnTiS to the task of product description generation, conditioning a
network on both product images and titles to generate descriptive text. We
demonstrate that MAnTiS outperforms strong baseline approaches on standard NLG
scoring metrics. Furthermore, qualitative assessments demonstrate that MAnTiS
can generate human quality descriptions consistent with given multimodal
inputs.

    

### [[2109.01242] Entity Linking and Discovery via Arborescence-based Supervised Clustering](http://arxiv.org/abs/2109.01242)


  Previous work has shown promising results in performing entity linking by
measuring not only the affinities between mentions and entities but also those
amongst mentions. In this paper, we present novel training and inference
procedures that fully utilize mention-to-mention affinities by building minimum
arborescences (i.e., directed spanning trees) over mentions and entities across
documents in order to make linking decisions. We also show that this method
gracefully extends to entity discovery, enabling the clustering of mentions
that do not have an associated entity in the knowledge base. We evaluate our
approach on the Zero-Shot Entity Linking dataset and MedMentions, the largest
publicly available biomedical dataset, and show significant improvements in
performance for both entity linking and discovery compared to identically
parameterized models. We further show significant efficiency improvements with
only a small loss in accuracy over previous work, which use more
computationally expensive models.

    

### [[2109.01246] Two Shifts for Crop Mapping: Leveraging Aggregate Crop Statistics to Improve Satellite-based Maps in New Regions](http://arxiv.org/abs/2109.01246)


  Crop type mapping at the field level is critical for a variety of
applications in agricultural monitoring, and satellite imagery is becoming an
increasingly abundant and useful raw input from which to create crop type maps.
Still, in many regions crop type mapping with satellite data remains
constrained by a scarcity of field-level crop labels for training supervised
classification models. When training data is not available in one region,
classifiers trained in similar regions can be transferred, but shifts in the
distribution of crop types as well as transformations of the features between
regions lead to reduced classification accuracy. We present a methodology that
uses aggregate-level crop statistics to correct the classifier by accounting
for these two types of shifts. To adjust for shifts in the crop type
composition we present a scheme for properly reweighting the posterior
probabilities of each class that are output by the classifier. To adjust for
shifts in features we propose a method to estimate and remove linear shifts in
the mean feature vector. We demonstrate that this methodology leads to
substantial improvements in overall classification accuracy when using Linear
Discriminant Analysis (LDA) to map crop types in Occitanie, France and in
Western Province, Kenya. When using LDA as our base classifier, we found that
in France our methodology led to percent reductions in misclassifications
ranging from 2.8% to 42.2% (mean = 21.9%) over eleven different training
departments, and in Kenya the percent reductions in misclassification were
6.6%, 28.4%, and 42.7% for three training regions. While our methodology was
statistically motivated by the LDA classifier, it can be applied to any type of
classifier. As an example, we demonstrate its successful application to improve
a Random Forest classifier.

    

### [[2109.01255] Provably Safe Model-Based Meta Reinforcement Learning: An Abstraction-Based Approach](http://arxiv.org/abs/2109.01255)


  While conventional reinforcement learning focuses on designing agents that
can perform one task, meta-learning aims, instead, to solve the problem of
designing agents that can generalize to different tasks (e.g., environments,
obstacles, and goals) that were not considered during the design or the
training of these agents. In this spirit, in this paper, we consider the
problem of training a provably safe Neural Network (NN) controller for
uncertain nonlinear dynamical systems that can generalize to new tasks that
were not present in the training data while preserving strong safety
guarantees. Our approach is to learn a set of NN controllers during the
training phase. When the task becomes available at runtime, our framework will
carefully select a subset of these NN controllers and compose them to form the
final NN controller. Critical to our approach is the ability to compute a
finite-state abstraction of the nonlinear dynamical system. This abstract model
captures the behavior of the closed-loop system under all possible NN weights,
and is used to train the NNs and compose them when the task becomes available.
We provide theoretical guarantees that govern the correctness of the resulting
NN. We evaluated our approach on the problem of controlling a wheeled robot in
cluttered environments that were not present in the training data.

    

### [[2109.01258] Estimating Demand Flexibility Using Siamese LSTM Neural Networks](http://arxiv.org/abs/2109.01258)


  There is an opportunity in modern power systems to explore the demand
flexibility by incentivizing consumers with dynamic prices. In this paper, we
quantify demand flexibility using an efficient tool called time-varying
elasticity, whose value may change depending on the prices and decision
dynamics. This tool is particularly useful for evaluating the demand response
potential and system reliability. Recent empirical evidences have highlighted
some abnormal features when studying demand flexibility, such as delayed
responses and vanishing elasticities after price spikes. Existing methods fail
to capture these complicated features because they heavily rely on some
predefined (often over-simplified) regression expressions. Instead, this paper
proposes a model-free methodology to automatically and accurately derive the
optimal estimation pattern. We further develop a two-stage estimation process
with Siamese long short-term memory (LSTM) networks. Here, a LSTM network
encodes the price response, while the other network estimates the time-varying
elasticities. In the case study, the proposed framework and models are
validated to achieve higher overall estimation accuracy and better description
for various abnormal features when compared with the state-of-the-art methods.

    

### [[2109.01262] On the Accuracy of Analog Neural Network Inference Accelerators](http://arxiv.org/abs/2109.01262)


  Specialized accelerators have recently garnered attention as a method to
reduce the power consumption of neural network inference. A promising category
of accelerators utilizes nonvolatile memory arrays to both store weights and
perform $\textit{in situ}$ analog computation inside the array. While prior
work has explored the design space of analog accelerators to optimize
performance and energy efficiency, there is seldom a rigorous evaluation of the
accuracy of these accelerators. This work shows how architectural design
decisions, particularly in mapping neural network parameters to analog memory
cells, influence inference accuracy. When evaluated using ResNet50 on ImageNet,
the resilience of the system to analog non-idealities - cell programming
errors, analog-to-digital converter resolution, and array parasitic resistances
- all improve when analog quantities in the hardware are made proportional to
the weights in the network. Moreover, contrary to the assumptions of prior
work, nearly equivalent resilience to cell imprecision can be achieved by fully
storing weights as analog quantities, rather than spreading weight bits across
multiple devices, often referred to as bit slicing. By exploiting
proportionality, analog system designers have the freedom to match the
precision of the hardware to the needs of the algorithm, rather than attempting
to guarantee the same level of precision in the intermediate results as an
equivalent digital accelerator. This ultimately results in an analog
accelerator that is more accurate, more robust to analog errors, and more
energy-efficient.

    

### [[2109.01275] A Synergetic Attack against Neural Network Classifiers combining Backdoor and Adversarial Examples](http://arxiv.org/abs/2109.01275)


  In this work, we show how to jointly exploit adversarial perturbation and
model poisoning vulnerabilities to practically launch a new stealthy attack,
dubbed AdvTrojan. AdvTrojan is stealthy because it can be activated only when:
1) a carefully crafted adversarial perturbation is injected into the input
examples during inference, and 2) a Trojan backdoor is implanted during the
training process of the model. We leverage adversarial noise in the input space
to move Trojan-infected examples across the model decision boundary, making it
difficult to detect. The stealthiness behavior of AdvTrojan fools the users
into accidentally trust the infected model as a robust classifier against
adversarial examples. AdvTrojan can be implemented by only poisoning the
training data similar to conventional Trojan backdoor attacks. Our thorough
analysis and extensive experiments on several benchmark datasets show that
AdvTrojan can bypass existing defenses with a success rate close to 100% in
most of our experimental scenarios and can be extended to attack federated
learning tasks as well.

    

### [[2109.01300] How to Inject Backdoors with Better Consistency: Logit Anchoring on Clean Data](http://arxiv.org/abs/2109.01300)


  Since training a large-scale backdoored model from scratch requires a large
training dataset, several recent attacks have considered to inject backdoors
into a trained clean model without altering model behaviors on the clean data.
Previous work finds that backdoors can be injected into a trained clean model
with Adversarial Weight Perturbation (AWP). Here AWPs refers to the variations
of parameters that are small in backdoor learning. In this work, we observe an
interesting phenomenon that the variations of parameters are always AWPs when
tuning the trained clean model to inject backdoors. We further provide
theoretical analysis to explain this phenomenon. We formulate the behavior of
maintaining accuracy on clean data as the consistency of backdoored models,
which includes both global consistency and instance-wise consistency. We
extensively analyze the effects of AWPs on the consistency of backdoored
models. In order to achieve better consistency, we propose a novel anchoring
loss to anchor or freeze the model behaviors on the clean data, with a
theoretical guarantee. Both the analytical and the empirical results validate
the effectiveness of the anchoring loss in improving the consistency,
especially the instance-wise consistency.

    

### [[2109.01306] J-Score: A Robust Measure of Clustering Accuracy](http://arxiv.org/abs/2109.01306)


  Background. Clustering analysis discovers hidden structures in a data set by
partitioning them into disjoint clusters. Robust accuracy measures that
evaluate the goodness of clustering results are critical for algorithm
development and model diagnosis. Common problems of current clustering accuracy
measures include overlooking unmatched clusters, biases towards excessive
clusters, unstable baselines, and difficult interpretation. In this study, we
presented a novel accuracy measure, J-score, that addresses these issues.
Methods. Given a data set with known class labels, J-score quantifies how
well the hypothetical clusters produced by clustering analysis recover the true
classes. It starts with bidirectional set matching to identify the
correspondence between true classes and hypothetical clusters based on Jaccard
index. It then computes two weighted sums of Jaccard indices measuring the
reconciliation from classes to clusters and vice versa. The final J-score is
the harmonic mean of the two weighted sums.
Results. Via simulation studies, we evaluated the performance of J-score and
compared with existing measures. Our results show that J-score is effective in
distinguishing partition structures that differ only by unmatched clusters,
rewarding correct inference of class numbers, addressing biases towards
excessive clusters, and having a relatively stable baseline. The simplicity of
its calculation makes the interpretation straightforward. It is a valuable tool
complementary to other accuracy measures. We released an R/jScore package
implementing the algorithm.

    

### [[2109.01313] Characterization and Prediction of Deep Learning Workloads in Large-Scale GPU Datacenters](http://arxiv.org/abs/2109.01313)


  Modern GPU datacenters are critical for delivering Deep Learning (DL) models
and services in both the research community and industry. When operating a
datacenter, optimization of resource scheduling and management can bring
significant financial benefits. Achieving this goal requires a deep
understanding of the job features and user behaviors. We present a
comprehensive study about the characteristics of DL jobs and resource
management. First, we perform a large-scale analysis of real-world job traces
from SenseTime. We uncover some interesting conclusions from the perspectives
of clusters, jobs and users, which can facilitate the cluster system designs.
Second, we introduce a general-purpose framework, which manages resources based
on historical data. As case studies, we design: a Quasi-Shortest-Service-First
scheduling service, which can minimize the cluster-wide average job completion
time by up to 6.5x; and a Cluster Energy Saving service, which improves overall
cluster utilization by up to 13%.

    

### [[2109.01326] Statistical Estimation and Inference via Local SGD in Federated Learning](http://arxiv.org/abs/2109.01326)


  Federated Learning (FL) makes a large amount of edge computing devices (e.g.,
mobile phones) jointly learn a global model without data sharing. In FL, data
are generated in a decentralized manner with high heterogeneity. This paper
studies how to perform statistical estimation and inference in the federated
setting. We analyze the so-called Local SGD, a multi-round estimation procedure
that uses intermittent communication to improve communication efficiency. We
first establish a {\it functional central limit theorem} that shows the
averaged iterates of Local SGD weakly converge to a rescaled Brownian motion.
We next provide two iterative inference methods: the {\it plug-in} and the {\it
random scaling}. Random scaling constructs an asymptotically pivotal statistic
for inference by using the information along the whole Local SGD path. Both the
methods are communication efficient and applicable to online data. Our
theoretical and empirical results show that Local SGD simultaneously achieves
both statistical efficiency and communication efficiency.

    

### [[2109.01332] Access Control Using Spatially Invariant Permutation of Feature Maps for Semantic Segmentation Models](http://arxiv.org/abs/2109.01332)


  In this paper, we propose an access control method that uses the spatially
invariant permutation of feature maps with a secret key for protecting semantic
segmentation models. Segmentation models are trained and tested by permuting
selected feature maps with a secret key. The proposed method allows rightful
users with the correct key not only to access a model to full capacity but also
to degrade the performance for unauthorized users. Conventional access control
methods have focused only on image classification tasks, and these methods have
never been applied to semantic segmentation tasks. In an experiment, the
protected models were demonstrated to allow rightful users to obtain almost the
same performance as that of non-protected models but also to be robust against
access by unauthorized users without a key. In addition, a conventional method
with block-wise transformations was also verified to have degraded performance
under semantic segmentation models.

    

### [[2109.01348] Ground-Assisted Federated Learning in LEO Satellite Constellations](http://arxiv.org/abs/2109.01348)


  In Low Earth Orbit (LEO) mega constellations, there are relevant use cases,
such as inference based on satellite imaging, in which a large number of
satellites collaboratively train a machine learning model without sharing their
local data sets. To address this problem, we propose a new set of algorithms
based of Federated learning (FL). Our approach differs substantially from the
standard FL algorithms, as it takes into account the predictable connectivity
patterns that are immanent to the LEO constellations. Extensive numerical
evaluations highlight the fast convergence speed and excellent asymptotic test
accuracy of the proposed method. In particular, the achieved test accuracy is
within 96% to 99.6% of the centralized solution and the proposed algorithm has
less hyperparameters to tune than state-of-the-art asynchronous FL methods.

    

### [[2109.01356] Edge-featured Graph Neural Architecture Search](http://arxiv.org/abs/2109.01356)


  Graph neural networks (GNNs) have been successfully applied to learning
representation on graphs in many relational tasks. Recently, researchers study
neural architecture search (NAS) to reduce the dependence of human expertise
and explore better GNN architectures, but they over-emphasize entity features
and ignore latent relation information concealed in the edges. To solve this
problem, we incorporate edge features into graph search space and propose
Edge-featured Graph Neural Architecture Search to find the optimal GNN
architecture. Specifically, we design rich entity and edge updating operations
to learn high-order representations, which convey more generic message passing
mechanisms. Moreover, the architecture topology in our search space allows to
explore complex feature dependence of both entities and edges, which can be
efficiently optimized by differentiable search strategy. Experiments at three
graph tasks on six datasets show EGNAS can search better GNNs with higher
performance than current state-of-the-art human-designed and searched-based
GNNs.

    

### [[2109.01369] Instance-wise or Class-wise? A Tale of Neighbor Shapley for Concept-based Explanation](http://arxiv.org/abs/2109.01369)


  Deep neural networks have demonstrated remarkable performance in many
data-driven and prediction-oriented applications, and sometimes even perform
better than humans. However, their most significant drawback is the lack of
interpretability, which makes them less attractive in many real-world
applications. When relating to the moral problem or the environmental factors
that are uncertain such as crime judgment, financial analysis, and medical
diagnosis, it is essential to mine the evidence for the model's prediction
(interpret model knowledge) to convince humans. Thus, investigating how to
interpret model knowledge is of paramount importance for both academic research
and real applications.

    

### [[2109.01372] Sample Noise Impact on Active Learning](http://arxiv.org/abs/2109.01372)


  This work explores the effect of noisy sample selection in active learning
strategies. We show on both synthetic problems and real-life use-cases that
knowledge of the sample noise can significantly improve the performance of
active learning strategies. Building on prior work, we propose a robust
sampler, Incremental Weighted K-Means that brings significant improvement on
the synthetic tasks but only a marginal uplift on real-life ones. We hope that
the questions raised in this paper are of interest to the community and could
open new paths for active learning research.

    

### [[2109.01377] A Bayesian Approach to (Online) Transfer Learning: Theory and Algorithms](http://arxiv.org/abs/2109.01377)


  Transfer learning is a machine learning paradigm where knowledge from one
problem is utilized to solve a new but related problem. On the one hand, it is
conceivable that knowledge from one task could be useful for solving a related
task. On the other hand, it is also recognized that if not executed properly,
transfer learning algorithms can in fact impair the learning performance
instead of improving it - commonly known as negative transfer. In this paper,
we study transfer learning from a Bayesian perspective, where a parametric
statistical model is used. Specifically, we study three variants of transfer
learning problems, instantaneous, online, and time-variant transfer learning.
For each problem, we define an appropriate objective function, and provide
either exact expressions or upper bounds on the learning performance using
information-theoretic quantities, which allow simple and explicit
characterizations when the sample size becomes large. Furthermore, examples
show that the derived bounds are accurate even for small sample sizes. The
obtained bounds give valuable insights on the effect of prior knowledge for
transfer learning in our formulation. In particular, we formally characterize
the conditions under which negative transfer occurs. Lastly, we devise two
(online) transfer learning algorithms that are amenable to practical
implementations. Specifically, one algorithm does not require the parametric
assumption, thus extending our results to more general models. We demonstrate
the effectiveness of our algorithms with real data set, especially when the
source and target data have a strong similarity.

    

### [[2109.01381] Segmentation of turbulent computational fluid dynamics simulations with unsupervised ensemble learning](http://arxiv.org/abs/2109.01381)


  Computer vision and machine learning tools offer an exciting new way for
automatically analyzing and categorizing information from complex computer
simulations. Here we design an ensemble machine learning framework that can
independently and robustly categorize and dissect simulation data output
contents of turbulent flow patterns into distinct structure catalogues. The
segmentation is performed using an unsupervised clustering algorithm, which
segments physical structures by grouping together similar pixels in simulation
images. The accuracy and robustness of the resulting segment region boundaries
are enhanced by combining information from multiple simultaneously-evaluated
clustering operations. The stacking of object segmentation evaluations is
performed using image mask combination operations. This statistically-combined
ensemble (SCE) of different cluster masks allows us to construct cluster
reliability metrics for each pixel and for the associated segments without any
prior user input. By comparing the similarity of different cluster occurrences
in the ensemble, we can also assess the optimal number of clusters needed to
describe the data. Furthermore, by relying on ensemble-averaged spatial segment
region boundaries, the SCE method enables reconstruction of more accurate and
robust region of interest (ROI) boundaries for the different image data
clusters. We apply the SCE algorithm to 2-dimensional simulation data snapshots
of magnetically-dominated fully-kinetic turbulent plasma flows where accurate
ROI boundaries are needed for geometrical measurements of intermittent flow
structures known as current sheets.

    

### [[2109.01394] Topographic VAEs learn Equivariant Capsules](http://arxiv.org/abs/2109.01394)


  In this work we seek to bridge the concepts of topographic organization and
equivariance in neural networks. To accomplish this, we introduce the
Topographic VAE: a novel method for efficiently training deep generative models
with topographically organized latent variables. We show that such a model
indeed learns to organize its activations according to salient characteristics
such as digit class, width, and style on MNIST. Furthermore, through
topographic organization over time (i.e. temporal coherence), we demonstrate
how predefined latent space transformation operators can be encouraged for
observed transformed input sequences -- a primitive form of unsupervised
learned equivariance. We demonstrate that this model successfully learns sets
of approximately equivariant features (i.e. "capsules") directly from sequences
and achieves higher likelihood on correspondingly transforming test sequences.
Equivariance is verified quantitatively by measuring the approximate
commutativity of the inference network and the sequence transformations.
Finally, we demonstrate approximate equivariance to complex transformations,
expanding upon the capabilities of existing group equivariant neural networks.

    

### [[2109.01401] CX-ToM: Counterfactual Explanations with Theory-of-Mind for Enhancing Human Trust in Image Recognition Models](http://arxiv.org/abs/2109.01401)


  We propose CX-ToM, short for counterfactual explanations with theory-of mind,
a new explainable AI (XAI) framework for explaining decisions made by a deep
convolutional neural network (CNN). In contrast to the current methods in XAI
that generate explanations as a single shot response, we pose explanation as an
iterative communication process, i.e. dialog, between the machine and human
user. More concretely, our CX-ToM framework generates sequence of explanations
in a dialog by mediating the differences between the minds of machine and human
user. To do this, we use Theory of Mind (ToM) which helps us in explicitly
modeling human's intention, machine's mind as inferred by the human as well as
human's mind as inferred by the machine. Moreover, most state-of-the-art XAI
frameworks provide attention (or heat map) based explanations. In our work, we
show that these attention based explanations are not sufficient for increasing
human trust in the underlying CNN model. In CX-ToM, we instead use
counterfactual explanations called fault-lines which we define as follows:
given an input image I for which a CNN classification model M predicts class
c_pred, a fault-line identifies the minimal semantic-level features (e.g.,
stripes on zebra, pointed ears of dog), referred to as explainable concepts,
that need to be added to or deleted from I in order to alter the classification
category of I by M to another specified class c_alt. We argue that, due to the
iterative, conceptual and counterfactual nature of CX-ToM explanations, our
framework is practical and more natural for both expert and non-expert users to
understand the internal workings of complex deep learning models. Extensive
quantitative and qualitative experiments verify our hypotheses, demonstrating
that our CX-ToM significantly outperforms the state-of-the-art explainable AI
models.

    

### [[2109.01413] Frequency-Severity Experience Rating based on Latent Markovian Risk Profiles](http://arxiv.org/abs/2109.01413)


  Bonus-Malus Systems traditionally consider a customer's number of claims
irrespective of their sizes, even though these components are dependent in
practice. We propose a novel joint experience rating approach based on latent
Markovian risk profiles to allow for a positive or negative individual
frequency-severity dependence. The latent profiles evolve over time in a Hidden
Markov Model to capture updates in a customer's claims experience, making claim
counts and sizes conditionally independent. We show that the resulting risk
premia lead to a dynamic, claims experience-weighted mixture of standard
credibility premia. The proposed approach is applied to a Dutch automobile
insurance portfolio and identifies customer risk profiles with distinctive
claiming behavior. These profiles, in turn, enable us to better distinguish
between customer risks.

    

### [[2109.01417] Efficient Communication in Multi-Agent Distributed Reinforcement Learning](http://arxiv.org/abs/2109.01417)


  We present in this work an approach to reduce the communication of
information needed on a multi-agent learning system inspired by Event Triggered
Control (ETC) techniques. We consider a baseline scenario of a distributed
Q-learning problem on a Markov Decision Process (MDP). Following an event-based
approach, N agents explore the MDP and communicate experiences to a central
learner only when necessary, which performs updates of the actor Q functions.
We analyse the convergence guarantees retained with respect to a regular
Q-learning algorithm, and present experimental results showing that event-based
communication results in a substantial reduction of data transmission rates in
such distributed systems. Additionally, we discuss what effects (desired and
undesired) these event-based approaches have on the learning processes studied,
and how they can be applied to more complex multi-agent learning systems.

    

### [[2109.01419] Building Interpretable Models for Business Process Prediction using Shared and Specialised Attention Mechanisms](http://arxiv.org/abs/2109.01419)


  In this paper, we address the "black-box" problem in predictive process
analytics by building interpretable models that are capable to inform both what
and why is a prediction. Predictive process analytics is a newly emerged
discipline dedicated to providing business process intelligence in modern
organisations. It uses event logs, which capture process execution traces in
the form of multi-dimensional sequence data, as the key input to train
predictive models. These predictive models, often built upon deep learning
techniques, can be used to make predictions about the future states of business
process execution. We apply attention mechanism to achieve model
interpretability. We propose i) two types of attentions: event attention to
capture the impact of specific process events on a prediction, and attribute
attention to reveal which attribute(s) of an event influenced the prediction;
and ii) two attention mechanisms: shared attention mechanism and specialised
attention mechanism to reflect different design decisions in when to construct
attribute attention on individual input features (specialised) or using the
concatenated feature tensor of all input feature vectors (shared). These lead
to two distinct attention-based models, and both are interpretable models that
incorporate interpretability directly into the structure of a process
predictive model. We conduct experimental evaluation of the proposed models
using real-life dataset, and comparative analysis between the models for
accuracy and interpretability, and draw insights from the evaluation and
analysis results.

    

### [[2109.01425] A New Approach to Multilabel Stratified Cross Validation with Application to Large and Sparse Gene Ontology Datasets](http://arxiv.org/abs/2109.01425)


  Multilabel learning is an important topic in machine learning research.
Evaluating models in multilabel settings requires specific cross validation
methods designed for multilabel data. In this article, we show a weakness in an
evaluation metric widely used in literature and we present improved versions of
this metric and a general method, optisplit, for optimising cross validations
splits. We present an extensive comparison of various types of cross validation
methods in which we show that optisplit produces better cross validation splits
than the existing methods and that it is fast enough to be used on big Gene
Ontology (GO) datasets

    

### [[2109.01433] Relating the Partial Dependence Plot and Permutation Feature Importance to the Data Generating Process](http://arxiv.org/abs/2109.01433)


  Scientists and practitioners increasingly rely on machine learning to model
data and draw conclusions. Compared to statistical modeling approaches, machine
learning makes fewer explicit assumptions about data structures, such as
linearity. However, their model parameters usually cannot be easily related to
the data generating process. To learn about the modeled relationships, partial
dependence (PD) plots and permutation feature importance (PFI) are often used
as interpretation methods. However, PD and PFI lack a theory that relates them
to the data generating process. We formalize PD and PFI as statistical
estimators of ground truth estimands rooted in the data generating process. We
show that PD and PFI estimates deviate from this ground truth due to
statistical biases, model variance and Monte Carlo approximation errors. To
account for model variance in PD and PFI estimation, we propose the learner-PD
and the learner-PFI based on model refits, and propose corrected variance and
confidence interval estimators.

    

### [[2109.01451] Impact of GPU uncertainty on the training of predictive deep neural networks](http://arxiv.org/abs/2109.01451)


  Deep neural networks often present uncertainties such as hardware- and
software-derived noise and randomness. We studied the effects of such
uncertainty on learning outcomes, with a particular focus on the function of
graphics processing units (GPUs), and found that GPU-induced uncertainty
increased learning accuracy of a certain deep neural network. When training a
predictive deep neural network using only the CPU without the GPU, the learning
error is higher than when training the same number of epochs using the GPU,
suggesting that the GPU plays a different role in the learning process than
just increasing the computational speed. Because this effect cannot be observed
in learning by a simple autoencoder, it could be a phenomenon specific to
certain types of neural networks. GPU-specific computational processing is more
indeterminate than that by CPUs, and hardware-derived uncertainties, which are
often considered obstacles that need to be eliminated, might, in some cases, be
successfully incorporated into the training of deep neural networks. Moreover,
such uncertainties might be interesting phenomena to consider in brain-related
computational processing, which comprises a large mass of uncertain signals.

    

### [[2109.01461] Dive into Layers: Neural Network Capacity Bounding using Algebraic Geometry](http://arxiv.org/abs/2109.01461)


  The empirical results suggest that the learnability of a neural network is
directly related to its size. To mathematically prove this, we borrow a tool in
topological algebra: Betti numbers to measure the topological geometric
complexity of input data and the neural network. By characterizing the
expressive capacity of a neural network with its topological complexity, we
conduct a thorough analysis and show that the network's expressive capacity is
limited by the scale of its layers. Further, we derive the upper bounds of the
Betti numbers on each layer within the network. As a result, the problem of
architecture selection of a neural network is transformed to determining the
scale of the network that can represent the input data complexity. With the
presented results, the architecture selection of a fully connected network
boils down to choosing a suitable size of the network such that it equips the
Betti numbers that are not smaller than the Betti numbers of the input data. We
perform the experiments on a real-world dataset MNIST and the results verify
our analysis and conclusion. The code will be publicly available.

    

### [[2109.01467] Semi-Implicit Neural Solver for Time-dependent Partial Differential Equations](http://arxiv.org/abs/2109.01467)


  Fast and accurate solutions of time-dependent partial differential equations
(PDEs) are of pivotal interest to many research fields, including physics,
engineering, and biology. Generally, implicit/semi-implicit schemes are
preferred over explicit ones to improve stability and correctness. However,
existing semi-implicit methods are usually iterative and employ a
general-purpose solver, which may be sub-optimal for a specific class of PDEs.
In this paper, we propose a neural solver to learn an optimal iterative scheme
in a data-driven fashion for any class of PDEs. Specifically, we modify a
single iteration of a semi-implicit solver using a deep neural network. We
provide theoretical guarantees for the correctness and convergence of neural
solvers analogous to conventional iterative solvers. In addition to the
commonly used Dirichlet boundary condition, we adopt a diffuse domain approach
to incorporate a diverse type of boundary conditions, e.g., Neumann. We show
that the proposed neural solver can go beyond linear PDEs and applies to a
class of non-linear PDEs, where the non-linear component is non-stiff. We
demonstrate the efficacy of our method on 2D and 3D scenarios. To this end, we
show how our model generalizes to parameter settings, which are different from
training; and achieves faster convergence than semi-implicit schemes.

    

### [[2109.01479] LG4AV: Combining Language Models and Graph Neural Networks for Author Verification](http://arxiv.org/abs/2109.01479)


  The automatic verification of document authorships is important in various
settings. Researchers are for example judged and compared by the amount and
impact of their publications and public figures are confronted by their posts
on social media platforms. Therefore, it is important that authorship
information in frequently used web services and platforms is correct. The
question whether a given document is written by a given author is commonly
referred to as authorship verification (AV). While AV is a widely investigated
problem in general, only few works consider settings where the documents are
short and written in a rather uniform style. This makes most approaches
unpractical for online databases and knowledge graphs in the scholarly domain.
Here, authorships of scientific publications have to be verified, often with
just abstracts and titles available. To this point, we present our novel
approach LG4AV which combines language models and graph neural networks for
authorship verification. By directly feeding the available texts in a
pre-trained transformer architecture, our model does not need any hand-crafted
stylometric features that are not meaningful in scenarios where the writing
style is, at least to some extent, standardized. By the incorporation of a
graph neural network structure, our model can benefit from relations between
authors that are meaningful with respect to the verification process. For
example, scientific authors are more likely to write about topics that are
addressed by their co-authors and twitter users tend to post about the same
subjects as people they follow. We experimentally evaluate our model and study
to which extent the inclusion of co-authorships enhances verification decisions
in bibliometric environments.

    

### [[2109.01484] Contrastive Representation Learning for Exemplar-Guided Paraphrase Generation](http://arxiv.org/abs/2109.01484)


  Exemplar-Guided Paraphrase Generation (EGPG) aims to generate a target
sentence which conforms to the style of the given exemplar while encapsulating
the content information of the source sentence. In this paper, we propose a new
method with the goal of learning a better representation of the style andthe
content. This method is mainly motivated by the recent success of contrastive
learning which has demonstrated its power in unsupervised feature extraction
tasks. The idea is to design two contrastive losses with respect to the content
and the style by considering two problem characteristics during training. One
characteristic is that the target sentence shares the same content with the
source sentence, and the second characteristic is that the target sentence
shares the same style with the exemplar. These two contrastive losses are
incorporated into the general encoder-decoder paradigm. Experiments on two
datasets, namely QQP-Pos and ParaNMT, demonstrate the effectiveness of our
proposed constrastive losses.

    

### [[2109.01494] Computing Graph Descriptors on Edge Streams](http://arxiv.org/abs/2109.01494)


  Graph feature extraction is a fundamental task in graphs analytics. Using
feature vectors (graph descriptors) in tandem with data mining algorithms that
operate on Euclidean data, one can solve problems such as classification,
clustering, and anomaly detection on graph-structured data. This idea has
proved fruitful in the past, with spectral-based graph descriptors providing
state-of-the-art classification accuracy on benchmark datasets. However, these
algorithms do not scale to large graphs since: 1) they require storing the
entire graph in memory, and 2) the end-user has no control over the algorithm's
runtime. In this paper, we present single-pass streaming algorithms to
approximate structural features of graphs (counts of subgraphs of order $k \geq
4$). Operating on edge streams allows us to avoid keeping the entire graph in
memory, and controlling the sample size enables us to control the time taken by
the algorithm. We demonstrate the efficacy of our descriptors by analyzing the
approximation error, classification accuracy, and scalability to massive
graphs. Our experiments showcase the effect of the sample size on approximation
error and predictive accuracy. The proposed descriptors are applicable on
graphs with millions of edges within minutes and outperform the
state-of-the-art descriptors in classification accuracy.

    

### [[2109.01514] Towards extraction of orthogonal and parsimonious non-linear modes from turbulent flows](http://arxiv.org/abs/2109.01514)


  We propose a deep probabilistic-neural-network architecture for learning a
minimal and near-orthogonal set of non-linear modes from high-fidelity
turbulent-flow-field data useful for flow analysis, reduced-order modeling, and
flow control. Our approach is based on $\beta$-variational autoencoders
($\beta$-VAEs) and convolutional neural networks (CNNs), which allow us to
extract non-linear modes from multi-scale turbulent flows while encouraging the
learning of independent latent variables and penalizing the size of the latent
vector. Moreover, we introduce an algorithm for ordering VAE-based modes with
respect to their contribution to the reconstruction. We apply this method for
non-linear mode decomposition of the turbulent flow through a simplified urban
environment, where the flow-field data is obtained based on well-resolved
large-eddy simulations (LESs). We demonstrate that by constraining the shape of
the latent space, it is possible to motivate the orthogonality and extract a
set of parsimonious modes sufficient for high-quality reconstruction. Our
results show the excellent performance of the method in the reconstruction
against linear-theory-based decompositions. Moreover, we compare our method
with available AE-based models. We show the ability of our approach in the
extraction of near-orthogonal modes that may lead to interpretability.

    

### [[2109.01518] Biomedical Data-to-Text Generation via Fine-Tuning Transformers](http://arxiv.org/abs/2109.01518)


  Data-to-text (D2T) generation in the biomedical domain is a promising - yet
mostly unexplored - field of research. Here, we apply neural models for D2T
generation to a real-world dataset consisting of package leaflets of European
medicines. We show that fine-tuned transformers are able to generate realistic,
multisentence text from data in the biomedical domain, yet have important
limitations. We also release a new dataset (BioLeaflets) for benchmarking D2T
generation models in the biomedical domain.

    

### [[2109.01528] LightAutoML: AutoML Solution for a Large Financial Services Ecosystem](http://arxiv.org/abs/2109.01528)


  We present an AutoML system called LightAutoML developed for a large European
financial services company and its ecosystem satisfying the set of
idiosyncratic requirements that this ecosystem has for AutoML solutions. Our
framework was piloted and deployed in numerous applications and performed at
the level of the experienced data scientists while building high-quality ML
models significantly faster than these data scientists. We also compare the
performance of our system with various general-purpose open source AutoML
solutions and show that it performs better for most of the ecosystem and OpenML
problems. We also present the lessons that we learned while developing the
AutoML system and moving it into production.

    

### [[2109.01531] MACEst: The reliable and trustworthy Model Agnostic Confidence Estimator](http://arxiv.org/abs/2109.01531)


  Reliable Confidence Estimates are hugely important for any machine learning
model to be truly useful. In this paper, we argue that any confidence estimates
based upon standard machine learning point prediction algorithms are
fundamentally flawed and under situations with a large amount of epistemic
uncertainty are likely to be untrustworthy. To address these issues, we present
MACEst, a Model Agnostic Confidence Estimator, which provides reliable and
trustworthy confidence estimates. The algorithm differs from current methods by
estimating confidence independently as a local quantity which explicitly
accounts for both aleatoric and epistemic uncertainty. This approach differs
from standard calibration methods that use a global point prediction model as a
starting point for the confidence estimate.

    

### [[2109.01538] Investigate the Correlation of Breast Cancer Dataset using Different Clustering Technique](http://arxiv.org/abs/2109.01538)


  The objectives of this paper are to explore ways to analyze breast cancer
dataset in the context of unsupervised learning without prior training model.
The paper investigates different ways of clustering techniques as well as
preprocessing. This in-depth analysis builds the footprint which can further
use for designing a most robust and accurate medical prognosis system. This
paper also give emphasis on correlations of data points with different standard
benchmark techniques. Keywords: Breast cancer dataset, Clustering Technique
Hopkins Statistic, K-means Clustering, k-medoids or partitioning around medoids
(PAM)

    

### [[2109.01545] Large-Scale Learning with Fourier Features and Tensor Decompositions](http://arxiv.org/abs/2109.01545)


  Random Fourier features provide a way to tackle large-scale machine learning
problems with kernel methods. Their slow Monte Carlo convergence rate has
motivated the research of deterministic Fourier features whose approximation
error decreases exponentially with the number of frequencies. However, due to
their tensor product structure these methods suffer heavily from the curse of
dimensionality, limiting their applicability to two or three-dimensional
scenarios. In our approach we overcome said curse of dimensionality by
exploiting the tensor product structure of deterministic Fourier features,
which enables us to represent the model parameters as a low-rank tensor
decomposition. We derive a monotonically converging block coordinate descent
algorithm with linear complexity in both the sample size and the dimensionality
of the inputs for a regularized squared loss function, allowing to learn a
parsimonious model in decomposed form using deterministic Fourier features. We
demonstrate by means of numerical experiments how our low-rank tensor approach
obtains the same performance of the corresponding nonparametric model,
consistently outperforming random Fourier features.

    

### [[2109.01556] Pareto-Optimal Learning-Augmented Algorithms for Online Conversion Problems](http://arxiv.org/abs/2109.01556)


  This paper leverages machine-learned predictions to design competitive
algorithms for online conversion problems with the goal of improving the
competitive ratio when predictions are accurate (i.e., consistency), while also
guaranteeing a worst-case competitive ratio regardless of the prediction
quality (i.e., robustness). We unify the algorithmic design of both integral
and fractional conversion problems, which are also known as the 1-max-search
and one-way trading problems, into a class of online threshold-based algorithms
(OTA). By incorporating predictions into design of OTA, we achieve the
Pareto-optimal trade-off of consistency and robustness, i.e., no online
algorithm can achieve a better consistency guarantee given for a robustness
guarantee. We demonstrate the performance of OTA using numerical experiments on
Bitcoin conversion.

    

### [[2109.01572] Using Topological Framework for the Design of Activation Function and Model Pruning in Deep Neural Networks](http://arxiv.org/abs/2109.01572)


  Success of deep neural networks in diverse tasks across domains of computer
vision, speech recognition and natural language processing, has necessitated
understanding the dynamics of training process and also working of trained
models. Two independent contributions of this paper are 1) Novel activation
function for faster training convergence 2) Systematic pruning of filters of
models trained irrespective of activation function. We analyze the topological
transformation of the space of training samples as it gets transformed by each
successive layer during training, by changing the activation function. The
impact of changing activation function on the convergence during training is
reported for the task of binary classification. A novel activation function
aimed at faster convergence for classification tasks is proposed. Here, Betti
numbers are used to quantify topological complexity of data. Results of
experiments on popular synthetic binary classification datasets with large
Betti numbers(>150) using MLPs are reported. Results show that the proposed
activation function results in faster convergence requiring fewer epochs by a
factor of 1.5 to 2, since Betti numbers reduce faster across layers with the
proposed activation function. The proposed methodology was verified on
benchmark image datasets: fashion MNIST, CIFAR-10 and cat-vs-dog images, using
CNNs. Based on empirical results, we propose a novel method for pruning a
trained model. The trained model was pruned by eliminating filters that
transform data to a topological space with large Betti numbers. All filters
with Betti numbers greater than 300 were removed from each layer without
significant reduction in accuracy. This resulted in faster prediction time and
reduced memory size of the model.

    

### [[2109.01621] Stochastic Physics-Informed Neural Networks (SPINN): A Moment-Matching Framework for Learning Hidden Physics within Stochastic Differential Equations](http://arxiv.org/abs/2109.01621)


  Stochastic differential equations (SDEs) are used to describe a wide variety
of complex stochastic dynamical systems. Learning the hidden physics within
SDEs is crucial for unraveling fundamental understanding of the stochastic and
nonlinear behavior of these systems. We propose a flexible and scalable
framework for training deep neural networks to learn constitutive equations
that represent hidden physics within SDEs. The proposed stochastic
physics-informed neural network framework (SPINN) relies on uncertainty
propagation and moment-matching techniques along with state-of-the-art deep
learning strategies. SPINN first propagates stochasticity through the known
structure of the SDE (i.e., the known physics) to predict the time evolution of
statistical moments of the stochastic states. SPINN learns (deep) neural
network representations of the hidden physics by matching the predicted moments
to those estimated from data. Recent advances in automatic differentiation and
mini-batch gradient descent are leveraged to establish the unknown parameters
of the neural networks. We demonstrate SPINN on three benchmark in-silico case
studies and analyze the framework's robustness and numerical stability. SPINN
provides a promising new direction for systematically unraveling the hidden
physics of multivariate stochastic dynamical systems with multiplicative noise.

    

### [[2109.01636] Empirical Study of Named Entity Recognition Performance Using Distribution-aware Word Embedding](http://arxiv.org/abs/2109.01636)


  With the fast development of Deep Learning techniques, Named Entity
Recognition (NER) is becoming more and more important in the information
extraction task. The greatest difficulty that the NER task faces is to keep the
detectability even when types of NE and documents are unfamiliar. Realizing
that the specificity information may contain potential meanings of a word and
generate semantic-related features for word embedding, we develop a
distribution-aware word embedding and implement three different methods to make
use of the distribution information in a NER framework. And the result shows
that the performance of NER will be improved if the word specificity is
incorporated into existing NER methods.

    

### [[2109.01654] Multi-agent Natural Actor-critic Reinforcement Learning Algorithms](http://arxiv.org/abs/2109.01654)


  Both single-agent and multi-agent actor-critic algorithms are an important
class of Reinforcement Learning algorithms. In this work, we propose three
fully decentralized multi-agent natural actor-critic (MAN) algorithms. The
agents' objective is to collectively learn a joint policy that maximizes the
sum of averaged long-term returns of these agents. In the absence of a central
controller, agents communicate the information to their neighbors via a
time-varying communication network while preserving privacy. We prove the
convergence of all the 3 MAN algorithms to a globally asymptotically stable
point of the ODE corresponding to the actor update; these use linear function
approximations. We use the Fisher information matrix to obtain the natural
gradients. The Fisher information matrix captures the curvature of the
Kullback-Leibler (KL) divergence between polices at successive iterates. We
also show that the gradient of this KL divergence between policies of
successive iterates is proportional to the objective function's gradient. Our
MAN algorithms indeed use this \emph{representation} of the objective
function's gradient. Under certain conditions on the Fisher information matrix,
we prove that at each iterate, the optimal value via MAN algorithms can be
better than that of the multi-agent actor-critic (MAAC) algorithm using the
standard gradients. To validate the usefulness of our proposed algorithms, we
implement all the 3 MAN algorithms on a bi-lane traffic network to reduce the
average network congestion. We observe an almost 25% reduction in the average
congestion in 2 MAN algorithms; the average congestion in another MAN algorithm
is on par with the MAAC algorithm. We also consider a generic 15 agent MARL;
the performance of the MAN algorithms is again as good as the MAAC algorithm.
We attribute the better performance of the MAN algorithms to their use of the
above representation.

    

### [[2006.09268] Metrizing Weak Convergence with Maximum Mean Discrepancies](http://arxiv.org/abs/2006.09268)


  This paper characterizes the maximum mean discrepancies (MMD) that metrize
the weak convergence of probability measures for a wide class of kernels. More
precisely, we prove that, on a locally compact, non-compact, Hausdorff space,
the MMD of a bounded continuous Borel measurable kernel k, whose reproducing
kernel Hilbert space (RKHS) functions vanish at infinity, metrizes the weak
convergence of probability measures if and only if k is continuous and
integrally strictly positive definite (i.s.p.d.) over all signed, finite,
regular Borel measures. We also correct a prior result of Simon-Gabriel &
Schlkopf (JMLR, 2018, Thm.12) by showing that there exist both bounded
continuous i.s.p.d. kernels that do not metrize weak convergence and bounded
continuous non-i.s.p.d. kernels that do metrize it.

    

### [[2006.13508] A Limitation of the PAC-Bayes Framework](http://arxiv.org/abs/2006.13508)


  PAC-Bayes is a useful framework for deriving generalization bounds which was
introduced by McAllester ('98). This framework has the flexibility of deriving
distribution- and algorithm-dependent bounds, which are often tighter than
VC-related uniform convergence bounds. In this manuscript we present a
limitation for the PAC-Bayes framework. We demonstrate an easy learning task
that is not amenable to a PAC-Bayes analysis.
Specifically, we consider the task of linear classification in 1D; it is
well-known that this task is learnable using just $O(\log(1/\delta)/\epsilon)$
examples. On the other hand, we show that this fact can not be proved using a
PAC-Bayes analysis: for any algorithm that learns 1-dimensional linear
classifiers there exists a (realizable) distribution for which the PAC-Bayes
bound is arbitrarily large.

    

### [[2007.01195] Hierarchically Organized Latent Modules for Exploratory Search in Morphogenetic Systems](http://arxiv.org/abs/2007.01195)


  Self-organization of complex morphological patterns from local interactions
is a fascinating phenomenon in many natural and artificial systems. In the
artificial world, typical examples of such morphogenetic systems are cellular
automata. Yet, their mechanisms are often very hard to grasp and so far
scientific discoveries of novel patterns have primarily been relying on manual
tuning and ad hoc exploratory search. The problem of automated diversity-driven
discovery in these systems was recently introduced [26, 62], highlighting that
two key ingredients are autonomous exploration and unsupervised representation
learning to describe "relevant" degrees of variations in the patterns. In this
paper, we motivate the need for what we call Meta-diversity search, arguing
that there is not a unique ground truth interesting diversity as it strongly
depends on the final observer and its motives. Using a continuous game-of-life
system for experiments, we provide empirical evidences that relying on
monolithic architectures for the behavioral embedding design tends to bias the
final discoveries (both for hand-defined and unsupervisedly-learned features)
which are unlikely to be aligned with the interest of a final end-user. To
address these issues, we introduce a novel dynamic and modular architecture
that enables unsupervised learning of a hierarchy of diverse representations.
Combined with intrinsically motivated goal exploration algorithms, we show that
this system forms a discovery assistant that can efficiently adapt its
diversity search towards preferences of a user using only a very small amount
of user feedback.

    

### [[2010.12575] Explanation and Use of Uncertainty Obtained by Bayesian Neural Network Classifiers for Breast Histopathology Images](http://arxiv.org/abs/2010.12575)


  Despite the promise of Convolutional neural network (CNN) based
classification models for histopathological images, it is infeasible to
quantify its uncertainties. Moreover, CNNs may suffer from overfitting when the
data is biased. We show that Bayesian-CNN can overcome these limitations by
regularizing automatically and by quantifying the uncertainty. In addition, it
can perform much better than the state-of-the-art transfer learning CNN by
reducing the false negative and false positive by 11% and 7.7% respectively. We
have developed a novel technique to utilize the uncertainties provided by the
Bayesian-CNN that significantly improves the performance on a large fraction of
the test data (about 6% improvement in accuracy on 77% of test data). Further,
we provide a novel explanation for the uncertainty by projecting the data into
a low dimensional space through a nonlinear dimensionality reduction technique.
This dimensionality reduction enables interpretation of the test data through
visualization and reveals the structure of the data in a low dimensional
feature space. Besides, we modify the Bayesian--CNN by introducing a stochastic
adaptive activation function. The modified Bayesian-CNN performs slightly
better than Bayesian-CNN on all performance metrics and significantly reduces
the number of false negatives and false positives (3% reduction for both). This
work shows the advantages of Bayesian-CNN against the state-of-the-art,
explains and utilizes the uncertainties for histopathological images. It should
find applications in various medical image classifications.

    

### [[2012.01982] Tensor Data Scattering and the Impossibility of Slicing Theorem](http://arxiv.org/abs/2012.01982)


  This paper proposes a standard way to represent sparse tensors. A broad
theoretical framework for tensor data scattering methods used in various deep
learning frameworks is established. This paper presents a theorem that is very
important for performance analysis and accelerator optimization for
implementing data scattering. The theorem shows how the impossibility of
slicing happens in tenser data scattering. A sparsity measuring formula is
provided, which can effectively indicate the storage efficiency of sparse
tensor and the possibility of parallelly using it. The source code, including
CUDA code, is provided in a related open-source project.

    

### [[2012.13025] Causal Inference from Slowly Varying Nonstationary Processes](http://arxiv.org/abs/2012.13025)


  Causal inference from observational data following the restricted structural
causal model (SCM) framework hinges largely on the asymmetry between cause and
effect from the data generating mechanisms, such as non-Gaussianity or
nonlinearity. This methodology can be adapted to stationary time series, yet
inferring causal relationships from nonstationary time series remains a
challenging task. In this work, we propose a new class of restricted SCM, via a
time-varying filter and stationary noise, and exploit the asymmetry from
nonstationarity for causal identification in both bivariate and network
settings. We propose efficient procedures by leveraging powerful estimates of
the bivariate evolutionary spectra for slowly varying processes. Various
synthetic and real datasets that involve high-order and non-smooth filters are
evaluated to demonstrate the effectiveness of our proposed methodology.

    

### [[2101.00962] Hybrid FEM-NN models: Combining artificial neural networks with the finite element method](http://arxiv.org/abs/2101.00962)


  We present a methodology combining neural networks with physical principle
constraints in the form of partial differential equations (PDEs). The approach
allows to train neural networks while respecting the PDEs as a strong
constraint in the optimisation as apposed to making them part of the loss
function. The resulting models are discretised in space by the finite element
method (FEM). The method applies to both stationary and transient as well as
linear/nonlinear PDEs. We describe implementation of the approach as an
extension of the existing FEM framework FEniCS and its algorithmic
differentiation tool dolfin-adjoint. Through series of examples we demonstrate
capabilities of the approach to recover coefficients and missing PDE operators
from observations. Further, the proposed method is compared with alternative
methodologies, namely, physics informed neural networks and standard
PDE-constrained optimisation. Finally, we demonstrate the method on a complex
cardiac cell model problem using deep neural networks.

    

### [[2101.11376] Learning Abstract Representations through Lossy Compression of Multi-Modal Signals](http://arxiv.org/abs/2101.11376)


  A key competence for open-ended learning is the formation of increasingly
abstract representations useful for driving complex behavior. Abstract
representations ignore specific details and facilitate generalization. Here we
consider the learning of abstract representations in a multi-modal setting with
two or more input modalities. We treat the problem as a lossy compression
problem and show that generic lossy compression of multimodal sensory input
naturally extracts abstract representations that tend to strip away modalitiy
specific details and preferentially retain information that is shared across
the different modalities. Furthermore, we propose an architecture to learn
abstract representations by identifying and retaining only the information that
is shared across multiple modalities while discarding any modality specific
information.

    

### [[2102.05714] Domain Adaptation In Reinforcement Learning Via Latent Unified State Representation](http://arxiv.org/abs/2102.05714)


  Despite the recent success of deep reinforcement learning (RL), domain
adaptation remains an open problem. Although the generalization ability of RL
agents is critical for the real-world applicability of Deep RL, zero-shot
policy transfer is still a challenging problem since even minor visual changes
could make the trained agent completely fail in the new task. To address this
issue, we propose a two-stage RL agent that first learns a latent unified state
representation (LUSR) which is consistent across multiple domains in the first
stage, and then do RL training in one source domain based on LUSR in the second
stage. The cross-domain consistency of LUSR allows the policy acquired from the
source domain to generalize to other target domains without extra training. We
first demonstrate our approach in variants of CarRacing games with customized
manipulations, and then verify it in CARLA, an autonomous driving simulator
with more complex and realistic visual observations. Our results show that this
approach can achieve state-of-the-art domain adaptation performance in related
RL tasks and outperforms prior approaches based on latent-representation based
RL and image-to-image translation.

    

### [[2102.10544] Rethinking Content and Style: Exploring Bias for Unsupervised Disentanglement](http://arxiv.org/abs/2102.10544)


  Content and style (C-S) disentanglement intends to decompose the underlying
explanatory factors of objects into two independent subspaces. From the
unsupervised disentanglement perspective, we rethink content and style and
propose a formulation for unsupervised C-S disentanglement based on our
assumption that different factors are of different importance and popularity
for image reconstruction, which serves as a data bias. The corresponding model
inductive bias is introduced by our proposed C-S disentanglement Module (C-S
DisMo), which assigns different and independent roles to content and style when
approximating the real data distributions. Specifically, each content embedding
from the dataset, which encodes the most dominant factors for image
reconstruction, is assumed to be sampled from a shared distribution across the
dataset. The style embedding for a particular image, encoding the remaining
factors, is used to customize the shared distribution through an affine
transformation. The experiments on several popular datasets demonstrate that
our method achieves the state-of-the-art unsupervised C-S disentanglement,
which is comparable or even better than supervised methods. We verify the
effectiveness of our method by downstream tasks: domain translation and
single-view 3D reconstruction. Project page at
this https URL.

    

### [[2103.06315] Affine-Mapping based Variational Ensemble Kalman Filter](http://arxiv.org/abs/2103.06315)


  We propose an affine-mapping based variational Ensemble Kalman filter for
sequential Bayesian filtering problems with generic observation models.
Specifically, the proposed method is formulated as to construct an affine
mapping from the prior ensemble to the posterior one, and the affine mapping is
computed via a variational Bayesian formulation, i.e., by minimizing the
Kullback-Leibler divergence between the transformed distribution through the
affine mapping and the actual posterior. Some theoretical properties of
resulting optimization problem are studied and a gradient descent scheme is
proposed to solve the resulting optimization problem. With numerical examples
we demonstrate that the method has competitive performance against existing
methods.

    

### [[2103.10919] Robustness via Cross-Domain Ensembles](http://arxiv.org/abs/2103.10919)


  We present a method for making neural network predictions robust to shifts
from the training data distribution. The proposed method is based on making
predictions via a diverse set of cues (called 'middle domains') and ensembling
them into one strong prediction. The premise of the idea is that predictions
made via different cues respond differently to a distribution shift, hence one
should be able to merge them into one robust final prediction. We perform the
merging in a straightforward but principled manner based on the uncertainty
associated with each prediction. The evaluations are performed using multiple
tasks and datasets (Taskonomy, Replica, ImageNet, CIFAR) under a wide range of
adversarial and non-adversarial distribution shifts which demonstrate the
proposed method is considerably more robust than its standard learning
counterpart, conventional deep ensembles, and several other baselines.

    

### [[2103.15035] Community Detection in General Hypergraph via Graph Embedding](http://arxiv.org/abs/2103.15035)


  Conventional network data has largely focused on pairwise interactions
between two entities, yet multi-way interactions among multiple entities have
been frequently observed in real-life hypergraph networks. In this article, we
propose a novel method for detecting community structure in general hypergraph
networks, uniform or non-uniform. The proposed method introduces a null vertex
to augment a non-uniform hypergraph into a uniform multi-hypergraph, and then
embeds the multi-hypergraph in a low-dimensional vector space such that
vertices within the same community are close to each other. The resultant
optimization task can be efficiently tackled by an alternative updating scheme.
The asymptotic consistencies of the proposed method are established in terms of
both community detection and hypergraph estimation, which are also supported by
numerical experiments on some synthetic and real-life hypergraph networks.

    

### [[2104.05158] Software-Hardware Co-design for Fast and Scalable Training of Deep Learning Recommendation Models](http://arxiv.org/abs/2104.05158)


  Deep learning recommendation models (DLRMs) are used across many
business-critical services at Facebook and are the single largest AI
application in terms of infrastructure demand in its data-centers. In this
paper we discuss the SW/HW co-designed solution for high-performance
distributed training of large-scale DLRMs. We introduce a high-performance
scalable software stack based on PyTorch and pair it with the new evolution of
Zion platform, namely ZionEX. We demonstrate the capability to train very large
DLRMs with up to 12 Trillion parameters and show that we can attain 40X speedup
in terms of time to solution over previous systems. We achieve this by (i)
designing the ZionEX platform with dedicated scale-out network, provisioned
with high bandwidth, optimal topology and efficient transport (ii) implementing
an optimized PyTorch-based training stack supporting both model and data
parallelism (iii) developing sharding algorithms capable of hierarchical
partitioning of the embedding tables along row, column dimensions and load
balancing them across multiple workers; (iv) adding high-performance core
operators while retaining flexibility to support optimizers with fully
deterministic updates (v) leveraging reduced precision communications,
multi-level memory hierarchy (HBM+DDR+SSD) and pipelining. Furthermore, we
develop and briefly comment on distributed data ingestion and other supporting
services that are required for the robust and efficient end-to-end training in
production environments.

    

### [[2104.08145] KI-BERT: Infusing Knowledge Context for Better Language and Domain Understanding](http://arxiv.org/abs/2104.08145)


  Contextualized entity representations learned by state-of-the-art
transformer-based language models (TLMs) like BERT, GPT, T5, etc., leverage the
attention mechanism to learn the data context from training data corpus.
However, these models do not use the knowledge context. Knowledge context can
be understood as semantics about entities and their relationship with
neighboring entities in knowledge graphs. We propose a novel and effective
technique to infuse knowledge context from multiple knowledge graphs for
conceptual and ambiguous entities into TLMs during fine-tuning. It projects
knowledge graph embeddings in the homogeneous vector-space, introduces new
token-types for entities, aligns entity position ids, and a selective attention
mechanism. We take BERT as a baseline model and implement the
"Knowledge-Infused BERT" by infusing knowledge context from ConceptNet and
WordNet, which significantly outperforms BERT and other recent knowledge-aware
BERT variants like ERNIE, SenseBERT, and BERT_CS over eight different subtasks
of GLUE benchmark. The KI-BERT-base model even significantly outperforms
BERT-large for domain-specific tasks like SciTail and academic subsets of QQP,
QNLI, and MNLI.

    

### [[2104.10696] Scaling of neural-network quantum states for time evolution](http://arxiv.org/abs/2104.10696)


  Simulating quantum many-body dynamics on classical computers is a challenging
problem due to the exponential growth of the Hilbert space. Artificial neural
networks have recently been introduced as a new tool to approximate
quantum-many body states. We benchmark the variational power of different
shallow and deep neural autoregressive quantum states to simulate global quench
dynamics of a non-integrable quantum Ising chain. We find that the number of
parameters required to represent the quantum state at a given accuracy
increases exponentially in time. The growth rate is only slightly affected by
the network architecture over a wide range of different design choices: shallow
and deep networks, small and large filter sizes, dilated and normal
convolutions, with and without shortcut connections.

    

### [[2105.09900] Online Binary Models are Promising for Distinguishing Temporally Consistent Computer Usage Profiles](http://arxiv.org/abs/2105.09900)


  This paper investigates whether computer usage profiles comprised of
process-, network-, mouse-, and keystroke-related events are unique and
consistent over time in a naturalistic setting, discussing challenges and
opportunities of using such profiles in applications of continuous
authentication. We collected ecologically-valid computer usage profiles from 31
MS Windows 10 computer users over 8 weeks and submitted this data to
comprehensive machine learning analysis involving a diverse set of online and
offline classifiers. We found that: (i) profiles were mostly consistent over
the 8-week data collection period, with most (83.9%) repeating computer usage
habits on a daily basis; (ii) computer usage profiling has the potential to
uniquely characterize computer users (with a maximum F-score of 99.90%); (iii)
network-related events were the most relevant features to accurately recognize
profiles (95.69% of the top features distinguishing users were
network-related); and (iv) binary models were the most well-suited for profile
recognition, with better results achieved in the online setting compared to the
offline setting (maximum F-score of 99.90% vs. 95.50%).

    

### [[2106.07613] Improving Metric Dimensionality Reduction with Distributed Topology](http://arxiv.org/abs/2106.07613)


  We propose a novel approach to dimensionality reduction combining techniques
of metric geometry and distributed persistent homology, in the form of a
gradient-descent based method called DIPOLE. DIPOLE is a
dimensionality-reduction post-processing step that corrects an initial
embedding by minimizing a loss functional with both a local, metric term and a
global, topological term. By fixing an initial embedding method (we use
Isomap), DIPOLE can also be viewed as a full dimensionality-reduction pipeline.
This framework is based on the strong theoretical and computational properties
of distributed persistent homology and comes with the guarantee of almost sure
convergence. We observe that DIPOLE outperforms popular methods like UMAP,
t-SNE, and Isomap on a number of popular datasets, both visually and in terms
of precise quantitative metrics.

    

### [[2106.08756] Automating Augmentation Through Random Unidimensional Search](http://arxiv.org/abs/2106.08756)


  It is no secret amongst deep learning researchers that finding the optimal
data augmentation strategy during training can mean the difference between
state-of-the-art performance and a run-of-the-mill result. To that end, the
community has seen many efforts to automate the process of finding the perfect
augmentation procedure for any task at hand. Unfortunately, even recent
cutting-edge methods bring massive computational overhead, requiring as many as
100 full model trainings to settle on an ideal configuration. We show how to
achieve equivalent performance in just 6: with Random Unidimensional
Augmentation. Source code is available at this https URL


### [[2106.08767] To Raise or Not To Raise: The Autonomous Learning Rate Question](http://arxiv.org/abs/2106.08767)


  There is a parameter ubiquitous throughout the deep learning world: learning
rate. There is likewise a ubiquitous question: what should that learning rate
be? The true answer to this question is often tedious and time consuming to
obtain, and a great deal of arcane knowledge has accumulated in recent years
over how to pick and modify learning rates to achieve optimal training
performance. Moreover, the long hours spent carefully crafting the perfect
learning rate can come to nothing the moment your network architecture,
optimizer, dataset, or initial conditions change ever so slightly. But it need
not be this way. We propose a new answer to the great learning rate question:
the Autonomous Learning Rate Controller. Find it at
this https URL


### [[2108.13320] Neural HMMs are all you need (for high-quality attention-free TTS)](http://arxiv.org/abs/2108.13320)


  Neural sequence-to-sequence TTS has demonstrated significantly better output
quality over classical statistical parametric speech synthesis using HMMs.
However, the new paradigm is not probabilistic and the use of non-monotonic
attention both increases training time and introduces "babbling" failure modes
that are unacceptable in production. In this paper, we demonstrate that the old
and new paradigms can be combined to obtain the advantages of both worlds, by
replacing the attention in Tacotron 2 with an autoregressive left-right no-skip
hidden Markov model defined by a neural network. This leads to an HMM-based
neural TTS model with monotonic alignment, trained to maximise the full
sequence likelihood without approximations. We discuss how to combine
innovations from both classical and contemporary TTS for best results. The
final system is smaller and simpler than Tacotron 2 and learns to align and
speak with fewer iterations, whilst achieving the same naturalness prior to the
post-net. Our system also allows easy control over speaking rate. Audio
examples and code are available at this https URL


### [[2109.01188] NVMExplorer: A Framework for Cross-Stack Comparisons of Embedded Non-Volatile Memories](http://arxiv.org/abs/2109.01188)


  Repeated off-chip memory accesses to DRAM drive up operating power for
data-intensive applications, and SRAM technology scaling and leakage power
limits the efficiency of embedded memories. Future on-chip storage will need
higher density and energy efficiency, and the actively expanding field of
emerging, embeddable non-volatile memory (eNVM) technologies is providing many
potential candidates to satisfy this need. Each technology proposal presents
distinct trade-offs in terms of density, read, write, and reliability
characteristics, and we present a comprehensive framework for navigating and
quantifying these design trade-offs alongside realistic system constraints and
application-level impacts. This work evaluates eNVM-based storage for a range
of application and system contexts including machine learning on the edge,
graph analytics, and general purpose cache hierarchy, in addition to describing
a freely available (this http URL) set of tools for
application experts, system designers, and device experts to better understand,
compare, and quantify the next generation of embedded memory solutions.

    

### [[2109.01269] SMART: A Heterogeneous Scratchpad Memory Architecture for Superconductor SFQ-based Systolic CNN Accelerators](http://arxiv.org/abs/2109.01269)


  Ultra-fast \& low-power superconductor single-flux-quantum (SFQ)-based CNN
systolic accelerators are built to enhance the CNN inference throughput.
However, shift-register (SHIFT)-based scratchpad memory (SPM) arrays prevent a
SFQ CNN accelerator from exceeding 40\% of its peak throughput, due to the lack
of random access capability. This paper first documents our study of a variety
of cryogenic memory technologies, including Vortex Transition Memory (VTM),
Josephson-CMOS SRAM, MRAM, and Superconducting Nanowire Memory, during which we
found that none of the aforementioned technologies made a SFQ CNN accelerator
achieve high throughput, small area, and low power simultaneously. Second, we
present a heterogeneous SPM architecture, SMART, composed of SHIFT arrays and a
random access array to improve the inference throughput of a SFQ CNN systolic
accelerator. Third, we propose a fast, low-power and dense pipelined random
access CMOS-SFQ array by building SFQ passive-transmission-line-based H-Trees
that connect CMOS sub-banks. Finally, we create an ILP-based compiler to deploy
CNN models on SMART. Experimental results show that, with the same chip area
overhead, compared to the latest SHIFT-based SFQ CNN accelerator, SMART
improves the inference throughput by $3.9\times$ ($2.2\times$), and reduces the
inference energy by $86\%$ ($71\%$) when inferring a single image (a batch of
images).

    

### [[2109.01404] End-to-end 100-TOPS/W Inference With Analog In-Memory Computing: Are We There Yet?](http://arxiv.org/abs/2109.01404)


  In-Memory Acceleration (IMA) promises major efficiency improvements in deep
neural network (DNN) inference, but challenges remain in the integration of IMA
within a digital system. We propose a heterogeneous architecture coupling 8
RISC-V cores with an IMA in a shared-memory cluster, analyzing the benefits and
trade-offs of in-memory computing on the realistic use case of a MobileNetV2
bottleneck layer. We explore several IMA integration strategies, analyzing
performance, area, and energy efficiency. We show that while pointwise layers
achieve significant speed-ups over software implementation, on depthwise layer
the inability to efficiently map parameters on the accelerator leads to a
significant trade-off between throughput and area. We propose a hybrid solution
where pointwise convolutions are executed on IMA while depthwise on the cluster
cores, achieving a speed-up of 3x over SW execution while saving 50% of area
when compared to an all-in IMA solution with similar performance.

    

### [[2105.10397] NVCache: A Plug-and-Play NVMM-based I/O Booster for Legacy Systems](http://arxiv.org/abs/2105.10397)


  This paper introduces NVCache, an approach that uses a non-volatile main
memory (NVMM) as a write cache to improve the write performance of legacy
applications. We compare NVCache against file systems tailored for NVMM
(Ext4-DAX and NOVA) and with I/O-heavy applications (SQLite, RocksDB). Our
evaluation shows that NVCache reaches the performance level of the existing
state-of-the-art systems for NVMM, but without their limitations: NVCache does
not limit the size of the stored data to the size of the NVMM, and works
transparently with unmodified legacy applications, providing additional
persistence guarantees even when their source code is not available.

    

### [[2109.01201] ECO: Edge-Cloud Optimization of 5G applications](http://arxiv.org/abs/2109.01201)


  Centralized cloud computing with 100+ milliseconds network latencies cannot
meet the tens of milliseconds to sub-millisecond response times required for
emerging 5G applications like autonomous driving, smart manufacturing, tactile
internet, and augmented or virtual reality. We describe a new, dynamic runtime
that enables such applications to make effective use of a 5G network, computing
at the edge of this network, and resources in the centralized cloud, at all
times. Our runtime continuously monitors the interaction among the
microservices, estimates the data produced and exchanged among the
microservices, and uses a novel graph min-cut algorithm to dynamically map the
microservices to the edge or the cloud to satisfy application-specific response
times. Our runtime also handles temporary network partitions, and maintains
data consistency across the distributed fabric by using microservice proxies to
reduce WAN bandwidth by an order of magnitude, all in an {\it
application-specific manner} by leveraging knowledge about the application's
functions, latency-critical pipelines and intermediate data. We illustrate the
use of our runtime by successfully mapping two complex, representative
real-world video analytics applications to the AWS/Verizon Wavelength
edge-cloud architecture, and improving application response times by 2x when
compared with a static edge-cloud implementation.

    

### [[2109.01205] Byzantine Consensus in Directed Hypergraphs](http://arxiv.org/abs/2109.01205)


  Byzantine consensus is a classical problem in distributed computing. Each
node in a synchronous system starts with a binary input. The goal is to reach
agreement in the presence of Byzantine faulty nodes. We consider the setting
where communication between nodes is modelled via a directed hypergraph. In the
classical point-to-point communication model, the communication between nodes
is modelled as a simple graph where all messages sent on an edge are private
between the two endpoints of the edge. This allows a faulty node to equivocate,
i.e., lie differently to its different neighbors. Different models have been
proposed in the literature that weaken equivocation. In the local broadcast
model, every message transmitted by a node is received identically and
correctly by all of its neighbors. In the hypergraph model, every message
transmitted by a node on a hyperedge is received identically and correctly by
all nodes on the hyperedge. Tight network conditions are known for each of the
three cases for undirected (hyper)graphs. For the directed models, tight
conditions are known for the point-to-point and local broadcast models.
In this work, we consider the directed hypergraph model that encompasses all
the models above. Each directed hyperedge consists of a single head (sender)
and at least one tail (receiver), This models a local multicast channel where
messages transmitted by the sender are received identically by all the
receivers in the hyperedge. For this model, we identify tight network
conditions for consensus. We observe how the directed hypergraph model reduces
to each of the three models above under specific conditions. In each case, we
relate our network condition to the corresponding known tight conditions. The
directed hypergraph model also encompasses other practical network models of
interest that have not been explored previously, as elaborated in the paper.

    

### [[2109.01212] A Reliable, Self-Adaptive Face Identification Framework via Lyapunov Optimization](http://arxiv.org/abs/2109.01212)


  Realtime face identification (FID) from a video feed is highly
computation-intensive, and may exhaust computation resources if performed on a
device with a limited amount of resources (e.g., a mobile device). In general,
FID performs better when images are sampled at a higher rate, minimizing false
negatives. However, performing it at an overwhelmingly high rate exposes the
system to the risk of a queue overflow that hampers the system's reliability.
This paper proposes a novel, queue-aware FID framework that adapts the sampling
rate to maximize the FID performance while avoiding a queue overflow by
implementing the Lyapunov optimization. A preliminary evaluation via a
trace-based simulation confirms the effectiveness of the framework.

    

### [[2109.01232] A Study of Mixed Precision Strategies for GMRES on GPUs](http://arxiv.org/abs/2109.01232)


  Support for lower precision computation is becoming more common in
accelerator hardware due to lower power usage, reduced data movement and
increased computational performance. However, computational science and
engineering (CSE) problems require double precision accuracy in several
domains. This conflict between hardware trends and application needs has
resulted in a need for mixed precision strategies at the linear algebra
algorithms level if we want to exploit the hardware to its full potential while
meeting the accuracy requirements. In this paper, we focus on preconditioned
sparse iterative linear solvers, a key kernel in several CSE applications. We
present a study of mixed precision strategies for accelerating this kernel on
an NVIDIA V$100$ GPU with a Power 9 CPU. We seek the best methods for
incorporating multiple precisions into the GMRES linear solver; these include
iterative refinement and parallelizable preconditioners. Our work presents
strategies to determine when mixed precision GMRES will be effective and to
choose parameters for a mixed precision iterative refinement solver to achieve
better performance. We use an implementation that is based on the Trilinos
library and employs Kokkos Kernels for performance portability of linear
algebra kernels. Performance results demonstrate the promise of mixed precision
approaches and demonstrate even further improvements are possible by optimizing
low-level kernels.

    

### [[2109.01329] Achieving near native runtime performance and cross-platform performance portability for random number generation through SYCL interoperability](http://arxiv.org/abs/2109.01329)


  High-performance computing (HPC) is a major driver accelerating scientific
research and discovery, from quantum simulations to medical therapeutics. The
growing number of new HPC systems coming online are being furnished with
various hardware components, engineered by competing industry entities, each
having their own architectures and platforms to be supported. While the
increasing availability of these resources is in many cases pivotal to
successful science, even the largest collaborations lack the computational
expertise required for maximal exploitation of current hardware capabilities.
The need to maintain multiple platform-specific codebases further complicates
matters, potentially adding a constraint on the number of machines that can be
utilized. Fortunately, numerous programming models are under development that
aim to facilitate software solutions for heterogeneous computing. In this
paper, we leverage the SYCL programming model to demonstrate cross-platform
performance portability across heterogeneous resources. We detail our NVIDIA
and AMD random number generator extensions to the oneMKL open-source interfaces
library. Performance portability is measured relative to platform-specific
baseline applications executed on four major hardware platforms using two
different compilers supporting SYCL. The utility of our extensions are
exemplified in a real-world setting via a high-energy physics simulation
application. We show the performance of implementations that capitalize on SYCL
interoperability are at par with native implementations, attesting to the
cross-platform performance portability of a SYCL-based approach to scientific
codes.

    

### [[2109.01439] Continuous Tasks and the Chromatic Simplicial Approximation Theorem](http://arxiv.org/abs/2109.01439)


  The celebrated 1999 Asynchronous Computability Theorem (ACT) of Herlihy and
Shavit characterized the distributed tasks that are wait-free solvable, and
thus uncovered a deep connection with algebraic topology. We present a novel
interpretation of this theorem, through the notion of continuous task, defined
by an input/output specification that is a continuous function. To do so, we
introduce a chromatic version of a foundational result for algebraic topology:
the simplicial approximation theorem. In addition to providing a different
proof of the ACT, the notion of continuous task seems interesting in itself.
Indeed, besides the fact that certain distributed problems are naturally
specified by continuous functions, continuous tasks have an expressive power
that also allows to specify the density of desired outputs for each combination
of possible inputs,for example.

    

### [[2109.01611] Multi-model Machine Learning Inference Serving with GPU Spatial Partitioning](http://arxiv.org/abs/2109.01611)


  As machine learning techniques are applied to a widening range of
applications, high throughput machine learning (ML) inference servers have
become critical for online service applications. Such ML inference servers pose
two challenges: first, they must provide a bounded latency for each request to
support consistent service-level objective (SLO), and second, they can serve
multiple heterogeneous ML models in a system as certain tasks involve
invocation of multiple models and consolidating multiple models can improve
system utilization. To address the two requirements of ML inference servers,
this paper proposes a new ML inference scheduling framework for multi-model ML
inference servers. The paper first shows that with SLO constraints, current
GPUs are not fully utilized for ML inference tasks. To maximize the resource
efficiency of inference servers, a key mechanism proposed in this paper is to
exploit hardware support for spatial partitioning of GPU resources. With the
partitioning mechanism, a new abstraction layer of GPU resources is created
with configurable GPU resources. The scheduler assigns requests to virtual
GPUs, called gpu-lets, with the most effective amount of resources. The paper
also investigates a remedy for potential interference effects when two ML tasks
are running concurrently in a GPU. Our prototype implementation proves that
spatial partitioning enhances throughput by 102.6% on average while satisfying
SLOs.

    

### [[2005.12911] Symbolic and Structural Model-Checking](http://arxiv.org/abs/2005.12911)


  Brute-force model-checking consists in exhaustive exploration of the
state-space of a Petri net, and meets the dreaded state-space explosion
problem.
In contrast, this paper shows how to solve model-checking problems using a
combination of techniques that stay in complexity proportional to the size of
the net structure rather than to the state-space size.
We combine an SMT based over-approximation to prove that some behaviors are
unfeasible, an under-approximation using memory-less sampling of runs to find
witness traces or counter-examples, and a set of structural reduction rules
that can simplify both the system and the property.
This approach was able to win by a clear margin the model-checking contest
2020 for reachability queries as well as deadlock detection, thus demonstrating
the practical effectiveness and general applicability of the system of rules
presented in this paper.

    

### [[2101.12149] Porting WarpX to GPU-accelerated platforms](http://arxiv.org/abs/2101.12149)


  WarpX is a general purpose electromagnetic particle-in-cell code that was
originally designed to run on many-core CPU architectures. We describe the
strategy followed to allow WarpX to use the GPU-accelerated nodes on OLCF's
Summit supercomputer, a strategy we believe will extend to the upcoming
machines Frontier and Aurora. We summarize the challenges encountered, lessons
learned, and give current performance results on a series of relevant benchmark
problems.

    

### [[2102.02402] SAFELearning: Enable Backdoor Detectability In Federated Learning With Secure Aggregation](http://arxiv.org/abs/2102.02402)


  For model privacy, local model parameters in federated learning shall be
obfuscated before sent to the remote aggregator. This technique is referred to
as \emph{secure aggregation}. However, secure aggregation makes model poisoning
attacks such backdooring more convenient considering that existing anomaly
detection methods mostly require access to plaintext local models. This paper
proposes SAFELearning which supports backdoor detection for secure aggregation.
We achieve this through two new primitives - \emph{oblivious random grouping
(ORG)} and \emph{partial parameter disclosure (PPD)}. ORG partitions
participants into one-time random subgroups with group configurations oblivious
to participants; PPD allows secure partial disclosure of aggregated subgroup
models for anomaly detection without leaking individual model privacy.
SAFELearning can significantly reduce backdoor model accuracy without
jeopardizing the main task accuracy under common backdoor strategies. Extensive
experiments show SAFELearning is robust against malicious and faulty
participants, whilst being more efficient than the state-of-art secure
aggregation protocol in terms of both communication and computation costs.

    

### [[2109.01156] Challenges in Generalization in Open Domain Question Answering](http://arxiv.org/abs/2109.01156)


  Recent work on Open Domain Question Answering has shown that there is a large
discrepancy in model performance between novel test questions and those that
largely overlap with training questions. However, it is as of yet unclear which
aspects of novel questions that make them challenging. Drawing upon studies on
systematic generalization, we introduce and annotate questions according to
three categories that measure different levels and kinds of generalization:
training set overlap, compositional generalization (comp-gen), and novel entity
generalization (novel-entity). When evaluating six popular parametric and
non-parametric models, we find that for the established Natural Questions and
TriviaQA datasets, even the strongest model performance for
comp-gen/novel-entity is 13.1/5.4% and 9.6/1.5% lower compared to that for the
full test set -- indicating the challenge posed by these types of questions.
Furthermore, we show that whilst non-parametric models can handle questions
containing novel entities, they struggle with those requiring compositional
generalization. Through thorough analysis we find that key question difficulty
factors are: cascading errors from the retrieval component, frequency of
question pattern, and frequency of the entity.

    

### [[2109.01165] Black-Box Attacks on Sequential Recommenders via Data-Free Model Extraction](http://arxiv.org/abs/2109.01165)


  We investigate whether model extraction can be used to "steal" the weights of
sequential recommender systems, and the potential threats posed to victims of
such attacks. This type of risk has attracted attention in image and text
classification, but to our knowledge not in recommender systems. We argue that
sequential recommender systems are subject to unique vulnerabilities due to the
specific autoregressive regimes used to train them. Unlike many existing
recommender attackers, which assume the dataset used to train the victim model
is exposed to attackers, we consider a data-free setting, where training data
are not accessible. Under this setting, we propose an API-based model
extraction method via limited-budget synthetic data generation and knowledge
distillation. We investigate state-of-the-art models for sequential
recommendation and show their vulnerability under model extraction and
downstream attacks. We perform attacks in two stages. (1) Model extraction:
given different types of synthetic data and their labels retrieved from a
black-box recommender, we extract the black-box model to a white-box model via
distillation. (2) Downstream attacks: we attack the black-box model with
adversarial samples generated by the white-box recommender. Experiments show
the effectiveness of our data-free model extraction and downstream attacks on
sequential recommenders in both profile pollution and data poisoning settings.

    

### [[2109.01220] An Oracle and Observations for the OpenAI Gym / ALE Freeway Environment](http://arxiv.org/abs/2109.01220)


  The OpenAI Gym project contains hundreds of control problems whose goal is to
provide a testbed for reinforcement learning algorithms. One such problem is
Freeway-ram-v0, where the observations presented to the agent are 128 bytes of
RAM. While the goals of the project are for non-expert AI agents to solve the
control problems with general training, in this work, we seek to learn more
about the problem, so that we can better evaluate solutions. In particular, we
develop on oracle to play the game, so that we may have baselines for success.
We present details of the oracle, plus optimal game-playing situations that can
be used for training and testing AI agents.

    

### [[2109.01238] An Empirical Study on Leveraging Position Embeddings for Target-oriented Opinion Words Extraction](http://arxiv.org/abs/2109.01238)


  Target-oriented opinion words extraction (TOWE) (Fan et al., 2019b) is a new
subtask of target-oriented sentiment analysis that aims to extract opinion
words for a given aspect in text. Current state-of-the-art methods leverage
position embeddings to capture the relative position of a word to the target.
However, the performance of these methods depends on the ability to incorporate
this information into word representations. In this paper, we explore a variety
of text encoders based on pretrained word embeddings or language models that
leverage part-of-speech and position embeddings, aiming to examine the actual
contribution of each component in TOWE. We also adapt a graph convolutional
network (GCN) to enhance word representations by incorporating syntactic
information. Our experimental results demonstrate that BiLSTM-based models can
effectively encode position information into word representations while using a
GCN only achieves marginal gains. Interestingly, our simple methods outperform
several state-of-the-art complex neural structures.

    

### [[2109.01281] Symbol Emergence and The Solutions to Any Task](http://arxiv.org/abs/2109.01281)


  The following defines intent, an arbitrary task and its solutions, and then
argues that an agent which always constructs what is called an Intensional
Solution would qualify as artificial general intelligence. We then explain how
natural language may emerge and be acquired by such an agent, conferring the
ability to model the intent of other individuals labouring under similar
compulsions, because an abstract symbol system and the solution to a task are
one and the same.

    

### [[2109.01295] Information Symmetry Matters: A Modal-Alternating Propagation Network for Few-Shot Learning](http://arxiv.org/abs/2109.01295)


  Semantic information provides intra-class consistency and inter-class
discriminability beyond visual concepts, which has been employed in Few-Shot
Learning (FSL) to achieve further gains. However, semantic information is only
available for labeled samples but absent for unlabeled samples, in which the
embeddings are rectified unilaterally by guiding the few labeled samples with
semantics. Therefore, it is inevitable to bring a cross-modal bias between
semantic-guided samples and nonsemantic-guided samples, which results in an
information asymmetry problem. To address this problem, we propose a
Modal-Alternating Propagation Network (MAP-Net) to supplement the absent
semantic information of unlabeled samples, which builds information symmetry
among all samples in both visual and semantic modalities. Specifically, the
MAP-Net transfers the neighbor information by the graph propagation to generate
the pseudo-semantics for unlabeled samples guided by the completed visual
relationships and rectify the feature embeddings. In addition, due to the large
discrepancy between visual and semantic modalities, we design a Relation
Guidance (RG) strategy to guide the visual relation vectors via semantics so
that the propagated information is more beneficial. Extensive experimental
results on three semantic-labeled datasets, i.e., Caltech-UCSD-Birds 200-2011,
SUN Attribute Database, and Oxford 102 Flower, have demonstrated that our
proposed method achieves promising performance and outperforms the
state-of-the-art approaches, which indicates the necessity of information
symmetry.

    

### [[2109.01302] Self-Taught Cross-Domain Few-Shot Learning with Weakly Supervised Object Localization and Task-Decomposition](http://arxiv.org/abs/2109.01302)


  The domain shift between the source and target domain is the main challenge
in Cross-Domain Few-Shot Learning (CD-FSL). However, the target domain is
absolutely unknown during the training on the source domain, which results in
lacking directed guidance for target tasks. We observe that since there are
similar backgrounds in target domains, it can apply self-labeled samples as
prior tasks to transfer knowledge onto target tasks. To this end, we propose a
task-expansion-decomposition framework for CD-FSL, called Self-Taught (ST)
approach, which alleviates the problem of non-target guidance by constructing
task-oriented metric spaces. Specifically, Weakly Supervised Object
Localization (WSOL) and self-supervised technologies are employed to enrich
task-oriented samples by exchanging and rotating the discriminative regions,
which generates a more abundant task set. Then these tasks are decomposed into
several tasks to finish the task of few-shot recognition and rotation
classification. It helps to transfer the source knowledge onto the target tasks
and focus on discriminative regions. We conduct extensive experiments under the
cross-domain setting including 8 target domains: CUB, Cars, Places, Plantae,
CropDieases, EuroSAT, ISIC, and ChestX. Experimental results demonstrate that
the proposed ST approach is applicable to various metric-based models, and
provides promising improvements in CD-FSL.

    

### [[2109.01411] An Exploratory Study on Utilising the Web of Linked Data for Product Data Mining](http://arxiv.org/abs/2109.01411)


  The Linked Open Data practice has led to a significant growth of structured
data on the Web in the last decade. Such structured data describe real-world
entities in a machine-readable way, and have created an unprecedented
opportunity for research in the field of Natural Language Processing. However,
there is a lack of studies on how such data can be used, for what kind of
tasks, and to what extent they can be useful for these tasks. This work focuses
on the e-commerce domain to explore methods of utilising such structured data
to create language resources that may be used for product classification and
linking. We process billions of structured data points in the form of RDF
n-quads, to create multi-million words of product-related corpora that are
later used in three different ways for creating of language resources: training
word embedding models, continued pre-training of BERT-like language models, and
training Machine Translation models that are used as a proxy to generate
product-related keywords. Our evaluation on an extensive set of benchmarks
shows word embeddings to be the most reliable and consistent method to improve
the accuracy on both tasks (with up to 6.9 percentage points in macro-average
F1 on some datasets). The other two methods however, are not as useful. Our
analysis shows that this could be due to a number of reasons, including the
biased domain representation in the structured data and lack of vocabulary
coverage. We share our datasets and discuss how our lessons learned could be
taken forward to inform future research in this direction.

    

### [[2109.01443] The Impact of Algorithmic Risk Assessments on Human Predictions and its Analysis via Crowdsourcing Studies](http://arxiv.org/abs/2109.01443)


  As algorithmic risk assessment instruments (RAIs) are increasingly adopted to
assist decision makers, their predictive performance and potential to promote
inequity have come under scrutiny. However, while most studies examine these
tools in isolation, researchers have come to recognize that assessing their
impact requires understanding the behavior of their human interactants. In this
paper, building off of several recent crowdsourcing works focused on criminal
justice, we conduct a vignette study in which laypersons are tasked with
predicting future re-arrests. Our key findings are as follows: (1) Participants
often predict that an offender will be rearrested even when they deem the
likelihood of re-arrest to be well below 50%; (2) Participants do not anchor on
the RAI's predictions; (3) The time spent on the survey varies widely across
participants and most cases are assessed in less than 10 seconds; (4) Judicial
decisions, unlike participants' predictions, depend in part on factors that are
orthogonal to the likelihood of re-arrest. These results highlight the
influence of several crucial but often overlooked design decisions and concerns
around generalizability when constructing crowdsourcing studies to analyze the
impacts of RAIs.

    

### [[2109.01517] A brief history of AI: how to prevent another winter (a critical review)](http://arxiv.org/abs/2109.01517)


  The field of artificial intelligence (AI), regarded as one of the most
enigmatic areas of science, has witnessed exponential growth in the past decade
including a remarkably wide array of applications, having already impacted our
everyday lives. Advances in computing power and the design of sophisticated AI
algorithms have enabled computers to outperform humans in a variety of tasks,
especially in the areas of computer vision and speech recognition. Yet, AI's
path has never been smooth, having essentially fallen apart twice in its
lifetime ('winters' of AI), both after periods of popular success ('summers' of
AI). We provide a brief rundown of AI's evolution over the course of decades,
highlighting its crucial moments and major turning points from inception to the
present. In doing so, we attempt to learn, anticipate the future, and discuss
what steps may be taken to prevent another 'winter'.

    

### [[2109.01537] A Longitudinal Multi-modal Dataset for Dementia Monitoring and Diagnosis](http://arxiv.org/abs/2109.01537)


  Dementia is a family of neurogenerative conditions affecting memory and
cognition in an increasing number of individuals in our globally aging
population. Automated analysis of language, speech and paralinguistic
indicators have been gaining popularity as potential indicators of cognitive
decline. Here we propose a novel longitudinal multi-modal dataset collected
from people with mild dementia and age matched controls over a period of
several months in a natural setting. The multi-modal data consists of spoken
conversations, a subset of which are transcribed, as well as typed and written
thoughts and associated extra-linguistic information such as pen strokes and
keystrokes. We describe the dataset in detail and proceed to focus on a task
using the speech modality. The latter involves distinguishing controls from
people with dementia by exploiting the longitudinal nature of the data. Our
experiments showed significant differences in how the speech varied from
session to session in the control and dementia groups.

    

### [[2109.01544] Ontology-driven Knowledge Graph for Android Malware](http://arxiv.org/abs/2109.01544)


  We present MalONT2.0 -- an ontology for malware threat intelligence
\cite{rastogi2020malont}. New classes (attack patterns, infrastructural
resources to enable attacks, malware analysis to incorporate static analysis,
and dynamic analysis of binaries) and relations have been added following a
broadened scope of core competency questions. MalONT2.0 allows researchers to
extensively capture all requisite classes and relations that gather semantic
and syntactic characteristics of an android malware attack. This ontology forms
the basis for the malware threat intelligence knowledge graph, MalKG, which we
exemplify using three different, non-overlapping demonstrations. Malware
features have been extracted from CTI reports on android threat intelligence
shared on the Internet and written in the form of unstructured text. Some of
these sources are blogs, threat intelligence reports, tweets, and news
articles. The smallest unit of information that captures malware features is
written as triples comprising head and tail entities, each connected with a
relation. In the poster and demonstration, we discuss MalONT2.0, MalKG, as well
as the dynamically growing knowledge graph, TINKER.

    

### [[2109.01552] Situated Conditional Reasoning](http://arxiv.org/abs/2109.01552)


  Conditionals are useful for modelling, but are not always sufficiently
expressive for capturing information accurately. In this paper we make the case
for a form of conditional that is situation-based. These conditionals are more
expressive than classical conditionals, are general enough to be used in
several application domains, and are able to distinguish, for example, between
expectations and counterfactuals. Formally, they are shown to generalise the
conditional setting in the style of Kraus, Lehmann, and Magidor. We show that
situation-based conditionals can be described in terms of a set of rationality
postulates. We then propose an intuitive semantics for these conditionals, and
present a representation result which shows that our semantic construction
corresponds exactly to the description in terms of postulates. With the
semantics in place, we proceed to define a form of entailment for situated
conditional knowledge bases, which we refer to as minimal closure. It is
reminiscent of and, indeed, inspired by, the version of entailment for
propositional conditional knowledge bases known as rational closure. Finally,
we proceed to show that it is possible to reduce the computation of minimal
closure to a series of propositional entailment and satisfiability checks.
While this is also the case for rational closure, it is somewhat surprising
that the result carries over to minimal closure.

    

### [[2109.01575] Continuous-Time Behavior Trees as Discontinuous Dynamical Systems](http://arxiv.org/abs/2109.01575)


  Behavior trees represent a hierarchical and modular way of combining several
low-level control policies into a high-level task-switching policy. Hybrid
dynamical systems can also be seen in terms of task switching between different
policies, and therefore several comparisons between behavior trees and hybrid
dynamical systems have been made, but only informally, and only in discrete
time. A formal continuous-time formulation of behavior trees has been lacking.
Additionally, convergence analyses of specific classes of behavior tree designs
have been made, but not for general designs.
In this letter, we provide the first continuous-time formulation of behavior
trees, show that they can be seen as discontinuous dynamical systems (a
subclass of hybrid dynamical systems), which enables the application of
existence and uniqueness results to behavior trees, and finally, provide
sufficient conditions under which such systems will converge to a desired
region of the state space for general designs. With these results, a large body
of results on continuous-time dynamical systems can be brought to use when
designing behavior tree controllers.

    

### [[2109.01583] Learning from Multiple Noisy Augmented Data Sets for Better Cross-Lingual Spoken Language Understanding](http://arxiv.org/abs/2109.01583)


  Lack of training data presents a grand challenge to scaling out spoken
language understanding (SLU) to low-resource languages. Although various data
augmentation approaches have been proposed to synthesize training data in
low-resource target languages, the augmented data sets are often noisy, and
thus impede the performance of SLU models. In this paper we focus on mitigating
noise in augmented data. We develop a denoising training approach. Multiple
models are trained with data produced by various augmented methods. Those
models provide supervision signals to each other. The experimental results show
that our method outperforms the existing state of the art by 3.05 and 4.24
percentage points on two benchmark datasets, respectively. The code will be
made open sourced on github.

    

### [[2109.01594] Super Neurons](http://arxiv.org/abs/2109.01594)


  Operational Neural Networks (ONNs) are new generation network models that can
perform any (non-linear) transformation with a proper combination of "nodal"
and "pool" operators. However, they still have a certain restriction, which is
the sole usage of a single nodal operator for all (synaptic) connections of
each neuron. The idea behind the "generative neurons" was born as a remedy for
this restriction where each nodal operator can be "customized" during the
training in order to maximize the learning performance. Self-Organized ONNs
(Self-ONNs) composed with the generative neurons can achieve an utmost level of
diversity even with a compact configuration; however, it still suffers from the
last property that was inherited from the CNNs: localized kernel operations
which imposes a severe limitation to the information flow between layers. It
is, therefore, desirable for the neurons to gather information from a larger
area in the previous layer maps without increasing the kernel size. For certain
applications, it might be even more desirable "to learn" the kernel locations
of each connection during the training process along with the customized nodal
operators so that both can be optimized simultaneously. This study introduces
the super (generative) neuron models that can accomplish this without altering
the kernel sizes and will enable a significant diversity in terms of
information flow. The two models of super neurons proposed in this study vary
on the localization process of the kernels: i) randomly localized kernels
within a bias range set for each layer, ii) optimized locations of each kernel
during the Back-Propagation (BP) training. The extensive set of comparative
evaluations show that Self-ONNs with super-neurons can indeed achieve a
superior learning and generalization capability without any significant rise of
the computational complexity.

    

### [[2109.01634] Integration of Data and Theory for Accelerated Derivable Symbolic Discovery](http://arxiv.org/abs/2109.01634)


  Scientists have long aimed to discover meaningful equations which accurately
describe data. Machine learning algorithms automate construction of accurate
data-driven models, but ensuring that these are consistent with existing
knowledge is a challenge. We developed a methodology combining automated
theorem proving with symbolic regression, enabling principled derivations of
laws of nature. We demonstrate this for Kepler's third law, Einstein's
relativistic time dilation, and Langmuir's theory of adsorption, in each case,
automatically connecting experimental data with background theory. The
combination of logical reasoning with machine learning provides generalizable
insights into key aspects of the natural phenomena.

    

### [[2109.01653] CREAK: A Dataset for Commonsense Reasoning over Entity Knowledge](http://arxiv.org/abs/2109.01653)


  Most benchmark datasets targeting commonsense reasoning focus on everyday
scenarios: physical knowledge like knowing that you could fill a cup under a
waterfall [Talmor et al., 2019], social knowledge like bumping into someone is
awkward [Sap et al., 2019], and other generic situations. However, there is a
rich space of commonsense inferences anchored to knowledge about specific
entities: for example, deciding the truthfulness of a claim "Harry Potter can
teach classes on how to fly on a broomstick." Can models learn to combine
entity knowledge with commonsense reasoning in this fashion? We introduce
CREAK, a testbed for commonsense reasoning about entity knowledge, bridging
fact-checking about entities (Harry Potter is a wizard and is skilled at riding
a broomstick) with commonsense inferences (if you're good at a skill you can
teach others how to do it). Our dataset consists of 13k human-authored English
claims about entities that are either true or false, in addition to a small
contrast set. Crowdworkers can easily come up with these statements and human
performance on the dataset is high (high 90s); we argue that models should be
able to blend entity knowledge and commonsense reasoning to do well here. In
our experiments, we focus on the closed-book setting and observe that a
baseline model finetuned on existing fact verification benchmark struggles on
CREAK. Training a model on CREAK improves accuracy by a substantial margin, but
still falls short of human performance. Our benchmark provides a unique probe
into natural language understanding models, testing both its ability to
retrieve facts (e.g., who teaches at the University of Chicago?) and unstated
commonsense knowledge (e.g., butlers do not yell at guests).

    

### [[2001.11695] Modeling Perception Errors towards Robust Decision Making in Autonomous Vehicles](http://arxiv.org/abs/2001.11695)


  Sensing and Perception (S&P) is a crucial component of an autonomous system
(such as a robot), especially when deployed in highly dynamic environments
where it is required to react to unexpected situations. This is particularly
true in case of Autonomous Vehicles (AVs) driving on public roads. However, the
current evaluation metrics for perception algorithms are typically designed to
measure their accuracy per se and do not account for their impact on the
decision making subsystem(s). This limitation does not help developers and
third party evaluators to answer a critical question: is the performance of a
perception subsystem sufficient for the decision making subsystem to make
robust, safe decisions? In this paper, we propose a simulation-based
methodology towards answering this question. At the same time, we show how to
analyze the impact of different kinds of sensing and perception errors on the
behavior of the autonomous system.

    

### [[2102.02441] Persistent Rule-based Interactive Reinforcement Learning](http://arxiv.org/abs/2102.02441)


  Interactive reinforcement learning has allowed speeding up the learning
process in autonomous agents by including a human trainer providing extra
information to the agent in real-time. Current interactive reinforcement
learning research has been limited to real-time interactions that offer
relevant user advice to the current state only. Additionally, the information
provided by each interaction is not retained and instead discarded by the agent
after a single-use. In this work, we propose a persistent rule-based
interactive reinforcement learning approach, i.e., a method for retaining and
reusing provided knowledge, allowing trainers to give general advice relevant
to more than just the current state. Our experimental results show persistent
advice substantially improves the performance of the agent while reducing the
number of interactions required for the trainer. Moreover, rule-based advice
shows similar performance impact as state-based advice, but with a
substantially reduced interaction count.

    

### [[2102.11361] The FaCells. An Exploratory Study about LSTM Layers on Face Sketches Classifiers](http://arxiv.org/abs/2102.11361)


  Lines are human mental abstractions. A bunch of lines may form a drawing. A
set of drawings can feed an LSTM network input layer, considering each draw as
a list of lines and a line a list of points. This paper proposes the pointless
motive to classify the gender of celebrities' portraits as an excuse for
exploration in a broad, more artistic sense. Investigation results drove
compelling ideas here discussed. The experiments compared different ways to
represent draws to be input in a network and showed that an absolute format of
coordinates (x, y) was a better performer than a relative one (Dx, Dy) with
respect to prior points, most frequent in the reviewed literature. Experiments
also showed that, due to the recurrent nature of LSTMs, the order of lines
forming a drawing is a relevant factor for input in an LSTM classifier not
studied before. A minimum 'pencil' traveled length criteria for line ordering
proved suitable, possible by reducing it to a TSP particular instance. The best
configuration for gender classification appears with an LSTM layer that returns
the hidden state value for each input point step, followed by a global average
layer along the sequence, before the output dense layer. That result guided the
idea of removing the average in the network pipeline and return a per-point
attribute score just by adjusting tensors dimensions. With this trick, the
model detects an attribute in a drawing and also recognizes the points linked
to it. Moreover, by overlapping filtered lines of portraits, an attribute's
visual essence is depicted. Meet the FaCells.

    

### [[2104.07782] Sublanguage: A Serious Issue Affects Pretrained Models in Legal Domain](http://arxiv.org/abs/2104.07782)


  Legal English is a sublanguage that is important for everyone but not for
everyone to understand. Pretrained models have become best practices among
current deep learning approaches for different problems. It would be a waste or
even a danger if these models were applied in practice without knowledge of the
sublanguage of the law. In this paper, we raise the issue and propose a trivial
solution by introducing BERTLaw a legal sublanguage pretrained model. The
paper's experiments demonstrate the superior effectiveness of the method
compared to the baseline pretrained model

    

### [[2105.14878] Verdi: Quality Estimation and Error Detection for Bilingual Corpora](http://arxiv.org/abs/2105.14878)


  Translation Quality Estimation is critical to reducing post-editing efforts
in machine translation and to cross-lingual corpus cleaning. As a research
problem, quality estimation (QE) aims to directly estimate the quality of
translation in a given pair of source and target sentences, and highlight the
words that need corrections, without referencing to golden translations. In
this paper, we propose Verdi, a novel framework for word-level and
sentence-level post-editing effort estimation for bilingual corpora. Verdi
adopts two word predictors to enable diverse features to be extracted from a
pair of sentences for subsequent quality estimation, including a
transformer-based neural machine translation (NMT) model and a pre-trained
cross-lingual language model (XLM). We exploit the symmetric nature of
bilingual corpora and apply model-level dual learning in the NMT predictor,
which handles a primal task and a dual task simultaneously with weight sharing,
leading to stronger context prediction ability than single-direction NMT
models. By taking advantage of the dual learning scheme, we further design a
novel feature to directly encode the translated target information without
relying on the source context. Extensive experiments conducted on WMT20 QE
tasks demonstrate that our method beats the winner of the competition and
outperforms other baseline methods by a great margin. We further use the
sentence-level scores provided by Verdi to clean a parallel corpus and observe
benefits on both model performance and training efficiency.

    

### [[2109.01321] Indexing Context-Sensitive Reachability](http://arxiv.org/abs/2109.01321)


  Many context-sensitive data flow analyses can be formulated as a variant of
the all-pairs Dyck-CFL reachability problem, which, in general, is of sub-cubic
time complexity and quadratic space complexity. Such high complexity
significantly limits the scalability of context-sensitive data flow analysis
and is not affordable for analyzing large-scale software. This paper presents
\textsc{Flare}, a reduction from the CFL reachability problem to the
conventional graph reachability problem for context-sensitive data flow
analysis. This reduction allows us to benefit from recent advances in
reachability indexing schemes, which often consume almost linear space for
answering reachability queries in almost constant time. We have applied our
reduction to a context-sensitive alias analysis and a context-sensitive
information-flow analysis for C/C++ programs. Experimental results on standard
benchmarks and open-source software demonstrate that we can achieve orders of
magnitude speedup at the cost of only moderate space to store the indexes. The
implementation of our approach is publicly available.

    

### [[2109.01386] Vivienne: Relational Verification of Cryptographic Implementations in WebAssembly](http://arxiv.org/abs/2109.01386)


  This paper explores the use of relational symbolic execution to counter
timing side channels in WebAssembly programs. We design and implement Vivienne,
an open-source tool to automatically analyze WebAssembly cryptographic
libraries for constant-time violations. Our approach features various
optimizations that leverage the structure of WebAssembly and automated theorem
provers, including support for loops via relational invariants. We evaluate
Vivienne on 57 real-world cryptographic implementations, including a previously
unverified implementation of the HACL* library in WebAssembly. The results
indicate that Vivienne is a practical solution for constant-time analysis of
cryptographic libraries in WebAssembly.

    

### [[2109.01483] A Survey of the Proof-Theoretic Foundations of Logic Programming](http://arxiv.org/abs/2109.01483)


  Several formal systems, such as resolution and minimal model semantics,
provide a framework for logic programming. In this paper, we will survey the
use of structural proof theory as an alternative foundation. Researchers have
been using this foundation for the past 35 years to elevate logic programming
from its roots in first-order classical logic into higher-order versions of
both intuitionistic and linear logic. These more expressive logic programming
languages allow for capturing stateful computations and rich forms of
abstractions, including higher-order programming, modularity, and abstract data
types. Term-level bindings are another kind of abstraction, and these are given
an elegant and direct treatment within both proof theory and these extended
logic programming languages. Logic programming has also inspired new results in
proof theory, such as polarity and focused proofs. These recent results have
also provided a high-level means for presenting the differences between
forward-chaining and backward-chaining style inferences. Anchoring logic
programming in proof theory has also helped identify its connections and
differences with functional programming, deductive databases, and model
checking.

    

### [<title>XGBoost4J (with Spark) 0.9 unable to train with udt column? - XGBoost</title>](https://discuss.xgboost.ai/t/xgboost4j-with-spark-0-9-unable-to-train-with-udt-column/1319/22)

### [<title>Save and Load model in XGBoost4j with Databricks DBFS - XGBoost</title>](https://discuss.xgboost.ai/t/save-and-load-model-in-xgboost4j-with-databricks-dbfs/1643/15)