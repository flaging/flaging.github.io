
## 2021-12-17

### [<title>Does one need to build from source for multi-GPU support? - XGBoost</title>](https://discuss.xgboost.ai/t/does-one-need-to-build-from-source-for-multi-gpu-support/2606/2)

### [[2112.08590] Federated 3GPP Mobile Edge Computing Systems: A Transparent Proxy for Third Party Authentication with Application Mobility Support](http://arxiv.org/abs/2112.08590)


  Multi-Access or Mobile Edge Computing (MEC) is being deployed by 4G/5G
operators to provide computational services at lower latencies. Federating MECs
across operators expands capability, capacity, and coverage but gives rise to
two issues - third-party authentication and application mobility - for
continuous service during roaming without re-authentication. In this work, we
propose a Federated State transfer and 3rd-party Authentication (FS3A)
mechanism that uses a transparent proxy to transfer the information of both
authentication and application state across operators to resolve these issues.
The FS3A proxy is kept transparent, with virtual counterparts, to avoid any
changes to the existing MEC and cellular architectures. FS3A provides users
with a token, when authenticated by an MEC, which can be reused across
operators for faster authentication. Prefetching of subscription and state is
also proposed to further reduce the authentication and application mobility
latencies. We evaluated FS3A on an OpenAirInterface (OAI)-based testbed and the
results show that token reuse and subscription prefetching reduce the
authentication latency by 53-65%, compared to complete re-authentication, while
state prefetching reduces application mobility latency by 51-91%, compared to
no prefetching. Overall, FS3A reduces the service interruption time by 33%,
compared to no token reuse and prefetching.

    

### [[2112.08686] Ruta: Dis-aggregated routing system over multi-cloud](http://arxiv.org/abs/2112.08686)


  Over the years, the SDN evolution create multiple overlay technologies which
is inefficient and hard to deploy end-to-end traffic engineering services, Ruta
is designed as an unified encapsulation with Segment Routing, Crypto and
NAT-Traversal capabilities over UDP.
Ruta could be deployed as a cloud native SDN platform globally over
multi-cloud and integrated with each applications on transport layer, which
provide nearly zero loss and almost less than 200ms latency to access anywhere
in the world over internet.

    

### [[2112.08720] Improvement of Indoor Radio Coverage at 60 GHz in NLOS Configuration](http://arxiv.org/abs/2112.08720)


  In the current development of new technologies, the world of communications
is experiencing significant growth thanks to the integration of wireless
communications in millimeter band. In this context, the purpose of this paper
is to assess the contribution that the use of a metallic reflector panel can
introduce to indoor radio coverage at 60 GHz in an NLOS (corridor)
configuration. The results obtained by measurement show that the use of such a
panel can lead to a significant path loss reduction up to 12 dB, which improves
the reception power.

    

### [[2112.08882] BitTorrent is Apt for Geophysical Data Collection and Distribution](http://arxiv.org/abs/2112.08882)


  This article covers a nouveau idea of how to collect and handle geophysical
data with a peer-to-peer network in near real-time. The text covers a brief
introduction to the cause, the technology, and the particular case of
collecting data from GNSS stations. We describe the proof-of-concept
implementation that has been tested. The test was conducted with an
experimental GNSS station and a data aggregation facility. In the test,
original raw GNSS signal measurements were transferred to the data aggregation
center and subsequently to the consumer. Our implementation utilized BitTorrent
to communicate and transfer data. The solution could be used to establish the
majority of data aggregation centers activities to provide fast, reliable, and
transparent real-time data handling experience to the scientific community.

    

### [[2112.08988] Interference Suppression Using Deep Learning: Current Approaches and Open Challenges](http://arxiv.org/abs/2112.08988)


  In light of the finite nature of the wireless spectrum and the increasing
demand for spectrum use arising from recent technological breakthroughs in
wireless communication, the problem of interference continues to persist.
Despite recent advancements in resolving interference issues, interference
still presents a difficult challenge to effective usage of the spectrum. This
is partly due to the rise in the use of license-free and managed shared bands
for Wi-Fi, long term evolution (LTE) unlicensed (LTE-U), LTE licensed assisted
access (LAA), 5G NR, and other opportunistic spectrum access solutions. As a
result of this, the need for efficient spectrum usage schemes that are robust
against interference has never been more important. In the past, most solutions
to interference have addressed the problem by using avoidance techniques as
well as non-AI mitigation approaches (for example, adaptive filters). The key
downside to non-AI techniques is the need for domain expertise in the
extraction or exploitation of signal features such as cyclostationarity,
bandwidth and modulation of the interfering signals. More recently, researchers
have successfully explored AI/ML enabled physical (PHY) layer techniques,
especially deep learning which reduces or compensates for the interfering
signal instead of simply avoiding it. The underlying idea of ML based
approaches is to learn the interference or the interference characteristics
from the data, thereby sidelining the need for domain expertise in suppressing
the interference. In this paper, we review a wide range of techniques that have
used deep learning to suppress interference. We provide comparison and
guidelines for many different types of deep learning techniques in interference
suppression. In addition, we highlight challenges and potential future research
directions for the successful adoption of deep learning in interference
suppression.

    

### [[2112.09067] Open-Source Software Radio Platform for Research on Cellular Networked UAVs -- It Works!](http://arxiv.org/abs/2112.09067)


  Cellular network-connected unmanned aerial vehicles (UAVs) experience
different radio propagation conditions than radio nodes on the ground.
Therefore, it has become critical to investigate the performance of aerial
radios, both theoretically and through field trials. In this paper, we consider
low-altitude aerial nodes that are served by an experimental cellular network.
We provide a detailed description of the hardware and software components
needed for establishing a broadband wireless testbed for UAV communications
research using software radios. Results show that a testbed for innovation in
UAV communications and networking is feasible with commercial off-the-shelf
hardware, open-source software, and low-power signaling.

    

### [[2112.09072] Sensor Sampling Trade-Offs for Air Quality Monitoring With Low-Cost Sensors](http://arxiv.org/abs/2112.09072)


  The calibration of low-cost sensors using machine learning techniques is a
methodology widely used nowadays. Although many challenges remain to be solved
in the deployment of low-cost sensors for air quality monitoring, low-cost
sensors have been shown to be useful in conjunction with high-precision
instrumentation. Thus, most research is focused on the application of different
calibration techniques using machine learning. Nevertheless, the successful
application of these models depends on the quality of the data obtained by the
sensors, and very little attention has been paid to the whole data gathering
process, from sensor sampling and data pre-processing, to the calibration of
the sensor itself. In this article, we show the main sensor sampling
parameters, with their corresponding impact on the quality of the resulting
machine learning-based sensor calibration and their impact on energy
consumption, thus showing the existing trade-offs. Finally, the results on an
experimental node show the impact of the data sampling strategy in the
calibration of tropospheric ozone, nitrogen dioxide and nitrogen monoxide
low-cost sensors. Specifically, we show how a sampling strategy that minimizes
the duty cycle of the sensing subsystem can reduce power consumption while
maintaining data quality.

    

### [[2006.14058] Anycast Agility: Network Playbooks to Fight DDoS](http://arxiv.org/abs/2006.14058)


  IP anycast is used for services such as DNS and Content Delivery Networks
(CDN) to provide the capacity to handle Distributed Denial-of-Service (DDoS)
attacks. During a DDoS attack service operators redistribute traffic between
anycast sites to take advantage of sites with unused or greater capacity.
Depending on site traffic and attack size, operators may instead concentrate
attackers in a few sites to preserve operation in others. Operators use these
actions during attacks, but how to do so has not been described systematically
or publicly. This paper describes several methods to use BGP to shift traffic
when under DDoS, and shows that a response playbook can provide a menu of
responses that are options during an attack. To choose an appropriate response
from this playbook, we also describe a new method to estimate true attack size,
even though the operator's view during the attack is incomplete. Finally,
operator choices are constrained by distributed routing policies, and not all
are helpful. We explore how specific anycast deployment can constrain options
in this playbook, and are the first to measure how generally applicable they
are across multiple anycast networks.

    

### [[2007.09948] Towards Joint Learning of Optimal MAC Signaling and Wireless Channel Access](http://arxiv.org/abs/2007.09948)


  Communication protocols are the languages used by network nodes. Before a
user equipment (UE) can exchange data with a base station (BS), it must first
negotiate the conditions and parameters for that transmission. This negotiation
is supported by signaling messages at all layers of the protocol stack. Each
year, the mobile communications industry defines and standardizes these
messages, which are designed by humans during lengthy technical (and often
political) debates. Following this standardization effort, the development
phase begins, wherein the industry interprets and implements the resulting
standards. But is this massive development undertaking the only way to
implement a given protocol? We address the question of whether radios can learn
a pre-given target protocol as an intermediate step towards evolving their own.
Furthermore, we train cellular radios to emerge a channel access policy that
performs optimally under the constraints of the target protocol. We show that
multi-agent reinforcement learning (MARL) and learning-to-communicate (L2C)
techniques achieve this goal with gains over expert systems. Finally, we
provide insight into the transferability of these results to scenarios never
seen during training.

    

### [[2112.08360] How to Learn and Represent Abstractions: An Investigation using Symbolic Alchemy](http://arxiv.org/abs/2112.08360)


  Alchemy is a new meta-learning environment rich enough to contain interesting
abstractions, yet simple enough to make fine-grained analysis tractable.
Further, Alchemy provides an optional symbolic interface that enables meta-RL
research without a large compute budget. In this work, we take the first steps
toward using Symbolic Alchemy to identify design choices that enable deep-RL
agents to learn various types of abstraction. Then, using a variety of
behavioral and introspective analyses we investigate how our trained agents use
and represent abstract task variables, and find intriguing connections to the
neuroscience of abstraction. We conclude by discussing the next steps for using
meta-RL and Alchemy to better understand the representation of abstract
variables in the brain.

    

### [[2112.08361] Deep Generative Models for Vehicle Speed Trajectories](http://arxiv.org/abs/2112.08361)


  Generating realistic vehicle speed trajectories is a crucial component in
evaluating vehicle fuel economy and in predictive control of self-driving cars.
Traditional generative models rely on Markov chain methods and can produce
accurate synthetic trajectories but are subject to the curse of dimensionality.
They do not allow to include conditional input variables into the generation
process. In this paper, we show how extensions to deep generative models allow
accurate and scalable generation. Proposed architectures involve recurrent and
feed-forward layers and are trained using adversarial techniques. Our models
are shown to perform well on generating vehicle trajectories using a model
trained on GPS data from Chicago metropolitan area.

    

### [[2112.08363] Performance or Trust? Why Not Both. Deep AUC Maximization with Self-Supervised Learning for COVID-19 Chest X-ray Classifications](http://arxiv.org/abs/2112.08363)


  Effective representation learning is the key in improving model performance
for medical image analysis. In training deep learning models, a compromise
often must be made between performance and trust, both of which are essential
for medical applications. Moreover, models optimized with cross-entropy loss
tend to suffer from unwarranted overconfidence in the majority class and
over-cautiousness in the minority class. In this work, we integrate a new
surrogate loss with self-supervised learning for computer-aided screening of
COVID-19 patients using radiography images. In addition, we adopt a new
quantification score to measure a model's trustworthiness. Ablation study is
conducted for both the performance and the trust on feature learning methods
and loss functions. Comparisons show that leveraging the new surrogate loss on
self-supervised models can produce label-efficient networks that are both
high-performing and trustworthy.

    

### [[2112.08364] Data Valuation for Vertical Federated Learning: An Information-Theoretic Approach](http://arxiv.org/abs/2112.08364)


  Federated learning (FL) is a promising machine learning paradigm that enables
cross-party data collaboration for real-world AI applications in a
privacy-preserving and law-regulated way. How to valuate parties' data is a
critical but challenging FL issue. In the literature, data valuation either
relies on running specific models for a given task or is just task irrelevant;
however, it is often requisite for party selection given a specific task when
FL models have not been determined yet. This work thus fills the gap and
proposes \emph{FedValue}, to our best knowledge, the first privacy-preserving,
task-specific but model-free data valuation method for vertical FL tasks.
Specifically, FedValue incorporates a novel information-theoretic metric termed
Shapley-CMI to assess data values of multiple parties from a game-theoretic
perspective. Moreover, a novel server-aided federated computation mechanism is
designed to compute Shapley-CMI and meanwhile protects each party from data
leakage. We also propose several techniques to accelerate Shapley-CMI
computation in practice. Extensive experiments on six open datasets validate
the effectiveness and efficiency of FedValue for data valuation of vertical FL
tasks. In particular, Shapley-CMI as a model-free metric performs comparably
with the measures that depend on running an ensemble of well-performing models.

    

### [[2112.08366] AGMI: Attention-Guided Multi-omics Integration for Drug Response Prediction with Graph Neural Networks](http://arxiv.org/abs/2112.08366)


  Accurate drug response prediction (DRP) is a crucial yet challenging task in
precision medicine. This paper presents a novel Attention-Guided Multi-omics
Integration (AGMI) approach for DRP, which first constructs a Multi-edge Graph
(MeG) for each cell line, and then aggregates multi-omics features to predict
drug response using a novel structure, called Graph edge-aware Network (GeNet).
For the first time, our AGMI approach explores gene constraint based
multi-omics integration for DRP with the whole-genome using GNNs. Empirical
experiments on the CCLE and GDSC datasets show that our AGMI largely
outperforms state-of-the-art DRP methods by 8.3%--34.2% on four metrics. Our
data and code are available at this https URL.

    

### [[2112.08369] Feature-Attending Recurrent Modules for Generalization in Reinforcement Learning](http://arxiv.org/abs/2112.08369)


  Deep reinforcement learning (Deep RL) has recently seen significant progress
in developing algorithms for generalization. However, most algorithms target a
single type of generalization setting. In this work, we study generalization
across three disparate task structures: (a) tasks composed of spatial and
temporal compositions of regularly occurring object motions; (b) tasks composed
of active perception of and navigation towards regularly occurring 3D objects;
and (c) tasks composed of remembering goal-information over sequences of
regularly occurring object-configurations. These diverse task structures all
share an underlying idea of compositionality: task completion always involves
combining recurring segments of task-oriented perception and behavior. We
hypothesize that an agent can generalize within a task structure if it can
discover representations that capture these recurring task-segments. For our
tasks, this corresponds to representations for recognizing individual object
motions, for navigation towards 3D objects, and for navigating through
object-configurations. Taking inspiration from cognitive science, we term
representations for recurring segments of an agent's experience, "perceptual
schemas". We propose Feature Attending Recurrent Modules (FARM), which learns a
state representation where perceptual schemas are distributed across multiple,
relatively small recurrent modules. We compare FARM to recurrent architectures
that leverage spatial attention, which reduces observation features to a
weighted average over spatial positions. Our experiments indicate that our
feature-attention mechanism better enables FARM to generalize across the
diverse object-centric domains we study.

    

### [[2112.08370] Lifelong Generative Modelling Using Dynamic Expansion Graph Model](http://arxiv.org/abs/2112.08370)


  Variational Autoencoders (VAEs) suffer from degenerated performance, when
learning several successive tasks. This is caused by catastrophic forgetting.
In order to address the knowledge loss, VAEs are using either Generative Replay
(GR) mechanisms or Expanding Network Architectures (ENA). In this paper we
study the forgetting behaviour of VAEs using a joint GR and ENA methodology, by
deriving an upper bound on the negative marginal log-likelihood. This
theoretical analysis provides new insights into how VAEs forget the previously
learnt knowledge during lifelong learning. The analysis indicates the best
performance achieved when considering model mixtures, under the ENA framework,
where there are no restrictions on the number of components. However, an
ENA-based approach may require an excessive number of parameters. This
motivates us to propose a novel Dynamic Expansion Graph Model (DEGM). DEGM
expands its architecture, according to the novelty associated with each new
databases, when compared to the information already learnt by the network from
previous tasks. DEGM training optimizes knowledge structuring, characterizing
the joint probabilistic representations corresponding to the past and more
recently learned tasks. We demonstrate that DEGM guarantees optimal performance
for each task while also minimizing the required number of parameters.
Supplementary materials (SM) and source code are available in
this https URL.

    

### [[2112.08391] Breeding realistic D-brane models](http://arxiv.org/abs/2112.08391)


  Intersecting branes provide a useful mechanism to construct particle physics
models from string theory with a wide variety of desirable characteristics. The
landscape of such models can be enormous, and navigating towards regions which
are most phenomenologically interesting is potentially challenging. Machine
learning techniques can be used to efficiently construct large numbers of
consistent and phenomenologically desirable models. In this work we phrase the
problem of finding consistent intersecting D-brane models in terms of genetic
algorithms, which mimic natural selection to evolve a population collectively
towards optimal solutions. For a four-dimensional ${\cal N}=1$ supersymmetric
type IIA orientifold with intersecting D6-branes, we demonstrate that
$\mathcal{O}(10^6)$ unique, fully consistent models can be easily constructed,
and, by a judicious choice of search environment and hyper-parameters,
$\mathcal{O}(30\%)$ of the found models contain the desired Standard Model
gauge group factor. Having a sizable sample allows us to draw some preliminary
landscape statistics of intersecting brane models both with and without the
restriction of having the Standard Model gauge factor.

    

### [[2112.08409] Quantum Model Learning Agent: characterisation of quantum systems through machine learning](http://arxiv.org/abs/2112.08409)


  Accurate models of real quantum systems are important for investigating their
behaviour, yet are difficult to distill empirically. Here, we report an
algorithm -- the Quantum Model Learning Agent (QMLA) -- to reverse engineer
Hamiltonian descriptions of a target system. We test the performance of QMLA on
a number of simulated experiments, demonstrating several mechanisms for the
design of candidate Hamiltonian models and simultaneously entertaining numerous
hypotheses about the nature of the physical interactions governing the system
under study. QMLA is shown to identify the true model in the majority of
instances, when provided with limited a priori information, and control of the
experimental setup. Our protocol can explore Ising, Heisenberg and Hubbard
families of models in parallel, reliably identifying the family which best
describes the system dynamics. We demonstrate QMLA operating on large model
spaces by incorporating a genetic algorithm to formulate new hypothetical
models. The selection of models whose features propagate to the next generation
is based upon an objective function inspired by the Elo rating scheme,
typically used to rate competitors in games such as chess and football. In all
instances, our protocol finds models that exhibit $F_1$-score $\geq 0.88$ when
compared with the true model, and it precisely identifies the true model in 72%
of cases, whilst exploring a space of over $250,000$ potential models. By
testing which interactions actually occur in the target system, QMLA is a
viable tool for both the exploration of fundamental physics and the
characterisation and calibration of quantum devices.

    

### [[2112.08415] Real-time Detection of Anomalies in Multivariate Time Series of Astronomical Data](http://arxiv.org/abs/2112.08415)


  Astronomical transients are stellar objects that become temporarily brighter
on various timescales and have led to some of the most significant discoveries
in cosmology and astronomy. Some of these transients are the explosive deaths
of stars known as supernovae while others are rare, exotic, or entirely new
kinds of exciting stellar explosions. New astronomical sky surveys are
observing unprecedented numbers of multi-wavelength transients, making standard
approaches of visually identifying new and interesting transients infeasible.
To meet this demand, we present two novel methods that aim to quickly and
automatically detect anomalous transient light curves in real-time. Both
methods are based on the simple idea that if the light curves from a known
population of transients can be accurately modelled, any deviations from model
predictions are likely anomalies. The first approach is a probabilistic neural
network built using Temporal Convolutional Networks (TCNs) and the second is an
interpretable Bayesian parametric model of a transient. We show that the
flexibility of neural networks, the attribute that makes them such a powerful
tool for many regression tasks, is what makes them less suitable for anomaly
detection when compared with our parametric model.

    

### [[2112.08417] Characterization of causal ancestral graphs for time series with latent confounders](http://arxiv.org/abs/2112.08417)


  Generalizing directed maximal ancestral graphs, we introduce a class of
graphical models for representing time lag specific causal relationships and
independencies among finitely many regularly sampled and regularly subsampled
time steps of multivariate time series with unobserved variables. We completely
characterize these graphs and show that they entail constraints beyond those
that have previously been considered in the literature. This allows for
stronger causal inferences without having imposed additional assumptions. In
generalization of directed partial ancestral graphs we further introduce a
graphical representation of Markov equivalence classes of the novel type of
graphs and show that these are more informative than what current
state-of-the-art causal discovery algorithms learn. We also analyze the
additional information gained by increasing the number of observed time steps.

    

### [[2112.08418] Neural Network-based Power Flow Model](http://arxiv.org/abs/2112.08418)


  Power flow analysis is used to evaluate the flow of electricity in the power
system network. Power flow calculation is used to determine the steady-state
variables of the system, such as the voltage magnitude /phase angle of each bus
and the active/reactive power flow on each branch. The DC power flow model is a
popular linear power flow model that is widely used in the power industry.
Although it is fast and robust, it may lead to inaccurate line flow results for
some critical transmission lines. This drawback can be partially addressed by
data-driven methods that take advantage of historical grid profiles. In this
paper, a neural network (NN) model is trained to predict power flow results
using historical power system data. Although the training process may take
time, once trained, it is very fast to estimate line flows. A comprehensive
performance analysis between the proposed NN-based power flow model and the
traditional DC power flow model is conducted. It can be concluded that the
proposed NN-based power flow model can find solutions quickly and more
accurately than DC power flow model.

    

### [[2112.08421] A White-Box SVM Framework and its Swarm-Based Optimization for Supervision of Toothed Milling Cutter through Characterization of Spindle Vibrations](http://arxiv.org/abs/2112.08421)


  In this paper, a white-Box support vector machine (SVM) framework and its
swarm-based optimization is presented for supervision of toothed milling cutter
through characterization of real-time spindle vibrations. The anomalous moments
of vibration evolved due to in-process tool failures (i.e., flank and nose
wear, crater and notch wear, edge fracture) have been investigated through
time-domain response of acceleration and statistical features. The Recursive
Feature Elimination with Cross-Validation (RFECV) with decision trees as the
estimator has been implemented for feature selection. Further, the competence
of standard SVM has been examined for tool health monitoring followed by its
optimization through application of swarm based algorithms. The comparative
analysis of performance of five meta-heuristic algorithms (Elephant Herding
Optimization, Monarch Butterfly Optimization, Harris Hawks Optimization, Slime
Mould Algorithm, and Moth Search Algorithm) has been carried out. The white-box
approach has been presented considering global and local representation that
provides insight into the performance of machine learning models in tool
condition monitoring.

    

### [[2112.08429] torch.fx: Practical Program Capture and Transformation for Deep Learning in Python](http://arxiv.org/abs/2112.08429)


  Modern deep learning frameworks provide imperative, eager execution
programming interfaces embedded in Python to provide a productive development
experience. However, deep learning practitioners sometimes need to capture and
transform program structure for performance optimization, visualization,
analysis, and hardware integration. We study the different designs for program
capture and transformation used in deep learning. By designing for typical deep
learning use cases rather than long tail ones, it is possible to create a
simpler framework for program capture and transformation. We apply this
principle in torch.fx, a program capture and transformation library for PyTorch
written entirely in Python and optimized for high developer productivity by ML
practitioners. We present case studies showing how torch.fx enables workflows
previously inaccessible in the PyTorch ecosystem.

    

### [[2112.08438] Programmatic Reward Design by Example](http://arxiv.org/abs/2112.08438)


  Reward design is a fundamental problem in reinforcement learning (RL). A
misspecified or poorly designed reward can result in low sample efficiency and
undesired behaviors. In this paper, we propose the idea of \textit{programmatic
reward design}, i.e. using programs to specify the reward functions in RL
environments. Programs allow human engineers to express sub-goals and complex
task scenarios in a structured and interpretable way. The challenge of
programmatic reward design, however, is that while humans can provide the
high-level structures, properly setting the low-level details, such as the
right amount of reward for a specific sub-task, remains difficult. A major
contribution of this paper is a probabilistic framework that can infer the best
candidate programmatic reward function from expert demonstrations. Inspired by
recent generative-adversarial approaches, our framework {searches for the most
likely programmatic reward function under which the optimally generated
trajectories cannot be differentiated from the demonstrated trajectories}.
Experimental results show that programmatic reward functions learned using this
framework can significantly outperform those learned using existing reward
learning algorithms, and enable RL agents to achieve state-of-the-art
performance on highly complex tasks.

    

### [[2112.08439] Generalization Bounds for Stochastic Gradient Langevin Dynamics: A Unified View via Information Leakage Analysis](http://arxiv.org/abs/2112.08439)


  Recently, generalization bounds of the non-convex empirical risk minimization
paradigm using Stochastic Gradient Langevin Dynamics (SGLD) have been
extensively studied. Several theoretical frameworks have been presented to
study this problem from different perspectives, such as information theory and
stability. In this paper, we present a unified view from privacy leakage
analysis to investigate the generalization bounds of SGLD, along with a
theoretical framework for re-deriving previous results in a succinct manner.
Aside from theoretical findings, we conduct various numerical studies to
empirically assess the information leakage issue of SGLD. Additionally, our
theoretical and empirical results provide explanations for prior works that
study the membership privacy of SGLD.

    

### [[2112.08440] Climate-Invariant Machine Learning](http://arxiv.org/abs/2112.08440)


  Data-driven algorithms, in particular neural networks, can emulate the
effects of unresolved processes in coarse-resolution climate models when
trained on high-resolution simulation data; however, they often make large
generalization errors when evaluated in conditions they were not trained on.
Here, we propose to physically rescale the inputs and outputs of machine
learning algorithms to help them generalize to unseen climates. Applied to
offline parameterizations of subgrid-scale thermodynamics in three distinct
climate models, we show that rescaled or "climate-invariant" neural networks
make accurate predictions in test climates that are 4K and 8K warmer than their
training climates. Additionally, "climate-invariant" neural nets facilitate
generalization between Aquaplanet and Earth-like simulations. Through
visualization and attribution methods, we show that compared to standard
machine learning models, "climate-invariant" algorithms learn more local and
robust relations between storm-scale convection, radiation, and their synoptic
thermodynamic environment. Overall, these results suggest that explicitly
incorporating physical knowledge into data-driven models of Earth system
processes can improve their consistency and ability to generalize across
climate regimes.

    

### [[2112.08441] Towards Explainable Artificial Intelligence in Banking and Financial Services](http://arxiv.org/abs/2112.08441)


  Artificial intelligence (AI) enables machines to learn from human experience,
adjust to new inputs, and perform human-like tasks. AI is progressing rapidly
and is transforming the way businesses operate, from process automation to
cognitive augmentation of tasks and intelligent process/data analytics.
However, the main challenge for human users would be to understand and
appropriately trust the result of AI algorithms and methods. In this paper, to
address this challenge, we study and analyze the recent work done in
Explainable Artificial Intelligence (XAI) methods and tools. We introduce a
novel XAI process, which facilitates producing explainable models while
maintaining a high level of learning performance. We present an interactive
evidence-based approach to assist human users in comprehending and trusting the
results and output created by AI-enabled algorithms. We adopt a typical
scenario in the Banking domain for analyzing customer transactions. We develop
a digital dashboard to facilitate interacting with the algorithm results and
discuss how the proposed XAI method can significantly improve the confidence of
data scientists in understanding the result of AI-enabled algorithms.

    

### [[2112.08442] Utilizing XAI technique to improve autoencoder based model for computer network anomaly detection with shapley additive explanation(SHAP)](http://arxiv.org/abs/2112.08442)


  Machine learning (ML) and Deep Learning (DL) methods are being adopted
rapidly, especially in computer network security, such as fraud detection,
network anomaly detection, intrusion detection, and much more. However, the
lack of transparency of ML and DL based models is a major obstacle to their
implementation and criticized due to its black-box nature, even with such
tremendous results. Explainable Artificial Intelligence (XAI) is a promising
area that can improve the trustworthiness of these models by giving
explanations and interpreting its output. If the internal working of the ML and
DL based models is understandable, then it can further help to improve its
performance. The objective of this paper is to show that how XAI can be used to
interpret the results of the DL model, the autoencoder in this case. And, based
on the interpretation, we improved its performance for computer network anomaly
detection. The kernel SHAP method, which is based on the shapley values, is
used as a novel feature selection technique. This method is used to identify
only those features that are actually causing the anomalous behaviour of the
set of attack/anomaly instances. Later, these feature sets are used to train
and validate the autoencoder but on benign data only. Finally, the built
SHAP_Model outperformed the other two models proposed based on the feature
selection method. This whole experiment is conducted on the subset of the
latest CICIDS2017 network dataset. The overall accuracy and AUC of SHAP_Model
is 94% and 0.969, respectively.

    

### [[2112.08443] Event-Aware Multimodal Mobility Nowcasting](http://arxiv.org/abs/2112.08443)


  As a decisive part in the success of Mobility-as-a-Service (MaaS),
spatio-temporal predictive modeling for crowd movements is a challenging task
particularly considering scenarios where societal events drive mobility
behavior deviated from the normality. While tremendous progress has been made
to model high-level spatio-temporal regularities with deep learning, most, if
not all of the existing methods are neither aware of the dynamic interactions
among multiple transport modes nor adaptive to unprecedented volatility brought
by potential societal events. In this paper, we are therefore motivated to
improve the canonical spatio-temporal network (ST-Net) from two perspectives:
(1) design a heterogeneous mobility information network (HMIN) to explicitly
represent intermodality in multimodal mobility; (2) propose a memory-augmented
dynamic filter generator (MDFG) to generate sequence-specific parameters in an
on-the-fly fashion for various scenarios. The enhanced event-aware
spatio-temporal network, namely EAST-Net, is evaluated on several real-world
datasets with a wide variety and coverage of societal events. Both quantitative
and qualitative experimental results verify the superiority of our approach
compared with the state-of-the-art baselines. Code and data are published on
this https URL.

    

### [[2112.08447] Positional Encoding Augmented GAN for the Assessment of Wind Flow for Pedestrian Comfort in Urban Areas](http://arxiv.org/abs/2112.08447)


  Approximating wind flows using computational fluid dynamics (CFD) methods can
be time-consuming. Creating a tool for interactively designing prototypes while
observing the wind flow change requires simpler models to simulate faster.
Instead of running numerical approximations resulting in detailed calculations,
data-driven methods in deep learning might be able to give similar results in a
fraction of the time. This work rephrases the problem from computing 3D flow
fields using CFD to a 2D image-to-image translation-based problem on the
building footprints to predict the flow field at pedestrian height level. We
investigate the use of generative adversarial networks (GAN), such as Pix2Pix
[1] and CycleGAN [2] representing state-of-the-art for image-to-image
translation task in various domains as well as U-Net autoencoder [3]. The
models can learn the underlying distribution of a dataset in a data-driven
manner, which we argue can help the model learn the underlying
Reynolds-averaged Navier-Stokes (RANS) equations from CFD. We experiment on
novel simulated datasets on various three-dimensional bluff-shaped buildings
with and without height information. Moreover, we present an extensive
qualitative and quantitative evaluation of the generated images for a selection
of models and compare their performance with the simulations delivered by CFD.
We then show that adding positional data to the input can produce more accurate
results by proposing a general framework for injecting such information on the
different architectures. Furthermore, we show that the models performances
improve by applying attention mechanisms and spectral normalization to
facilitate stable training.

    

### [[2112.08453] The Need for Ethical, Responsible, and Trustworthy Artificial Intelligence for Environmental Sciences](http://arxiv.org/abs/2112.08453)


  Given the growing use of Artificial Intelligence (AI) and machine learning
(ML) methods across all aspects of environmental sciences, it is imperative
that we initiate a discussion about the ethical and responsible use of AI. In
fact, much can be learned from other domains where AI was introduced, often
with the best of intentions, yet often led to unintended societal consequences,
such as hard coding racial bias in the criminal justice system or increasing
economic inequality through the financial system. A common misconception is
that the environmental sciences are immune to such unintended consequences when
AI is being used, as most data come from observations, and AI algorithms are
based on mathematical formulas, which are often seen as objective. In this
article, we argue the opposite can be the case. Using specific examples, we
demonstrate many ways in which the use of AI can introduce similar consequences
in the environmental sciences. This article will stimulate discussion and
research efforts in this direction. As a community, we should avoid repeating
any foreseeable mistakes made in other domains through the introduction of AI.
In fact, with proper precautions, AI can be a great tool to help {\it reduce}
climate and environmental injustice. We primarily focus on weather and climate
examples but the conclusions apply broadly across the environmental sciences.

    

### [[2112.08458] Leveraging the structure of dynamical systems for data-driven modeling](http://arxiv.org/abs/2112.08458)


  The reliable prediction of the temporal behavior of complex systems is
required in numerous scientific fields. This strong interest is however
hindered by modeling issues: often, the governing equations describing the
physics of the system under consideration are not accessible or, when known,
their solution might require a computational time incompatible with the
prediction time constraints.
Nowadays, approximating complex systems at hand in a generic functional
format and informing it ex nihilo from available observations has become a
common practice, as illustrated by the enormous amount of scientific work
appeared in the last years. Numerous successful examples based on deep neural
networks are already available, although generalizability of the models and
margins of guarantee are often overlooked. Here, we consider Long-Short Term
Memory neural networks and thoroughly investigate the impact of the training
set and its structure on the quality of the long-term prediction. Leveraging
ergodic theory, we analyze the amount of data sufficient for a priori
guaranteeing a faithful model of the physical system.
We show how an informed design of the training set, based on invariants of
the system and the structure of the underlying attractor, significantly
improves the resulting models, opening up avenues for research within the
context of active learning. Further, the non-trivial effects of the memory
initializations when relying on memory-capable models will be illustrated. Our
findings provide evidence-based good-practice on the amount and the choice of
data required for an effective data-driven modeling of any complex dynamical
system.

    

### [[2112.08493] StyleMC: Multi-Channel Based Fast Text-Guided Image Generation and Manipulation](http://arxiv.org/abs/2112.08493)


  Discovering meaningful directions in the latent space of GANs to manipulate
semantic attributes typically requires large amounts of labeled data. Recent
work aims to overcome this limitation by leveraging the power of Contrastive
Language-Image Pre-training (CLIP), a joint text-image model. While promising,
these methods require several hours of preprocessing or training to achieve the
desired manipulations. In this paper, we present StyleMC, a fast and efficient
method for text-driven image generation and manipulation. StyleMC uses a
CLIP-based loss and an identity loss to manipulate images via a single text
prompt without significantly affecting other attributes. Unlike prior work,
StyleMC requires only a few seconds of training per text prompt to find stable
global directions, does not require prompt engineering and can be used with any
pre-trained StyleGAN2 model. We demonstrate the effectiveness of our method and
compare it to state-of-the-art methods. Our code can be found at
this http URL.

    

### [[2112.08497] Predicting Levels of Household Electricity Consumption in Low-Access Settings](http://arxiv.org/abs/2112.08497)


  In low-income settings, the most critical piece of information for electric
utilities is the anticipated consumption of a customer. Electricity consumption
assessment is difficult to do in settings where a significant fraction of
households do not yet have an electricity connection. In such settings the
absolute levels of anticipated consumption can range from 5-100 kWh/month,
leading to high variability amongst these customers. Precious resources are at
stake if a significant fraction of low consumers are connected over those with
higher consumption.
This is the first study of it's kind in low-income settings that attempts to
predict a building's consumption and not that of an aggregate administrative
area. We train a Convolutional Neural Network (CNN) over pre-electrification
daytime satellite imagery with a sample of utility bills from 20,000
geo-referenced electricity customers in Kenya (0.01% of Kenya's residential
customers). This is made possible with a two-stage approach that uses a novel
building segmentation approach to leverage much larger volumes of no-cost
satellite imagery to make the most of scarce and expensive customer data. Our
method shows that competitive accuracies can be achieved at the building level,
addressing the challenge of consumption variability. This work shows that the
building's characteristics and it's surrounding context are both important in
predicting consumption levels. We also evaluate the addition of lower
resolution geospatial datasets into the training process, including nighttime
lights and census-derived data. The results are already helping inform site
selection and distribution-level planning, through granular predictions at the
level of individual structures in Kenya and there is no reason this cannot be
extended to other countries.

    

### [[2112.08507] Algorithms for Adaptive Experiments that Trade-off Statistical Analysis with Reward: Combining Uniform Random Assignment and Reward Maximization](http://arxiv.org/abs/2112.08507)


  Multi-armed bandit algorithms like Thompson Sampling can be used to conduct
adaptive experiments, in which maximizing reward means that data is used to
progressively assign more participants to more effective arms. Such assignment
strategies increase the risk of statistical hypothesis tests identifying a
difference between arms when there is not one, and failing to conclude there is
a difference in arms when there truly is one. We present simulations for 2-arm
experiments that explore two algorithms that combine the benefits of uniform
randomization for statistical analysis, with the benefits of reward
maximization achieved by Thompson Sampling (TS). First, Top-Two Thompson
Sampling adds a fixed amount of uniform random allocation (UR) spread evenly
over time. Second, a novel heuristic algorithm, called TS PostDiff (Posterior
Probability of Difference). TS PostDiff takes a Bayesian approach to mixing TS
and UR: the probability a participant is assigned using UR allocation is the
posterior probability that the difference between two arms is `small' (below a
certain threshold), allowing for more UR exploration when there is little or no
reward to be gained. We find that TS PostDiff method performs well across
multiple effect sizes, and thus does not require tuning based on a guess for
the true effect size.

    

### [[2112.08511] OptABC: an Optimal Hyperparameter Tuning Approach for Machine Learning Algorithms](http://arxiv.org/abs/2112.08511)


  Hyperparameter tuning in machine learning algorithms is a computationally
challenging task due to the large-scale nature of the problem. In order to
develop an efficient strategy for hyper-parameter tuning, one promising
solution is to use swarm intelligence algorithms. Artificial Bee Colony (ABC)
optimization lends itself as a promising and efficient optimization algorithm
for this purpose. However, in some cases, ABC can suffer from a slow
convergence rate or execution time due to the poor initial population of
solutions and expensive objective functions. To address these concerns, a novel
algorithm, OptABC, is proposed to help ABC algorithm in faster convergence
toward a near-optimum solution. OptABC integrates artificial bee colony
algorithm, K-Means clustering, greedy algorithm, and opposition-based learning
strategy for tuning the hyper-parameters of different machine learning models.
OptABC employs these techniques in an attempt to diversify the initial
population, and hence enhance the convergence ability without significantly
decreasing the accuracy. In order to validate the performance of the proposed
method, we compare the results with previous state-of-the-art approaches.
Experimental results demonstrate the effectiveness of the OptABC compared to
existing approaches in the literature.

    

### [[2112.08512] ELight: Enabling Efficient Photonic In-Memory Neurocomputing with Life Enhancement](http://arxiv.org/abs/2112.08512)


  With the recent advances in optical phase change material (PCM), photonic
in-memory neurocomputing has demonstrated its superiority in optical neural
network (ONN) designs with near-zero static power consumption, time-of-light
latency, and compact footprint. However, photonic tensor cores require massive
hardware reuse to implement large matrix multiplication due to the limited
single-core scale. The resultant large number of PCM writes leads to serious
dynamic power and overwhelms the fragile PCM with limited write endurance. In
this work, we propose a synergistic optimization framework, ELight, to minimize
the overall write efforts for efficient and reliable optical in-memory
neurocomputing. We first propose write-aware training to encourage the
similarity among weight blocks, and combine it with a post-training
optimization method to reduce programming efforts by eliminating redundant
writes. Experiments show that ELight can achieve over 20X reduction in the
total number of writes and dynamic power with comparable accuracy. With our
ELight, photonic in-memory neurocomputing will step forward towards viable
applications in machine learning with preserved accuracy, order-of-magnitude
longer lifetime, and lower programming energy.

    

### [[2112.08524] FLoRA: Single-shot Hyper-parameter Optimization for Federated Learning](http://arxiv.org/abs/2112.08524)


  We address the relatively unexplored problem of hyper-parameter optimization
(HPO) for federated learning (FL-HPO). We introduce Federated Loss suRface
Aggregation (FLoRA), the first FL-HPO solution framework that can address use
cases of tabular data and gradient boosting training algorithms in addition to
stochastic gradient descent/neural networks commonly addressed in the FL
literature. The framework enables single-shot FL-HPO, by first identifying a
good set of hyper-parameters that are used in a **single** FL training. Thus,
it enables FL-HPO solutions with minimal additional communication overhead
compared to FL training without HPO. Our empirical evaluation of FLoRA for
Gradient Boosted Decision Trees on seven OpenML data sets demonstrates
significant model accuracy improvements over the considered baseline, and
robustness to increasing number of parties involved in FL-HPO training.

    

### [[2112.08526] Invariance Through Inference](http://arxiv.org/abs/2112.08526)


  We introduce a general approach, called Invariance through Inference, for
improving the test-time performance of an agent in deployment environments with
unknown perceptual variations. Instead of producing invariant visual features
through interpolation, invariance through inference turns adaptation at
deployment-time into an unsupervised learning problem. This is achieved in
practice by deploying a straightforward algorithm that tries to match the
distribution of latent features to the agent's prior experience, without
relying on paired data. Although simple, we show that this idea leads to
surprising improvements on a variety of adaptation scenarios without access to
deployment-time rewards, including changes in camera poses and lighting
conditions. Results are presented on challenging distractor control suite, a
robotics environment with image-based observations.

    

### [[2112.08534] Trading with the Momentum Transformer: An Intelligent and Interpretable Architecture](http://arxiv.org/abs/2112.08534)


  Deep learning architectures, specifically Deep Momentum Networks (DMNs)
[1904.04912], have been found to be an effective approach to momentum and
mean-reversion trading. However, some of the key challenges in recent years
involve learning long-term dependencies, degradation of performance when
considering returns net of transaction costs and adapting to new market
regimes, notably during the SARS-CoV-2 crisis. Attention mechanisms, or
Transformer-based architectures, are a solution to such challenges because they
allow the network to focus on significant time steps in the past and
longer-term patterns. We introduce the Momentum Transformer, an attention-based
architecture which outperforms the benchmarks, and is inherently interpretable,
providing us with greater insights into our deep learning trading strategy. Our
model is an extension to the LSTM-based DMN, which directly outputs position
sizing by optimising the network on a risk-adjusted performance metric, such as
Sharpe ratio. We find an attention-LSTM hybrid Decoder-Only Temporal Fusion
Transformer (TFT) style architecture is the best performing model. In terms of
interpretability, we observe remarkable structure in the attention patterns,
with significant peaks of importance at momentum turning points. The time
series is thus segmented into regimes and the model tends to focus on previous
time-steps in alike regimes. We find changepoint detection (CPD) [2105.13727],
another technique for responding to regime change, can complement multi-headed
attention, especially when we run CPD at multiple timescales. Through the
addition of an interpretable variable selection network, we observe how CPD
helps our model to move away from trading predominantly on daily returns data.
We note that the model can intelligently switch between, and blend, classical
strategies - basing its decision on patterns in the data.

    

### [[2112.08538] Visualizing the Loss Landscape of Winning Lottery Tickets](http://arxiv.org/abs/2112.08538)


  The underlying loss landscapes of deep neural networks have a great impact on
their training, but they have mainly been studied theoretically due to
computational constraints. This work vastly reduces the time required to
compute such loss landscapes, and uses them to study winning lottery tickets
found via iterative magnitude pruning. We also share results that contradict
previously claimed correlations between certain loss landscape projection
methods and model trainability and generalization error.

    

### [[2112.08541] BGL: GPU-Efficient GNN Training by Optimizing Graph Data I/O and Preprocessing](http://arxiv.org/abs/2112.08541)


  Graph neural networks (GNNs) have extended the success of deep neural
networks (DNNs) to non-Euclidean graph data, achieving ground-breaking
performance on various tasks such as node classification and graph property
prediction. Nonetheless, existing systems are inefficient to train large graphs
with billions of nodes and edges with GPUs. The main bottlenecks are the
process of preparing data for GPUs - subgraph sampling and feature retrieving.
This paper proposes BGL, a distributed GNN training system designed to address
the bottlenecks with a few key ideas. First, we propose a dynamic cache engine
to minimize feature retrieving traffic. By a co-design of caching policy and
the order of sampling, we find a sweet spot of low overhead and high cache hit
ratio. Second, we improve the graph partition algorithm to reduce
cross-partition communication during subgraph sampling. Finally, careful
resource isolation reduces contention between different data preprocessing
stages. Extensive experiments on various GNN models and large graph datasets
show that BGL significantly outperforms existing GNN training systems by 20.68x
on average.

    

### [[2112.08547] Learning Rich Representation of Keyphrases from Text](http://arxiv.org/abs/2112.08547)


  In this work, we explore how to learn task-specific language models aimed
towards learning rich representation of keyphrases from text documents. We
experiment with different masking strategies for pre-training transformer
language models (LMs) in discriminative as well as generative settings. In the
discriminative setting, we introduce a new pre-training objective - Keyphrase
Boundary Infilling with Replacement (KBIR), showing large gains in performance
(upto 9.26 points in F1) over SOTA, when LM pre-trained using KBIR is
fine-tuned for the task of keyphrase extraction. In the generative setting, we
introduce a new pre-training setup for BART - KeyBART, that reproduces the
keyphrases related to the input text in the CatSeq format, instead of the
denoised original input. This also led to gains in performance (upto 4.33
points in F1@M) over SOTA for keyphrase generation. Additionally, we also
fine-tune the pre-trained language models on named entity recognition (NER),
question answering (QA), relation extraction (RE), abstractive summarization
and achieve comparable performance with that of the SOTA, showing that learning
rich representation of keyphrases is indeed beneficial for many other
fundamental NLP tasks.

    

### [[2112.08549] A prediction-based approach for online dynamic radiotherapy scheduling](http://arxiv.org/abs/2112.08549)


  Patient scheduling is a difficult task as it involves dealing with stochastic
factors such as an unknown arrival flow of patients. Scheduling radiotherapy
treatments for cancer patients faces a similar problem. Curative patients need
to start their treatment within the recommended deadlines, i.e., 14 or 28 days
after their admission while reserving treatment capacity for palliative
patients who require urgent treatments within 1 to 3 days after their
admission. Most cancer centers solve the problem by reserving a fixed number of
treatment slots for emergency patients. However, this flat-reservation approach
is not ideal and can cause overdue treatments for emergency patients on some
days while not fully exploiting treatment capacity on some other days, which
also leads to delaying treatment for curative patients. This problem is
especially severe in large and crowded hospitals. In this paper, we propose a
prediction-based approach for online dynamic radiotherapy scheduling. An
offline problem where all future patient arrivals are known in advance is
solved to optimality using Integer Programming. A regression model is then
trained to recognize the links between patients' arrival patterns and their
ideal waiting time. The trained regression model is then embedded in a
prediction-based approach that schedules a patient based on their
characteristics and the present state of the calendar. The numerical results
show that our prediction-based approach efficiently prevents overdue treatments
for emergency patients while maintaining a good waiting time compared to other
scheduling approaches based on a flat-reservation policy.

    

### [[2112.08553] UMAD: Universal Model Adaptation under Domain and Category Shift](http://arxiv.org/abs/2112.08553)


  Learning to reject unknown samples (not present in the source classes) in the
target domain is fairly important for unsupervised domain adaptation (UDA).
There exist two typical UDA scenarios, i.e., open-set, and open-partial-set,
and the latter assumes that not all source classes appear in the target domain.
However, most prior methods are designed for one UDA scenario and always
perform badly on the other UDA scenario. Moreover, they also require the
labeled source data during adaptation, limiting their usability in data
privacy-sensitive applications. To address these issues, this paper proposes a
Universal Model ADaptation (UMAD) framework which handles both UDA scenarios
without access to the source data nor prior knowledge about the category shift
between domains. Specifically, we aim to learn a source model with an elegantly
designed two-head classifier and provide it to the target domain. During
adaptation, we develop an informative consistency score to help distinguish
unknown samples from known samples. To achieve bilateral adaptation in the
target domain, we further maximize localized mutual information to align known
samples with the source classifier and employ an entropic loss to push unknown
samples far away from the source classification boundary, respectively.
Experiments on open-set and open-partial-set UDA scenarios demonstrate that
UMAD, as a unified approach without access to source data, exhibits comparable,
if not superior, performance to state-of-the-art data-dependent methods.

    

### [[2112.08567] HampDTI: a heterogeneous graph automatic meta-path learning method for drug-target interaction prediction](http://arxiv.org/abs/2112.08567)


  Motivation: Identifying drug-target interactions (DTIs) is a key step in drug
repositioning. In recent years, the accumulation of a large number of genomics
and pharmacology data has formed mass drug and target related heterogeneous
networks (HNs), which provides new opportunities of developing HN-based
computational models to accurately predict DTIs. The HN implies lots of useful
information about DTIs but also contains irrelevant data, and how to make the
best of heterogeneous networks remains a challenge. Results: In this paper, we
propose a heterogeneous graph automatic meta-path learning based DTI prediction
method (HampDTI). HampDTI automatically learns the important meta-paths between
drugs and targets from the HN, and generates meta-path graphs. For each
meta-path graph, the features learned from drug molecule graphs and target
protein sequences serve as the node attributes, and then a node-type specific
graph convolutional network (NSGCN) which efficiently considers node type
information (drugs or targets) is designed to learn embeddings of drugs and
targets. Finally, the embeddings from multiple meta-path graphs are combined to
predict novel DTIs. The experiments on benchmark datasets show that our
proposed HampDTI achieves superior performance compared with state-of-the-art
DTI prediction methods. More importantly, HampDTI identifies the important
meta-paths for DTI prediction, which could explain how drugs connect with
targets in HNs.

    

### [[2112.08572] Predictive Price-Performance Optimization for Serverless Query Processing](http://arxiv.org/abs/2112.08572)


  We present an efficient, parametric modeling framework for predictive
resource allocations, focusing on the amount of computational resources, that
can optimize for a range of price-performance objectives for data analytics in
serverless query processing settings. We discuss and evaluate in depth how our
system, AutoExecutor, can use this framework to automatically select
near-optimal executor and core counts for Spark SQL queries running on Azure
Synapse. Our techniques improve upon Spark's in-built, reactive, dynamic
executor allocation capabilities by substantially reducing the total executors
allocated and executor occupancy while running queries, thereby freeing up
executors that can potentially be used by other concurrent queries or in
reducing the overall cluster provisioning needs. In contrast with
post-execution analysis tools such as Sparklens, we predict resource
allocations for queries before executing them and can also account for changes
in input data sizes for predicting the desired allocations.

    

### [[2112.08587] SGEITL: Scene Graph Enhanced Image-Text Learning for Visual Commonsense Reasoning](http://arxiv.org/abs/2112.08587)


  Answering complex questions about images is an ambitious goal for machine
intelligence, which requires a joint understanding of images, text, and
commonsense knowledge, as well as a strong reasoning ability. Recently,
multimodal Transformers have made great progress in the task of Visual
Commonsense Reasoning (VCR), by jointly understanding visual objects and text
tokens through layers of cross-modality attention. However, these approaches do
not utilize the rich structure of the scene and the interactions between
objects which are essential in answering complex commonsense questions. We
propose a Scene Graph Enhanced Image-Text Learning (SGEITL) framework to
incorporate visual scene graphs in commonsense reasoning. To exploit the scene
graph structure, at the model structure level, we propose a multihop graph
transformer for regularizing attention interaction among hops. As for
pre-training, a scene-graph-aware pre-training method is proposed to leverage
structure knowledge extracted in the visual scene graph. Moreover, we introduce
a method to train and generate domain-relevant visual scene graphs using
textual annotations in a weakly-supervised manner. Extensive experiments on VCR
and other tasks show a significant performance boost compared with the
state-of-the-art methods and prove the efficacy of each proposed component.

    

### [[2112.08616] Masked Measurement Prediction: Learning to Jointly Predict Quantities and Units from Textual Context](http://arxiv.org/abs/2112.08616)


  Physical measurements constitute a large portion of numbers in academic
papers, engineering reports, and web tables. Current benchmarks fall short of
properly evaluating numeracy of pretrained language models on measurements,
hindering research on developing new methods and applying them to numerical
tasks. To that end, we introduce a novel task, Masked Measurement Prediction
(MMP), where a model learns to reconstruct a number together with its
associated unit given masked text. MMP is useful for both training new
numerically informed models as well as evaluating numeracy of existing systems.
In order to address this task, we introduce a new Generative Masked Measurement
(GeMM) model that jointly learns to predict numbers along with their units. We
perform fine-grained analyses comparing our model with various ablations and
baselines. We use linear probing of traditional pretrained transformer models
(RoBERTa) to show that they significantly underperform jointly trained
number-unit models, highlighting the difficulty of this new task and the
benefits of our proposed pretraining approach. We hope this framework
accelerates the progress towards building more robust numerical reasoning
systems in the future.

    

### [[2112.08618] A Statistics and Deep Learning Hybrid Method for Multivariate Time Series Forecasting and Mortality Modeling](http://arxiv.org/abs/2112.08618)


  Hybrid methods have been shown to outperform pure statistical and pure deep
learning methods at forecasting tasks and quantifying the associated
uncertainty with those forecasts (prediction intervals). One example is
Exponential Smoothing Recurrent Neural Network (ES-RNN), a hybrid between a
statistical forecasting model and a recurrent neural network variant. ES-RNN
achieves a 9.4\% improvement in absolute error in the Makridakis-4 Forecasting
Competition. This improvement and similar outperformance from other hybrid
models have primarily been demonstrated only on univariate datasets.
Difficulties with applying hybrid forecast methods to multivariate data include
($i$) the high computational cost involved in hyperparameter tuning for models
that are not parsimonious, ($ii$) challenges associated with auto-correlation
inherent in the data, as well as ($iii$) complex dependency (cross-correlation)
between the covariates that may be hard to capture. This paper presents
Multivariate Exponential Smoothing Long Short Term Memory (MES-LSTM), a
generalized multivariate extension to ES-RNN, that overcomes these challenges.
MES-LSTM utilizes a vectorized implementation. We test MES-LSTM on several
aggregated coronavirus disease of 2019 (COVID-19) morbidity datasets and find
our hybrid approach shows consistent, significant improvement over pure
statistical and deep learning methods at forecast accuracy and prediction
interval construction.

    

### [[2112.08628] Explainable Natural Language Processing with Matrix Product States](http://arxiv.org/abs/2112.08628)


  Despite empirical successes of recurrent neural networks (RNNs) in natural
language processing (NLP), theoretical understanding of RNNs is still limited
due to intrinsically complex computations in RNNs. We perform a systematic
analysis of RNNs' behaviors in a ubiquitous NLP task, the sentiment analysis of
movie reviews, via the mapping between a class of RNNs called recurrent
arithmetic circuits (RACs) and a matrix product state (MPS). Using the
von-Neumann entanglement entropy (EE) as a proxy for information propagation,
we show that single-layer RACs possess a maximum information propagation
capacity, reflected by the saturation of the EE. Enlarging the bond dimension
of an MPS beyond the EE saturation threshold does not increase the prediction
accuracies, so a minimal model that best estimates the data statistics can be
constructed. Although the saturated EE is smaller than the maximum EE
achievable by the area law of an MPS, our model achieves ~99% training
accuracies in realistic sentiment analysis data sets. Thus, low EE alone is not
a warrant against the adoption of single-layer RACs for NLP. Contrary to a
common belief that long-range information propagation is the main source of
RNNs' expressiveness, we show that single-layer RACs also harness high
expressiveness from meaningful word vector embeddings. Our work sheds light on
the phenomenology of learning in RACs and more generally on the explainability
aspects of RNNs for NLP, using tools from many-body quantum physics.

    

### [[2112.08633] Learning To Retrieve Prompts for In-Context Learning](http://arxiv.org/abs/2112.08633)


  In-context learning is a recent paradigm in natural language understanding,
where a large pre-trained language model (LM) observes a test instance and a
few training examples as its input, and directly decodes the output without any
update to its parameters. However, performance has been shown to strongly
depend on the selected training examples (termed prompt). In this work, we
propose an efficient method for retrieving prompts for in-context learning
using annotated data and a LM. Given an input-output pair, we estimate the
probability of the output given the input and a candidate training example as
the prompt, and label training examples as positive or negative based on this
probability. We then train an efficient dense retriever from this data, which
is used to retrieve training examples as prompts at test time. We evaluate our
approach on three sequence-to-sequence tasks where language utterances are
mapped to meaning representations, and find that it substantially outperforms
prior work and multiple baselines across the board.

    

### [[2112.08645] Learning Interpretable Models Through Multi-Objective Neural Architecture Search](http://arxiv.org/abs/2112.08645)


  Monumental advances in deep learning have led to unprecedented achievements
across a multitude of domains. While the performance of deep neural networks is
indubitable, the architectural design and interpretability of such models are
nontrivial. Research has been introduced to automate the design of neural
network architectures through neural architecture search (NAS). Recent progress
has made these methods more pragmatic by exploiting distributed computation and
novel optimization algorithms. However, there is little work in optimizing
architectures for interpretability. To this end, we propose a multi-objective
distributed NAS framework that optimizes for both task performance and
introspection. We leverage the non-dominated sorting genetic algorithm
(NSGA-II) and explainable AI (XAI) techniques to reward architectures that can
be better comprehended by humans. The framework is evaluated on several image
classification datasets. We demonstrate that jointly optimizing for
introspection ability and task error leads to more disentangled architectures
that perform within tolerable error.

    

### [[2112.08652] Extreme Zero-Shot Learning for Extreme Text Classification](http://arxiv.org/abs/2112.08652)


  The eXtreme Multi-label text Classification (XMC) problem concerns finding
most relevant labels for an input text instance from a large label set.
However, the XMC setup faces two challenges: (1) it is not generalizable to
predict unseen labels in dynamic environments, and (2) it requires a large
amount of supervised (instance, label) pairs, which can be difficult to obtain
for emerging domains. Recently, the generalized zero-shot XMC (GZ-XMC) setup
has been studied and ZestXML is proposed accordingly to handle the unseen
labels, which still requires a large number of annotated (instance, label)
pairs. In this paper, we consider a more practical scenario called Extreme
Zero-Shot XMC (EZ-XMC), in which no supervision is needed and merely raw text
of instances and labels are accessible. Few-Shot XMC (FS-XMC), an extension to
EZ-XMC with limited supervision is also investigated. To learn the semantic
embeddings of instances and labels with raw text, we propose to pre-train
Transformer-based encoders with self-supervised contrastive losses.
Specifically, we develop a pre-training method MACLR, which thoroughly
leverages the raw text with techniques including Multi-scale Adaptive
Clustering, Label Regularization, and self-training with pseudo positive pairs.
Experimental results on four public EZ-XMC datasets demonstrate that MACLR
achieves superior performance compared to all other leading baseline methods,
in particular with approximately 5-10% improvement in precision and recall on
average. Moreover, we also show that our pre-trained encoder can be further
improved on FS-XMC when there are a limited number of ground-truth positive
pairs in training. By fine-tuning the encoder on such a few-shot subset, MACLR
still outperforms other extreme classifiers significantly.

    

### [[2112.08654] Learning to Prompt for Continual Learning](http://arxiv.org/abs/2112.08654)


  The mainstream paradigm behind continual learning has been to adapt the model
parameters to non-stationary data distributions, where catastrophic forgetting
is the central challenge. Typical methods rely on a rehearsal buffer or known
task identity at test time to retrieve learned knowledge and address
forgetting, while this work presents a new paradigm for continual learning that
aims to train a more succinct memory system without accessing task identity at
test time. Our method learns to dynamically prompt (L2P) a pre-trained model to
learn tasks sequentially under different task transitions. In our proposed
framework, prompts are small learnable parameters, which are maintained in a
memory space. The objective is to optimize prompts to instruct the model
prediction and explicitly manage task-invariant and task-specific knowledge
while maintaining model plasticity. We conduct comprehensive experiments under
popular image classification benchmarks with different challenging continual
learning settings, where L2P consistently outperforms prior state-of-the-art
methods. Surprisingly, L2P achieves competitive results against rehearsal-based
methods even without a rehearsal buffer and is directly applicable to
challenging task-agnostic continual learning. Source code is available at
this https URL.

    

### [[2112.08670] Amortized Noisy Channel Neural Machine Translation](http://arxiv.org/abs/2112.08670)


  Noisy channel models have been especially effective in neural machine
translation (NMT). However, recent approaches like "beam search and rerank"
(BSR) incur significant computation overhead during inference, making
real-world application infeasible. We aim to build an amortized noisy channel
NMT model such that greedily decoding from it would generate translations that
maximize the same reward as translations generated using BSR. We attempt three
approaches: knowledge distillation, 1-step-deviation imitation learning, and Q
learning. The first approach obtains the noisy channel signal from a
pseudo-corpus, and the latter two approaches aim to optimize toward a
noisy-channel MT reward directly. All three approaches speed up inference by
1-2 orders of magnitude. For all three approaches, the generated translations
fail to achieve rewards comparable to BSR, but the translation quality
approximated by BLEU is similar to the quality of BSR-produced translations.

    

### [[2112.08673] Intelligent Bearing Fault Diagnosis Method Combining Mixed Input and Hybrid CNN-MLP model](http://arxiv.org/abs/2112.08673)


  Rolling bearings are one of the most widely used bearings in industrial
machines. Deterioration in the condition of rolling bearings can result in the
total failure of rotating machinery. AI-based methods are widely applied in the
diagnosis of rolling bearings. Hybrid NN-based methods have been shown to
achieve the best diagnosis results. Typically, raw data is generated from
accelerometers mounted on the machine housing. However, the diagnostic utility
of each signal is highly dependent on the location of the corresponding
accelerometer. This paper proposes a novel hybrid CNN-MLP model-based
diagnostic method which combines mixed input to perform rolling bearing
diagnostics. The method successfully detects and localizes bearing defects
using acceleration data from a shaft-mounted wireless acceleration sensor. The
experimental results show that the hybrid model is superior to the CNN and MLP
models operating separately, and can deliver a high detection accuracy of 99,6%
for the bearing faults compared to 98% for CNN and 81% for MLP models.

    

### [[2112.08676] Machine Learning-Accelerated Computational Solid Mechanics: Application to Linear Elasticity](http://arxiv.org/abs/2112.08676)


  This work presents a novel physics-informed deep learning based
super-resolution framework to reconstruct high-resolution deformation fields
from low-resolution counterparts, obtained from coarse mesh simulations or
experiments. We leverage the governing equations and boundary conditions of the
physical system to train the model without using any high-resolution labeled
data. The proposed approach is applied to obtain the super-resolved deformation
fields from the low-resolution stress and displacement fields obtained by
running simulations on a coarse mesh for a body undergoing linear elastic
deformation. We demonstrate that the super-resolved fields match the accuracy
of an advanced numerical solver running at 400 times the coarse mesh
resolution, while simultaneously satisfying the governing laws. A brief
evaluation study comparing the performance of two deep learning based
super-resolution architectures is also presented.

    

### [[2112.08682] IsometricMT: Neural Machine Translation for Automatic Dubbing](http://arxiv.org/abs/2112.08682)


  Automatic dubbing (AD) is among the use cases where translations should fit a
given length template in order to achieve synchronicity between source and
target speech. For neural machine translation (MT), generating translations of
length close to the source length (e.g. within +-10% in character count), while
preserving quality is a challenging task. Controlling NMT output length comes
at a cost to translation quality which is usually mitigated with a two step
approach of generation of n-best hypotheses and then re-ranking them based on
length and quality. This work, introduces a self-learning approach that allows
a transformer model to directly learn to generate outputs that closely match
the source length, in short isometric MT. In particular, our approach for
isometric MT does not require to generate multiple hypotheses nor any auxiliary
scoring function. We report results on four language pairs (English - French,
Italian, German, Spanish) with a publicly available benchmark based on TED Talk
data. Both automatic and manual evaluations show that our self-learning
approach to performs on par with more complex isometric MT approaches.

    

### [[2112.08692] Lacuna Reconstruction: Self-supervised Pre-training for Low-Resource Historical Document Transcription](http://arxiv.org/abs/2112.08692)


  We present a self-supervised pre-training approach for learning rich visual
language representations for both handwritten and printed historical document
transcription. After supervised fine-tuning of our pre-trained encoder
representations for low-resource document transcription on two languages, (1) a
heterogeneous set of handwritten Islamicate manuscript images and (2) early
modern English printed documents, we show a meaningful improvement in
recognition accuracy over the same supervised model trained from scratch with
as few as 30 line image transcriptions for training. Our masked language
model-style pre-training strategy, where the model is trained to be able to
identify the true masked visual representation from distractors sampled from
within the same line, encourages learning robust contextualized language
representations invariant to scribal writing style and printing noise present
across documents.

    

### [[2112.08702] Learning to Share in Multi-Agent Reinforcement Learning](http://arxiv.org/abs/2112.08702)


  In this paper, we study the problem of networked multi-agent reinforcement
learning (MARL), where a number of agents are deployed as a partially connected
network and each interacts only with nearby agents. Networked MARL requires all
agents make decision in a decentralized manner to optimize a global objective
with restricted communication between neighbors over the network. Inspired by
the fact that \textit{sharing} plays a key role in human's learning of
cooperation, we propose LToS, a hierarchically decentralized MARL framework
that enables agents to learn to dynamically share reward with neighbors so as
to encourage agents to cooperate on the global objective. For each agent, the
high-level policy learns how to share reward with neighbors to decompose the
global objective, while the low-level policy learns to optimize local objective
induced by the high-level policies in the neighborhood. The two policies form a
bi-level optimization and learn alternately. We empirically demonstrate that
LToS outperforms existing methods in both social dilemma and networked MARL
scenario.

    

### [[2112.08706] Forecasting sales with Bayesian networks: a case study of a supermarket product in the presence of promotions](http://arxiv.org/abs/2112.08706)


  Sales forecasting is the prerequisite for a lot of managerial decisions such
as production planning, material resource planning and budgeting in the supply
chain. Promotions are one of the most important business strategies that are
often used to boost sales. While promotions are attractive for generating
demand, it is often difficult to forecast demand in their presence. In the past
few decades, several quantitative models have been developed to forecast sales
including statistical and machine learning models. However, these methods may
not be adequate to account for all the internal and external factors that may
impact sales. As a result, qualitative models have been adopted along with
quantitative methods as consulting experts has been proven to improve forecast
accuracy by providing contextual information. Such models are being used
extensively to account for factors that can lead to a rapid change in sales,
such as during promotions. In this paper, we aim to use Bayesian Networks to
forecast promotional sales where a combination of factors such as price, type
of promotions, and product location impacts sales. We choose to develop a BN
model because BN models essentially have the capability to combine various
qualitative and quantitative factors with causal forms, making it an attractive
tool for sales forecasting during promotions. This can be used to adjust a
company's promotional strategy in the context of this case study. We gather
sales data for a particular product from a retailer that sells products in
Australia. We develop a Bayesian Network for this product and validate our
results by empirical analysis. This paper confirms that BNs can be effectively
used to forecast sales, especially during promotions. In the end, we provide
some research avenues for using BNs in forecasting sales.

    

### [[2112.08718] Domain Prompts: Towards memory and compute efficient domain adaptation of ASR systems](http://arxiv.org/abs/2112.08718)


  Automatic Speech Recognition (ASR) systems have found their use in numerous
industrial applications in very diverse domains. Since domain-specific systems
perform better than their generic counterparts on in-domain evaluation, the
need for memory and compute-efficient domain adaptation is obvious.
Particularly, adapting parameter-heavy transformer-based language models used
for rescoring ASR hypothesis is challenging. In this work, we introduce
domain-prompts, a methodology that trains a small number of domain token
embedding parameters to prime a transformer-based LM to a particular domain.
With just a handful of extra parameters per domain, we achieve 7-14% WER
improvement over the baseline of using an unadapted LM. Despite being
parameter-efficient, these improvements are comparable to those of
fully-fine-tuned models with hundreds of millions of parameters. With ablations
on prompt-sizes, dataset sizes, initializations and domains, we provide
evidence for the benefits of using domain-prompts in ASR systems.

    

### [[2112.08733] Self-Supervised Dynamic Graph Representation Learning via Temporal Subgraph Contrast](http://arxiv.org/abs/2112.08733)


  Self-supervised learning on graphs has recently drawn a lot of attention due
to its independence from labels and its robustness in representation. Current
studies on this topic mainly use static information such as graph structures
but cannot well capture dynamic information such as timestamps of edges.
Realistic graphs are often dynamic, which means the interaction between nodes
occurs at a specific time. This paper proposes a self-supervised dynamic graph
representation learning framework (DySubC), which defines a temporal subgraph
contrastive learning task to simultaneously learn the structural and
evolutional features of a dynamic graph. Specifically, a novel temporal
subgraph sampling strategy is firstly proposed, which takes each node of the
dynamic graph as the central node and uses both neighborhood structures and
edge timestamps to sample the corresponding temporal subgraph. The subgraph
representation function is then designed according to the influence of
neighborhood nodes on the central node after encoding the nodes in each
subgraph. Finally, the structural and temporal contrastive loss are defined to
maximize the mutual information between node representation and temporal
subgraph representation. Experiments on five real-world datasets demonstrate
that (1) DySubC performs better than the related baselines including two graph
contrastive learning models and four dynamic graph representation learning
models in the downstream link prediction task, and (2) the use of temporal
information can not only sample more effective subgraphs, but also learn better
representation by temporal contrastive loss.

    

### [[2112.08736] Learning to Minimize Cost-to-Serve for Multi-Node Multi-Product Order Fulfilment in Electronic Commerce](http://arxiv.org/abs/2112.08736)


  We describe a novel decision-making problem developed in response to the
demands of retail electronic commerce (e-commerce). While working with
logistics and retail industry business collaborators, we found that the cost of
delivery of products from the most opportune node in the supply chain (a
quantity called the cost-to-serve or CTS) is a key challenge. The large scale,
high stochasticity, and large geographical spread of e-commerce supply chains
make this setting ideal for a carefully designed data-driven decision-making
algorithm. In this preliminary work, we focus on the specific subproblem of
delivering multiple products in arbitrary quantities from any warehouse to
multiple customers in each time period. We compare the relative performance and
computational efficiency of several baselines, including heuristics and
mixed-integer linear programming. We show that a reinforcement learning based
algorithm is competitive with these policies, with the potential of efficient
scale-up in the real world.

    

### [[2112.08746] Unsupervised Reinforcement Learning in Multiple Environments](http://arxiv.org/abs/2112.08746)


  Several recent works have been dedicated to unsupervised reinforcement
learning in a single environment, in which a policy is first pre-trained with
unsupervised interactions, and then fine-tuned towards the optimal policy for
several downstream supervised tasks defined over the same environment. Along
this line, we address the problem of unsupervised reinforcement learning in a
class of multiple environments, in which the policy is pre-trained with
interactions from the whole class, and then fine-tuned for several tasks in any
environment of the class. Notably, the problem is inherently multi-objective as
we can trade off the pre-training objective between environments in many ways.
In this work, we foster an exploration strategy that is sensitive to the most
adverse cases within the class. Hence, we cast the exploration problem as the
maximization of the mean of a critical percentile of the state visitation
entropy induced by the exploration strategy over the class of environments.
Then, we present a policy gradient algorithm, $\alpha$MEPOL, to optimize the
introduced objective through mediated interactions with the class. Finally, we
empirically demonstrate the ability of the algorithm in learning to explore
challenging classes of continuous environments and we show that reinforcement
learning greatly benefits from the pre-trained exploration strategy w.r.t.
learning from scratch.

    

### [[2112.08754] CLIN-X: pre-trained language models and a study on cross-task transfer for concept extraction in the clinical domain](http://arxiv.org/abs/2112.08754)


  The field of natural language processing (NLP) has recently seen a large
change towards using pre-trained language models for solving almost any task.
Despite showing great improvements in benchmark datasets for various tasks,
these models often perform sub-optimal in non-standard domains like the
clinical domain where a large gap between pre-training documents and target
documents is observed. In this paper, we aim at closing this gap with
domain-specific training of the language model and we investigate its effect on
a diverse set of downstream tasks and settings. We introduce the pre-trained
CLIN-X (Clinical XLM-R) language models and show how CLIN-X outperforms other
pre-trained transformer models by a large margin for ten clinical concept
extraction tasks from two languages. In addition, we demonstrate how the
transformer model can be further improved with our proposed task- and
language-agnostic model architecture based on ensembles over random splits and
cross-sentence context. Our studies in low-resource and transfer settings
reveal stable model performance despite a lack of annotated data with
improvements of up to 47 F1points when only 250 labeled sentences are
available. Our results highlight the importance of specialized language models
as CLIN-X for concept extraction in non-standard domains, but also show that
our task-agnostic model architecture is robust across the tested tasks and
languages so that domain- or task-specific adaptations are not required. The
CLIN-Xlanguage models and source code for fine-tuning and transferring the
model are publicly available at this https URL\_x/ and
the huggingface model hub.

    

### [[2112.08759] KnAC: an approach for enhancing cluster analysis with background knowledge and explanations](http://arxiv.org/abs/2112.08759)


  Pattern discovery in multidimensional data sets has been a subject of
research since decades. There exists a wide spectrum of clustering algorithms
that can be used for that purpose. However, their practical applications share
in common the post-clustering phase, which concerns expert-based interpretation
and analysis of the obtained results. We argue that this can be a bottleneck of
the process, especially in the cases where domain knowledge exists prior to
clustering. Such a situation requires not only a proper analysis of
automatically discovered clusters, but also a conformance checking with
existing knowledge. In this work, we present Knowledge Augmented Clustering
(KnAC), which main goal is to confront expert-based labelling with automated
clustering for the sake of updating and refining the former. Our solution does
not depend on any ready clustering algorithm, nor introduce one. Instead KnAC
can serve as an augmentation of an arbitrary clustering algorithm, making the
approach robust and model-agnostic. We demonstrate the feasibility of our
method on artificially, reproducible examples and on a real life use case
scenario.

    

### [[2112.08760] Constrained multi-objective optimization of process design parameters in settings with scarce data: an application to adhesive bonding](http://arxiv.org/abs/2112.08760)


  Adhesive joints are increasingly used in industry for a wide variety of
applications because of their favorable characteristics such as high
strength-to-weight ratio, design flexibility, limited stress concentrations,
planar force transfer, good damage tolerance and fatigue resistance. Finding
the optimal process parameters for an adhesive bonding process is challenging:
the optimization is inherently multi-objective (aiming to maximize break
strength while minimizing cost) and constrained (the process should not result
in any visual damage to the materials, and stress tests should not result in
failures that are adhesion-related). Real life physical experiments in the lab
are expensive to perform; traditional evolutionary approaches (such as genetic
algorithms) are then ill-suited to solve the problem, due to the prohibitive
amount of experiments required for evaluation. In this research, we
successfully applied specific machine learning techniques (Gaussian Process
Regression and Logistic Regression) to emulate the objective and constraint
functions based on a limited amount of experimental data. The techniques are
embedded in a Bayesian optimization algorithm, which succeeds in detecting
Pareto-optimal process settings in a highly efficient way (i.e., requiring a
limited number of extra experiments).

    

### [[2112.08761] DISTREAL: Distributed Resource-Aware Learning in Heterogeneous Systems](http://arxiv.org/abs/2112.08761)


  We study the problem of distributed training of neural networks (NNs) on
devices with heterogeneous, limited, and time-varying availability of
computational resources. We present an adaptive, resource-aware, on-device
learning mechanism, DISTREAL, which is able to fully and efficiently utilize
the available resources on devices in a distributed manner, increasing the
convergence speed. This is achieved with a dropout mechanism that dynamically
adjusts the computational complexity of training an NN by randomly dropping
filters of convolutional layers of the model. Our main contribution is the
introduction of a design space exploration (DSE) technique, which finds
Pareto-optimal per-layer dropout vectors with respect to resource requirements
and convergence speed of the training. Applying this technique, each device is
able to dynamically select the dropout vector that fits its available resource
without requiring any assistance from the server. We implement our solution in
a federated learning (FL) system, where the availability of computational
resources varies both between devices and over time, and show through extensive
evaluation that we are able to significantly increase the convergence speed
over the state of the art without compromising on the final accuracy.

    

### [[2112.08764] Graph Convolutional Networks with Dual Message Passing for Subgraph Isomorphism Counting and Matching](http://arxiv.org/abs/2112.08764)


  Graph neural networks (GNNs) and message passing neural networks (MPNNs) have
been proven to be expressive for subgraph structures in many applications. Some
applications in heterogeneous graphs require explicit edge modeling, such as
subgraph isomorphism counting and matching. However, existing message passing
mechanisms are not designed well in theory. In this paper, we start from a
particular edge-to-vertex transform and exploit the isomorphism property in the
edge-to-vertex dual graphs. We prove that searching isomorphisms on the
original graph is equivalent to searching on its dual graph. Based on this
observation, we propose dual message passing neural networks (DMPNNs) to
enhance the substructure representation learning in an asynchronous way for
subgraph isomorphism counting and matching as well as unsupervised node
classification. Extensive experiments demonstrate the robust performance of
DMPNNs by combining both node and edge representation learning in synthetic and
real heterogeneous graphs. Code is available at
this https URL.

    

### [[2112.08767] Adaptation and Attention for Neural Video Coding](http://arxiv.org/abs/2112.08767)


  Neural image coding represents now the state-of-the-art image compression
approach. However, a lot of work is still to be done in the video domain. In
this work, we propose an end-to-end learned video codec that introduces several
architectural novelties as well as training novelties, revolving around the
concepts of adaptation and attention. Our codec is organized as an intra-frame
codec paired with an inter-frame codec. As one architectural novelty, we
propose to train the inter-frame codec model to adapt the motion estimation
process based on the resolution of the input video. A second architectural
novelty is a new neural block that combines concepts from split-attention based
neural networks and from DenseNets. Finally, we propose to overfit a set of
decoder-side multiplicative parameters at inference time. Through ablation
studies and comparisons to prior art, we show the benefits of our proposed
techniques in terms of coding gains. We compare our codec to VVC/H.266 and
RLVC, which represent the state-of-the-art traditional and end-to-end learned
codecs, respectively, and to the top performing end-to-end learned approach in
2021 CLIC competition, E2E_T_OL. Our codec clearly outperforms E2E_T_OL, and
compare favorably to VVC and RLVC in some settings.

    

### [[2112.08770] A Proposition-Level Clustering Approach for Multi-Document Summarization](http://arxiv.org/abs/2112.08770)


  Text clustering methods were traditionally incorporated into multi-document
summarization (MDS) as a means for coping with considerable information
repetition. Clusters were leveraged to indicate information saliency and to
avoid redundancy. These methods focused on clustering sentences, even though
closely related sentences also usually contain non-aligning information. In
this work, we revisit the clustering approach, grouping together propositions
for more precise information alignment. Specifically, our method detects
salient propositions, clusters them into paraphrastic clusters, and generates a
representative sentence for each cluster by fusing its propositions. Our
summarization method improves over the previous state-of-the-art MDS method in
the DUC 2004 and TAC 2011 datasets, both in automatic ROUGE scores and human
preference.

    

### [[2112.08772] δ-SAM: Sharpness-Aware Minimization with Dynamic Reweighting](http://arxiv.org/abs/2112.08772)


  Deep neural networks are often overparameterized and may not easily achieve
model generalization. Adversarial training has shown effectiveness in improving
generalization by regularizing the change of loss on top of adversarially
chosen perturbations. The recently proposed sharpness-aware minimization (SAM)
algorithm adopts adversarial weight perturbation, encouraging the model to
converging to a flat minima. Unfortunately, due to increased computational
cost, adversarial weight perturbation can only be efficiently approximated
per-batch instead of per-instance, leading to degraded performance. In this
paper, we propose that dynamically reweighted perturbation within each batch,
where unguarded instances are up-weighted, can serve as a better approximation
to per-instance perturbation. We propose sharpness-aware minimization with
dynamic reweighting ({\delta}-SAM), which realizes the idea with efficient
guardedness estimation. Experiments on the GLUE benchmark demonstrate the
effectiveness of {\delta}-SAM.

    

### [[2112.08774] BoGraph: Structured Bayesian Optimization From Logs for Systems with High-dimensional Parameter Space](http://arxiv.org/abs/2112.08774)


  Current auto-tuning frameworks struggle with tuning computer systems
configurations due to their large parameter space, complex interdependencies,
and high evaluation cost. Utilizing probabilistic models, Structured Bayesian
Optimization (SBO) has recently overcome these difficulties. SBO decomposes the
parameter space by utilizing contextual information provided by system experts
leading to fast convergence. However, the complexity of building probabilistic
models has hindered its wider adoption. We propose BoAnon, a SBO framework that
learns the system structure from its logs. BoAnon provides an API enabling
experts to encode knowledge of the system as performance models or components
dependency. BoAnon takes in the learned structure and transforms it into a
probabilistic graph model. Then it applies the expert-provided knowledge to the
graph to further contextualize the system behavior. BoAnon probabilistic graph
allows the optimizer to find efficient configurations faster than other
methods. We evaluate BoAnon via a hardware architecture search problem,
achieving an improvement in energy-latency objectives ranging from $5-7$
x-factors improvement over the default architecture. With its novel contextual
structure learning pipeline, BoAnon makes using SBO accessible for a wide range
of other computer systems such as databases and stream processors.

    

### [[2112.08782] Improved YOLOv5 network for real-time multi-scale traffic sign detection](http://arxiv.org/abs/2112.08782)


  Traffic sign detection is a challenging task for the unmanned driving system,
especially for the detection of multi-scale targets and the real-time problem
of detection. In the traffic sign detection process, the scale of the targets
changes greatly, which will have a certain impact on the detection accuracy.
Feature pyramid is widely used to solve this problem but it might break the
feature consistency across different scales of traffic signs. Moreover, in
practical application, it is difficult for common methods to improve the
detection accuracy of multi-scale traffic signs while ensuring real-time
detection. In this paper, we propose an improved feature pyramid model, named
AF-FPN, which utilizes the adaptive attention module (AAM) and feature
enhancement module (FEM) to reduce the information loss in the process of
feature map generation and enhance the representation ability of the feature
pyramid. We replaced the original feature pyramid network in YOLOv5 with
AF-FPN, which improves the detection performance for multi-scale targets of the
YOLOv5 network under the premise of ensuring real-time detection. Furthermore,
a new automatic learning data augmentation method is proposed to enrich the
dataset and improve the robustness of the model to make it more suitable for
practical scenarios. Extensive experimental results on the Tsinghua-Tencent
100K (TT100K) dataset demonstrate the effectiveness and superiority of the
proposed method when compared with several state-of-the-art methods.

    

### [[2112.08796] Saliency Grafting: Innocuous Attribution-Guided Mixup with Calibrated Label Mixing](http://arxiv.org/abs/2112.08796)


  The Mixup scheme suggests mixing a pair of samples to create an augmented
training sample and has gained considerable attention recently for improving
the generalizability of neural networks. A straightforward and widely used
extension of Mixup is to combine with regional dropout-like methods: removing
random patches from a sample and replacing it with the features from another
sample. Albeit their simplicity and effectiveness, these methods are prone to
create harmful samples due to their randomness. To address this issue, 'maximum
saliency' strategies were recently proposed: they select only the most
informative features to prevent such a phenomenon. However, they now suffer
from lack of sample diversification as they always deterministically select
regions with maximum saliency, injecting bias into the augmented data. In this
paper, we present, a novel, yet simple Mixup-variant that captures the best of
both worlds. Our idea is two-fold. By stochastically sampling the features and
'grafting' them onto another sample, our method effectively generates diverse
yet meaningful samples. Its second ingredient is to produce the label of the
grafted sample by mixing the labels in a saliency-calibrated fashion, which
rectifies supervision misguidance introduced by the random sampling procedure.
Our experiments under CIFAR, Tiny-ImageNet, and ImageNet datasets show that our
scheme outperforms the current state-of-the-art augmentation strategies not
only in terms of classification accuracy, but is also superior in coping under
stress conditions such as data corruption and object occlusion.

    

### [[2112.08798] Understanding Memorization from the Perspective of Optimization via Efficient Influence Estimation](http://arxiv.org/abs/2112.08798)


  Over-parameterized deep neural networks are able to achieve excellent
training accuracy while maintaining a small generalization error. It has also
been found that they are able to fit arbitrary labels, and this behaviour is
referred to as the phenomenon of memorization. In this work, we study the
phenomenon of memorization with turn-over dropout, an efficient method to
estimate influence and memorization, for data with true labels (real data) and
data with random labels (random data). Our main findings are: (i) For both real
data and random data, the optimization of easy examples (e.g., real data) and
difficult examples (e.g., random data) are conducted by the network
simultaneously, with easy ones at a higher speed; (ii) For real data, a correct
difficult example in the training dataset is more informative than an easy one.
By showing the existence of memorization on random data and real data, we
highlight the consistency between them regarding optimization and we emphasize
the implication of memorization during optimization.

    

### [[2112.08806] Dataset correlation inference attacks against machine learning models](http://arxiv.org/abs/2112.08806)


  Machine learning models are increasingly used by businesses and organizations
around the world to automate tasks and decision-making. Trained on potentially
sensitive datasets, machine learning models have been shown to leak information
about individuals in the dataset as well as global dataset information. We here
take research in dataset property inference attacks one step further by
proposing a new attack against ML models: a dataset correlation inference
attack, where an attacker's goal is to infer the correlation between input
variables of a model. We first show that an attacker can exploit the spherical
parametrization of correlation matrices, to make an informed guess. This means
that using only the correlation between the input variables and the target
variable, an attacker can infer the correlation between two input variables
much better than a random guess baseline. We propose a second attack which
exploits the access to a machine learning model using shadow modeling to refine
the guess. Our attack uses Gaussian copula-based generative modeling to
generate synthetic datasets with a wide variety of correlations in order to
train a meta-model for the correlation inference task. We evaluate our attack
against Logistic Regression and Multi-layer perceptron models and show it to
outperform the model-less attack. Our results show that the accuracy of the
second, machine learning-based attack decreases with the number of variables
and converges towards the accuracy of the model-less attack. However,
correlations between input variables which are highly correlated with the
target variable are more vulnerable regardless of the number of variables. Our
work bridges the gap between what can be considered a global leakage about the
training dataset and individual-level leakages. When coupled with marginal
leakage attacks,it might also constitute a first step towards dataset
reconstruction.

    

### [[2112.08812] Ditch the Gold Standard: Re-evaluating Conversational Question Answering](http://arxiv.org/abs/2112.08812)


  Conversational question answering (CQA) systems aim to provide
natural-language answers to users in information-seeking conversations.
Existing CQA benchmarks compare models with pre-collected human-human
conversations, using ground-truth answers provided in conversational history.
It remains unclear whether we can rely on this static evaluation for model
development and whether current systems can well generalize to real-world
human-machine conversations. In this work, we conduct the first large-scale
human evaluation of state-of-the-art CQA systems, where human evaluators
converse with models and judge the correctness of their answers. We find that
the distribution of human-machine conversations differs drastically from that
of human-human conversations, and there is a disagreement between human and
gold-history evaluation in terms of model ranking. We further investigate how
to improve automatic evaluations, and propose a question rewriting mechanism
based on predicted history, which better correlates with human judgments.
Finally, we discuss the impact of various modeling strategies and future
directions towards better conversational question answering systems.

    

### [[2112.08817] Search for temporal cell segmentation robustness in phase-contrast microscopy videos](http://arxiv.org/abs/2112.08817)


  Studying cell morphology changes in time is critical to understanding cell
migration mechanisms. In this work, we present a deep learning-based workflow
to segment cancer cells embedded in 3D collagen matrices and imaged with
phase-contrast microscopy. Our approach uses transfer learning and recurrent
convolutional long-short term memory units to exploit the temporal information
from the past and provide a consistent segmentation result. Lastly, we propose
a geometrical-characterization approach to studying cancer cell morphology. Our
approach provides stable results in time, and it is robust to the different
weight initialization or training data sampling. We introduce a new annotated
dataset for 2D cell segmentation and tracking, and an open-source
implementation to replicate the experiments or adapt them to new image
processing problems.

    

### [[2112.08830] Graph-wise Common Latent Factor Extraction for Unsupervised Graph Representation Learning](http://arxiv.org/abs/2112.08830)


  Unsupervised graph-level representation learning plays a crucial role in a
variety of tasks such as molecular property prediction and community analysis,
especially when data annotation is expensive. Currently, most of the
best-performing graph embedding methods are based on Infomax principle. The
performance of these methods highly depends on the selection of negative
samples and hurt the performance, if the samples were not carefully selected.
Inter-graph similarity-based methods also suffer if the selected set of graphs
for similarity matching is low in quality. To address this, we focus only on
utilizing the current input graph for embedding learning. We are motivated by
an observation from real-world graph generation processes where the graphs are
formed based on one or more global factors which are common to all elements of
the graph (e.g., topic of a discussion thread, solubility level of a molecule).
We hypothesize extracting these common factors could be highly beneficial.
Hence, this work proposes a new principle for unsupervised graph representation
learning: Graph-wise Common latent Factor EXtraction (GCFX). We further propose
a deep model for GCFX, deepGCFX, based on the idea of reversing the
above-mentioned graph generation process which could explicitly extract common
latent factors from an input graph and achieve improved results on downstream
tasks to the current state-of-the-art. Through extensive experiments and
analysis, we demonstrate that, while extracting common latent factors is
beneficial for graph-level tasks to alleviate distractions caused by local
variations of individual nodes or local neighbourhoods, it also benefits
node-level tasks by enabling long-range node dependencies, especially for
disassortative graphs.

    

### [[2112.08835] Self-supervised Enhancement of Latent Discovery in GANs](http://arxiv.org/abs/2112.08835)


  Several methods for discovering interpretable directions in the latent space
of pre-trained GANs have been proposed. Latent semantics discovered by
unsupervised methods are relatively less disentangled than supervised methods
since they do not use pre-trained attribute classifiers. We propose Scale
Ranking Estimator (SRE), which is trained using self-supervision. SRE enhances
the disentanglement in directions obtained by existing unsupervised
disentanglement techniques. These directions are updated to preserve the
ordering of variation within each direction in latent space. Qualitative and
quantitative evaluation of the discovered directions demonstrates that our
proposed method significantly improves disentanglement in various datasets. We
also show that the learned SRE can be used to perform Attribute-based image
retrieval task without further training.

    

### [[2112.08836] Imbalanced Sample Generation and Evaluation for Power System Transient Stability Using CTGAN](http://arxiv.org/abs/2112.08836)


  Although deep learning has achieved impressive advances in transient
stability assessment of power systems, the insufficient and imbalanced samples
still trap the training effect of the data-driven methods. This paper proposes
a controllable sample generation framework based on Conditional Tabular
Generative Adversarial Network (CTGAN) to generate specified transient
stability samples. To fit the complex feature distribution of the transient
stability samples, the proposed framework firstly models the samples as tabular
data and uses Gaussian mixture models to normalize the tabular data. Then we
transform multiple conditions into a single conditional vector to enable
multi-conditional generation. Furthermore, this paper introduces three
evaluation metrics to verify the quality of generated samples based on the
proposed framework. Experimental results on the IEEE 39-bus system show that
the proposed framework effectively balances the transient stability samples and
significantly improves the performance of transient stability assessment
models.

    

### [[2112.08841] A CNN based method for Sub-pixel Urban Land Cover Classification using Landsat-5 TM and Resourcesat-1 LISS-IV Imagery](http://arxiv.org/abs/2112.08841)


  Time series data of urban land cover is of great utility in analyzing urban
growth patterns, changes in distribution of impervious surface and vegetation
and resulting impacts on urban micro climate. While Landsat data is ideal for
such analysis due to the long time series of free imagery, traditional
per-pixel hard classification fails to yield full potential of the Landsat
data. This paper proposes a sub-pixel classification method that leverages the
temporal overlap of Landsat-5 TM and Resourcesat-1 LISS-IV sensors. We train a
convolutional neural network to predict fractional land cover maps from 30m
Landsat-5 TM data. The reference land cover fractions are estimated from a
hard-classified 5.8m LISS-IV image for Bengaluru from 2011. Further, we
demonstrate the generalizability and superior performance of the proposed model
using data for Mumbai from 2009 and comparing it to the results obtained using
a Random Forest classifier. For both Bengaluru (2011) and Mumbai (2009) data,
Mean Absolute Percentage Error of our CNN model is in the range of 7.2 to 11.3
for both built-up and vegetation fraction prediction at the 30m cell level.
Unlike most recent studies where validation is conducted using data for a
limited spatial extent, our model has been trained and validated using data for
the complete spatial extent of two mega cities for two different time periods.
Hence it can reliably generate 30m built-up and vegetation fraction maps from
Landsat-5 TM time series data to analyze long term urban growth patterns.

    

### [[2112.08844] Adapting Document-Grounded Dialog Systems to Spoken Conversations using Data Augmentation and a Noisy Channel Model](http://arxiv.org/abs/2112.08844)


  This paper summarizes our submission to Task 2 of the second track of the
10th Dialog System Technology Challenge (DSTC10) "Knowledge-grounded
Task-oriented Dialogue Modeling on Spoken Conversations". Similar to the
previous year's iteration, the task consists of three subtasks: detecting
whether a turn is knowledge seeking, selecting the relevant knowledge document
and finally generating a grounded response. This year, the focus lies on
adapting the system to noisy ASR transcripts. We explore different approaches
to make the models more robust to this type of input and to adapt the generated
responses to the style of spoken conversations. For the latter, we get the best
results with a noisy channel model that additionally reduces the number of
short and generic responses. Our best system achieved the 1st rank in the
automatic and the 3rd rank in the human evaluation of the challenge.

    

### [[2112.08845] Multiple Instance Learning for Brain Tumor Detection from Magnetic Resonance Spectroscopy Data](http://arxiv.org/abs/2112.08845)


  We apply deep learning (DL) on Magnetic resonance spectroscopy (MRS) data for
the task of brain tumor detection. Medical applications often suffer from data
scarcity and corruption by noise. Both of these problems are prominent in our
data set. Furthermore, a varying number of spectra are available for the
different patients. We address these issues by considering the task as a
multiple instance learning (MIL) problem. Specifically, we aggregate multiple
spectra from the same patient into a "bag" for classification and apply data
augmentation techniques. To achieve the permutation invariance during the
process of bagging, we proposed two approaches: (1) to apply min-, max-, and
average-pooling on the features of all samples in one bag and (2) to apply an
attention mechanism. We tested these two approaches on multiple neural network
architectures. We demonstrate that classification performance is significantly
improved when training on multiple instances rather than single spectra. We
propose a simple oversampling data augmentation method and show that it could
further improve the performance. Finally, we demonstrate that our proposed
model outperforms manual classification by neuroradiologists according to most
performance metrics.

    

### [[2112.08851] Classification Under Ambiguity: When Is Average-K Better Than Top-K?](http://arxiv.org/abs/2112.08851)


  When many labels are possible, choosing a single one can lead to low
precision. A common alternative, referred to as top-$K$ classification, is to
choose some number $K$ (commonly around 5) and to return the $K$ labels with
the highest scores. Unfortunately, for unambiguous cases, $K>1$ is too many
and, for very ambiguous cases, $K \leq 5$ (for example) can be too small. An
alternative sensible strategy is to use an adaptive approach in which the
number of labels returned varies as a function of the computed ambiguity, but
must average to some particular $K$ over all the samples. We denote this
alternative average-$K$ classification. This paper formally characterizes the
ambiguity profile when average-$K$ classification can achieve a lower error
rate than a fixed top-$K$ classification. Moreover, it provides natural
estimation procedures for both the fixed-size and the adaptive classifier and
proves their consistency. Finally, it reports experiments on real-world image
data sets revealing the benefit of average-$K$ classification over top-$K$ in
practice. Overall, when the ambiguity is known precisely, average-$K$ is never
worse than top-$K$, and, in our experiments, when it is estimated, this also
holds.

    

### [[2112.08866] BayesFlow can reliably detect Model Misspecification and Posterior Errors in Amortized Bayesian Inference](http://arxiv.org/abs/2112.08866)


  Neural density estimators have proven remarkably powerful in performing
efficient simulation-based Bayesian inference in various research domains. In
particular, the BayesFlow framework uses a two-step approach to enable
amortized parameter estimation in settings where the likelihood function is
implicitly defined by a simulation program. But how faithful is such inference
when simulations are poor representations of reality? In this paper, we
conceptualize the types of model misspecification arising in simulation-based
inference and systematically investigate the performance of the BayesFlow
framework under these misspecifications. We propose an augmented optimization
objective which imposes a probabilistic structure on the latent data space and
utilize maximum mean discrepancy (MMD) to detect potentially catastrophic
misspecifications during inference undermining the validity of the obtained
results. We verify our detection criterion on a number of artificial and
realistic misspecifications, ranging from toy conjugate models to complex
models of decision making and disease outbreak dynamics applied to real data.
Further, we show that posterior inference errors increase as a function of the
distance between the true data-generating distribution and the typical set of
simulations in the latent summary space. Thus, we demonstrate the dual utility
of MMD as a method for detecting model misspecification and as a proxy for
verifying the faithfulness of amortized Bayesian inference.

    

### [[2112.08903] Graph Structure Learning with Variational Information Bottleneck](http://arxiv.org/abs/2112.08903)


  Graph Neural Networks (GNNs) have shown promising results on a broad spectrum
of applications. Most empirical studies of GNNs directly take the observed
graph as input, assuming the observed structure perfectly depicts the accurate
and complete relations between nodes. However, graphs in the real world are
inevitably noisy or incomplete, which could even exacerbate the quality of
graph representations. In this work, we propose a novel Variational Information
Bottleneck guided Graph Structure Learning framework, namely VIB-GSL, in the
perspective of information theory. VIB-GSL advances the Information Bottleneck
(IB) principle for graph structure learning, providing a more elegant and
universal framework for mining underlying task-relevant relations. VIB-GSL
learns an informative and compressive graph structure to distill the actionable
information for specific downstream tasks. VIB-GSL deduces a variational
approximation for irregular graph data to form a tractable IB objective
function, which facilitates training stability. Extensive experimental results
demonstrate that the superior effectiveness and robustness of VIB-GSL.

    

### [[2112.08909] CodedPaddedFL and CodedSecAgg: Straggler Mitigation and Secure Aggregation in Federated Learning](http://arxiv.org/abs/2112.08909)


  We present two novel coded federated learning (FL) schemes for linear
regression that mitigate the effect of straggling devices. The first scheme,
CodedPaddedFL, mitigates the effect of straggling devices while retaining the
privacy level of conventional FL. Particularly, it combines one-time padding
for user data privacy with gradient codes to yield resiliency against
straggling devices. To apply one-time padding to real data, our scheme exploits
a fixed-point arithmetic representation of the data. For a scenario with 25
devices, CodedPaddedFL achieves a speed-up factor of 6.6 and 9.2 for an
accuracy of 95\% and 85\% on the MMIST and Fashion-MNIST datasets,
respectively, compared to conventional FL. Furthermore, it yields similar
performance in terms of latency compared to a recently proposed scheme by
Prakash \emph{et al.} without the shortcoming of additional leakage of private
data. The second scheme, CodedSecAgg, provides straggler resiliency and
robustness against model inversion attacks and is based on Shamir's secret
sharing. CodedSecAgg outperforms state-of-the-art secure aggregation schemes
such as LightSecAgg by a speed-up factor of 6.6--14.6, depending on the number
of colluding devices, on the MNIST dataset for a scenario with 120 devices, at
the expense of a 30\% increase in latency compared to CodedPaddedFL.

    

### [[2112.08914] Characterizing and addressing the issue of oversmoothing in neural autoregressive sequence modeling](http://arxiv.org/abs/2112.08914)


  Neural autoregressive sequence models smear the probability among many
possible sequences including degenerate ones, such as empty or repetitive
sequences. In this work, we tackle one specific case where the model assigns a
high probability to unreasonably short sequences. We define the oversmoothing
rate to quantify this issue. After confirming the high degree of oversmoothing
in neural machine translation, we propose to explicitly minimize the
oversmoothing rate during training. We conduct a set of experiments to study
the effect of the proposed regularization on both model distribution and
decoding performance. We use a neural machine translation task as the testbed
and consider three different datasets of varying size. Our experiments reveal
three major findings. First, we can control the oversmoothing rate of the model
by tuning the strength of the regularization. Second, by enhancing the
oversmoothing loss contribution, the probability and the rank of <eos> token
decrease heavily at positions where it is not supposed to be. Third, the
proposed regularization impacts the outcome of beam search especially when a
large beam is used. The degradation of translation quality (measured in BLEU)
with a large beam significantly lessens with lower oversmoothing rate, but the
degradation compared to smaller beam sizes remains to exist. From these
observations, we conclude that the high degree of oversmoothing is the main
reason behind the degenerate case of overly probable short sequences in a
neural autoregressive model.

    

### [[2112.08916] GOSH: Task Scheduling Using Deep Surrogate Models in Fog Computing Environments](http://arxiv.org/abs/2112.08916)


  Recently, intelligent scheduling approaches using surrogate models have been
proposed to efficiently allocate volatile tasks in heterogeneous fog
environments. Advances like deterministic surrogate models, deep neural
networks (DNN) and gradient-based optimization allow low energy consumption and
response times to be reached. However, deterministic surrogate models, which
estimate objective values for optimization, do not consider the uncertainties
in the distribution of the Quality of Service (QoS) objective function that can
lead to high Service Level Agreement (SLA) violation rates. Moreover, the
brittle nature of DNN training and prevent such models from reaching minimal
energy or response times. To overcome these difficulties, we present a novel
scheduler: GOSH i.e. Gradient Based Optimization using Second Order derivatives
and Heteroscedastic Deep Surrogate Models. GOSH uses a second-order gradient
based optimization approach to obtain better QoS and reduce the number of
iterations to converge to a scheduling decision, subsequently lowering the
scheduling time. Instead of a vanilla DNN, GOSH uses a Natural Parameter
Network to approximate objective scores. Further, a Lower Confidence Bound
optimization approach allows GOSH to find an optimal trade-off between greedy
minimization of the mean latency and uncertainty reduction by employing
error-based exploration. Thus, GOSH and its co-simulation based extension
GOSH*, can adapt quickly and reach better objective scores than baseline
methods. We show that GOSH* reaches better objective scores than GOSH, but it
is suitable only for high resource availability settings, whereas GOSH is apt
for limited resource settings. Real system experiments for both GOSH and GOSH*
show significant improvements against the state-of-the-art in terms of energy
consumption, response time and SLA violations by up to 18, 27 and 82 percent,
respectively.

    

### [[2112.08919] Deep Generative Models for Geometric Design Under Uncertainty](http://arxiv.org/abs/2112.08919)


  Deep generative models have demonstrated effectiveness in learning compact
and expressive design representations that significantly improve geometric
design optimization. However, these models do not consider the uncertainty
introduced by manufacturing or fabrication. Past work that quantifies such
uncertainty often makes simplified assumptions on geometric variations, while
the "real-world" uncertainty and its impact on design performance are difficult
to quantify due to the high dimensionality. To address this issue, we propose a
Generative Adversarial Network-based Design under Uncertainty Framework
(GAN-DUF), which contains a deep generative model that simultaneously learns a
compact representation of nominal (ideal) designs and the conditional
distribution of fabricated designs given any nominal design. We demonstrated
the framework on two real-world engineering design examples and showed its
capability of finding the solution that possesses better performances after
fabrication.

    

### [[2112.08929] Bootstrap Equilibrium and Probabilistic Speaker Representation Learning for Self-supervised Speaker Verification](http://arxiv.org/abs/2112.08929)


  In this paper, we propose self-supervised speaker representation learning
strategies, which comprise of a bootstrap equilibrium speaker representation
learning in the front-end and an uncertainty-aware probabilistic speaker
embedding training in the back-end. In the front-end stage, we learn the
speaker representations via the bootstrap training scheme with the uniformity
regularization term. In the back-end stage, the probabilistic speaker
embeddings are estimated by maximizing the mutual likelihood score between the
speech samples belonging to the same speaker, which provide not only speaker
representations but also data uncertainty. Experimental results show that the
proposed bootstrap equilibrium training strategy can effectively help learn the
speaker representations and outperforms the conventional methods based on
contrastive learning. Also, we demonstrate that the integrated two-stage
framework further improves the speaker verification performance on the
VoxCeleb1 test set in terms of EER and MinDCF.

    

### [[2112.08930] Intelli-Paint: Towards Developing Human-like Painting Agents](http://arxiv.org/abs/2112.08930)


  The generation of well-designed artwork is often quite time-consuming and
assumes a high degree of proficiency on part of the human painter. In order to
facilitate the human painting process, substantial research efforts have been
made on teaching machines how to "paint like a human", and then using the
trained agent as a painting assistant tool for human users. However, current
research in this direction is often reliant on a progressive grid-based
division strategy wherein the agent divides the overall image into successively
finer grids, and then proceeds to paint each of them in parallel. This
inevitably leads to artificial painting sequences which are not easily
intelligible to human users. To address this, we propose a novel painting
approach which learns to generate output canvases while exhibiting a more
human-like painting style. The proposed painting pipeline Intelli-Paint
consists of 1) a progressive layering strategy which allows the agent to first
paint a natural background scene representation before adding in each of the
foreground objects in a progressive fashion. 2) We also introduce a novel
sequential brushstroke guidance strategy which helps the painting agent to
shift its attention between different image regions in a semantic-aware manner.
3) Finally, we propose a brushstroke regularization strategy which allows for
~60-80% reduction in the total number of required brushstrokes without any
perceivable differences in the quality of the generated canvases. Through both
quantitative and qualitative results, we show that the resulting agents not
only show enhanced efficiency in output canvas generation but also exhibit a
more natural-looking painting style which would better assist human users
express their ideas through digital artwork.

    

### [[2112.08931] COVID-19 Electrocardiograms Classification using CNN Models](http://arxiv.org/abs/2112.08931)


  With the periodic rise and fall of COVID-19 and numerous countries being
affected by its ramifications, there has been a tremendous amount of work that
has been done by scientists, researchers, and doctors all over the world.
Prompt intervention is keenly needed to tackle the unconscionable dissemination
of the disease. The implementation of Artificial Intelligence (AI) has made a
significant contribution to the digital health district by applying the
fundamentals of deep learning algorithms. In this study, a novel approach is
proposed to automatically diagnose the COVID-19 by the utilization of
Electrocardiogram (ECG) data with the integration of deep learning algorithms,
specifically the Convolutional Neural Network (CNN) models. Several CNN models
have been utilized in this proposed framework, including VGG16, VGG19,
InceptionResnetv2, InceptionV3, Resnet50, and Densenet201. The VGG16 model has
outperformed the rest of the models, with an accuracy of 85.92%. Our results
show a relatively low accuracy in the rest of the models compared to the VGG16
model, which is due to the small size of the utilized dataset, in addition to
the exclusive utilization of the Grid search hyperparameters optimization
approach for the VGG16 model only. Moreover, our results are preparatory, and
there is a possibility to enhance the accuracy of all models by further
expanding the dataset and adapting a suitable hyperparameters optimization
technique.

    

### [[2112.08932] Learning from Guided Play: A Scheduled Hierarchical Approach for Improving Exploration in Adversarial Imitation Learning](http://arxiv.org/abs/2112.08932)


  Effective exploration continues to be a significant challenge that prevents
the deployment of reinforcement learning for many physical systems. This is
particularly true for systems with continuous and high-dimensional state and
action spaces, such as robotic manipulators. The challenge is accentuated in
the sparse rewards setting, where the low-level state information required for
the design of dense rewards is unavailable. Adversarial imitation learning
(AIL) can partially overcome this barrier by leveraging expert-generated
demonstrations of optimal behaviour and providing, essentially, a replacement
for dense reward information. Unfortunately, the availability of expert
demonstrations does not necessarily improve an agent's capability to explore
effectively and, as we empirically show, can lead to inefficient or stagnated
learning. We present Learning from Guided Play (LfGP), a framework in which we
leverage expert demonstrations of, in addition to a main task, multiple
auxiliary tasks. Subsequently, a hierarchical model is used to learn each task
reward and policy through a modified AIL procedure, in which exploration of all
tasks is enforced via a scheduler composing different tasks together. This
affords many benefits: learning efficiency is improved for main tasks with
challenging bottleneck transitions, expert data becomes reusable between tasks,
and transfer learning through the reuse of learned auxiliary task models
becomes possible. Our experimental results in a challenging multitask robotic
manipulation domain indicate that our method compares favourably to supervised
imitation learning and to a state-of-the-art AIL method. Code is available at
this https URL.

    

### [[2112.08933] Responsive parallelized architecture for deploying deep learning models in production environments](http://arxiv.org/abs/2112.08933)


  Recruiters can easily shortlist candidates for jobs via viewing their
curriculum vitae document. Unstructured document CV beholds candidates
portfolio and named entities listing details. The main aim of this study is to
design and propose a web oriented, highly responsive, computational pipeline
that systematically predicts CV entities using hierarchically refined label
attention networks.

    

### [[2112.08940] Challenges and Solutions to Build a Data Pipeline to Identify Anomalies in Enterprise System Performance](http://arxiv.org/abs/2112.08940)


  We discuss how VMware is solving the following challenges to harness data to
operate our ML-based anomaly detection system to detect performance issues in
our Software Defined Data Center (SDDC) enterprise deployments: (i) label
scarcity and label bias due to heavy dependency on unscalable human annotators,
and (ii) data drifts due to ever-changing workload patterns, software stack and
underlying hardware. Our anomaly detection system has been deployed in
production for many years and has successfully detected numerous major
performance issues. We demonstrate that by addressing these data challenges, we
not only improve the accuracy of our performance anomaly detection model by
30%, but also ensure that the model performance to never degrade over time.

    

### [[2112.08949] Slot-VPS: Object-centric Representation Learning for Video Panoptic Segmentation](http://arxiv.org/abs/2112.08949)


  Video Panoptic Segmentation (VPS) aims at assigning a class label to each
pixel, uniquely segmenting and identifying all object instances consistently
across all frames. Classic solutions usually decompose the VPS task into
several sub-tasks and utilize multiple surrogates (e.g. boxes and masks,
centres and offsets) to represent objects. However, this divide-and-conquer
strategy requires complex post-processing in both spatial and temporal domains
and is vulnerable to failures from surrogate tasks. In this paper, inspired by
object-centric learning which learns compact and robust object representations,
we present Slot-VPS, the first end-to-end framework for this task. We encode
all panoptic entities in a video, including both foreground instances and
background semantics, with a unified representation called panoptic slots. The
coherent spatio-temporal object's information is retrieved and encoded into the
panoptic slots by the proposed Video Panoptic Retriever, enabling it to
localize, segment, differentiate, and associate objects in a unified manner.
Finally, the output panoptic slots can be directly converted into the class,
mask, and object ID of panoptic objects in the video. We conduct extensive
ablation studies and demonstrate the effectiveness of our approach on two
benchmark datasets, Cityscapes-VPS (\textit{val} and test sets) and VIPER
(\textit{val} set), achieving new state-of-the-art performance of 63.7, 63.3
and 56.2 VPQ, respectively.

    

### [[2112.08950] Stable Long-Term Recurrent Video Super-Resolution](http://arxiv.org/abs/2112.08950)


  Recurrent models have gained popularity in deep learning (DL) based video
super-resolution (VSR), due to their increased computational efficiency,
temporal receptive field and temporal consistency compared to sliding-window
based models. However, when inferring on long video sequences presenting low
motion (i.e. in which some parts of the scene barely move), recurrent models
diverge through recurrent processing, generating high frequency artifacts. To
the best of our knowledge, no study about VSR pointed out this instability
problem, which can be critical for some real-world applications. Video
surveillance is a typical example where such artifacts would occur, as both the
camera and the scene stay static for a long time.
In this work, we expose instabilities of existing recurrent VSR networks on
long sequences with low motion. We demonstrate it on a new long sequence
dataset Quasi-Static Video Set, that we have created. Finally, we introduce a
new framework of recurrent VSR networks that is both stable and competitive,
based on Lipschitz stability theory. We propose a new recurrent VSR network,
coined Middle Recurrent Video Super-Resolution (MRVSR), based on this
framework. We empirically show its competitive performance on long sequences
with low motion.

    

### [[2112.08954] Advancing Residual Learning towards Powerful Deep Spiking Neural Networks](http://arxiv.org/abs/2112.08954)


  Despite the rapid progress of neuromorphic computing, inadequate capacity and
insufficient representation power of spiking neural networks (SNNs) severely
restrict their application scope in practice. Residual learning and shortcuts
have been evidenced as an important approach for training deep neural networks,
but rarely did previous work assess their applicability to the characteristics
of spike-based communication and spatiotemporal dynamics. In this paper, we
first identify that this negligence leads to impeded information flow and
accompanying degradation problem in previous residual SNNs. Then we propose a
novel SNN-oriented residual block, MS-ResNet, which is able to significantly
extend the depth of directly trained SNNs, e.g. up to 482 layers on CIFAR-10
and 104 layers on ImageNet, without observing any slight degradation problem.
We validate the effectiveness of MS-ResNet on both frame-based and neuromorphic
datasets, and MS-ResNet104 achieves a superior result of 76.02% accuracy on
ImageNet, the first time in the domain of directly trained SNNs. Great energy
efficiency is also observed that on average only one spike per neuron is needed
to classify an input sample. We believe our powerful and scalable models will
provide a strong support for further exploration of SNNs.

    

### [[2112.08959] A molecular generative model with genetic algorithm and tree search for cancer samples](http://arxiv.org/abs/2112.08959)


  Personalized medicine is expected to maximize the intended drug effects and
minimize side effects by treating patients based on their genetic profiles.
Thus, it is important to generate drugs based on the genetic profiles of
diseases, especially in anticancer drug discovery. However, this is challenging
because the vast chemical space and variations in cancer properties require a
huge time resource to search for proper molecules. Therefore, an efficient and
fast search method considering genetic profiles is required for de novo
molecular design of anticancer drugs. Here, we propose a faster molecular
generative model with genetic algorithm and tree search for cancer samples
(FasterGTS). FasterGTS is constructed with a genetic algorithm and a Monte
Carlo tree search with three deep neural networks: supervised learning,
self-trained, and value networks, and it generates anticancer molecules based
on the genetic profiles of a cancer sample. When compared to other methods,
FasterGTS generated cancer sample-specific molecules with general chemical
properties required for cancer drugs within the limited numbers of samplings.
We expect that FasterGTS contributes to the anticancer drug generation.

    

### [[2112.08961] Objective hearing threshold identification from auditory brainstem response measurements using supervised and self-supervised approaches](http://arxiv.org/abs/2112.08961)


  Hearing loss is a major health problem and psychological burden in humans.
Mouse models offer a possibility to elucidate genes involved in the underlying
developmental and pathophysiological mechanisms of hearing impairment. To this
end, large-scale mouse phenotyping programs include auditory phenotyping of
single-gene knockout mouse lines. Using the auditory brainstem response (ABR)
procedure, the German Mouse Clinic and similar facilities worldwide have
produced large, uniform data sets of averaged ABR raw data of mutant and
wildtype mice. In the course of standard ABR analysis, hearing thresholds are
assessed visually by trained staff from series of signal curves of increasing
sound pressure level. This is time-consuming and prone to be biased by the
reader as well as the graphical display quality and scale. In an attempt to
reduce workload and improve quality and reproducibility, we developed and
compared two methods for automated hearing threshold identification from
averaged ABR raw data: a supervised approach involving two combined neural
networks trained on human-generated labels and a self-supervised approach,
which exploits the signal power spectrum and combines random forest sound level
estimation with a piece-wise curve fitting algorithm for threshold finding. We
show that both models work well, outperform human threshold detection, and are
suitable for fast, reliable, and unbiased hearing threshold detection and
quality control. In a high-throughput mouse phenotyping environment, both
methods perform well as part of an automated end-to-end screening pipeline to
detect candidate genes for hearing involvement. Code for both models as well as
data used for this work are freely available.

    

### [[2112.08967] End-to-End Multi-Task Deep Learning and Model Based Control Algorithm for Autonomous Driving](http://arxiv.org/abs/2112.08967)


  End-to-end driving with a deep learning neural network (DNN) has become a
rapidly growing paradigm of autonomous driving in industry and academia. Yet
safety measures and interpretability still pose challenges to this paradigm. We
propose an end-to-end driving algorithm that integrates multi-task DNN, path
prediction, and control models in a pipeline of data flow from sensory devices
through these models to driving decisions. It provides quantitative measures to
evaluate the holistic, dynamic, and real-time performance of end-to-end driving
systems, and thus allows to quantify their safety and interpretability. The DNN
is a modified UNet, a well known encoder-decoder neural network of semantic
segmentation. It consists of one segmentation, one regression, and two
classification tasks for lane segmentation, path prediction, and vehicle
controls. We present three variants of the modified UNet architecture having
different complexities, compare them on different tasks in four static measures
for both single and multi-task (MT) architectures, and then identify the best
one by two additional dynamic measures in real-time simulation. We also propose
a learning- and model-based longitudinal controller using model predictive
control method. With the Stanley lateral controller, our results show that
MTUNet outperforms an earlier modified UNet in terms of curvature and lateral
offset estimation on curvy roads at normal speed, which has been tested in a
real car driving on real roads.

    

### [[2112.08974] Quality monitoring of federated Covid-19 lesion segmentation](http://arxiv.org/abs/2112.08974)


  Federated Learning is the most promising way to train robust Deep Learning
models for the segmentation of Covid-19-related findings in chest CTs. By
learning in a decentralized fashion, heterogeneous data can be leveraged from a
variety of sources and acquisition protocols whilst ensuring patient privacy.
It is, however, crucial to continuously monitor the performance of the model.
Yet when it comes to the segmentation of diffuse lung lesions, a quick visual
inspection is not enough to assess the quality, and thorough monitoring of all
network outputs by expert radiologists is not feasible. In this work, we
present an array of lightweight metrics that can be calculated locally in each
hospital and then aggregated for central monitoring of a federated system. Our
linear model detects over 70% of low-quality segmentations on an
out-of-distribution dataset and thus reliably signals a decline in model
performance.

    

### [[2112.08986] A Heterogeneous Graph Learning Model for Cyber-Attack Detection](http://arxiv.org/abs/2112.08986)


  A cyber-attack is a malicious attempt by experienced hackers to breach the
target information system. Usually, the cyber-attacks are characterized as
hybrid TTPs (Tactics, Techniques, and Procedures) and long-term adversarial
behaviors, making the traditional intrusion detection methods ineffective. Most
existing cyber-attack detection systems are implemented based on manually
designed rules by referring to domain knowledge (e.g., threat models, threat
intelligences). However, this process is lack of intelligence and
generalization ability. Aiming at this limitation, this paper proposes an
intelligent cyber-attack detection method based on provenance data. To
effective and efficient detect cyber-attacks from a huge number of system
events in the provenance data, we firstly model the provenance data by a
heterogeneous graph to capture the rich context information of each system
entities (e.g., process, file, socket, etc.), and learns a semantic vector
representation for each system entity. Then, we perform online cyber-attack
detection by sampling a small and compact local graph from the heterogeneous
graph, and classifying the key system entities as malicious or benign. We
conducted a series of experiments on two provenance datasets with real
cyber-attacks. The experiment results show that the proposed method outperforms
other learning based detection models, and has competitive performance against
state-of-the-art rule based cyber-attack detection systems.

    

### [[2112.09012] Centralizing State-Values in Dueling Networks for Multi-Robot Reinforcement Learning Mapless Navigation](http://arxiv.org/abs/2112.09012)


  We study the problem of multi-robot mapless navigation in the popular
Centralized Training and Decentralized Execution (CTDE) paradigm. This problem
is challenging when each robot considers its path without explicitly sharing
observations with other robots and can lead to non-stationary issues in Deep
Reinforcement Learning (DRL). The typical CTDE algorithm factorizes the joint
action-value function into individual ones, to favor cooperation and achieve
decentralized execution. Such factorization involves constraints (e.g.,
monotonicity) that limit the emergence of novel behaviors in an individual as
each agent is trained starting from a joint action-value. In contrast, we
propose a novel architecture for CTDE that uses a centralized state-value
network to compute a joint state-value, which is used to inject global state
information in the value-based updates of the agents. Consequently, each model
computes its gradient update for the weights, considering the overall state of
the environment. Our idea follows the insights of Dueling Networks as a
separate estimation of the joint state-value has both the advantage of
improving sample efficiency, while providing each robot information whether the
global state is (or is not) valuable. Experiments in a robotic navigation task
with 2 4, and 8 robots, confirm the superior performance of our approach over
prior CTDE methods (e.g., VDN, QMIX).

    

### [[2112.09015] Multivariate Realized Volatility Forecasting with Graph Neural Network](http://arxiv.org/abs/2112.09015)


  The existing publications demonstrate that the limit order book data is
useful in predicting short-term volatility in stock markets. Since stocks are
not independent, changes on one stock can also impact other related stocks. In
this paper, we are interested in forecasting short-term realized volatility in
a multivariate approach based on limit order book data and relational data. To
achieve this goal, we introduce Graph Transformer Network for Volatility
Forecasting. The model allows to combine limit order book features and an
unlimited number of temporal and cross-sectional relations from different
sources. Through experiments based on about 500 stocks from S&P 500 index, we
find a better performance for our model than for other benchmarks.

    

### [[2112.09025] Deep Reinforcement Learning Policies Learn Shared Adversarial Features Across MDPs](http://arxiv.org/abs/2112.09025)


  The use of deep neural networks as function approximators has led to striking
progress for reinforcement learning algorithms and applications. Yet the
knowledge we have on decision boundary geometry and the loss landscape of
neural policies is still quite limited. In this paper we propose a framework to
investigate the decision boundary and loss landscape similarities across states
and across MDPs. We conduct experiments in various games from Arcade Learning
Environment, and discover that high sensitivity directions for neural policies
are correlated across MDPs. We argue that these high sensitivity directions
support the hypothesis that non-robust features are shared across training
environments of reinforcement learning agents. We believe our results reveal
fundamental properties of the environments used in deep reinforcement learning
training, and represent a tangible step towards building robust and reliable
deep reinforcement learning agents.

    

### [[2112.09036] The Dual PC Algorithm for Structure Learning](http://arxiv.org/abs/2112.09036)


  While learning the graphical structure of Bayesian networks from
observational data is key to describing and helping understand data generating
processes in complex applications, the task poses considerable challenges due
to its computational complexity. The directed acyclic graph (DAG) representing
a Bayesian network model is generally not identifiable from observational data,
and a variety of methods exist to estimate its equivalence class instead. Under
certain assumptions, the popular PC algorithm can consistently recover the
correct equivalence class by testing for conditional independence (CI),
starting from marginal independence relationships and progressively expanding
the conditioning set. Here, we propose the dual PC algorithm, a novel scheme to
carry out the CI tests within the PC algorithm by leveraging the inverse
relationship between covariance and precision matrices. Notably, the elements
of the precision matrix coincide with partial correlations for Gaussian data.
Our algorithm then exploits block matrix inversions on the covariance and
precision matrices to simultaneously perform tests on partial correlations of
complementary (or dual) conditioning sets. The multiple CI tests of the dual PC
algorithm, therefore, proceed by first considering marginal and full-order CI
relationships and progressively moving to central-order ones. Simulation
studies indicate that the dual PC algorithm outperforms the classical PC
algorithm both in terms of run time and in recovering the underlying network
structure.

    

### [[2112.09037] A Static Analyzer for Detecting Tensor Shape Errors in Deep Neural Network Training Code](http://arxiv.org/abs/2112.09037)


  We present an automatic static analyzer PyTea that detects tensor-shape
errors in PyTorch code. The tensor-shape error is critical in the deep neural
net code; much of the training cost and intermediate results are to be lost
once a tensor shape mismatch occurs in the midst of the training phase. Given
the input PyTorch source, PyTea statically traces every possible execution
path, collects tensor shape constraints required by the tensor operation
sequence of the path, and decides if the constraints are unsatisfiable (hence a
shape error can occur). PyTea's scalability and precision hinges on the
characteristics of real-world PyTorch applications: the number of execution
paths after PyTea's conservative pruning rarely explodes and loops are simple
enough to be circumscribed by our symbolic abstraction. We tested PyTea against
the projects in the official PyTorch repository and some tensor-error code
questioned in the StackOverflow. PyTea successfully detects tensor shape errors
in these codes, each within a few seconds.

    

### [[2112.09043] Neural Style Transfer and Unpaired Image-to-Image Translation to deal with the Domain Shift Problem on Spheroid Segmentation](http://arxiv.org/abs/2112.09043)


  Background and objectives. Domain shift is a generalisation problem of
machine learning models that occurs when the data distribution of the training
set is different to the data distribution encountered by the model when it is
deployed. This is common in the context of biomedical image segmentation due to
the variance of experimental conditions, equipment, and capturing settings. In
this work, we address this challenge by studying both neural style transfer
algorithms and unpaired image-to-image translation methods in the context of
the segmentation of tumour spheroids.
Methods. We have illustrated the domain shift problem in the context of
spheroid segmentation with 4 deep learning segmentation models that achieved an
IoU over 97% when tested with images following the training distribution, but
whose performance decreased up to an 84\% when applied to images captured under
different conditions. In order to deal with this problem, we have explored 3
style transfer algorithms (NST, deep image analogy, and STROTSS), and 6
unpaired image-to-image translations algorithms (CycleGAN, DualGAN, ForkGAN,
GANILLA, CUT, and FastCUT). These algorithms have been integrated into a
high-level API that facilitates their application to other contexts where the
domain-shift problem occurs.
Results. We have considerably improved the performance of the 4 segmentation
models when applied to images captured under different conditions by using both
style transfer and image-to-image translation algorithms. In particular, there
are 2 style transfer algorithms (NST and deep image analogy) and 1 unpaired
image-to-image translations algorithm (CycleGAN) that improve the IoU of the
models in a range from 0.24 to 76.07. Therefore, reaching a similar performance
to the one obtained with the models are applied to images following the
training distribution.

    

### [[2112.09046] Distributed neural network control with dependability guarantees: a compositional port-Hamiltonian approach](http://arxiv.org/abs/2112.09046)


  Large-scale cyber-physical systems require that control policies are
distributed, that is, that they only rely on local real-time measurements and
communication with neighboring agents. Optimal Distributed Control (ODC)
problems are, however, highly intractable even in seemingly simple cases.
Recent work has thus proposed training Neural Network (NN) distributed
controllers. A main challenge of NN controllers is that they are not dependable
during and after training, that is, the closed-loop system may be unstable, and
the training may fail due to vanishing and exploding gradients. In this paper,
we address these issues for networks of nonlinear port-Hamiltonian (pH)
systems, whose modeling power ranges from energy systems to non-holonomic
vehicles and chemical reactions. Specifically, we embrace the compositional
properties of pH systems to characterize deep Hamiltonian control policies with
built-in closed-loop stability guarantees, irrespective of the interconnection
topology and the chosen NN parameters. Furthermore, our setup enables
leveraging recent results on well-behaved neural ODEs to prevent the phenomenon
of vanishing gradients by design. Numerical experiments corroborate the
dependability of the proposed architecture, while matching the performance of
general neural network policies.

    

### [[2112.09051] Simultaneous Multivariate Forecast of Space Weather Indices using Deep Neural Network Ensembles](http://arxiv.org/abs/2112.09051)


  Solar radio flux along with geomagnetic indices are important indicators of
solar activity and its effects. Extreme solar events such as flares and
geomagnetic storms can negatively affect the space environment including
satellites in low-Earth orbit. Therefore, forecasting these space weather
indices is of great importance in space operations and science. In this study,
we propose a model based on long short-term memory neural networks to learn the
distribution of time series data with the capability to provide a simultaneous
multivariate 27-day forecast of the space weather indices using time series as
well as solar image data. We show a 30-40\% improvement of the root mean-square
error while including solar image data with time series data compared to using
time series data alone. Simple baselines such as a persistence and running
average forecasts are also compared with the trained deep neural network
models. We also quantify the uncertainty in our prediction using a model
ensemble.

    

### [[2112.09055] Hierarchical Clustering: $O(1)$-Approximation for Well-Clustered Graphs](http://arxiv.org/abs/2112.09055)


  Hierarchical clustering studies a recursive partition of a data set into
clusters of successively smaller size, and is a fundamental problem in data
analysis. In this work we study the cost function for hierarchical clustering
introduced by Dasgupta, and present two polynomial-time approximation
algorithms: Our first result is an $O(1)$-approximation algorithm for graphs of
high conductance. Our simple construction bypasses complicated recursive
routines of finding sparse cuts known in the literature. Our second and main
result is an $O(1)$-approximation algorithm for a wide family of graphs that
exhibit a well-defined structure of clusters. This result generalises the
previous state-of-the-art, which holds only for graphs generated from
stochastic models. The significance of our work is demonstrated by the
empirical analysis on both synthetic and real-world data sets, on which our
presented algorithm outperforms the previously proposed algorithm for graphs
with a well-defined cluster structure.

    

### [[2112.09060] Towards Robust Real-time Audio-Visual Speech Enhancement](http://arxiv.org/abs/2112.09060)


  The human brain contextually exploits heterogeneous sensory information to
efficiently perform cognitive tasks including vision and hearing. For example,
during the cocktail party situation, the human auditory cortex contextually
integrates audio-visual (AV) cues in order to better perceive speech. Recent
studies have shown that AV speech enhancement (SE) models can significantly
improve speech quality and intelligibility in very low signal to noise ratio
(SNR) environments as compared to audio-only SE models. However, despite
significant research in the area of AV SE, development of real-time processing
models with low latency remains a formidable technical challenge. In this
paper, we present a novel framework for low latency speaker-independent AV SE
that can generalise on a range of visual and acoustic noises. In particular, a
generative adversarial networks (GAN) is proposed to address the practical
issue of visual imperfections in AV SE. In addition, we propose a deep neural
network based real-time AV SE model that takes into account the cleaned visual
speech output from GAN to deliver more robust SE. The proposed framework is
evaluated on synthetic and real noisy AV corpora using objective speech quality
and intelligibility metrics and subjective listing tests. Comparative
simulation results show that our real time AV SE framework outperforms
state-of-the-art SE approaches, including recent DNN based SE models.

    

### [[2112.09061] Solving Inverse Problems with NerfGANs](http://arxiv.org/abs/2112.09061)


  We introduce a novel framework for solving inverse problems using NeRF-style
generative models. We are interested in the problem of 3-D scene reconstruction
given a single 2-D image and known camera parameters. We show that naively
optimizing the latent space leads to artifacts and poor novel view rendering.
We attribute this problem to volume obstructions that are clear in the 3-D
geometry and become visible in the renderings of novel views. We propose a
novel radiance field regularization method to obtain better 3-D surfaces and
improved novel views given single view observations. Our method naturally
extends to general inverse problems including inpainting where one observes
only partially a single view. We experimentally evaluate our method, achieving
visual improvements and performance boosts over the baselines in a wide range
of tasks. Our method achieves $30-40\%$ MSE reduction and $15-25\%$ reduction
in LPIPS loss compared to the previous state of the art.

    

### [[2112.09068] Estimation of Physical Activity Level and Ambient Condition Thresholds for Respiratory Health using Smartphone Sensors](http://arxiv.org/abs/2112.09068)


  While physical activity has been described as a primary prevention against
chronic diseases, strenuous physical exertion under adverse ambient conditions
has also been reported as a major contributor to exacerbation of chronic
respiratory conditions. Maintaining a balance by monitoring the type and the
level of physical activities of affected individuals, could help in reducing
the cost and burden of managing respiratory ailments. This paper explores the
potentiality of motion sensors in Smartphones to estimate physical activity
thresholds that could trigger symptoms of exercise induced respiratory
conditions (EiRCs). The focus is on the extraction of measurements from the
embedded motion sensors to determine the activity level and the type of
activity that is tolerable to individuals respiratory health. The calculations
are based on the correlation between Signal Magnitude Area (SMA) and Energy
Expenditure (EE). We also consider the effect of changes in the ambient
conditions like temperature and humidity, as contributing factors to
respiratory distress during physical exercise. Real time data collected from
healthy individuals were used to demonstrate the potentiality of a mobile phone
as tool to regulate the level of physical activities of individuals with EiRCs.
We describe a practical situation where the experimental outcomes can be
applied to promote good respiratory health.

    

### [[2112.09069] Progressive Graph Convolution Network for EEG Emotion Recognition](http://arxiv.org/abs/2112.09069)


  Studies in the area of neuroscience have revealed the relationship between
emotional patterns and brain functional regions, demonstrating that dynamic
relationships between different brain regions are an essential factor affecting
emotion recognition determined through electroencephalography (EEG). Moreover,
in EEG emotion recognition, we can observe that clearer boundaries exist
between coarse-grained emotions than those between fine-grained emotions, based
on the same EEG data; this indicates the concurrence of large coarse- and small
fine-grained emotion variations. Thus, the progressive classification process
from coarse- to fine-grained categories may be helpful for EEG emotion
recognition. Consequently, in this study, we propose a progressive graph
convolution network (PGCN) for capturing this inherent characteristic in EEG
emotional signals and progressively learning the discriminative EEG features.
To fit different EEG patterns, we constructed a dual-graph module to
characterize the intrinsic relationship between different EEG channels,
containing the dynamic functional connections and static spatial proximity
information of brain regions from neuroscience research. Moreover, motivated by
the observation of the relationship between coarse- and fine-grained emotions,
we adopt a dual-head module that enables the PGCN to progressively learn more
discriminative EEG features, from coarse-grained (easy) to fine-grained
categories (difficult), referring to the hierarchical characteristic of
emotion. To verify the performance of our model, extensive experiments were
conducted on two public datasets: SEED-IV and multi-modal physiological emotion
database (MPED).

    

### [[2112.09071] A Deep Learning Based Multitask Network for Respiration Rate Estimation -- A Practical Perspective](http://arxiv.org/abs/2112.09071)


  The exponential rise in wearable sensors has garnered significant interest in
assessing the physiological parameters during day-to-day activities.
Respiration rate is one of the vital parameters used in the performance
assessment of lifestyle activities. However, obtrusive setup for measurement,
motion artifacts, and other noises complicate the process. This paper presents
a multitasking architecture based on Deep Learning (DL) for estimating
instantaneous and average respiration rate from ECG and accelerometer signals,
such that it performs efficiently under daily living activities like cycling,
walking, etc. The multitasking network consists of a combination of
Encoder-Decoder and Encoder-IncResNet, to fetch the average respiration rate
and the respiration signal. The respiration signal can be leveraged to obtain
the breathing peaks and instantaneous breathing cycles. Mean absolute
error(MAE), Root mean square error (RMSE), inference time, and parameter count
analysis has been used to compare the network with the current state of art
Machine Learning (ML) model and other DL models developed in previous studies.
Other DL configurations based on a variety of inputs are also developed as a
part of the work. The proposed model showed better overall accuracy and gave
better results than individual modalities during different activities.

    

### [[2112.09076] SanMove: Next Location Recommendation via Self-Attention Network](http://arxiv.org/abs/2112.09076)


  Currently, next location recommendation plays a vital role in location-based
social network applications and services. Although many methods have been
proposed to solve this problem, three important challenges have not been well
addressed so far: (1) most existing methods are based on recurrent network,
which is time-consuming to train long sequences due to not allowing for full
parallelism; (2) personalized preferences generally are not considered
reasonably; (3) existing methods rarely systematically studied how to
efficiently utilize various auxiliary information (e.g., user ID and timestamp)
in trajectory data and the spatio-temporal relations among non-consecutive
locations. To address the above challenges, we propose a novel method named
SanMove, a self-attention network based model, to predict the next location via
capturing the long- and short-term mobility patterns of users. Specifically,
SanMove introduces a long-term preference learning module, and it uses a
self-attention module to capture the users long-term mobility pattern which can
represent personalized location preferences of users. Meanwhile, SanMove uses a
spatial-temporal guided non-invasive self-attention (STNOVA) to exploit
auxiliary information to learn short-term preferences. We evaluate SanMove with
two real-world datasets, and demonstrate SanMove is not only faster than the
state-of-the-art RNN-based predict model but also outperforms the baselines for
next location prediction.

    

### [[2112.09086] A new locally linear embedding scheme in light of Hessian eigenmap](http://arxiv.org/abs/2112.09086)


  We provide a new interpretation of Hessian locally linear embedding (HLLE),
revealing that it is essentially a variant way to implement the same idea of
locally linear embedding (LLE). Based on the new interpretation, a substantial
simplification can be made, in which the idea of "Hessian" is replaced by
rather arbitrary weights. Moreover, we show by numerical examples that HLLE may
produce projection-like results when the dimension of the target space is
larger than that of the data manifold, and hence one further modification
concerning the manifold dimension is suggested. Combining all the observations,
we finally achieve a new LLE-type method, which is called tangential LLE
(TLLE). It is simpler and more robust than HLLE.

    

### [[2112.09097] Learning and Analyzing Generation Order for Undirected Sequence Models](http://arxiv.org/abs/2112.09097)


  Undirected neural sequence models have achieved performance competitive with
the state-of-the-art directed sequence models that generate monotonically from
left to right in machine translation tasks. In this work, we train a policy
that learns the generation order for a pre-trained, undirected translation
model via reinforcement learning. We show that the translations decoded by our
learned orders achieve higher BLEU scores than the outputs decoded from left to
right or decoded by the learned order from Mansimov et al. (2019) on the WMT'14
German-English translation task. On examples with a maximum source and target
length of 30 from De-En, WMT'16 English-Romanian, and WMT'21 English-Chinese
translation tasks, our learned order outperforms all heuristic generation
orders on four out of six tasks. We next carefully analyze the learned order
patterns via qualitative and quantitative analysis. We show that our policy
generally follows an outer-to-inner order, predicting the left-most and
right-most positions first, and then moving toward the middle while skipping
less important words at the beginning. Furthermore, the policy usually predicts
positions for a single syntactic constituent structure in consecutive steps. We
believe our findings could provide more insights on the mechanism of undirected
generation models and encourage further research in this direction. Our code is
publicly available at this https URL


### [[2112.09104] Non-Gaussian Component Analysis via Lattice Basis Reduction](http://arxiv.org/abs/2112.09104)


  Non-Gaussian Component Analysis (NGCA) is the following distribution learning
problem: Given i.i.d. samples from a distribution on $\mathbb{R}^d$ that is
non-gaussian in a hidden direction $v$ and an independent standard Gaussian in
the orthogonal directions, the goal is to approximate the hidden direction $v$.
Prior work \cite{DKS17-sq} provided formal evidence for the existence of an
information-computation tradeoff for NGCA under appropriate moment-matching
conditions on the univariate non-gaussian distribution $A$. The latter result
does not apply when the distribution $A$ is discrete. A natural question is
whether information-computation tradeoffs persist in this setting. In this
paper, we answer this question in the negative by obtaining a sample and
computationally efficient algorithm for NGCA in the regime that $A$ is discrete
or nearly discrete, in a well-defined technical sense. The key tool leveraged
in our algorithm is the LLL method \cite{LLL82} for lattice basis reduction.

    

### [[2112.09106] RegionCLIP: Region-based Language-Image Pretraining](http://arxiv.org/abs/2112.09106)


  Contrastive language-image pretraining (CLIP) using image-text pairs has
achieved impressive results on image classification in both zero-shot and
transfer learning settings. However, we show that directly applying such models
to recognize image regions for object detection leads to poor performance due
to a domain shift: CLIP was trained to match an image as a whole to a text
description, without capturing the fine-grained alignment between image regions
and text spans. To mitigate this issue, we propose a new method called
RegionCLIP that significantly extends CLIP to learn region-level visual
representations, thus enabling fine-grained alignment between image regions and
textual concepts. Our method leverages a CLIP model to match image regions with
template captions and then pretrains our model to align these region-text pairs
in the feature space. When transferring our pretrained model to the
open-vocabulary object detection tasks, our method significantly outperforms
the state of the art by 3.8 AP50 and 2.2 AP for novel categories on COCO and
LVIS datasets, respectively. Moreoever, the learned region representations
support zero-shot inference for object detection, showing promising results on
both COCO and LVIS datasets. Our code is available at
this https URL.

    

### [[2112.09117] Machine Learning Kreuzer--Skarke Calabi--Yau Threefolds](http://arxiv.org/abs/2112.09117)


  Using a fully connected feedforward neural network we study topological
invariants of a class of Calabi--Yau manifolds constructed as hypersurfaces in
toric varieties associated with reflexive polytopes from the Kreuzer--Skarke
database. In particular, we find the existence of a simple expression for the
Euler number that can be learned in terms of limited data extracted from the
polytope and its dual.

    

### [[2112.09120] Human Hands as Probes for Interactive Object Understanding](http://arxiv.org/abs/2112.09120)


  Interactive object understanding, or what we can do to objects and how is a
long-standing goal of computer vision. In this paper, we tackle this problem
through observation of human hands in in-the-wild egocentric videos. We
demonstrate that observation of what human hands interact with and how can
provide both the relevant data and the necessary supervision. Attending to
hands, readily localizes and stabilizes active objects for learning and reveals
places where interactions with objects occur. Analyzing the hands shows what we
can do to objects and how. We apply these basic principles on the EPIC-KITCHENS
dataset, and successfully learn state-sensitive features, and object
affordances (regions of interaction and afforded grasps), purely by observing
hands in egocentric videos.

    

### [[2112.09126] IS-COUNT: Large-scale Object Counting from Satellite Images with Covariate-based Importance Sampling](http://arxiv.org/abs/2112.09126)


  Object detection in high-resolution satellite imagery is emerging as a
scalable alternative to on-the-ground survey data collection in many
environmental and socioeconomic monitoring applications. However, performing
object detection over large geographies can still be prohibitively expensive
due to the high cost of purchasing imagery and compute. Inspired by traditional
survey data collection strategies, we propose an approach to estimate object
count statistics over large geographies through sampling. Given a cost budget,
our method selects a small number of representative areas by sampling from a
learnable proposal distribution. Using importance sampling, we are able to
accurately estimate object counts after processing only a small fraction of the
images compared to an exhaustive approach. We show empirically that the
proposed framework achieves strong performance on estimating the number of
buildings in the United States and Africa, cars in Kenya, brick kilns in
Bangladesh, and swimming pools in the U.S., while requiring as few as 0.01% of
satellite images compared to an exhaustive approach.

    

### [[2112.09130] Ensembling Off-the-shelf Models for GAN Training](http://arxiv.org/abs/2112.09130)


  The advent of large-scale training has produced a cornucopia of powerful
visual recognition models. However, generative models, such as GANs, have
traditionally been trained from scratch in an unsupervised manner. Can the
collective "knowledge" from a large bank of pretrained vision models be
leveraged to improve GAN training? If so, with so many models to choose from,
which one(s) should be selected, and in what manner are they most effective? We
find that pretrained computer vision models can significantly improve
performance when used in an ensemble of discriminators. Notably, the particular
subset of selected models greatly affects performance. We propose an effective
selection mechanism, by probing the linear separability between real and fake
samples in pretrained model embeddings, choosing the most accurate model, and
progressively adding it to the discriminator ensemble. Interestingly, our
method can improve GAN training in both limited data and large-scale settings.
Given only 10k training samples, our FID on LSUN Cat matches the StyleGAN2
trained on 1.6M images. On the full dataset, our method improves FID by 1.5x to
2x on cat, church, and horse categories of LSUN.

    

### [[2112.09133] Masked Feature Prediction for Self-Supervised Visual Pre-Training](http://arxiv.org/abs/2112.09133)


  We present Masked Feature Prediction (MaskFeat) for self-supervised
pre-training of video models. Our approach first randomly masks out a portion
of the input sequence and then predicts the feature of the masked regions. We
study five different types of features and find Histograms of Oriented
Gradients (HOG), a hand-crafted feature descriptor, works particularly well in
terms of both performance and efficiency. We observe that the local contrast
normalization in HOG is essential for good results, which is in line with
earlier work using HOG for visual recognition. Our approach can learn abundant
visual knowledge and drive large-scale Transformer-based models. Without using
extra model weights or supervision, MaskFeat pre-trained on unlabeled videos
achieves unprecedented results of 86.7% with MViT-L on Kinetics-400, 88.3% on
Kinetics-600, 80.4% on Kinetics-700, 38.8 mAP on AVA, and 75.0% on SSv2.
MaskFeat further generalizes to image input, which can be interpreted as a
video with a single frame and obtains competitive results on ImageNet.

    

### [[1906.06659] A Generalized Minimax Q-learning Algorithm for Two-Player Zero-Sum Stochastic Games](http://arxiv.org/abs/1906.06659)


  We consider the problem of two-player zero-sum games. This problem is
formulated as a min-max Markov game in the literature. The solution of this
game, which is the min-max payoff, starting from a given state is called the
min-max value of the state. In this work, we compute the solution of the
two-player zero-sum game utilizing the technique of successive relaxation that
has been successfully applied in the literature to compute a faster value
iteration algorithm in the context of Markov Decision Processes. We extend the
concept of successive relaxation to the setting of two-player zero-sum games.
We show that, under a special structure on the game, this technique facilitates
faster computation of the min-max value of the states. We then derive a
generalized minimax Q-learning algorithm that computes the optimal policy when
the model information is not known. Finally, we prove the convergence of the
proposed generalized minimax Q-learning algorithm utilizing stochastic
approximation techniques, under an assumption on the boundedness of iterates.
Through experiments, we demonstrate the effectiveness of our proposed
algorithm.

    

### [[1906.07633] From Clustering to Cluster Explanations via Neural Networks](http://arxiv.org/abs/1906.07633)


  A recent trend in machine learning has been to enrich learned models with the
ability to explain their own predictions. The emerging field of Explainable AI
(XAI) has so far mainly focused on supervised learning, in particular, deep
neural network classifiers. In many practical problems however, label
information is not given and the goal is instead to discover the underlying
structure of the data, for example, its clusters. While powerful methods exist
for extracting the cluster structure in data, they typically do not answer the
question why a certain data point has been assigned to a given cluster. We
propose a new framework that can, for the first time, explain cluster
assignments in terms of input features in an efficient and reliable manner. It
is based on the novel insight that clustering models can be rewritten as neural
networks - or 'neuralized'. Cluster predictions of the obtained networks can
then be quickly and accurately attributed to the input features. Several
showcases demonstrate the ability of our method to assess the quality of
learned clusters and to extract novel insights from the analyzed data and
representations.

    

### [[1907.07585] Deep Metric Learning with Alternating Projections onto Feasible Sets](http://arxiv.org/abs/1907.07585)


  During the training of networks for distance metric learning, minimizers of
the typical loss functions can be considered as "feasible points" satisfying a
set of constraints imposed by the training data. To this end, we reformulate
distance metric learning problem as finding a feasible point of a constraint
set where the embedding vectors of the training data satisfy desired
intra-class and inter-class proximity. The feasible set induced by the
constraint set is expressed as the intersection of the relaxed feasible sets
which enforce the proximity constraints only for particular samples (a sample
from each class) of the training data. Then, the feasible point problem is to
be approximately solved by performing alternating projections onto those
feasible sets. Such an approach introduces a regularization term and results in
minimizing a typical loss function with a systematic batch set construction
where these batches are constrained to contain the same sample from each class
for a certain number of iterations. Moreover, these particular samples can be
considered as the class representatives, allowing efficient utilization of hard
class mining during batch construction. The proposed technique is applied with
the well-accepted losses and evaluated on Stanford Online Products, CAR196 and
CUB200-2011 datasets for image retrieval and clustering. Outperforming
state-of-the-art, the proposed approach consistently improves the performance
of the integrated loss functions with no additional computational cost and
boosts the performance further by hard negative class mining.

    

### [[1910.03779] Forecast Aggregation via Peer Prediction](http://arxiv.org/abs/1910.03779)


  Crowdsourcing enables the solicitation of forecasts on a variety of
prediction tasks from distributed groups of people. How to aggregate the
solicited forecasts, which may vary in quality, into an accurate final
prediction remains a challenging yet critical question. Studies have found that
weighing expert forecasts more in aggregation can improve the accuracy of the
aggregated prediction. However, this approach usually requires access to the
historical performance data of the forecasters, which are often not available.
In this paper, we study the problem of aggregating forecasts without having
historical performance data. We propose using peer prediction methods, a family
of mechanisms initially designed to truthfully elicit private information in
the absence of ground truth verification, to assess the expertise of
forecasters, and then using this assessment to improve forecast aggregation. We
evaluate our peer-prediction-aided aggregators on a diverse collection of 14
human forecast datasets. Compared with a variety of existing aggregators, our
aggregators achieve a significant and consistent improvement on aggregation
accuracy measured by the Brier score and the log score. Our results reveal the
effectiveness of identifying experts to improve aggregation even without
historical data.

    

### [[2001.02856] D-GCCA: Decomposition-based Generalized Canonical Correlation Analysis for Multi-view High-dimensional Data](http://arxiv.org/abs/2001.02856)


  Modern biomedical studies often collect multi-view data, that is, multiple
types of data measured on the same set of objects. A popular model in
high-dimensional multi-view data analysis is to decompose each view's data
matrix into a low-rank common-source matrix generated by latent factors common
across all data views, a low-rank distinctive-source matrix corresponding to
each view, and an additive noise matrix. We propose a novel decomposition
method for this model, called decomposition-based generalized canonical
correlation analysis (D-GCCA). The D-GCCA rigorously defines the decomposition
on the L2 space of random variables in contrast to the Euclidean dot product
space used by most existing methods, thereby being able to provide the
estimation consistency for the low-rank matrix recovery. Moreover, to well
calibrate common latent factors, we impose a desirable orthogonality constraint
on distinctive latent factors. Existing methods, however, inadequately consider
such orthogonality and may thus suffer from substantial loss of undetected
common-source variation. Our D-GCCA takes one step further than generalized
canonical correlation analysis by separating common and distinctive components
among canonical variables, while enjoying an appealing interpretation from the
perspective of principal component analysis. Furthermore, we propose to use the
variable-level proportion of signal variance explained by common or distinctive
latent factors for selecting the variables most influenced. Consistent
estimators of our D-GCCA method are established with good finite-sample
numerical performance, and have closed-form expressions leading to efficient
computation especially for large-scale data. The superiority of D-GCCA over
state-of-the-art methods is also corroborated in simulations and real-world
data examples.

    

### [[2003.09660] NeuCrowd: Neural Sampling Network for Representation Learning with Crowdsourced Labels](http://arxiv.org/abs/2003.09660)


  Representation learning approaches require a massive amount of discriminative
training data, which is unavailable in many scenarios, such as healthcare,
smart city, education, etc. In practice, people refer to crowdsourcing to get
annotated labels. However, due to issues like data privacy, budget limitation,
shortage of domain-specific annotators, the number of crowdsourced labels is
still very limited. Moreover, because of annotators' diverse expertise,
crowdsourced labels are often inconsistent. Thus, directly applying existing
supervised representation learning (SRL) algorithms may easily get the
overfitting problem and yield suboptimal solutions. In this paper, we propose
\emph{NeuCrowd}, a unified framework for SRL from crowdsourced labels. The
proposed framework (1) creates a sufficient number of high-quality
\emph{n}-tuplet training samples by utilizing safety-aware sampling and robust
anchor generation; and (2) automatically learns a neural sampling network that
adaptively learns to select effective samples for SRL networks. The proposed
framework is evaluated on both one synthetic and three real-world data sets.
The results show that our approach outperforms a wide range of state-of-the-art
baselines in terms of prediction accuracy and AUC. To encourage reproducible
results, we make our code publicly available at
\url{this https URL}.

    

### [[2004.14120] Learning Non-Monotonic Automatic Post-Editing of Translations from Human Orderings](http://arxiv.org/abs/2004.14120)


  Recent research in neural machine translation has explored flexible
generation orders, as an alternative to left-to-right generation. However,
training non-monotonic models brings a new complication: how to search for a
good ordering when there is a combinatorial explosion of orderings arriving at
the same final result? Also, how do these automatic orderings compare with the
actual behaviour of human translators? Current models rely on manually built
biases or are left to explore all possibilities on their own. In this paper, we
analyze the orderings produced by human post-editors and use them to train an
automatic post-editing system. We compare the resulting system with those
trained with left-to-right and random post-editing orderings. We observe that
humans tend to follow a nearly left-to-right order, but with interesting
deviations, such as preferring to start by correcting punctuation or verbs.

    

### [[2006.07002] Double Double Descent: On Generalization Errors in Transfer Learning between Linear Regression Tasks](http://arxiv.org/abs/2006.07002)


  We study the transfer learning process between two linear regression
problems. An important and timely special case is when the regressors are
overparameterized and perfectly interpolate their training data. We examine a
parameter transfer mechanism whereby a subset of the parameters of the target
task solution are constrained to the values learned for a related source task.
We analytically characterize the generalization error of the target task in
terms of the salient factors in the transfer learning architecture, i.e., the
number of examples available, the number of (free) parameters in each of the
tasks, the number of parameters transferred from the source to target task, and
the correlation between the two tasks. Our non-asymptotic analysis shows that
the generalization error of the target task follows a two-dimensional double
descent trend (with respect to the number of free parameters in each of the
tasks) that is controlled by the transfer learning factors. Our analysis points
to specific cases where the transfer of parameters is beneficial as a
substitute for extra overparameterization (i.e., additional free parameters in
the target task). Specifically, we show that the usefulness of a transfer
learning setting is fragile and depends on a delicate interplay among the set
of transferred parameters, the relation between the tasks, and the true
solution. We also demonstrate that overparameterized transfer learning is not
necessarily more beneficial when the source task is closer or identical to the
target task.

    

### [[2006.11165] Backdoor Attacks to Graph Neural Networks](http://arxiv.org/abs/2006.11165)


  In this work, we propose the first backdoor attack to graph neural networks
(GNN). Specifically, we propose a \emph{subgraph based backdoor attack} to GNN
for graph classification. In our backdoor attack, a GNN classifier predicts an
attacker-chosen target label for a testing graph once a predefined subgraph is
injected to the testing graph. Our empirical results on three real-world graph
datasets show that our backdoor attacks are effective with a small impact on a
GNN's prediction accuracy for clean testing graphs. Moreover, we generalize a
randomized smoothing based certified defense to defend against our backdoor
attacks. Our empirical results show that the defense is effective in some cases
but ineffective in other cases, highlighting the needs of new defenses for our
backdoor attacks.

    

### [[2006.11695] Uncertainty-Aware (UNA) Bases for Deep Bayesian Regression Using Multi-Headed Auxiliary Networks](http://arxiv.org/abs/2006.11695)


  Neural Linear Models (NLM) are deep Bayesian models that produce predictive
uncertainties by learning features from the data and then performing Bayesian
linear regression over these features. Despite their popularity, few works have
focused on methodically evaluating the predictive uncertainties of these
models. In this work, we demonstrate that traditional training procedures for
NLMs drastically underestimate uncertainty on out-of-distribution inputs, and
that they therefore cannot be naively deployed in risk-sensitive applications.
We identify the underlying reasons for this behavior and propose a novel
training framework that captures useful predictive uncertainties for downstream
tasks.

    

### [[2006.15857] Constructing a Chain Event Graph from a Staged Tree](http://arxiv.org/abs/2006.15857)


  Chain Event Graphs (CEGs) are a recent family of probabilistic graphical
models - a generalisation of Bayesian Networks - providing an explicit
representation of structural zeros, structural missing values and
context-specific conditional independences within their graph topology. A CEG
is constructed from an event tree through a sequence of transformations
beginning with the colouring of the vertices of the event tree to identify
one-step transition symmetries. This coloured event tree, also known as a
staged tree, is the output of the learning algorithms used for this family.
Surprisingly, no general algorithm has yet been devised that automatically
transforms any staged tree into a CEG representation. In this paper we provide
a simple iterative backward algorithm for this transformation. Additionally, we
show that no information is lost from transforming a staged tree into a CEG.
Finally, we demonstrate that with an optimal stopping criterion, our algorithm
is more efficient than the generalisation of a special case presented in
Silander and Leong (2013). We also provide Python code using this algorithm to
obtain a CEG from any staged tree along with the functionality to add edges
with sampling zeros.

    

### [[2009.12326] Online Missing Value Imputation and Change Point Detection with the Gaussian Copula](http://arxiv.org/abs/2009.12326)


  Missing value imputation is crucial for real-world data science workflows.
Imputation is harder in the online setting, as it requires the imputation
method itself to be able to evolve over time. For practical applications,
imputation algorithms should produce imputations that match the true data
distribution, handle data of mixed types, including ordinal, boolean, and
continuous variables, and scale to large datasets. In this work we develop a
new online imputation algorithm for mixed data using the Gaussian copula. The
online Gaussian copula model meets all the desiderata: its imputations match
the data distribution even for mixed data, improve over its offline counterpart
on the accuracy when the streaming data has a changing distribution, and on the
speed (up to an order of magnitude) especially on large scale datasets. By
fitting the copula model to online data, we also provide a new method to detect
change points in the multivariate dependence structure with missing values.
Experimental results on synthetic and real world data validate the performance
of the proposed methods.

    

### [[2010.05949] Towards human-level performance on automatic pose estimation of infant spontaneous movements](http://arxiv.org/abs/2010.05949)


  Assessment of spontaneous movements can predict the long-term developmental
disorders in high-risk infants. In order to develop algorithms for automated
prediction of later disorders, highly precise localization of segments and
joints by infant pose estimation is required. Four types of convolutional
neural networks were trained and evaluated on a novel infant pose dataset,
covering the large variation in 1 424 videos from a clinical international
community. The localization performance of the networks was evaluated as the
deviation between the estimated keypoint positions and human expert
annotations. The computational efficiency was also assessed to determine the
feasibility of the neural networks in clinical practice. The best performing
neural network had a similar localization error to the inter-rater spread of
human expert annotations, while still operating efficiently. Overall, the
results of our study show that pose estimation of infant spontaneous movements
has a great potential to support research initiatives on early detection of
developmental disorders in children with perinatal brain injuries by
quantifying infant movements from video recordings with human-level
performance.

    

### [[2011.09865] An experiment on the mechanisms of racial bias in ML-based credit scoring in Brazil](http://arxiv.org/abs/2011.09865)


  We dissect an experimental credit scoring model developed with real data and
demonstrate - without access to protected attributes - how the use of location
information introduces racial bias. We analyze the tree gradient boosting model
with the aid of a game-theoretic inspired machine learning explainability
technique, counterfactual experiments and Brazilian census data. By exposing
algorithmic racial bias explaining the trained machine learning model inner
mechanisms, this experiment comprises an interesting artifact to aid the
endeavor of theoretical understanding of the emergence of racial bias in
machine learning systems. Without access to individuals' racial categories, we
show how classification parity measures using geographically defined groups
could carry information about model racial bias. The experiment testifies to
the need for methods and language that do not presuppose access to protected
attributes when auditing ML models, the importance of considering regional
specifics when addressing racial issues, and the central role of census data in
the AI research community. To the best of our knowledge, this is the first
documented case of algorithmic racial bias in ML-based credit scoring in
Brazil, the country with the second largest Black population in the world.

    

### [[2012.06922] Decimated Framelet System on Graphs and Fast G-Framelet Transforms](http://arxiv.org/abs/2012.06922)


  Graph representation learning has many real-world applications, from
super-resolution imaging, 3D computer vision to drug repurposing, protein
classification, social networks analysis. An adequate representation of graph
data is vital to the learning performance of a statistical or machine learning
model for graph-structured data. In this paper, we propose a novel multiscale
representation system for graph data, called decimated framelets, which form a
localized tight frame on the graph. The decimated framelet system allows
storage of the graph data representation on a coarse-grained chain and
processes the graph data at multi scales where at each scale, the data is
stored at a subgraph. Based on this, we then establish decimated G-framelet
transforms for the decomposition and reconstruction of the graph data at multi
resolutions via a constructive data-driven filter bank. The graph framelets are
built on a chain-based orthonormal basis that supports fast graph Fourier
transforms. From this, we give a fast algorithm for the decimated G-framelet
transforms, or FGT, that has linear computational complexity O(N) for a graph
of size N. The theory of decimated framelets and FGT is verified with numerical
examples for random graphs. The effectiveness is demonstrated by real-world
applications, including multiresolution analysis for traffic network, and graph
neural networks for graph classification tasks.

    

### [[2101.02180] Maximum a Posteriori Inference of Random Dot Product Graphs via Conic Programming](http://arxiv.org/abs/2101.02180)


  We present a convex cone program to infer the latent probability matrix of a
random dot product graph (RDPG). The optimization problem maximizes the
Bernoulli maximum likelihood function with an added nuclear norm regularization
term. The dual problem has a particularly nice form, related to the well-known
semidefinite program relaxation of the MaxCut problem. Using the primal-dual
optimality conditions, we bound the entries and rank of the primal and dual
solutions. Furthermore, we bound the optimal objective value and prove
asymptotic consistency of the probability estimates of a slightly modified
model under mild technical assumptions. Our experiments on synthetic RDPGs not
only recover natural clusters, but also reveal the underlying low-dimensional
geometry of the original data. We also demonstrate that the method recovers
latent structure in the Karate Club Graph and synthetic U.S. Senate vote graphs
and is scalable to graphs with up to a few hundred nodes.

    

### [[2101.03164] E(3)-Equivariant Graph Neural Networks for Data-Efficient and Accurate Interatomic Potentials](http://arxiv.org/abs/2101.03164)


  This work presents Neural Equivariant Interatomic Potentials (NequIP), an
E(3)-equivariant neural network approach for learning interatomic potentials
from ab-initio calculations for molecular dynamics simulations. While most
contemporary symmetry-aware models use invariant convolutions and only act on
scalars, NequIP employs E(3)-equivariant convolutions for interactions of
geometric tensors, resulting in a more information-rich and faithful
representation of atomic environments. The method achieves state-of-the-art
accuracy on a challenging and diverse set of molecules and materials while
exhibiting remarkable data efficiency. NequIP outperforms existing models with
up to three orders of magnitude fewer training data, challenging the widely
held belief that deep neural networks require massive training sets. The high
data efficiency of the method allows for the construction of accurate
potentials using high-order quantum chemical level of theory as reference and
enables high-fidelity molecular dynamics simulations over long time scales.

    

### [[2102.01223] Inducing Meaningful Units from Character Sequences with Slot Attention](http://arxiv.org/abs/2102.01223)


  Characters do not convey meaning, but sequences of characters do. We propose
an unsupervised distributional method to learn the abstract meaning-bearing
units in a sequence of characters. Rather than segmenting the sequence, this
model discovers continuous representations of the "objects" in the sequence,
using a recently proposed architecture for object discovery in images called
Slot Attention. We train our model on different languages and evaluate the
quality of the obtained representations with probing classifiers. Our
experiments show promising results in the ability of our units to capture
meaning at a higher level of abstraction.

    

### [[2102.05143] Classifier Calibration: with implications to threat scores in cybersecurity](http://arxiv.org/abs/2102.05143)


  This paper explores the calibration of a classifier output score in binary
classification problems. A calibrator is a function that maps the arbitrary
classifier score, of a testing observation, onto $[0,1]$ to provide an estimate
for the posterior probability of belonging to one of the two classes.
Calibration is important for two reasons; first, it provides a meaningful
score, that is the posterior probability; second, it puts the scores of
different classifiers on the same scale for comparable interpretation. The
paper presents three main contributions: (1) Introducing multi-score
calibration, when more than one classifier provides a score for a single
observation. (2) Introducing the idea that the classifier scores to a
calibration process are nothing but features to a classifier, hence proposing
expanding the classifier scores to higher dimensions to boost the calibrator's
performance. (3) Conducting a massive simulation study, in the order of 24,000
experiments, that incorporates different configurations, in addition to
experimenting on two real datasets from the cybersecurity domain. The results
show that there is no overall winner among the different calibrators and
different configurations. However, general advices for practitioners include
the following: the Platt's
calibrator~\citep{Platt1999ProbabilisticOutputsForSupport}, a version of the
logistic regression that decreases bias for a small sample size, has a very
stable and acceptable performance among all experiments; our suggested
multi-score calibration provides better performance than single score
calibration in the majority of experiments, including the two real datasets. In
addition, expanding the scores can help in some experiments.

    

### [[2102.11485] Generalized Equivariance and Preferential Labeling for GNN Node Classification](http://arxiv.org/abs/2102.11485)


  Existing graph neural networks (GNNs) largely rely on node embeddings, which
represent a node as a vector by its identity, type, or content. However, graphs
with unattributed nodes widely exist in real-world applications (e.g.,
anonymized social networks). Previous GNNs either assign random labels to nodes
(which introduces artefacts to the GNN) or assign one embedding to all nodes
(which fails to explicitly distinguish one node from another). Further, when
these GNNs are applied to unattributed node classification problems, they have
an undesired equivariance property, which are fundamentally unable to address
the data with multiple possible outputs. In this paper, we analyze the
limitation of existing approaches to node classification problems. Inspired by
our analysis, we propose a generalized equivariance property and a Preferential
Labeling technique that satisfies the desired property asymptotically.
Experimental results show that we achieve high performance in several
unattributed node classification tasks.

    

### [[2102.11724] Causal Mediation Analysis with Hidden Confounders](http://arxiv.org/abs/2102.11724)


  An important problem in causal inference is to break down the total effect of
a treatment on an outcome into different causal pathways and to quantify the
causal effect in each pathway. For instance, in causal fairness, the total
effect of being a male employee (i.e., treatment) constitutes its direct effect
on annual income (i.e., outcome) and the indirect effect via the employee's
occupation (i.e., mediator). Causal mediation analysis (CMA) is a formal
statistical framework commonly used to reveal such underlying causal
mechanisms. One major challenge of CMA in observational studies is handling
confounders, variables that cause spurious causal relationships among
treatment, mediator, and outcome. Conventional methods assume sequential
ignorability that implies all confounders can be measured, which is often
unverifiable in practice. This work aims to circumvent the stringent sequential
ignorability assumptions and consider hidden confounders. Drawing upon proxy
strategies and recent advances in deep learning, we propose to simultaneously
uncover the latent variables that characterize hidden confounders and estimate
the causal effects. Empirical evaluations using both synthetic and
semi-synthetic datasets validate the effectiveness of the proposed method. We
further show the potentials of our approach for causal fairness analysis.

    

### [[2103.00794] Early-Bird GCNs: Graph-Network Co-Optimization Towards More Efficient GCN Training and Inference via Drawing Early-Bird Lottery Tickets](http://arxiv.org/abs/2103.00794)


  Graph Convolutional Networks (GCNs) have emerged as the state-of-the-art deep
learning model for representation learning on graphs. However, it remains
notoriously challenging to train and inference GCNs over large graph datasets,
limiting their application to large real-world graphs and hindering the
exploration of deeper and more sophisticated GCN graphs. This is because as the
graph size grows, the sheer number of node features and the large adjacency
matrix can easily explode the required memory and data movements. To tackle the
aforementioned challenges, we explore the possibility of drawing lottery
tickets when sparsifying GCN graphs, i.e., subgraphs that largely shrink the
adjacency matrix yet are capable of achieving accuracy comparable to or even
better than their full graphs. Specifically, we for the first time discover the
existence of graph early-bird (GEB) tickets that emerge at the very early stage
when sparsifying GCN graphs, and propose a simple yet effective detector to
automatically identify the emergence of such GEB tickets. Furthermore, we
advocate graph-model co-optimization and develop a generic efficient GCN
early-bird training framework dubbed GEBT that can significantly boost the
efficiency of GCN training by (1) drawing joint early-bird tickets between the
GCN graphs and models and (2) enabling simultaneously sparsification of both
the GCN graphs and models. Experiments on various GCN models and datasets
consistently validate our GEB finding and the effectiveness of our GEBT, e.g.,
our GEBT achieves up to 80.2% ~ 85.6% and 84.6% ~ 87.5% savings of GCN training
and inference costs while offering a comparable or even better accuracy as
compared to state-of-the-art methods. Our source code and supplementary
appendix are available at this https URL.

    

### [[2103.01007] Error Estimates for the Variational Training of Neural Networks with Boundary Penalty](http://arxiv.org/abs/2103.01007)


  We establish estimates on the error made by the Deep Ritz Method for elliptic
problems on the space $H^1(\Omega)$ with different boundary conditions. For
Dirichlet boundary conditions, we estimate the error when the boundary values
are approximately enforced through the boundary penalty method. Our results
apply to arbitrary and in general non linear classes $V\subseteq H^1(\Omega)$
of ansatz functions and estimate the error in dependence of the optimization
accuracy, the approximation capabilities of the ansatz class and -- in the case
of Dirichlet boundary values -- the penalisation strength $\lambda$. For
non-essential boundary conditions the error of the Ritz method decays with the
same rate as the approximation rate of the ansatz classes. For essential
boundary conditions, given an approximation rate of $r$ in $H^1(\Omega)$ and an
approximation rate of $s$ in $L^2(\partial\Omega)$ of the ansatz classes, the
optimal decay rate of the estimated error is $\min(s/2, r)$ and achieved by
choosing $\lambda_n\sim n^{s}$. We discuss the implications for ansatz classes
which are given through ReLU networks and the relation to existing estimates
for finite element functions.

    

### [[2103.10918] Play the Shannon Game With Language Models: A Human-Free Approach to Summary Evaluation](http://arxiv.org/abs/2103.10918)


  The goal of a summary is to concisely state the most important information in
a document. With this principle in mind, we introduce new reference-free
summary evaluation metrics that use a pretrained language model to estimate the
information content shared between a document and its summary. These metrics
are a modern take on the Shannon Game, a method for summary quality scoring
proposed decades ago, where we replace human annotators with language models.
We also view these metrics as an extension of BLANC, a recently proposed
approach to summary quality measurement based on the performance of a language
model with and without the help of a summary. Using transformer based language
models, we empirically verify that our metrics achieve state-of-the-art
correlation with human judgement of the summary quality dimensions of both
coherence and relevance, as well as competitive correlation with human
judgement of consistency and fluency.

    

### [[2103.14347] Combating Adversaries with Anti-Adversaries](http://arxiv.org/abs/2103.14347)


  Deep neural networks are vulnerable to small input perturbations known as
adversarial attacks. Inspired by the fact that these adversaries are
constructed by iteratively minimizing the confidence of a network for the true
class label, we propose the anti-adversary layer, aimed at countering this
effect. In particular, our layer generates an input perturbation in the
opposite direction of the adversarial one and feeds the classifier a perturbed
version of the input. Our approach is training-free and theoretically
supported. We verify the effectiveness of our approach by combining our layer
with both nominally and robustly trained models and conduct large-scale
experiments from black-box to adaptive attacks on CIFAR10, CIFAR100, and
ImageNet. Our layer significantly enhances model robustness while coming at no
cost on clean accuracy.

    

### [[2106.00592] Semi-Supervised Domain Generalization with Stochastic StyleMatch](http://arxiv.org/abs/2106.00592)


  Ideally, visual learning algorithms should be generalizable, for dealing with
any unseen domain shift when deployed in a new target environment; and
data-efficient, for reducing development costs by using as little labels as
possible. To this end, we study semi-supervised domain generalization (SSDG),
which aims to learn a domain-generalizable model using multi-source,
partially-labeled training data. We design two benchmarks that cover
state-of-the-art methods developed in two related fields, i.e., domain
generalization (DG) and semi-supervised learning (SSL). We find that the DG
methods, which by design are unable to handle unlabeled data, perform poorly
with limited labels in SSDG; the SSL methods, especially FixMatch, obtain much
better results but are still far away from the basic vanilla model trained
using full labels. We propose StyleMatch, a simple approach that extends
FixMatch with a couple of new ingredients tailored for SSDG: 1) stochastic
modeling for reducing overfitting in scarce labels, and 2) multi-view
consistency learning for enhancing domain generalization. Despite the concise
designs, StyleMatch achieves significant improvements in SSDG. We hope our
approach and the comprehensive benchmarks can pave the way for future research
on generalizable and data-efficient learning systems. The source code is
released at \url{this https URL}.

    

### [[2106.01808] MINIMALIST: Mutual INformatIon Maximization for Amortized Likelihood Inference from Sampled Trajectories](http://arxiv.org/abs/2106.01808)


  Simulation-based inference enables learning the parameters of a model even
when its likelihood cannot be computed in practice. One class of methods uses
data simulated with different parameters to infer an amortized estimator for
the likelihood-to-evidence ratio, or equivalently the posterior function. We
show that this approach can be formulated in terms of mutual information
maximization between model parameters and simulated data. We use this
equivalence to reinterpret existing approaches for amortized inference and
propose two new methods that rely on lower bounds of the mutual information. We
apply our framework to the inference of parameters of stochastic processes and
chaotic dynamical systems from sampled trajectories, using artificial neural
networks for posterior prediction. Our approach provides a unified framework
that leverages the power of mutual information estimators for inference.

    

### [[2106.03035] Online Trading Models with Deep Reinforcement Learning in the Forex Market Considering Transaction Costs](http://arxiv.org/abs/2106.03035)


  In recent years, a wide range of investment models have been created using
artificial intelligence. Automatic trading by artificial intelligence can
expand the range of trading methods, such as by conferring the ability to
operate 24 hours a day and the ability to trade with high frequency. Automatic
trading can also be expected to trade with more information than is available
to humans if it can sufficiently consider past data. In this paper, we propose
an investment agent based on a deep reinforcement learning model, which is an
artificial intelligence model. The model considers the transaction costs
involved in actual trading and creates a framework for trading over a long
period of time so that it can make a large profit on a single trade. In doing
so, it can maximize the profit while keeping transaction costs low. In
addition, in consideration of actual operations, we use online learning so that
the system can continue to learn by constantly updating the latest online data
instead of learning with static data. This makes it possible to trade in
non-stationary financial markets by always incorporating current market trend
information.

    

### [[2106.04692] Provably Faster Algorithms for Bilevel Optimization](http://arxiv.org/abs/2106.04692)


  Bilevel optimization has been widely applied in many important machine
learning applications such as hyperparameter optimization and meta-learning.
Recently, several momentum-based algorithms have been proposed to solve bilevel
optimization problems faster. However, those momentum-based algorithms do not
achieve provably better computational complexity than $\mathcal{\widetilde
O}(\epsilon^{-2})$ of the SGD-based algorithm. In this paper, we propose two
new algorithms for bilevel optimization, where the first algorithm adopts
momentum-based recursive iterations, and the second algorithm adopts recursive
gradient estimations in nested loops to decrease the variance. We show that
both algorithms achieve the complexity of $\mathcal{\widetilde
O}(\epsilon^{-1.5})$, which outperforms all existing algorithms by the order of
magnitude. Our experiments validate our theoretical results and demonstrate the
superior empirical performance of our algorithms in hyperparameter
applications.

    

### [[2107.00379] On the Expected Complexity of Maxout Networks](http://arxiv.org/abs/2107.00379)


  Learning with neural networks relies on the complexity of the representable
functions, but more importantly, the particular assignment of typical
parameters to functions of different complexity. Taking the number of
activation regions as a complexity measure, recent works have shown that the
practical complexity of deep ReLU networks is often far from the theoretical
maximum. In this work, we show that this phenomenon also occurs in networks
with maxout (multi-argument) activation functions and when considering the
decision boundaries in classification tasks. We also show that the parameter
space has a multitude of full-dimensional regions with widely different
complexity, and obtain nontrivial lower bounds on the expected complexity.
Finally, we investigate different parameter initialization procedures and show
that they can increase the speed of convergence in training.

    

### [[2108.00946] StyleGAN-NADA: CLIP-Guided Domain Adaptation of Image Generators](http://arxiv.org/abs/2108.00946)


  Can a generative model be trained to produce images from a specific domain,
guided by a text prompt only, without seeing any image? In other words: can an
image generator be trained "blindly"? Leveraging the semantic power of large
scale Contrastive-Language-Image-Pre-training (CLIP) models, we present a
text-driven method that allows shifting a generative model to new domains,
without having to collect even a single image. We show that through natural
language prompts and a few minutes of training, our method can adapt a
generator across a multitude of domains characterized by diverse styles and
shapes. Notably, many of these modifications would be difficult or outright
impossible to reach with existing methods. We conduct an extensive set of
experiments and comparisons across a wide range of domains. These demonstrate
the effectiveness of our approach and show that our shifted models maintain the
latent-space properties that make generative models appealing for downstream
tasks.

    

### [[2110.13097] Rotation Equivariant Deforestation Segmentation and Driver Classification](http://arxiv.org/abs/2110.13097)


  Deforestation has become a significant contributing factor to climate change
and, due to this, both classifying the drivers and predicting segmentation maps
of deforestation has attracted significant interest. In this work, we develop a
rotation equivariant convolutional neural network model to predict the drivers
and generate segmentation maps of deforestation events from Landsat 8 satellite
images. This outperforms previous methods in classifying the drivers and
predicting the segmentation map of deforestation, offering a 9% improvement in
classification accuracy and a 7% improvement in segmentation map accuracy. In
addition, this method predicts stable segmentation maps under rotation of the
input image, which ensures that predicted regions of deforestation are not
dependent upon the rotational orientation of the satellite.

    

### [[2111.14733] Crime Prediction with Graph Neural Networks and Multivariate Normal Distributions](http://arxiv.org/abs/2111.14733)


  Existing approaches to the crime prediction problem are unsuccessful in
expressing the details since they assign the probability values to large
regions. This paper introduces a new architecture with the graph convolutional
networks (GCN) and multivariate Gaussian distributions to perform
high-resolution forecasting that applies to any spatiotemporal data. We tackle
the sparsity problem in high resolution by leveraging the flexible structure of
GCNs and providing a subdivision algorithm. We build our model with Graph
Convolutional Gated Recurrent Units (Graph-ConvGRU) to learn spatial, temporal,
and categorical relations. In each node of the graph, we learn a multivariate
probability distribution from the extracted features of GCNs. We perform
experiments on real-life and synthetic datasets, and our model obtains the best
validation and the best test score among the baseline models with significant
improvements. We show that our model is not only generative but also precise.

    

### [[2112.06431] GM Score: Incorporating inter-class and intra-class generator diversity, discriminability of disentangled representation, and sample fidelity for evaluating GANs](http://arxiv.org/abs/2112.06431)


  While generative adversarial networks (GAN) are popular for their higher
sample quality as opposed to other generative models like the variational
autoencoders (VAE) and Boltzmann machines, they suffer from the same difficulty
of the evaluation of generated samples. Various aspects must be kept in mind,
such as the quality of generated samples, the diversity of classes (within a
class and among classes), the use of disentangled latent spaces, agreement of
said evaluation metric with human perception, etc. In this paper, we propose a
new score, namely, GM Score, which takes into various factors such as sample
quality, disentangled representation, intra-class and inter-class diversity,
and other metrics such as precision, recall, and F1 score are employed for
discriminability of latent space of deep belief network (DBN) and restricted
Boltzmann machine (RBM). The evaluation is done for different GANs (GAN, DCGAN,
BiGAN, CGAN, CoupledGAN, LSGAN, SGAN, WGAN, and WGAN Improved) trained on the
benchmark MNIST dataset.

    

### [[2112.06580] How to Find a Good Explanation for Clustering?](http://arxiv.org/abs/2112.06580)


  $k$-means and $k$-median clustering are powerful unsupervised machine
learning techniques. However, due to complicated dependences on all the
features, it is challenging to interpret the resulting cluster assignments.
Moshkovitz, Dasgupta, Rashtchian, and Frost [ICML 2020] proposed an elegant
model of explainable $k$-means and $k$-median clustering. In this model, a
decision tree with $k$ leaves provides a straightforward characterization of
the data set into clusters.
We study two natural algorithmic questions about explainable clustering. (1)
For a given clustering, how to find the "best explanation" by using a decision
tree with $k$ leaves? (2) For a given set of points, how to find a decision
tree with $k$ leaves minimizing the $k$-means/median objective of the resulting
explainable clustering? To address the first question, we introduce a new model
of explainable clustering. Our model, inspired by the notion of outliers in
robust statistics, is the following. We are seeking a small number of points
(outliers) whose removal makes the existing clustering well-explainable. For
addressing the second question, we initiate the study of the model of
Moshkovitz et al. from the perspective of multivariate complexity. Our rigorous
algorithmic analysis sheds some light on the influence of parameters like the
input size, dimension of the data, the number of outliers, the number of
clusters, and the approximation ratio, on the computational complexity of
explainable clustering.

    

### [[2112.06660] Subspace Decomposition based DNN algorithm for elliptic type multi-scale PDEs](http://arxiv.org/abs/2112.06660)


  While deep learning algorithms demonstrate a great potential in scientific
computing, its application to multi-scale problems remains to be a big
challenge. This is manifested by the "frequency principle" that neural networks
tend to learn low frequency components first. Novel architectures such as
multi-scale deep neural network (MscaleDNN) were proposed to alleviate this
problem to some extent. In this paper, we construct a subspace decomposition
based DNN (dubbed SD$^2$NN) architecture for a class of multi-scale problems by
combining traditional numerical analysis ideas and MscaleDNN algorithms. The
proposed architecture includes one low frequency normal DNN submodule, and one
(or a few) high frequency MscaleDNN submodule(s), which are designed to capture
the smooth part and the oscillatory part of the multi-scale solutions,
respectively. In addition, a novel trigonometric activation function is
incorporated in the SD$^2$NN model. We demonstrate the performance of the
SD$^2$NN architecture through several benchmark multi-scale problems in regular
or irregular geometric domains. Numerical results show that the SD$^2$NN model
is superior to existing models such as MscaleDNN.

    

### [[2112.08371] Blockchain as an IoT intermediary](http://arxiv.org/abs/2112.08371)


  Blockchain technology provides a private, secure, transparent decentralized
exchange of data. Also, blockchain is not limited to a particular area, but it
has a wide range of applications and can be integrated into a variety of
Internet interactive systems. For example, the Internet of Things (IoT), supply
chain tracking, Electronic Health Records (EHR), digital forensics, identity
management, trustless payments, and other key business elements will all
benefit from its implementation. Next layer solutions such as Ethereum 2.0,
Polkadot, Cardano, and other Web 3.0 technologies provide developers
versatility. Moreover, these platforms utilize smart contracts which are
similar to standard, traditionalized software during development but offer key
utilities to end-users such as online wallets, secure data with transparent
rules. Blockchain is receiving a lot of attention in educational technology
(EduTech) as it aims to achieve a more transparent and multipurpose educational
system. In addition to smart contract technology which defines how data should
be registered, gathered and processed, blockchain can be used as an IoT
intermediary for mobile usage. Therefore, we implemented an educational
learning platform powered by blockchain technology to examine feasibility in
industry and academic environment. In essence, this is a web application which
is adapted to mobile platform and connected to blockchain for crucial data
exchanges. In this paper we want to emphasize the potential of blockchain
technology in multiple sectors as well as the need to really understand the
underlying principles which are allowing disruptability of traditional
centralized software solutions.

    

### [[2112.08862] Addressing Adversarial Machine Learning Attacks in Smart Healthcare Perspectives](http://arxiv.org/abs/2112.08862)


  Smart healthcare systems are gaining popularity with the rapid development of
intelligent sensors, the Internet of Things (IoT) applications and services,
and wireless communications. However, at the same time, several vulnerabilities
and adversarial attacks make it challenging for a safe and secure smart
healthcare system from a security point of view. Machine learning has been used
widely to develop suitable models to predict and mitigate attacks. Still, the
attacks could trick the machine learning models and misclassify outputs
generated by the model. As a result, it leads to incorrect decisions, for
example, false disease detection and wrong treatment plans for patients. In
this paper, we address the type of adversarial attacks and their impact on
smart healthcare systems. We propose a model to examine how adversarial attacks
impact machine learning classifiers. To test the model, we use a medical image
dataset. Our model can classify medical images with high accuracy. We then
attacked the model with a Fast Gradient Sign Method attack (FGSM) to cause the
model to predict the images and misclassify them inaccurately. Using transfer
learning, we train a VGG-19 model with the medical dataset and later implement
the FGSM to the Convolutional Neural Network (CNN) to examine the significant
impact it causes on the performance and accuracy of the machine learning model.
Our results demonstrate that the adversarial attack misclassifies the images,
causing the model's accuracy rate to drop from 88% to 11%.

    

### [[2112.08926] On the accuracy and performance of the lattice Boltzmann method with 64-bit, 32-bit and novel 16-bit number formats](http://arxiv.org/abs/2112.08926)


  Fluid dynamics simulations with the lattice Boltzmann method (LBM) are very
memory-intensive. Alongside reduction in memory footprint, significant
performance benefits can be achieved by using FP32 (single) precision compared
to FP64 (double) precision, especially on GPUs. Here, we evaluate the
possibility to use even FP16 and Posit16 (half) precision for storing fluid
populations, while still carrying arithmetic operations in FP32. For this, we
first show that the commonly occurring number range in the LBM is a lot smaller
than the FP16 number range. Based on this observation, we develop novel 16-bit
formats - based on a modified IEEE-754 and on a modified Posit standard - that
are specifically tailored to the needs of the LBM. We then carry out an
in-depth characterization of LBM accuracy for six different test systems with
increasing complexity: Poiseuille flow, Taylor-Green vortices, Karman vortex
streets, lid-driven cavity, a microcapsule in shear flow (utilizing the
immersed-boundary method) and finally the impact of a raindrop (based on a
Volume-of-Fluid approach). We find that the difference in accuracy between FP64
and FP32 is negligible in almost all cases, and that for a large number of
cases even 16-bit is sufficient. Finally, we provide a detailed performance
analysis of all precision levels on a large number of hardware
microarchitectures and show that significant speedup is achieved with mixed
FP32/16-bit.

    

### [[2112.08980] Performant, Multi-objective Scheduling of Highly Interleaved Task Graphs on Heterogeneous System on Chip Devices](http://arxiv.org/abs/2112.08980)


  Performance-, power-, and energy-aware scheduling techniques play an
essential role in optimally utilizing processing elements (PEs) of
heterogeneous systems. List schedulers, a class of low-complexity static
schedulers, have commonly been used in static execution scenarios. However,
list schedulers are not suitable for runtime decision making, particularly when
multiple concurrent applications are interleaved dynamically. For such cases,
the static task execution times and expectation of idle PEs assumed by list
schedulers lead to inefficient system utilization and poor performance. To
address this problem, we present techniques for optimizing execution of list
scheduling algorithms in dynamic runtime scenarios via a family of algorithms
inspired by the well-known heterogeneous earliest finish time (HEFT) list
scheduler. Through dynamically arriving, realistic workload scenarios that are
simulated in an open-source discrete event heterogeneous SoC simulator, we
exhaustively evaluate each of the proposed algorithms across two SoCs modeled
after the Xilinx Zynq Ultrascale+ ZCU102 and O-Droid XU3 development boards.
Altogether, depending on the chosen variant in this family of algorithms, we
are able to achieve an up to 39% execution time improvement, up to 7.24x
algorithmic speedup, or up to 30% energy consumption improvement compared to
the baseline HEFT implementation.

    

### [[2112.09017] Large Scale Distributed Linear Algebra With Tensor Processing Units](http://arxiv.org/abs/2112.09017)


  We have repurposed Google Tensor Processing Units (TPUs),
application-specific chips developed for machine learning, into large-scale
dense linear algebra supercomputers. The TPUs' fast inter-core interconnects
(ICI)s, physically two-dimensional network topology, and high-bandwidth memory
(HBM) permit distributed matrix multiplication algorithms to rapidly become
computationally bound. In this regime, the matrix-multiply units (MXU)s
dominate the runtime, yielding impressive scaling, performance, and raw size:
operating in float32 precision, a full 2048-core pod of third generation TPUs
can multiply two matrices with linear size $N= 220= 1 048 576$ in about 2
minutes. Via curated algorithms emphasizing large, single-core matrix
multiplications, other tasks in dense linear algebra can similarly scale. As
examples, we present (i) QR decomposition; (ii) resolution of linear systems;
and (iii) the computation of matrix functions by polynomial iteration,
demonstrated by the matrix polar factorization.

    

### [[1806.05971] An Enhanced Binary Particle-Swarm Optimization (E-BPSO) Algorithm for Service Placement in Hybrid Cloud Platforms](http://arxiv.org/abs/1806.05971)


  Nowadays, hybrid cloud platforms stand as an attractive solution for
organizations intending to implement combined private and public cloud
applications, in order to meet their profitability requirements. However, this
can only be achieved through the utilization of available resources while
speeding up execution processes. Accordingly, deploying new applications
entails dedicating some of these processes to a private cloud solution, while
allocating others to the public cloud. In this context, the present work is set
to help minimize relevant costs and deliver effective choices for an optimal
service placement solution within minimal execution time. Several evolutionary
algorithms have been applied to solve the service placement problem and are
used when dealing with complex solution spaces to provide an optimal placement
and often produce a short execution time. The standard BPSO algorithm is found
to display a significant disadvantage, namely, of easily trapping into local
optima, in addition to demonstrating a noticeable lack of robustness in dealing
with service placement problems. Hence, to overcome critical shortcomings
associated with the standard BPSO, an Enhanced Binary Particle Swarm
Optimization (E-BPSO) algorithm is proposed, consisting of a modification of
the particle position updating equation, initially inspired from the continuous
PSO. Our proposed E-BPSO algorithm is shown to outperform state-of-the-art
approaches in terms of both cost and execution time, using a real benchmark.

    

### [[2007.03576] Algorithm 1019: A Task-based Multi-shift QR/QZ Algorithm with Aggressive Early Deflation](http://arxiv.org/abs/2007.03576)


  The QR algorithm is one of the three phases in the process of computing the
eigenvalues and the eigenvectors of a dense nonsymmetric matrix. This paper
describes a task-based QR algorithm for reducing an upper Hessenberg matrix to
real Schur form. The task-based algorithm also supports generalized eigenvalue
problems (QZ algorithm) but this paper concentrates on the standard case. The
task-based algorithm adopts previous algorithmic improvements, such as
tightly-coupled multi-shifts and Aggressive Early Deflation (AED), and also
incorporates several new ideas that significantly improve the performance. This
includes, but is not limited to, the elimination of several synchronization
points, the dynamic merging of previously separate computational steps, the
shortening and the prioritization of the critical path, and experimental GPU
support. The task-based implementation is demonstrated to be multiple times
faster than multi-threaded LAPACK and ScaLAPACK in both single-node and
multi-node configurations on two different machines based on Intel and AMD
CPUs. The implementation is built on top of the StarPU runtime system and is
part of the open-source StarNEig library.

    

### [[2112.08414] DSGPT: Domain-Specific Generative Pre-Training of Transformers for Text Generation in E-commerce Title and Review Summarization](http://arxiv.org/abs/2112.08414)


  We propose a novel domain-specific generative pre-training (DS-GPT) method
for text generation and apply it to the product titleand review summarization
problems on E-commerce mobile display.First, we adopt a decoder-only
transformer architecture, which fitswell for fine-tuning tasks by combining
input and output all to-gether. Second, we demonstrate utilizing only small
amount of pre-training data in related domains is powerful. Pre-training a
languagemodel from a general corpus such as Wikipedia or the CommonCrawl
requires tremendous time and resource commitment, andcan be wasteful if the
downstream tasks are limited in variety. OurDSGPT is pre-trained on a limited
dataset, the Chinese short textsummarization dataset (LCSTS). Third, our model
does not requireproduct-related human-labeled data. For title summarization
task,the state of art explicitly uses additional background knowledgein
training and predicting stages. In contrast, our model implic-itly captures
this knowledge and achieves significant improvementover other methods, after
fine-tuning on the public Taobao.comdataset. For review summarization task, we
utilize this http URL in-housedataset, and observe similar improvement over standard
machinetranslation methods which lack the flexibility of fine-tuning.
Ourproposed work can be simply extended to other domains for a widerange of
text generation tasks.

    

### [[2112.08444] Combating Collusion Rings is Hard but Possible](http://arxiv.org/abs/2112.08444)


  A recent report of Littmann [Commun. ACM '21] outlines the existence and the
fatal impact of collusion rings in academic peer reviewing. We introduce and
analyze the problem Cycle-Free Reviewing that aims at finding a review
assignment without the following kind of collusion ring: A sequence of
reviewers each reviewing a paper authored by the next reviewer in the sequence
(with the last reviewer reviewing a paper of the first), thus creating a review
cycle where each reviewer gives favorable reviews. As a result, all papers in
that cycle have a high chance of acceptance independent of their respective
scientific merit.
We observe that review assignments computed using a standard Linear
Programming approach typically admit many short review cycles. On the negative
side, we show that Cycle-Free Reviewing is NP-hard in various restricted cases
(i.e., when every author is qualified to review all papers and one wants to
prevent that authors review each other's or their own papers or when every
author has only one paper and is only qualified to review few papers). On the
positive side, among others, we show that, in some realistic settings, an
assignment without any review cycles of small length always exists. This result
also gives rise to an efficient heuristic for computing (weighted) cycle-free
review assignments, which we show to be of excellent quality in practice.

    

### [[2112.08486] Text Mining Through Label Induction Grouping Algorithm Based Method](http://arxiv.org/abs/2112.08486)


  The main focus of information retrieval methods is to provide accurate and
efficient results which are cost-effective too. LINGO (Label Induction Grouping
Algorithm) is a clustering algorithm that aims to provide search results in
form of quality clusters but also has a few limitations. In this paper, our
focus is based on achieving results that are more meaningful and improving the
overall performance of the algorithm. LINGO works on two main steps; Cluster
Label Induction by using Latent Semantic Indexing technique (LSI) and Cluster
content discovery by using the Vector Space Model (VSM). As LINGO uses VSM in
cluster content discovery, our task is to replace VSM with LSI for cluster
content discovery and to analyze the feasibility of using LSI with Okapi BM25.
The next task is to compare the results of a modified method with the LINGO
original method. The research is applied to five different text-based data sets
to get more reliable results for every method. Research results show that LINGO
produces 40-50% better results when using LSI for content Discovery. From
theoretical evidence using Okapi BM25 for scoring method in LSI (LSI+Okapi
BM25) for cluster content discovery instead of VSM, also results in better
clusters generation in terms of scalability and performance when compares to
both VSM and LSI's Results.

    

### [[2112.08540] Integrated Guidance and Control for Lunar Landing using a Stabilized Seeker](http://arxiv.org/abs/2112.08540)


  We develop an integrated guidance and control system that in conjunction with
a stabilized seeker and landing site detection software can achieve precise and
safe planetary landing. The seeker tracks the designated landing site by
adjusting seeker elevation and azimuth angles to center the designated landing
site in the sensor field of view. The seeker angles, closing speed, and range
to the designated landing site are used to formulate a velocity field that is
used by the guidance and control system to achieve a safe landing at the
designated landing site. The guidance and control system maps this velocity
field, attitude, and rotational velocity directly to a commanded thrust vector
for the lander's four engines. The guidance and control system is implemented
as a policy optimized using reinforcement meta learning. We demonstrate that
the guidance and control system is compatible with multiple diverts during the
powered descent phase, and is robust to seeker lag, actuator lag and
degradation, and center of mass variation induced by fuel consumption. We
outline several concepts of operations, including an approach using a preplaced
landing beacon.

    

### [[2112.08544] NewsClaims: A New Benchmark for Claim Detection from News with Background Knowledge](http://arxiv.org/abs/2112.08544)


  Claim detection and verification are crucial for news understanding and have
emerged as promising technologies for mitigating misinformation in news.
However, most existing work focus on analysis of claim sentences while
overlooking crucial background attributes, such as the claimer, claim objects,
and other knowledge connected to the claim. In this work, we present NewsClaims
, a new benchmark for knowledge-aware claim detection in the news domain. We
re-define the claim detection problem to include extraction of additional
background attributes related to the claim and release 529 claims annotated
over 103 news articles. In addition, NewsClaims aims to benchmark claim
detection systems in emerging scenarios, comprising unseen topics with little
or no training data. Finally, we provide a comprehensive evaluation of various
zero-shot and prompt-based baselines for this new benchmark.

    

### [[2112.08581] A First Mathematical Runtime Analysis of the Non-Dominated Sorting Genetic Algorithm II (NSGA-II)](http://arxiv.org/abs/2112.08581)


  The non-dominated sorting genetic algorithm II (NSGA-II) is the most
intensively used multi-objective evolutionary algorithm (MOEA) in real-world
applications. However, in contrast to several simple MOEAs analyzed also via
mathematical means, no such study exists for the NSGA-II so far. In this work,
we show that mathematical runtime analyses are feasible also for the NSGA-II.
As particular results, we prove that with a population size larger than the
Pareto front size by a constant factor, the NSGA-II with two classic mutation
operators and three different ways to select the parents satisfies the same
asymptotic runtime guarantees as the SEMO and GSEMO algorithms on the basic
OneMinMax and LOTZ benchmark functions. However, if the population size is only
equal to the size of the Pareto front, then the NSGA-II cannot efficiently
compute the full Pareto front (for an exponential number of iterations, the
population will always miss a constant fraction of the Pareto front). Our
experiments confirm the above findings.

    

### [[2112.08588] Learning to acquire novel cognitive tasks with evolution, plasticity and meta-meta-learning](http://arxiv.org/abs/2112.08588)


  In meta-learning, networks are trained with external algorithms to learn
tasks that require acquiring, storing and exploiting unpredictable information
for each new instance of the task. However, animals are able to pick up such
cognitive tasks automatically, as a result of their evolved neural architecture
and synaptic plasticity mechanisms. Here we evolve neural networks, endowed
with plastic connections, over a sizable set of simple meta-learning tasks
based on a neuroscience modelling framework. The resulting evolved network can
automatically acquire a novel simple cognitive task, never seen during
training, through the spontaneous operation of its evolved neural organization
and plasticity structure. We suggest that attending to the multiplicity of
loops involved in natural learning may provide useful insight into the
emergence of intelligent behavior.

    

### [[2112.08589] Knowledge Graph Embedding in E-commerce Applications: Attentive Reasoning, Explanations, and Transferable Rules](http://arxiv.org/abs/2112.08589)


  Knowledge Graphs (KGs), representing facts as triples, have been widely
adopted in many applications. Reasoning tasks such as link prediction and rule
induction are important for the development of KGs. Knowledge Graph Embeddings
(KGEs) embedding entities and relations of a KG into continuous vector spaces,
have been proposed for these reasoning tasks and proven to be efficient and
robust. But the plausibility and feasibility of applying and deploying KGEs in
real-work applications has not been well-explored. In this paper, we discuss
and report our experiences of deploying KGEs in a real domain application:
e-commerce. We first identity three important desiderata for e-commerce KG
systems: 1) attentive reasoning, reasoning over a few target relations of more
concerns instead of all; 2) explanation, providing explanations for a
prediction to help both users and business operators understand why the
prediction is made; 3) transferable rules, generating reusable rules to
accelerate the deployment of a KG to new systems. While non existing KGE could
meet all these desiderata, we propose a novel one, an explainable knowledge
graph attention network that make prediction through modeling correlations
between triples rather than purely relying on its head entity, relation and
tail entity embeddings. It could automatically selects attentive triples for
prediction and records the contribution of them at the same time, from which
explanations could be easily provided and transferable rules could be
efficiently produced. We empirically show that our method is capable of meeting
all three desiderata in our e-commerce application and outperform typical
baselines on datasets from real domain applications.

    

### [[2112.08593] Goal-Directed Story Generation: Augmenting Generative Language Models with Reinforcement Learning](http://arxiv.org/abs/2112.08593)


  The advent of large pre-trained generative language models has provided a
common framework for AI story generation via sampling the model to create
sequences that continue the story. However, sampling alone is insufficient for
story generation. In particular, it is hard to direct a language model to
create stories to reach a specific goal event. We present two automated
techniques grounded in deep reinforcement learning and reward shaping to
control the plot of computer-generated stories. The first utilizes proximal
policy optimization to fine-tune an existing transformer-based language model
to generate text continuations but also be goal-seeking. The second extracts a
knowledge graph from the unfolding story, which is used by a policy network
with graph attention to select a candidate continuation generated by a language
model. We report on automated metrics pertaining to how often stories achieve a
given goal event as well as human participant rankings of coherence and overall
story quality compared to baselines and ablations.

    

### [[2112.08605] Frequency Spectrum Augmentation Consistency for Domain Adaptive Object Detection](http://arxiv.org/abs/2112.08605)


  Domain adaptive object detection (DAOD) aims to improve the generalization
ability of detectors when the training and test data are from different
domains. Considering the significant domain gap, some typical methods, e.g.,
CycleGAN-based methods, adopt the intermediate domain to bridge the source and
target domains progressively. However, the CycleGAN-based intermediate domain
lacks the pix- or instance-level supervision for object detection, which leads
to semantic differences. To address this problem, in this paper, we introduce a
Frequency Spectrum Augmentation Consistency (FSAC) framework with four
different low-frequency filter operations. In this way, we can obtain a series
of augmented data as the intermediate domain. Concretely, we propose a
two-stage optimization framework. In the first stage, we utilize all the
original and augmented source data to train an object detector. In the second
stage, augmented source and target data with pseudo labels are adopted to
perform the self-training for prediction consistency. And a teacher model
optimized using Mean Teacher is used to further revise the pseudo labels. In
the experiment, we evaluate our method on the single- and compound- target DAOD
separately, which demonstrate the effectiveness of our method.

    

### [[2112.08619] Call for Customized Conversation: Customized Conversation Grounding Persona and Knowledge](http://arxiv.org/abs/2112.08619)


  Humans usually have conversations by making use of prior knowledge about a
topic and background information of the people whom they are talking to.
However, existing conversational agents and datasets do not consider such
comprehensive information, and thus they have a limitation in generating the
utterances where the knowledge and persona are fused properly. To address this
issue, we introduce a call For Customized conversation (FoCus) dataset where
the customized answers are built with the user's persona and Wikipedia
knowledge. To evaluate the abilities to make informative and customized
utterances of pre-trained language models, we utilize BART and GPT-2 as well as
transformer-based models. We assess their generation abilities with automatic
scores and conduct human evaluations for qualitative results. We examine
whether the model reflects adequate persona and knowledge with our proposed two
sub-tasks, persona grounding (PG) and knowledge grounding (KG). Moreover, we
show that the utterances of our data are constructed with the proper knowledge
and persona through grounding quality assessment.

    

### [[2112.08637] Analyzing the Limits of Self-Supervision in Handling Bias in Language](http://arxiv.org/abs/2112.08637)


  Prompting inputs with natural language task descriptions has emerged as a
popular mechanism to elicit reasonably accurate outputs from large-scale
generative language models with little to no in-context supervision. This also
helps gain insight into how well language models capture the semantics of a
wide range of downstream tasks purely from self-supervised pre-training on
massive corpora of unlabeled text. Such models have naturally also been exposed
to a lot of undesirable content like racist and sexist language and there is
limited work on awareness of models along these dimensions. In this paper, we
define and comprehensively evaluate how well such language models capture the
semantics of four tasks for bias: diagnosis, identification, extraction and
rephrasing. We define three broad classes of task descriptions for these tasks:
statement, question, and completion, with numerous lexical variants within each
class. We study the efficacy of prompting for each task using these classes and
the null task description across several decoding methods and few-shot
examples. Our analyses indicate that language models are capable of performing
these tasks to widely varying degrees across different bias dimensions, such as
gender and political affiliation. We believe our work is an important step
towards unbiased language models by quantifying the limits of current
self-supervision objectives at accomplishing such sociologically challenging
tasks.

    

### [[2112.08643] TransZero++: Cross Attribute-Guided Transformer for Zero-Shot Learning](http://arxiv.org/abs/2112.08643)


  Zero-shot learning (ZSL) tackles the novel class recognition problem by
transferring semantic knowledge from seen classes to unseen ones. Existing
attention-based models have struggled to learn inferior region features in a
single image by solely using unidirectional attention, which ignore the
transferability and discriminative attribute localization of visual features.
In this paper, we propose a cross attribute-guided Transformer network, termed
TransZero++, to refine visual features and learn accurate attribute
localization for semantic-augmented visual embedding representations in ZSL.
TransZero++ consists of an attribute$\rightarrow$visual Transformer sub-net
(AVT) and a visual$\rightarrow$attribute Transformer sub-net (VAT).
Specifically, AVT first takes a feature augmentation encoder to alleviate the
cross-dataset problem, and improves the transferability of visual features by
reducing the entangled relative geometry relationships among region features.
Then, an attribute$\rightarrow$visual decoder is employed to localize the image
regions most relevant to each attribute in a given image for attribute-based
visual feature representations. Analogously, VAT uses the similar feature
augmentation encoder to refine the visual features, which are further applied
in visual$\rightarrow$attribute decoder to learn visual-based attribute
features. By further introducing semantical collaborative losses, the two
attribute-guided transformers teach each other to learn semantic-augmented
visual embeddings via semantical collaborative learning. Extensive experiments
show that TransZero++ achieves the new state-of-the-art results on three
challenging ZSL benchmarks. The codes are available at:
\url{this https URL}.

    

### [[2112.08656] DREAM: Uncovering Mental Models behind Language Models](http://arxiv.org/abs/2112.08656)


  To what extent do language models (LMs) build "mental models" of a scene when
answering situated questions (e.g., questions about a specific ethical
dilemma)? While cognitive science has shown that mental models play a
fundamental role in human problem-solving, it is unclear whether the high
question-answering performance of existing LMs is backed by similar model
building - and if not, whether that can explain their well-known catastrophic
failures. We observed that Macaw, an existing T5-based LM, when probed provides
somewhat useful but inadequate mental models for situational questions
(estimated accuracy=43%, usefulness=21%, consistency=42%). We propose DREAM, a
model that takes a situational question as input to produce a mental model
elaborating the situation, without any additional task specific training data
for mental models. It inherits its social commonsense through distant
supervision from existing NLP resources. Our analysis shows that DREAM can
produce significantly better mental models (estimated accuracy=67%,
usefulness=37%, consistency=71%) compared to Macaw. Finally, mental models
generated by DREAM can be used as additional context for situational QA tasks.
This additional context improves the answer accuracy of a Macaw zero-shot model
by between +1% and +4% (absolute) on three different datasets.

    

### [[2112.08684] META: Mimicking Embedding via oThers' Aggregation for Generalizable Person Re-identification](http://arxiv.org/abs/2112.08684)


  Domain generalizable (DG) person re-identification (ReID) aims to test across
unseen domains without access to the target domain data at training time, which
is a realistic but challenging problem. In contrast to methods assuming an
identical model for different domains, Mixture of Experts (MoE) exploits
multiple domain-specific networks for leveraging complementary information
between domains, obtaining impressive results. However, prior MoE-based DG ReID
methods suffer from a large model size with the increase of the number of
source domains, and most of them overlook the exploitation of domain-invariant
characteristics. To handle the two issues above, this paper presents a new
approach called Mimicking Embedding via oThers' Aggregation (META) for DG ReID.
To avoid the large model size, experts in META do not add a branch network for
each source domain but share all the parameters except for the batch
normalization layers. Besides multiple experts, META leverages Instance
Normalization (IN) and introduces it into a global branch to pursue invariant
features across domains. Meanwhile, META considers the relevance of an unseen
target sample and source domains via normalization statistics and develops an
aggregation network to adaptively integrate multiple experts for mimicking
unseen target domain. Benefiting from a proposed consistency loss and an
episodic training algorithm, we can expect META to mimic embedding for a truly
unseen target domain. Extensive experiments verify that META surpasses
state-of-the-art DG ReID methods by a large margin.

    

### [[2112.08717] GIMIRec: Global Interaction Information Aware Multi-Interest Framework for Sequential Recommendation](http://arxiv.org/abs/2112.08717)


  Sequential recommendation based on multi-interest framework models the user's
recent interaction sequence into multiple different interest vectors, since a
single low-dimensional vector cannot fully represent the diversity of user
interests. However, most existing models only intercept users' recent
interaction behaviors as training data, discarding a large amount of historical
interaction sequences. This may raise two issues. On the one hand, data
reflecting multiple interests of users is missing; on the other hand, the
co-occurrence between items in historical user-item interactions is not fully
explored. To tackle the two issues, this paper proposes a novel sequential
recommendation model called "Global Interaction Aware Multi-Interest Framework
for Sequential Recommendation (GIMIRec)". Specifically, a global context
extraction module is firstly proposed without introducing any external
information, which calculates a weighted co-occurrence matrix based on the
constrained co-occurrence number of each item pair and their time interval from
the historical interaction sequences of all users and then obtains the global
context embedding of each item by using a simplified graph convolution.
Secondly, the time interval of each item pair in the recent interaction
sequence of each user is captured and combined with the global context item
embedding to get the personalized item embedding. Finally, a self-attention
based multi-interest framework is applied to learn the diverse interests of
users for sequential recommendation. Extensive experiments on the three
real-world datasets of Amazon-Books, Taobao-Buy and Amazon-Hybrid show that the
performance of GIMIRec on the Recall, NDCG and Hit Rate indicators is
significantly superior to that of the state-of-the-art methods. Moreover, the
proposed global context extraction module can be easily transplanted to most
sequential recommendation models.

    

### [[2112.08735] Pay More Attention to History: A Context Modeling Strategy for Conversational Text-to-SQL](http://arxiv.org/abs/2112.08735)


  Conversational text-to-SQL aims at converting multi-turn natural language
queries into their corresponding SQL representations. One of the most
intractable problem of conversational text-to-SQL is modeling the semantics of
multi-turn queries and gathering proper information required for the current
query. This paper shows that explicit modeling the semantic changes by adding
each turn and the summarization of the whole context can bring better
performance on converting conversational queries into SQLs. In particular, we
propose two conversational modeling tasks in both turn grain and conversation
grain. These two tasks simply work as auxiliary training tasks to help with
multi-turn conversational semantic parsing. We conducted empirical studies and
achieve new state-of-the-art results on large-scale open-domain conversational
text-to-SQL dataset. The results demonstrate that the proposed mechanism
significantly improves the performance of multi-turn semantic parsing.

    

### [[2112.08739] Forensic Analysis of Synthetically Generated Scientific Images](http://arxiv.org/abs/2112.08739)


  The widespread diffusion of synthetically generated content is a serious
threat that needs urgent countermeasures. The generation of synthetic content
is not restricted to multimedia data like videos, photographs, or audio
sequences, but covers a significantly vast area that can include biological
images as well, such as western-blot and microscopic images. In this paper, we
focus on the detection of synthetically generated western-blot images.
Western-blot images are largely explored in the biomedical literature and it
has been already shown how these images can be easily counterfeited with few
hope to spot manipulations by visual inspection or by standard forensics
detectors. To overcome the absence of a publicly available dataset, we create a
new dataset comprising more than 14K original western-blot images and 18K
synthetic western-blot images, generated by three different state-of-the-art
generation methods. Then, we investigate different strategies to detect
synthetic western blots, exploring binary classification methods as well as
one-class detectors. In both scenarios, we never exploit synthetic western-blot
images at training stage. The achieved results show that synthetically
generated western-blot images can be spot with good accuracy, even though the
exploited detectors are not optimized over synthetic versions of these
scientific images.

    

### [[2112.08766] CODER: An efficient framework for improving retrieval through COntextualized Document Embedding Reranking](http://arxiv.org/abs/2112.08766)


  We present a framework for improving the performance of a wide class of
retrieval models at minimal computational cost. It utilizes precomputed
document representations extracted by a base dense retrieval method and
involves training a model to jointly score a large set of retrieved candidate
documents for each query, while potentially transforming on the fly the
representation of each document in the context of the other candidates as well
as the query itself. When scoring a document representation based on its
similarity to a query, the model is thus aware of the representation of its
"peer" documents. We show that our approach leads to substantial improvement in
retrieval performance over the base method and over scoring candidate documents
in isolation from one another, as in a pair-wise training setting. Crucially,
unlike term-interaction rerankers based on BERT-like encoders, it incurs a
negligible computational overhead on top of any first-stage method at run time,
allowing it to be easily combined with any state-of-the-art dense retrieval
method. Finally, concurrently considering a set of candidate documents for a
given query enables additional valuable capabilities in retrieval, such as
score calibration and mitigating societal biases in ranking.

    

### [[2112.08776] Unsupervised Matching of Data and Text](http://arxiv.org/abs/2112.08776)


  Entity resolution is a widely studied problem with several proposals to match
records across relations. Matching textual content is a widespread task in many
applications, such as question answering and search. While recent methods
achieve promising results for these two tasks, there is no clear solution for
the more general problem of matching textual content and structured data. We
introduce a framework that supports this new task in an unsupervised setting
for any pair of corpora, being relational tables or text documents. Our method
builds a fine-grained graph over the content of the corpora and derives word
embeddings to represent the objects to match in a low dimensional space. The
learned representation enables effective and efficient matching at different
granularity, from relational tuples to text sentences and paragraphs. Our
flexible framework can exploit pre-trained resources, but it does not depends
on their existence and achieves better quality performance in matching content
when the vocabulary is domain specific. We also introduce optimizations in the
graph creation process with an "expand and compress" approach that first
identifies new valid relationships across elements, to improve matching, and
then prunes nodes and edges, to reduce the graph size. Experiments on real use
cases and public datasets show that our framework produces embeddings that
outperform word embeddings and fine-tuned language models both in results'
quality and in execution times.

    

### [[2112.08777] Utilizing Evidence Spans via Sequence-Level Contrastive Learning for Long-Context Question Answering](http://arxiv.org/abs/2112.08777)


  Long-range transformer models have achieved encouraging results on
long-context question answering (QA) tasks. Such tasks often require reasoning
over a long document, and they benefit from identifying a set of evidence spans
(e.g., sentences) that provide supporting evidence for addressing the question.
In this work, we propose a novel method for equipping long-range transformers
with an additional sequence-level objective for better identification of
supporting evidence spans. We achieve this by proposing an additional
contrastive supervision signal in finetuning, where the model is encouraged to
explicitly discriminate supporting evidence sentences from negative ones by
maximizing the question-evidence similarity. The proposed additional loss
exhibits consistent improvements on three different strong long-context
transformer models, across two challenging question answering benchmarks -
HotpotQA and QAsper.

    

### [[2112.08814] An Unsupervised Way to Understand Artifact Generating Internal Units in Generative Neural Networks](http://arxiv.org/abs/2112.08814)


  Despite significant improvements on the image generation performance of
Generative Adversarial Networks (GANs), generations with low visual fidelity
still have been observed. As widely used metrics for GANs focus more on the
overall performance of the model, evaluation on the quality of individual
generations or detection of defective generations is challenging. While recent
studies try to detect featuremap units that cause artifacts and evaluate
individual samples, these approaches require additional resources such as
external networks or a number of training data to approximate the real data
manifold. In this work, we propose the concept of local activation, and devise
a metric on the local activation to detect artifact generations without
additional supervision. We empirically verify that our approach can detect and
correct artifact generations from GANs with various datasets. Finally, we
discuss a geometrical analysis to partially reveal the relation between the
proposed concept and low visual fidelity.

    

### [[2112.08831] Bridging between Cognitive Processing Signals and Linguistic Features via a Unified Attentional Network](http://arxiv.org/abs/2112.08831)


  Cognitive processing signals can be used to improve natural language
processing (NLP) tasks. However, it is not clear how these signals correlate
with linguistic information. Bridging between human language processing and
linguistic features has been widely studied in neurolinguistics, usually via
single-variable controlled experiments with highly-controlled stimuli. Such
methods not only compromises the authenticity of natural reading, but also are
time-consuming and expensive. In this paper, we propose a data-driven method to
investigate the relationship between cognitive processing signals and
linguistic features. Specifically, we present a unified attentional framework
that is composed of embedding, attention, encoding and predicting layers to
selectively map cognitive processing signals to linguistic features. We define
the mapping procedure as a bridging task and develop 12 bridging tasks for
lexical, syntactic and semantic features. The proposed framework only requires
cognitive processing signals recorded under natural reading as inputs, and can
be used to detect a wide range of linguistic features with a single cognitive
dataset. Observations from experiment results resonate with previous
neuroscience findings. In addition to this, our experiments also reveal a
number of interesting findings, such as the correlation between contextual
eye-tracking features and tense of sentence.

    

### [[2112.08907] Inherently Explainable Reinforcement Learning in Natural Language](http://arxiv.org/abs/2112.08907)


  We focus on the task of creating a reinforcement learning agent that is
inherently explainable -- with the ability to produce immediate local
explanations by thinking out loud while performing a task and analyzing entire
trajectories post-hoc to produce causal explanations. This Hierarchically
Explainable Reinforcement Learning agent (HEX-RL), operates in Interactive
Fictions, text-based game environments in which an agent perceives and acts
upon the world using textual natural language. These games are usually
structured as puzzles or quests with long-term dependencies in which an agent
must complete a sequence of actions to succeed -- providing ideal environments
in which to test an agent's ability to explain its actions. Our agent is
designed to treat explainability as a first-class citizen, using an extracted
symbolic knowledge graph-based state representation coupled with a Hierarchical
Graph Attention mechanism that points to the facts in the internal graph
representation that most influenced the choice of actions. Experiments show
that this agent provides significantly improved explanations over strong
baselines, as rated by human participants generally unfamiliar with the
environment, while also matching state-of-the-art task performance.

    

### [[2112.08935] MVSS-Net: Multi-View Multi-Scale Supervised Networks for Image Manipulation Detection](http://arxiv.org/abs/2112.08935)


  The key research question for image manipulation detection is how to learn
generalizable features that are sensitive to manipulations in novel data,
whilst specific to prevent false alarms on authentic images. Current research
emphasizes the sensitivity, with the specificity mostly ignored. In this paper
we address both aspects by multi-view feature learning and multi-scale
supervision. By exploiting noise distribution and boundary artifacts
surrounding tampered regions, the former aims to learn semantic-agnostic and
thus more generalizable features. The latter allows us to learn from authentic
images which are nontrivial to be taken into account by the prior art that
relies on a semantic segmentation loss. Our thoughts are realized by a new
network which we term MVSS-Net and its enhanced version MVSS-Net++.
Comprehensive experiments on six public benchmark datasets justify the
viability of the MVSS-Net series for both pixel-level and image-level
manipulation detection.

    

### [[2112.08991] ADBCMM : Acronym Disambiguation by Building Counterfactuals and Multilingual Mixing](http://arxiv.org/abs/2112.08991)


  Scientific documents often contain a large number of acronyms. Disambiguation
of these acronyms will help researchers better understand the meaning of
vocabulary in the documents. In the past, thanks to large amounts of data from
English literature, acronym task was mainly applied in English literature.
However, for other low-resource languages, this task is difficult to obtain
good performance and receives less attention due to the lack of large amount of
annotation data. To address the above issue, this paper proposes an new method
for acronym disambiguation, named as ADBCMM, which can significantly improve
the performance of low-resource languages by building counterfactuals and
multilingual mixing. Specifically, by balancing data bias in low-resource
langauge, ADBCMM will able to improve the test performance outside the data
set. In SDU@AAAI-22 - Shared Task 2: Acronym Disambiguation, the proposed
method won first place in French and Spanish. You can repeat our results here
this https URL.

    

### [[2112.09081] CrossLoc: Scalable Aerial Localization Assisted by Multimodal Synthetic Data](http://arxiv.org/abs/2112.09081)


  We present a visual localization system that learns to estimate camera poses
in the real world with the help of synthetic data. Despite significant progress
in recent years, most learning-based approaches to visual localization target
at a single domain and require a dense database of geo-tagged images to
function well. To mitigate the data scarcity issue and improve the scalability
of the neural localization models, we introduce TOPO-DataGen, a versatile
synthetic data generation tool that traverses smoothly between the real and
virtual world, hinged on the geographic camera viewpoint. New large-scale
sim-to-real benchmark datasets are proposed to showcase and evaluate the
utility of the said synthetic data. Our experiments reveal that synthetic data
generically enhances the neural network performance on real data. Furthermore,
we introduce CrossLoc, a cross-modal visual representation learning approach to
pose estimation that makes full use of the scene coordinate ground truth via
self-supervision. Without any extra data, CrossLoc significantly outperforms
the state-of-the-art methods and achieves substantially higher real-data sample
efficiency. Our code is available at this https URL.

    

### [[2112.09118] Towards Unsupervised Dense Information Retrieval with Contrastive Learning](http://arxiv.org/abs/2112.09118)


  Information retrieval is an important component in natural language
processing, for knowledge intensive tasks such as question answering and fact
checking. Recently, information retrieval has seen the emergence of dense
retrievers, based on neural networks, as an alternative to classical sparse
methods based on term-frequency. These models have obtained state-of-the-art
results on datasets and tasks where large training sets are available. However,
they do not transfer well to new domains or applications with no training data,
and are often outperformed by term-frequency methods such as BM25 which are not
supervised. Thus, a natural question is whether it is possible to train dense
retrievers without supervision. In this work, we explore the limits of
contrastive learning as a way to train unsupervised dense retrievers, and show
that it leads to strong retrieval performance. More precisely, we show on the
BEIR benchmark that our model outperforms BM25 on 11 out of 15 datasets.
Furthermore, when a few thousands examples are available, we show that
fine-tuning our model on these leads to strong improvements compared to BM25.
Finally, when used as pre-training before fine-tuning on the MS-MARCO dataset,
our technique obtains state-of-the-art results on the BEIR benchmark.

    

### [[2112.09127] ICON: Implicit Clothed humans Obtained from Normals](http://arxiv.org/abs/2112.09127)


  Current methods for learning realistic and animatable 3D clothed avatars need
either posed 3D scans or 2D images with carefully controlled user poses. In
contrast, our goal is to learn the avatar from only 2D images of people in
unconstrained poses. Given a set of images, our method estimates a detailed 3D
surface from each image and then combines these into an animatable avatar.
Implicit functions are well suited to the first task, as they can capture
details like hair or clothes. Current methods, however, are not robust to
varied human poses and often produce 3D surfaces with broken or disembodied
limbs, missing details, or non-human shapes. The problem is that these methods
use global feature encoders that are sensitive to global pose. To address this,
we propose ICON ("Implicit Clothed humans Obtained from Normals"), which uses
local features, instead. ICON has two main modules, both of which exploit the
SMPL(-X) body model. First, ICON infers detailed clothed-human normals
(front/back) conditioned on the SMPL(-X) normals. Second, a visibility-aware
implicit surface regressor produces an iso-surface of a human occupancy field.
Importantly, at inference time, a feedback loop alternates between refining the
SMPL(-X) mesh using the inferred clothed normals and then refining the normals.
Given multiple reconstructed frames of a subject in varied poses, we use
SCANimate to produce an animatable avatar from them. Evaluation on the AGORA
and CAPE datasets shows that ICON outperforms the state of the art in
reconstruction, even with heavily limited training data. Additionally, it is
much more robust to out-of-distribution samples, e.g., in-the-wild poses/images
and out-of-frame cropping. ICON takes a step towards robust 3D clothed human
reconstruction from in-the-wild images. This enables creating avatars directly
from video with personalized and natural pose-dependent cloth deformation.

    

### [[2112.09131] HODOR: High-level Object Descriptors for Object Re-segmentation in Video Learned from Static Images](http://arxiv.org/abs/2112.09131)


  Existing state-of-the-art methods for Video Object Segmentation (VOS) learn
low-level pixel-to-pixel correspondences between frames to propagate object
masks across video. This requires a large amount of densely annotated video
data, which is costly to annotate, and largely redundant since frames within a
video are highly correlated. In light of this, we propose HODOR: a novel method
that tackles VOS by effectively leveraging annotated static images for
understanding object appearance and scene context. We encode object instances
and scene information from an image frame into robust high-level descriptors
which can then be used to re-segment those objects in different frames. As a
result, HODOR achieves state-of-the-art performance on the DAVIS and
YouTube-VOS benchmarks compared to existing methods trained without video
annotations. Without any architectural modification, HODOR can also learn from
video context around single annotated video frames by utilizing cyclic
consistency, whereas other methods rely on dense, temporally consistent
annotations.

    

### [[1904.11128] CBHE: Corner-based Building Height Estimation for Complex Street Scene Images](http://arxiv.org/abs/1904.11128)


  Building height estimation is important in many applications such as 3D city
reconstruction, urban planning, and navigation. Recently, a new building height
estimation method using street scene images and 2D maps was proposed. This
method is more scalable than traditional methods that use high-resolution
optical data, LiDAR data, or RADAR data which are expensive to obtain. The
method needs to detect building rooflines and then compute building height via
the pinhole camera model. We observe that this method has limitations in
handling complex street scene images in which buildings overlap with each other
and the rooflines are difficult to locate. We propose CBHE, a building height
estimation algorithm considering both building corners and rooflines. CBHE
first obtains building corner and roofline candidates in street scene images
based on building footprints from 2D maps and the camera parameters. Then, we
use a deep neural network named BuildingNet to classify and filter corner and
roofline candidates. Based on the valid corners and rooflines from BuildingNet,
CBHE computes building height via the pinhole camera model. Experimental
results show that the proposed BuildingNet yields a higher accuracy on building
corner and roofline candidate filtering compared with the state-of-the-art open
set classifiers. Meanwhile, CBHE outperforms the baseline algorithm by over 10%
in building height estimation accuracy.

    

### [[2001.11797] A comparison of Vector Symbolic Architectures](http://arxiv.org/abs/2001.11797)


  Vector Symbolic Architectures combine a high-dimensional vector space with a
set of carefully designed operators in order to perform symbolic computations
with large numerical vectors. Major goals are the exploitation of their
representational power and ability to deal with fuzziness and ambiguity. Over
the past years, several VSA implementations have been proposed. The available
implementations differ in the underlying vector space and the particular
implementations of the VSA operators. This paper provides an overview of eleven
available VSA implementations and discusses their commonalities and differences
in the underlying vector space and operators. We create a taxonomy of available
binding operations and show an important ramification for non self-inverse
binding operations using an example from analogical reasoning. A main
contribution is the experimental comparison of the available implementations in
order to evaluate (1) the capacity of bundles, (2) the approximation quality of
non-exact unbinding operations, (3) the influence of combining binding and
bundling operations on the query answering performance, and (4) the performance
on two example applications: visual place- and language-recognition. We expect
this comparison and systematization to be relevant for development of VSAs, and
to support the selection of an appropriate VSA for a particular task. The
implementations are available.

    

### [[2002.04734] Fast Complete Algorithm for Multiplayer Nash Equilibrium](http://arxiv.org/abs/2002.04734)


  We describe a new complete algorithm for computing Nash equilibrium in
multiplayer general-sum games, based on a quadratically-constrained feasibility
program formulation. We demonstrate that the algorithm runs significantly
faster than the prior fastest complete algorithm on several game classes
previously studied and that its runtimes even outperform the best incomplete
algorithms.

    

### [[2008.07644] Pictorial and apictorial polygonal jigsaw puzzles: The lazy caterer model, properties, and solvers](http://arxiv.org/abs/2008.07644)


  Jigsaw puzzle solving, the problem of constructing a coherent whole from a
set of non-overlapping unordered visual fragments, is fundamental to numerous
applications and yet most of the literature of the last two decades has focused
thus far on less realistic puzzles whose pieces are identical squares. Here we
formalize a new type of jigsaw puzzle where the pieces are general convex
polygons generated by cutting through a global polygonal shape/image with an
arbitrary number of straight cuts, a generation model inspired by the
celebrated Lazy caterer's sequence. We analyze the theoretical properties of
such puzzles, including the inherent challenges in solving them once pieces are
contaminated with geometrical noise. To cope with such difficulties and obtain
tractable solutions, we abstract the problem as a multi-body spring-mass
dynamical system endowed with hierarchical loop constraints and a layered
reconstruction process. We define evaluation metrics and present experimental
results on both apictorial and pictorial puzzles to show that they are solvable
completely automatically.

    

### [[2103.12701] A*+BFHS: A Hybrid Heuristic Search Algorithm](http://arxiv.org/abs/2103.12701)


  We present a new algorithm A*+BFHS for solving problems with unit-cost
operators where A* and IDA* fail due to memory limitations and/or the existence
of many distinct paths between the same pair of nodes. A*+BFHS is based on A*
and breadth-first heuristic search (BFHS). A*+BFHS combines advantages from
both algorithms, namely A*'s node ordering, BFHS's memory savings, and both
algorithms' duplicate detection. On easy problems, A*+BFHS behaves the same as
A*. On hard problems, it is slower than A* but saves a large amount of memory.
Compared to BFIDA*, A*+BFHS reduces the search time and/or memory requirement
by several times on a variety of planning domains.

    

### [[2104.11927] Anomaly Detection for Solder Joints Using $β$-VAE](http://arxiv.org/abs/2104.11927)


  In the assembly process of printed circuit boards (PCB), most of the errors
are caused by solder joints in Surface Mount Devices (SMD). In the literature,
traditional feature extraction based methods require designing hand-crafted
features and rely on the tiered RGB illumination to detect solder joint errors,
whereas the supervised Convolutional Neural Network (CNN) based approaches
require a lot of labelled abnormal samples (defective solder joints) to achieve
high accuracy. To solve the optical inspection problem in unrestricted
environments with no special lighting and without the existence of error-free
reference boards, we propose a new beta-Variational Autoencoders (beta-VAE)
architecture for anomaly detection that can work on both IC and non-IC
components. We show that the proposed model learns disentangled representation
of data, leading to more independent features and improved latent space
representations. We compare the activation and gradient-based representations
that are used to characterize anomalies; and observe the effect of different
beta parameters on accuracy and on untwining the feature representations in
beta-VAE. Finally, we show that anomalies on solder joints can be detected with
high accuracy via a model trained on directly normal samples without designated
hardware or feature engineering.

    

### [[2106.01263] Selecting the optimal dialogue response once for all from a panoramic view](http://arxiv.org/abs/2106.01263)


  As an essential component of dialogue systems, response selection aims to
pick out the optimal response among candidates to continue the dialogue. In
existing studies, this task is usually regarded as a binary classification
problem, where every candidate is ranked respectively for appropriateness. To
improve its performance, we reformulate this task as a multiple-choice problem
that allows the best selection to be made in one-shot inference. This new view
inspires us to propose an architecture called Panoramic-encoder (Our work will
be open-source for reproducibility and future research.) with a novel
Candidates Attention Mechanism (CAM), which allows context-wise attention
between responses and leads to fine-grained comparisons. Furthermore, we
investigate and incorporate several techniques that have been proven effective
for improving response selection. Experiments on three benchmarks show that our
method pushes the state-of-the-art while achieving approximately 3X faster
inference speed.

    

### [[2112.08958] Utility maximizing load balancing policies](http://arxiv.org/abs/2112.08958)


  Consider a service system where incoming tasks are instantaneously dispatched
to one out of many heterogeneous server pools. Associated with each server pool
is a concave utility function which depends on the class of the server pool and
its current occupancy. We derive an upper bound for the mean normalized
aggregate utility in stationarity and introduce two load balancing policies
that achieve this upper bound in a large-scale regime. Furthermore, the
transient and stationary behavior of these asymptotically optimal load
balancing policies is characterized on the scale of the number of server pools,
in the same large-scale regime.

    

### [[2012.10142] Learning and balancing unknown loads in large-scale systems](http://arxiv.org/abs/2012.10142)


  Consider a system of identical server pools where tasks with exponentially
distributed service times arrive as a time-inhomogenenous Poisson process. An
admission threshold is used in an inner control loop to assign incoming tasks
to server pools while, in an outer control loop, a learning scheme adjusts this
threshold over time to keep it aligned with the unknown offered load of the
system. In a many-server regime, we prove that the learning scheme reaches an
equilibrium along intervals of time where the normalized offered load per
server pool is suitably bounded, and that this results in a balanced
distribution of the load. Furthermore, we establish a similar result when tasks
with Coxian distributed service times arrive at a constant rate and the
threshold is adjusted using only the total number of tasks in the system. The
novel proof technique developed in this paper, which differs from a traditional
fluid limit analysis, allows to handle rapid variations of the first learning
scheme, triggered by excursions of the occupancy process that have vanishing
size. Moreover, our approach allows to characterize the asymptotic behavior of
the system with Coxian distributed service times without relying on a fluid
limit of a detailed state descriptor.

    

### [[2103.14466] Prioritise the Best Variation](http://arxiv.org/abs/2103.14466)


  Binary session types guarantee communication safety and session fidelity, but
alone they cannot rule out deadlocks arising from the interleaving of different
sessions. In Classical Processes (CP)$-$a process calculus based on classical
linear logic$-$deadlock freedom is guaranteed by combining channel creation and
parallel composition under the same logical cut rule. Similarly, in Good
Variation (GV)$-$a linear concurrent $\lambda$-calculus$-$deadlock freedom is
guaranteed by combining channel creation and thread spawning under the same
operation, called fork.
In both CP and GV, deadlock freedom is achieved at the expense of
expressivity, as the only processes allowed are tree-structured. Dardha and Gay
define Priority CP (PCP), which allows cyclic-structured processes and restores
deadlock freedom by using priorities, in line with Kobayashi and Padovani.
Following PCP, we present Priority GV (PGV), a variant of GV which decouples
channel creation from thread spawning. Consequently, we type cyclic-structured
processes and restore deadlock freedom by using priorities. We show that our
type system is sound by proving subject reduction and progress. We define an
encoding from PCP to PGV and prove that the encoding preserves typing and is
sound and complete with respect to the operational semantics.

    

### [<title>A new cycle-stepped Z80 emulator</title>](https://floooh.github.io/2021/12/17/cycle-stepped-z80.html)