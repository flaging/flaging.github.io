
## 2021-9-28

### [[2109.12143] Weather of the Dorm WIFI Ecosystem at the University of Colorado Boulder for Fall Semester 2019 to Spring Semester 2020 a Case Study of WIFI and a Campus Response to the COVID-19 Perturbation](http://arxiv.org/abs/2109.12143)


  Growing use of network technology in Higher Education means that there has
been increasing demand to adapt technology platforms and tools that transform
student learning strategies, faculty teaching, research modalities, as well as
general operations. Many of the new modalities are necessary for IHE business.
In August 2019, we began collecting and analyzing data from the campus WIFI
network. A goal of the research was to answer question like what passive
sensing of the IHE WIFI might tell us about the dynamics of the WIFI weather in
the IHE ecosystem and what does anonymized data tell us about the IHE
ecosystem. The analogy with weather prediction seemed appropriate and a viable
approach. Starting Fall 2019, data were collected in the observational phase.
In the analysis phase, we applied Singular Spectrum Analysis decomposition, to
deconstruct WIFI data from dorms, the central campus dining cafeteria, the
recreation center, and other buildings on campus. That analysis led to the
identification of clusters of buildings that behaved similarly. Just as in the
case of models of the weather, a final component of this research was
forecasting. We found that weekly forecast of WIFI behavior in the Fall 2019,
were straight forward using SSA and seemed to present behavior of a low
dimensional dynamical system. However, in Spring 2020, and the COVID
perturbation, the campus ecosystem received a shock and data show that the
campus changed very quickly. We found that as the campus moved to conduct
remote learning, teaching, the closure of research labs, and the edict to work
remotely, SSA forecasting techniques not trained on the Spring 2020, data after
the shock, performed poorly. While SSA forecasting trained on a portion of the
data did better.

    

### [[2109.12293] Adaptive video transmission using QUBO method and Digital Annealer based on Ising machine](http://arxiv.org/abs/2109.12293)


  With the dramatically increasing video streaming in the total network
traffic, it is critical to develop effective algorithms to promote the content
delivery service of high quality. Adaptive bitrate (ABR) control is the most
essential technique which determines the proper bitrate to be chosen based on
network conditions, thus realize high-quality video streaming. In this paper, a
novel ABR strategy is proposed based on Ising machine by using the quadratic
unconstrained binary optimization (QUBO) method and Digital Annealer (DA) for
the first time. The proposed method is evaluated by simulation with the
real-world measured throughput, and compared with other state-of-the-art
methods. Experiment results show that the proposed QUBO-based method can
outperform the existing methods, which demonstrating the superior of the
proposed QUBO-based method.

    

### [[2109.12408] Analysing GSM Insecurity](http://arxiv.org/abs/2109.12408)


  In the 1990s, GSM emerged as a cutting-edge technology that promised improved
services, mobility, security, and increase in the revenues of companies through
improved, secure communication for business transactions. The practical
experience, however, has shown global subscribers and companies that mere
technological excellence of GSM has not resulted in its success. This paper has
been carried out to analyse the existing GSM subscriber activities in both
personal and business worlds by implementing a customized GSM baseband protocol
stack on a phone. The paper encompasses the tasks of analysing the prevailing
GSM insecurity, establishing a conceptual design towards the findings revealed
in the practical personal and business environments and knowledge gathered from
the literature.

    

### [[2109.12409] Motivating Learners in Multi-Orchestrator Mobile Edge Learning: A Stackelberg Game Approach](http://arxiv.org/abs/2109.12409)


  Mobile Edge Learning (MEL) is a learning paradigm that enables distributed
training of Machine Learning models over heterogeneous edge devices (e.g., IoT
devices). Multi-orchestrator MEL refers to the coexistence of multiple learning
tasks with different datasets, each of which being governed by an orchestrator
to facilitate the distributed training process. In MEL, the training
performance deteriorates without the availability of sufficient training data
or computing resources. Therefore, it is crucial to motivate edge devices to
become learners and offer their computing resources, and either offer their
private data or receive the needed data from the orchestrator and participate
in the training process of a learning task. In this work, we propose an
incentive mechanism, where we formulate the orchestrators-learners interactions
as a 2-round Stackelberg game to motivate the participation of the learners. In
the first round, the learners decide which learning task to get engaged in, and
then in the second round, the amount of data for training in case of
participation such that their utility is maximized. We then study the game
analytically and derive the learners' optimal strategy. Finally, numerical
experiments have been conducted to evaluate the performance of the proposed
incentive mechanism.

    

### [[2109.12433] Blind Interference Alignment in 6G Optical Wireless Communications](http://arxiv.org/abs/2109.12433)


  In recent years, the demand for high speed wireless networking has increased
considerably due to the enormous number of devices connected to the Internet.
In this context, optical wireless communication (OWC) has received tremendous
interest in the context of next generation wireless networks where OWC offers a
huge unlicensed bandwidth using optical bands. OWC systems are directional and
can naturally provide multiple-input and multiple-output (MIMO) configurations
serving multiple users using a high number of transmitters in the indoor
environment to ensure coverage. Therefore, multiuser interference must be
managed efficiently to enhance the performance of OWC networks considering
different metrics. A transmission scheme referred to as blind interference
alignment (BIA) is proposed for OWC systems to maximize the multiplexing gain
without the need for channel state information (CSI) at the transmitters, which
is difficult to achieve in MIMO scenarios. However, standard BIA avoids the
need for CSI at the cost of requiring channel coherence time large enough for
transmitting the whole transmission block. Moreover, the methodology of BIA
results in increased noise with increase in the number of transmitters and
users. Therefore, various network topologies such as network centric (NC) and
user centric (UC) designs are proposed to relax the limitations of BIA where
these topologies divide the receiving area into multiple clusters. The results
show a significant enhancement in the performance of topological BIA compared
with standard BIA.

    

### [[2109.12452] Optimal Precoder Design for MIMO-OFDM-based Joint Automotive Radar-Communication Networks](http://arxiv.org/abs/2109.12452)


  Large-scale deployment of connected vehicles with cooperative awareness
technologies increases the demand for vehicle-to-everything (V2X) communication
spectrum in 5.9 GHz that is mainly allocated for the exchange of safety
messages. To supplement V2X communication and support the high data rates
needed by broadband applications, the millimeter-wave (mmWave) automotive radar
spectrum at 76-81 GHz can be utilized. For this purpose, joint
radar-communication systems have been proposed in the literature to perform
both functions using the same waveform and hardware. While multiple-input and
multiple-output (MIMO) communication with multiple users enables independent
data streaming for high throughput, MIMO radar processing provides
high-resolution imaging that is crucial for safety-critical systems. However,
employing conventional precoding methods designed for communication generates
directional beams that impair MIMO radar imaging and target tracking
capabilities during data streaming. In this paper, we propose a MIMO joint
automotive radar-communication (JARC) framework based on orthogonal frequency
division multiplexing (OFDM) waveform. First, we show that the MIMO-OFDM
preamble can be exploited for both MIMO radar processing and estimation of the
communication channel. Then, we propose an optimal precoder design method that
enables high accuracy target tracking while transmitting independent data
streams to multiple receivers. The proposed methods provide high-resolution
radar imaging and high throughput capabilities for MIMO JARC networks. Finally,
we evaluate the efficacy of the proposed methods through numerical simulations.

    

### [[2109.12810] Neighbor Discovery for VANET with Gossip Mechanism and Multi-packet Reception](http://arxiv.org/abs/2109.12810)


  Neighbor discovery (ND) is a key initial step of network configuration and
prerequisite of vehicular ad hoc network (VANET). However, the convergence
efficiency of ND is facing the requirements of multi-vehicle fast networking of
VANET with frequent topology changes. This paper proposes gossip-based
information dissemination and sensing information assisted ND with MPR
(GSIM-ND) algorithm for VANET. GSIM-ND algorithm leverages efficient
gossip-based information dissemination in the case of multi-packet reception
(MPR). Besides, through multi-target detection function of multiple sensors
installed in roadside unit (RSU), RSU can sense the distribution of vehicles
and help vehicles to obtain the distribution of their neighbors. Thus, GSIM-ND
algorithm leverages the dissemination of sensing information as well. The
expected number of discovered neighbors within a given period is theoretically
derived and used as the critical metric to evaluate the performance of GSIM-ND
algorithm. The expected bounds of the number of time slots when a given number
of neighbors needs to be discovered is derived as well. The simulation results
verify the correctness of theoretical derivation. It is discovered that GSIM-ND
algorithm proposed in this paper can always reach the short-term convergence
quickly. Moreover, GSIM-ND algorithm is more efficient and stable compared with
completely random algorithm (CRA), scan-based algorithm (SBA) and gossip-based
algorithm. The convergence time of GSIM-ND algorithm is 40\%-90\% lower than
that of these existing algorithms for both low density and high density
networks. Thus, GSIM-ND can improve the efficiency of ND algorithm.

    

### [[2003.09257] Grant-Free Coexistence of Critical and Non-Critical IoT Services in Two-Hop Satellite and Terrestrial Networks](http://arxiv.org/abs/2003.09257)


  Terrestrial and satellite communication networks often rely on two-hop
wireless architectures with an access channel followed by backhaul links.
Examples include Cloud-Radio Access Networks (C-RAN) and Low-Earth Orbit (LEO)
satellite systems. Furthermore, communication services characterized by the
coexistence of heterogeneous requirements are emerging as key use cases. This
paper studies the performance of critical service (CS) and non-critical service
(NCS) for Internet of Things (IoT) systems sharing a grant-free channel
consisting of radio access and backhaul segments. On the radio access segment,
IoT devices send packets to a set of non-cooperative access points (APs) using
slotted ALOHA (SA). The APs then forward correctly received messages to a base
station over a shared wireless backhaul segment adopting SA. We study first a
simplified erasure channel model, which is well suited for satellite
applications. Then, in order to account for terrestrial scenarios, the impact
of fading is considered. Among the main conclusions, we show that orthogonal
inter-service resource allocation is generally preferred for NCS devices, while
non-orthogonal protocols can improve the throughput and packet success rate of
CS devices for both terrestrial and satellite scenarios.

    

### [[2009.12471] Reducing Operation Cost of LPWAN Roadside Sensors Using Cross Technology Communication](http://arxiv.org/abs/2009.12471)


  Low-Power Wide-Area Network (LPWAN) is an emerging communication standard for
Internet of Things (IoT) that has strong potential to support connectivity of a
large number of roadside sensors with an extremely long communication range.
However, the high operation cost to manage such a large-scale roadside sensor
network remains as a significant challenge. In this article, we propose Low
Operation-Cost LPWAN (LOC-LPWAN), a novel optimization framework that is
designed to reduce the operation cost using the cross-technology communication
(CTC). LOC-LPWAN allows roadside sensors to offload sensor data to passing
vehicles that in turn forward the data to a LPWAN server using CTC aiming to
reduce the data subscription cost. LOC-LPWAN finds the optimal communication
schedule between sensors and vehicles to maximize the throughput given an
available budget. Furthermore, LOC-LPWAN optimizes the fairness among sensors
by preventing certain sensors from dominating the channel for data
transmission. LOC-LPWAN can also be configured to ensure that data packets are
received within a specific time bound. Extensive numerical analysis performed
with real-world taxi data consisting of 40 vehicles with 24-hour trajectories
demonstrate that LOC-LPWAN reduces the cost by 50% compared with the baseline
approach where no vehicle is used to relay packets. The results also show that
LOC-LPWAN improves the throughput by 72.6%, enhances the fairness by 65.7%, and
reduces the delay by 28.8% compared with a greedy algorithm given the same
amount of budget.

    

### [[2104.03182] Deep Learning and Traffic Classification: Lessons learned from a commercial-grade dataset with hundreds of encrypted and zero-day applications](http://arxiv.org/abs/2104.03182)


  The increasing success of Machine Learning (ML) and Deep Learning (DL) has
recently re-sparked interest towards traffic classification. While
classification of known traffic is a well investigated subject with supervised
classification tools (such as ML and DL models) are known to provide
satisfactory performance, detection of unknown (or zero-day) traffic is more
challenging and typically handled by unsupervised techniques (such as
clustering algorithms).
In this paper, we share our experience on a commercial-grade DL traffic
classification engine that is able to (i) identify known applications from
encrypted traffic, as well as (ii) handle unknown zero-day applications. In
particular, our contribution for (i) is to perform a thorough assessment of
state of the art traffic classifiers in commercial-grade settings comprising
few thousands of very fine grained application labels, as opposite to the few
tens of classes generally targeted in academic evaluations. Additionally, we
contribute to the problem of (ii) detection of zero-day applications by
proposing a novel technique, tailored for DL models, that is significantly more
accurate and light-weight than the state of the art.
Summarizing our main findings, we gather that (i) while ML and DL models are
both equally able to provide satisfactory solution for classification of known
traffic, however (ii) the non-linear feature extraction process of the DL
backbone provides sizeable advantages for the detection of unknown classes.

    

### [[2105.01194] Network Coding in Photonic-land: Three Commandments for Future-proof Optical Core Networks](http://arxiv.org/abs/2105.01194)


  The digital transformation has been underway, creating digital shadows of
(almost) all physical entities and moving them to the Internet. The era of
Internet of Everything has therefore started to come into play, giving rise to
unprecedented traffic growths. In this context, optical core networks forming
the backbone of Internet infrastructure have been under critical issues of
reaching the capacity limit of conventional fiber, a phenomenon widely referred
as capacity crunch. For many years, the many-fold increases in fiber capacity
is thanks to exploiting physical dimensions for multiplexing optical signals
such as wavelength, polarization, time and lately space-division multiplexing
using multi-core fibers and such route seems to come to an end as almost all
known ways have been exploited. This necessitates for a departure from
traditional approaches to use the fiber capacity more efficiently and thereby
improve economics of scale. This paper lays out a new perspective to integrate
network coding (NC) functions into optical networks to achieve greater capacity
efficiency by upgrading intermediate nodes functionalities. In addition to the
review of recent proposals on new research problems enabled by NC operation in
optical networks, we also report state-of-the-art findings in the literature in
an effort to renew the interest of NC in optical networks and discuss three
critical points for pushing forward its applicability and practicality
including i) NC as a new dimension for multiplexing optical signals ii)
algorithmic aspects of NC-enabled optical networks design iii) NC as an
entirely fresh way for securing optical signals at physical layers

    

### [[2105.07088] On Achilles Heel of Some Optical Network Designs and Performance Comparisons](http://arxiv.org/abs/2105.07088)


  This non-conventional paper represents the first attempt to uncover a
possible vulnerability in some proposals for optical network designs and
performance comparisons. While optical network designs and planning lie at the
heart of achieving fiber capacity efficiency and/or operational efficiency, its
combinatorial nature makes it computationally hard to reach optimal solutions
for realistic scenarios. Therefore, the well-established way that have been
taken for granted by not-so-small number of research papers is that an
optimization model based on mixed integer linear programming (MILP) is first
proposed and then due to the intractability of such combinatorial model, an
heuristic algorithm is offered as an approximation. The solution-quality
comparison between the MILP and heuristic is then carried out on small-scale
instances including topologies and traffic tests to verify the efficacy of the
proposed heuristic and the next step is to use such allegedly verified
heuristic for optical network designs of realistic scenarios. This approach may
nevertheless leave a critical vulnerability as there is no guarantee that one
performs well in small tests will generalize adequately for large-scale cases,
a common pitfall widely referred as the peril of extrapolation and/or
overfitting. Besides, it is not uncommon that in some research works, for
benchmarking purpose, the comparison between a new design proposal whose
performance is obtained from on one heuristic and a reference design based on
another heuristic is carried out. As the result of missing solution quality
check, such performance comparison relied merely on heuristic solutions may be
equally vulnerable as its results can be distorted and thus, be far from the
possibly achieved zones. In this work, we pinpoint those issues and provide a
realistic case study to highlight and demonstrate the impact of such
vulnerabilities.

    

### [[2107.01002] WiCluster: Passive Indoor 2D/3D Positioning using WiFi without Precise Labels](http://arxiv.org/abs/2107.01002)


  We introduce WiCluster, a new machine learning (ML) approach for passive
indoor positioning using radio frequency (RF) channel state information (CSI).
WiCluster can predict both a zone-level position and a precise 2D or 3D
position, without using any precise position labels during training. Prior
CSI-based indoor positioning work has relied on non-parametric approaches using
digital signal-processing (DSP) and, more recently, parametric approaches
(e.g., fully supervised ML methods). However these do not handle the complexity
of real-world environments well and do not meet requirements for large-scale
commercial deployments: the accuracy of DSP-based method deteriorates
significantly in non-line-of-sight conditions, while supervised ML methods need
large amounts of hard-to-acquire centimeter accuracy position labels. In
contrast, WiCluster is precise, requires weaker label-information that can be
easily collected, and works well in non-line-of-sight conditions. Our first
contribution is a novel dimensionality reduction method for charting. It
combines a triplet-loss with a multi-scale clustering-loss to map the
high-dimensional CSI representation to a 2D/3D latent space. Our second
contribution is two weakly supervised losses that map this latent space into a
Cartesian map, resulting in meter-accuracy position results. These losses only
require simple to acquire priors: a sketch of the floorplan, approximate
access-point locations and a few CSI packets that are labelled with the
corresponding zone in the floorplan. Thirdly, we report results and a
robustness study for 2D positioning in two single-floor office buildings and 3D
positioning in a two-story home.

    

### [[2109.12109] Robotic Vision for Space Mining](http://arxiv.org/abs/2109.12109)


  Future Moon bases will likely be constructed using resources mined from the
surface of the Moon. The difficulty of maintaining a human workforce on the
Moon and communications lag with Earth means that mining will need to be
conducted using collaborative robots with a high degree of autonomy. In this
paper, we explore the utility of robotic vision towards addressing several
major challenges in autonomous mining in the lunar environment: lack of
satellite positioning systems, navigation in hazardous terrain, and delicate
robot interactions. Specifically, we describe and report the results of robotic
vision algorithms that we developed for Phase 2 of the NASA Space Robotics
Challenge, which was framed in the context of autonomous collaborative robots
for mining on the Moon. The competition provided a simulated lunar environment
that exhibits the complexities alluded to above. We show how machine
learning-enabled vision could help alleviate the challenges posed by the lunar
environment. A robust multi-robot coordinator was also developed to achieve
long-term operation and effective collaboration between robots.

    

### [[2109.12111] Accurate Remaining Useful Life Prediction with Uncertainty Quantification: a Deep Learning and Nonstationary Gaussian Process Approach](http://arxiv.org/abs/2109.12111)


  Remaining useful life (RUL) refers to the expected remaining lifespan of a
component or system. Accurate RUL prediction is critical for prognostic and
health management and for maintenance planning. In this work, we address three
prevalent challenges in data-driven RUL prediction, namely the handling of high
dimensional input features, the robustness to noise in sensor data and
prognostic datasets, and the capturing of the time-dependency between system
degradation and RUL prediction. We devise a highly accurate RUL prediction
model with uncertainty quantification, which integrates and leverages the
advantages of deep learning and nonstationary Gaussian process regression
(DL-NSGPR). We examine and benchmark our model against other advanced
data-driven RUL prediction models using the turbofan engine dataset from the
NASA prognostic repository. Our computational experiments show that the
DL-NSGPR predictions are highly accurate with root mean square error 1.7 to 6.2
times smaller than those of competing RUL models. Furthermore, the results
demonstrate that RUL uncertainty bounds with the proposed DL-NSGPR are both
valid and significantly tighter than other stochastic RUL prediction models. We
unpack and discuss the reasons for this excellent performance of the DL-NSGPR.

    

### [[2109.12112] MCTS Based Agents for Multistage Single-Player Card Game](http://arxiv.org/abs/2109.12112)


  The article presents the use of Monte Carlo Tree Search algorithms for the
card game Lord of the Rings. The main challenge was the complexity of the game
mechanics, in which each round consists of 5 decision stages and 2 random
stages. To test various decision-making algorithms, a game simulator has been
implemented. The research covered an agent based on expert rules, using flat
Monte-Carlo search, as well as complete MCTS-UCB. Moreover different playout
strategies has been compared. As a result of experiments, an optimal (assuming
a limited time) combination of algorithms were formulated. The developed MCTS
based method have demonstrated a advantage over agent with expert knowledge.

    

### [[2109.12115] Use of the Deep Learning Approach to Measure Alveolar Bone Level](http://arxiv.org/abs/2109.12115)


  Abstract:
Aim: The goal was to use a Deep Convolutional Neural Network to measure the
radiographic alveolar bone level to aid periodontal diagnosis.
Material and methods: A Deep Learning (DL) model was developed by integrating
three segmentation networks (bone area, tooth, cementoenamel junction) and
image analysis to measure the radiographic bone level and assign radiographic
bone loss (RBL) stages. The percentage of RBL was calculated to determine the
stage of RBL for each tooth. A provisional periodontal diagnosis was assigned
using the 2018 periodontitis classification. RBL percentage, staging, and
presumptive diagnosis were compared to the measurements and diagnoses made by
the independent examiners.
Results: The average Dice Similarity Coefficient (DSC) for segmentation was
over 0.91. There was no significant difference in RBL percentage measurements
determined by DL and examiners (p=0.65). The Area Under the Receiver Operating
Characteristics Curve of RBL stage assignment for stage I, II and III was 0.89,
0.90 and 0.90, respectively. The accuracy of the case diagnosis was 0.85.
Conclusion: The proposed DL model provides reliable RBL measurements and
image-based periodontal diagnosis using periapical radiographic images.
However, this model has to be further optimized and validated by a larger
number of images to facilitate its application.

    

### [[2109.12144] Spatial Aggregation and Temporal Convolution Networks for Real-time Kriging](http://arxiv.org/abs/2109.12144)


  Spatiotemporal kriging is an important application in spatiotemporal data
analysis, aiming to recover/interpolate signals for unsampled/unobserved
locations based on observed signals. The principle challenge for spatiotemporal
kriging is how to effectively model and leverage the spatiotemporal
dependencies within the data. Recently, graph neural networks (GNNs) have shown
great promise for spatiotemporal kriging tasks. However, standard GNNs often
require a carefully designed adjacency matrix and specific aggregation
functions, which are inflexible for general applications/problems. To address
this issue, we present SATCN -- Spatial Aggregation and Temporal Convolution
Networks -- a universal and flexible framework to perform spatiotemporal
kriging for various spatiotemporal datasets without the need for model
specification. Specifically, we propose a novel spatial aggregation network
(SAN) inspired by Principal Neighborhood Aggregation, which uses multiple
aggregation functions to help one node gather diverse information from its
neighbors. To exclude information from unsampled nodes, a masking strategy that
prevents the unsampled sensors from sending messages to their neighborhood is
introduced to SAN. We capture temporal dependencies by the temporal
convolutional networks, which allows our model to cope with data of diverse
sizes. To make SATCN generalizable to unseen nodes and even unseen graph
structures, we employ an inductive strategy to train SATCN. We conduct
extensive experiments on three real-world spatiotemporal datasets, including
traffic speed and climate recordings. Our results demonstrate the superiority
of SATCN over traditional and GNN-based kriging models.

    

### [[2109.12151] AI Explainability 360: Impact and Design](http://arxiv.org/abs/2109.12151)


  As artificial intelligence and machine learning algorithms become
increasingly prevalent in society, multiple stakeholders are calling for these
algorithms to provide explanations. At the same time, these stakeholders,
whether they be affected citizens, government regulators, domain experts, or
system developers, have different explanation needs. To address these needs, in
2019, we created AI Explainability 360 (Arya et al. 2020), an open source
software toolkit featuring ten diverse and state-of-the-art explainability
methods and two evaluation metrics. This paper examines the impact of the
toolkit with several case studies, statistics, and community feedback. The
different ways in which users have experienced AI Explainability 360 have
resulted in multiple types of impact and improvements in multiple metrics,
highlighted by the adoption of the toolkit by the independent LF AI & Data
Foundation. The paper also describes the flexible design of the toolkit,
examples of its use, and the significant educational material and documentation
available to its users.

    

### [[2109.12162] POSSE: Patterns of Systems During Software Encryption](http://arxiv.org/abs/2109.12162)


  This research recasts ransomware detection using performance monitoring and
statistical machine learning. The work builds a test environment with 41 input
variables to label and compares three computing states: idle, encryption and
compression. A common goal of this behavioral detector seeks to anticipate and
short-circuit the final step of hard-drive locking with encryption and the
demand for payment to return the file system to its baseline. Comparing machine
learning techniques, linear regression outperforms random forest, decision
trees, and support vector machines (SVM). All algorithms classified the 3
possible classes (idle, encryption, and compression) with greater than 91%
accuracy.

    

### [[2109.12171] NICE: Robust Scheduling through Reinforcement Learning-Guided Integer Programming](http://arxiv.org/abs/2109.12171)


  Integer programs provide a powerful abstraction for representing a wide range
of real-world scheduling problems. Despite their ability to model general
scheduling problems, solving large-scale integer programs (IP) remains a
computational challenge in practice. The incorporation of more complex
objectives such as robustness to disruptions further exacerbates the
computational challenge. We present NICE (Neural network IP Coefficient
Extraction), a novel technique that combines reinforcement learning and integer
programming to tackle the problem of robust scheduling. More specifically, NICE
uses reinforcement learning to approximately represent complex objectives in an
integer programming formulation. We use NICE to determine assignments of pilots
to a flight crew schedule so as to reduce the impact of disruptions. We compare
NICE with (1) a baseline integer programming formulation that produces a
feasible crew schedule, and (2) a robust integer programming formulation that
explicitly tries to minimize the impact of disruptions. Our experiments show
that, across a variety of scenarios, NICE produces schedules resulting in 33\%
to 48\% fewer disruptions than the baseline formulation. Moreover, in more
severely constrained scheduling scenarios in which the robust integer program
fails to produce a schedule within 90 minutes, NICE is able to build robust
schedules in less than 2 seconds on average.

    

### [[2109.12174] Leveraging Pretrained Models for Automatic Summarization of Doctor-Patient Conversations](http://arxiv.org/abs/2109.12174)


  Fine-tuning pretrained models for automatically summarizing doctor-patient
conversation transcripts presents many challenges: limited training data,
significant domain shift, long and noisy transcripts, and high target summary
variability. In this paper, we explore the feasibility of using pretrained
transformer models for automatically summarizing doctor-patient conversations
directly from transcripts. We show that fluent and adequate summaries can be
generated with limited training data by fine-tuning BART on a specially
constructed dataset. The resulting models greatly surpass the performance of an
average human annotator and the quality of previous published work for the
task. We evaluate multiple methods for handling long conversations, comparing
them to the obvious baseline of truncating the conversation to fit the
pretrained model length limit. We introduce a multistage approach that tackles
the task by learning two fine-tuned models: one for summarizing conversation
chunks into partial summaries, followed by one for rewriting the collection of
partial summaries into a complete summary. Using a carefully chosen fine-tuning
dataset, this method is shown to be effective at handling longer conversations,
improving the quality of generated summaries. We conduct both an automatic
evaluation (through ROUGE and two concept-based metrics focusing on medical
findings) and a human evaluation (through qualitative examples from literature,
assessing hallucination, generalization, fluency, and general quality of the
generated summaries).

    

### [[2109.12176] On the Fairness of Swarm Learning in Skin Lesion Classification](http://arxiv.org/abs/2109.12176)


  in healthcare. However, the existing AI model may be biased in its decision
marking. The bias induced by data itself, such as collecting data in subgroups
only, can be mitigated by including more diversified data. Distributed and
collaborative learning is an approach to involve training models in massive,
heterogeneous, and distributed data sources, also known as nodes. In this work,
we target on examining the fairness issue in Swarm Learning (SL), a recent
edge-computing based decentralized machine learning approach, which is designed
for heterogeneous illnesses detection in precision medicine. SL has achieved
high performance in clinical applications, but no attempt has been made to
evaluate if SL can improve fairness. To address the problem, we present an
empirical study by comparing the fairness among single (node) training, SL,
centralized training. Specifically, we evaluate on large public available skin
lesion dataset, which contains samples from various subgroups. The experiments
demonstrate that SL does not exacerbate the fairness problem compared to
centralized training and improves both performance and fairness compared to
single training. However, there still exists biases in SL model and the
implementation of SL is more complex than the alternative two strategies.

    

### [[2109.12178] MLIM: Vision-and-Language Model Pre-training with Masked Language and Image Modeling](http://arxiv.org/abs/2109.12178)


  Vision-and-Language Pre-training (VLP) improves model performance for
downstream tasks that require image and text inputs. Current VLP approaches
differ on (i) model architecture (especially image embedders), (ii) loss
functions, and (iii) masking policies. Image embedders are either deep models
like ResNet or linear projections that directly feed image-pixels into the
transformer. Typically, in addition to the Masked Language Modeling (MLM) loss,
alignment-based objectives are used for cross-modality interaction, and RoI
feature regression and classification tasks for Masked Image-Region Modeling
(MIRM). Both alignment and MIRM objectives mostly do not have ground truth.
Alignment-based objectives require pairings of image and text and heuristic
objective functions. MIRM relies on object detectors. Masking policies either
do not take advantage of multi-modality or are strictly coupled with alignments
generated by other models. In this paper, we present Masked Language and Image
Modeling (MLIM) for VLP. MLIM uses two loss functions: Masked Language Modeling
(MLM) loss and image reconstruction (RECON) loss. We propose Modality Aware
Masking (MAM) to boost cross-modality interaction and take advantage of MLM and
RECON losses that separately capture text and image reconstruction quality.
Using MLM + RECON tasks coupled with MAM, we present a simplified VLP
methodology and show that it has better downstream task performance on a
proprietary e-commerce multi-modal dataset.

    

### [[2109.12191] NanoBatch DPSGD: Exploring Differentially Private learning on ImageNet with low batch sizes on the IPU](http://arxiv.org/abs/2109.12191)


  Differentially private SGD (DPSGD) has recently shown promise in deep
learning. However, compared to non-private SGD, the DPSGD algorithm places
computational overheads that can undo the benefit of batching in GPUs.
Microbatching is a standard method to alleviate this and is fully supported in
the TensorFlow Privacy library (TFDP). However, this technique, while improving
training times also reduces the quality of the gradients and degrades the
classification accuracy. Recent works that for example use the JAX framework
show promise in also alleviating this but still show degradation in throughput
from non-private to private SGD on CNNs, and have not yet shown ImageNet
implementations. In our work, we argue that low batch sizes using group
normalization on ResNet-50 can yield high accuracy and privacy on Graphcore
IPUs. This enables DPSGD training of ResNet-50 on ImageNet in just 6 hours (100
epochs) on an IPU-POD16 system.

    

### [[2109.12204] MIIDL: a Python package for microbial biomarkers identification powered by interpretable deep learning](http://arxiv.org/abs/2109.12204)


  Detecting microbial biomarkers used to predict disease phenotypes and
clinical outcomes is crucial for disease early-stage screening and diagnosis.
Most methods for biomarker identification are linear-based, which is very
limited as biological processes are rarely fully linear. The introduction of
machine learning to this field tends to bring a promising solution. However,
identifying microbial biomarkers in an interpretable, data-driven and robust
manner remains challenging. We present MIIDL, a Python package for the
identification of microbial biomarkers based on interpretable deep learning.
MIIDL innovatively applies convolutional neural networks, a variety of
interpretability algorithms and plenty of pre-processing methods to provide a
one-stop and robust pipeline for microbial biomarkers identification from
high-dimensional and sparse data sets.

    

### [[2109.12218] Long-Range Transformers for Dynamic Spatiotemporal Forecasting](http://arxiv.org/abs/2109.12218)


  Multivariate Time Series Forecasting (TSF) focuses on the prediction of
future values based on historical context. In these problems, dependent
variables provide additional information or early warning signs of changes in
future behavior. State-of-the-art forecasting models rely on neural attention
between timesteps. This allows for temporal learning but fails to consider
distinct spatial relationships between variables. This paper addresses the
problem by translating multivariate TSF into a novel spatiotemporal sequence
formulation where each input token represents the value of a single variable at
a given timestep. Long-Range Transformers can then learn interactions between
space, time, and value information jointly along this extended sequence. Our
method, which we call Spacetimeformer, scales to high dimensional forecasting
problems dominated by Graph Neural Networks that rely on predefined variable
graphs. We achieve competitive results on benchmarks from traffic forecasting
to electricity demand and weather prediction while learning spatial and
temporal relationships purely from data.

    

### [[2109.12219] Influence of Mobility Restrictions on Transmission of COVID-19 in the state of Maryland -- the USA](http://arxiv.org/abs/2109.12219)


  Background: The novel coronavirus, COVID-19, was first detected in the United
States in January 2020. To curb the spread of the disease in mid-March,
different states issued mandatory stay-at-home (SAH) orders. These
nonpharmaceutical interventions were mandated based on prior experiences, such
as the 1918 influenza epidemic. Hence, we decided to study the impact of
restrictions on mobility on reducing COVID-19 transmission. Methods: We
designed an ecological time series study with our exposure variable as Mobility
patterns in the state of Maryland for March- December 2020 and our outcome
variable as the COVID-19 hospitalizations for the same period. We built an
Extreme Gradient Boosting (XGBoost) ensemble machine learning model and
regressed the lagged COVID-19 hospitalizations with Mobility volume for
different regions of Maryland. Results: We found an 18% increase in COVID-19
hospitalizations when mobility was increased by a factor of five, similarly a
43% increase when mobility was further increased by a factor of ten.
Conclusion: The findings of our study demonstrated a positive linear
relationship between mobility and the incidence of COVID-19 cases. These
findings are partially consistent with other studies suggesting the benefits of
mobility restrictions. Although more detailed approach is needed to precisely
understand the benefits and limitations of mobility restrictions as part of a
response to the COVID-19 pandemic.

    

### [[2109.12222] Accelerated nonlinear primal-dual hybrid gradient algorithms with applications to machine learning](http://arxiv.org/abs/2109.12222)


  The primal-dual hybrid gradient (PDHG) algorithm is a first-order method that
splits convex optimization problems with saddle-point structure into smaller
subproblems. Those subproblems, unlike those obtained from most other splitting
methods, can generally be solved efficiently because they involve simple
operations such as matrix-vector multiplications or proximal mappings that are
easy to evaluate. In order to work fast, however, the PDHG algorithm requires
stepsize parameters fine-tuned for the problem at hand. Unfortunately, the
stepsize parameters must often be estimated from quantities that are
prohibitively expensive to compute for large-scale optimization problems, such
as those in machine learning. In this paper, we introduce accelerated nonlinear
variants of the PDHG algorithm that can achieve, for a broad class of
optimization problems relevant to machine learning, an optimal rate of
convergence with stepsize parameters that are simple to compute. We prove
rigorous convergence results, including for problems posed on
infinite-dimensional reflexive Banach spaces. We also provide practical
implementations of accelerated nonlinear PDHG algorithms for solving several
regression tasks in machine learning, including support vector machines without
offset, kernel ridge regression, elastic net regularized linear regression, and
the least absolute shrinkage selection operator.

    

### [[2109.12252] Long-Range Feature Propagating for Natural Image Matting](http://arxiv.org/abs/2109.12252)


  Natural image matting estimates the alpha values of unknown regions in the
trimap. Recently, deep learning based methods propagate the alpha values from
the known regions to unknown regions according to the similarity between them.
However, we find that more than 50\% pixels in the unknown regions cannot be
correlated to pixels in known regions due to the limitation of small effective
reception fields of common convolutional neural networks, which leads to
inaccurate estimation when the pixels in the unknown regions cannot be inferred
only with pixels in the reception fields. To solve this problem, we propose
Long-Range Feature Propagating Network (LFPNet), which learns the long-range
context features outside the reception fields for alpha matte estimation.
Specifically, we first design the propagating module which extracts the context
features from the downsampled image. Then, we present Center-Surround Pyramid
Pooling (CSPP) that explicitly propagates the context features from the
surrounding context image patch to the inner center image patch. Finally, we
use the matting module which takes the image, trimap and context features to
estimate the alpha matte. Experimental results demonstrate that the proposed
method performs favorably against the state-of-the-art methods on the
AlphaMatting and Adobe Image Matting datasets.

    

### [[2109.12257] Tensor Full Feature Measure and Its Nonconvex Relaxation Applications to Tensor Recovery](http://arxiv.org/abs/2109.12257)


  Tensor sparse modeling as a promising approach, in the whole of science and
engineering has been a huge success. As is known to all, various data in
practical application are often generated by multiple factors, so the use of
tensors to represent the data containing the internal structure of multiple
factors came into being. However, different from the matrix case, constructing
reasonable sparse measure of tensor is a relatively difficult and very
important task. Therefore, in this paper, we propose a new tensor sparsity
measure called Tensor Full Feature Measure (FFM). It can simultaneously
describe the feature information of each dimension of the tensor and the
related features between two dimensions, and connect the Tucker rank with the
tensor tube rank. This measurement method can describe the sparse features of
the tensor more comprehensively. On this basis, we establish its non-convex
relaxation, and apply FFM to low rank tensor completion (LRTC) and tensor
robust principal component analysis (TRPCA). LRTC and TRPCA models based on FFM
are proposed, and two efficient Alternating Direction Multiplier Method (ADMM)
algorithms are developed to solve the proposed model. A variety of real
numerical experiments substantiate the superiority of the proposed methods
beyond state-of-the-arts.

    

### [[2109.12261] An embarrassingly simple comparison of machine learning algorithms for indoor scene classification](http://arxiv.org/abs/2109.12261)


  With the emergence of autonomous indoor robots, the computer vision task of
indoor scene recognition has gained the spotlight. Indoor scene recognition is
a challenging problem in computer vision that relies on local and global
features in a scene. This study aims to compare the performance of five machine
learning algorithms on the task of indoor scene classification to identify the
pros and cons of each classifier. It also provides a comparison of low latency
feature extractors versus enormous feature extractors to understand the
performance effects. Finally, a simple MnasNet based indoor classification
system is proposed, which can achieve 72% accuracy at 23 ms latency.

    

### [[2109.12266] Learning Stereopsis from Geometric Synthesis for 6D Object Pose Estimation](http://arxiv.org/abs/2109.12266)


  Current monocular-based 6D object pose estimation methods generally achieve
less competitive results than RGBD-based methods, mostly due to the lack of 3D
information. To make up this gap, this paper proposes a 3D geometric volume
based pose estimation method with a short baseline two-view setting. By
constructing a geometric volume in the 3D space, we combine the features from
two adjacent images to the same 3D space. Then a network is trained to learn
the distribution of the position of object keypoints in the volume, and a
robust soft RANSAC solver is deployed to solve the pose in closed form. To
balance accuracy and cost, we propose a coarse-to-fine framework to improve the
performance in an iterative way. The experiments show that our method
outperforms state-of-the-art monocular-based methods, and is robust in
different objects and scenes, especially in serious occlusion situations.

    

### [[2109.12269] Integrating Recurrent Neural Networks with Data Assimilation for Scalable Data-Driven State Estimation](http://arxiv.org/abs/2109.12269)


  Data assimilation (DA) is integrated with machine learning in order to
perform entirely data-driven online state estimation. To achieve this,
recurrent neural networks (RNNs) are implemented as surrogate models to replace
key components of the DA cycle in numerical weather prediction (NWP), including
the conventional numerical forecast model, the forecast error covariance
matrix, and the tangent linear and adjoint models. It is shown how these RNNs
can be initialized using DA methods to directly update the hidden/reservoir
state with observations of the target system. The results indicate that these
techniques can be applied to estimate the state of a system for the repeated
initialization of short-term forecasts, even in the absence of a traditional
numerical forecast model. Further, it is demonstrated how these integrated
RNN-DA methods can scale to higher dimensions by applying domain localization
and parallelization, providing a path for practical applications in NWP.

    

### [[2109.12271] BiTr-Unet: a CNN-Transformer Combined Network for MRI Brain Tumor Segmentation](http://arxiv.org/abs/2109.12271)


  Convolutional neural networks (CNNs) have recently achieved remarkable
success in automatically identifying organs or lesions on 3D medical images.
Meanwhile, vision transformer networks have exhibited exceptional performance
in 2D image classification tasks. Compared with CNNs, transformer networks have
an obvious advantage of extracting long-range features due to their
self-attention algorithm. Therefore, in this paper we present a CNN-Transformer
combined model called BiTr-Unet for brain tumor segmentation on multi-modal MRI
scans. The proposed BiTr-Unet achieves good performance on the BraTS 2021
validation dataset with mean Dice score 0.9076, 0.8392 and 0.8231, and mean
Hausdorff distance 4.5322, 13.4592 and 14.9963 for the whole tumor, tumor core,
and enhancing tumor, respectively.

    

### [[2109.12273] FedProc: Prototypical Contrastive Federated Learning on Non-IID data](http://arxiv.org/abs/2109.12273)


  Federated learning allows multiple clients to collaborate to train
high-performance deep learning models while keeping the training data locally.
However, when the local data of all clients are not independent and identically
distributed (i.e., non-IID), it is challenging to implement this form of
efficient collaborative learning. Although significant efforts have been
dedicated to addressing this challenge, the effect on the image classification
task is still not satisfactory. In this paper, we propose FedProc: prototypical
contrastive federated learning, which is a simple and effective federated
learning framework. The key idea is to utilize the prototypes as global
knowledge to correct the local training of each client. We design a local
network architecture and a global prototypical contrastive loss to regulate the
training of local models, which makes local objectives consistent with the
global optima. Eventually, the converged global model obtains a good
performance on non-IID data. Experimental results show that, compared to
state-of-the-art federated learning methods, FedProc improves the accuracy by
$1.6\%\sim7.9\%$ with acceptable computation cost.

    

### [[2109.12276] Cardiac Complication Risk Profiling for Cancer Survivors via Multi-View Multi-Task Learning](http://arxiv.org/abs/2109.12276)


  Complication risk profiling is a key challenge in the healthcare domain due
to the complex interaction between heterogeneous entities (e.g., visit,
disease, medication) in clinical data. With the availability of real-world
clinical data such as electronic health records and insurance claims, many deep
learning methods are proposed for complication risk profiling. However, these
existing methods face two open challenges. First, data heterogeneity relates to
those methods leveraging clinical data from a single view only while the data
can be considered from multiple views (e.g., sequence of clinical visits, set
of clinical features). Second, generalized prediction relates to most of those
methods focusing on single-task learning, whereas each complication onset is
predicted independently, leading to suboptimal models. We propose a multi-view
multi-task network (MuViTaNet) for predicting the onset of multiple
complications to tackle these issues. In particular, MuViTaNet complements
patient representation by using a multi-view encoder to effectively extract
information by considering clinical data as both sequences of clinical visits
and sets of clinical features. In addition, it leverages additional information
from both related labeled and unlabeled datasets to generate more generalized
representations by using a new multi-task learning scheme for making more
accurate predictions. The experimental results show that MuViTaNet outperforms
existing methods for profiling the development of cardiac complications in
breast cancer survivors. Furthermore, thanks to its multi-view multi-task
architecture, MuViTaNet also provides an effective mechanism for interpreting
its predictions in multiple perspectives, thereby helping clinicians discover
the underlying mechanism triggering the onset and for making better clinical
treatments in real-world scenarios.

    

### [[2109.12283] Scalable deeper graph neural networks for high-performance materials property prediction](http://arxiv.org/abs/2109.12283)


  Machine learning (ML) based materials discovery has emerged as one of the
most promising approaches for breakthroughs in materials science. While
heuristic knowledge based descriptors have been combined with ML algorithms to
achieve good performance, the complexity of the physicochemical mechanisms
makes it urgently needed to exploit representation learning from either
compositions or structures for building highly effective materials machine
learning models. Among these methods, the graph neural networks have shown the
best performance by its capability to learn high-level features from crystal
structures. However, all these models suffer from their inability to scale up
the models due to the over-smoothing issue of their message-passing GNN
architecture. Here we propose a novel graph attention neural network model
DeeperGATGNN with differentiable group normalization and skip-connections,
which allows to train very deep graph neural network models (e.g. 30 layers
compared to 3-9 layers in previous works). Through systematic benchmark studies
over six benchmark datasets for energy and band gap predictions, we show that
our scalable DeeperGATGNN model needs little costly hyper-parameter tuning for
different datasets and achieves the state-of-the-art prediction performances
over five properties out of six with up to 10\% improvement. Our work shows
that to deal with the high complexity of mapping the crystal materials
structures to their properties, large-scale very deep graph neural networks are
needed to achieve robust performances.

    

### [[2109.12286] Stackelberg Actor-Critic: Game-Theoretic Reinforcement Learning Algorithms](http://arxiv.org/abs/2109.12286)


  The hierarchical interaction between the actor and critic in actor-critic
based reinforcement learning algorithms naturally lends itself to a
game-theoretic interpretation. We adopt this viewpoint and model the actor and
critic interaction as a two-player general-sum game with a leader-follower
structure known as a Stackelberg game. Given this abstraction, we propose a
meta-framework for Stackelberg actor-critic algorithms where the leader player
follows the total derivative of its objective instead of the usual individual
gradient. From a theoretical standpoint, we develop a policy gradient theorem
for the refined update and provide a local convergence guarantee for the
Stackelberg actor-critic algorithms to a local Stackelberg equilibrium. From an
empirical standpoint, we demonstrate via simple examples that the learning
dynamics we study mitigate cycling and accelerate convergence compared to the
usual gradient dynamics given cost structures induced by actor-critic
formulations. Finally, extensive experiments on OpenAI gym environments show
that Stackelberg actor-critic algorithms always perform at least as well and
often significantly outperform the standard actor-critic algorithm
counterparts.

    

### [[2109.12298] Opacus: User-Friendly Differential Privacy Library in PyTorch](http://arxiv.org/abs/2109.12298)


  We introduce Opacus, a free, open-source PyTorch library for training deep
learning models with differential privacy (hosted at this http URL). Opacus is
designed for simplicity, flexibility, and speed. It provides a simple and
user-friendly API, and enables machine learning practitioners to make a
training pipeline private by adding as little as two lines to their code. It
supports a wide variety of layers, including multi-head attention, convolution,
LSTM, and embedding, right out of the box, and it also provides the means for
supporting other user-defined layers. Opacus computes batched per-sample
gradients, providing better efficiency compared to the traditional "micro
batch" approach. In this paper we present Opacus, detail the principles that
drove its implementation and unique features, and compare its performance
against other frameworks for differential privacy in ML.

    

### [[2109.12306] Topic Model Robustness to Automatic Speech Recognition Errors in Podcast Transcripts](http://arxiv.org/abs/2109.12306)


  For a multilingual podcast streaming service, it is critical to be able to
deliver relevant content to all users independent of language. Podcast content
relevance is conventionally determined using various metadata sources. However,
with the increasing quality of speech recognition in many languages, utilizing
automatic transcriptions to provide better content recommendations becomes
possible. In this work, we explore the robustness of a Latent Dirichlet
Allocation topic model when applied to transcripts created by an automatic
speech recognition engine. Specifically, we explore how increasing
transcription noise influences topics obtained from transcriptions in Danish; a
low resource language. First, we observe a baseline of cosine similarity scores
between topic embeddings from automatic transcriptions and the descriptions of
the podcasts written by the podcast creators. We then observe how the cosine
similarities decrease as transcription noise increases and conclude that even
when automatic speech recognition transcripts are erroneous, it is still
possible to obtain high-quality topic embeddings from the transcriptions.

    

### [[2109.12311] Constructing Sub-scale Surrogate Model for Proppant Settling in Inclined Fractures from Simulation Data with Multi-fidelity Neural Network](http://arxiv.org/abs/2109.12311)


  Particle settling in inclined channels is an important phenomenon that occurs
during hydraulic fracturing of shale gas production. Generally, in order to
accurately simulate the large-scale (field-scale) proppant transport process,
constructing a fast and accurate sub-scale proppant settling model, or
surrogate model, becomes a critical issue, since mapping between physical
parameters and proppant settling velocity is complex. Previously, particle
settling has usually been investigated via high-fidelity experiments and
meso-scale numerical simulations, both of which are time-consuming. In this
work, a new method is proposed and utilized, i.e., the multi-fidelity neural
network (MFNN), to construct a settling surrogate model, which could utilize
both high-fidelity and low-fidelity (thus, less expensive) data. The results
demonstrate that constructing the settling surrogate with the MFNN can reduce
the need for high-fidelity data and thus computational cost by 80%, while the
accuracy lost is less than 5% compared to a high-fidelity surrogate. Moreover,
the investigated particle settling surrogate is applied in macro-scale proppant
transport simulation, which shows that the settling model is significant to
proppant transport and yields accurate results. This opens novel pathways for
rapidly predicting proppant settling velocity in reservoir applications.

    

### [[2109.12321] Under the Skin of Foundation NFT Auctions](http://arxiv.org/abs/2109.12321)


  Non Fungible Tokens (NFTs) have gained a solid foothold within the crypto
community, and substantial amounts of money have been allocated to their
trades. In this paper, we studied one of the most prominent marketplaces
dedicated to NFT auctions and trades, Foundation. We analyzed the activities on
Foundation and identified several intriguing underlying dynamics that occur on
this platform. Moreover, We performed social network analysis on a graph that
we had created based on transferred NFTs on Foundation, and then described the
characteristics of this graph. Lastly, We built a neural network-based
similarity model for retrieving and clustering similar NFTs. We also showed
that for most NFTs, their performances in auctions were comparable with the
auction performance of other NFTs in their cluster.

    

### [[2109.12323] Deep Learning-Based Detection of the Acute Respiratory Distress Syndrome: What Are the Models Learning?](http://arxiv.org/abs/2109.12323)


  The acute respiratory distress syndrome (ARDS) is a severe form of hypoxemic
respiratory failure with in-hospital mortality of 35-46%. High mortality is
thought to be related in part to challenges in making a prompt diagnosis, which
may in turn delay implementation of evidence-based therapies. A deep neural
network (DNN) algorithm utilizing unbiased ventilator waveform data (VWD) may
help to improve screening for ARDS. We first show that a convolutional neural
network-based ARDS detection model can outperform prior work with random forest
models in AUC (0.95+/-0.019 vs. 0.88+/-0.064), accuracy (0.84+/-0.026 vs
0.80+/-0.078), and specificity (0.81+/-0.06 vs 0.71+/-0.089). Frequency
ablation studies imply that our model can learn features from low frequency
domains typically used for expert feature engineering, and high-frequency
information that may be difficult to manually featurize. Further experiments
suggest that subtle, high-frequency components of physiologic signals may
explain the superior performance of DL models over traditional ML when using
physiologic waveform data. Our observations may enable improved
interpretability of DL-based physiologic models and may improve the
understanding of how high-frequency information in physiologic data impacts the
performance our DL model.

    

### [[2109.12340] Distributed Online Optimization with Byzantine Adversarial Agents](http://arxiv.org/abs/2109.12340)


  We study the problem of non-constrained, discrete-time, online distributed
optimization in a multi-agent system where some of the agents do not follow the
prescribed update rule either due to failures or malicious intentions. None of
the agents have prior information about the identities of the faulty agents and
any agent can communicate only with its immediate neighbours. At each time
step, a Lipschitz strongly convex cost function is revealed locally to all the
agents and the non-faulty agents update their states using their local
information and the information obtained from their neighbours. We measure the
performance of the online algorithm by comparing it to its offline version when
the cost functions are known apriori. The difference between the same is termed
as regret. Under sufficient conditions on the graph topology, the number and
location of the adversaries, the defined regret grows sublinearly. We further
conduct numerical experiments to validate our theoretical results.

    

### [[2109.12343] Beyond Robustness: A Taxonomy of Approaches towards Resilient Multi-Robot Systems](http://arxiv.org/abs/2109.12343)


  Robustness is key to engineering, automation, and science as a whole.
However, the property of robustness is often underpinned by costly requirements
such as over-provisioning, known uncertainty and predictive models, and known
adversaries. These conditions are idealistic, and often not satisfiable.
Resilience on the other hand is the capability to endure unexpected
disruptions, to recover swiftly from negative events, and bounce back to
normality. In this survey article, we analyze how resilience is achieved in
networks of agents and multi-robot systems that are able to overcome adversity
by leveraging system-wide complementarity, diversity, and redundancy - often
involving a reconfiguration of robotic capabilities to provide some key ability
that was not present in the system a priori. As society increasingly depends on
connected automated systems to provide key infrastructure services (e.g.,
logistics, transport, and precision agriculture), providing the means to
achieving resilient multi-robot systems is paramount. By enumerating the
consequences of a system that is not resilient (fragile), we argue that
resilience must become a central engineering design consideration. Towards this
goal, the community needs to gain clarity on how it is defined, measured, and
maintained. We address these questions across foundational robotics domains,
spanning perception, control, planning, and learning. One of our key
contributions is a formal taxonomy of approaches, which also helps us discuss
the defining factors and stressors for a resilient system. Finally, this survey
article gives insight as to how resilience may be achieved. Importantly, we
highlight open problems that remain to be tackled in order to reap the benefits
of resilient robotic systems.

    

### [[2109.12346] DziriBERT: a Pre-trained Language Model for the Algerian Dialect](http://arxiv.org/abs/2109.12346)


  Pre-trained transformers are now the de facto models in Natural Language
Processing given their state-of-the-art results in many tasks and languages.
However, most of the current models have been trained on languages for which
large text resources are already available (such as English, French, Arabic,
etc.). Therefore, there is still a number of low-resource languages that need
more attention from the community. In this paper, we study the Algerian dialect
which has several specificities that make the use of Arabic or multilingual
models inappropriate. To address this issue, we collected more than one Million
Algerian tweets, and pre-trained the first Algerian language model: DziriBERT.
When compared to existing models, DziriBERT achieves the best results on two
Algerian downstream datasets. The obtained results show that pre-training a
dedicated model on a small dataset (150 MB) can outperform existing models that
have been trained on much more data (hundreds of GB). Finally, our model is
publicly available to the community.

    

### [[2109.12347] A Principled Approach to Failure Analysis and Model Repairment: Demonstration in Medical Imaging](http://arxiv.org/abs/2109.12347)


  Machine learning models commonly exhibit unexpected failures post-deployment
due to either data shifts or uncommon situations in the training environment.
Domain experts typically go through the tedious process of inspecting the
failure cases manually, identifying failure modes and then attempting to fix
the model. In this work, we aim to standardise and bring principles to this
process through answering two critical questions: (i) how do we know that we
have identified meaningful and distinct failure types?; (ii) how can we
validate that a model has, indeed, been repaired? We suggest that the quality
of the identified failure types can be validated through measuring the intra-
and inter-type generalisation after fine-tuning and introduce metrics to
compare different subtyping methods. Furthermore, we argue that a model can be
considered repaired if it achieves high accuracy on the failure types while
retaining performance on the previously correct data. We combine these two
ideas into a principled framework for evaluating the quality of both the
identified failure subtypes and model repairment. We evaluate its utility on a
classification and an object detection tasks. Our code is available at
this https URL


### [[2109.12379] TEMGNet: Deep Transformer-based Decoding of Upperlimb sEMG for Hand Gestures Recognition](http://arxiv.org/abs/2109.12379)


  There has been a surge of recent interest in Machine Learning (ML),
particularly Deep Neural Network (DNN)-based models, to decode muscle
activities from surface Electromyography (sEMG) signals for myoelectric control
of neurorobotic systems. DNN-based models, however, require large training sets
and, typically, have high structural complexity, i.e., they depend on a large
number of trainable parameters. To address these issues, we developed a
framework based on the Transformer architecture for processing sEMG signals. We
propose a novel Vision Transformer (ViT)-based neural network architecture
(referred to as the TEMGNet) to classify and recognize upperlimb hand gestures
from sEMG to be used for myocontrol of prostheses. The proposed TEMGNet
architecture is trained with a small dataset without the need for pre-training
or fine-tuning. To evaluate the efficacy, following the-recent literature, the
second subset (exercise B) of the NinaPro DB2 dataset was utilized, where the
proposed TEMGNet framework achieved a recognition accuracy of 82.93% and 82.05%
for window sizes of 300ms and 200ms, respectively, outperforming its
state-of-the-art counterparts. Moreover, the proposed TEMGNet framework is
superior in terms of structural capacity while having seven times fewer
trainable parameters. These characteristics and the high performance make
DNN-based models promising approaches for myoelectric control of neurorobots.

    

### [[2109.12390] Model reduction for the material point method via learning the deformation map and its spatial-temporal gradients](http://arxiv.org/abs/2109.12390)


  This work proposes a model-reduction approach for the material point method
on nonlinear manifolds. The technique approximates the $\textit{kinematics}$ by
approximating the deformation map in a manner that restricts deformation
trajectories to reside on a low-dimensional manifold expressed from the
extrinsic view via a parameterization function. By explicitly approximating the
deformation map and its spatial-temporal gradients, the deformation gradient
and the velocity can be computed simply by differentiating the associated
parameterization function. Unlike classical model reduction techniques that
build a subspace for a finite number of degrees of freedom, the proposed method
approximates the entire deformation map with infinite degrees of freedom.
Therefore, the technique supports resolution changes in the reduced simulation,
attaining the challenging task of zero-shot super-resolution by generating
material points unseen in the training data. The ability to generate material
points also allows for adaptive quadrature rules for stress update. A family of
projection methods is devised to generate $\textit{dynamics}$, i.e., at every
time step, the methods perform three steps: (1) generate quadratures in the
full space from the reduced space, (2) compute position and velocity updates in
the full space, and (3) perform a least-squares projection of the updated
position and velocity onto the low-dimensional manifold and its tangent space.
Computational speedup is achieved via hyper-reduction, i.e., only a subset of
the original material points are needed for dynamics update. Large-scale
numerical examples with millions of material points illustrate the method's
ability to gain an order-of-magnitude computational-cost saving -- indeed
$\textit{real-time simulations}$ in some cases -- with negligible errors.

    

### [[2109.12391] Multi-source Few-shot Domain Adaptation](http://arxiv.org/abs/2109.12391)


  Multi-source Domain Adaptation (MDA) aims to transfer predictive models from
multiple, fully-labeled source domains to an unlabeled target domain. However,
in many applications, relevant labeled source datasets may not be available,
and collecting source labels can be as expensive as labeling the target data
itself. In this paper, we investigate Multi-source Few-shot Domain Adaptation
(MFDA): a new domain adaptation scenario with limited multi-source labels and
unlabeled target data. As we show, existing methods often fail to learn
discriminative features for both source and target domains in the MFDA setting.
Therefore, we propose a novel framework, termed Multi-Source Few-shot
Adaptation Network (MSFAN), which can be trained end-to-end in a
non-adversarial manner. MSFAN operates by first using a type of prototypical,
multi-domain, self-supervised learning to learn features that are not only
domain-invariant but also class-discriminative. Second, MSFAN uses a small,
labeled support set to enforce feature consistency and domain invariance across
domains. Finally, prototypes from multiple sources are leveraged to learn
better classifiers. Compared with state-of-the-art MDA methods, MSFAN improves
the mean classification accuracy over different domain pairs on MFDA by 20.2%,
9.4%, and 16.2% on Office, Office-Home, and DomainNet, respectively.

    

### [[2109.12398] Channel State Information Based Localization with Deep Learning](http://arxiv.org/abs/2109.12398)


  Localization is one of the most important problems in various fields such as
robotics and wireless communications. For instance, Unmanned Aerial Vehicles
(UAVs) require the information of the position precisely for an adequate
control strategy. This problem is handled very efficiently with integrated GPS
units for outdoor applications. However, indoor applications require special
treatment due to the unavailability of GPS signals. Another aspect of mobile
robots such as UAVs is that there is constant wireless communication between
the mobile robot and a computational unit. This communication is mainly done
for obtaining telemetry information or computation of control actions directly.
The responsible integrated units for this transmission are commercial wireless
communication chipsets. These units on the receiver side are responsible for
getting rid of the diverse effects of the communication channel with various
mathematical techniques. These techniques mainly require the Channel State
Information (CSI) of the current channel to compensate the channel itself.
After the compensation, the chipset has nothing to do with CSI. However, the
locations of both the transmitter and receiver have a direct impact on CSI.
Even though CSI contains such rich information about the environment, the
accessibility of these data is blocked by the commercial wireless chipsets
since they are manufactured to provide only the processed information data bits
to the user. However, with the IEEE 802.11n standardization, certain chipsets
provide access to CSI. Therefore, CSI data became processible and integrable to
localization schemes. In this project, a test environment was constructed for
the localization task. Two routers with proper chipsets were assigned as
transmitter and receiver. They were operationalized for the CSI data
collection. Lastly, these data were processed with various deep learning
models.

    

### [[2109.12400] Communication-Efficient Distributed Linear and Deep Generalized Canonical Correlation Analysis](http://arxiv.org/abs/2109.12400)


  Classic and deep learning-based generalized canonical correlation analysis
(GCCA) algorithms seek low-dimensional common representations of data entities
from multiple ``views'' (e.g., audio and image) using linear transformations
and neural networks, respectively. When the views are acquired and stored at
different locations, organizations and edge devices, computing GCCA in a
distributed, parallel and efficient manner is well-motivated. However, existing
distributed GCCA algorithms may incur prohitively high communication overhead.
This work puts forth a communication-efficient distributed framework for both
linear and deep GCCA under the maximum variance (MAX-VAR) paradigm. The
overhead issue is addressed by aggressively compressing (via quantization) the
exchanging information between the distributed computing agents and a central
controller. Compared to the unquantized version, the proposed algorithm
consistently reduces the communication overhead by about $90\%$ with virtually
no loss in accuracy and convergence speed. Rigorous convergence analyses are
also presented -- which is a nontrivial effort since no existing generic result
from quantized distributed optimization covers the special problem structure of
GCCA. Our result shows that the proposed algorithms for both linear and deep
GCCA converge to critical points in a sublinear rate, even under heavy
quantization and stochastic approximations. In addition, it is shown that in
the linear MAX-VAR case, the quantized algorithm approaches a {\it global
optimum} in a {\it geometric} rate -- if the computing agents' updates meet a
certain accuracy level. Synthetic and real data experiments are used to
showcase the effectiveness of the proposed approach.

    

### [[2109.12406] MINIMAL: Mining Models for Data Free Universal Adversarial Triggers](http://arxiv.org/abs/2109.12406)


  It is well known that natural language models are vulnerable to adversarial
attacks, which are mostly input-specific in nature. Recently, it has been shown
that there also exist input-agnostic attacks in NLP models, called universal
adversarial triggers. However, existing methods to craft universal triggers are
data intensive. They require large amounts of data samples to generate
adversarial triggers, which are typically inaccessible by attackers. For
instance, previous works take 3000 data samples per class for the SNLI dataset
to generate adversarial triggers. In this paper, we present a novel data-free
approach, MINIMAL, to mine input-agnostic adversarial triggers from models.
Using the triggers produced with our data-free algorithm, we reduce the
accuracy of Stanford Sentiment Treebank's positive class from 93.6% to 9.6%.
Similarly, for the Stanford Natural Language Inference (SNLI), our single-word
trigger reduces the accuracy of the entailment class from 90.95% to less than
0.6\%. Despite being completely data-free, we get equivalent accuracy drops as
data-dependent methods.

    

### [[2109.12421] Integrating Unsupervised Clustering and Label-specific Oversampling to Tackle Imbalanced Multi-label Data](http://arxiv.org/abs/2109.12421)


  There is often a mixture of very frequent labels and very infrequent labels
in multi-label datatsets. This variation in label frequency, a type class
imbalance, creates a significant challenge for building efficient multi-label
classification algorithms. In this paper, we tackle this problem by proposing a
minority class oversampling scheme, UCLSO, which integrates Unsupervised
Clustering and Label-Specific data Oversampling. Clustering is performed to
find out the key distinct and locally connected regions of a multi-label
dataset (irrespective of the label information). Next, for each label, we
explore the distributions of minority points in the cluster sets. Only the
minority points within a cluster are used to generate the synthetic minority
points that are used for oversampling. Even though the cluster set is the same
across all labels, the distributions of the synthetic minority points will vary
across the labels. The training dataset is augmented with the set of
label-specific synthetic minority points, and classifiers are trained to
predict the relevance of each label independently. Experiments using 12
multi-label datasets and several multi-label algorithms show that the proposed
method performed very well compared to the other competing algorithms.

    

### [[2109.12422] Equality of opportunity in travel behavior prediction with deep neural networks and discrete choice models](http://arxiv.org/abs/2109.12422)


  Although researchers increasingly adopt machine learning to model travel
behavior, they predominantly focus on prediction accuracy, ignoring the ethical
challenges embedded in machine learning algorithms. This study introduces an
important missing dimension - computational fairness - to travel behavior
analysis. We first operationalize computational fairness by equality of
opportunity, then differentiate between the bias inherent in data and the bias
introduced by modeling. We then demonstrate the prediction disparities in
travel behavior modeling using the 2017 National Household Travel Survey (NHTS)
and the 2018-2019 My Daily Travel Survey in Chicago. Empirically, deep neural
network (DNN) and discrete choice models (DCM) reveal consistent prediction
disparities across multiple social groups: both over-predict the false negative
rate of frequent driving for the ethnic minorities, the low-income and the
disabled populations, and falsely predict a higher travel burden of the
socially disadvantaged groups and the rural populations than reality. Comparing
DNN with DCM, we find that DNN can outperform DCM in prediction disparities
because of DNN's smaller misspecification error. To mitigate prediction
disparities, this study introduces an absolute correlation regularization
method, which is evaluated with synthetic and real-world data. The results
demonstrate the prevalence of prediction disparities in travel behavior
modeling, and the disparities still persist regarding a variety of model
specifics such as the number of DNN layers, batch size and weight
initialization. Since these prediction disparities can exacerbate social
inequity if prediction results without fairness adjustment are used for
transportation policy making, we advocate for careful consideration of the
fairness problem in travel behavior modeling, and the use of bias mitigation
algorithms for fair transport decisions.

    

### [[2109.12423] Random Walk-steered Majority Undersampling](http://arxiv.org/abs/2109.12423)


  In this work, we propose Random Walk-steered Majority Undersampling (RWMaU),
which undersamples the majority points of a class imbalanced dataset, in order
to balance the classes. Rather than marking the majority points which belong to
the neighborhood of a few minority points, we are interested to perceive the
closeness of the majority points to the minority class. Random walk, a powerful
tool for perceiving the proximities of connected points in a graph, is used to
identify the majority points which lie close to the minority class of a
class-imbalanced dataset. The visit frequencies and the order of visits of the
majority points in the walks enable us to perceive an overall closeness of the
majority points to the minority class. The ones lying close to the minority
class are subsequently undersampled. Empirical evaluation on 21 datasets and 3
classifiers demonstrate substantial improvement in performance of RWMaU over
the competing methods.

    

### [[2109.12424] Coreference Resolution for the Biomedical Domain: A Survey](http://arxiv.org/abs/2109.12424)


  Issues with coreference resolution are one of the most frequently mentioned
challenges for information extraction from the biomedical literature. Thus, the
biomedical genre has long been the second most researched genre for coreference
resolution after the news domain, and the subject of a great deal of research
for NLP in general. In recent years this interest has grown enormously leading
to the development of a number of substantial datasets, of domain-specific
contextual language models, and of several architectures. In this paper we
review the state-of-the-art of coreference in the biomedical domain with a
particular attention on these most recent developments.

    

### [[2109.12425] L$^{2}$NAS: Learning to Optimize Neural Architectures via Continuous-Action Reinforcement Learning](http://arxiv.org/abs/2109.12425)


  Neural architecture search (NAS) has achieved remarkable results in deep
neural network design. Differentiable architecture search converts the search
over discrete architectures into a hyperparameter optimization problem which
can be solved by gradient descent. However, questions have been raised
regarding the effectiveness and generalizability of gradient methods for
solving non-convex architecture hyperparameter optimization problems. In this
paper, we propose L$^{2}$NAS, which learns to intelligently optimize and update
architecture hyperparameters via an actor neural network based on the
distribution of high-performing architectures in the search history. We
introduce a quantile-driven training procedure which efficiently trains
L$^{2}$NAS in an actor-critic framework via continuous-action reinforcement
learning. Experiments show that L$^{2}$NAS achieves state-of-the-art results on
NAS-Bench-201 benchmark as well as DARTS search space and Once-for-All
MobileNetV3 search space. We also show that search policies generated by
L$^{2}$NAS are generalizable and transferable across different training
datasets with minimal fine-tuning.

    

### [[2109.12426] Profiling Neural Blocks and Design Spaces for Mobile Neural Architecture Search](http://arxiv.org/abs/2109.12426)


  Neural architecture search automates neural network design and has achieved
state-of-the-art results in many deep learning applications. While recent
literature has focused on designing networks to maximize accuracy, little work
has been conducted to understand the compatibility of architecture design
spaces to varying hardware. In this paper, we analyze the neural blocks used to
build Once-for-All (MobileNetV3), ProxylessNAS and ResNet families, in order to
understand their predictive power and inference latency on various devices,
including Huawei Kirin 9000 NPU, RTX 2080 Ti, AMD Threadripper 2990WX, and
Samsung Note10. We introduce a methodology to quantify the friendliness of
neural blocks to hardware and the impact of their placement in a macro network
on overall network performance via only end-to-end measurements. Based on
extensive profiling results, we derive design insights and apply them to
hardware-specific search space reduction. We show that searching in the reduced
search space generates better accuracy-latency Pareto frontiers than searching
in the original search spaces, customizing architecture search according to the
hardware. Moreover, insights derived from measurements lead to notably higher
ImageNet top-1 scores on all search spaces investigated.

    

### [[2109.12434] Emergent behavior and neural dynamics in artificial agents tracking turbulent plumes](http://arxiv.org/abs/2109.12434)


  Tracking a turbulent plume to locate its source is a complex control problem
because it requires multi-sensory integration and must be robust to
intermittent odors, changing wind direction, and variable plume statistics.
This task is routinely performed by flying insects, often over long distances,
in pursuit of food or mates. Several aspects of this remarkable behavior have
been studied in detail in many experimental studies. Here, we take a
complementary in silico approach, using artificial agents trained with
reinforcement learning to develop an integrated understanding of the behaviors
and neural computations that support plume tracking. Specifically, we use deep
reinforcement learning (DRL) to train recurrent neural network (RNN) agents to
locate the source of simulated turbulent plumes. Interestingly, the agents'
emergent behaviors resemble those of flying insects, and the RNNs learn to
represent task-relevant variables, such as head direction and time since last
odor encounter. Our analyses suggest an intriguing experimentally testable
hypothesis for tracking plumes in changing wind direction -- that agents follow
local plume shape rather than the current wind direction. While reflexive
short-memory behaviors are sufficient for tracking plumes in constant wind,
longer timescales of memory are essential for tracking plumes that switch
direction. At the level of neural dynamics, the RNNs' population activity is
low-dimensional and organized into distinct dynamical structures, with some
correspondence to behavioral modules. Our in silico approach provides key
intuitions for turbulent plume tracking strategies and motivates future
targeted experimental and theoretical developments.

    

### [[2109.12440] Smart Home Energy Management: Sequence-to-Sequence Load Forecasting and Q-Learning](http://arxiv.org/abs/2109.12440)


  A smart home energy management system (HEMS) can contribute towards reducing
the energy costs of customers; however, HEMS suffers from uncertainty in both
energy generation and consumption patterns. In this paper, we propose a
sequence to sequence (Seq2Seq) learning-based supply and load prediction along
with reinforcement learning-based HEMS control. We investigate how the
prediction method affects the HEMS operation. First, we use Seq2Seq learning to
predict photovoltaic (PV) power and home devices' load. We then apply
Q-learning for offline optimization of HEMS based on the prediction results.
Finally, we test the online performance of the trained Q-learning scheme with
actual PV and load data. The Seq2Seq learning is compared with VARMA, SVR, and
LSTM in both prediction and operation levels. The simulation results show that
Seq2Seq performs better with a lower prediction error and online operation
performance.

    

### [[2109.12448] ReCal-Net: Joint Region-Channel-Wise Calibrated Network for Semantic Segmentation in Cataract Surgery Videos](http://arxiv.org/abs/2109.12448)


  Semantic segmentation in surgical videos is a prerequisite for a broad range
of applications towards improving surgical outcomes and surgical video
analysis. However, semantic segmentation in surgical videos involves many
challenges. In particular, in cataract surgery, various features of the
relevant objects such as blunt edges, color and context variation, reflection,
transparency, and motion blur pose a challenge for semantic segmentation. In
this paper, we propose a novel convolutional module termed as \textit{ReCal}
module, which can calibrate the feature maps by employing region
intra-and-inter-dependencies and channel-region cross-dependencies. This
calibration strategy can effectively enhance semantic representation by
correlating different representations of the same semantic label, considering a
multi-angle local view centering around each pixel. Thus the proposed module
can deal with distant visual characteristics of unique objects as well as
cross-similarities in the visual characteristics of different objects.
Moreover, we propose a novel network architecture based on the proposed module
termed as ReCal-Net. Experimental results confirm the superiority of ReCal-Net
compared to rival state-of-the-art approaches for all relevant objects in
cataract surgery. Moreover, ablation studies reveal the effectiveness of the
ReCal module in boosting semantic segmentation accuracy.

    

### [[2109.12449] AbstractDifferentiation.jl: Backend-Agnostic Differentiable Programming in Julia](http://arxiv.org/abs/2109.12449)


  No single Automatic Differentiation (AD) system is the optimal choice for all
problems. This means informed selection of an AD system and combinations can be
a problem-specific variable that can greatly impact performance. In the Julia
programming language, the major AD systems target the same input and thus in
theory can compose. Hitherto, switching between AD packages in the Julia
Language required end-users to familiarize themselves with the user-facing API
of the respective packages. Furthermore, implementing a new, usable AD package
required AD package developers to write boilerplate code to define convenience
API functions for end-users. As a response to these issues, we present
AbstractDifferentiation.jl for the automatized generation of an extensive,
unified, user-facing API for any AD package. By splitting the complexity
between AD users and AD developers, AD package developers only need to
implement one or two primitive definitions to support various utilities for AD
users like Jacobians, Hessians and lazy product operators from native
primitives such as pullbacks or pushforwards, thus removing tedious -- but so
far inevitable -- boilerplate code, and enabling the easy switching and
composing between AD implementations for end-users.

    

### [[2109.12453] Classification of COVID-19 from CXR Images in a 15-class Scenario: an Attempt to Avoid Bias in the System](http://arxiv.org/abs/2109.12453)


  As of June 2021, the World Health Organization (WHO) has reported 171.7
million confirmed cases including 3,698,621 deaths from COVID-19. Detecting
COVID-19 and other lung diseases from Chest X-Ray (CXR) images can be very
effective for emergency diagnosis and treatment as CXR is fast and cheap. The
objective of this study is to develop a system capable of detecting COVID-19
along with 14 other lung diseases from CXRs in a fair and unbiased manner. The
proposed system consists of a CXR image selection technique and a deep learning
based model to classify 15 diseases including COVID-19. The proposed CXR
selection technique aims to retain the maximum variation uniformly and
eliminate poor quality CXRs with the goal of reducing the training dataset size
without compromising classifier accuracy. More importantly, it reduces the
often hidden bias and unfairness in decision making. The proposed solution
exhibits a promising COVID-19 detection scheme in a more realistic situation
than most existing studies as it deals with 15 lung diseases together. We hope
the proposed method will have wider adoption in medical image classification
and other related fields.

    

### [[2109.12456] Auditing AI models for Verified Deployment under Semantic Specifications](http://arxiv.org/abs/2109.12456)


  Auditing trained deep learning (DL) models prior to deployment is vital in
preventing unintended consequences. One of the biggest challenges in auditing
is in understanding how we can obtain human-interpretable specifications that
are directly useful to the end-user. We address this challenge through a
sequence of semantically-aligned unit tests, where each unit test verifies
whether a predefined specification (e.g., accuracy over 95%) is satisfied with
respect to controlled and semantically aligned variations in the input space
(e.g., in face recognition, the angle relative to the camera). We perform these
unit tests by directly verifying the semantically aligned variations in an
interpretable latent space of a generative model. Our framework, AuditAI,
bridges the gap between interpretable formal verification and scalability. With
evaluations on four different datasets, covering images of towers, chest
X-rays, human faces, and ImageNet classes, we show how AuditAI allows us to
obtain controlled variations for verification and certified training while
addressing the limitations of verifying using only pixel-space perturbations. A
blog post accompanying the paper is at this link
this https URL


### [[2109.12474] EllipseNet: Anchor-Free Ellipse Detection for Automatic Cardiac Biometrics in Fetal Echocardiography](http://arxiv.org/abs/2109.12474)


  As an important scan plane, four chamber view is routinely performed in both
second trimester perinatal screening and fetal echocardiographic examinations.
The biometrics in this plane including cardio-thoracic ratio (CTR) and cardiac
axis are usually measured by sonographers for diagnosing congenital heart
disease. However, due to the commonly existing artifacts like acoustic
shadowing, the traditional manual measurements not only suffer from the low
efficiency, but also with the inconsistent results depending on the operators'
skills. In this paper, we present an anchor-free ellipse detection network,
namely EllipseNet, which detects the cardiac and thoracic regions in ellipse
and automatically calculates the CTR and cardiac axis for fetal cardiac
biometrics in 4-chamber view. In particular, we formulate the network that
detects the center of each object as points and regresses the ellipses'
parameters simultaneously. We define an intersection-over-union loss to further
regulate the regression procedure. We evaluate EllipseNet on clinical
echocardiogram dataset with more than 2000 subjects. Experimental results show
that the proposed framework outperforms several state-of-the-art methods.
Source code will be available at this https URL .

    

### [[2109.12482] Physics-informed Convolutional Neural Networks for Temperature Field Prediction of Heat Source Layout without Labeled Data](http://arxiv.org/abs/2109.12482)


  Recently, surrogate models based on deep learning have attracted much
attention for engineering analysis and optimization. As the construction of
data pairs in most engineering problems is time-consuming, data acquisition is
becoming the predictive capability bottleneck of most deep surrogate models,
which also exists in surrogate for thermal analysis and design. To address this
issue, this paper develops a physics-informed convolutional neural network
(CNN) for the thermal simulation surrogate. The network can learn a mapping
from heat source layout to the steady-state temperature field without labeled
data, which equals solving an entire family of partial difference equations
(PDEs). To realize the physics-guided training without labeled data, we employ
the heat conduction equation and finite difference method to construct the loss
function. Since the solution is sensitive to boundary conditions, we properly
impose hard constraints by padding in the Dirichlet and Neumann boundary
conditions. In addition, the neural network architecture is well-designed to
improve the prediction precision of the problem at hand, and pixel-level online
hard example mining is introduced to overcome the imbalance of optimization
difficulty in the computation domain. The experiments demonstrate that the
proposed method can provide comparable predictions with numerical method and
data-driven deep learning models. We also conduct various ablation studies to
investigate the effectiveness of the network component and training methods
proposed in this paper.

    

### [[2109.12497] Quantization for Distributed Optimization](http://arxiv.org/abs/2109.12497)


  Massive amounts of data have led to the training of large-scale machine
learning models on a single worker inefficient. Distributed machine learning
methods such as Parallel-SGD have received significant interest as a solution
to tackle this problem. However, the performance of distributed systems does
not scale linearly with the number of workers due to the high network
communication cost for synchronizing gradients and parameters. Researchers have
proposed techniques such as quantization and sparsification to alleviate this
problem by compressing the gradients. Most of the compression schemes result in
compressed gradients that cannot be directly aggregated with efficient
protocols such as all-reduce. In this paper, we present a set of all-reduce
compatible gradient compression schemes which significantly reduce the
communication overhead while maintaining the performance of vanilla SGD. We
present the results of our experiments with the CIFAR10 dataset and
observations derived during the process. Our compression methods perform better
than the in-built methods currently offered by the deep learning frameworks.
Code is available at the repository:
\url{this https URL}.

    

### [[2109.12498] Short-Term Load Forecasting Using Time Pooling Deep Recurrent Neural Network](http://arxiv.org/abs/2109.12498)


  Integration of renewable energy sources and emerging loads like electric
vehicles to smart grids brings more uncertainty to the distribution system
management. Demand Side Management (DSM) is one of the approaches to reduce the
uncertainty. Some applications like Nonintrusive Load Monitoring (NILM) can
support DSM, however they require accurate forecasting on high resolution data.
This is challenging when it comes to single loads like one residential
household due to its high volatility. In this paper, we review some of the
existing Deep Learning-based methods and present our solution using Time
Pooling Deep Recurrent Neural Network. The proposed method augments data using
time pooling strategy and can overcome overfitting problems and model
uncertainties of data more efficiently. Simulation and implementation results
show that our method outperforms the existing algorithms in terms of RMSE and
MAE metrics.

    

### [[2109.12499] PETA: Photo Albums Event Recognition using Transformers Attention](http://arxiv.org/abs/2109.12499)


  In recent years the amounts of personal photos captured increased
significantly, giving rise to new challenges in multi-image understanding and
high-level image understanding. Event recognition in personal photo albums
presents one challenging scenario where life events are recognized from a
disordered collection of images, including both relevant and irrelevant images.
Event recognition in images also presents the challenge of high-level image
understanding, as opposed to low-level image object classification. In absence
of methods to analyze multiple inputs, previous methods adopted temporal
mechanisms, including various forms of recurrent neural networks. However,
their effective temporal window is local. In addition, they are not a natural
choice given the disordered characteristic of photo albums. We address this gap
with a tailor-made solution, combining the power of CNNs for image
representation and transformers for album representation to perform global
reasoning on image collection, offering a practical and efficient solution for
photo albums event recognition. Our solution reaches state-of-the-art results
on 3 prominent benchmarks, achieving above 90\% mAP on all datasets. We further
explore the related image-importance task in event recognition, demonstrating
how the learned attentions correlate with the human-annotated importance for
this subjective task, thus opening the door for new applications.

    

### [[2109.12500] Electoral Programs of German Parties 2021: A Computational Analysis Of Their Comprehensibility and Likeability Based On SentiArt](http://arxiv.org/abs/2109.12500)


  The electoral programs of six German parties issued before the parliamentary
elections of 2021 are analyzed using state-of-the-art computational tools for
quantitative narrative, topic and sentiment analysis. We compare different
methods for computing the textual similarity of the programs, Jaccard Bag
similarity, Latent Semantic Analysis, doc2vec, and sBERT, the representational
and computational complexity increasing from the 1st to the 4th method. A new
similarity measure for entire documents derived from the Fowlkes Mallows Score
is applied to kmeans clustering of sBERT transformed sentences. Using novel
indices of the readability and emotion potential of texts computed via SentiArt
(Jacobs, 2019), our data shed light on the similarities and differences of the
programs regarding their length, main ideas, comprehensibility, likeability,
and semantic complexity. Among others, they reveal that the programs of the SPD
and CDU have the best chances to be comprehensible and likeable -all other
things being equal-, and they raise the important issue of which similarity
measure is optimal for comparing texts such as electoral programs which
necessarily share a lot of words. While such analyses can not replace
qualitative analyses or a deep reading of the texts, they offer predictions
that can be verified in empirical studies and may serve as a motivation for
changing aspects of future electoral programs potentially making them more
comprehensible and/or likeable.

    

### [[2109.12504] Curvature Injected Adaptive Momentum Optimizer for Convolutional Neural Networks](http://arxiv.org/abs/2109.12504)


  In this paper, we propose a new approach, hereafter referred as AdaInject,
for the gradient descent optimizers by injecting the curvature information with
adaptive momentum. Specifically, the curvature information is used as a weight
to inject the second order moment in the update rule. The curvature information
is captured through the short-term parameter history. The AdaInject approach
boosts the parameter update by exploiting the curvature information. The
proposed approach is generic in nature and can be integrated with any existing
adaptive momentum stochastic gradient descent optimizers. The effectiveness of
the AdaInject optimizer is tested using a theoretical analysis as well as
through toy examples. We also show the convergence property of the proposed
injection based optimizer. Further, we depict the efficacy of the AdaInject
approach through extensive experiments in conjunction with the state-of-the-art
optimizers, i.e., AdamInject, diffGradInject, RadamInject, and AdaBeliefInject
on four benchmark datasets. Different CNN models are used in the experiments. A
highest improvement in the top-1 classification error rate of $16.54\%$ is
observed using diffGradInject optimizer with ResNeXt29 model over the CIFAR10
dataset. Overall, we observe very promising performance improvement of existing
optimizers with the proposed AdaInject approach.

    

### [[2109.12508] LINDA: Multi-Agent Local Information Decomposition for Awareness of Teammates](http://arxiv.org/abs/2109.12508)


  In cooperative multi-agent reinforcement learning (MARL), where agents only
have access to partial observations, efficiently leveraging local information
is critical. During long-time observations, agents can build \textit{awareness}
for teammates to alleviate the problem of partial observability. However,
previous MARL methods usually neglect this kind of utilization of local
information. To address this problem, we propose a novel framework, multi-agent
\textit{Local INformation Decomposition for Awareness of teammates} (LINDA),
with which agents learn to decompose local information and build awareness for
each teammate. We model the awareness as stochastic random variables and
perform representation learning to ensure the informativeness of awareness
representations by maximizing the mutual information between awareness and the
actual trajectory of the corresponding agent. LINDA is agnostic to specific
algorithms and can be flexibly integrated to different MARL methods. Sufficient
experiments show that the proposed framework learns informative awareness from
local partial observations for better collaboration and significantly improves
the learning performance, especially on challenging tasks.

    

### [[2109.12509] Deep Exploration for Recommendation Systems](http://arxiv.org/abs/2109.12509)


  We investigate the design of recommendation systems that can efficiently
learn from sparse and delayed feedback. Deep Exploration can play an important
role in such contexts, enabling a recommendation system to much more quickly
assess a user's needs and personalize service. We design an algorithm based on
Thompson Sampling that carries out Deep Exploration. We demonstrate through
simulations that the algorithm can substantially amplify the rate of positive
feedback relative to common recommendation system designs in a scalable
fashion. These results demonstrate promise that we hope will inspire
engineering of production recommendation systems that leverage Deep
Exploration.

    

### [[2109.12513] Generalized multiscale feature extraction for remaining useful life prediction of bearings with generative adversarial networks](http://arxiv.org/abs/2109.12513)


  Bearing is a key component in industrial machinery and its failure may lead
to unwanted downtime and economic loss. Hence, it is necessary to predict the
remaining useful life (RUL) of bearings. Conventional data-driven approaches of
RUL prediction require expert domain knowledge for manual feature extraction
and may suffer from data distribution discrepancy between training and test
data. In this study, we propose a novel generalized multiscale feature
extraction method with generative adversarial networks. The adversarial
training learns the distribution of training data from different bearings and
is introduced for health stage division and RUL prediction. To capture the
sequence feature from a one-dimensional vibration signal, we adapt a U-Net
architecture that reconstructs features to process them with multiscale layers
in the generator of the adversarial network. To validate the proposed method,
comprehensive experiments on two rotating machinery datasets have been
conducted to predict the RUL. The experimental results show that the proposed
feature extraction method can effectively predict the RUL and outperforms the
conventional RUL prediction approaches based on deep neural networks. The
implementation code is available at this https URL.

    

### [[2109.12516] Prioritized Experience-based Reinforcement Learning with Human Guidance: Methdology and Application to Autonomous Driving](http://arxiv.org/abs/2109.12516)


  Reinforcement learning requires skillful definition and remarkable
computational efforts to solve optimization and control problems, which could
impair its prospect. Introducing human guidance into reinforcement learning is
a promising way to improve learning performance. In this paper, a comprehensive
human guidance-based reinforcement learning framework is established. A novel
prioritized experience replay mechanism that adapts to human guidance in the
reinforcement learning process is proposed to boost the efficiency and
performance of the reinforcement learning algorithm. To relieve the heavy
workload on human participants, a behavior model is established based on an
incremental online learning method to mimic human actions. We design two
challenging autonomous driving tasks for evaluating the proposed algorithm.
Experiments are conducted to access the training and testing performance and
learning mechanism of the proposed algorithm. Comparative results against the
state-of-the-arts suggest the advantages of our algorithm in terms of learning
efficiency, performance, and robustness.

    

### [[2109.12517] Dynamic Adaptive Spatio-temporal Graph Convolution for fMRI Modelling](http://arxiv.org/abs/2109.12517)


  The characterisation of the brain as a functional network in which the
connections between brain regions are represented by correlation values across
time series has been very popular in the last years. Although this
representation has advanced our understanding of brain function, it represents
a simplified model of brain connectivity that has a complex dynamic
spatio-temporal nature. Oversimplification of the data may hinder the merits of
applying advanced non-linear feature extraction algorithms. To this end, we
propose a dynamic adaptive spatio-temporal graph convolution (DAST-GCN) model
to overcome the shortcomings of pre-defined static correlation-based graph
structures. The proposed approach allows end-to-end inference of dynamic
connections between brain regions via layer-wise graph structure learning
module while mapping brain connectivity to a phenotype in a supervised learning
framework. This leverages the computational power of the model, data and
targets to represent brain connectivity, and could enable the identification of
potential biomarkers for the supervised target in question. We evaluate our
pipeline on the UKBiobank dataset for age and gender classification tasks from
resting-state functional scans and show that it outperforms currently adapted
linear and non-linear methods in neuroimaging. Further, we assess the
generalizability of the inferred graph structure by transferring the
pre-trained graph to an independent dataset for the same task. Our results
demonstrate the task-robustness of the graph against different scanning
parameters and demographics.

    

### [[2109.12519] AsySQN: Faster Vertical Federated Learning Algorithms with Better Computation Resource Utilization](http://arxiv.org/abs/2109.12519)


  Vertical federated learning (VFL) is an effective paradigm of training the
emerging cross-organizational (e.g., different corporations, companies and
organizations) collaborative learning with privacy preserving. Stochastic
gradient descent (SGD) methods are the popular choices for training VFL models
because of the low per-iteration computation. However, existing SGD-based VFL
algorithms are communication-expensive due to a large number of communication
rounds. Meanwhile, most existing VFL algorithms use synchronous computation
which seriously hamper the computation resource utilization in real-world
applications. To address the challenges of communication and computation
resource utilization, we propose an asynchronous stochastic quasi-Newton
(AsySQN) framework for VFL, under which three algorithms, i.e. AsySQN-SGD,
-SVRG and -SAGA, are proposed. The proposed AsySQN-type algorithms making
descent steps scaled by approximate (without calculating the inverse Hessian
matrix explicitly) Hessian information convergence much faster than SGD-based
methods in practice and thus can dramatically reduce the number of
communication rounds. Moreover, the adopted asynchronous computation can make
better use of the computation resource. We theoretically prove the convergence
rates of our proposed algorithms for strongly convex problems. Extensive
numerical experiments on real-word datasets demonstrate the lower communication
costs and better computation resource utilization of our algorithms compared
with state-of-the-art VFL algorithms.

    

### [[2109.12523] A Study of Fake News Reading and Annotating in Social Media Context](http://arxiv.org/abs/2109.12523)


  The online spreading of fake news is a major issue threatening entire
societies. Much of this spreading is enabled by new media formats, namely
social networks and online media sites. Researchers and practitioners have been
trying to answer this by characterizing the fake news and devising automated
methods for detecting them. The detection methods had so far only limited
success, mostly due to the complexity of the news content and context and lack
of properly annotated datasets. One possible way to boost the efficiency of
automated misinformation detection methods, is to imitate the detection work of
humans. It is also important to understand the news consumption behavior of
online users. In this paper, we present an eye-tracking study, in which we let
44 lay participants to casually read through a social media feed containing
posts with news articles, some of which were fake. In a second run, we asked
the participants to decide on the truthfulness of these articles. We also
describe a follow-up qualitative study with a similar scenario but this time
with 7 expert fake news annotators. We present the description of both studies,
characteristics of the resulting dataset (which we hereby publish) and several
findings.

    

### [[2109.12533] BioCopy: A Plug-And-Play Span Copy Mechanism in Seq2Seq Models](http://arxiv.org/abs/2109.12533)


  Copy mechanisms explicitly obtain unchanged tokens from the source (input)
sequence to generate the target (output) sequence under the neural seq2seq
framework. However, most of the existing copy mechanisms only consider single
word copying from the source sentences, which results in losing essential
tokens while copying long spans. In this work, we propose a plug-and-play
architecture, namely BioCopy, to alleviate the problem aforementioned.
Specifically, in the training stage, we construct a BIO tag for each token and
train the original model with BIO tags jointly. In the inference stage, the
model will firstly predict the BIO tag at each time step, then conduct
different mask strategies based on the predicted BIO label to diminish the
scope of the probability distributions over the vocabulary list. Experimental
results on two separate generative tasks show that they all outperform the
baseline models by adding our BioCopy to the original model structure.

    

### [[2109.12534] Data Summarization via Bilevel Optimization](http://arxiv.org/abs/2109.12534)


  The increasing availability of massive data sets poses a series of challenges
for machine learning. Prominent among these is the need to learn models under
hardware or human resource constraints. In such resource-constrained settings,
a simple yet powerful approach is to operate on small subsets of the data.
Coresets are weighted subsets of the data that provide approximation guarantees
for the optimization objective. However, existing coreset constructions are
highly model-specific and are limited to simple models such as linear
regression, logistic regression, and $k$-means. In this work, we propose a
generic coreset construction framework that formulates the coreset selection as
a cardinality-constrained bilevel optimization problem. In contrast to existing
approaches, our framework does not require model-specific adaptations and
applies to any twice differentiable model, including neural networks. We show
the effectiveness of our framework for a wide range of models in various
settings, including training non-convex models online and batch active
learning.

    

### [[2109.12541] Dynamic Sequential Graph Learning for Click-Through Rate Prediction](http://arxiv.org/abs/2109.12541)


  Click-through rate prediction plays an important role in the field of
recommender system and many other applications. Existing methods mainly extract
user interests from user historical behaviors. However, behavioral sequences
only contain users' directly interacted items, which are limited by the
system's exposure, thus they are often not rich enough to reflect all the
potential interests. In this paper, we propose a novel method, named Dynamic
Sequential Graph Learning (DSGL), to enhance users or items' representations by
utilizing collaborative information from the local sub-graphs associated with
users or items. Specifically, we design the Dynamic Sequential Graph (DSG),
i.e., a lightweight ego subgraph with timestamps induced from historical
interactions. At every scoring moment, we construct DSGs for the target user
and the candidate item respectively. Based on the DSGs, we perform graph
convolutional operations iteratively in a bottom-up manner to obtain the final
representations of the target user and the candidate item. As for the graph
convolution, we design a Time-aware Sequential Encoding Layer that leverages
the interaction time information as well as temporal dependencies to learn
evolutionary user and item dynamics. Besides, we propose a Target-Preference
Dual Attention Layer, composed of a preference-aware attention module and a
target-aware attention module, to automatically search for parts of behaviors
that are relevant to the target and alleviate the noise from unreliable
neighbors. Results on real-world CTR prediction benchmarks demonstrate the
improvements brought by DSGL.

    

### [[2109.12546] Synthetic Data Generation for Fraud Detection using GANs](http://arxiv.org/abs/2109.12546)


  Detecting money laundering in gambling is becoming increasingly challenging
for the gambling industry as consumers migrate to online channels. Whilst
increasingly stringent regulations have been applied over the years to prevent
money laundering in gambling, despite this, online gambling is still a channel
for criminals to spend proceeds from crime. Complementing online gambling's
growth more concerns are raised to its effects compared with gambling in
traditional, physical formats, as it might introduce higher levels of problem
gambling or fraudulent behaviour due to its nature of immediate interaction
with online gambling experience. However, in most cases the main issue when
organisations try to tackle those areas is the absence of high quality data.
Since fraud detection related issues face the significant problem of the class
imbalance, in this paper we propose a novel system based on Generative
Adversarial Networks (GANs) for generating synthetic data in order to train a
supervised classifier. Our framework Synthetic Data Generation GAN (SDG-GAN),
manages to outperformed density based over-sampling methods and improve the
classification performance of benchmarks datasets and the real world gambling
fraud dataset.

    

### [[2109.12550] MixNN: Protection of Federated Learning Against Inference Attacks by Mixing Neural Network Layers](http://arxiv.org/abs/2109.12550)


  Machine Learning (ML) has emerged as a core technology to provide learning
models to perform complex tasks. Boosted by Machine Learning as a Service
(MLaaS), the number of applications relying on ML capabilities is ever
increasing. However, ML models are the source of different privacy violations
through passive or active attacks from different entities. In this paper, we
present MixNN a proxy-based privacy-preserving system for federated learning to
protect the privacy of participants against a curious or malicious aggregation
server trying to infer sensitive attributes. MixNN receives the model updates
from participants and mixes layers between participants before sending the
mixed updates to the aggregation server. This mixing strategy drastically
reduces privacy without any trade-off with utility. Indeed, mixing the updates
of the model has no impact on the result of the aggregation of the updates
computed by the server. We experimentally evaluate MixNN and design a new
attribute inference attack, Sim, exploiting the privacy vulnerability of SGD
algorithm to quantify privacy leakage in different settings (i.e., the
aggregation server can conduct a passive or an active attack). We show that
MixNN significantly limits the attribute inference compared to a baseline using
noisy gradient (well known to damage the utility) while keeping the same level
of utility as classic federated learning.

    

### [[2109.12561] Neural Augmentation of Kalman Filter with Hypernetwork for Channel Tracking](http://arxiv.org/abs/2109.12561)


  We propose Hypernetwork Kalman Filter (HKF) for tracking applications with
multiple different dynamics. The HKF combines generalization power of Kalman
filters with expressive power of neural networks. Instead of keeping a bank of
Kalman filters and choosing one based on approximating the actual dynamics, HKF
adapts itself to each dynamics based on the observed sequence. Through
extensive experiments on CDL-B channel model, we show that the HKF can be used
for tracking the channel over a wide range of Doppler values, matching Kalman
filter performance with genie Doppler information. At high Doppler values, it
achieves around 2dB gain over genie Kalman filter. The HKF generalizes well to
unseen Doppler, SNR values and pilot patterns unlike LSTM, which suffers from
severe performance degradation.

    

### [[2109.12584] Curb Your Carbon Emissions: Benchmarking Carbon Emissions in Machine Translation](http://arxiv.org/abs/2109.12584)


  In recent times, there has been definitive progress in the field of NLP, with
its applications growing as the utility of our language models increases with
advances in their performance. However, these models require a large amount of
computational power and data to train, consequently leading to large carbon
footprints. Therefore, is it imperative that we study the carbon efficiency and
look for alternatives to reduce the overall environmental impact of training
models, in particular large language models. In our work, we assess the
performance of models for machine translation, across multiple language pairs
to assess the difference in computational power required to train these models
for each of these language pairs and examine the various components of these
models to analyze aspects of our pipeline that can be optimized to reduce these
carbon emissions.

    

### [[2109.12606] Autoregressive neural-network wavefunctions for ab initio quantum chemistry](http://arxiv.org/abs/2109.12606)


  Performing electronic structure calculations is a canonical many-body problem
that has recently emerged as a challenging new paradigm for neural network
quantum states (NNQS). Here, we parameterise the electronic wavefunction with a
novel autoregressive neural network (ARN) that permits highly efficient and
scalable sampling, whilst also embedding physical priors that reflect the
structure of molecular systems without sacrificing expressibility. This allows
us to perform electronic structure calculations on molecules with up to 30
spin-orbitals - which consider multiple orders of magnitude more Slater
determinants than previous applications of conventional NNQS - and we find that
our ansatz can outperform the de-facto gold-standard coupled cluster methods
even in the presence of strong quantum correlations. With a highly expressive
neural network for which sampling is no longer a computational bottleneck, we
conclude that the barriers to further scaling are not associated with the
wavefunction ansatz itself, but rather are inherent to any variational Monte
Carlo approach.

    

### [[2109.12617] Structure-aware scale-adaptive networks for cancer segmentation in whole-slide images](http://arxiv.org/abs/2109.12617)


  Cancer segmentation in whole-slide images is a fundamental step for viable
tumour burden estimation, which is of great value for cancer assessment.
However, factors like vague boundaries or small regions dissociated from viable
tumour areas make it a challenging task. Considering the usefulness of
multi-scale features in various vision-related tasks, we present a
structure-aware scale-adaptive feature selection method for efficient and
accurate cancer segmentation. Based on a segmentation network with a popular
encoder-decoder architecture, a scale-adaptive module is proposed for selecting
more robust features to represent the vague, non-rigid boundaries. Furthermore,
a structural similarity metric is proposed for better tissue structure
awareness to deal with small region segmentation. In addition, advanced designs
including several attention mechanisms and the selective-kernel convolutions
are applied to the baseline network for comparative study purposes. Extensive
experimental results show that the proposed structure-aware scale-adaptive
networks achieve outstanding performance on liver cancer segmentation when
compared to top ten submitted results in the challenge of PAIP 2019. Further
evaluation on colorectal cancer segmentation shows that the scale-adaptive
module improves the baseline network or outperforms the other excellent designs
of attention mechanisms when considering the tradeoff between efficiency and
accuracy.

    

### [[2109.12621] Multi-Transformer: A New Neural Network-Based Architecture for Forecasting S&P Volatility](http://arxiv.org/abs/2109.12621)


  Events such as the Financial Crisis of 2007-2008 or the COVID-19 pandemic
caused significant losses to banks and insurance entities. They also
demonstrated the importance of using accurate equity risk models and having a
risk management function able to implement effective hedging strategies. Stock
volatility forecasts play a key role in the estimation of equity risk and,
thus, in the management actions carried out by financial institutions.
Therefore, this paper has the aim of proposing more accurate stock volatility
models based on novel machine and deep learning techniques. This paper
introduces a neural network-based architecture, called Multi-Transformer.
Multi-Transformer is a variant of Transformer models, which have already been
successfully applied in the field of natural language processing. Indeed, this
paper also adapts traditional Transformer layers in order to be used in
volatility forecasting models. The empirical results obtained in this paper
suggest that the hybrid models based on Multi-Transformer and Transformer
layers are more accurate and, hence, they lead to more appropriate risk
measures than other autoregressive algorithms or hybrid models based on feed
forward layers or long short term memory cells.

    

### [[2109.12622] Using Soft Labels to Model Uncertainty in Medical Image Segmentation](http://arxiv.org/abs/2109.12622)


  Medical image segmentation is inherently uncertain. For a given image, there
may be multiple plausible segmentation hypotheses, and physicians will often
disagree on lesion and organ boundaries. To be suited to real-world
application, automatic segmentation systems must be able to capture this
uncertainty and variability. Thus far, this has been addressed by building deep
learning models that, through dropout, multiple heads, or variational
inference, can produce a set - infinite, in some cases - of plausible
segmentation hypotheses for any given image. However, in clinical practice, it
may not be practical to browse all hypotheses. Furthermore, recent work shows
that segmentation variability plateaus after a certain number of independent
annotations, suggesting that a large enough group of physicians may be able to
represent the whole space of possible segmentations. Inspired by this, we
propose a simple method to obtain soft labels from the annotations of multiple
physicians and train models that, for each image, produce a single
well-calibrated output that can be thresholded at multiple confidence levels,
according to each application's precision-recall requirements. We evaluated our
method on the MICCAI 2021 QUBIQ challenge, showing that it performs well across
multiple medical image segmentation tasks, produces well-calibrated
predictions, and, on average, performs better at matching physicians'
predictions than other physicians.

    

### [[2109.12628] Logo Generation Using Regional Features: A Faster R-CNN Approach to Generative Adversarial Networks](http://arxiv.org/abs/2109.12628)


  In this paper we introduce the Local Logo Generative Adversarial Network
(LL-GAN) that uses regional features extracted from the Faster Regional
Convolutional Neural Network (Faster R-CNN) to generate logos. We demonstrate
the strength of this approach by training the framework on a small style-rich
dataset collected online to generate large impressive logos. Our approach beats
the state-of-the-art models (StyleGAN2, Self-Attention GANs) that suffer from
mode collapse due to the size of the data.

    

### [[2109.12630] Decision Making For Celebrity Branding: An Opinion Mining Approach Based On Polarity And Sentiment Analysis Using Twitter Consumer-Generated Content (CGC)](http://arxiv.org/abs/2109.12630)


  The volume of discussions concerning brands within social media provides
digital marketers with great opportunities for tracking and analyzing the
feelings and views of consumers toward brands, products, influencers, services,
and ad campaigns in CGC. The present study aims to assess and compare the
performance of firms and celebrities (i.e., influencers that with the
experience of being in an ad campaign of those companies) with the automated
sentiment analysis that was employed for CGC at social media while exploring
the feeling of the consumers toward them to observe which influencer (of two
for each company) had a closer effect with the corresponding corporation on
consumer minds. For this purpose, several consumer tweets from the pages of
brands and influencers were utilized to make a comparison of machine learning
and lexicon-based approaches to the sentiment analysis through the Naive
algorithm (lexicon-based) and Naive Bayes algorithm (machine learning method)
and obtain the desired results to assess the campaigns. The findings suggested
that the approaches were dissimilar in terms of accuracy; the machine learning
method yielded higher accuracy. Finally, the results showed which influencer
was more appropriate according to their existence in previous campaigns and
helped choose the right influencer in the future for our company and have a
better, more appropriate, and more efficient ad campaign subsequently. It is
required to conduct further studies on the accuracy improvement of the
sentiment classification. This approach should be employed for other social
media CGC types. The results revealed decision-making for which sentiment
analysis methods are the best approaches for the analysis of social media. It
was also found that companies should be aware of their consumers' sentiments
and choose the right person every time they think of a campaign.

    

### [[2109.12636] Hybrid Quantum Classical Graph Neural Networks for Particle Track Reconstruction](http://arxiv.org/abs/2109.12636)


  The Large Hadron Collider (LHC) at the European Organisation for Nuclear
Research (CERN) will be upgraded to further increase the instantaneous rate of
particle collisions (luminosity) and become the High Luminosity LHC (HL-LHC).
This increase in luminosity will significantly increase the number of particles
interacting with the detector. The interaction of particles with a detector is
referred to as "hit". The HL-LHC will yield many more detector hits, which will
pose a combinatorial challenge by using reconstruction algorithms to determine
particle trajectories from those hits. This work explores the possibility of
converting a novel Graph Neural Network model, that can optimally take into
account the sparse nature of the tracking detector data and their complex
geometry, to a Hybrid Quantum-Classical Graph Neural Network that benefits from
using Variational Quantum layers. We show that this hybrid model can perform
similar to the classical approach. Also, we explore Parametrized Quantum
Circuits (PQC) with different expressibility and entangling capacities, and
compare their training performance in order to quantify the expected benefits.
These results can be used to build a future road map to further develop circuit
based Hybrid Quantum-Classical Graph Neural Networks.

    

### [[2109.12662] Improving Question Answering Performance Using Knowledge Distillation and Active Learning](http://arxiv.org/abs/2109.12662)


  Contemporary question answering (QA) systems, including transformer-based
architectures, suffer from increasing computational and model complexity which
render them inefficient for real-world applications with limited resources.
Further, training or even fine-tuning such models requires a vast amount of
labeled data which is often not available for the task at hand. In this
manuscript, we conduct a comprehensive analysis of the mentioned challenges and
introduce suitable countermeasures. We propose a novel knowledge distillation
(KD) approach to reduce the parameter and model complexity of a pre-trained
BERT system and utilize multiple active learning (AL) strategies for immense
reduction in annotation efforts. In particular, we demonstrate that our model
achieves the performance of a 6-layer TinyBERT and DistilBERT, whilst using
only 2% of their total parameters. Finally, by the integration of our AL
approaches into the BERT framework, we show that state-of-the-art results on
the SQuAD dataset can be achieved when we only use 20% of the training data.

    

### [[2109.12674] MetaDrive: Composing Diverse Driving Scenarios for Generalizable Reinforcement Learning](http://arxiv.org/abs/2109.12674)


  Driving safely requires multiple capabilities from human and intelligent
agents, such as the generalizability to unseen environments, the decision
making in complex multi-agent settings, and the safety awareness of the
surrounding traffic. Despite the great success of reinforcement learning, most
of the RL research studies each capability separately due to the lack of the
integrated interactive environments. In this work, we develop a new driving
simulation platform called MetaDrive for the study of generalizable
reinforcement learning algorithms. MetaDrive is highly compositional, which can
generate an infinite number of diverse driving scenarios from both the
procedural generation and the real traffic data replay. Based on MetaDrive, we
construct a variety of RL tasks and baselines in both single-agent and
multi-agent settings, including benchmarking generalizability across unseen
scenes, safe exploration, and learning multi-agent traffic. We open-source this
simulator and maintain its development at:
this https URL


### [[2109.12679] Be More Active! Understanding the Differences between Mean and Sampled Representations of Variational Autoencoders](http://arxiv.org/abs/2109.12679)


  The ability of Variational Autoencoders to learn disentangled representations
has made them appealing for practical applications. However, their mean
representations, which are generally used for downstream tasks, have recently
been shown to be more correlated than their sampled counterpart, on which
disentanglement is usually measured. In this paper, we refine this observation
through the lens of selective posterior collapse, which states that only a
subset of the learned representations, the active variables, is encoding useful
information while the rest (the passive variables) is discarded. We first
extend the existing definition, originally proposed for sampled
representations, to mean representations and show that active variables are
equally disentangled in both representations. Based on this new definition and
the pre-trained models from disentanglement lib, we then isolate the passive
variables and show that they are responsible for the discrepancies between mean
and sampled representations. Specifically, passive variables exhibit high
correlation scores with other variables in mean representations while being
fully uncorrelated in sampled ones. We thus conclude that despite what their
higher correlation might suggest, mean representations are still good
candidates for downstream tasks applications. However, it may be beneficial to
remove their passive variables, especially when used with models sensitive to
correlated features.

    

### [[2109.12686] Efficient Non-linear Calculators](http://arxiv.org/abs/2109.12686)


  A novel algorithm for producing smooth nonlinearities on digital hardware is
presented. The non-linearities are inherently quadratic and have both
symmetrical and asymmetrical variants. The integer (and fixed point)
implementation is highly amenable for use with digital gates on an ASIC or
FPGA. The implementations are multiplier-less. Scaling of the non-linear
output, as required in an LSTM cell, is integrated into the implementation.
This too does not require a multiplier.
The non-linearities are useful as activation functions in a variety of ANN
architectures. The floating point mappings have been compared with other
non-linearities and have been benchmarked. Results show that these functions
should be considered in the ANN design phase.
The hardware resource usage of the implementations have been thoroughly
investigated. Our results make a strong case for implementions in edge
applications. This document summarizes the findings and serves to give a quick
overview of the outcomes of our research\footnote{The authors peer-reviewed
manuscripts (available at this https URL) offer
more detail and may be better suited for a thorough consideration}.

    

### [[2109.12690] Soundata: A Python library for reproducible use of audio datasets](http://arxiv.org/abs/2109.12690)


  Soundata is a Python library for loading and working with audio datasets in a
standardized way, removing the need for writing custom loaders in every
project, and improving reproducibility by providing tools to validate data
against a canonical version. It speeds up research pipelines by allowing users
to quickly download a dataset, load it into memory in a standardized and
reproducible way, validate that the dataset is complete and correct, and more.
Soundata is based and inspired on mirdata and design to complement mirdata by
working with environmental sound, bioacoustic and speech datasets, among
others. Soundata was created to be easy to use, easy to contribute to, and to
increase reproducibility and standardize usage of sound datasets in a flexible
way.

    

### [[2109.12696] Finite State Machine Policies Modulating Trajectory Generator](http://arxiv.org/abs/2109.12696)


  Deep reinforcement learning (deep RL) has emerged as an effective tool for
developing controllers for legged robots. However, a simple neural network
representation is known for its poor extrapolation ability, making the learned
behavior vulnerable to unseen perturbations or challenging terrains. Therefore,
researchers have investigated a novel architecture, Policies Modulating
Trajectory Generators (PMTG), which combines trajectory generators (TG) and
feedback control signals to achieve more robust behaviors. In this work, we
propose to extend the PMTG framework with a finite state machine PMTG by
replacing simple TGs with asynchronous finite state machines (Async FSMs). This
invention offers an explicit notion of contact events to the policy to
negotiate unexpected perturbations. We demonstrated that the proposed
architecture could achieve more robust behaviors in various scenarios, such as
challenging terrains or external perturbations, on both simulated and real
robots. The supplemental video can be found at: this http URL.

    

### [[2109.12701] Sparse Plus Low Rank Matrix Decomposition: A Discrete Optimization Approach](http://arxiv.org/abs/2109.12701)


  We study the Sparse Plus Low Rank decomposition problem (SLR), which is the
problem of decomposing a corrupted data matrix $\mathbf{D}$ into a sparse
matrix $\mathbf{Y}$ containing the perturbations plus a low rank matrix
$\mathbf{X}$. SLR is a fundamental problem in Operations Research and Machine
Learning arising in many applications such as data compression, latent semantic
indexing, collaborative filtering and medical imaging. We introduce a novel
formulation for SLR that directly models the underlying discreteness of the
problem. For this formulation, we develop an alternating minimization heuristic
to compute high quality solutions and a novel semidefinite relaxation that
provides meaningful bounds for the solutions returned by our heuristic. We
further develop a custom branch and bound routine that leverages our heuristic
and convex relaxation that solves small instances of SLR to certifiable
near-optimality. Our heuristic can scale to $n=10000$ in hours, our relaxation
can scale to $n=200$ in hours, and our branch and bound algorithm can scale to
$n=25$ in minutes. Our numerical results demonstrate that our approach
outperforms existing state-of-the-art approaches in terms of the MSE of the low
rank matrix and that of the sparse matrix.

    

### [[2109.12713] Provable Low Rank Plus Sparse Matrix Separation Via Nonconvex Regularizers](http://arxiv.org/abs/2109.12713)


  This paper considers a large class of problems where we seek to recover a low
rank matrix and/or sparse vector from some set of measurements. While methods
based on convex relaxations suffer from a (possibly large) estimator bias, and
other nonconvex methods require the rank or sparsity to be known a priori, we
use nonconvex regularizers to minimize the rank and $l_0$ norm without the
estimator bias from the convex relaxation. We present a novel analysis of the
alternating proximal gradient descent algorithm applied to such problems, and
bound the error between the iterates and the ground truth sparse and low rank
matrices. The algorithm and error bound can be applied to sparse optimization,
matrix completion, and robust principal component analysis as special cases of
our results.

    

### [[2109.12714] Cluster Analysis with Deep Embeddings and Contrastive Learning](http://arxiv.org/abs/2109.12714)


  Unsupervised disentangled representation learning is a long-standing problem
in computer vision. This work proposes a novel framework for performing image
clustering from deep embeddings by combining instance-level contrastive
learning with a deep embedding based cluster center predictor. Our approach
jointly learns representations and predicts cluster centers in an end-to-end
manner. This is accomplished via a three-pronged approach that combines a
clustering loss, an instance-wise contrastive loss, and an anchor loss. Our
fundamental intuition is that using an ensemble loss that incorporates
instance-level features and a clustering procedure focusing on semantic
similarity reinforces learning better representations in the latent space. We
observe that our method performs exceptionally well on popular vision datasets
when evaluated using standard clustering metrics such as Normalized Mutual
Information (NMI), in addition to producing geometrically well-separated
cluster embeddings as defined by the Euclidean distance. Our framework performs
on par with widely accepted clustering methods and outperforms the
state-of-the-art contrastive learning method on the CIFAR-10 dataset with an
NMI score of 0.772, a 7-8% improvement on the strong baseline.

    

### [[2109.12742] FewNLU: Benchmarking State-of-the-Art Methods for Few-Shot Natural Language Understanding](http://arxiv.org/abs/2109.12742)


  The few-shot natural language understanding (NLU) task has attracted much
recent attention. However, prior methods have been evaluated under a disparate
set of protocols, which hinders fair comparison and measuring progress of the
field. To address this issue, we introduce an evaluation framework that
improves previous evaluation procedures in three key aspects, i.e., test
performance, dev-test correlation, and stability. Under this new evaluation
framework, we re-evaluate several state-of-the-art few-shot methods for NLU
tasks. Our framework reveals new insights: (1) both the absolute performance
and relative gap of the methods were not accurately estimated in prior
literature; (2) no single method dominates most tasks with consistent
performance; (3) improvements of some methods diminish with a larger pretrained
model; and (4) gains from different methods are often complementary and the
best combined model performs close to a strong fully-supervised baseline. We
open-source our toolkit, FewNLU, that implements our evaluation framework along
with a number of state-of-the-art methods.

    

### [[2109.12746] Training on Test Data with Bayesian Adaptation for Covariate Shift](http://arxiv.org/abs/2109.12746)


  When faced with distribution shift at test time, deep neural networks often
make inaccurate predictions with unreliable uncertainty estimates. While
improving the robustness of neural networks is one promising approach to
mitigate this issue, an appealing alternate to robustifying networks against
all possible test-time shifts is to instead directly adapt them to unlabeled
inputs from the particular distribution shift we encounter at test time.
However, this poses a challenging question: in the standard Bayesian model for
supervised learning, unlabeled inputs are conditionally independent of model
parameters when the labels are unobserved, so what can unlabeled data tell us
about the model parameters at test-time? In this paper, we derive a Bayesian
model that provides for a well-defined relationship between unlabeled inputs
under distributional shift and model parameters, and show how approximate
inference in this model can be instantiated with a simple regularized entropy
minimization procedure at test-time. We evaluate our method on a variety of
distribution shifts for image classification, including image corruptions,
natural distribution shifts, and domain adaptation settings, and show that our
method improves both accuracy and uncertainty estimation.

    

### [[2109.12750] Learning Multimodal Rewards from Rankings](http://arxiv.org/abs/2109.12750)


  Learning from human feedback has shown to be a useful approach in acquiring
robot reward functions. However, expert feedback is often assumed to be drawn
from an underlying unimodal reward function. This assumption does not always
hold including in settings where multiple experts provide data or when a single
expert provides data for different tasks -- we thus go beyond learning a
unimodal reward and focus on learning a multimodal reward function. We
formulate the multimodal reward learning as a mixture learning problem and
develop a novel ranking-based learning approach, where the experts are only
required to rank a given set of trajectories. Furthermore, as access to
interaction data is often expensive in robotics, we develop an active querying
approach to accelerate the learning process. We conduct experiments and user
studies using a multi-task variant of OpenAI's LunarLander and a real Fetch
robot, where we collect data from multiple users with different preferences.
The results suggest that our approach can efficiently learn multimodal reward
functions, and improve data-efficiency over benchmark methods that we adapt to
our learning problem.

    

### [[2109.12764] Graph-Based Spatial-Temporal Convolutional Network for Vehicle Trajectory Prediction in Autonomous Driving](http://arxiv.org/abs/2109.12764)


  Forecasting the trajectories of neighbor vehicles is a crucial step for
decision making and motion planning of autonomous vehicles. This paper proposes
a graph-based spatial-temporal convolutional network (GSTCN) to predict future
trajectory distributions of all neighbor vehicles using past trajectories. This
network tackles the spatial interactions using a graph convolutional network
(GCN), and captures the temporal features with a convolutional neural network
(CNN). The spatial-temporal features are encoded and decoded by a gated
recurrent unit (GRU) network to generate future trajectory distributions.
Besides, we propose a weighted adjacency matrix to describe the intensities of
mutual influence between vehicles, and the ablation study demonstrates the
effectiveness of our proposed scheme. Our network is evaluated on two
real-world freeway trajectory datasets: I-80 and US-101 in the Next Generation
Simulation (NGSIM).Comparisons in three aspects, including prediction errors,
model sizes, and inference speeds, show that our network can achieve
state-of-the-art performance.

    

### [[2109.12769] Heterogeneous Treatment Effect Estimation using machine learning for Healthcare application: tutorial and benchmark](http://arxiv.org/abs/2109.12769)


  Developing new drugs for target diseases is a time-consuming and expensive
task, drug repurposing has become a popular topic in the drug development
field. As much health claim data become available, many studies have been
conducted on the data. The real-world data is noisy, sparse, and has many
confounding factors. In addition, many studies have shown that drugs effects
are heterogeneous among the population. Lots of advanced machine learning
models about estimating heterogeneous treatment effects (HTE) have emerged in
recent years, and have been applied to in econometrics and machine learning
communities. These studies acknowledge medicine and drug development as the
main application area, but there has been limited translational research from
the HTE methodology to drug development. We aim to introduce the HTE
methodology to the healthcare area and provide feasibility consideration when
translating the methodology with benchmark experiments on healthcare
administrative claim data. Also, we want to use benchmark experiments to show
how to interpret and evaluate the model when it is applied to healthcare
research. By introducing the recent HTE techniques to a broad readership in
biomedical informatics communities, we expect to promote the wide adoption of
causal inference using machine learning. We also expect to provide the
feasibility of HTE for personalized drug effectiveness.

    

### [[2109.12772] Distributionally Robust Multiclass Classification and Applications in Deep CNN Image Classifiers](http://arxiv.org/abs/2109.12772)


  We develop a Distributionally Robust Optimization (DRO) formulation for
Multiclass Logistic Regression (MLR), which could tolerate data contaminated by
outliers. The DRO framework uses a probabilistic ambiguity set defined as a
ball of distributions that are close to the empirical distribution of the
training set in the sense of the Wasserstein metric. We relax the DRO
formulation into a regularized learning problem whose regularizer is a norm of
the coefficient matrix. We establish out-of-sample performance guarantees for
the solutions to our model, offering insights on the role of the regularizer in
controlling the prediction error. We apply the proposed method in rendering
deep CNN-based image classifiers robust to random and adversarial attacks.
Specifically, using the MNIST and CIFAR-10 datasets, we demonstrate reductions
in test error rate by up to 78.8% and loss by up to 90.8%. We also show that
with a limited number of perturbed images in the training set, our method can
improve the error rate by up to 49.49% and the loss by up to 68.93% compared to
Empirical Risk Minimization (ERM), converging faster to an ideal loss/error
rate as the number of perturbed images increases.

    

### [[2109.12777] ReINTEL Challenge 2020: A Comparative Study of Hybrid Deep Neural Network for Reliable Intelligence Identification on Vietnamese SNSs](http://arxiv.org/abs/2109.12777)


  The overwhelming abundance of data has created a misinformation crisis.
Unverified sensationalism that is designed to grab the readers' short attention
span, when crafted with malice, has caused irreparable damage to our society's
structure. As a result, determining the reliability of an article has become a
crucial task. After various ablation studies, we propose a multi-input model
that can effectively leverage both tabular metadata and post content for the
task. Applying state-of-the-art finetuning techniques for the pretrained
component and training strategies for our complete model, we have achieved a
0.9462 ROC-score on the VLSP private test set.

    

### [[2109.12783] Leveraging Multiple CNNs for Triaging Medical Workflow](http://arxiv.org/abs/2109.12783)


  High hospitalization rates due to the global spread of Covid-19 bring about a
need for improvements to classical triaging workflows. To this end,
convolutional neural networks (CNNs) can effectively differentiate critical
from non-critical images so that critical cases may be addressed quickly, so
long as there exists some representative image for the illness. Presented is a
conglomerate neural network system consisting of multiple VGG16 CNNs; the
system trains on weighted skin disease images re-labelled as critical or
non-critical, to then attach to input images a critical index between 0 and 10.
A critical index offers a more comprehensive rating system compared to binary
critical/non-critical labels. Results for batches of input images run through
the trained network are promising. A batch is shown being re-ordered by the
proposed architecture from most critical to least critical roughly accurately.

    

### [[2109.12784] Learning from Small Samples: Transformation-Invariant SVMs with Composition and Locality at Multiple Scales](http://arxiv.org/abs/2109.12784)


  Motivated by the problem of learning when the number of training samples is
small, this paper shows how to incorporate into support-vector machines (SVMs)
those properties that have made convolutional neural networks (CNNs)
successful. Particularly important is the ability to incorporate domain
knowledge of invariances, e.g., translational invariance of images. Kernels
based on the \textit{minimum} distance over a group of transformations, which
corresponds to defining similarity as the \textit{best} over the possible
transformations, are not generally positive definite. Perhaps it is for this
reason that they have neither previously been experimentally tested for their
performance nor studied theoretically. Instead, previous attempts have employed
kernels based on the \textit{average} distance over a group of transformations,
which are trivially positive definite, but which generally yield both poor
margins as well as poor performance, as we show. We address this lacuna and
show that positive definiteness indeed holds \textit{with high probability} for
kernels based on the minimum distance in the small training sample set regime
of interest, and that they do yield the best results in that regime. Another
important property of CNNs is their ability to incorporate local features at
multiple spatial scales, e.g., through max pooling. A third important property
is their ability to provide the benefits of composition through the
architecture of multiple layers. We show how these additional properties can
also be embedded into SVMs. We verify through experiments on widely available
image sets that the resulting SVMs do provide superior accuracy in comparison
to well-established neural network (DNN) benchmarks for small sample sizes.

    

### [[2109.12786] Self-Replicating Neural Programs](http://arxiv.org/abs/2109.12786)


  In this work, a neural network is trained to replicate the code that trains
it using only its own output as input. A paradigm for evolutionary
self-replication in neural programs is introduced, where program parameters are
mutated, and the ability for the program to more efficiently train itself leads
to greater reproductive success. This evolutionary paradigm is demonstrated to
produce more efficient learning in organisms from a setting without any
explicit guidance, solely based on natural selection favoring organisms with
faster reproductive maturity.

    

### [[2109.12800] Machine Learning based Medical Image Deepfake Detection: A Comparative Study](http://arxiv.org/abs/2109.12800)


  Deep generative networks in recent years have reinforced the need for caution
while consuming various modalities of digital information. One avenue of
deepfake creation is aligned with injection and removal of tumors from medical
scans. Failure to detect medical deepfakes can lead to large setbacks on
hospital resources or even loss of life. This paper attempts to address the
detection of such attacks with a structured case study. We evaluate different
machine learning algorithms and pretrained convolutional neural networks on
distinguishing between tampered and untampered data. The findings of this work
show near perfect accuracy in detecting instances of tumor injections and
removals.

    

### [[2109.12803] Distributionally Robust Multi-Output Regression Ranking](http://arxiv.org/abs/2109.12803)


  Despite their empirical success, most existing listwiselearning-to-rank (LTR)
models are not built to be robust to errors in labeling or annotation,
distributional data shift, or adversarial data perturbations. To fill this gap,
we introduce a new listwise LTR model called Distributionally Robust
Multi-output Regression Ranking (DRMRR). Different from existing methods, the
scoring function of DRMRR was designed as a multivariate mapping from a feature
vector to a vector of deviation scores, which captures local context
information and cross-document interactions. DRMRR uses a Distributionally
Robust Optimization (DRO) framework to minimize a multi-output loss function
under the most adverse distributions in the neighborhood of the empirical data
distribution defined by a Wasserstein ball. We show that this is equivalent to
a regularized regression problem with a matrix norm regularizer. Our
experiments were conducted on two real-world applications, medical document
retrieval, and drug response prediction, showing that DRMRR notably outperforms
state-of-the-art LTR models. We also conducted a comprehensive analysis to
assess the resilience of DRMRR against various types of noise: Gaussian noise,
adversarial perturbations, and label poisoning. We show that DRMRR is not only
able to achieve significantly better performance than other baselines, but it
can maintain a relatively stable performance as more noise is added to the
data.

    

### [[1809.04564] On the Generalization of Stochastic Gradient Descent with Momentum](http://arxiv.org/abs/1809.04564)


  While momentum-based methods, in conjunction with stochastic gradient descent
(SGD), are widely used when training machine learning models, there is little
theoretical understanding on the generalization error of such methods. In this
work, we first show that there exists a convex loss function for which the
stability gap for multiple epochs of SGD with standard heavy-ball momentum
(SGDM) becomes unbounded. Then, for smooth Lipschitz loss functions, we analyze
a modified momentum-based update rule, i.e., SGD with early momentum (SGDEM),
and show that it admits an upper-bound on the generalization error. Thus, our
results show that machine learning models can be trained for multiple epochs of
SGDEM with a guarantee for generalization. Finally, for the special case of
strongly convex loss functions, we find a range of momentum such that multiple
epochs of standard SGDM, as a special form of SGDEM, also generalizes.
Extending our results on generalization, we also develop an upper-bound on the
expected true risk, in terms of the number of training steps, the size of the
training set, and the momentum parameter. Our experimental evaluations verify
the consistency between the numerical results and our theoretical bounds. SGDEM
improves the generalization error of SGDM when training ResNet-18 on ImageNet
in practical distributed settings.

    

### [[1908.02388] Benchmarking Bonus-Based Exploration Methods on the Arcade Learning Environment](http://arxiv.org/abs/1908.02388)


  This paper provides an empirical evaluation of recently developed exploration
algorithms within the Arcade Learning Environment (ALE). We study the use of
different reward bonuses that incentives exploration in reinforcement learning.
We do so by fixing the learning algorithm used and focusing only on the impact
of the different exploration bonuses in the agent's performance. We use
Rainbow, the state-of-the-art algorithm for value-based agents, and focus on
some of the bonuses proposed in the last few years. We consider the impact
these algorithms have on performance within the popular game Montezuma's
Revenge which has gathered a lot of interest from the exploration community,
across the the set of seven games identified by Bellemare et al. (2016) as
challenging for exploration, and easier games where exploration is not an
issue. We find that, in our setting, recently developed bonuses do not provide
significantly improved performance on Montezuma's Revenge or hard exploration
games. We also find that existing bonus-based methods may negatively impact
performance on games in which exploration is not an issue and may even perform
worse than $\epsilon$-greedy exploration.

    

### [[1910.01738] State Representation Learning from Demonstration](http://arxiv.org/abs/1910.01738)


  Robots could learn their own state and world representation from perception
and experience without supervision. This desirable goal is the main focus of
our field of interest, state representation learning (SRL). Indeed, a compact
representation of such a state is beneficial to help robots grasp onto their
environment for interacting. The properties of this representation have a
strong impact on the adaptive capability of the agent. In this article we
present an approach based on imitation learning. The idea is to train several
policies that share the same representation to reproduce various
demonstrations. To do so, we use a multi-head neural network with a shared
state representation feeding a task-specific agent. If the demonstrations are
diverse, the trained representation will eventually contain the information
necessary for all tasks, while discarding irrelevant information. As such, it
will potentially become a compact state representation useful for new tasks. We
call this approach SRLfD (State Representation Learning from Demonstration).
Our experiments confirm that when a controller takes SRLfD-based
representations as input, it can achieve better performance than with other
representation strategies and promote more efficient reinforcement learning
(RL) than with an end-to-end RL strategy.

    

### [[1912.05759] GPRInvNet: Deep Learning-Based Ground Penetrating Radar Data Inversion for Tunnel Lining](http://arxiv.org/abs/1912.05759)


  A DNN architecture referred to as GPRInvNet was proposed to tackle the
challenges of mapping the ground-penetrating radar (GPR) B-Scan data to complex
permittivity maps of subsurface structures. The GPRInvNet consisted of a
trace-to-trace encoder and a decoder. It was specially designed to take into
account the characteristics of GPR inversion when faced with complex GPR B-Scan
data, as well as addressing the spatial alignment issues between time-series
B-Scan data and spatial permittivity maps. It displayed the ability to fuse
features from several adjacent traces on the B-Scan data to enhance each trace,
and then further condense the features of each trace separately. As a result,
the sensitive zones on the permittivity maps spatially aligned to the enhanced
trace could be reconstructed accurately. The GPRInvNet has been utilized to
reconstruct the permittivity map of tunnel linings. A diverse range of
dielectric models of tunnel linings containing complex defects has been
reconstructed using GPRInvNet. The results have demonstrated that the GPRInvNet
is capable of effectively reconstructing complex tunnel lining defects with
clear boundaries. Comparative results with existing baseline methods also
demonstrated the superiority of the GPRInvNet. For the purpose of generalizing
the GPRInvNet to real GPR data, some background noise patches recorded from
practical model testing were integrated into the synthetic GPR data to retrain
the GPRInvNet. The model testing has been conducted for validation, and
experimental results revealed that the GPRInvNet had also achieved satisfactory
results with regard to the real data.

    

### [[1912.07018] Disentanglement based Active Learning](http://arxiv.org/abs/1912.07018)


  We propose Disentanglement based Active Learning (DAL), a new active learning
technique based on self-supervision which leverages the concept of
disentanglement. Instead of requesting labels from human oracle, our method
automatically labels the majority of the datapoints, thus drastically reducing
the human labeling budget in Generative Adversarial Net (GAN) based active
learning approaches. The proposed method uses Information Maximizing Generative
Adversarial Nets (InfoGAN) to learn disentangled class category
representations. Disagreement between active learner predictions and InfoGAN
labels decides if the datapoints need to be human-labeled. We also introduce a
label correction mechanism that aims to filter out label noise that occurs due
to automatic labeling. Results on three benchmark datasets for the image
classification task demonstrate that our method achieves better performance
compared to existing GAN-based active learning approaches.

    

### [[2005.08931] Joint Multi-Dimension Pruning via Numerical Gradient Update](http://arxiv.org/abs/2005.08931)


  We present joint multi-dimension pruning (abbreviated as JointPruning), an
effective method of pruning a network on three crucial aspects: spatial, depth
and channel simultaneously. To tackle these three naturally different
dimensions, we proposed a general framework by defining pruning as seeking the
best pruning vector (i.e., the numerical value of layer-wise channel number,
spacial size, depth) and construct a unique mapping from the pruning vector to
the pruned network structures. Then we optimize the pruning vector with
gradient update and model joint pruning as a numerical gradient optimization
process. To overcome the challenge that there is no explicit function between
the loss and the pruning vectors, we proposed self-adapted stochastic gradient
estimation to construct a gradient path through network loss to pruning vectors
and enable efficient gradient update. We show that the joint strategy discovers
a better status than previous studies that focused on individual dimensions
solely, as our method is optimized collaboratively across the three dimensions
in a single end-to-end training and it is more efficient than the previous
exhaustive methods. Extensive experiments on large-scale ImageNet dataset
across a variety of network architectures MobileNet V1&V2&V3 and ResNet
demonstrate the effectiveness of our proposed method. For instance, we achieve
significant margins of 2.5% and 2.6% improvement over the state-of-the-art
approach on the already compact MobileNet V1&V2 under an extremely large
compression ratio.

    

### [[2006.10189] Revisiting minimum description length complexity in overparameterized models](http://arxiv.org/abs/2006.10189)


  Complexity is a fundamental concept underlying statistical learning theory
that aims to inform generalization performance. Parameter count, while
successful in low-dimensional settings, is not well-justified for
overparameterized settings when the number of parameters is more than the
number of training samples. We revisit complexity measures based on Rissanen's
principle of minimum description length (MDL) and define a novel MDL-based
complexity (MDL-COMP) that remains valid for overparameterized models. MDL-COMP
is defined via an optimality criterion over the encodings induced by a good
Ridge estimator class. We provide an extensive theoretical characterization of
MDL-COMP for linear models and kernel methods and show that it is not just a
function of parameter count, but rather a function of the singular values of
the design or the kernel matrix and the signal-to-noise ratio.
For a linear model with $n$ observations, $d$ parameters, and i.i.d. Gaussian
predictors, MDL-COMP scales linearly with $d$ when $d<n$, but the scaling is
exponentially smaller -- $\log d$ for $d>n$. For kernel methods, we show that
MDL-COMP informs minimax in-sample error, and can decrease as the
dimensionality of the input increases. We also prove that MDL-COMP upper bounds
the in-sample mean squared error (MSE). Via an array of simulations and
real-data experiments, we show that a data-driven Prac-MDL-COMP informs
hyper-parameter tuning for optimizing test MSE with ridge regression in limited
data settings, sometimes improving upon cross-validation and (always) saving
computational costs. Finally, our findings also suggest that the recently
observed double decent phenomenons in overparameterized models might be a
consequence of the choice of non-ideal estimators.

    

### [[2007.02486] Regularization Matters: A Nonparametric Perspective on Overparametrized Neural Network](http://arxiv.org/abs/2007.02486)


  Overparametrized neural networks trained by gradient descent (GD) can
provably overfit any training data. However, the generalization guarantee may
not hold for noisy data. From a nonparametric perspective, this paper studies
how well overparametrized neural networks can recover the true target function
in the presence of random noises. We establish a lower bound on the $L_2$
estimation error with respect to the GD iterations, which is away from zero
without a delicate scheme of early stopping. In turn, through a comprehensive
analysis of $\ell_2$-regularized GD trajectories, we prove that for
overparametrized one-hidden-layer ReLU neural network with the $\ell_2$
regularization: (1) the output is close to that of the kernel ridge regression
with the corresponding neural tangent kernel; (2) minimax {optimal} rate of
$L_2$ estimation error can be achieved. Numerical experiments confirm our
theory and further demonstrate that the $\ell_2$ regularization approach
improves the training robustness and works for a wider range of neural
networks.

    

### [[2008.05558] On the complexity of finding a local minimizer of a quadratic function over a polytope](http://arxiv.org/abs/2008.05558)


  We show that unless P=NP, there cannot be a polynomial-time algorithm that
finds a point within Euclidean distance $c^n$ (for any constant $c \ge 0$) of a
local minimizer of an $n$-variate quadratic function over a polytope. This
result (even with $c=0$) answers a question of Pardalos and Vavasis that
appeared in 1992 on a list of seven open problems in complexity theory for
numerical optimization. Our proof technique also implies that the problem of
deciding whether a quadratic function has a local minimizer over an (unbounded)
polyhedron, and that of deciding if a quartic polynomial has a local minimizer
are NP-hard.

    

### [[2008.08642] $\ell_p$-Norm Multiple Kernel One-Class Fisher Null-Space](http://arxiv.org/abs/2008.08642)


  The paper addresses the multiple kernel learning (MKL) problem for one-class
classification (OCC). For this purpose, based on the Fisher null-space
one-class classification principle, we present a multiple kernel learning
algorithm where a general $\ell_p$-norm constraint ($p\geq1$) on kernel weights
is considered. We cast the proposed one-class MKL task as a min-max saddle
point Lagrangian optimisation problem and propose an efficient method to solve
it. An extension of the proposed one-class MKL approach is also considered
where several related one-class MKL tasks are learned jointly by constraining
them to share common kernel weights.
An extensive assessment of the proposed method on a range of data sets from
different application domains confirms its merits against the baseline and
several other algorithms.

    

### [[2009.03107] sunny-as2: Enhancing SUNNY for Algorithm Selection](http://arxiv.org/abs/2009.03107)


  SUNNY is an Algorithm Selection (AS) technique originally tailored for
Constraint Programming (CP). SUNNY enables to schedule, from a portfolio of
solvers, a subset of solvers to be run on a given CP problem. This approach has
proved to be effective for CP problems, and its parallel version won many gold
medals in the Open category of the MiniZinc Challenge -- the yearly
international competition for CP solvers. In 2015, the ASlib benchmarks were
released for comparing AS systems coming from disparate fields (e.g., ASP, QBF,
and SAT) and SUNNY was extended to deal with generic AS problems. This led to
the development of sunny-as2, an algorithm selector based on SUNNY for ASlib
scenarios. A preliminary version of sunny-as2 was submitted to the Open
Algorithm Selection Challenge (OASC) in 2017, where it turned out to be the
best approach for the runtime minimization of decision problems. In this work,
we present the technical advancements of sunny-as2, including: (i)
wrapper-based feature selection; (ii) a training approach combining feature
selection and neighbourhood size configuration; (iii) the application of nested
cross-validation. We show how sunny-as2 performance varies depending on the
considered AS scenarios, and we discuss its strengths and weaknesses. Finally,
we also show how sunny-as2 improves on its preliminary version submitted to
OASC.

    

### [[2009.04899] Meta-learning based Alternating Minimization Algorithm for Non-convex Optimization](http://arxiv.org/abs/2009.04899)


  In this paper, we propose a novel solution for non-convex problems of
multiple variables, especially for those typically solved by an alternating
minimization (AM) strategy that splits the original optimization problem into a
set of sub-problems corresponding to each variable, and then iteratively
optimize each sub-problem using a fixed updating rule. However, due to the
intrinsic non-convexity of the original optimization problem, the optimization
can usually be trapped into spurious local minimum even when each sub-problem
can be optimally solved at each iteration. Meanwhile, learning-based
approaches, such as deep unfolding algorithms, are highly limited by the lack
of labelled data and restricted explainability. To tackle these issues, we
propose a meta-learning based alternating minimization (MLAM) method, which
aims to minimize a partial of the global losses over iterations instead of
carrying minimization on each sub-problem, and it tends to learn an adaptive
strategy to replace the handcrafted counterpart resulting in advance on
superior performance. Meanwhile, the proposed MLAM still maintains the original
algorithmic principle, which contributes to a better interpretability. We
evaluate the proposed method on two representative problems, namely, bi-linear
inverse problem: matrix completion, and non-linear problem: Gaussian mixture
models. The experimental results validate that our proposed approach
outperforms AM-based methods in standard settings, and is able to achieve
effective optimization in challenging cases while other comparing methods would
typically fail.

    

### [[2009.06182] Density Estimation via Bayesian Inference Engines](http://arxiv.org/abs/2009.06182)


  We explain how effective automatic probability density function estimates can
be constructed using contemporary Bayesian inference engines such as those
based on no-U-turn sampling and expectation propagation. Extensive simulation
studies demonstrate that the proposed density estimates have excellent
comparative performance and scale well to very large sample sizes due to a
binning strategy. Moreover, the approach is fully Bayesian and all estimates
are accompanied by pointwise credible intervals. An accompanying package in the
R language facilitates easy use of the new density estimates.

    

### [[2010.03250] DiffMG: Differentiable Meta Graph Search for Heterogeneous Graph Neural Networks](http://arxiv.org/abs/2010.03250)


  In this paper, we propose a novel framework to automatically utilize
task-dependent semantic information which is encoded in heterogeneous
information networks (HINs). Specifically, we search for a meta graph, which
can capture more complex semantic relations than a meta path, to determine how
graph neural networks (GNNs) propagate messages along different types of edges.
We formalize the problem within the framework of neural architecture search
(NAS) and then perform the search in a differentiable manner. We design an
expressive search space in the form of a directed acyclic graph (DAG) to
represent candidate meta graphs for a HIN, and we propose task-dependent type
constraint to filter out those edge types along which message passing has no
effect on the representations of nodes that are related to the downstream task.
The size of the search space we define is huge, so we further propose a novel
and efficient search algorithm to make the total search cost on a par with
training a single GNN once. Compared with existing popular NAS algorithms, our
proposed search algorithm improves the search efficiency. We conduct extensive
experiments on different HINs and downstream tasks to evaluate our method, and
experimental results show that our method can outperform state-of-the-art
heterogeneous GNNs and also improves efficiency compared with those methods
which can implicitly learn meta paths.

    

### [[2010.07047] A Predictive Visual Analytics System for Studying Neurodegenerative Disease based on DTI Fiber Tracts](http://arxiv.org/abs/2010.07047)


  Diffusion tensor imaging (DTI) has been used to study the effects of
neurodegenerative diseases on neural pathways, which may lead to more reliable
and early diagnosis of these diseases as well as a better understanding of how
they affect the brain. We introduce an intelligent visual analytics system for
studying patient groups based on their labeled DTI fiber tract data and
corresponding statistics. The system's AI-augmented interface guides the user
through an organized and holistic analysis space, including the statistical
feature space, the physical space, and the space of patients over different
groups. We use a custom machine learning pipeline to help narrow down this
large analysis space, and then explore it pragmatically through a range of
linked visualizations. We conduct several case studies using real data from the
research database of Parkinson's Progression Markers Initiative.

    

### [[2010.08603] A Sequential Framework Towards an Exact SDP Verification of Neural Networks](http://arxiv.org/abs/2010.08603)


  Although neural networks have been applied to several systems in recent
years, they still cannot be used in safety-critical systems due to the lack of
efficient techniques to certify their robustness. A number of techniques based
on convex optimization have been proposed in the literature to study the
robustness of neural networks, and the semidefinite programming (SDP) approach
has emerged as a leading contender for the robust certification of neural
networks. The major challenge to the SDP approach is that it is prone to a
large relaxation gap. In this work, we address this issue by developing a
sequential framework to shrink this gap to zero by adding non-convex cuts to
the optimization problem via disjunctive programming. We analyze the
performance of this sequential SDP method both theoretically and empirically,
and show that it bridges the gap as the number of cuts increases.

    

### [[2010.09443] Efficient Estimation and Evaluation of Prediction Rules in Semi-Supervised Settings under Stratified Sampling](http://arxiv.org/abs/2010.09443)


  In many contemporary applications, large amounts of unlabeled data are
readily available while labeled examples are limited. There has been
substantial interest in semi-supervised learning (SSL) which aims to leverage
unlabeled data to improve estimation or prediction. However, current SSL
literature focuses primarily on settings where labeled data is selected
randomly from the population of interest. Non-random sampling, while posing
additional analytical challenges, is highly applicable to many real world
problems. Moreover, no SSL methods currently exist for estimating the
prediction performance of a fitted model under non-random sampling. In this
paper, we propose a two-step SSL procedure for evaluating a prediction rule
derived from a working binary regression model based on the Brier score and
overall misclassification rate under stratified sampling. In step I, we impute
the missing labels via weighted regression with nonlinear basis functions to
account for nonrandom sampling and to improve efficiency. In step II, we
augment the initial imputations to ensure the consistency of the resulting
estimators regardless of the specification of the prediction model or the
imputation model. The final estimator is then obtained with the augmented
imputations. We provide asymptotic theory and numerical studies illustrating
that our proposals outperform their supervised counterparts in terms of
efficiency gain. Our methods are motivated by electronic health records (EHR)
research and validated with a real data analysis of an EHR-based study of
diabetic neuropathy.

    

### [[2010.12675] Overcoming Conflicting Data when Updating a Neural Semantic Parser](http://arxiv.org/abs/2010.12675)


  In this paper, we explore how to use a small amount of new data to update a
task-oriented semantic parsing model when the desired output for some examples
has changed. When making updates in this way, one potential problem that arises
is the presence of conflicting data, or out-of-date labels in the original
training set. To evaluate the impact of this understudied problem, we propose
an experimental setup for simulating changes to a neural semantic parser. We
show that the presence of conflicting data greatly hinders learning of an
update, then explore several methods to mitigate its effect. Our multi-task and
data selection methods lead to large improvements in model accuracy compared to
a naive data-mixing strategy, and our best method closes 86% of the accuracy
gap between this baseline and an oracle upper bound.

    

### [[2011.00517] Ask Your Humans: Using Human Instructions to Improve Generalization in Reinforcement Learning](http://arxiv.org/abs/2011.00517)


  Complex, multi-task problems have proven to be difficult to solve efficiently
in a sparse-reward reinforcement learning setting. In order to be sample
efficient, multi-task learning requires reuse and sharing of low-level
policies. To facilitate the automatic decomposition of hierarchical tasks, we
propose the use of step-by-step human demonstrations in the form of natural
language instructions and action trajectories. We introduce a dataset of such
demonstrations in a crafting-based grid world. Our model consists of a
high-level language generator and low-level policy, conditioned on language. We
find that human demonstrations help solve the most complex tasks. We also find
that incorporating natural language allows the model to generalize to unseen
tasks in a zero-shot setting and to learn quickly from a few demonstrations.
Generalization is not only reflected in the actions of the agent, but also in
the generated natural language instructions in unseen tasks. Our approach also
gives our trained agent interpretable behaviors because it is able to generate
a sequence of high-level descriptions of its actions.

    

### [[2011.02707] Dynamically Sampled Nonlocal Gradients for Stronger Adversarial Attacks](http://arxiv.org/abs/2011.02707)


  The vulnerability of deep neural networks to small and even imperceptible
perturbations has become a central topic in deep learning research. Although
several sophisticated defense mechanisms have been introduced, most were later
shown to be ineffective. However, a reliable evaluation of model robustness is
mandatory for deployment in safety-critical scenarios. To overcome this problem
we propose a simple yet effective modification to the gradient calculation of
state-of-the-art first-order adversarial attacks. Normally, the gradient update
of an attack is directly calculated for the given data point. This approach is
sensitive to noise and small local optima of the loss function. Inspired by
gradient sampling techniques from non-convex optimization, we propose
Dynamically Sampled Nonlocal Gradient Descent (DSNGD). DSNGD calculates the
gradient direction of the adversarial attack as the weighted average over past
gradients of the optimization history. Moreover, distribution hyperparameters
that define the sampling operation are automatically learned during the
optimization scheme. We empirically show that by incorporating this nonlocal
gradient information, we are able to give a more accurate estimation of the
global descent direction on noisy and non-convex loss surfaces. In addition, we
show that DSNGD-based attacks are on average 35% faster while achieving 0.9% to
27.1% higher success rates compared to their gradient descent-based
counterparts.

    

### [[2011.03715] Bayesian Nonparametric Dimensionality Reduction of Categorical Data for Predicting Severity of COVID-19 in Pregnant Women](http://arxiv.org/abs/2011.03715)


  The coronavirus disease (COVID-19) has rapidly spread throughout the world
and while pregnant women present the same adverse outcome rates, they are
underrepresented in clinical research. We collected clinical data of 155
test-positive COVID-19 pregnant women at Stony Brook University Hospital. Many
of these collected data are of multivariate categorical type, where the number
of possible outcomes grows exponentially as the dimension of data increases. We
modeled the data within the unsupervised Bayesian framework and mapped them
into a lower-dimensional space using latent Gaussian processes. The latent
features in the lower dimensional space were further used for predicting if a
pregnant woman would be admitted to a hospital due to COVID-19 or would remain
with mild symptoms. We compared the prediction accuracy with the dummy/one-hot
encoding of categorical data and found that the latent Gaussian process had
better accuracy.

    

### [[2011.05136] Predicting Hydroxyl Mediated Nucleophilic Degradation and Molecular Stability of RNA Sequences through the Application of Deep Learning Methods](http://arxiv.org/abs/2011.05136)


  Synthesis and efficient implementation mRNA strands has been shown to have
wide utility, especially recently in the development of COVID vaccines.
However, the intrinsic chemical stability of mRNA poses a challenge due to the
presence of 2'-hydroxyl groups in ribose sugars. The -OH group in the backbone
structure enables a base-catalyzed nucleophilic attack by the deprotonated
hydroxyl on the adjacent phosphorous and consequent self-hydrolysis of the
phosphodiester bond. As expected for in-line hydrolytic cleavage reactions, the
chemical stability of mRNA strands is highly dependent on external
environmental factors, e.g. pH, temperature, oxidizers, etc. Predicting this
chemical instability using a computational model will reduce the number of
sequences synthesized and tested through identifying the most promising
candidates, aiding the development of mRNA related therapies. This paper
proposes and evaluates three deep learning models (Long Short Term Memory,
Gated Recurrent Unit, and Graph Convolutional Networks) as methods to predict
the reactivity and risk of degradation of mRNA sequences. The Stanford Open
Vaccine dataset of 6034 mRNA sequences was used in this study. The training set
consisted of 3029 of these sequences (length of 107 nucleotide bases) while the
testing dataset consisted of 3005 sequences (length of 130 nucleotide bases),
in structured (Lowest Entropy Base Pair Probability Matrix) and unstructured
(Nodes and Edges) forms. The stability of mRNA strands was accurately
generated, with the Graph Convolutional Network being the best predictor of
reactivity ($RMSE = 0.249$) while the Gated Recurrent Unit Network was the best
at predicting risks of degradation ($RMSE = 0.266$). Combining all target
variables, the GRU performed the best with 76% accuracy. Results suggest these
models can be applied to understand and predict the chemical stability of mRNA
in the near future.

    

### [[2011.11959] Provably-Robust Runtime Monitoring of Neuron Activation Patterns](http://arxiv.org/abs/2011.11959)


  For deep neural networks (DNNs) to be used in safety-critical autonomous
driving tasks, it is desirable to monitor in operation time if the input for
the DNN is similar to the data used in DNN training. While recent results in
monitoring DNN activation patterns provide a sound guarantee due to building an
abstraction out of the training data set, reducing false positives due to
slight input perturbation has been an issue towards successfully adapting the
techniques. We address this challenge by integrating formal symbolic reasoning
inside the monitor construction process. The algorithm performs a sound
worst-case estimate of neuron values with inputs (or features) subject to
perturbation, before the abstraction function is applied to build the monitor.
The provable robustness is further generalized to cases where monitoring a
single neuron can use more than one bit, implying that one can record
activation patterns with a fine-grained decision on the neuron value interval.

    

### [[2012.00646] On hallucinations in tomographic image reconstruction](http://arxiv.org/abs/2012.00646)


  Tomographic image reconstruction is generally an ill-posed linear inverse
problem. Such ill-posed inverse problems are typically regularized using prior
knowledge of the sought-after object property. Recently, deep neural networks
have been actively investigated for regularizing image reconstruction problems
by learning a prior for the object properties from training images. However, an
analysis of the prior information learned by these deep networks and their
ability to generalize to data that may lie outside the training distribution is
still being explored. An inaccurate prior might lead to false structures being
hallucinated in the reconstructed image and that is a cause for serious concern
in medical imaging. In this work, we propose to illustrate the effect of the
prior imposed by a reconstruction method by decomposing the image estimate into
generalized measurement and null components. The concept of a hallucination map
is introduced for the general purpose of understanding the effect of the prior
in regularized reconstruction methods. Numerical studies are conducted
corresponding to a stylized tomographic imaging modality. The behavior of
different reconstruction methods under the proposed formalism is discussed with
the help of the numerical studies.

    

### [[2012.03426] Deep Learning Based Signal Enhancement of Low-Resolution Accelerometer for Fall Detection Systems](http://arxiv.org/abs/2012.03426)


  In the last two decades, fall detection (FD) systems have been developed as a
popular assistive technology. Such systems automatically detect critical fall
events and immediately alert medical professionals or caregivers. To support
long-term FD services, various power-saving strategies have been implemented.
Among them, a reduced sampling rate is a common approach for an
energy-efficient system in the real-world. However, the performance of FD
systems is diminished owing to low-resolution (LR) accelerometer signals. To
improve the detection accuracy with LR accelerometer signals, several technical
challenges must be considered, including misalignment, mismatch of effective
features, and the degradation effects. In this work, a deep-learning-based
accelerometer signal enhancement (ASE) model is proposed to improve the
detection performance of LR-FD systems. This proposed model reconstructs
high-resolution (HR) signals from the LR signals by learning the relationship
between the LR and HR signals. The results show that the FD system using
support vector machine and the proposed ASE model at an extremely low sampling
rate (sampling rate < 2 Hz) achieved 97.34% and 90.52% accuracies in the
SisFall and FallAllD datasets, respectively, while those without ASE models
only achieved 95.92% and 87.47% accuracies in the SisFall and FallAllD
datasets, respectively. This study demonstrates that the ASE model helps the FD
systems tackle the technical challenges of LR signals and achieve better
detection performance.

    

### [[2012.09037] Copula-based synthetic data augmentation for machine-learning emulators](http://arxiv.org/abs/2012.09037)


  Can we improve machine-learning (ML) emulators with synthetic data? If data
are scarce or expensive to source and a physical model is available,
statistically generated data may be useful for augmenting training sets
cheaply. Here we explore the use of copula-based models for generating
synthetically augmented datasets in weather and climate by testing the method
on a toy physical model of downwelling longwave radiation and corresponding
neural network emulator. Results show that for copula-augmented datasets,
predictions are improved by up to 62 % for the mean absolute error (from 1.17
to 0.44 W m$^{-2}$).

    

### [[2012.11524] A Meta-Learning Approach to the Optimal Power Flow Problem Under Topology Reconfigurations](http://arxiv.org/abs/2012.11524)


  Recently, there has been a surge of interest in adopting deep neural networks
(DNNs) for solving the optimal power flow (OPF) problem in power systems.
Computing optimal generation dispatch decisions using a trained DNN takes
significantly less time when compared to using conventional optimization
solvers. However, a major drawback of existing work is that the machine
learning models are trained for a specific system topology. Hence, the DNN
predictions are only useful as long as the system topology remains unchanged.
Changes to the system topology (initiated by the system operator) would require
retraining the DNN, which incurs significant training overhead and requires an
extensive amount of training data (corresponding to the new system topology).
To overcome this drawback, we propose a DNN-based OPF predictor that is trained
using a meta-learning (MTL) approach. The key idea behind this approach is to
find a common initialization vector that enables fast training for any system
topology. The developed OPF-predictor is validated through simulations using
benchmark IEEE bus systems. The results show that the MTL approach achieves
significant training speeds-ups and requires only a few gradient steps with a
few data samples to achieve high OPF prediction accuracy.

    

### [[2101.06072] Video Summarization Using Deep Neural Networks: A Survey](http://arxiv.org/abs/2101.06072)


  Video summarization technologies aim to create a concise and complete
synopsis by selecting the most informative parts of the video content. Several
approaches have been developed over the last couple of decades and the current
state of the art is represented by methods that rely on modern deep neural
network architectures. This work focuses on the recent advances in the area and
provides a comprehensive survey of the existing deep-learning-based methods for
generic video summarization. After presenting the motivation behind the
development of technologies for video summarization, we formulate the video
summarization task and discuss the main characteristics of a typical
deep-learning-based analysis pipeline. Then, we suggest a taxonomy of the
existing algorithms and provide a systematic review of the relevant literature
that shows the evolution of the deep-learning-based video summarization
technologies and leads to suggestions for future developments. We then report
on protocols for the objective evaluation of video summarization algorithms and
we compare the performance of several deep-learning-based approaches. Based on
the outcomes of these comparisons, as well as some documented considerations
about the amount of annotated data and the suitability of evaluation protocols,
we indicate potential future research directions.

    

### [[2101.06542] ConE: A Concurrent Edit Detection Tool for Large Scale Software Development](http://arxiv.org/abs/2101.06542)


  Modern, complex software systems are being continuously extended and
adjusted. The developers responsible for this may come from different teams or
organizations, and may be distributed over the world. This may make it
difficult to keep track of what other developers are doing, which may result in
multiple developers concurrently editing the same code areas. This, in turn,
may lead to hard-to-merge changes or even merge conflicts, logical bugs that
are difficult to detect, duplication of work, and wasted developer
productivity. To address this, we explore the extent of this problem in the
pull request based software development model. We study half a year of changes
made to six large repositories in Microsoft in which at least 1,000 pull
requests are created each month. We find that files concurrently edited in
different pull requests are more likely to introduce bugs. Motivated by these
findings, we design, implement, and deploy a service named ConE (Concurrent
Edit Detector) that proactively detects pull requests containing concurrent
edits, to help mitigate the problems caused by them. ConE has been designed to
scale, and to minimize false alarms while still flagging relevant concurrently
edited files. Key concepts of ConE include the detection of the Extent of
Overlap between pull requests, and the identification of Rarely Concurrently
Edited Files. To evaluate ConE, we report on its operational deployment on 234
repositories inside Microsoft. ConE assessed 26,000 pull requests and made 775
recommendations about conflicting changes, which were rated as useful in over
70% (554) of the cases. From interviews with 48 users we learned that they
believed ConE would save time in conflict resolution and avoiding duplicate
work, and that over 90% intend to keep using the service on a daily basis.

    

### [[2101.06589] Data-driven discovery of multiscale chemical reactions governed by the law of mass action](http://arxiv.org/abs/2101.06589)


  In this paper, we propose a data-driven method to discover multiscale
chemical reactions governed by the law of mass action. First, we use a single
matrix to represent the stoichiometric coefficients for both the reactants and
products in a system without catalysis reactions. The negative entries in the
matrix denote the stoichiometric coefficients for the reactants and the
positive ones for the products. Second, we find that the conventional
optimization methods usually get stuck in the local minima and could not find
the true solution in learning the multiscale chemical reactions. To overcome
this difficulty, we propose a partial-parameters-freezing (PPF) technique to
progressively determine the network parameters by using the fact that the
stoichiometric coefficients are integers. With such a technique, the dimension
of the searching space is gradually reduced in the training process and the
global mimina can be eventually obtained. Several numerical experiments
including the classical Michaelis-Menten kinetics, the hydrogen oxidation
reactions, and the simplified GRI-3.0 mechanism verify the good performance of
our algorithm in learning the multiscale chemical reactions. The code is
available at \url{this https URL}.

    

### [[2101.10357] Regret-Optimal Filtering](http://arxiv.org/abs/2101.10357)


  In this paper, we study the filtering problem of causally estimating a
desired signal from a related observation signal, through the lens of regret
optimization. Classical filter designs, such as $\mathcal H_2$ and $\mathcal
H_\infty$, minimize the average and worst-case estimation errors, respectively.
As a result $\mathcal H_2$ filters are sensitive to inaccuracies in the
underlying statistical model, and $\mathcal H_\infty$ filters are overly
conservative since they safeguard against the worst-case. To obtain filters
that perform better under all possible environments, we propose to minimize the
\emph{regret} of the causal filter relative to a clairvoyant one. More
explicitly, we minimize the largest deviation of the squared estimation error
of a causal filter from that of a clairvoyant estimator that also has access to
future observations. In this sense, our proposed filter will have the smallest
regret no matter what the true signal and the observation process are and thus
is more adaptive to different environments. When the underlying signals have
linear time-invariant state-space structure, we provide an explicit
construction for the regret optimal filter. The solution is obtained by
reducing regret-optimal filtering a Nehari problem, i.e., approximating a
non-causal operator by a causal one in spectral norm. The regret-optimal filter
bears some resemblance to Kalman and $\mathcal H_\infty$ filters: the
regret-optimal filter expressed as a state-space model inherits the finite
dimension of the original state-space, and its solution requires the solution
of $3$ algebraic Riccati equations. Our results and simulations demonstrate
that regret-optimality is a viable approach to filters design.

    

### [[2102.05126] AuGPT: Auxiliary Tasks and Data Augmentation for End-To-End Dialogue with Pre-Trained Language Models](http://arxiv.org/abs/2102.05126)


  Attention-based pre-trained language models such as GPT-2 brought
considerable progress to end-to-end dialogue modelling. However, they also
present considerable risks for task-oriented dialogue, such as lack of
knowledge grounding or diversity. To address these issues, we introduce
modified training objectives for language model finetuning, and we employ
massive data augmentation via back-translation to increase the diversity of the
training data. We further examine the possibilities of combining data from
multiples sources to improve performance on the target dataset. We carefully
evaluate our contributions with both human and automatic methods. Our model
substantially outperforms the baseline on the MultiWOZ data and shows
competitive performance with state of the art in both automatic and human
evaluation.

    

### [[2102.05509] Robustness in Compressed Neural Networks for Object Detection](http://arxiv.org/abs/2102.05509)


  Model compression techniques allow to significantly reduce the computational
cost associated with data processing by deep neural networks with only a minor
decrease in average accuracy. Simultaneously, reducing the model size may have
a large effect on noisy cases or objects belonging to less frequent classes. It
is a crucial problem from the perspective of the models' safety, especially for
object detection in the autonomous driving setting, which is considered in this
work. It was shown in the paper that the sensitivity of compressed models to
different distortion types is nuanced, and some of the corruptions are heavily
impacted by the compression methods (i.e., additive noise), while others (blur
effect) are only slightly affected. A common way to improve the robustness of
models is to use data augmentation, which was confirmed to positively affect
models' robustness, also for highly compressed models. It was further shown
that while data imbalance methods brought only a slight increase in accuracy
for the baseline model (without compression), the impact was more striking at
higher compression rates for the structured pruning. Finally, methods for
handling data imbalance brought a significant improvement of the pruned models'
worst-detected class accuracy.

    

### [[2102.06202] Private Prediction Sets](http://arxiv.org/abs/2102.06202)


  In real-world settings involving consequential decision-making, the
deployment of machine learning systems generally requires both reliable
uncertainty quantification and protection of individuals' privacy. We present a
framework that treats these two desiderata jointly. Our framework is based on
conformal prediction, a methodology that augments predictive models to return
prediction sets that provide uncertainty quantification -- they provably cover
the true response with a user-specified probability, such as 90%. One might
hope that when used with privately-trained models, conformal prediction would
yield privacy guarantees for the resulting prediction sets; unfortunately this
is not the case. To remedy this key problem, we develop a method that takes any
pre-trained predictive model and outputs differentially private prediction
sets. Our method follows the general approach of split conformal prediction; we
use holdout data to calibrate the size of the prediction sets but preserve
privacy by using a privatized quantile subroutine. This subroutine compensates
for the noise introduced to preserve privacy in order to guarantee correct
coverage. We evaluate the method on large-scale computer vision datasets.

    

### [[2103.09410] Contrastive Learning of Musical Representations](http://arxiv.org/abs/2103.09410)


  While deep learning has enabled great advances in many areas of music,
labeled music datasets remain especially hard, expensive, and time-consuming to
create. In this work, we introduce SimCLR to the music domain and contribute a
large chain of audio data augmentations to form a simple framework for
self-supervised, contrastive learning of musical representations: CLMR. This
approach works on raw time-domain music data and requires no labels to learn
useful representations. We evaluate CLMR in the downstream task of music
classification on the MagnaTagATune and Million Song datasets and present an
ablation study to test which of our music-related innovations over SimCLR are
most effective. A linear classifier trained on the proposed representations
achieves a higher average precision than supervised models on the MagnaTagATune
dataset, and performs comparably on the Million Song dataset. Moreover, we show
that CLMR's representations are transferable using out-of-domain datasets,
indicating that our method has strong generalisability in music classification.
Lastly, we show that the proposed method allows data-efficient learning on
smaller labeled datasets: we achieve an average precision of 33.1% despite
using only 259 labeled songs in the MagnaTagATune dataset (1% of the full
dataset) during linear evaluation. To foster reproducibility and future
research on self-supervised learning in music, we publicly release the
pre-trained models and the source code of all experiments of this paper.

    

### [[2103.10255] Equivariant Filters for Efficient Tracking in 3D Imaging](http://arxiv.org/abs/2103.10255)


  We demonstrate an object tracking method for 3D images with fixed
computational cost and state-of-the-art performance. Previous methods predicted
transformation parameters from convolutional layers. We instead propose an
architecture that does not include either flattening of convolutional features
or fully connected layers, but instead relies on equivariant filters to
preserve transformations between inputs and outputs (e.g. rot./trans. of inputs
rotate/translate outputs). The transformation is then derived in closed form
from the outputs of the filters. This method is useful for applications
requiring low latency, such as real-time tracking. We demonstrate our model on
synthetically augmented adult brain MRI, as well as fetal brain MRI, which is
the intended use-case.

    

### [[2103.14600] Model-Free Learning of Safe yet Effective Controllers](http://arxiv.org/abs/2103.14600)


  We study the problem of learning safe control policies that are also
effective; i.e., maximizing the probability of satisfying a linear temporal
logic (LTL) specification of a task, and the discounted reward capturing the
(classic) control performance. We consider unknown environments modeled as
Markov decision processes. We propose a model-free reinforcement learning
algorithm that learns a policy that first maximizes the probability of ensuring
safety, then the probability of satisfying the given LTL specification and
lastly, the sum of discounted Quality of Control rewards. Finally, we
illustrate applicability of our RL-based approach.

    

### [[2104.00164] Time Series Analysis and Modeling to Forecast: a Survey](http://arxiv.org/abs/2104.00164)


  Time series modeling for predictive purpose has been an active research area
of machine learning for many years. However, no sufficiently comprehensive and
meanwhile substantive survey was offered so far. This survey strives to meet
this need. A unified presentation has been adopted for entire parts of this
compilation.
A red thread guides the reader from time series preprocessing to forecasting.
Time series decomposition is a major preprocessing task, to separate
nonstationary effects (the deterministic components) from the remaining
stochastic constituent, assumed to be stationary. The deterministic components
are predictable and contribute to the prediction through estimations or
extrapolation. Fitting the most appropriate model to the remaining stochastic
component aims at capturing the relationship between past and future values, to
allow prediction.
We cover a sufficiently broad spectrum of models while nonetheless offering
substantial methodological developments. We describe three major linear
parametric models, together with two nonlinear extensions, and present five
categories of nonlinear parametric models. Beyond conventional statistical
models, we highlight six categories of deep neural networks appropriate for
time series forecasting in nonlinear framework.
Finally, we enlighten new avenues of research for time series modeling and
forecasting. We also report software made publicly available for the models
presented.

    

### [[2104.01231] Misclassification-Aware Gaussian Smoothing and Mixed Augmentations improves Robustness against Domain Shifts](http://arxiv.org/abs/2104.01231)


  Deep neural networks achieve high prediction accuracy when the train and test
distributions coincide. In practice though, various types of corruptions can
deviate from this setup and cause severe performance degradations. Few methods
have been proposed to address generalization in the presence of unforeseen
domain shifts. In this paper, we propose a misclassification-aware Gaussian
smoothing approach, coupled with mixed data augmentations, for improving
robustness of image classifiers against a variety of corruptions while still
maintaining high clean accuracy. We show that our method improves upon the
state-of-the-art in robustness and uncertainty calibration on several image
classification benchmarks and network architectures.

    

### [[2104.06506] SAINT-ACC: Safety-Aware Intelligent Adaptive Cruise Control for Autonomous Vehicles Using Deep Reinforcement Learning](http://arxiv.org/abs/2104.06506)


  We present a novel adaptive cruise control (ACC) system namely SAINT-ACC:
{S}afety-{A}ware {Int}elligent {ACC} system (SAINT-ACC) that is designed to
achieve simultaneous optimization of traffic efficiency, driving safety, and
driving comfort through dynamic adaptation of the inter-vehicle gap based on
deep reinforcement learning (RL). A novel dual RL agent-based approach is
developed to seek and adapt the optimal balance between traffic efficiency and
driving safety/comfort by effectively controlling the driving safety model
parameters and inter-vehicle gap based on macroscopic and microscopic traffic
information collected from dynamically changing and complex traffic
environments. Results obtained through over 12,000 simulation runs with varying
traffic scenarios and penetration rates demonstrate that SAINT-ACC
significantly enhances traffic flow, driving safety and comfort compared with a
state-of-the-art approach.

    

### [[2104.09425] Robust Learning Meets Generative Models: Can Proxy Distributions Improve Adversarial Robustness?](http://arxiv.org/abs/2104.09425)


  While additional training data improves the robustness of deep neural
networks against adversarial examples, it presents the challenge of curating a
large number of specific real-world samples. We circumvent this challenge by
using additional data from proxy distributions learned by state-of-the-art
generative models. We first seek to formally understand the transfer of
robustness from classifiers trained on proxy distributions to the real data
distribution. We prove that the difference between the robustness of a
classifier on the two distributions is upper bounded by the conditional
Wasserstein distance between them. Motivated by our result, we next ask how to
empirically select an appropriate generative model? We find that existing
distance metrics, such as FID, fail to correctly determine the robustness
transfer from proxy distributions. We propose a robust discrimination approach,
which measures the distinguishability of synthetic and real samples under
adversarial perturbations. Our approach accurately predicts the robustness
transfer from different proxy distributions. After choosing a proxy
distribution, the next question is which samples are most beneficial? We
successfully optimize this selection by estimating the importance of each
sample in robustness transfer. Finally, using our selection criterion for proxy
distribution and individual samples, we curate a set of ten million most
beneficial synthetic samples for robust training on the CIFAR-10 dataset. Using
this set we improve robust accuracy by up to 7.5% and 6.7% in $\ell_{\infty}$
and $\ell_2$ threat model, and certified robust accuracy by 7.6% in $\ell_2$
threat model over baselines not using proxy distributions on the CIFAR-10
dataset.

    

### [[2104.15135] Explanation-Based Human Debugging of NLP Models: A Survey](http://arxiv.org/abs/2104.15135)


  Debugging a machine learning model is hard since the bug usually involves the
training data and the learning process. This becomes even harder for an opaque
deep learning model if we have no clue about how the model actually works. In
this survey, we review papers that exploit explanations to enable humans to
give feedback and debug NLP models. We call this problem explanation-based
human debugging (EBHD). In particular, we categorize and discuss existing work
along three dimensions of EBHD (the bug context, the workflow, and the
experimental setting), compile findings on how EBHD components affect the
feedback providers, and highlight open problems that could be future research
directions.

    

### [[2105.02702] MIMII DUE: Sound Dataset for Malfunctioning Industrial Machine Investigation and Inspection with Domain Shifts due to Changes in Operational and Environmental Conditions](http://arxiv.org/abs/2105.02702)


  In this paper, we introduce MIMII DUE, a new dataset for malfunctioning
industrial machine investigation and inspection with domain shifts due to
changes in operational and environmental conditions. Conventional methods for
anomalous sound detection face practical challenges because the distribution of
features changes between the training and operational phases (called domain
shift) due to various real-world factors. To check the robustness against
domain shifts, we need a dataset that actually includes domain shifts, but such
a dataset does not exist so far. The new dataset we created consists of the
normal and abnormal operating sounds of five different types of industrial
machines under two different operational/environmental conditions (source
domain and target domain) independent of normal/abnormal, with domain shifts
occurring between the two domains. Experimental results showed significant
performance differences between the source and target domains, indicating that
the dataset contains the domain shifts. These findings demonstrate that the
dataset will be helpful for checking the robustness against domain shifts. The
dataset is a subset of the dataset for DCASE 2021 Challenge Task 2 and freely
available for download at this https URL


### [[2105.03361] What Kinds of Functions do Deep Neural Networks Learn? Insights from Variational Spline Theory](http://arxiv.org/abs/2105.03361)


  We develop a variational framework to understand the properties of functions
learned by fitting deep neural networks with rectified linear unit activations
to data. We propose a new function space, which is reminiscent of classical
bounded variation-type spaces, that captures the compositional structure
associated with deep neural networks. We derive a representer theorem showing
that deep ReLU networks are solutions to regularized data fitting problems over
functions from this space. The function space consists of compositions of
functions from the Banach spaces of second-order bounded variation in the Radon
domain. These are Banach spaces with sparsity-promoting norms, giving insight
into the role of sparsity in deep neural networks. The neural network solutions
have skip connections and rank bounded weight matrices, providing new
theoretical support for these common architectural choices. The variational
problem we study can be recast as a finite-dimensional neural network training
problem with regularization schemes related to the notions of weight decay and
path-norm regularization. Finally, our analysis builds on techniques from
variational spline theory, providing new connections between deep neural
networks and splines.

    

### [[2105.04187] A Rigorous Information-Theoretic Definition of Redundancy and Relevancy in Feature Selection Based on (Partial) Information Decomposition](http://arxiv.org/abs/2105.04187)


  Selecting a minimal feature set that is maximally informative about a target
variable is a central task in machine learning and statistics. Information
theory provides a powerful framework for formulating feature selection
algorithms -- yet, a rigorous, information-theoretic definition of feature
relevancy, which accounts for feature interactions such as redundant and
synergistic contributions, is still missing. We argue that this lack is
inherent to classical information theory which does not provide measures to
decompose the information a set of variables provides about a target into
unique, redundant, and synergistic contributions. Such a decomposition has been
introduced only recently by the partial information decomposition (PID)
framework. Using PID, we clarify why feature selection is a conceptually
difficult problem when approached using information theory and provide a novel
definition of feature relevancy and redundancy in PID terms. From this
definition, we show that the conditional mutual information (CMI) maximizes
relevancy while minimizing redundancy and propose an iterative, CMI-based
algorithm for practical feature selection. We demonstrate the power of our
CMI-based algorithm in comparison to the unconditional mutual information on
benchmark examples and provide corresponding PID estimates to highlight how PID
allows to quantify information contribution of features and their interactions
in feature-selection problems.

    

### [[2105.05650] Unbiased Monte Carlo Cluster Updates with Autoregressive Neural Networks](http://arxiv.org/abs/2105.05650)


  Efficient sampling of complex high-dimensional probability densities is a
central task in computational science. Machine Learning techniques based on
autoregressive neural networks have been recently shown to provide good
approximations of probability distributions of interest in physics. In this
work, we propose a systematic way to remove the intrinsic bias associated with
these variational approximations, combining it with Markov-chain Monte Carlo in
an automatic scheme to efficiently generate cluster updates, which is
particularly useful for models for which no efficient cluster update scheme is
known. Our approach is based on symmetry-enforced cluster updates building on
the neural-network representation of conditional probabilities. We demonstrate
that such finite-cluster updates are crucial to circumvent ergodicity problems
associated with global neural updates. We test our method for first- and
second-order phase transitions in classical spin systems, proving in particular
its viability for critical systems, or in the presence of metastable states.

    

### [[2105.10142] Safety Metrics for Semantic Segmentation in Autonomous Driving](http://arxiv.org/abs/2105.10142)


  Within the context of autonomous driving, safety-related metrics for deep
neural networks have been widely studied for image classification and object
detection. In this paper, we further consider safety-aware correctness and
robustness metrics specialized for semantic segmentation. The novelty of our
proposal is to move beyond pixel-level metrics: Given two images with each
having N pixels being class-flipped, the designed metrics should, depending on
the clustering of pixels being class-flipped or the location of occurrence,
reflect a different level of safety criticality. The result evaluated on an
autonomous driving dataset demonstrates the validity and practicality of our
proposed methodology.

    

### [[2105.14162] EDDA: Explanation-driven Data Augmentation to Improve Explanation Faithfulness](http://arxiv.org/abs/2105.14162)


  Recent years have seen the introduction of a range of methods for post-hoc
explainability of image classifier predictions. However, these post-hoc
explanations may not always be faithful to classifier predictions, which poses
a significant challenge when attempting to debug models based on such
explanations. To this end, we seek a methodology that can improve the
faithfulness of an explanation method with respect to model predictions which
does not require ground truth explanations. We achieve this through a novel
explanation-driven data augmentation (EDDA) technique that augments the
training data with occlusions inferred from model explanations; this is based
on the simple motivating principle that \emph{if} the explainer is faithful to
the model \emph{then} occluding salient regions for the model prediction should
decrease the model confidence in the prediction, while occluding non-salient
regions should not change the prediction. To verify that the proposed
augmentation method has the potential to improve faithfulness, we evaluate EDDA
using a variety of datasets and classification models. We demonstrate
empirically that our approach leads to a significant increase of faithfulness,
which can facilitate better debugging and successful deployment of image
classification models in real-world applications.

    

### [[2106.02266] SAND-mask: An Enhanced Gradient Masking Strategy for the Discovery of Invariances in Domain Generalization](http://arxiv.org/abs/2106.02266)


  A major bottleneck in the real-world applications of machine learning models
is their failure in generalizing to unseen domains whose data distribution is
not i.i.d to the training domains. This failure often stems from learning
non-generalizable features in the training domains that are spuriously
correlated with the label of data. To address this shortcoming, there has been
a growing surge of interest in learning good explanations that are hard to
vary, which is studied under the notion of Out-of-Distribution (OOD)
Generalization. The search for good explanations that are \textit{invariant}
across different domains can be seen as finding local (global) minimas in the
loss landscape that hold true across all of the training domains. In this
paper, we propose a masking strategy, which determines a continuous weight
based on the agreement of gradients that flow in each edge of network, in order
to control the amount of update received by the edge in each step of
optimization. Particularly, our proposed technique referred to as "Smoothed-AND
(SAND)-masking", not only validates the agreement in the direction of gradients
but also promotes the agreement among their magnitudes to further ensure the
discovery of invariances across training domains. SAND-mask is validated over
the Domainbed benchmark for domain generalization and significantly improves
the state-of-the-art accuracy on the Colored MNIST dataset while providing
competitive results on other domain generalization datasets.

    

### [[2106.04492] Description and Discussion on DCASE 2021 Challenge Task 2: Unsupervised Anomalous Sound Detection for Machine Condition Monitoring under Domain Shifted Conditions](http://arxiv.org/abs/2106.04492)


  We present the task description and discussion on the results of the DCASE
2021 Challenge Task 2. In 2020, we organized an unsupervised anomalous sound
detection (ASD) task, identifying whether a given sound was normal or anomalous
without anomalous training data. In 2021, we organized an advanced unsupervised
ASD task under domain-shift conditions, which focuses on the inevitable problem
of the practical use of ASD systems. The main challenge of this task is to
detect unknown anomalous sounds where the acoustic characteristics of the
training and testing samples are different, i.e., domain-shifted. This problem
frequently occurs due to changes in seasons, manufactured products, and/or
environmental noise. We received 75 submissions from 26 teams, and several
novel approaches have been developed in this challenge. On the basis of the
analysis of the evaluation results, we found that there are two types of
remarkable approaches that TOP-5 winning teams adopted: 1) ensemble approaches
of ``outlier exposure'' (OE)-based detectors and ``inlier modeling'' (IM)-based
detectors and 2) approaches based on IM-based detection for features learned in
a machine-identification task.

    

### [[2106.08060] Privacy Assessment of Federated Learning using Private Personalized Layers](http://arxiv.org/abs/2106.08060)


  Federated Learning (FL) is a collaborative scheme to train a learning model
across multiple participants without sharing data. While FL is a clear step
forward towards enforcing users' privacy, different inference attacks have been
developed. In this paper, we quantify the utility and privacy trade-off of a FL
scheme using private personalized layers. While this scheme has been proposed
as local adaptation to improve the accuracy of the model through local
personalization, it has also the advantage to minimize the information about
the model exchanged with the server. However, the privacy of such a scheme has
never been quantified. Our evaluations using motion sensor dataset show that
personalized layers speed up the convergence of the model and slightly improve
the accuracy for all users compared to a standard FL scheme while better
preventing both attribute and membership inferences compared to a FL scheme
using local differential privacy.

    

### [[2106.11732] FLEA: Provably Fair Multisource Learning from Unreliable Training Data](http://arxiv.org/abs/2106.11732)


  Fairness-aware learning aims at constructing classifiers that not only make
accurate predictions, but also do not discriminate against specific groups. It
is a fast-growing area of machine learning with far-reaching societal impact.
However, existing fair learning methods are vulnerable to accidental or
malicious artifacts in the training data, which can cause them to unknowingly
produce unfair classifiers. In this work we address the problem of fair
learning from unreliable training data in the robust multisource setting, where
the available training data comes from multiple sources, a fraction of which
might not be representative of the true data distribution. We introduce FLEA, a
filtering-based algorithm that allows the learning system to identify and
suppress those data sources that would have a negative impact on fairness or
accuracy if they were used for training. We show the effectiveness of our
approach by a diverse range of experiments on multiple datasets. Additionally,
we prove formally that - given enough data - FLEA protects the learner against
corruptions as long as the fraction of affected data sources is less than half.

    

### [[2106.12062] A Practical & Unified Notation for Information-Theoretic Quantities in ML](http://arxiv.org/abs/2106.12062)


  Information theory is of importance to machine learning, but the notation for
information-theoretic quantities is sometimes opaque. The right notation can
convey valuable intuitions and concisely express new ideas. We propose such a
notation for machine learning users and expand it to include
information-theoretic quantities between observed outcomes (events) and random
variables. To demonstrate the value of our notation, first, we apply it to
elegantly prove a version of Stirling's approximation for binomial coefficients
mentioned by MacKay. Second, we apply the notation to a popular
information-theoretic acquisition function in Bayesian active learning which
selects the most informative (unlabelled) samples to be labelled by an expert
and extend this acquisition function to the core-set problem, which consists of
selecting the most informative samples \emph{given} the labels.

    

### [[2106.13301] Physics perception in sloshing scenes with guaranteed thermodynamic consistency](http://arxiv.org/abs/2106.13301)


  Physics perception very often faces the problem that only limited data or
partial measurements on the scene are available. In this work, we propose a
strategy to learn the full state of sloshing liquids from measurements of the
free surface. Our approach is based on recurrent neural networks (RNN) that
project the limited information available to a reduced-order manifold so as to
not only reconstruct the unknown information, but also to be capable of
performing fluid reasoning about future scenarios in real time. To obtain
physically consistent predictions, we train deep neural networks on the
reduced-order manifold that, through the employ of inductive biases, ensure the
fulfillment of the principles of thermodynamics. RNNs learn from history the
required hidden information to correlate the limited information with the
latent space where the simulation occurs. Finally, a decoder returns data back
to the high-dimensional manifold, so as to provide the user with insightful
information in the form of augmented reality. This algorithm is connected to a
computer vision system to test the performance of the proposed methodology with
real information, resulting in a system capable of understanding and predicting
future states of the observed fluid in real-time.

    

### [[2106.15367] MAML is a Noisy Contrastive Learner](http://arxiv.org/abs/2106.15367)


  Model-agnostic meta-learning (MAML) is one of the most popular and
widely-adopted meta-learning algorithms nowadays, which achieves remarkable
success in various learning problems. Yet, with the unique design of nested
inner-loop and outer-loop updates which respectively govern the task-specific
and meta-model-centric learning, the underlying learning objective of MAML
still remains implicit and thus impedes a more straightforward understanding of
it. In this paper, we provide a new perspective to the working mechanism of
MAML and discover that: MAML is analogous to a meta-learner using a supervised
contrastive objective function, where the query features are pulled towards the
support features of the same class and against those of different classes, in
which such contrastiveness is experimentally verified via an analysis based on
the cosine similarity. Moreover, our analysis reveals that the vanilla MAML
algorithm has an undesirable interference term originating from the random
initialization and the cross-task interaction. We therefore propose a simple
but effective technique, zeroing trick, to alleviate such interference, where
the extensive experiments are then conducted on both miniImagenet and Omniglot
datasets to demonstrate the consistent improvement brought by our proposed
technique thus well validating its effectiveness.

    

### [[2107.00272] A Survey on Graph-Based Deep Learning for Computational Histopathology](http://arxiv.org/abs/2107.00272)


  With the remarkable success of representation learning for prediction
problems, we have witnessed a rapid expansion of the use of machine learning
and deep learning for the analysis of digital pathology and biopsy image
patches. However, learning over patch-wise features using convolutional neural
networks limits the ability of the model to capture global contextual
information and comprehensively model tissue composition. The phenotypical and
topological distribution of constituent histological entities play a critical
role in tissue diagnosis. As such, graph data representations and deep learning
have attracted significant attention for encoding tissue representations, and
capturing intra- and inter- entity level interactions. In this review, we
provide a conceptual grounding for graph analytics in digital pathology,
including entity-graph construction and graph architectures, and present their
current success for tumor localization and classification, tumor invasion and
staging, image retrieval, and survival prediction. We provide an overview of
these methods in a systematic manner organized by the graph representation of
the input image, scale, and organ on which they operate. We also outline the
limitations of existing techniques, and suggest potential future research
directions in this domain.

    

### [[2107.00465] Physics-Informed Neural Networks for Minimising Worst-Case Violations in DC Optimal Power Flow](http://arxiv.org/abs/2107.00465)


  Physics-informed neural networks exploit the existing models of the
underlying physical systems to generate higher accuracy results with fewer
data. Such approaches can help drastically reduce the computation time and
generate a good estimate of computationally intensive processes in power
systems, such as dynamic security assessment or optimal power flow. Combined
with the extraction of worst-case guarantees for the neural network
performance, such neural networks can be applied in safety-critical
applications in power systems and build a high level of trust among power
system operators. This paper takes the first step and applies, for the first
time to our knowledge, Physics-Informed Neural Networks with Worst-Case
Guarantees for the DC Optimal Power Flow problem. We look for guarantees
related to (i) maximum constraint violations, (ii) maximum distance between
predicted and optimal decision variables, and (iii) maximum sub-optimality in
the entire input domain. In a range of PGLib-OPF networks, we demonstrate how
physics-informed neural networks can be supplied with worst-case guarantees and
how they can lead to reduced worst-case violations compared with conventional
neural networks.

    

### [[2107.09044] Just Train Twice: Improving Group Robustness without Training Group Information](http://arxiv.org/abs/2107.09044)


  Standard training via empirical risk minimization (ERM) can produce models
that achieve high accuracy on average but low accuracy on certain groups,
especially in the presence of spurious correlations between the input and
label. Prior approaches that achieve high worst-group accuracy, like group
distributionally robust optimization (group DRO) require expensive group
annotations for each training point, whereas approaches that do not use such
group annotations typically achieve unsatisfactory worst-group accuracy. In
this paper, we propose a simple two-stage approach, JTT, that first trains a
standard ERM model for several epochs, and then trains a second model that
upweights the training examples that the first model misclassified.
Intuitively, this upweights examples from groups on which standard ERM models
perform poorly, leading to improved worst-group performance. Averaged over four
image classification and natural language processing tasks with spurious
correlations, JTT closes 75% of the gap in worst-group accuracy between
standard ERM and group DRO, while only requiring group annotations on a small
validation set in order to tune hyperparameters.

    

### [[2109.12405] CoMeT: An Integrated Interval Thermal Simulation Toolchain for 2D, 2.5D, and 3D Processor-Memory Systems](http://arxiv.org/abs/2109.12405)


  Processing cores and the accompanying main memory working in tandem enable
the modern processors. Dissipating heat produced from computation, memory
access remains a significant problem for processors. Therefore, processor
thermal management continues to be an active research topic. Most thermal
management research takes place using simulations, given the challenges of
measuring temperature in real processors. Since core and memory are fabricated
on separate packages in most existing processors, with the memory having lower
power densities, thermal management research in processors has primarily
focused on the cores.
Memory bandwidth limitations associated with 2D processors lead to
high-density 2.5D and 3D packaging technology. 2.5D packaging places cores and
memory on the same package. 3D packaging technology takes it further by
stacking layers of memory on the top of cores themselves. Such packagings
significantly increase the power density, making processors prone to heating.
Therefore, mitigating thermal issues in high-density processors (packaged with
stacked memory) becomes an even more pressing problem. However, given the lack
of thermal modeling for memories in existing interval thermal simulation
toolchains, they are unsuitable for studying thermal management for
high-density processors.
To address this issue, we present CoMeT, the first integrated Core and Memory
interval Thermal simulation toolchain. CoMeT comprehensively supports thermal
simulation of high- and low-density processors corresponding to four different
core-memory configurations - off-chip DDR memory, off-chip 3D memory, 2.5D, and
3D. CoMeT supports several novel features that facilitate overlying system
research. Compared to an equivalent state-of-the-art core-only toolchain, CoMeT
adds only a ~5% simulation-time overhead. The source code of CoMeT has been
made open for public use under the MIT license.

    

### [[2109.12506] A Simple Self-calibration Method for The Internal Time Synchronization of MEMS LiDAR](http://arxiv.org/abs/2109.12506)


  This paper proposes a simple self-calibration method for the internal time
synchronization of MEMS(Micro-electromechanical systems) LiDAR during research
and development. Firstly, we introduced the problem of internal time
misalignment in MEMS lidar. Then, a robust Minimum Vertical Gradient(MVG) prior
is proposed to calibrate the time difference between the laser and MEMS mirror,
which can be calculated automatically without any artificial participation or
specially designed cooperation target. Finally, actual experiments on MEMS
LiDARs are implemented to demonstrate the effectiveness of the proposed method.
It should be noted that the calibration can be implemented in a simple
laboratory environment without any ranging equipment and artificial
participation, which greatly accelerate the progress of research and
development in practical applications.

    

### [[2109.12697] HARP: Practically and Effectively Identifying Uncorrectable Errors in Memory Chips That Use On-Die Error-Correcting Codes](http://arxiv.org/abs/2109.12697)


  State-of-the-art techniques for addressing scaling-related main memory errors
identify and repair bits that are at risk of error from within the memory
controller. Unfortunately, modern main memory chips internally use on-die error
correcting codes (on-die ECC) that obfuscate the memory controller's view of
errors, complicating the process of identifying at-risk bits (i.e., error
profiling). To understand the problems that on-die ECC causes for error
profiling, we analytically study how on-die ECC changes the way that memory
errors appear outside of the memory chip (e.g., to the memory controller). We
show that on-die ECC introduces statistical dependence between errors in
different bit positions, raising three key challenges for practical and
effective error profiling.
To address the three challenges, we introduce Hybrid Active-Reactive
Profiling (HARP), a new error profiling algorithm that rapidly achieves full
coverage of at-risk bits in memory chips that use on-die ECC. HARP separates
error profiling into two phases: (1) using existing profiling techniques with
the help of small modifications to the on-die ECC mechanism to quickly identify
a subset of at-risk bits; and (2) using a secondary ECC within the memory
controller to safely identify the remaining at-risk bits, if and when they
fail. Our evaluations show that HARP achieves full coverage of all at-risk bits
faster (e.g., 99th-percentile coverage 20.6%/36.4%/52.9%/62.1% faster, on
average, given 2/3/4/5 raw bit errors per ECC word) than two state-of-the-art
baseline error profiling algorithms, which sometimes fail to achieve full
coverage. We perform a case study of how each profiler impacts the system's
overall bit error rate (BER) when using a repair mechanism to tolerate DRAM
data-retention errors. We show that HARP outperforms the best baseline
algorithm (e.g., by 3.7x for a raw per-bit error probability of 0.75).

    

### [[2109.12167] Invited Paper: Failure is (literally) an Option: Atomic Commitment vs Optionality in Decentralized Finance](http://arxiv.org/abs/2109.12167)


  Many aspects of blockchain-based decentralized finance can be understood as
an extension of classical distributed computing. In this paper, we trace the
evolution of two interrelated notions: failure and fault-tolerance. In
classical distributed computing, a failure to complete a multi-party protocol
is typically attributed to hardware malfunctions. A fault-tolerant protocol is
one that responds to such failures by rolling the system back to an earlier
consistent state. In the presence of Byzantine failures, a failure may be the
result of an attack, and a fault-tolerant protocol is one that ensures that
attackers will be punished and victims compensated. In modern decentralized
finance however, failure to complete a protocol can be considered a legitimate
option, not a transgression. A fault-tolerant protocol is one that ensures that
the party offering the option cannot renege, and the party purchasing the
option provides fair compensation (in the form of a fee) to the offering party.
We sketch the evolution of such protocols, starting with two-phase commit, and
finishing with timed hashlocked smart contracts.

    

### [[2109.12185] Message Delivery in the Plane by Robots with Different Speeds](http://arxiv.org/abs/2109.12185)


  We study a fundamental cooperative message-delivery problem on the plane.
Assume $n$ robots which can move in any direction, are placed arbitrarily on
the plane. Robots each have their own maximum speed and can communicate with
each other face-to-face (i.e., when they are at the same location at the same
time). There are also two designated points on the plane, $S$ (the source) and
$D$ (the destination). The robots are required to transmit the message from the
source to the destination as quickly as possible by face-to-face message
passing. We consider both the offline setting where all information (the
locations and maximum speeds of the robots) are known in advance and the online
setting where each robot knows only its own position and speed along with the
positions of $S$ and $D$.
In the offline case, we discover an important connection between the problem
for two-robot systems and the well-known Apollonius circle which we employ to
design an optimal algorithm. We also propose a $\sqrt 2$ approximation
algorithm for systems with any number of robots. In the online setting, we
provide an algorithm with competitive ratio $\frac 17 \left( 5+ 4 \sqrt{2}
\right)$ for two-robot systems and show that the same algorithm has a
competitive ratio less than $2$ for systems with any number of robots. We also
show these results are tight for the given algorithm. Finally, we give two
lower bounds (employing different arguments) on the competitive ratio of any
online algorithm, one of $1.0391$ and the other of $1.0405$.

    

### [[2109.12186] Aristotle Cloud Federation: Container Runtimes Technical Report](http://arxiv.org/abs/2109.12186)


  A National Science Foundation-sponsored container runtimes investigation was
conducted by the Aristotle Cloud Federation to better understand the challenges
of selecting and using Docker, Singularity, and X-Containers. The main goal of
this investigation was to identify the "pain points" experienced by users when
selecting and using containers for scientific research and to share lessons
learned. Application performance characteristics are included in this report as
well as user experiences with Kubernetes and container orchestration on cloud
and HPC platforms. Scientists, research computing practitioners, and educators
may find value in this report when considering the use and/or deployment of
containers or when preparing students to meet the unique challenges of using
containers in scientific research.

    

### [[2109.12194] Universal Payment Channels: An Interoperability Platform for Digital Currencies](http://arxiv.org/abs/2109.12194)


  With the innovation of distributed ledger technology (DLT), often known as
blockchain technology, there has been significant growth of digital tokens in
the form of cryptocurrencies, stablecoins, and central bank digital currencies.
As the number of DLT networks increases, each with varying design
characteristics, the likelihood that transacting parties are on the same
network decreases. Thus, it is crucial to facilitate payments that are
universal across networks, scalable to massive loads, and highly available. We
envision a future payment network that may be built on top of DLT networks
without being subject to their limitations on interoperability, scalability,
and availability faced by DLT payment solutions today. Specifically, we propose
a hub-and-spoke payment route, referred to here as Universal Payment Channels
(UPC), that can be used to support digital token transfers of funds across
different networks through payment channels. We further discuss the potential
use cases of the UPC technology to support, and not complicate, an already
robust digital payment ecosystem. Finally, through the paper, we share some
future directions of the UPC technology.

    

### [[2109.12195] A Manifesto for Modern Fog and Edge Computing: Vision, New Paradigms, Opportunities, and Future Directions](http://arxiv.org/abs/2109.12195)


  The advancements in the use of Internet of Things (IoT) devices is increasing
continuously and generating huge amounts of data in a fast manner. Cloud
computing is an important paradigm which processes and manages user data
effectively. Further, fog and edge computing paradigms are introduced to
improve user service by reducing latency and response time. This chapter
presents a manifesto for modern fog and edge computing systems based on the
current research trends. Further, architectures and applications of fog and
edge computing are explained. Moreover, research opportunities and promising
future directions are presented with respect to the new paradigms, which will
be helpful for practitioners, researchers, and academicians to continue their
research.

    

### [[2109.12259] NUMA-aware FFT-based Convolution on ARMv8 Many-core CPUs](http://arxiv.org/abs/2109.12259)


  Convolutional Neural Networks (CNNs), one of the most representative
algorithms of deep learning, are widely used in various artificial intelligence
applications. Convolution operations often take most of the computational
overhead of CNNs. The FFT-based algorithm can improve the efficiency of
convolution by reducing its algorithm complexity, there are a lot of works
about the high-performance implementation of FFT-based convolution on many-core
CPUs. However, there is no optimization for the non-uniform memory access
(NUMA) characteristics in many-core CPUs. In this paper, we present a
NUMA-aware FFT-based convolution implementation on ARMv8 many-core CPUs with
NUMA architectures. The implementation can reduce a number of remote memory
access through the data reordering of FFT transformations and the three-level
parallelization of the complex matrix multiplication. The experiment results on
a ARMv8 many-core CPU with NUMA architectures demonstrate that our NUMA-aware
implementation has much better performance than the state-of-the-art work in
most cases.

    

### [[2109.12289] Asynchronous Gathering Algorithms for Autonomous Mobile Robots with Lights](http://arxiv.org/abs/2109.12289)


  We consider a Gathering problem for n autonomous mobile robots with
persistent memory called light in an asynchronous scheduler (ASYNC). It is well
known that Gathering is impossible when robots have no lights in basic common
models, if the system is semi-synchronous (SSYNC) or even centralized (only one
robot is active in each time). It is known that Gathering can be solved by
robots with 10 colors of lights in ASYNC. This result is obtained by combining
the following results. (1) The simulation of SSYNC robots with k colors by
ASYNC robots with 5k colors, and (2) Gathering is solved by SSYNC robots with 2
colors.
In this paper, we improve the result by reducing the number of colors and
show that Gathering can be solved by ASYNC robots with 3 colors of lights. We
also show that we can construct a simulation algorithm of any unfair SSYNC
algorithm using k colors by ASYNC robots with 3k colors, where unfairness does
not guarantee that every robot is activated infinitely often. Combining this
simulation and the Gathering algorithm by SSYNC robots with 2 colors, we obtain
a Gathering algorithm by ASYNC robots with 6 colors. Our main result can be
obtained by reducing the number of colors from 6 to 3.

    

### [[2109.12375] Local Learning at the Network Edge for Efficient & Secure Real-Time Predictive Analytics](http://arxiv.org/abs/2109.12375)


  The ability to perform computation on devices, such as smartphones, cars, or
other nodes present at the Internet of Things leads to constraints regarding
bandwidth, storage, and energy, as most of these devices are mobile and operate
on batteries. Using their computational power to perform locally machine
learning and analytics tasks can enable accurate and real-time predictions at
the network edge. A trained machine learning model requires high accuracy
towards the prediction outcome, as wrong decisions can lead to negative
consequences on the efficient conclusion of applications. Most of the data
sensed in these devices are contextual and personal requiring
privacy-preserving without their distribution over the network. When working
with these privacy-preserving data, not only the protection is important but,
also, the model needs the ability to adapt to regular occurring concept drifts
and data distribution changes to guarantee a high accuracy of the prediction
outcome. We address the importance of personalization and generalization in
edge devices to adapt to data distribution updates over continuously evolving
environments. The methodology we propose relies on the principles of Federated
Learning and Optimal Stopping Theory extended with a personalization component.
The privacy-efficient and quality-awareness of personalization and
generalization is the overarching aim of this work.

    

### [[2109.12443] Basil: Breaking up BFT with ACID (transactions)](http://arxiv.org/abs/2109.12443)


  This paper presents Basil, the first transactional, leaderless Byzantine
Fault Tolerant key-value store. Basil leverages ACID transactions to scalably
implement the abstraction of a trusted shared log in the presence of Byzantine
actors. Unlike traditional BFT approaches, Basil executes non-conflicting
operations in parallel and commits transactions in a single round-trip during
fault-free executions. Basil improves throughput over traditional BFT systems
by four to five times, and is only four times slower than TAPIR, a
non-Byzantine replicated system. Basil's novel recovery mechanism further
minimizes the impact of failures: with 30% Byzantine clients, throughput drops
by less than 25% in the worst-case.

    

### [[2109.12454] Good-case and Bad-case Latency of Unauthenticated Byzantine Broadcast: A Complete Categorization](http://arxiv.org/abs/2109.12454)


  This paper studies the {\em good-case latency} of {\em unauthenticated}
Byzantine fault-tolerant broadcast, which measures the time it takes for all
non-faulty parties to commit given a non-faulty broadcaster. For both
asynchrony and synchrony, we show that $n\geq 4f$ is the tight resilience
threshold that separates good-case 2 rounds and 3 rounds. For asynchronous
Byzantine reliable broadcast (BRB), we also investigate the {\em bad-case
latency} for all non-faulty parties to commit when the broadcaster is faulty
but some non-faulty party commits. We provide matching upper and lower bounds
on the resilience threshold of bad-case latency for BRB protocols with optimal
good-case latency of 2 rounds. In particular, we show 2 impossibility results
and propose 4 asynchronous BRB protocols.

    

### [[2109.12616] Rabia: Simplifying State-Machine Replication Through Randomization](http://arxiv.org/abs/2109.12616)


  We introduce Rabia, a simple and high performance framework for implementing
state-machine replication (SMR) within a datacenter. The main innovation of
Rabia is in using randomization to simplify the design. Rabia provides the
following two features: (i) It does not need any fail-over protocol and
supports trivial auxiliary protocols like log compaction, snapshotting, and
reconfiguration, components that are often considered the most challenging when
developing SMR systems; and (ii) It provides high performance, up to 1.5x
higher throughput than the closest competitor (i.e., EPaxos) in a favorable
setup (same availability zone with three replicas) and is comparable with a
larger number of replicas or when deployed in multiple availability zones.

    

### [[2109.12626] A Doubly-pipelined, Dual-root Reduction-to-all Algorithm and Implementation](http://arxiv.org/abs/2109.12626)


  We discuss a simple, binary tree-based algorithm for the collective allreduce
(reduction-to-all, MPI_Allreduce) operation for parallel systems consisting of
$p$ suitably interconnected processors. The algorithm can be doubly pipelined
to exploit bidirectional (telephone-like) communication capabilities of the
communication system. In order to make the algorithm more symmetric, the
processors are organized into two rooted trees with communication between the
two roots. For each pipeline block, each non-leaf processor takes three
communication steps, consisting in receiving and sending from and to the two
children, and sending and receiving to and from the root. In a round-based,
uniform, linear-cost communication model in which simultaneously sending and
receiving $n$ data elements takes time $\alpha+\beta n$ for system dependent
constants $\alpha$ (communication start-up latency) and $\beta$ (time per
element), the time for the allreduce operation on vectors of $m$ elements is
$O(\log p+\sqrt{m\log p})+3\beta m$ by suitable choice of the pipeline block
size. We compare the performance of an implementation in MPI to similar reduce
followed by broadcast algorithms, and the native MPI_Allreduce collective on a
modern, small $36\times 32$ processor cluster. With proper choice of the number
of pipeline blocks, it is possible to achieve better performance than pipelined
algorithms that do not exploit bidirectional communication.

    

### [[2109.12664] BigBFT: A Multileader Byzantine Fault Tolerance Protocol for High Throughput](http://arxiv.org/abs/2109.12664)


  This paper describes BigBFT, a multi-leader Byzantine fault tolerance
protocol that achieves high throughput and scalable consensus in blockchain
systems. BigBFT achieves this by (1) enabling every node to be a leader that
can propose and order the blocks in parallel, (2) piggybacking votes within
rounds, (3) pipelining blocks across rounds, and (4) using only two
communication steps to order blocks in the common case.
BigBFT has an amortized communication cost of $O(n)$ over $n$ requests. We
evaluate BigBFT's performance both analytically, using back-of-the-envelope
load formulas to construct a cost analysis, and also empirically by
implementing it in our PaxiBFT framework. Our evaluation compares BigBFT with
PBFT, Tendermint, Streamlet, and Hotstuff under various workloads using
deployments of 4 to 20 nodes. Our results show that BigBFT outperforms PBFT,
Tendermint, Streamlet, and Hotstuff protocols either in terms of latency (by up
to $40\%$) or in terms of throughput (by up to $190\%$).

    

### [[1907.02805] Energy of Computing on Multicore CPUs: Predictive Models and Energy Conservation Law](http://arxiv.org/abs/1907.02805)


  Energy is now a first-class design constraint along with performance in all
computing settings. Energy predictive modelling based on performance monitoring
counts (PMCs) is the leading method used for prediction of energy consumption
during an application execution. We use a model-theoretic approach to formulate
the assumed properties of existing models in a mathematical form. We extend the
formalism by adding properties, heretofore unconsidered, that account for a
limited form of energy conservation law. The extended formalism defines our
theory of energy of computing. By applying the basic practical implications of
the theory, we improve the prediction accuracy of state-of-the-art energy
models from 31% to 18%. We also demonstrate that use of state-of-the-art
measurement tools for energy optimisation may lead to significant losses of
energy (ranging from 56% to 65% for applications used in experiments) since
they do not take into account the energy conservation properties.

    

### [[2102.07528] Byzantine Dispersion on Graphs](http://arxiv.org/abs/2102.07528)


  This paper considers the problem of Byzantine dispersion and extends previous
work along several parameters. The problem of Byzantine dispersion asks: given
$n$ robots, up to $f$ of which are Byzantine, initially placed arbitrarily on
an $n$ node anonymous graph, design a terminating algorithm to be run by the
robots such that they eventually reach a configuration where each node has at
most one non-Byzantine robot on it.
Previous work solved this problem for rings and tolerated up to $n-1$
Byzantine robots. In this paper, we investigate the problem on more general
graphs. We first develop an algorithm that tolerates up to $n-1$ Byzantine
robots and works for a more general class of graphs.
We then develop an algorithm that works for any graph but tolerates a lesser
number of Byzantine robots.
We subsequently turn our focus to the strength of the Byzantine robots.
Previous work considers only ``weak" Byzantine robots that cannot fake their
IDs. We develop an algorithm that solves the problem when Byzantine robots are
not weak and can fake IDs.
Finally, we study the situation where the number of the robots is not $n$ but
some $k$. We show that in such a scenario, the number of Byzantine robots that
can be tolerated is severely restricted. Specifically, we show that it is
impossible to deterministically solve Byzantine dispersion when $\lceil k/n
\rceil > \lceil (k-f)/n \rceil$.

    

### [[2109.09527] An Improved and Optimized Practical Non-Blocking PageRank Algorithm for Massive Graphs](http://arxiv.org/abs/2109.09527)


  PageRank is a well-known algorithm whose robustness helps set a standard
benchmark when processing graphs and analytical problems. The PageRank
algorithm serves as a standard for many graph analytics and a foundation for
extracting graph features and predicting user ratings in recommendation
systems. The PageRank algorithm iterates continuously, updating the ranks of
the pages till convergence is achieved. Nevertheless, the implementation of the
PageRank algorithm on large-scale graphs that on shared memory architecture
utilizing fine-grained parallelism is a difficult task at hand. The
experimental study and analysis of the Parallel PageRank kernel on large graphs
and shared memory architectures using different programming models have been
studied extensively. This paper presents the asynchronous execution of the
PageRank algorithm to leverage the computations on massive graphs, especially
on shared memory architectures. We evaluate the performance of our proposed
non-blocking algorithms for PageRank computation on real-world and synthetic
datasets using Posix Multithreaded Library on a 56 core Intel(R) Xeon
processor. We observed that our asynchronous implementations achieve 10x to 30x
speedup with respect to sequential runs and 5x to 10x improvements over
synchronous variants.

    

### [[2109.12113] Identifying Women with Mammographically-Occult Breast Cancer Leveraging GAN-Simulated Mammograms](http://arxiv.org/abs/2109.12113)


  Our objective is to show the feasibility of using simulated mammograms to
detect mammographically-occult (MO) cancer in women with dense breasts and a
normal screening mammogram who could be triaged for additional screening with
magnetic resonance imaging (MRI) or ultrasound. We developed a Conditional
Generative Adversarial Network (CGAN) to simulate a mammogram with normal
appearance using the opposite mammogram as the condition. We used a
Convolutional Neural Network (CNN) trained on Radon Cumulative Distribution
Transform (RCDT) processed mammograms to detect MO cancer. For training CGAN,
we used screening mammograms of 1366 women. For MO cancer detection, we used
screening mammograms of 333 women (97 MO cancer) with dense breasts. We
simulated the right mammogram for normal controls and the cancer side for MO
cancer cases. We created two RCDT images, one from a real mammogram pair and
another from a real-simulated mammogram pair. We finetuned a VGG16 on resulting
RCDT images to classify the women with MO cancer. We compared the
classification performance of the CNN trained on fused RCDT images, CNN_{Fused}
to that of trained only on real RCDT images, CNN_{Real}, and to that of trained
only on simulated RCDT images, CNN_{Simulated}. The test AUC for CNN_{Fused}
was 0.77 with a 95% confidence interval (95CI) of [0.71, 0.83], which was
statistically better (p-value < 0.02) than the CNN_{Real} AUC of 0.70 with a
95CI of [0.64, 0.77] and CNN_{Simulated} AUC of 0.68 with a 95CI of [0.62,
0.75]. It showed that CGAN simulated mammograms can help MO cancer detection.

    

### [[2109.12131] Automatic Map Update Using Dashcam Videos](http://arxiv.org/abs/2109.12131)


  Autonomous driving requires 3D maps that provide accurate and up-to-date
information about semantic landmarks. Due to the wider availability and lower
cost of cameras compared with laser scanners, vision-based mapping has
attracted much attention from academia and industry. Among the existing
solutions, Structure-from-Motion (SfM) technology has proved to be feasible for
building 3D maps from crowdsourced data, since it allows unordered images as
input. Previous works on SfM have mainly focused on issues related to building
3D point clouds and calculating camera poses, leaving the issues of automatic
change detection and localization open.
We propose in this paper an SfM-based solution for automatic map update, with
a focus on real-time change detection and localization. Our solution builds on
comparison of semantic map data (e.g. types and locations of traffic signs).
Through a novel design of the pixel-wise 3D localization algorithm, our system
can locate the objects detected from 2D images in a 3D space, utilizing sparse
SfM point clouds. Experiments with dashcam videos collected from two urban
areas prove that the system is able to locate visible traffic signs in front
along the driving direction with a median distance error of 1.52 meters.
Moreover, it can detect up to 80\% of the changes with a median distance error
of 2.21 meters. The result analysis also shows the potential of significantly
improving the system performance in the future by increasing the accuracy of
the background technology in use, including in particularly the object
detection and point cloud geo-registration algorithms.

    

### [[2109.12149] Airfoil's Aerodynamic Coefficients Prediction using Artificial Neural Network](http://arxiv.org/abs/2109.12149)


  Figuring out the right airfoil is a crucial step in the preliminary stage of
any aerial vehicle design, as its shape directly affects the overall
aerodynamic characteristics of the aircraft or rotorcraft. Besides being a
measure of performance, the aerodynamic coefficients are used to design
additional subsystems such as a flight control system, or predict complex
dynamic phenomena such as aeroelastic instability. The coefficients in question
can either be obtained experimentally through wind tunnel testing or, depending
upon the accuracy requirements, by numerically simulating the underlying
fundamental equations of fluid dynamics. In this paper, the feasibility of
applying Artificial Neural Networks (ANNs) to estimate the aerodynamic
coefficients of differing airfoil geometries at varying Angle of Attack, Mach
and Reynolds number is investigated. The ANNs are computational entities that
have the ability to learn highly nonlinear spatial and temporal patterns.
Therefore, they are increasingly being used to approximate complex real-world
phenomenon. However, despite their significant breakthrough in the past few
years, ANNs' spreading in the field of Computational Fluid Dynamics (CFD) is
fairly recent, and many applications within this field remain unexplored. This
study thus compares different network architectures and training datasets in an
attempt to gain insight as to how the network perceives the given airfoil
geometries, while producing an acceptable neuronal model for faster and easier
prediction of lift, drag and moment coefficients in steady state,
incompressible flow regimes. This data-driven method produces sufficiently
accurate results, with the added benefit of saving high computational and
experimental costs.

    

### [[2109.12179] Constrained Optimization with Qualitative Preferences](http://arxiv.org/abs/2109.12179)


  The Conditional Preference Network (CP-net) graphically represents user's
qualitative and conditional preference statements under the ceteris paribus
interpretation. The constrained CP-net is an extension of the CP-net, to a set
of constraints. The existing algorithms for solving the constrained CP-net
require the expensive dominance testing operation. We propose three approaches
to tackle this challenge. In our first solution, we alter the constrained
CP-net by eliciting additional relative importance statements between
variables, in order to have a total order over the outcomes. We call this new
model, the constrained Relative Importance Network (constrained CPR-net).
Consequently, We show that the Constrained CPR-net has one single optimal
outcome (assuming the constrained CPR-net is consistent) that we can obtain
without dominance testing. In our second solution, we extend the Lexicographic
Preference Tree (LP-tree) to a set of constraints. Then, we propose a recursive
backtrack search algorithm, that we call Search-LP, to find the most preferable
outcome. We prove that the first feasible outcome returned by Search-LP
(without dominance testing) is also preferable to any other feasible outcome.
Finally, in our third solution, we preserve the semantics of the CP-net and
propose a divide and conquer algorithm that compares outcomes according to
dominance testing.

    

### [[2109.12213] Adaptive Sampling Quasi-Newton Methods for Zeroth-Order Stochastic Optimization](http://arxiv.org/abs/2109.12213)


  We consider unconstrained stochastic optimization problems with no available
gradient information. Such problems arise in settings from derivative-free
simulation optimization to reinforcement learning. We propose an adaptive
sampling quasi-Newton method where we estimate the gradients of a stochastic
function using finite differences within a common random number framework. We
develop modified versions of a norm test and an inner product quasi-Newton test
to control the sample sizes used in the stochastic approximations and provide
global convergence results to the neighborhood of the optimal solution. We
present numerical experiments on simulation optimization problems to illustrate
the performance of the proposed algorithm. When compared with classical
zeroth-order stochastic gradient methods, we observe that our strategies of
adapting the sample sizes significantly improve performance in terms of the
number of stochastic function evaluations required.

    

### [[2109.12240] Logical Credal Networks](http://arxiv.org/abs/2109.12240)


  This paper introduces Logical Credal Networks, an expressive probabilistic
logic that generalizes many prior models that combine logic and probability.
Given imprecise information represented by probability bounds and conditional
probability bounds of logic formulas, this logic specifies a set of probability
distributions over all interpretations. On the one hand, our approach allows
propositional and first-order logic formulas with few restrictions, e.g.,
without requiring acyclicity. On the other hand, it has a Markov condition
similar to Bayesian networks and Markov random fields that is critical in
real-world applications. Having both these properties makes this logic unique,
and we investigate its performance on maximum a posteriori inference tasks,
including solving Mastermind games with uncertainty and detecting credit card
fraud. The results show that the proposed method outperforms existing
approaches, and its advantage lies in aggregating multiple sources of imprecise
information.

    

### [[2109.12258] Pushing on Text Readability Assessment: A Transformer Meets Handcrafted Linguistic Features](http://arxiv.org/abs/2109.12258)


  We report two essential improvements in readability assessment: 1. three
novel features in advanced semantics and 2. the timely evidence that
traditional ML models (e.g. Random Forest, using handcrafted features) can
combine with transformers (e.g. RoBERTa) to augment model performance. First,
we explore suitable transformers and traditional ML models. Then, we extract
255 handcrafted linguistic features using self-developed extraction software.
Finally, we assemble those to create several hybrid models, achieving
state-of-the-art (SOTA) accuracy on popular datasets in readability assessment.
The use of handcrafted features help model performance on smaller datasets.
Notably, our RoBERTA-RF-T1 hybrid achieves the near-perfect classification
accuracy of 99%, a 20.3% increase from the previous SOTA.

    

### [[2109.12264] More Than Reading Comprehension: A Survey on Datasets and Metrics of Textual Question Answering](http://arxiv.org/abs/2109.12264)


  Textual Question Answering (QA) aims to provide precise answers to user's
questions in natural language using unstructured data. One of the most popular
approaches to this goal is machine reading comprehension(MRC). In recent years,
many novel datasets and evaluation metrics based on classical MRC tasks have
been proposed for broader textual QA tasks. In this paper, we survey 47 recent
textual QA benchmark datasets and propose a new taxonomy from an application
point of view. In addition, We summarize 8 evaluation metrics of textual QA
tasks. Finally, we discuss current trends in constructing textual QA benchmarks
and suggest directions for future work.

    

### [[2109.12265] Data, Assemble: Leveraging Multiple Datasets with Heterogeneous and Partial Labels](http://arxiv.org/abs/2109.12265)


  The success of deep learning relies heavily on large datasets with extensive
labels, but we often only have access to several small, heterogeneous datasets
associated with partial labels, particularly in the field of medical imaging.
When learning from multiple datasets, existing challenges include incomparable,
heterogeneous, or even conflicting labeling protocols across datasets. In this
paper, we propose a new initiative--"data, assemble"--which aims to unleash the
full potential of partially labeled data and enormous unlabeled data from an
assembly of datasets. To accommodate the supervised learning paradigm to
partial labels, we introduce a dynamic adapter that encodes multiple visual
tasks and aggregates image features in a question-and-answer manner.
Furthermore, we employ pseudo-labeling and consistency constraints to harness
images with missing labels and to mitigate the domain gap across datasets. From
proof-of-concept studies on three natural imaging datasets and rigorous
evaluations on two large-scale thorax X-ray benchmarks, we discover that
learning from "negative examples" facilitates both classification and
segmentation of classes of interest. This sheds new light on the computer-aided
diagnosis of rare diseases and emerging pandemics, wherein "positive examples"
are hard to collect, yet "negative examples" are relatively easier to assemble.
As a result, besides exceeding the prior art in the NIH ChestXray benchmark,
our model is particularly strong in identifying diseases of minority classes,
yielding over 3-point improvement on average. Remarkably, when using existing
partial labels, our model performance is on-par (p>0.05) with that using a
fully curated dataset with exhaustive labels, eliminating the need for
additional 40% annotation costs.

    

### [[2109.12300] Finetuning Transformer Models to Build ASAG System](http://arxiv.org/abs/2109.12300)


  Research towards creating systems for automatic grading of student answers to
quiz and exam questions in educational settings has been ongoing since 1966.
Over the years, the problem was divided into many categories. Among them,
grading text answers were divided into short answer grading, and essay grading.
The goal of this work was to develop an ML-based short answer grading system. I
hence built a system which uses finetuning on Roberta Large Model pretrained on
STS benchmark dataset and have also created an interface to show the production
readiness of the system. I evaluated the performance of the system on the
Mohler extended dataset and SciEntsBank Dataset. The developed system achieved
a Pearsons Correlation of 0.82 and RMSE of 0.7 on the Mohler Dataset which
beats the SOTA performance on this dataset which is correlation of 0.805 and
RMSE of 0.793. Additionally, Pearsons Correlation of 0.79 and RMSE of 0.56 was
achieved on the SciEntsBank Dataset, which only reconfirms the robustness of
the system. A few observations during achieving these results included usage of
batch size of 1 produced better results than using batch size of 16 or 32 and
using huber loss as loss function performed well on this regression task. The
system was tried and tested on train and validation splits using various random
seeds and still has been tweaked to achieve a minimum of 0.76 of correlation
and a maximum 0.15 (out of 1) RMSE on any dataset.

    

### [[2109.12302] Learning Neural Templates for Recommender Dialogue System](http://arxiv.org/abs/2109.12302)


  Though recent end-to-end neural models have shown promising progress on
Conversational Recommender System (CRS), two key challenges still remain.
First, the recommended items cannot be always incorporated into the generated
replies precisely and appropriately. Second, only the items mentioned in the
training corpus have a chance to be recommended in the conversation. To tackle
these challenges, we introduce a novel framework called NTRD for recommender
dialogue system that decouples the dialogue generation from the item
recommendation. NTRD has two key components, i.e., response template generator
and item selector. The former adopts an encoder-decoder model to generate a
response template with slot locations tied to target items, while the latter
fills in slot locations with the proper items using a sufficient attention
mechanism. Our approach combines the strengths of both classical slot filling
approaches (that are generally controllable) and modern neural NLG approaches
(that are generally more natural and accurate). Extensive experiments on the
benchmark ReDial show our NTRD significantly outperforms the previous
state-of-the-art methods. Besides, our approach has the unique advantage to
produce novel items that do not appear in the training set of dialogue corpus.
The code is available at \url{this https URL}.

    

### [[2109.12307] Multi-Modal Multi-Instance Learning for Retinal Disease Recognition](http://arxiv.org/abs/2109.12307)


  This paper attacks an emerging challenge of multi-modal retinal disease
recognition. Given a multi-modal case consisting of a color fundus photo (CFP)
and an array of OCT B-scan images acquired during an eye examination, we aim to
build a deep neural network that recognizes multiple vision-threatening
diseases for the given case. As the diagnostic efficacy of CFP and OCT is
disease-dependent, the network's ability of being both selective and
interpretable is important. Moreover, as both data acquisition and manual
labeling are extremely expensive in the medical domain, the network has to be
relatively lightweight for learning from a limited set of labeled multi-modal
samples. Prior art on retinal disease recognition focuses either on a single
disease or on a single modality, leaving multi-modal fusion largely
underexplored. We propose in this paper Multi-Modal Multi-Instance Learning
(MM-MIL) for selectively fusing CFP and OCT modalities. Its lightweight
architecture (as compared to current multi-head attention modules) makes it
suited for learning from relatively small-sized datasets. For an effective use
of MM-MIL, we propose to generate a pseudo sequence of CFPs by over sampling a
given CFP. The benefits of this tactic include well balancing instances across
modalities, increasing the resolution of the CFP input, and finding out regions
of the CFP most relevant with respect to the final diagnosis. Extensive
experiments on a real-world dataset consisting of 1,206 multi-modal cases from
1,193 eyes of 836 subjects demonstrate the viability of the proposed model.

    

### [[2109.12314] MC$^2$-SF: Slow-Fast Learning for Mobile-Cloud Collaborative Recommendation](http://arxiv.org/abs/2109.12314)


  With the hardware development of mobile devices, it is possible to build the
recommendation models on the mobile side to utilize the fine-grained features
and the real-time feedbacks. Compared to the straightforward mobile-based
modeling appended to the cloud-based modeling, we propose a Slow-Fast learning
mechanism to make the Mobile-Cloud Collaborative recommendation (MC$^2$-SF)
mutual benefit. Specially, in our MC$^2$-SF, the cloud-based model and the
mobile-based model are respectively treated as the slow component and the fast
component, according to their interaction frequency in real-world scenarios.
During training and serving, they will communicate the prior/privileged
knowledge to each other to help better capture the user interests about the
candidates, resembling the role of System I and System II in the human
cognition. We conduct the extensive experiments on three benchmark datasets and
demonstrate the proposed MC$^2$-SF outperforms several state-of-the-art
methods.

    

### [[2109.12445] Algorithmic Information Design in Multi-Player Games: Possibility and Limits in Singleton Congestion](http://arxiv.org/abs/2109.12445)


  Most algorithmic studies on multi-agent information design so far have
focused on the restricted situation with no inter-agent externalities; a few
exceptions investigated special game classes such as zero-sum games and
second-price auctions but have all focused only on optimal public signaling and
exhibit sweepingly negative results. This paper initiates the algorithmic
information design of both \emph{public} and \emph{private} signaling in a
fundamental class of games with negative externalities, i.e., atomic singleton
congestion games, with wide application in today's digital economy, machine
scheduling, routing, etc.
For both public and private signaling, we show that the optimal information
design can be efficiently computed when the number of resources is a constant.
To our knowledge, this is the first set of computationally efficient algorithms
for information design in succinctly representable many-player games. Our
results hinge on novel techniques such as developing ``reduced forms'' to
compactly represent players' marginal beliefs. When there are many resources,
we show computational intractability results. To overcome the challenge of
multiple equilibria, here we introduce a new notion of
equilibrium-\emph{oblivious} NP-hardness, which rules out any possibility of
computing a good signaling scheme, irrespective of the equilibrium selection
rule.

    

### [[2109.12480] Explainability Pitfalls: Beyond Dark Patterns in Explainable AI](http://arxiv.org/abs/2109.12480)


  To make Explainable AI (XAI) systems trustworthy, understanding harmful
effects is just as important as producing well-designed explanations. In this
paper, we address an important yet unarticulated type of negative effect in
XAI. We introduce explainability pitfalls(EPs), unanticipated negative
downstream effects from AI explanations manifesting even when there is no
intention to manipulate users. EPs are different from, yet related to, dark
patterns, which are intentionally deceptive practices. We articulate the
concept of EPs by demarcating it from dark patterns and highlighting the
challenges arising from uncertainties around pitfalls. We situate and
operationalize the concept using a case study that showcases how, despite best
intentions, unsuspecting negative effects such as unwarranted trust in
numerical explanations can emerge. We propose proactive and preventative
strategies to address EPs at three interconnected levels: research, design, and
organizational.

    

### [[2109.12484] Excavating the Potential Capacity of Self-Supervised Monocular Depth Estimation](http://arxiv.org/abs/2109.12484)


  Self-supervised methods play an increasingly important role in monocular
depth estimation due to their great potential and low annotation cost. To close
the gap with supervised methods, recent works take advantage of extra
constraints, e.g., semantic segmentation. However, these methods will
inevitably increase the burden on the model. In this paper, we show theoretical
and empirical evidence that the potential capacity of self-supervised monocular
depth estimation can be excavated without increasing this cost. In particular,
we propose (1) a novel data augmentation approach called data grafting, which
forces the model to explore more cues to infer depth besides the vertical image
position, (2) an exploratory self-distillation loss, which is supervised by the
self-distillation label generated by our new post-processing method - selective
post-processing, and (3) the full-scale network, designed to endow the encoder
with the specialization of depth estimation task and enhance the
representational power of the model. Extensive experiments show that our
contributions can bring significant performance improvement to the baseline
with even less computational overhead, and our model, named EPCDepth, surpasses
the previous state-of-the-art methods even those supervised by additional
constraints.

    

### [[2109.12520] Entity Linking Meets Deep Learning: Techniques and Solutions](http://arxiv.org/abs/2109.12520)


  Entity linking (EL) is the process of linking entity mentions appearing in
web text with their corresponding entities in a knowledge base. EL plays an
important role in the fields of knowledge engineering and data mining,
underlying a variety of downstream applications such as knowledge base
population, content analysis, relation extraction, and question answering. In
recent years, deep learning (DL), which has achieved tremendous success in
various domains, has also been leveraged in EL methods to surpass traditional
machine learning based methods and yield the state-of-the-art performance. In
this survey, we present a comprehensive review and analysis of existing DL
based EL methods. First of all, we propose a new taxonomy, which organizes
existing DL based EL methods using three axes: embedding, feature, and
algorithm. Then we systematically survey the representative EL methods along
the three axes of the taxonomy. Later, we introduce ten commonly used EL data
sets and give a quantitative performance analysis of DL based EL methods over
these data sets. Finally, we discuss the remaining limitations of existing
methods and highlight some promising future directions.

    

### [[2109.12562] Deep Reinforcement Learning for Wireless Scheduling in Distributed Networked Control](http://arxiv.org/abs/2109.12562)


  In the literature of transmission scheduling in wireless networked control
systems (WNCSs) over shared wireless resources, most research works have
focused on partially distributed settings, i.e., where either the controller
and actuator, or the sensor and controller are co-located. To overcome this
limitation, the present work considers a fully distributed WNCS with
distributed plants, sensors, actuators and a controller, sharing a limited
number of frequency channels. To overcome communication limitations, the
controller schedules the transmissions and generates sequential predictive
commands for control. Using elements of stochastic systems theory, we derive a
sufficient stability condition of the WNCS, which is stated in terms of both
the control and communication system parameters. Once the condition is
satisfied, there exists at least one stationary and deterministic scheduling
policy that can stabilize all plants of the WNCS. By analyzing and representing
the per-step cost function of the WNCS in terms of a finite-length countable
vector state, we formulate the optimal transmission scheduling problem into a
Markov decision process problem and develop a deep-reinforcement-learning-based
algorithm for solving it. Numerical results show that the proposed algorithm
significantly outperforms the benchmark policies.

    

### [[2109.12575] Paradigm Shift in Natural Language Processing](http://arxiv.org/abs/2109.12575)


  In the era of deep learning, modeling for most NLP tasks has converged to
several mainstream paradigms. For example, we usually adopt the sequence
labeling paradigm to solve a bundle of tasks such as POS-tagging, NER,
Chunking, and adopt the classification paradigm to solve tasks like sentiment
analysis. With the rapid progress of pre-trained language models, recent years
have observed a rising trend of Paradigm Shift, which is solving one NLP task
by reformulating it as another one. Paradigm shift has achieved great success
on many tasks, becoming a promising way to improve model performance. Moreover,
some of these paradigms have shown great potential to unify a large number of
NLP tasks, making it possible to build a single model to handle diverse tasks.
In this paper, we review such phenomenon of paradigm shifts in recent years,
highlighting several paradigms that have the potential to solve different NLP
tasks.

    

### [[2109.12613] SimpleX: A Simple and Strong Baseline for Collaborative Filtering](http://arxiv.org/abs/2109.12613)


  Collaborative filtering (CF) is a widely studied research topic in
recommender systems. The learning of a CF model generally depends on three
major components, namely interaction encoder, loss function, and negative
sampling. While many existing studies focus on the design of more powerful
interaction encoders, the impacts of loss functions and negative sampling
ratios have not yet been well explored. In this work, we show that the choice
of loss function as well as negative sampling ratio is equivalently important.
More specifically, we propose the cosine contrastive loss (CCL) and further
incorporate it to a simple unified CF model, dubbed SimpleX. Extensive
experiments have been conducted on 11 benchmark datasets and compared with 29
existing CF models in total. Surprisingly, the results show that, under our CCL
loss and a large negative sampling ratio, SimpleX can surpass most
sophisticated state-of-the-art models by a large margin (e.g., max 48.5%
improvement in NDCG@20 over LightGCN). We believe that SimpleX could not only
serve as a simple strong baseline to foster future research on CF, but also
shed light on the potential research direction towards improving loss function
and negative sampling.

    

### [[2109.12624] A Clustering and Demotion Based Algorithm for Inductive Learning of Default Theories](http://arxiv.org/abs/2109.12624)


  We present a clustering- and demotion-based algorithm called Kmeans-FOLD to
induce nonmonotonic logic programs from positive and negative examples. Our
algorithm improves upon-and is inspired by-the FOLD algorithm. The FOLD
algorithm itself is an improvement over the FOIL algorithm. Our algorithm
generates a more concise logic program compared to the FOLD algorithm. Our
algorithm uses the K-means based clustering method to cluster the input
positive samples before applying the FOLD algorithm. Positive examples that are
covered by the partially learned program in intermediate steps are not
discarded as in the FOLD algorithm, rather they are demoted, i.e., their
weights are reduced in subsequent iterations of the algorithm. Our experiments
on the UCI dataset show that a combination of K-Means clustering and our
demotion strategy produces significant improvement for datasets with more than
one cluster of positive examples. The resulting induced program is also more
concise and therefore easier to understand compared to the FOLD and ALEPH
systems, two state of the art inductive logic programming (ILP) systems.

    

### [[2109.12651] Why Do We Click: Visual Impression-aware News Recommendation](http://arxiv.org/abs/2109.12651)


  There is a soaring interest in the news recommendation research scenario due
to the information overload. To accurately capture users' interests, we propose
to model multi-modal features, in addition to the news titles that are widely
used in existing works, for news recommendation. Besides, existing research
pays little attention to the click decision-making process in designing
multi-modal modeling modules. In this work, inspired by the fact that users
make their click decisions mostly based on the visual impression they perceive
when browsing news, we propose to capture such visual impression information
with visual-semantic modeling for news recommendation. Specifically, we devise
the local impression modeling module to simultaneously attend to decomposed
details in the impression when understanding the semantic meaning of news
title, which could explicitly get close to the process of users reading news.
In addition, we inspect the impression from a global view and take structural
information, such as the arrangement of different fields and spatial position
of different words on the impression, into the modeling of multiple modalities.
To accommodate the research of visual impression-aware news recommendation, we
extend the text-dominated news recommendation dataset MIND by adding snapshot
impression images and will release it to nourish the research field. Extensive
comparisons with the state-of-the-art news recommenders along with the in-depth
analyses demonstrate the effectiveness of the proposed method and the promising
capability of modeling visual impressions for the content-based recommenders.

    

### [[2109.12691] Applying supervised and reinforcement learning methods to create neural-network-based agents for playing StarCraft II](http://arxiv.org/abs/2109.12691)


  Recently, multiple approaches for creating agents for playing various complex
real-time computer games such as StarCraft II or Dota 2 were proposed, however,
they either embed a significant amount of expert knowledge into the agent or
use a prohibitively large for most researchers amount of computational
resources. We propose a neural network architecture for playing the full
two-player match of StarCraft II trained with general-purpose supervised and
reinforcement learning, that can be trained on a single consumer-grade PC with
a single GPU. We also show that our implementation achieves a non-trivial
performance when compared to the in-game scripted bots. We make no simplifying
assumptions about the game except for playing on a single chosen map, and we
use very little expert knowledge. In principle, our approach can be applied to
any RTS game with small modifications. While our results are far behind the
state-of-the-art large-scale approaches in terms of the final performance, we
believe our work can serve as a solid baseline for other small-scale
experiments.

    

### [[2109.12720] On the Feasibility of Learning Finger-gaiting In-hand Manipulation with Intrinsic Sensing](http://arxiv.org/abs/2109.12720)


  Finger-gaiting manipulation is an important skill to achieve large-angle
in-hand re-orientation of objects. However, achieving these gaits with
arbitrary orientations of the hand is challenging due to the unstable nature of
the task. In this work, we use model-free reinforcement learning (RL) to learn
finger-gaiting only via precision grasps and demonstrate finger-gaiting for
rotation about an axis purely using on-board proprioceptive and tactile
feedback. To tackle the inherent instability of precision grasping, we propose
the use of initial state distributions that enable effective exploration of the
state space. Our method can learn finger-gaiting with significantly improved
sample complexity than the state-of-the-art. The policies we obtain are robust
and also transfer to novel objects.

    

### [[2109.12724] Research on facial expression recognition based on Multimodal data fusion and neural network](http://arxiv.org/abs/2109.12724)


  Facial expression recognition is a challenging task when neural network is
applied to pattern recognition. Most of the current recognition research is
based on single source facial data, which generally has the disadvantages of
low accuracy and low robustness. In this paper, a neural network algorithm of
facial expression recognition based on multimodal data fusion is proposed. The
algorithm is based on the multimodal data, and it takes the facial image, the
histogram of oriented gradient of the image and the facial landmarks as the
input, and establishes CNN, LNN and HNN three sub neural networks to extract
data features, using multimodal data feature fusion mechanism to improve the
accuracy of facial expression recognition. Experimental results show that,
benefiting by the complementarity of multimodal data, the algorithm has a great
improvement in accuracy, robustness and detection speed compared with the
traditional facial expression recognition algorithm. Especially in the case of
partial occlusion, illumination and head posture transformation, the algorithm
also shows a high confidence.

    

### [[2109.12755] Abstraction, Reasoning and Deep Learning: A Study of the "Look and Say" Sequence](http://arxiv.org/abs/2109.12755)


  The ability to abstract, count, and use System 2 reasoning are well-known
manifestations of intelligence and understanding. In this paper, we argue,
using the example of the ``Look and Say" puzzle, that although deep neural
networks can exhibit high `competence' (as measured by accuracy) when trained
on large data sets (2M examples in our case), they do not show any sign on the
deeper understanding of the problem, or what D. Dennett calls `comprehension'.
We report on two sets experiments on the ``Look and Say" puzzle data. We view
the problem as building a translator from one set of tokens to another. We
apply both standard LSTMs and Transformer/Attention -- based neural networks,
using publicly available machine translation software. We observe that despite
the amazing accuracy (on both, training and test data), the performance of the
trained programs on the actual L\&S sequence is bad. We then discuss a few
possible ramifications of this finding and connections to other work,
experimental and theoretical. First, from the cognitive science perspective, we
argue that we need better mathematical models of abstraction. Second, the
classical and more recent results on the universality of neural networks should
be re-examined for functions acting on discrete data sets. Mapping on discrete
sets usually have no natural continuous extensions. This connects the results
on a simple puzzle to more sophisticated results on modeling of mathematical
functions, where algebraic functions are more difficult to model than e.g.
differential equations. Third, we hypothesize that for problems such as ``Look
and Say", computing the parity of bitstrings, or learning integer addition, it
might be worthwhile to introduce concepts from topology, where continuity is
defined without the reference to the concept of distance.

    

### [[2109.12781] Effective Use of Graph Convolution Network and Contextual Sub-Tree forCommodity News Event Extraction](http://arxiv.org/abs/2109.12781)


  Event extraction in commodity news is a less researched area as compared to
generic event extraction. However, accurate event extraction from commodity
news is useful in abroad range of applications such as under-standing event
chains and learning event-event relations, which can then be used for commodity
price prediction. The events found in commodity news exhibit characteristics
different from generic events, hence posing a unique challenge in event
extraction using existing methods. This paper proposes an effective use of
Graph Convolutional Networks(GCN) with a pruned dependency parse tree, termed
contextual sub-tree, for better event ex-traction in commodity news. The event
ex-traction model is trained using feature embed-dings from ComBERT, a
BERT-based masked language model that was produced through domain-adaptive
pre-training on a commodity news corpus. Experimental results show the
efficiency of the proposed solution, which out-performs existing methods with
F1 scores as high as 0.90. Furthermore, our pre-trained language model
outperforms GloVe by 23%, and BERT and RoBERTa by 7% in terms of argument roles
classification. For the goal of re-producibility, the code and trained models
are made publicly available1.

    

### [[2109.12788] Multiplicative Position-aware Transformer Models for Language Understanding](http://arxiv.org/abs/2109.12788)


  Transformer models, which leverage architectural improvements like
self-attention, perform remarkably well on Natural Language Processing (NLP)
tasks. The self-attention mechanism is position agnostic. In order to capture
positional ordering information, various flavors of absolute and relative
position embeddings have been proposed. However, there is no systematic
analysis on their contributions and a comprehensive comparison of these methods
is missing in the literature. In this paper, we review major existing position
embedding methods and compare their accuracy on downstream NLP tasks, using our
own implementations. We also propose a novel multiplicative embedding method
which leads to superior accuracy when compared to existing methods. Finally, we
show that our proposed embedding method, served as a drop-in replacement of the
default absolute position embedding, can improve the RoBERTa-base and
RoBERTa-large models on SQuAD1.1 and SQuAD2.0 datasets.

    

### [[2109.12797] An Adaptive PID Autotuner for Multicopters with Experimental Results](http://arxiv.org/abs/2109.12797)


  This paper develops an adaptive PID autotuner for multicopters, and presents
simulation and experimental results. The autotuner consists of adaptive digital
control laws based on retrospective cost adaptive control implemented in the
PX4 flight stack. A learning trajectory is used to optimize the autopilot
during a single flight. The autotuned autopilot is then compared with the
default PX4 autopilot by flying a test trajectory constructed using the
second-order Hilbert curve. In order to investigate the sensitivity of the
autotuner to the quadcopter dynamics, the mass of the quadcopter is varied, and
the performance of the autotuned and default autopilot is compared. It is
observed that the autotuned autopilot outperforms the default autopilot.

    

### [[2109.12798] From internal models toward metacognitive AI](http://arxiv.org/abs/2109.12798)


  In several papers published in Biological Cybernetics in the 1980s and 1990s,
Kawato and colleagues proposed computational models explaining how internal
models are acquired in the cerebellum. These models were later supported by
neurophysiological experiments using monkeys and neuroimaging experiments
involving humans. These early studies influenced neuroscience from basic,
sensory-motor control to higher cognitive functions. One of the most perplexing
enigmas related to internal models is to understand the neural mechanisms that
enable animals to learn large-dimensional problems with so few trials.
Consciousness and metacognition -- the ability to monitor one's own thoughts,
may be part of the solution to this enigma. Based on literature reviews of the
past 20 years, here we propose a computational neuroscience model of
metacognition. The model comprises a modular hierarchical
reinforcement-learning architecture of parallel and layered, generative-inverse
model pairs. In the prefrontal cortex, a distributed executive network called
the "cognitive reality monitoring network" (CRMN) orchestrates conscious
involvement of generative-inverse model pairs in perception and action. Based
on mismatches between computations by generative and inverse models, as well as
reward prediction errors, CRMN computes a "responsibility signal" that gates
selection and learning of pairs in perception, action, and reinforcement
learning. A high responsibility signal is given to the pairs that best capture
the external world, that are competent in movements (small mismatch), and that
are capable of reinforcement learning (small reward prediction error). CRMN
selects pairs with higher responsibility signals as objects of metacognition,
and consciousness is determined by the entropy of responsibility signals across
all pairs.

    

### [[1904.04475] Private Hierarchical Clustering and Efficient Approximation](http://arxiv.org/abs/1904.04475)


  In collaborative learning, multiple parties contribute their datasets to
jointly deduce global machine learning models for numerous predictive tasks.
Despite its efficacy, this learning paradigm fails to encompass critical
application domains that involve highly sensitive data, such as healthcare and
security analytics, where privacy risks limit entities to individually train
models using only their own datasets. In this work, we target
privacy-preserving collaborative hierarchical clustering. We introduce a formal
security definition that aims to achieve the balance between utility and
privacy and present a two-party protocol that provably satisfies it. We then
extend our protocol with: (i) an optimized version for the single-linkage
clustering, and (ii) scalable approximation variants. We implement all our
schemes and experimentally evaluate their performance and accuracy on synthetic
and real datasets, obtaining very encouraging results. For example, end-to-end
execution of our secure approximate protocol for over 1M 10-dimensional data
samples requires 35sec of computation and achieves 97.09% accuracy.

    

### [[2005.01192] Philosophy-Guided Mathematical Formalism for Complex Systems Modelling](http://arxiv.org/abs/2005.01192)


  Mathematical modelling heavily employs differential equations to describe the
macroscopic or global behaviour of systems. The dynamics of complex systems is
in contrast more efficiently described by local rules and thus in an
algorithmic and local or microscopic manner. The theory of such an approach has
to be established still. We recently presented the so-called allagmatic method,
which includes a system metamodel providing a framework for describing,
modelling, simulating, and interpreting complex systems. Its development and
programming was guided by philosophy, especially by Gilbert Simondon's
philosophy of individuation, and concepts from cybernetics. Here, a
mathematical formalism is presented to more precisely describe and define the
system metamodel of the allagmatic method, further generalising it and
extending its reach to a more formal treatment and allowing more theoretical
studies. Using the formalism, an example for such a further study is finally
provided with mathematical definitions and proofs for model creation and
equivalence of cellular automata and artificial neural networks.

    

### [[2101.06399] Latent Variable Models for Visual Question Answering](http://arxiv.org/abs/2101.06399)


  Current work on Visual Question Answering (VQA) explore deterministic
approaches conditioned on various types of image and question features. We
posit that, in addition to image and question pairs, other modalities are
useful for teaching machine to carry out question answering. Hence in this
paper, we propose latent variable models for VQA where extra information (e.g.
captions and answer categories) are incorporated as latent variables, which are
observed during training but in turn benefit question-answering performance at
test time. Experiments on the VQA v2.0 benchmarking dataset demonstrate the
effectiveness of our proposed models: they improve over strong baselines,
especially those that do not rely on extensive language-vision pre-training.

    

### [[2104.02206] Hypothesis-driven Stream Learning with Augmented Memory](http://arxiv.org/abs/2104.02206)


  Stream learning refers to the ability to acquire and transfer knowledge
across a continuous stream of data without forgetting and without repeated
passes over the data. A common way to avoid catastrophic forgetting is to
intersperse new examples with replays of old examples stored as image pixels or
reproduced by generative models. Here, we consider stream learning in image
classification tasks and propose a novel hypotheses-driven Augmented Memory
Network, which efficiently consolidates previous knowledge with a limited
number of hypotheses in the augmented memory and replays relevant hypotheses to
avoid catastrophic forgetting. The advantages of hypothesis-driven replay over
pixel-level replay and generative replay are two-fold. First, hypothesis-based
knowledge consolidation avoids redundant information in the image pixel space
and makes memory usage more efficient. Second, hypotheses in the augmented
memory can be re-used for learning new tasks, improving generalization and
transfer learning ability. We evaluated our method on three stream learning
object recognition datasets. Our method performs comparably well or better than
state-of-the-art methods, while offering more efficient memory usage. All
source code and data are publicly available
this https URL.

    

### [[2105.07351] Model-Based Offline Planning with Trajectory Pruning](http://arxiv.org/abs/2105.07351)


  Offline reinforcement learning (RL) enables learning policies using
pre-collected datasets without environment interaction, which provides a
promising direction to make RL usable in real-world systems. Although recent
offline RL studies have achieved much progress, existing methods still face
many practical challenges in real-world system control tasks, such as
computational restriction during agent training and the requirement of extra
control flexibility. Model-based planning framework provides an attractive
solution for such tasks. However, most model-based planning algorithms are not
designed for offline settings. Simply combining the ingredients of offline RL
with existing methods either provides over-restrictive planning or leads to
inferior performance. We propose a new light-weighted model-based offline
planning framework, namely MOPP, which tackles the dilemma between the
restrictions of offline learning and high-performance planning. MOPP encourages
more aggressive trajectory rollout guided by the behavior policy learned from
data, and prunes out problematic trajectories to avoid potential
out-of-distribution samples. Experimental results show that MOPP provides
competitive performance compared with existing model-based offline planning and
RL approaches.

    

### [[2106.07015] Siamese Network Training Using Artificial Triplets By Sampling and Image Transformation](http://arxiv.org/abs/2106.07015)


  The device used in this work detects the objects over the surface of the
water using two thermal cameras which aid the users to detect and avoid the
objects in scenarios where the human eyes cannot (night, fog, etc.). To avoid
the obstacle collision autonomously, it is required to track the objects in
real-time and assign a specific identity to each object to determine its
dynamics (trajectory, velocity, etc.) for making estimated collision
predictions. In the following work, a Machine Learning (ML) approach for
Computer Vision (CV) called Convolutional Neural Network (CNN) was used using
TensorFlow as the high-level programming environment in Python. To validate the
algorithm a test set was generated using an annotation tool that was created
during the work for proper evaluation. Once validated, the algorithm was
deployed on the platform and tested with the sequence generated by the test
boat.

    

### [[2109.05779] Deep Joint Source-Channel Coding for Multi-Task Network](http://arxiv.org/abs/2109.05779)


  Multi-task learning (MTL) is an efficient way to improve the performance of
related tasks by sharing knowledge. However, most existing MTL networks run on
a single end and are not suitable for collaborative intelligence (CI)
scenarios. In this work, we propose an MTL network with a deep joint
source-channel coding (JSCC) framework, which allows operating under CI
scenarios. We first propose a feature fusion based MTL network (FFMNet) for
joint object detection and semantic segmentation. Compared with other MTL
networks, FFMNet gets higher performance with fewer parameters. Then FFMNet is
split into two parts, which run on a mobile device and an edge server
respectively. The feature generated by the mobile device is transmitted through
the wireless channel to the edge server. To reduce the transmission overhead of
the intermediate feature, a deep JSCC network is designed. By combining two
networks together, the whole model achieves 512x compression for the
intermediate feature and a performance loss within 2% on both tasks. At last,
by training with noise, the FFMNet with JSCC is robust to various channel
conditions and outperforms the separate source and channel coding scheme.

    

### [[2109.12317] A fluid reservoir model for the Age of Information through energy-harvesting transmitters](http://arxiv.org/abs/2109.12317)


  We apply a fluid-reservoir model to study the Age-of-Information (AoI) of
update packets through energy-harvesting transmitters. The model is closer to
how energy is stored and depleted in reality, and can reveal the system
behavior for different settings of packet arrival rates, service rates, and
energy charging and depletion rates. We present detailed results for both
finite and infinite transmitter buffers and an infinite energy reservoir, and
some indicative results for a finite reservoir. The results are derived for the
mean AoI in the case of an infinite transmitter buffer and an infinite
reservoir, and for the mean peak AoI for the remaining cases. The results show
that, similar to a system without energy constraints, the transmitter buffer
should be kept to a minimum in order to avoid queueing delays and maintain
freshness of updates. Furthermore, a high update packet rate is only helpful in
energy-rich regimes, whereas in energy-poor regimes more frequent updates
deplete the energy reservoir and result in higher AoI values.

    

### [[2109.12567] An Analysis into the Performance and Memory Usage of MATLAB Strings](http://arxiv.org/abs/2109.12567)


  MATLAB is a mathematical computing environment used by many engineers,
mathematicians, and students to process and understand their data. Important to
all data science is the managing of textual data. MATLAB supports two textual
data containers: (1) cell arrays of characters and (2) string arrays. This
research showcases the strengths of string arrays over cell arrays by
quantifying their performance, memory contiguity, syntax readability, interface
fluidity, and autocomplete capabilities. These results demonstrate that string
arrays often run 2x to 40x faster than cell arrays for common string
benchmarks, are optimized for data locality by reducing metadata overhead, and
offer a more expressive syntax due to their automatic data type conversions and
vectorized methods.

    

### [[2109.12663] The Finite-Skip Method for Multiserver Analysis](http://arxiv.org/abs/2109.12663)


  Multiserver queueing systems are found at the core of a wide variety of
practical systems. Unfortunately, existing tools for analyzing multiserver
models have major limitations: Techniques for exact analysis often struggle
with high-dimensional models, while techniques for deriving bounds are often
too specialized to handle realistic system features, such as variable service
rates of jobs. New techniques are needed to handle these complex, important,
high-dimensional models.
In this paper we introduce the work-conserving finite-skip class of models.
This class includes many important models, such as the heterogeneous M/G/k, the
limited processor sharing policy for the M/G/1, the threshold parallelism
model, and the multiserver-job model under a simple scheduling policy.
We prove upper and lower bounds on mean response time for any model in the
work-conserving finite-skip class. Our bounds are separated by an additive
constant, giving a strong characterization of mean response time at all loads.
When specialized to each of the models above, these bounds represent the first
bounds on mean response time known for each setting.

    

### [[2109.12473] Statically Bounded-Memory Delayed Sampling for Probabilistic Streams](http://arxiv.org/abs/2109.12473)


  Probabilistic programming languages aid developers performing Bayesian
inference. These languages provide programming constructs and tools for
probabilistic modeling and automated inference. Prior work introduced a
probabilistic programming language, ProbZelus, to extend probabilistic
programming functionality to unbounded streams of data. This work demonstrated
that the delayed sampling inference algorithm could be extended to work in a
streaming context. ProbZelus showed that while delayed sampling could be
effectively deployed on some programs, depending on the probabilistic model
under consideration, delayed sampling is not guaranteed to use a bounded amount
of memory over the course of the execution of the program.
In this paper, we the present conditions on a probabilistic program's
execution under which delayed sampling will execute in bounded memory. The two
conditions are dataflow properties of the core operations of delayed sampling:
the $m$-consumed property and the unseparated paths property. A program
executes in bounded memory under delayed sampling if, and only if, it satisfies
the $m$-consumed and unseparated paths properties. We propose a static analysis
that abstracts over these properties to soundly ensure that any program that
passes the analysis satisfies these properties, and thus executes in bounded
memory under delayed sampling.

    

### [[2009.13619] Ferrite: A Judgmental Embedding of Session Types in Rust](http://arxiv.org/abs/2009.13619)


  This paper introduces Ferrite, a shallow embedding of session types in Rust.
In contrast to existing session type libraries and embeddings for mainstream
languages, Ferrite not only supports linear session types but also shared
session types. Shared session types allow sharing (aliasing) of channels while
preserving session fidelity (preservation) using type modalities for acquiring
and releasing sessions. Ferrite adopts a propositions as types approach and
encodes typing derivations as Rust functions, with the proof of successful
type-checking manifesting as a Rust program. We provide an evaluation of
Ferrite using Servo as a practical example, and demonstrate how safe
communication can be achieved in the canvas component using Ferrite.

    

### [[2011.10618] Gradualizing the Calculus of Inductive Constructions](http://arxiv.org/abs/2011.10618)


  Acknowledging the ordeal of a fully formal development in a proof assistant
such as Coq, we investigate gradual variations on the Calculus of Inductive
Construction (CIC) for swifter prototyping with imprecise types and terms. We
observe, with a no-go theorem, a crucial tradeoff between graduality and the
key properties of normalization and closure of universes under dependent
product that CIC enjoys. Beyond this Fire Triangle of Graduality, we explore
the gradualization of CIC with three different compromises, each relaxing one
edge of the Fire Triangle. We develop a parametrized presentation of gradual
CIC (GCIC) that encompasses all three variations, and develop their metatheory.
We first present a bidirectional elaboration of GCIC to a dependently-typed
cast calculus, CastCIC, which elucidates the interrelation between typing,
conversion, and the gradual guarantees. We use a syntactic model of CastCIC to
inform the design of a safe, confluent reduction, and establish, when
applicable, normalization. We study the static and dynamic gradual guarantees
as well as the stronger notion of graduality with embedding-projection pairs
formulated by New and Ahmed, using appropriate semantic model constructions.
This work informs and paves the way towards the development of malleable proof
assistants and dependently-typed programming languages.

    

### [<title>XGBoost support for Windows 11? - RFC - XGBoost</title>](https://discuss.xgboost.ai/t/xgboost-support-for-windows-11/2477/1)