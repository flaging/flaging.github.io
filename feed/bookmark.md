
## 2021-12-16

### [<title>Xgb.importance with multiple categories - XGBoost</title>](https://discuss.xgboost.ai/t/xgb-importance-with-multiple-categories/2604/1)

### [<title>【央视新闻】南昌如何可以代开具水泥发票【手机搜狐网】 - DockOne.io</title>](http://dockone.io/question/1854346)

### [<title>「今日发布」武汉如何代开具物流发票【手机搜狐网】 - DockOne.io</title>](http://dockone.io/question/1854345)

### [<title>【央视新闻】福州如何可以代开具水泥发票【手机搜狐网】 - DockOne.io</title>](http://dockone.io/question/1854344)

### [<title>「今日发布」南京如何代开具物流发票【手机搜狐网】 - DockOne.io</title>](http://dockone.io/question/1854343)

### [<title>【央视新闻】惠州如何可以代开具水泥发票【手机搜狐网】 - DockOne.io</title>](http://dockone.io/question/1854342)

### [<title>「今日发布」苏州如何代开具物流发票【手机搜狐网】 - DockOne.io</title>](http://dockone.io/question/1854341)

### [<title>「今日发布」宁波如何代开具物流发票【手机搜狐网】 - DockOne.io</title>](http://dockone.io/question/1854340)

### [<title>【央视新闻】佛山如何可以代开具水泥发票【手机搜狐网】 - DockOne.io</title>](http://dockone.io/question/1854339)

### [<title>「今日发布」郑州如何代开具物流发票【手机搜狐网】 - DockOne.io</title>](http://dockone.io/question/1854338)

### [<title>【央视新闻】东莞如何可以代开具水泥发票【手机搜狐网】 - DockOne.io</title>](http://dockone.io/question/1854337)

### [<title>「今日发布」重庆如何代开具物流发票【手机搜狐网】 - DockOne.io</title>](http://dockone.io/question/1854336)

### [<title>【央视新闻】青岛如何可以代开具水泥发票【手机搜狐网】 - DockOne.io</title>](http://dockone.io/question/1854335)

### [<title>【央视新闻】南宁如何可以代开具水泥发票【手机搜狐网】 - DockOne.io</title>](http://dockone.io/question/1854334)

### [<title>「今日发布」天津如何代开具物流发票【手机搜狐网】 - DockOne.io</title>](http://dockone.io/question/1854333)

### [<title>「今日发布」杭州如何代开具物流发票【手机搜狐网】 - DockOne.io</title>](http://dockone.io/question/1854332)

### [<title>【央视新闻】厦门如何可以代开具水泥发票【手机搜狐网】 - DockOne.io</title>](http://dockone.io/question/1854331)

### [<title>「今日发布」成都如何代开具物流发票【手机搜狐网】 - DockOne.io</title>](http://dockone.io/question/1854330)

### [<title>「今日发布」广州如何代开具物流发票【手机搜狐网】 - DockOne.io</title>](http://dockone.io/question/1854329)

### [<title>【央视新闻】贵阳如何可以代开具水泥发票【手机搜狐网】 - DockOne.io</title>](http://dockone.io/question/1854328)

### [<title>【央视新闻】兰州如何可以代开具水泥发票【手机搜狐网】 - DockOne.io</title>](http://dockone.io/question/1854327)

### [[2112.07831] Optimal Slot Size under Various Bandwidth Distributions in the Flexible-grid Optical Networks](http://arxiv.org/abs/2112.07831)


  Flexible grid Optical Networks are efficient mechanism to provide flexibility
in the optical spectrum utilization. For such networks, the slot width size as
specified by the ITU-T G.694.1 is 12.5 GHz. However, one should question if it
is the optimal grid size? In this paper, under different bandwidth distribution
scenarios, we review which slot size give appropriate spectrum efficiency.
Moreover, we present a study of the slot sizes with varying incoming traffic
having some bandwidth requirement under different scenarios.

    

### [[2112.07833] Reconfigurable Intelligent Surface-Empowered Self-Interference Cancellation for 6G Full-Duplex MIMO Communication Systems](http://arxiv.org/abs/2112.07833)


  With the advent of sixth-generation (6G) wireless communication networks, it
requires substantially increasing wireless traffic and extending serving
coverage. Reconfigurable intelligent surface (RIS) is widely considered as a
promising technique which is capable of improving the system data rate, energy
efficiency and coverage extension as well as the benefit of low power
consumption. Moreover, full-duplex (FD) transmission provides simultaneous
transmit and received signals, which theoretically enhances twice spectrum
efficiency. However, the self-interference (SI) in FD is a challenging task
requiring complex and high-overhead cancellation, which can be resolved by
configuring appropriate phase of RIS elements. This paper has proposed an
RIS-empowered full-duplex self-interference cancellation (RFSC) scheme to
alleviate the severe SI in an RIS-FD system. We consider the SI minimization of
RIS-FD uplink (UL) while guaranteeing quality-of-service (QoS) of UL users. The
closed-form solution is theoretically derived by exploiting Lagrangian method
under different numbers of RIS elements and receiving antennas. Simulation
results reveal that the proposed RFSC scheme outperforms the scenario without
RIS deployment in terms of higher signal-to-interference-plus-noise ratio
(SINR). Due to effective interference mitigation, the proposed RFSC can achieve
the highest SINR compared to other existing schemes in open literatures.

    

### [[2112.07938] Blockchain-enabled Server-less Federated Learning](http://arxiv.org/abs/2112.07938)


  Motivated by the heterogeneous nature of devices participating in large-scale
Federated Learning (FL) optimization, we focus on an asynchronous server-less
FL solution empowered by Blockchain (BC) technology. In contrast to mostly
adopted FL approaches, which assume synchronous operation, we advocate an
asynchronous method whereby model aggregation is done as clients submit their
local updates. The asynchronous setting fits well with the federated
optimization idea in practical large-scale settings with heterogeneous clients.
Thus, it potentially leads to higher efficiency in terms of communication
overhead and idle periods. To evaluate the learning completion delay of
BC-enabled FL, we provide an analytical model based on batch service queue
theory. Furthermore, we provide simulation results to assess the performance of
both synchronous and asynchronous mechanisms. Important aspects involved in the
BC-enabled FL optimization, such as the network size, link capacity, or user
requirements, are put together and analyzed. As our results show, the
synchronous setting leads to higher prediction accuracy than the asynchronous
case. Nevertheless, asynchronous federated optimization provides much lower
latency in many cases, thus becoming an appealing FL solution when dealing with
large data sets, tough timing constraints (e.g., near-real-time applications),
or highly varying training data.

    

### [[2112.07941] DRaGon: Mining Latent Radio Channel Information from Geographical Data Leveraging Deep Learning](http://arxiv.org/abs/2112.07941)


  Radio channel modeling is one of the most fundamental aspects in the process
of designing, optimizing, and simulating wireless communication networks. In
this field, long-established approaches such as analytical channel models and
ray tracing techniques represent the de-facto standard methodologies. However,
as demonstrated by recent results, there remains an untapped potential to
innovate this research field by enriching model-based approaches with machine
learning techniques. In this paper, we present Deep RAdio channel modeling from
GeOinformatioN (DRaGon) as a novel machine learning-enabled method for
automatic generation of Radio Environmental Maps (REMs) from geographical data.
For achieving accurate path loss prediction results, DRaGon combines
determining features extracted from a three-dimensional model of the radio
propagation environment with raw images of the receiver area within a deep
learning model. In a comprehensive performance evaluation and validation
campaign, we compare the accuracy of the proposed approach with real world
measurements, ray tracing analyses, and well-known channel models. It is found
that the combination of expert knowledge from the communications domain and the
data analysis capabilities of deep learning allows to achieve a significantly
higher prediction accuracy than the reference methods.

    

### [[2108.00815] Estimating the Peer Degree of Reachable Peers in the Bitcoin P2P Network](http://arxiv.org/abs/2108.00815)


  A recent spam wave of IP addresses in the Bitcoin P2P network allowed us to
estimate the degree distribution of reachable peers in the network. The
resulting distribution shows that about every second reachable peer runs with
Bitcoin Core's default setting of a maximum of 125 concurrent connections and
nearly all connection slots are taken. We validate this result and, in
addition, use our observations of the spam wave to group addresses that belong
to the same peer. By doing this grouping, we improve on previous measurements
and show that simply counting addresses overestimates the number of reachable
peers by 13 %.

    

### [[2112.07673] Machine learning a manifold](http://arxiv.org/abs/2112.07673)


  We propose a simple method to identify a continuous Lie algebra symmetry in a
dataset through regression by an artificial neural network. Our proposal takes
advantage of the $ \mathcal{O}(\epsilon^2)$ scaling of the output variable
under infinitesimal symmetry transformations on the input variables. As
symmetry transformations are generated post-training, the methodology does not
rely on sampling of the full representation space or binning of the dataset,
and the possibility of false identification is minimised. We demonstrate our
method in the SU(3)-symmetric (non-) linear $\Sigma$ model.

    

### [[2112.07701] Conservative and Adaptive Penalty for Model-Based Safe Reinforcement Learning](http://arxiv.org/abs/2112.07701)


  Reinforcement Learning (RL) agents in the real world must satisfy safety
constraints in addition to maximizing a reward objective. Model-based RL
algorithms hold promise for reducing unsafe real-world actions: they may
synthesize policies that obey all constraints using simulated samples from a
learned model. However, imperfect models can result in real-world constraint
violations even for actions that are predicted to satisfy all constraints. We
propose Conservative and Adaptive Penalty (CAP), a model-based safe RL
framework that accounts for potential modeling errors by capturing model
uncertainty and adaptively exploiting it to balance the reward and the cost
objectives. First, CAP inflates predicted costs using an uncertainty-based
penalty. Theoretically, we show that policies that satisfy this conservative
cost constraint are guaranteed to also be feasible in the true environment. We
further show that this guarantees the safety of all intermediate solutions
during RL training. Further, CAP adaptively tunes this penalty during training
using true cost feedback from the environment. We evaluate this conservative
and adaptive penalty-based approach for model-based safe RL extensively on
state and image-based environments. Our results demonstrate substantial gains
in sample-efficiency while incurring fewer violations than prior safe RL
algorithms. Code is available at: this https URL


### [[2112.07718] Scatterbrained: A flexible and expandable pattern for decentralized machine learning](http://arxiv.org/abs/2112.07718)


  Federated machine learning is a technique for training a model across
multiple devices without exchanging data between them. Because data remains
local to each compute node, federated learning is well-suited for use-cases in
fields where data is carefully controlled, such as medicine, or in domains with
bandwidth constraints. One weakness of this approach is that most federated
learning tools rely upon a central server to perform workload delegation and to
produce a single shared model. Here, we suggest a flexible framework for
decentralizing the federated learning pattern, and provide an open-source,
reference implementation compatible with PyTorch.

    

### [[2112.07742] Classifying Emails into Human vs Machine Category](http://arxiv.org/abs/2112.07742)


  It is an essential product requirement of Yahoo Mail to distinguish between
personal and machine-generated emails. The old production classifier in Yahoo
Mail was based on a simple logistic regression model. That model was trained by
aggregating features at the SMTP address level. We propose building deep
learning models at the message level. We built and trained four individual CNN
models: (1) a content model with subject and content as input; (2) a sender
model with sender email address and name as input; (3) an action model by
analyzing email recipients' action patterns and correspondingly generating
target labels based on senders' opening/deleting behaviors; (4) a salutation
model by utilizing senders' "explicit salutation" signal as positive labels.
Next, we built a final full model after exploring different combinations of the
above four models. Experimental results on editorial data show that our full
model improves the adjusted-recall from 70.5% to 78.8% compared to the old
production model, while at the same time lifts the precision from 94.7% to
96.0%. Our full model also significantly beats the state-of-the-art Bert model
at this task. This full model has been deployed into the current production
system (Yahoo Mail 6).

    

### [[2112.07743] Neighborhood Random Walk Graph Sampling for Regularized Bayesian Graph Convolutional Neural Networks](http://arxiv.org/abs/2112.07743)


  In the modern age of social media and networks, graph representations of
real-world phenomena have become an incredibly useful source to mine insights.
Often, we are interested in understanding how entities in a graph are
interconnected. The Graph Neural Network (GNN) has proven to be a very useful
tool in a variety of graph learning tasks including node classification, link
prediction, and edge classification. However, in most of these tasks, the graph
data we are working with may be noisy and may contain spurious edges. That is,
there is a lot of uncertainty associated with the underlying graph structure.
Recent approaches to modeling uncertainty have been to use a Bayesian framework
and view the graph as a random variable with probabilities associated with
model parameters. Introducing the Bayesian paradigm to graph-based models,
specifically for semi-supervised node classification, has been shown to yield
higher classification accuracies. However, the method of graph inference
proposed in recent work does not take into account the structure of the graph.
In this paper, we propose a novel algorithm called Bayesian Graph Convolutional
Network using Neighborhood Random Walk Sampling (BGCN-NRWS), which uses a
Markov Chain Monte Carlo (MCMC) based graph sampling algorithm utilizing graph
structure, reduces overfitting by using a variational inference layer, and
yields consistently competitive classification results compared to the
state-of-the-art in semi-supervised node classification.

    

### [[2112.07745] Learning to track environment state via predictive autoencoding](http://arxiv.org/abs/2112.07745)


  This work introduces a neural architecture for learning forward models of
stochastic environments. The task is achieved solely through learning from
temporal unstructured observations in the form of images. Once trained, the
model allows for tracking of the environment state in the presence of noise or
with new percepts arriving intermittently. Additionally, the state estimate can
be propagated in observation-blind mode, thus allowing for long-term
predictions. The network can output both expectation over future observations
and samples from belief distribution. The resulting functionalities are similar
to those of a Particle Filter (PF). The architecture is evaluated in an
environment where we simulate objects moving. As the forward and sensor models
are available, we implement a PF to gauge the quality of the models learnt from
the data.

    

### [[2112.07746] CEM-GD: Cross-Entropy Method with Gradient Descent Planner for Model-Based Reinforcement Learning](http://arxiv.org/abs/2112.07746)


  Current state-of-the-art model-based reinforcement learning algorithms use
trajectory sampling methods, such as the Cross-Entropy Method (CEM), for
planning in continuous control settings. These zeroth-order optimizers require
sampling a large number of trajectory rollouts to select an optimal action,
which scales poorly for large prediction horizons or high dimensional action
spaces. First-order methods that use the gradients of the rewards with respect
to the actions as an update can mitigate this issue, but suffer from local
optima due to the non-convex optimization landscape. To overcome these issues
and achieve the best of both worlds, we propose a novel planner, Cross-Entropy
Method with Gradient Descent (CEM-GD), that combines first-order methods with
CEM. At the beginning of execution, CEM-GD uses CEM to sample a significant
amount of trajectory rollouts to explore the optimization landscape and avoid
poor local minima. It then uses the top trajectories as initialization for
gradient descent and applies gradient updates to each of these trajectories to
find the optimal action sequence. At each subsequent time step, however, CEM-GD
samples much fewer trajectories from CEM before applying gradient updates. We
show that as the dimensionality of the planning problem increases, CEM-GD
maintains desirable performance with a constant small number of samples by
using the gradient information, while avoiding local optima using initially
well-sampled trajectories. Furthermore, CEM-GD achieves better performance than
CEM on a variety of continuous control benchmarks in MuJoCo with 100x fewer
samples per time step, resulting in around 25% less computation time and 10%
less memory usage. The implementation of CEM-GD is available at
$\href{this https URL}{\text{this https URL}}$.

    

### [[2112.07752] Representation and Invariance in Reinforcement Learning](http://arxiv.org/abs/2112.07752)


  If we changed the rules, would the wise trade places with the fools?
Different groups formalize reinforcement learning (RL) in different ways. If an
agent in one RL formalization is to run within another RL formalization's
environment, the agent must first be converted, or mapped. A criterion of
adequacy for any such mapping is that it preserves relative intelligence. This
paper investigates the formulation and properties of this criterion of
adequacy. However, prior to the problem of formulation is, we argue, the
problem of comparative intelligence. We compare intelligence using
ultrafilters, motivated by viewing agents as candidates in intelligence
elections where voters are environments. These comparators are
counterintuitive, but we prove an impossibility theorem about RL intelligence
measurement, suggesting such counterintuitions are unavoidable. Given a mapping
between RL frameworks, we establish sufficient conditions to ensure that, for
any ultrafilter-based intelligence comparator in the destination framework,
there exists an ultrafilter-based intelligence comparator in the source
framework such that the mapping preserves relative intelligence. We consider
three concrete mappings between various RL frameworks and show that they
satisfy these sufficient conditions and therefore preserve suitably-measured
relative intelligence.

    

### [[2112.07765] Nonlinear Discrete-time Systems' Identification without Persistence of Excitation: A Finite-time Concurrent Learning](http://arxiv.org/abs/2112.07765)


  This paper deals with the problem of finite-time learning for unknown
discrete-time nonlinear systems' dynamics, without the requirement of the
persistence of excitation. A finite-time concurrent learning approach is
presented to approximate the uncertainties of the discrete-time nonlinear
systems in an on-line fashion by employing current data along with recorded
experienced data satisfying an easy-to-check rank condition on the richness of
the recorded data which is less restrictive in comparison with persistence of
excitation condition. Rigorous proofs guarantee the finite-time convergence of
the estimated parameters to their optimal values based on a discrete-time
Lyapunov analysis. Compared with the existing work in the literature,
simulation results illustrate that the proposed method can timely and precisely
approximate the uncertainties.

    

### [[2112.07768] Efficient Dynamic Graph Representation Learning at Scale](http://arxiv.org/abs/2112.07768)


  Dynamic graphs with ordered sequences of events between nodes are prevalent
in real-world industrial applications such as e-commerce and social platforms.
However, representation learning for dynamic graphs has posed great
computational challenges due to the time and structure dependency and irregular
nature of the data, preventing such models from being deployed to real-world
applications. To tackle this challenge, we propose an efficient algorithm,
Efficient Dynamic Graph lEarning (EDGE), which selectively expresses certain
temporal dependency via training loss to improve the parallelism in
computations. We show that EDGE can scale to dynamic graphs with millions of
nodes and hundreds of millions of temporal events and achieve new
state-of-the-art (SOTA) performance.

    

### [[2112.07782] Deciphering antibody affinity maturation with language models and weakly supervised learning](http://arxiv.org/abs/2112.07782)


  In response to pathogens, the adaptive immune system generates specific
antibodies that bind and neutralize foreign antigens. Understanding the
composition of an individual's immune repertoire can provide insights into this
process and reveal potential therapeutic antibodies. In this work, we explore
the application of antibody-specific language models to aid understanding of
immune repertoires. We introduce AntiBERTy, a language model trained on 558M
natural antibody sequences. We find that within repertoires, our model clusters
antibodies into trajectories resembling affinity maturation. Importantly, we
show that models trained to predict highly redundant sequences under a multiple
instance learning framework identify key binding residues in the process. With
further development, the methods presented here will provide new insights into
antigen binding from repertoire sequences alone.

    

### [[2112.07785] Variable Selection and Regularization via Arbitrary Rectangle-range Generalized Elastic Net](http://arxiv.org/abs/2112.07785)


  We introduce the arbitrary rectangle-range generalized elastic net penalty
method, abbreviated to ARGEN, for performing constrained variable selection and
regularization in high-dimensional sparse linear models. As a natural extension
of the nonnegative elastic net penalty method, ARGEN is proved to have variable
selection consistency and estimation consistency under some conditions. The
asymptotic behavior in distribution of the ARGEN estimators have been studied.
We also propose an algorithm called MU-QP-RR-W-$l_1$ to efficiently solve
ARGEN. By conducting simulation study we show that ARGEN outperforms the
elastic net in a number of settings. Finally an application of S&P 500 index
tracking with constraints on the stock allocations is performed to provide
general guidance for adapting ARGEN to solve real-world problems.

    

### [[2112.07791] A Simple But Powerful Graph Encoder for Temporal Knowledge Graph Completion](http://arxiv.org/abs/2112.07791)


  While knowledge graphs contain rich semantic knowledge of various entities
and the relational information among them, temporal knowledge graphs (TKGs)
further indicate the interactions of the entities over time. To study how to
better model TKGs, automatic temporal knowledge graph completion (TKGC) has
gained great interest. Recent TKGC methods aim to integrate advanced deep
learning techniques, e.g., attention mechanism and Transformer, to boost model
performance. However, we find that compared to adopting various kinds of
complex modules, it is more beneficial to better utilize the whole amount of
temporal information along the time axis. In this paper, we propose a simple
but powerful graph encoder TARGCN for TKGC. TARGCN is parameter-efficient, and
it extensively utilizes the information from the whole temporal context. We
perform experiments on three benchmark datasets. Our model can achieve a more
than 42% relative improvement on GDELT dataset compared with the
state-of-the-art model. Meanwhile, it outperforms the strongest baseline on
ICEWS05-15 dataset with around 18.5% fewer parameters.

    

### [[2112.07804] Tackling the Generative Learning Trilemma with Denoising Diffusion GANs](http://arxiv.org/abs/2112.07804)


  A wide variety of deep generative models has been developed in the past
decade. Yet, these models often struggle with simultaneously addressing three
key requirements including: high sample quality, mode coverage, and fast
sampling. We call the challenge imposed by these requirements the generative
learning trilemma, as the existing models often trade some of them for others.
Particularly, denoising diffusion models have shown impressive sample quality
and diversity, but their expensive sampling does not yet allow them to be
applied in many real-world applications. In this paper, we argue that slow
sampling in these models is fundamentally attributed to the Gaussian assumption
in the denoising step which is justified only for small step sizes. To enable
denoising with large steps, and hence, to reduce the total number of denoising
steps, we propose to model the denoising distribution using a complex
multimodal distribution. We introduce denoising diffusion generative
adversarial networks (denoising diffusion GANs) that model each denoising step
using a multimodal conditional GAN. Through extensive evaluations, we show that
denoising diffusion GANs obtain sample quality and diversity competitive with
original diffusion models while being 2000$\times$ faster on the CIFAR-10
dataset. Compared to traditional GANs, our model exhibits better mode coverage
and sample diversity. To the best of our knowledge, denoising diffusion GAN is
the first model that reduces sampling cost in diffusion models to an extent
that allows them to be applied to real-world applications inexpensively.
Project page and code: this https URL


### [[2112.07805] Network Graph Based Neural Architecture Search](http://arxiv.org/abs/2112.07805)


  Neural architecture search enables automation of architecture design. Despite
its success, it is computationally costly and does not provide an insight on
how to design a desirable architecture. Here we propose a new way of searching
neural network where we search neural architecture by rewiring the
corresponding graph and predict the architecture performance by graph
properties. Because we do not perform machine learning over the entire graph
space and use predicted architecture performance to search architecture, the
searching process is remarkably efficient. We find graph based search can give
a reasonably good prediction of desirable architecture. In addition, we find
graph properties that are effective to predict architecture performance. Our
work proposes a new way of searching neural architecture and provides insights
on neural architecture design.

    

### [[2112.07806] Understanding Feature Transfer Through Representation Alignment](http://arxiv.org/abs/2112.07806)


  Training with the true labels of a dataset as opposed to randomized labels
leads to faster optimization and better generalization. This difference is
attributed to a notion of alignment between inputs and labels in natural
datasets. We find that training neural networks with different architectures
and optimizers on random or true labels enforces the same relationship between
the hidden representations and the training labels, elucidating why neural
network representations have been so successful for transfer. We first
highlight why aligned features promote transfer and show in a classic synthetic
transfer problem that alignment is the determining factor for positive and
negative transfer to similar and dissimilar tasks. We then investigate a
variety of neural network architectures and find that (a) alignment emerges
across a variety of different architectures and optimizers, with more alignment
arising from depth (b) alignment increases for layers closer to the output and
(c) existing high-performance deep CNNs exhibit high levels of alignment.

    

### [[2112.07823] Bayesian Graph Contrastive Learning](http://arxiv.org/abs/2112.07823)


  Contrastive learning has become a key component of self-supervised learning
approaches for graph-structured data. However, despite their success, existing
graph contrastive learning methods are incapable of uncertainty quantification
for node representations or their downstream tasks, limiting their application
in high-stakes domains. In this paper, we propose a novel Bayesian perspective
of graph contrastive learning methods showing random augmentations leads to
stochastic encoders. As a result, our proposed method represents each node by a
distribution in the latent space in contrast to existing techniques which embed
each node to a deterministic vector. By learning distributional
representations, we provide uncertainty estimates in downstream graph analytics
tasks and increase the expressive power of the predictive model. In addition,
we propose a Bayesian framework to infer the probability of perturbations in
each view of the contrastive model, eliminating the need for a computationally
expensive search for hyperparameter tuning. We empirically show a considerable
improvement in performance compared to existing state-of-the-art methods on
several benchmark datasets.

    

### [[2112.07824] Analog/Mixed-Signal Circuit Synthesis Enabled by the Advancements of Circuit Architectures and Machine Learning Algorithms](http://arxiv.org/abs/2112.07824)


  Analog mixed-signal (AMS) circuit architecture has evolved towards more
digital friendly due to technology scaling and demand for higher
flexibility/reconfigurability. Meanwhile, the design complexity and cost of AMS
circuits has substantially increased due to the necessity of optimizing the
circuit sizing, layout, and verification of a complex AMS circuit. On the other
hand, machine learning (ML) algorithms have been under exponential growth over
the past decade and actively exploited by the electronic design automation
(EDA) community. This paper will identify the opportunities and challenges
brought about by this trend and overview several emerging AMS design
methodologies that are enabled by the recent evolution of AMS circuit
architectures and machine learning algorithms. Specifically, we will focus on
using neural-network-based surrogate models to expedite the circuit design
parameter search and layout iterations. Lastly, we will demonstrate the rapid
synthesis of several AMS circuit examples from specification to silicon
prototype, with significantly reduced human intervention.

    

### [[2112.07825] TAFA: Design Automation of Analog Mixed-Signal FIR Filters Using Time Approximation Architecture](http://arxiv.org/abs/2112.07825)


  A digital finite impulse response (FIR) filter design is fully synthesizable,
thanks to the mature CAD support of digital circuitry. On the contrary, analog
mixed-signal (AMS) filter design is mostly a manual process, including
architecture selection, schematic design, and layout. This work presents a
systematic design methodology to automate AMS FIR filter design using a time
approximation architecture without any tunable passive component, such as
switched capacitor or resistor. It not only enhances the flexibility of the
filter but also facilitates design automation with reduced analog complexity.
The proposed design flow features a hybrid approximation scheme that
automatically optimize the filter's impulse response in light of time
quantization effects, which shows significant performance improvement with
minimum designer's efforts in the loop. Additionally, a layout-aware regression
model based on an artificial neural network (ANN), in combination with
gradient-based search algorithm, is used to automate and expedite the filter
design. With the proposed framework, we demonstrate rapid synthesis of AMS FIR
filters in 65nm process from specification to layout.

    

### [[2112.07836] Communication-Efficient Distributed SGD with Compressed Sensing](http://arxiv.org/abs/2112.07836)


  We consider large scale distributed optimization over a set of edge devices
connected to a central server, where the limited communication bandwidth
between the server and edge devices imposes a significant bottleneck for the
optimization procedure. Inspired by recent advances in federated learning, we
propose a distributed stochastic gradient descent (SGD) type algorithm that
exploits the sparsity of the gradient, when possible, to reduce communication
burden. At the heart of the algorithm is to use compressed sensing techniques
for the compression of the local stochastic gradients at the device side; and
at the server side, a sparse approximation of the global stochastic gradient is
recovered from the noisy aggregated compressed local gradients. We conduct
theoretical analysis on the convergence of our algorithm in the presence of
noise perturbation incurred by the communication channels, and also conduct
numerical experiments to corroborate its effectiveness.

    

### [[2112.07837] CentSmoothie: Central-Smoothing Hypergraph Neural Networks for Predicting Drug-Drug Interactions](http://arxiv.org/abs/2112.07837)


  Predicting drug-drug interactions (DDI) is the problem of predicting side
effects (unwanted outcomes) of a pair of drugs using drug information and known
side effects of many pairs. This problem can be formulated as predicting labels
(i.e. side effects) for each pair of nodes in a DDI graph, of which nodes are
drugs and edges are interacting drugs with known labels. State-of-the-art
methods for this problem are graph neural networks (GNNs), which leverage
neighborhood information in the graph to learn node representations. For DDI,
however, there are many labels with complicated relationships due to the nature
of side effects. Usual GNNs often fix labels as one-hot vectors that do not
reflect label relationships and potentially do not obtain the highest
performance in the difficult cases of infrequent labels. In this paper, we
formulate DDI as a hypergraph where each hyperedge is a triple: two nodes for
drugs and one node for a label. We then present CentSmoothie, a hypergraph
neural network that learns representations of nodes and labels altogether with
a novel central-smoothing formulation. We empirically demonstrate the
performance advantages of CentSmoothie in simulations as well as real datasets.

    

### [[2112.07839] LoSAC: An Efficient Local Stochastic Average Control Method for Federated Optimization](http://arxiv.org/abs/2112.07839)


  Federated optimization (FedOpt), which targets at collaboratively training a
learning model across a large number of distributed clients, is vital for
federated learning. The primary concerns in FedOpt can be attributed to the
model divergence and communication efficiency, which significantly affect the
performance. In this paper, we propose a new method, i.e., LoSAC, to learn from
heterogeneous distributed data more efficiently. Its key algorithmic insight is
to locally update the estimate for the global full gradient after {each}
regular local model update. Thus, LoSAC can keep clients' information refreshed
in a more compact way. In particular, we have studied the convergence result
for LoSAC. Besides, the bonus of LoSAC is the ability to defend the information
leakage from the recent technique Deep Leakage Gradients (DLG). Finally,
experiments have verified the superiority of LoSAC comparing with
state-of-the-art FedOpt algorithms. Specifically, LoSAC significantly improves
communication efficiency by more than $100\%$ on average, mitigates the model
divergence problem and equips with the defense ability against DLG.

    

### [[2112.07844] Fix your Models by Fixing your Datasets](http://arxiv.org/abs/2112.07844)


  The quality of underlying training data is very crucial for building
performant machine learning models with wider generalizabilty. However, current
machine learning (ML) tools lack streamlined processes for improving the data
quality. So, getting data quality insights and iteratively pruning the errors
to obtain a dataset which is most representative of downstream use cases is
still an ad-hoc manual process. Our work addresses this data tooling gap,
required to build improved ML workflows purely through data-centric techniques.
More specifically, we introduce a systematic framework for (1) finding noisy or
mislabelled samples in the dataset and, (2) identifying the most informative
samples, which when included in training would provide maximal model
performance lift. We demonstrate the efficacy of our framework on public as
well as private enterprise datasets of two Fortune 500 companies, and are
confident this work will form the basis for ML teams to perform more
intelligent data discovery and pruning.

    

### [[2112.07858] EDAssistant: Supporting Exploratory Data Analysis in Computational Notebooks with In-Situ Code Search and Recommendation](http://arxiv.org/abs/2112.07858)


  Using computational notebooks (e.g., Jupyter Notebook), data scientists
rationalize their exploratory data analysis (EDA) based on their prior
experience and external knowledge such as online examples. For novices or data
scientists who lack specific knowledge about the dataset or problem to
investigate, effectively obtaining and understanding the external information
is critical to carry out EDA. This paper presents EDAssistant, a JupyterLab
extension that supports EDA with in-situ search of example notebooks and
recommendation of useful APIs, powered by novel interactive visualization of
search results. The code search and recommendation are enabled by
state-of-the-art machine learning models, trained on a large corpus of EDA
notebooks collected online. A user study is conducted to investigate both
EDAssistant and data scientists' current practice (i.e., using external search
engines). The results demonstrate the effectiveness and usefulness of
EDAssistant, and participants appreciated its smooth and in-context support of
EDA. We also report several design implications regarding code recommendation
tools.

    

### [[2112.07859] Finite-Sample Analysis of Decentralized Q-Learning for Stochastic Games](http://arxiv.org/abs/2112.07859)


  Learning in stochastic games is arguably the most standard and fundamental
setting in multi-agent reinforcement learning (MARL). In this paper, we
consider decentralized MARL in stochastic games in the non-asymptotic regime.
In particular, we establish the finite-sample complexity of fully decentralized
Q-learning algorithms in a significant class of general-sum stochastic games
(SGs) - weakly acyclic SGs, which includes the common cooperative MARL setting
with an identical reward to all agents (a Markov team problem) as a special
case. We focus on the practical while challenging setting of fully
decentralized MARL, where neither the rewards nor the actions of other agents
can be observed by each agent. In fact, each agent is completely oblivious to
the presence of other decision makers. Both the tabular and the linear function
approximation cases have been considered. In the tabular setting, we analyze
the sample complexity for the decentralized Q-learning algorithm to converge to
a Markov perfect equilibrium (Nash equilibrium). With linear function
approximation, the results are for convergence to a linear approximated
equilibrium - a new notion of equilibrium that we propose - which describes
that each agent's policy is a best reply (to other agents) within a linear
space. Numerical experiments are also provided for both settings to demonstrate
the results.

    

### [[2112.07862] Fast Computation of Generalized Eigenvectors for Manifold Graph Embedding](http://arxiv.org/abs/2112.07862)


  Our goal is to efficiently compute low-dimensional latent coordinates for
nodes in an input graph -- known as graph embedding -- for subsequent data
processing such as clustering. Focusing on finite graphs that are interpreted
as uniformly samples on continuous manifolds (called manifold graphs), we
leverage existing fast extreme eigenvector computation algorithms for speedy
execution. We first pose a generalized eigenvalue problem for sparse matrix
pair $(\A,\B)$, where $\A = Ł- \mu \Q + \epsilon \I$ is a sum of graph
Laplacian $Ł$ and disconnected two-hop difference matrix $\Q$. Eigenvector
$\v$ minimizing Rayleigh quotient $\frac{\v^{\top} \A \v}{\v^{\top} \v}$ thus
minimizes $1$-hop neighbor distances while maximizing distances between
disconnected $2$-hop neighbors, preserving graph structure. Matrix $\B =
\text{diag}(\{\b_i\})$ that defines eigenvector orthogonality is then chosen so
that boundary / interior nodes in the sampling domain have the same generalized
degrees. $K$-dimensional latent vectors for the $N$ graph nodes are the first
$K$ generalized eigenvectors for $(\A,\B)$, computed in $\cO(N)$ using LOBPCG,
where $K \ll N$. Experiments show that our embedding is among the fastest in
the literature, while producing the best clustering performance for manifold
graphs.

    

### [[2112.07869] Fine-Tuning Large Neural Language Models for Biomedical Natural Language Processing](http://arxiv.org/abs/2112.07869)


  Motivation: A perennial challenge for biomedical researchers and clinical
practitioners is to stay abreast with the rapid growth of publications and
medical notes. Natural language processing (NLP) has emerged as a promising
direction for taming information overload. In particular, large neural language
models facilitate transfer learning by pretraining on unlabeled text, as
exemplified by the successes of BERT models in various NLP applications.
However, fine-tuning such models for an end task remains challenging,
especially with small labeled datasets, which are common in biomedical NLP.
Results: We conduct a systematic study on fine-tuning stability in biomedical
NLP. We show that finetuning performance may be sensitive to pretraining
settings, especially in low-resource domains. Large models have potential to
attain better performance, but increasing model size also exacerbates
finetuning instability. We thus conduct a comprehensive exploration of
techniques for addressing fine-tuning instability. We show that these
techniques can substantially improve fine-tuning performance for lowresource
biomedical NLP applications. Specifically, freezing lower layers is helpful for
standard BERT-BASE models, while layerwise decay is more effective for
BERT-LARGE and ELECTRA models. For low-resource text similarity tasks such as
BIOSSES, reinitializing the top layer is the optimal strategy. Overall,
domainspecific vocabulary and pretraining facilitate more robust models for
fine-tuning. Based on these findings, we establish new state of the art on a
wide range of biomedical NLP applications.
Availability and implementation: To facilitate progress in biomedical NLP, we
release our state-of-the-art pretrained and fine-tuned models:
this https URL.

    

### [[2112.07878] Gaze Estimation with Eye Region Segmentation and Self-Supervised Multistream Learning](http://arxiv.org/abs/2112.07878)


  We present a novel multistream network that learns robust eye representations
for gaze estimation. We first create a synthetic dataset containing eye region
masks detailing the visible eyeball and iris using a simulator. We then perform
eye region segmentation with a U-Net type model which we later use to generate
eye region masks for real-world eye images. Next, we pretrain an eye image
encoder in the real domain with self-supervised contrastive learning to learn
generalized eye representations. Finally, this pretrained eye encoder, along
with two additional encoders for visible eyeball region and iris, are used in
parallel in our multistream framework to extract salient features for gaze
estimation from real-world images. We demonstrate the performance of our method
on the EYEDIAP dataset in two different evaluation settings and achieve
state-of-the-art results, outperforming all the existing benchmarks on this
dataset. We also conduct additional experiments to validate the robustness of
our self-supervised network with respect to different amounts of labeled data
used for training.

    

### [[2112.07884] Experimental quantum advantage with quantum coupon collector](http://arxiv.org/abs/2112.07884)


  An increasing number of communication and computational schemes with quantum
advantages have recently been proposed, which implies that quantum technology
has fertile application prospects. However, demonstrating these schemes
experimentally continues to be a central challenge because of the difficulty in
preparing high-dimensional states or highly entangled states. In this study, we
introduce and analyse a quantum coupon collector protocol by employing coherent
states and simple linear optical elements, which was successfully demonstrated
using realistic experimental equipment. We showed that our protocol can
significantly reduce the number of samples needed to learn a specific set
compared with the classical limit of the coupon collector problem. We also
discuss the potential values and expansions of the quantum coupon collector by
constructing a quantum blind box game. The information transmitted by the
proposed game also broke the classical limit. These results strongly prove the
advantages of quantum mechanics in machine learning and communication
complexity.

    

### [[2112.07890] Investigating myocardial infarction and its effects in patients with urgent medical problems using advanced data mining tools](http://arxiv.org/abs/2112.07890)


  In medical science, it is very important to gather multiple data on different
diseases and one of the most important objectives of the data is to investigate
the diseases. Myocardial infarction is a serious risk factor in mortality and
in previous studies, the main emphasis has been on people with heart disease
and measuring the likelihood of myocardial infarction in them through
demographic features, echocardiography, and electrocardiogram. In contrast, the
purpose of the present study is to utilize data analysis algorithms and compare
their accuracy in patients with a heart attack in order to identify the heart
muscle strength during myocardial infarction by taking into account emergency
operations and consequently predict myocardial infarction. For this purpose,
105 medical records of myocardial infarction patients with fourteen features
including age, the time of emergency operation, Creatine Phosphokinase (CPK)
test, heart rate, blood sugar, and vein are gathered and investigated through
classification techniques of data analysis including random decision forests,
decision tree, support vector machine (SVM), k-nearest neighbor, and ordinal
logistic regression. Finally, the model of random decision forests with an
accuracy of 76% is selected as the best model in terms of the mean evaluation
indicator. Also, seven features of the creatine Phosphokinase test, urea, white
and red blood cell count, blood sugar, time, and hemoglobin are identified as
the most effective features of the ejection fraction variable.

    

### [[2112.07891] Zero-shot Audio Source Separation through Query-based Learning from Weakly-labeled Data](http://arxiv.org/abs/2112.07891)


  Deep learning techniques for separating audio into different sound sources
face several challenges. Standard architectures require training separate
models for different types of audio sources. Although some universal separators
employ a single model to target multiple sources, they have difficulty
generalizing to unseen sources. In this paper, we propose a three-component
pipeline to train a universal audio source separator from a large, but
weakly-labeled dataset: AudioSet. First, we propose a transformer-based sound
event detection system for processing weakly-labeled training data. Second, we
devise a query-based audio separation model that leverages this data for model
training. Third, we design a latent embedding processor to encode queries that
specify audio targets for separation, allowing for zero-shot generalization.
Our approach uses a single model for source separation of multiple sound types,
and relies solely on weakly-labeled data for training. In addition, the
proposed audio separator can be used in a zero-shot setting, learning to
separate types of audio sources that were never seen in training. To evaluate
the separation performance, we test our model on MUSDB18, while training on the
disjoint AudioSet. We further verify the zero-shot performance by conducting
another experiment on audio source types that are held-out from training. The
model achieves comparable Source-to-Distortion Ratio (SDR) performance to
current supervised models in both cases.

    

### [[2112.07893] Graph-based Ensemble Machine Learning for Student Performance Prediction](http://arxiv.org/abs/2112.07893)


  Student performance prediction is a critical research problem to understand
the students' needs, present proper learning opportunities/resources, and
develop the teaching quality. However, traditional machine learning methods
fail to produce stable and accurate prediction results. In this paper, we
propose a graph-based ensemble machine learning method that aims to improve the
stability of single machine learning methods via the consensus of multiple
methods. To be specific, we leverage both supervised prediction methods and
unsupervised clustering methods, build an iterative approach that propagates in
a bipartite graph as well as converges to more stable and accurate prediction
results. Extensive experiments demonstrate the effectiveness of our proposed
method in predicting more accurate student performance. Specifically, our model
outperforms the best traditional machine learning algorithms by up to 14.8% in
prediction accuracy.

    

### [[2112.07895] Robust Depth Completion with Uncertainty-Driven Loss Functions](http://arxiv.org/abs/2112.07895)


  Recovering a dense depth image from sparse LiDAR scans is a challenging task.
Despite the popularity of color-guided methods for sparse-to-dense depth
completion, they treated pixels equally during optimization, ignoring the
uneven distribution characteristics in the sparse depth map and the accumulated
outliers in the synthesized ground truth. In this work, we introduce
uncertainty-driven loss functions to improve the robustness of depth completion
and handle the uncertainty in depth completion. Specifically, we propose an
explicit uncertainty formulation for robust depth completion with Jeffrey's
prior. A parametric uncertain-driven loss is introduced and translated to new
loss functions that are robust to noisy or missing data. Meanwhile, we propose
a multiscale joint prediction model that can simultaneously predict depth and
uncertainty maps. The estimated uncertainty map is also used to perform
adaptive prediction on the pixels with high uncertainty, leading to a residual
map for refining the completion results. Our method has been tested on KITTI
Depth Completion Benchmark and achieved the state-of-the-art robustness
performance in terms of MAE, IMAE, and IRMSE metrics.

    

### [[2112.07897] Learning Graph Partitions](http://arxiv.org/abs/2112.07897)


  Given a partition of a graph into connected components, the membership oracle
asserts whether any two vertices of the graph lie in the same component or not.
We prove that for $n\ge k\ge 2$, learning the components of an $n$-vertex
hidden graph with $k$ components requires at least $\frac{1}{2}(n-k)(k-1)$
membership queries. This proves the optimality of the $O(nk)$ algorithm
proposed by Reyzin and Srivastava (2007) for this problem, improving on the
best known information-theoretic bound of $\Omega(n\log k)$ queries. Further,
we construct an oracle that can learn the number of components of $G$ in
asymptotically fewer queries than learning the full partition, thus answering
another question posed by the same authors. Lastly, we introduce a more
applicable version of this oracle, and prove asymptotically tight bounds of
$\widetilde\Theta(m)$ queries for both learning and verifying an $m$-edge
hidden graph $G$ using this oracle.

    

### [[2112.07901] Energy-Efficient Real-Time Heart Monitoring on Edge-Fog-Cloud Internet-of-Medical-Things](http://arxiv.org/abs/2112.07901)


  The recent developments in wearable devices and the Internet of Medical
Things (IoMT) allow real-time monitoring and recording of electrocardiogram
(ECG) signals. However, continuous monitoring of ECG signals is challenging in
low-power wearable devices due to energy and memory constraints. Therefore, in
this paper, we present a novel and energy-efficient methodology for
continuously monitoring the heart for low-power wearable devices. The proposed
methodology is composed of three different layers: 1) a Noise/Artifact
detection layer to grade the quality of the ECG signals; 2) a Normal/Abnormal
beat classification layer to detect the anomalies in the ECG signals, and 3) an
Abnormal beat classification layer to detect diseases from ECG signals.
Moreover, a distributed multi-output Convolutional Neural Network (CNN)
architecture is used to decrease the energy consumption and latency between the
edge-fog/cloud. Our methodology reaches an accuracy of 99.2% on the well-known
MIT-BIH Arrhythmia dataset. Evaluation on real hardware shows that our
methodology is suitable for devices having a minimum RAM of 32KB. Moreover, the
proposed methodology achieves $7\times$ more energy efficiency compared to
state-of-the-art works.

    

### [[2112.07913] A Comparative Analysis of Machine Learning Approaches for Automated Face Mask Detection During COVID-19](http://arxiv.org/abs/2112.07913)


  The World Health Organization (WHO) has recommended wearing face masks as one
of the most effective measures to prevent COVID-19 transmission. In many
countries, it is now mandatory to wear face masks, specially in public places.
Since manual monitoring of face masks is often infeasible in the middle of the
crowd, automatic detection can be beneficial. To facilitate that, we explored a
number of deep learning models (i.e., VGG1, VGG19, ResNet50) for face-mask
detection and evaluated them on two benchmark datasets. We also evaluated
transfer learning (i.e., VGG19, ResNet50 pre-trained on ImageNet) in this
context. We find that while the performances of all the models are quite good,
transfer learning models achieve the best performance. Transfer learning
improves the performance by 0.10\%--0.40\% with 30\% less training time. Our
experiment also shows these high-performing models are not quite robust for
real-world cases where the test dataset comes from a different distribution.
Without any fine-tuning, the performance of these models drops by 47\% in
cross-domain settings.

    

### [[2112.07922] Ten years of image analysis and machine learning competitions in dementia](http://arxiv.org/abs/2112.07922)


  Machine learning methods exploiting multi-parametric biomarkers, especially
based on neuroimaging, have huge potential to improve early diagnosis of
dementia and to predict which individuals are at-risk of developing dementia.
To benchmark algorithms in the field of machine learning and neuroimaging in
dementia and assess their potential for use in clinical practice and clinical
trials, seven grand challenges have been organized in the last decade: MIRIAD,
Alzheimer's Disease Big Data DREAM, CADDementia, Machine Learning Challenge,
MCI Neuroimaging, TADPOLE, and the Predictive Analytics Competition. Based on
two challenge evaluation frameworks, we analyzed how these grand challenges are
complementing each other regarding research questions, datasets, validation
approaches, results and impact. The seven grand challenges addressed questions
related to screening, diagnosis, prediction and monitoring in (pre-clinical)
dementia. There was little overlap in clinical questions, tasks and performance
metrics. Whereas this has the advantage of providing insight on a broad range
of questions, it also limits the validation of results across challenges. In
general, winning algorithms performed rigorous data pre-processing and combined
a wide range of input features. Despite high state-of-the-art performances,
most of the methods evaluated by the challenges are not clinically used. To
increase impact, future challenges could pay more attention to statistical
analysis of which factors (i.e., features, models) relate to higher
performance, to clinical questions beyond Alzheimer's disease, and to using
testing data beyond the Alzheimer's Disease Neuroimaging Initiative. Given the
potential and lessons learned in the past ten years, we are excited by the
prospects of grand challenges in machine learning and neuroimaging for the next
ten years and beyond.

    

### [[2112.07934] Graph Representation Learning via Contrasting Cluster Assignments](http://arxiv.org/abs/2112.07934)


  With the rise of contrastive learning, unsupervised graph representation
learning has been booming recently, even surpassing the supervised counterparts
in some machine learning tasks. Most of existing contrastive models for graph
representation learning either focus on maximizing mutual information between
local and global embeddings, or primarily depend on contrasting embeddings at
node level. However, they are still not exquisite enough to comprehensively
explore the local and global views of network topology. Although the former
considers local-global relationship, its coarse global information leads to
grudging cooperation between local and global views. The latter pays attention
to node-level feature alignment, so that the role of global view appears
inconspicuous. To avoid falling into these two extreme cases, we propose a
novel unsupervised graph representation model by contrasting cluster
assignments, called as GRCCA. It is motivated to make good use of local and
global information synthetically through combining clustering algorithms and
contrastive learning. This not only facilitates the contrastive effect, but
also provides the more high-quality graph information. Meanwhile, GRCCA further
excavates cluster-level information, which make it get insight to the elusive
association between nodes beyond graph topology. Specifically, we first
generate two augmented graphs with distinct graph augmentation strategies, then
employ clustering algorithms to obtain their cluster assignments and prototypes
respectively. The proposed GRCCA further compels the identical nodes from
different augmented graphs to recognize their cluster assignments mutually by
minimizing a cross entropy loss. To demonstrate its effectiveness, we compare
with the state-of-the-art models in three different downstream tasks. The
experimental results show that GRCCA has strong competitiveness in most tasks.

    

### [[2112.07945] Efficient Geometry-aware 3D Generative Adversarial Networks](http://arxiv.org/abs/2112.07945)


  Unsupervised generation of high-quality multi-view-consistent images and 3D
shapes using only collections of single-view 2D photographs has been a
long-standing challenge. Existing 3D GANs are either compute-intensive or make
approximations that are not 3D-consistent; the former limits quality and
resolution of the generated images and the latter adversely affects multi-view
consistency and shape quality. In this work, we improve the computational
efficiency and image quality of 3D GANs without overly relying on these
approximations. For this purpose, we introduce an expressive hybrid
explicit-implicit network architecture that, together with other design
choices, synthesizes not only high-resolution multi-view-consistent images in
real time but also produces high-quality 3D geometry. By decoupling feature
generation and neural rendering, our framework is able to leverage
state-of-the-art 2D CNN generators, such as StyleGAN2, and inherit their
efficiency and expressiveness. We demonstrate state-of-the-art 3D-aware
synthesis with FFHQ and AFHQ Cats, among other experiments.

    

### [[2112.07955] Channel Parameter Estimation in the Presence of Phase Noise Based on Maximum Correntropy Criterion](http://arxiv.org/abs/2112.07955)


  Oscillator output generally has phase noise causing the output power spectral
density (PSD) to disperse around a Dirac delta function. In this paper, the
AWGN channel is considered, where the sent signal accompanying with phase noise
is added to the channel Gaussian noise and received at the receiver.
Conventional channel estimation algorithms such as least mean square (LMS) and
mean MSE criterion are not suitable for this channel estimation. We (i) analyze
this phase noise channel estimation with information theoretic learning (ITL)
criterion, i.e., maximum correntropy criterion (MCC), leading to robustness in
the channel estimator's steady state behavior; and (ii) improve the convergence
rate by combining MSE and MCC as a novel mixed-LMS algorithm.

    

### [[2112.07962] A learning-based approach to feature recognition of Engineering shapes](http://arxiv.org/abs/2112.07962)


  In this paper, we propose a machine learning approach to recognise
engineering shape features such as holes, slots, etc. in a CAD mesh model. With
the advent of digital archiving, newer manufacturing techniques such as 3D
printing, scanning of components and reverse engineering, CAD data is
proliferated in the form of mesh model representation. As the number of nodes
and edges become larger in a mesh model as well as the possibility of presence
of noise, direct application of graph-based approaches would not only be
expensive but also difficult to be tuned for noisy data. Hence, this calls for
newer approaches to be devised for feature recognition for CAD models
represented in the form of mesh. Here, we show that a discrete version of Gauss
map can be used as a signature for a feature learning. We show that this
approach not only requires fewer memory requirements but also the training time
is quite less. As no network architecture is involved, the number of
hyperparameters are much lesser and can be tuned in a much faster time. The
recognition accuracy is also very similar to that of the one obtained using 3D
convolutional neural networks (CNN) but in much lesser running time and storage
requirements. A comparison has been done with other non-network based machine
learning approaches to show that our approach has the highest accuracy. We also
show the recognition results for CAD models having multiple features as well as
complex/interacting features obtained from public benchmarks. The ability to
handle noisy data has also been demonstrated.

    

### [[2112.07963] Towards General and Efficient Active Learning](http://arxiv.org/abs/2112.07963)


  Active learning aims to select the most informative samples to exploit
limited annotation budgets. Most existing work follows a cumbersome pipeline by
repeating the time-consuming model training and batch data selection multiple
times on each dataset separately. We challenge this status quo by proposing a
novel general and efficient active learning (GEAL) method in this paper.
Utilizing a publicly available model pre-trained on a large dataset, our method
can conduct data selection processes on different datasets with a single-pass
inference of the same model. To capture the subtle local information inside
images, we propose knowledge clusters that are easily extracted from the
intermediate features of the pre-trained network. Instead of the troublesome
batch selection strategy, all data samples are selected in one go by performing
K-Center-Greedy in the fine-grained knowledge cluster level. The entire
procedure only requires single-pass model inference without training or
supervision, making our method notably superior to prior arts in terms of time
complexity by up to hundreds of times. Extensive experiments widely demonstrate
the promising performance of our method on object detection, semantic
segmentation, depth estimation, and image classification.

    

### [[2112.07985] Solving the Data Sparsity Problem in Predicting the Success of the Startups with Machine Learning Methods](http://arxiv.org/abs/2112.07985)


  Predicting the success of startup companies is of great importance for both
startup companies and investors. It is difficult due to the lack of available
data and appropriate general methods. With data platforms like Crunchbase
aggregating the information of startup companies, it is possible to predict
with machine learning algorithms. Existing research suffers from the data
sparsity problem as most early-stage startup companies do not have much data
available to the public. We try to leverage the recent algorithms to solve this
problem. We investigate several machine learning algorithms with a large
dataset from Crunchbase. The results suggest that LightGBM and XGBoost perform
best and achieve 53.03% and 52.96% F1 scores. We interpret the predictions from
the perspective of feature contribution. We construct portfolios based on the
models and achieve high success rates. These findings have substantial
implications on how machine learning methods can help startup companies and
investors.

    

### [[2112.07995] Domain-informed neural networks for interaction localization within astroparticle experiments](http://arxiv.org/abs/2112.07995)


  This work proposes a domain-informed neural network architecture for
experimental particle physics, using particle interaction localization with the
time-projection chamber (TPC) technology for dark matter research as an example
application. A key feature of the signals generated within the TPC is that they
allow localization of particle interactions through a process called
reconstruction. While multilayer perceptrons (MLPs) have emerged as a leading
contender for reconstruction in TPCs, such a black-box approach does not
reflect prior knowledge of the underlying scientific processes. This paper
looks anew at neural network-based interaction localization and encodes prior
detector knowledge, in terms of both signal characteristics and detector
geometry, into the feature encoding and the output layers of a multilayer
neural network. The resulting Domain-informed Neural Network (DiNN limits the
receptive fields of the neurons in the initial feature encoding layers in order
to account for the spatially localized nature of the signals produced within
the TPC. This aspect of the DiNN, which has similarities with the emerging area
of graph neural networks in that the neurons in the initial layers only connect
to a handful of neurons in their succeeding layer, significantly reduces the
number of parameters in the network in comparison to an MLP. In addition, in
order to account for the detector geometry, the output layers of the network
are modified using two geometric transformations to ensure the DiNN produces
localizations within the interior of the detector. The end result is a neural
network architecture that has 60% fewer parameters than an MLP, but that still
achieves similar localization performance and provides a path to future
architectural developments with improved performance because of their ability
to encode additional domain knowledge into the architecture.

    

### [[2112.07998] Multi-modal Networks Reveal Patterns of Operational Similarity of Terrorist Organizations](http://arxiv.org/abs/2112.07998)


  Capturing dynamics of operational similarity among terrorist groups is
critical to provide actionable insights for counter-terrorism and intelligence
monitoring. Yet, in spite of its theoretical and practical relevance, research
addressing this problem is currently lacking. We tackle this problem proposing
a novel computational framework for detecting clusters of terrorist groups
sharing similar behaviors, focusing on groups' yearly repertoire of deployed
tactics, attacked targets, and utilized weapons. Specifically considering those
organizations that have plotted at least 50 attacks from 1997 to 2018,
accounting for a total of 105 groups responsible for more than 42,000 events
worldwide, we offer three sets of results. First, we show that over the years
global terrorism has been characterized by increasing operational cohesiveness.
Second, we highlight that year-to-year stability in co-clustering among groups
has been particularly high from 2009 to 2018, indicating temporal consistency
of similarity patterns in the last decade. Third, we demonstrate that
operational similarity between two organizations is driven by three factors:
(a) their overall activity; (b) the difference in the diversity of their
operational repertoires; (c) the difference in a combined measure of diversity
and activity. Groups' operational preferences, geographical homophily and
ideological affinity have no consistent role in determining operational
similarity.

    

### [[2112.08018] MissMarple : A Novel Socio-inspired Feature-transfer Learning Deep Network for Image Splicing Detection](http://arxiv.org/abs/2112.08018)


  In this paper we propose a novel socio-inspired convolutional neural network
(CNN) deep learning model for image splicing detection. Based on the premise
that learning from the detection of coarsely spliced image regions can improve
the detection of visually imperceptible finely spliced image forgeries, the
proposed model referred to as, MissMarple, is a twin CNN network involving
feature-transfer learning. Results obtained from training and testing the
proposed model using the benchmark datasets like Columbia splicing, WildWeb,
DSO1 and a proposed dataset titled AbhAS consisting of realistic splicing
forgeries revealed improvement in detection accuracy over the existing deep
learning models.

    

### [[2112.08025] TLogic: Temporal Logical Rules for Explainable Link Forecasting on Temporal Knowledge Graphs](http://arxiv.org/abs/2112.08025)


  Conventional static knowledge graphs model entities in relational data as
nodes, connected by edges of specific relation types. However, information and
knowledge evolve continuously, and temporal dynamics emerge, which are expected
to influence future situations. In temporal knowledge graphs, time information
is integrated into the graph by equipping each edge with a timestamp or a time
range. Embedding-based methods have been introduced for link prediction on
temporal knowledge graphs, but they mostly lack explainability and
comprehensible reasoning chains. Particularly, they are usually not designed to
deal with link forecasting -- event prediction involving future timestamps. We
address the task of link forecasting on temporal knowledge graphs and introduce
TLogic, an explainable framework that is based on temporal logical rules
extracted via temporal random walks. We compare TLogic with state-of-the-art
baselines on three benchmark datasets and show better overall performance while
our method also provides explanations that preserve time consistency.
Furthermore, in contrast to most state-of-the-art embedding-based methods,
TLogic works well in the inductive setting where already learned rules are
transferred to related datasets with a common vocabulary.

    

### [[2112.08052] Optimal Latent Space Forecasting for Large Collections of Short Time Series Using Temporal Matrix Factorization](http://arxiv.org/abs/2112.08052)


  In the context of time series forecasting, it is a common practice to
evaluate multiple methods and choose one of these methods or an ensemble for
producing the best forecasts. However, choosing among different ensembles over
multiple methods remains a challenging task that undergoes a combinatorial
explosion as the number of methods increases. In the context of demand
forecasting or revenue forecasting, this challenge is further exacerbated by a
large number of time series as well as limited historical data points available
due to changing business context. Although deep learning forecasting methods
aim to simultaneously forecast large collections of time series, they become
challenging to apply in such scenarios due to the limited history available and
might not yield desirable results. We propose a framework for forecasting short
high-dimensional time series data by combining low-rank temporal matrix
factorization and optimal model selection on latent time series using
cross-validation. We demonstrate that forecasting the latent factors leads to
significant performance gains as compared to directly applying different
uni-variate models on time series. Performance has been validated on a
truncated version of the M4 monthly dataset which contains time series data
from multiple domains showing the general applicability of the method.
Moreover, it is amenable to incorporating the analyst view of the future owing
to the low number of latent factors which is usually impractical when applying
forecasting methods directly to high dimensional datasets.

    

### [[2112.08055] Building separable approximations for quantum states via neural networks](http://arxiv.org/abs/2112.08055)


  Finding the closest separable state to a given target state is a notoriously
difficult task, even more difficult than deciding whether a state is entangled
or separable. To tackle this task, we parametrize separable states with a
neural network and train it to minimize the distance to a given target state,
with respect to a differentiable distance, such as the trace distance or
Hilbert-Schmidt distance. By examining the output of the algorithm, we can
deduce whether the target state is entangled or not, and construct an
approximation for its closest separable state. We benchmark the method on a
variety of well-known classes of bipartite states and find excellent agreement,
even up to local dimension of $d=10$. Moreover, we show our method to be
efficient in the multipartite case, considering different notions of
separability. Examining three and four-party GHZ and W states we recover known
bounds and obtain novel ones, for instance for triseparability. Finally, we
show how to use the neural network's results to gain analytic insight.

    

### [[2112.08060] Leveraging Image-based Generative Adversarial Networks for Time Series Generation](http://arxiv.org/abs/2112.08060)


  Generative models synthesize image data with great success regarding sampling
quality, diversity and feature disentanglement. Generative models for time
series lack these benefits due to a missing representation, which captures
temporal dynamics and allows inversion for sampling. The paper proposes the
intertemporal return plot (IRP) representation to facilitate the use of
image-based generative adversarial networks for time series generation. The
representation proves effective in capturing time series characteristics and,
compared to alternative representations, benefits from invertibility and
scale-invariance. Empirical benchmarks confirm these features and demonstrate
that the IRP enables an off-the-shelf Wasserstein GAN with gradient penalty to
sample realistic time series, which outperform a specialized RNN-based GAN,
while simultaneously reducing model complexity.

    

### [[2112.08068] Head Matters: Explainable Human-centered Trait Prediction from Head Motion Dynamics](http://arxiv.org/abs/2112.08068)


  We demonstrate the utility of elementary head-motion units termed kinemes for
behavioral analytics to predict personality and interview traits. Transforming
head-motion patterns into a sequence of kinemes facilitates discovery of latent
temporal signatures characterizing the targeted traits, thereby enabling both
efficient and explainable trait prediction. Utilizing Kinemes and Facial Action
Coding System (FACS) features to predict (a) OCEAN personality traits on the
First Impressions Candidate Screening videos, and (b) Interview traits on the
MIT dataset, we note that: (1) A Long-Short Term Memory (LSTM) network trained
with kineme sequences performs better than or similar to a Convolutional Neural
Network (CNN) trained with facial images; (2) Accurate predictions and
explanations are achieved on combining FACS action units (AUs) with kinemes,
and (3) Prediction performance is affected by the time-length over which head
and facial movements are observed.

    

### [[2112.08069] Funnels: Exact maximum likelihood with dimensionality reduction](http://arxiv.org/abs/2112.08069)


  Normalizing flows are diffeomorphic, typically dimension-preserving, models
trained using the likelihood of the model. We use the SurVAE framework to
construct dimension reducing surjective flows via a new layer, known as the
funnel. We demonstrate its efficacy on a variety of datasets, and show it
improves upon or matches the performance of existing flows while having a
reduced latent space size. The funnel layer can be constructed from a wide
range of transformations including restricted convolution and feed forward
layers.

    

### [[2112.08075] Fast characterization of inducible regions of atrial fibrillation models with multi-fidelity Gaussian process classification](http://arxiv.org/abs/2112.08075)


  Computational models of atrial fibrillation have successfully been used to
predict optimal ablation sites. A critical step to assess the effect of an
ablation pattern is to pace the model from different, potentially random,
locations to determine whether arrhythmias can be induced in the atria. In this
work, we propose to use multi-fidelity Gaussian process classification on
Riemannian manifolds to efficiently determine the regions in the atria where
arrhythmias are inducible. We build a probabilistic classifier that operates
directly on the atrial surface. We take advantage of lower resolution models to
explore the atrial surface and combine seamlessly with high-resolution models
to identify regions of inducibility. When trained with 40 samples, our
multi-fidelity classifier shows a balanced accuracy that is 10% higher than a
nearest neighbor classifier used as a baseline atrial fibrillation model, and
9% higher in presence of atrial fibrillation with ablations. We hope that this
new technique will allow faster and more precise clinical applications of
computational models for atrial fibrillation.

    

### [[2112.08078] Joint Demand Prediction for Multimodal Systems: A Multi-task Multi-relational Spatiotemporal Graph Neural Network Approach](http://arxiv.org/abs/2112.08078)


  Dynamic demand prediction is crucial for the efficient operation and
management of urban transportation systems. Extensive research has been
conducted on single-mode demand prediction, ignoring the fact that the demands
for different transportation modes can be correlated with each other. Despite
some recent efforts, existing approaches to multimodal demand prediction are
generally not flexible enough to account for multiplex networks with diverse
spatial units and heterogeneous spatiotemporal correlations across different
modes. To tackle these issues, this study proposes a multi-relational
spatiotemporal graph neural network (ST-MRGNN) for multimodal demand
prediction. Specifically, the spatial dependencies across modes are encoded
with multiple intra- and inter-modal relation graphs. A multi-relational graph
neural network (MRGNN) is introduced to capture cross-mode heterogeneous
spatial dependencies, consisting of generalized graph convolution networks to
learn the message passing mechanisms within relation graphs and an
attention-based aggregation module to summarize different relations. We further
integrate MRGNNs with temporal gated convolution layers to jointly model
heterogeneous spatiotemporal correlations. Extensive experiments are conducted
using real-world subway and ride-hailing datasets from New York City, and the
results verify the improved performance of our proposed approach over existing
methods across modes. The improvement is particularly large for demand-sparse
locations. Further analysis of the attention mechanisms of ST-MRGNN also
demonstrates its good interpretability for understanding cross-mode
interactions.

    

### [[2112.08091] Ensuring DNN Solution Feasibility for Optimization Problems with Convex Constraints and Its Application to DC Optimal Power Flow Problems](http://arxiv.org/abs/2112.08091)


  Ensuring solution feasibility is a key challenge in developing Deep Neural
Network (DNN) schemes for solving constrained optimization problems, due to
inherent DNN prediction errors. In this paper, we propose a "preventive
learning'" framework to systematically guarantee DNN solution feasibility for
problems with convex constraints and general objective functions. We first
apply a predict-and-reconstruct design to not only guarantee equality
constraints but also exploit them to reduce the number of variables to be
predicted by DNN. Then, as a key methodological contribution, we systematically
calibrate inequality constraints used in DNN training, thereby anticipating
prediction errors and ensuring the resulting solutions remain feasible. We
characterize the calibration magnitudes and the DNN size sufficient for
ensuring universal feasibility. We propose a new Adversary-Sample Aware
training algorithm to improve DNN's optimality performance without sacrificing
feasibility guarantee. Overall, the framework provides two DNNs. The first one
from characterizing the sufficient DNN size can guarantee universal feasibility
while the other from the proposed training algorithm further improves
optimality and maintains DNN's universal feasibility simultaneously. We apply
the preventive learning framework to develop DeepOPF+ for solving the essential
DC optimal power flow problem in grid operation. It improves over existing
DNN-based schemes in ensuring feasibility and attaining consistent desirable
speedup performance in both light-load and heavy-load regimes. Simulation
results over IEEE Case-30/118/300 test cases show that DeepOPF+ generates
$100\%$ feasible solutions with $<$0.5% optimality loss and up to two orders of
magnitude computational speedup, as compared to a state-of-the-art iterative
solver.

    

### [[2112.08093] Towards Controllable Agent in MOBA Games with Generative Modeling](http://arxiv.org/abs/2112.08093)


  We propose novel methods to develop action controllable agent that behaves
like a human and has the ability to align with human players in Multiplayer
Online Battle Arena (MOBA) games. By modeling the control problem as an action
generation process, we devise a deep latent alignment neural network model for
training agent, and a corresponding sampling algorithm for controlling an
agent's action. Particularly, we propose deterministic and stochastic attention
implementations of the core latent alignment model. Both simulated and online
experiments in the game Honor of Kings demonstrate the efficacy of the proposed
methods.

    

### [[2112.08094] Automatic tuning of hyper-parameters of reinforcement learning algorithms using Bayesian optimization with behavioral cloning](http://arxiv.org/abs/2112.08094)


  Optimal setting of several hyper-parameters in machine learning algorithms is
key to make the most of available data. To this aim, several methods such as
evolutionary strategies, random search, Bayesian optimization and heuristic
rules of thumb have been proposed. In reinforcement learning (RL), the
information content of data gathered by the learning agent while interacting
with its environment is heavily dependent on the setting of many
hyper-parameters. Therefore, the user of an RL algorithm has to rely on
search-based optimization methods, such as grid search or the Nelder-Mead
simplex algorithm, that are very inefficient for most RL tasks, slows down
significantly the learning curve and leaves to the user the burden of
purposefully biasing data gathering. In this work, in order to make an RL
algorithm more user-independent, a novel approach for autonomous
hyper-parameter setting using Bayesian optimization is proposed. Data from past
episodes and different hyper-parameter values are used at a meta-learning level
by performing behavioral cloning which helps improving the effectiveness in
maximizing a reinforcement learning variant of an acquisition function. Also,
by tightly integrating Bayesian optimization in a reinforcement learning agent
design, the number of state transitions needed to converge to the optimal
policy for a given task is reduced. Computational experiments reveal promising
results compared to other manual tweaking and optimization-based approaches
which highlights the benefits of changing the algorithm hyper-parameters to
increase the information content of generated data.

    

### [[2112.08098] Mask-combine Decoding and Classification Approach for Punctuation Prediction with real-time Inference Constraints](http://arxiv.org/abs/2112.08098)


  In this work, we unify several existing decoding strategies for punctuation
prediction in one framework and introduce a novel strategy which utilises
multiple predictions at each word across different windows. We show that
significant improvements can be achieved by optimising these strategies after
training a model, only leading to a potential increase in inference time, with
no requirement for retraining. We further use our decoding strategy framework
for the first comparison of tagging and classification approaches for
punctuation prediction in a real-time setting. Our results show that a
classification approach for punctuation prediction can be beneficial when
little or no right-side context is available.

    

### [[2112.08102] Robust Neural Network Classification via Double Regularization](http://arxiv.org/abs/2112.08102)


  The presence of mislabeled observations in data is a notoriously challenging
problem in statistics and machine learning, associated with poor generalization
properties for both traditional classifiers and, perhaps even more so, flexible
classifiers like neural networks. Here we propose a novel double regularization
of the neural network training loss that combines a penalty on the complexity
of the classification model and an optimal reweighting of training
observations. The combined penalties result in improved generalization
properties and strong robustness against overfitting in different settings of
mislabeled training data and also against variation in initial parameter values
when training. We provide a theoretical justification for our proposed method
derived for a simple case of logistic regression. We demonstrate the double
regularization model, here denoted by DRFit, for neural net classification of
(i) MNIST and (ii) CIFAR-10, in both cases with simulated mislabeling. We also
illustrate that DRFit identifies mislabeled data points with very good
precision. This provides strong support for DRFit as a practical of-the-shelf
classifier, since, without any sacrifice in performance, we get a classifier
that simultaneously reduces overfitting against mislabeling and gives an
accurate measure of the trustworthiness of the labels.

    

### [[2112.08125] Exponential Convergence of Deep Operator Networks for Elliptic Partial Differential Equations](http://arxiv.org/abs/2112.08125)


  We construct deep operator networks (ONets) between infinite-dimensional
spaces that emulate with an exponential rate of convergence the
coefficient-to-solution map of elliptic second-order PDEs. In particular, we
consider problems set in $d$-dimensional periodic domains, $d=1, 2, \dots$, and
with analytic right-hand sides and coefficients. Our analysis covers
diffusion-reaction problems, parametric diffusion equations, and elliptic
systems such as linear isotropic elastostatics in heterogeneous materials.
We leverage the exponential convergence of spectral collocation methods for
boundary value problems whose solutions are analytic. In the present periodic
and analytic setting, this follows from classical elliptic regularity. Within
the ONet branch and trunk construction of [Chen and Chen, 1993] and of [Lu et
al., 2021], we show the existence of deep ONets which emulate the
coefficient-to-solution map to accuracy $\varepsilon>0$ in the $H^1$ norm,
uniformly over the coefficient set. We prove that the neural networks in the
ONet have size $\mathcal{O}(\left|\log(\varepsilon)\right|^\kappa)$ for some
$\kappa>0$ depending on the physical space dimension.

    

### [[2112.08132] Improving Self-supervised Learning with Automated Unsupervised Outlier Arbitration](http://arxiv.org/abs/2112.08132)


  Our work reveals a structured shortcoming of the existing mainstream
self-supervised learning methods. Whereas self-supervised learning frameworks
usually take the prevailing perfect instance level invariance hypothesis for
granted, we carefully investigate the pitfalls behind. Particularly, we argue
that the existing augmentation pipeline for generating multiple positive views
naturally introduces out-of-distribution (OOD) samples that undermine the
learning of the downstream tasks. Generating diverse positive augmentations on
the input does not always pay off in benefiting downstream tasks. To overcome
this inherent deficiency, we introduce a lightweight latent variable model
UOTA, targeting the view sampling issue for self-supervised learning. UOTA
adaptively searches for the most important sampling region to produce views,
and provides viable choice for outlier-robust self-supervised learning
approaches. Our method directly generalizes to many mainstream self-supervised
learning approaches, regardless of the loss's nature contrastive or not. We
empirically show UOTA's advantage over the state-of-the-art self-supervised
paradigms with evident margin, which well justifies the existence of the OOD
sample issue embedded in the existing approaches. Especially, we theoretically
prove that the merits of the proposal boil down to guaranteed estimator
variance and bias reduction. Code is available: at
this https URL.

    

### [[2112.08165] Chimpanzee voice prints? Insights from transfer learning experiments from human voices](http://arxiv.org/abs/2112.08165)


  Individual vocal differences are ubiquitous in the animal kingdom. In humans,
these differences pervade the entire vocal repertoire and constitute a "voice
print". Apes, our closest-living relatives, possess individual signatures
within specific call types, but the potential for a unique voice print has been
little investigated. This is partially attributed to the limitations associated
with extracting meaningful features from small data sets. Advances in machine
learning have highlighted an alternative to traditional acoustic features,
namely pre-trained learnt extractors. Here, we present an approach building on
these developments: leveraging a feature extractor based on a deep neural
network trained on over 10,000 human voice prints to provide an informative
space over which we identify chimpanzee voice prints. We compare our results
with those obtained by using traditional acoustic features and discuss the
benefits of our methodology and the significance of our findings for the
identification of "voice prints" in non-human animals.

    

### [[2112.08176] AMSER: Adaptive Multi-modal Sensing for Energy Efficient and Resilient eHealth Systems](http://arxiv.org/abs/2112.08176)


  eHealth systems deliver critical digital healthcare and wellness services for
users by continuously monitoring physiological and contextual data. eHealth
applications use multi-modal machine learning kernels to analyze data from
different sensor modalities and automate decision-making. Noisy inputs and
motion artifacts during sensory data acquisition affect the i) prediction
accuracy and resilience of eHealth services and ii) energy efficiency in
processing garbage data. Monitoring raw sensory inputs to identify and drop
data and features from noisy modalities can improve prediction accuracy and
energy efficiency. We propose a closed-loop monitoring and control framework
for multi-modal eHealth applications, AMSER, that can mitigate garbage-in
garbage-out by i) monitoring input modalities, ii) analyzing raw input to
selectively drop noisy data and features, and iii) choosing appropriate machine
learning models that fit the configured data and feature vector - to improve
prediction accuracy and energy efficiency. We evaluate our AMSER approach using
multi-modal eHealth applications of pain assessment and stress monitoring over
different levels and types of noisy components incurred via different sensor
modalities. Our approach achieves up to 22\% improvement in prediction accuracy
and 5.6$\times$ energy consumption reduction in the sensing phase against the
state-of-the-art multi-modal monitoring application.

    

### [[2112.08181] Hierarchical Variational Memory for Few-shot Learning Across Domains](http://arxiv.org/abs/2112.08181)


  Neural memory enables fast adaptation to new tasks with just a few training
samples. Existing memory models store features only from the single last layer,
which does not generalize well in presence of a domain shift between training
and test distributions. Rather than relying on a flat memory, we propose a
hierarchical alternative that stores features at different semantic levels. We
introduce a hierarchical prototype model, where each level of the prototype
fetches corresponding information from the hierarchical memory. The model is
endowed with the ability to flexibly rely on features at different semantic
levels if the domain shift circumstances so demand. We meta-learn the model by
a newly derived hierarchical variational inference framework, where
hierarchical memory and prototypes are jointly optimized. To explore and
exploit the importance of different semantic levels, we further propose to
learn the weights associated with the prototype at each level in a data-driven
way, which enables the model to adaptively choose the most generalizable
features. We conduct thorough ablation studies to demonstrate the effectiveness
of each component in our model. The new state-of-the-art performance on
cross-domain and competitive performance on traditional few-shot classification
further substantiates the benefit of hierarchical variational memory.

    

### [[2112.08184] Interactive Visualization and Representation Analysis Applied to Glacier Segmentation](http://arxiv.org/abs/2112.08184)


  Interpretability has attracted increasing attention in earth observation
problems. We apply interactive visualization and representation analysis to
guide interpretation of glacier segmentation models. We visualize the
activations from a U-Net to understand and evaluate the model performance. We
build an online interface using the Shiny R package to provide comprehensive
error analysis of the predictions. Users can interact with the panels and
discover model failure modes. Further, we discuss how visualization can provide
sanity checks during data preprocessing and model training.

    

### [[2112.08195] Generative Adversarial Networks for Labelled Vibration Data Generation](http://arxiv.org/abs/2112.08195)


  As Structural Health Monitoring (SHM) being implemented more over the years,
the use of operational modal analysis of civil structures has become more
significant for the assessment and evaluation of engineering structures.
Machine Learning (ML) and Deep Learning (DL) algorithms have been in use for
structural damage diagnostics of civil structures in the last couple of
decades. While collecting vibration data from civil structures is a challenging
and expensive task for both undamaged and damaged cases, in this paper, the
authors are introducing Generative Adversarial Networks (GAN) that is built on
the Deep Convolutional Neural Network (DCNN) and using Wasserstein Distance for
generating artificial labelled data to be used for structural damage diagnostic
purposes. The authors named the developed model 1D W-DCGAN and successfully
generated vibration data which is very similar to the input. The methodology
presented in this paper will pave the way for vibration data generation for
numerous future applications in the SHM domain.

    

### [[2112.08196] Generative Adversarial Networks for Data Generation in Structural Health Monitoring](http://arxiv.org/abs/2112.08196)


  Structural Health Monitoring (SHM) has been continuously benefiting from the
advancements in the field of data science. Various types of Artificial
Intelligence (AI) methods have been utilized for the assessment and evaluation
of civil structures. In AI, Machine Learning (ML) and Deep Learning (DL)
algorithms require plenty of datasets to train; particularly, the more data DL
models are trained with, the better output it yields. Yet, in SHM applications,
collecting data from civil structures through sensors is expensive and
obtaining useful data (damage associated data) is challenging. In this paper,
1-D Wasserstein loss Deep Convolutional Generative Adversarial Networks using
Gradient Penalty (1-D WDCGAN-GP) is utilized to generate damage associated
vibration datasets that are similar to the input. For the purpose of
vibration-based damage diagnostics, a 1-D Deep Convolutional Neural Network
(1-D DCNN) is built, trained, and tested on both real and generated datasets.
The classification results from the 1-D DCNN on both datasets resulted to be
very similar to each other. The presented work in this paper shows that for the
cases of insufficient data in DL or ML-based damage diagnostics, 1-D WDCGAN-GP
can successfully generate data for the model to be trained on. Keywords: 1-D
Generative Adversarial Networks (GAN), Deep Convolutional Generative
Adversarial Networks (DCGAN), Wasserstein Generative Adversarial Networks with
Gradient Penalty (WGAN-GP), 1-D Convolutional Neural Networks (CNN), Structural
Health Monitoring (SHM), Structural Damage Diagnostics, Structural Damage
Detection

    

### [[2112.08200] Taming Overconfident Prediction on Unlabeled Data from Hindsight](http://arxiv.org/abs/2112.08200)


  Minimizing prediction uncertainty on unlabeled data is a key factor to
achieve good performance in semi-supervised learning (SSL). The prediction
uncertainty is typically expressed as the \emph{entropy} computed by the
transformed probabilities in output space. Most existing works distill
low-entropy prediction by either accepting the determining class (with the
largest probability) as the true label or suppressing subtle predictions (with
the smaller probabilities). Unarguably, these distillation strategies are
usually heuristic and less informative for model training. From this
discernment, this paper proposes a dual mechanism, named ADaptive Sharpening
(\ADS), which first applies a soft-threshold to adaptively mask out determinate
and negligible predictions, and then seamlessly sharpens the informed
predictions, distilling certain predictions with the informed ones only. More
importantly, we theoretically analyze the traits of \ADS by comparing with
various distillation strategies. Numerous experiments verify that \ADS
significantly improves the state-of-the-art SSL methods by making it a plug-in.
Our proposed \ADS forges a cornerstone for future distillation-based SSL
research.

    

### [[2112.08211] TrialGraph: Machine Intelligence Enabled Insight from Graph Modelling of Clinical Trials](http://arxiv.org/abs/2112.08211)


  A major impediment to successful drug development is the complexity, cost,
and scale of clinical trials. The detailed internal structure of clinical trial
data can make conventional optimization difficult to achieve. Recent advances
in machine learning, specifically graph-structured data analysis, have the
potential to enable significant progress in improving the clinical trial
design. TrialGraph seeks to apply these methodologies to produce a
proof-of-concept framework for developing models which can aid drug development
and benefit patients. In this work, we first introduce a curated clinical trial
data set compiled from the this http URL, AACT and TrialTrove databases (n=1191
trials; representing one million patients) and describe the conversion of this
data to graph-structured formats. We then detail the mathematical basis and
implementation of a selection of graph machine learning algorithms, which
typically use standard machine classifiers on graph data embedded in a
low-dimensional feature space. We trained these models to predict side effect
information for a clinical trial given information on the disease, existing
medical conditions, and treatment. The MetaPath2Vec algorithm performed
exceptionally well, with standard Logistic Regression, Decision Tree, Random
Forest, Support Vector, and Neural Network classifiers exhibiting typical
ROC-AUC scores of 0.85, 0.68, 0.86, 0.80, and 0.77, respectively. Remarkably,
the best performing classifiers could only produce typical ROC-AUC scores of
0.70 when trained on equivalent array-structured data. Our work demonstrates
that graph modelling can significantly improve prediction accuracy on
appropriate datasets. Successive versions of the project that refine modelling
assumptions and incorporate more data types can produce excellent predictors
with real-world applications in drug development.

    

### [[2112.08213] Enhancing operations management through smart sensors: measuring and improving well-being, interaction and performance of logistics workers](http://arxiv.org/abs/2112.08213)


  Purpose The purpose of the research is to conduct an exploratory
investigation of the material handling activities of an Italian logistics hub.
Wearable sensors and other smart tools were used for collecting human and
environmental features during working activities. These factors were correlated
with workers' performance and well-being.
Design/methodology/approach Human and environmental factors play an important
role in operations management activities since they significantly influence
employees' performance, well-being and safety. Surprisingly, empirical studies
about the impact of such aspects on logistics operations are still very
limited. Trying to fill this gap, the research empirically explores human and
environmental factors affecting the performance of logistics workers exploiting
smart tools.
Findings Results suggest that human attitudes, interactions, emotions and
environmental conditions remarkably influence workers' performance and
well-being, however, showing different relationships depending on individual
characteristics of each worker.
Practical implications The authors' research opens up new avenues for
profiling employees and adopting an individualized human resource management,
providing managers with an operational system capable to potentially check and
improve workers' well-being and performance.
Originality/value The originality of the study comes from the in-depth
exploration of human and environmental factors using body-worn sensors during
work activities, by recording individual, collaborative and environmental data
in real-time. To the best of the authors' knowledge, the current paper is the
first time that such a detailed analysis has been carried out in real-world
logistics operations.

    

### [[2112.08217] Probabilistic Forecasting with Conditional Generative Networks via Scoring Rule Minimization](http://arxiv.org/abs/2112.08217)


  Probabilistic forecasting consists of stating a probability distribution for
a future outcome based on past observations. In meteorology, ensembles of
physics-based numerical models are run to get such distribution. Usually,
performance is evaluated with scoring rules, functions of the forecast
distribution and the observed outcome. With some scoring rules, calibration and
sharpness of the forecast can be assessed at the same time.
In deep learning, generative neural networks parametrize distributions on
high-dimensional spaces and easily allow sampling by transforming draws from a
latent variable. Conditional generative networks additionally constrain the
distribution on an input variable. In this manuscript, we perform probabilistic
forecasting with conditional generative networks trained to minimize scoring
rule values. In contrast to Generative Adversarial Networks (GANs), no
discriminator is required and training is stable. We perform experiments on two
chaotic models and a global dataset of weather observations; results are
satisfactory and better calibrated than what achieved by GANs.

    

### [[2112.08222] Guaranteed Contraction Control in the Presence of Imperfectly Learned Dynamics](http://arxiv.org/abs/2112.08222)


  This paper presents an approach for trajectory-centric learning control based
on contraction metrics and disturbance estimation for nonlinear systems subject
to matched uncertainties. The approach allows for the use of a broad class of
model learning tools including deep neural networks to learn uncertain dynamics
while still providing guarantees of transient tracking performance throughout
the learning phase, including the special case of no learning. Within the
proposed approach, a disturbance estimation law is proposed to estimate the
pointwise value of the uncertainty, with pre-computable estimation error bounds
(EEBs). The learned dynamics, the estimated disturbances, and the EEBs are then
incorporated in a robust Riemannian energy condition to compute the control law
that guarantees exponential convergence of actual trajectories to desired ones
throughout the learning phase, even when the learned model is poor. On the
other hand, with improved accuracy, the learned model can be incorporated in a
high-level planner to plan better trajectories with improved performance, e.g.,
lower energy consumption and shorter travel time. The proposed framework is
validated on a planar quadrotor navigation example.

    

### [[2112.08224] Disparities in Social Determinants among Performances of Mortality Prediction with Machine Learning for Sepsis Patients](http://arxiv.org/abs/2112.08224)


  Background Sepsis is one of the most life-threatening circumstances for
critically ill patients in the US, while a standardized criteria for sepsis
identification is still under development. Disparities in social determinants
of sepsis patients can interfere with the risk prediction performances using
machine learning. Methods Disparities in social determinants, including race,
gender, marital status, insurance types and languages, among patients
identified by six available sepsis criteria were revealed by forest plots.
Sixteen machine learning classifiers were trained to predict in-hospital
mortality for sepsis patients. The performance of the trained model was tested
on the entire randomly conducted test set and each sub-population built based
on each of the following social determinants: race, gender, marital status,
insurance type, and language. Results We analyzed a total of 11,791 critical
care patients from the MIMIC-III database. Within the population identified by
each sepsis identification method, significant differences were observed among
sub-populations regarding race, marital status, insurance type, and language.
On the 5,783 sepsis patients identified by the Sepsis-3 criteria statistically
significant performance decreases for mortality prediction were observed when
applying the trained machine learning model on Asian and Hispanic patients.
With pairwise comparison, we detected performance discrepancies in mortality
prediction between Asian and White patients, Asians and patients of other
races, as well as English-speaking and Spanish-speaking patients. Conclusions
Disparities in proportions of patients identified by various sepsis criteria
were detected among the different social determinant groups. To achieve
accurate diagnosis, a versatile diagnostic system for sepsis is needed to
overcome the social determinant disparities of patients.

    

### [[2112.08232] RA V-Net: Deep learning network for automated liver segmentation](http://arxiv.org/abs/2112.08232)


  Accurate segmentation of the liver is a prerequisite for the diagnosis of
disease. Automated segmentation is an important application of computer-aided
detection and diagnosis of liver disease. In recent years, automated processing
of medical images has gained breakthroughs. However, the low contrast of
abdominal scan CT images and the complexity of liver morphology make accurate
automatic segmentation challenging. In this paper, we propose RA V-Net, which
is an improved medical image automatic segmentation model based on U-Net. It
has the following three main innovations. CofRes Module (Composite Original
Feature Residual Module) is proposed. With more complex convolution layers and
skip connections to make it obtain a higher level of image feature extraction
capability and prevent gradient disappearance or explosion. AR Module
(Attention Recovery Module) is proposed to reduce the computational effort of
the model. In addition, the spatial features between the data pixels of the
encoding and decoding modules are sensed by adjusting the channels and LSTM
convolution. Finally, the image features are effectively retained. CA Module
(Channel Attention Module) is introduced, which used to extract relevant
channels with dependencies and strengthen them by matrix dot product, while
weakening irrelevant channels without dependencies. The purpose of channel
attention is achieved. The attention mechanism provided by LSTM convolution and
CA Module are strong guarantees for the performance of the neural network. The
accuracy of U-Net network: 0.9862, precision: 0.9118, DSC: 0.8547, JSC: 0.82.
The evaluation metrics of RA V-Net, accuracy: 0.9968, precision: 0.9597, DSC:
0.9654, JSC: 0.9414. The most representative metric for the segmentation effect
is DSC, which improves 0.1107 over U-Net, and JSC improves 0.1214.

    

### [[2112.08250] Predicting the utility of search spaces for black-box optimization:a simple, budget-aware approach](http://arxiv.org/abs/2112.08250)


  Black box optimization requires specifying a search space to explore for
solutions, e.g. a d-dimensional compact space, and this choice is critical for
getting the best results at a reasonable budget. Unfortunately, determining a
high quality search space can be challenging in many applications. For example,
when tuning hyperparameters for machine learning pipelines on a new problem
given a limited budget, one must strike a balance between excluding potentially
promising regions and keeping the search space small enough to be tractable.
The goal of this work is to motivate -- through example applications in tuning
deep neural networks -- the problem of predicting the quality of search spaces
conditioned on budgets, as well as to provide a simple scoring method based on
a utility function applied to a probabilistic response surface model, similar
to Bayesian optimization. We show that the method we present can compute
meaningful budget-conditional scores in a variety of situations. We also
provide experimental evidence that accurate scores can be useful in
constructing and pruning search spaces. Ultimately, we believe scoring search
spaces should become standard practice in the experimental workflow for deep
learning.

    

### [[2112.08253] Online Feature Selection for Efficient Learning in Networked Systems](http://arxiv.org/abs/2112.08253)


  Current AI/ML methods for data-driven engineering use models that are mostly
trained offline. Such models can be expensive to build in terms of
communication and computing cost, and they rely on data that is collected over
extended periods of time. Further, they become out-of-date when changes in the
system occur. To address these challenges, we investigate online learning
techniques that automatically reduce the number of available data sources for
model training. We present an online algorithm called Online Stable Feature Set
Algorithm (OSFS), which selects a small feature set from a large number of
available data sources after receiving a small number of measurements. The
algorithm is initialized with a feature ranking algorithm, a feature set
stability metric, and a search policy. We perform an extensive experimental
evaluation of this algorithm using traces from an in-house testbed and from a
data center in operation. We find that OSFS achieves a massive reduction in the
size of the feature set by 1-3 orders of magnitude on all investigated
datasets. Most importantly, we find that the accuracy of a predictor trained on
a OSFS-produced feature set is somewhat better than when the predictor is
trained on a feature set obtained through offline feature selection. OSFS is
thus shown to be effective as an online feature selection algorithm and robust
regarding the sample interval used for feature selection. We also find that,
when concept drift in the data underlying the model occurs, its effect can be
mitigated by recomputing the feature set and retraining the prediction model.

    

### [[2112.08268] Prescriptive Machine Learning for Automated Decision Making: Challenges and Opportunities](http://arxiv.org/abs/2112.08268)


  Recent applications of machine learning (ML) reveal a noticeable shift from
its use for predictive modeling in the sense of a data-driven construction of
models mainly used for the purpose of prediction (of ground-truth facts) to its
use for prescriptive modeling. What is meant by this is the task of learning a
model that stipulates appropriate decisions about the right course of action in
real-world scenarios: Which medical therapy should be applied? Should this
person be hired for the job? As argued in this article, prescriptive modeling
comes with new technical conditions for learning and new demands regarding
reliability, responsibility, and the ethics of decision making. Therefore, to
support the data-driven design of decision-making agents that act in a rational
but at the same time responsible manner, a rigorous methodological foundation
of prescriptive ML is needed. The purpose of this short paper is to elaborate
on specific characteristics of prescriptive ML and to highlight some key
challenges it implies. Besides, drawing connections to other branches of
contemporary AI research, the grounding of prescriptive ML in a (generalized)
decision-theoretic framework is advocated.

    

### [[2112.08297] Rethinking Influence Functions of Neural Networks in the Over-parameterized Regime](http://arxiv.org/abs/2112.08297)


  Understanding the black-box prediction for neural networks is challenging. To
achieve this, early studies have designed influence function (IF) to measure
the effect of removing a single training point on neural networks. However, the
classic implicit Hessian-vector product (IHVP) method for calculating IF is
fragile, and theoretical analysis of IF in the context of neural networks is
still lacking. To this end, we utilize the neural tangent kernel (NTK) theory
to calculate IF for the neural network trained with regularized mean-square
loss, and prove that the approximation error can be arbitrarily small when the
width is sufficiently large for two-layer ReLU networks. We analyze the error
bound for the classic IHVP method in the over-parameterized regime to
understand when and why it fails or not. In detail, our theoretical analysis
reveals that (1) the accuracy of IHVP depends on the regularization term, and
is pretty low under weak regularization; (2) the accuracy of IHVP has a
significant correlation with the probability density of corresponding training
points. We further borrow the theory from NTK to understand the IFs better,
including quantifying the complexity for influential samples and depicting the
variation of IFs during the training dynamics. Numerical experiments on
real-world data confirm our theoretical results and demonstrate our findings.

    

### [[2112.08304] On the Convergence and Robustness of Adversarial Training](http://arxiv.org/abs/2112.08304)


  Improving the robustness of deep neural networks (DNNs) to adversarial
examples is an important yet challenging problem for secure deep learning.
Across existing defense techniques, adversarial training with Projected
Gradient Decent (PGD) is amongst the most effective. Adversarial training
solves a min-max optimization problem, with the \textit{inner maximization}
generating adversarial examples by maximizing the classification loss, and the
\textit{outer minimization} finding model parameters by minimizing the loss on
adversarial examples generated from the inner maximization. A criterion that
measures how well the inner maximization is solved is therefore crucial for
adversarial training. In this paper, we propose such a criterion, namely
First-Order Stationary Condition for constrained optimization (FOSC), to
quantitatively evaluate the convergence quality of adversarial examples found
in the inner maximization. With FOSC, we find that to ensure better robustness,
it is essential to use adversarial examples with better convergence quality at
the \textit{later stages} of training. Yet at the early stages, high
convergence quality adversarial examples are not necessary and may even lead to
poor robustness. Based on these observations, we propose a \textit{dynamic}
training strategy to gradually increase the convergence quality of the
generated adversarial examples, which significantly improves the robustness of
adversarial training. Our theoretical and empirical results show the
effectiveness of the proposed method.

    

### [[2112.08313] Measure and Improve Robustness in NLP Models: A Survey](http://arxiv.org/abs/2112.08313)


  As NLP models achieved state-of-the-art performances over benchmarks and
gained wide applications, it has been increasingly important to ensure the safe
deployment of these models in the real world, e.g., making sure the models are
robust against unseen or challenging scenarios. Despite robustness being an
increasingly studied topic, it has been separately explored in applications
like vision and NLP, with various definitions, evaluation and mitigation
strategies in multiple lines of research. In this paper, we aim to provide a
unifying survey of how to define, measure and improve robustness in NLP. We
first connect multiple definitions of robustness, then unify various lines of
work on identifying robustness failures and evaluating models' robustness.
Correspondingly, we present mitigation strategies that are data-driven,
model-driven, and inductive-prior-based, with a more systematic view of how to
effectively improve robustness in NLP models. Finally, we conclude by outlining
open challenges and future directions to motivate further research in this
area.

    

### [[2112.08331] Model Stealing Attacks Against Inductive Graph Neural Networks](http://arxiv.org/abs/2112.08331)


  Many real-world data come in the form of graphs. Graph neural networks
(GNNs), a new family of machine learning (ML) models, have been proposed to
fully leverage graph data to build powerful applications. In particular, the
inductive GNNs, which can generalize to unseen data, become mainstream in this
direction. Machine learning models have shown great potential in various tasks
and have been deployed in many real-world scenarios. To train a good model, a
large amount of data as well as computational resources are needed, leading to
valuable intellectual property. Previous research has shown that ML models are
prone to model stealing attacks, which aim to steal the functionality of the
target models. However, most of them focus on the models trained with images
and texts. On the other hand, little attention has been paid to models trained
with graph data, i.e., GNNs. In this paper, we fill the gap by proposing the
first model stealing attacks against inductive GNNs. We systematically define
the threat model and propose six attacks based on the adversary's background
knowledge and the responses of the target models. Our evaluation on six
benchmark datasets shows that the proposed model stealing attacks against GNNs
achieve promising performance.

    

### [[2112.08340] GenIE: Generative Information Extraction](http://arxiv.org/abs/2112.08340)


  Structured and grounded representation of text is typically formalized by
closed information extraction, the problem of extracting an exhaustive set of
(subject, relation, object) triplets that are consistent with a predefined set
of entities and relations from a knowledge base schema. Most existing works are
pipelines prone to error accumulation, and all approaches are only applicable
to unrealistically small numbers of entities and relations. We introduce GenIE
(generative information extraction), the first end-to-end autoregressive
formulation of closed information extraction. GenIE naturally exploits the
language knowledge from the pre-trained transformer by autoregressively
generating relations and entities in textual form. Thanks to a new bi-level
constrained generation strategy, only triplets consistent with the predefined
knowledge base schema are produced. Our experiments show that GenIE is
state-of-the-art on closed information extraction, generalizes from fewer
training data points than baselines, and scales to a previously unmanageable
number of entities and relations. With this work, closed information extraction
becomes practical in realistic scenarios, providing new opportunities for
downstream tasks. Finally, this work paves the way towards a unified end-to-end
approach to the core tasks of information extraction. Code and models available
at this https URL.

    

### [[2112.08346] Simple Text Detoxification by Identifying a Linear Toxic Subspace in Language Model Embeddings](http://arxiv.org/abs/2112.08346)


  Large pre-trained language models are often trained on large volumes of
internet data, some of which may contain toxic or abusive language.
Consequently, language models encode toxic information, which makes the
real-world usage of these language models limited. Current methods aim to
prevent toxic features from appearing generated text. We hypothesize the
existence of a low-dimensional toxic subspace in the latent space of
pre-trained language models, the existence of which suggests that toxic
features follow some underlying pattern and are thus removable. To construct
this toxic subspace, we propose a method to generalize toxic directions in the
latent space. We also provide a methodology for constructing parallel datasets
using a context based word masking system. Through our experiments, we show
that when the toxic subspace is removed from a set of sentence representations,
almost no toxic representations remain in the result. We demonstrate
empirically that the subspace found using our method generalizes to multiple
toxicity corpora, indicating the existence of a low-dimensional toxic subspace.

    

### [[2112.08352] Textless Speech-to-Speech Translation on Real Data](http://arxiv.org/abs/2112.08352)


  We present a textless speech-to-speech translation (S2ST) system that can
translate speech from one language into another language and can be built
without the need of any text data. Different from existing work in the
literature, we tackle the challenge in modeling multi-speaker target speech and
train the systems with real-world S2ST data. The key to our approach is a
self-supervised unit-based speech normalization technique, which finetunes a
pre-trained speech encoder with paired audios from multiple speakers and a
single reference speaker to reduce the variations due to accents, while
preserving the lexical content. With only 10 minutes of paired data for speech
normalization, we obtain on average 3.2 BLEU gain when training the S2ST model
on the \vp~S2ST dataset, compared to a baseline trained on un-normalized speech
target. We also incorporate automatically mined S2ST data and show an
additional 2.0 BLEU gain. To our knowledge, we are the first to establish a
textless S2ST technique that can be trained with real-world data and works for
multiple language pairs.

    

### [[2112.08355] Estimating Uncertainty For Vehicle Motion Prediction on Yandex Shifts Dataset](http://arxiv.org/abs/2112.08355)


  Motion prediction of surrounding agents is an important task in context of
autonomous driving since it is closely related to driver's safety. Vehicle
Motion Prediction (VMP) track of Shifts Challenge focuses on developing models
which are robust to distributional shift and able to measure uncertainty of
their predictions. In this work we present the approach that significantly
improved provided benchmark and took 2nd place on the leaderboard.

    

### [[1812.02353] Top-K Off-Policy Correction for a REINFORCE Recommender System](http://arxiv.org/abs/1812.02353)


  Industrial recommender systems deal with extremely large action spaces --
many millions of items to recommend. Moreover, they need to serve billions of
users, who are unique at any point in time, making a complex user state space.
Luckily, huge quantities of logged implicit feedback (e.g., user clicks, dwell
time) are available for learning. Learning from the logged feedback is however
subject to biases caused by only observing feedback on recommendations selected
by the previous versions of the recommender. In this work, we present a general
recipe of addressing such biases in a production top-K recommender system at
Youtube, built with a policy-gradient-based algorithm, i.e. REINFORCE. The
contributions of the paper are: (1) scaling REINFORCE to a production
recommender system with an action space on the orders of millions; (2) applying
off-policy correction to address data biases in learning from logged feedback
collected from multiple behavior policies; (3) proposing a novel top-K
off-policy correction to account for our policy recommending multiple items at
a time; (4) showcasing the value of exploration. We demonstrate the efficacy of
our approaches through a series of simulations and multiple live experiments on
Youtube.

    

### [[1904.11416] Bayesian Search for Robust Optima](http://arxiv.org/abs/1904.11416)


  Many expensive black-box optimisation problems are sensitive to their inputs.
In these problems it makes more sense to locate a region of good designs, than
a single-possibly fragile-optimal design. Expensive black-box functions can be
optimised effectively with Bayesian optimisation, where a Gaussian process is a
popular choice as a prior over the expensive function. We propose a method for
robust optimisation using Bayesian optimisation to find a region of design
space in which the expensive function's performance is relatively insensitive
to the inputs whilst retaining a good quality. This is achieved by sampling
realisations from a Gaussian process that is modelling the expensive function,
and evaluating the improvement for each realisation. The expectation of these
improvements can be optimised cheaply with an evolutionary algorithm to
determine the next location at which to evaluate the expensive function. We
describe an efficient process to locate the optimum expected improvement. We
show empirically that evaluating the expensive function at the location in the
candidate uncertainty region about which the model is most uncertain, or at
random, yield the best convergence in contrast to exploitative schemes. We
illustrate our method on six test functions in two, five, and ten dimensions,
and demonstrate that it is able to outperform two state-of-the-art approaches
from the literature. We also demonstrate our method one two real-world problems
in 4 and 8 dimensions, which involve training robot arms to push objects onto
targets.

    

### [[1909.02746] NEAR: Neighborhood Edge AggregatoR for Graph Classification](http://arxiv.org/abs/1909.02746)


  Learning graph-structured data with graph neural networks (GNNs) has been
recently emerging as an important field because of its wide applicability in
bioinformatics, chemoinformatics, social network analysis and data mining.
Recent GNN algorithms are based on neural message passing, which enables GNNs
to integrate local structures and node features recursively. However, past GNN
algorithms based on 1-hop neighborhood neural message passing are exposed to a
risk of loss of information on local structures and relationships. In this
paper, we propose Neighborhood Edge AggregatoR (NEAR), a framework that
aggregates relations between the nodes in the neighborhood via edges. NEAR,
which can be orthogonally combined with Graph Isomorphism Network (GIN), gives
integrated information that describes which nodes in the neighborhood are
connected. Therefore, NEAR can reflect additional information of a local
structure of each node beyond the nodes themselves in 1-hop neighborhood.
Experimental results on multiple graph classification tasks show that our
algorithm makes a good improvement over other existing 1-hop based GNN-based
algorithms.

    

### [[2002.04756] Average-case Acceleration Through Spectral Density Estimation](http://arxiv.org/abs/2002.04756)


  We develop a framework for the average-case analysis of random quadratic
problems and derive algorithms that are optimal under this analysis. This
yields a new class of methods that achieve acceleration given a model of the
Hessian's eigenvalue distribution. We develop explicit algorithms for the
uniform, Marchenko-Pastur, and exponential distributions. These methods are
momentum-based algorithms, whose hyper-parameters can be estimated without
knowledge of the Hessian's smallest singular value, in contrast with classical
accelerated methods like Nesterov acceleration and Polyak momentum. Through
empirical benchmarks on quadratic and logistic regression problems, we identify
regimes in which the the proposed methods improve over classical (worst-case)
accelerated methods.

    

### [[2003.08798] Incremental Object Detection via Meta-Learning](http://arxiv.org/abs/2003.08798)


  In a real-world setting, object instances from new classes can be
continuously encountered by object detectors. When existing object detectors
are applied to such scenarios, their performance on old classes deteriorates
significantly. A few efforts have been reported to address this limitation, all
of which apply variants of knowledge distillation to avoid catastrophic
forgetting. We note that although distillation helps to retain previous
learning, it obstructs fast adaptability to new tasks, which is a critical
requirement for incremental learning. In this pursuit, we propose a
meta-learning approach that learns to reshape model gradients, such that
information across incremental tasks is optimally shared. This ensures a
seamless information transfer via a meta-learned gradient preconditioning that
minimizes forgetting and maximizes knowledge transfer. In comparison to
existing meta-learning methods, our approach is task-agnostic, allows
incremental addition of new-classes and scales to high-capacity models for
object detection. We evaluate our approach on a variety of incremental learning
settings defined on PASCAL-VOC and MS COCO datasets, where our approach
performs favourably well against state-of-the-art methods.

    

### [[2004.01584] How Good are Low-Rank Approximations in Gaussian Process Regression?](http://arxiv.org/abs/2004.01584)


  We provide guarantees for approximate Gaussian Process (GP) regression
resulting from two common low-rank kernel approximations: based on random
Fourier features, and based on truncating the kernel's Mercer expansion. In
particular, we bound the Kullback-Leibler divergence between an exact GP and
one resulting from one of the afore-described low-rank approximations to its
kernel, as well as between their corresponding predictive densities, and we
also bound the error between predictive mean vectors and between predictive
covariance matrices computed using the exact versus using the approximate GP.
We provide experiments on both simulated data and standard benchmarks to
evaluate the effectiveness of our theoretical bounds.

    

### [[2005.01185] Multivariate Time Series Forecasting with Transfer Entropy Graph](http://arxiv.org/abs/2005.01185)


  Multivariate time series (MTS) forecasting is an essential problem in many
fields. Accurate forecasting results can effectively help decision-making. To
date, many MTS forecasting methods have been proposed and widely applied.
However, these methods assume that the predicted value of a single variable is
affected by all other variables, which ignores the causal relationship among
variables. To address the above issue, we propose a novel end-to-end deep
learning model, termed graph neural network with Neural Granger Causality
(CauGNN) in this paper. To characterize the causal information among variables,
we introduce the Neural Granger Causality graph in our model. Each variable is
regarded as a graph node, and each edge represents the casual relationship
between variables. In addition, convolutional neural network (CNN) filters with
different perception scales are used for time series feature extraction, which
is used to generate the feature of each node. Finally, Graph Neural Network
(GNN) is adopted to tackle the forecasting problem of graph structure generated
by MTS. Three benchmark datasets from the real world are used to evaluate the
proposed CauGNN. The comprehensive experiments show that the proposed method
achieves state-of-the-art results in the MTS forecasting task.

    

### [[2005.07609] An invertible crystallographic representation for general inverse design of inorganic crystals with targeted properties](http://arxiv.org/abs/2005.07609)


  Realizing general inverse design could greatly accelerate the discovery of
new materials with user-defined properties. However, state-of-the-art
generative models tend to be limited to a specific composition or crystal
structure. Herein, we present a framework capable of general inverse design
(not limited to a given set of elements or crystal structures), featuring a
generalized invertible representation that encodes crystals in both real and
reciprocal space, and a property-structured latent space from a variational
autoencoder (VAE). In three design cases, the framework generates 142 new
crystals with user-defined formation energies, bandgap, thermoelectric (TE)
power factor, and combinations thereof. These generated crystals, absent in the
training database, are validated by first-principles calculations. The success
rates (number of first-principles-validated target-satisfying crystals/number
of designed crystals) ranges between 7.1% and 38.9%. These results represent a
significant step toward property-driven general inverse design using generative
models, although practical challenges remain when coupled with experimental
synthesis.

    

### [[2005.11098] Deep Learning Based Detection and Localization of Intracranial Aneurysms in Computed Tomography Angiography](http://arxiv.org/abs/2005.11098)


  Purpose: To develop CADIA, a supervised deep learning model based on a region
proposal network coupled with a false-positive reduction module for the
detection and localization of intracranial aneurysms (IA) from computed
tomography angiography (CTA), and to assess our model's performance to a
similar detection network. Methods: In this retrospective study, we evaluated
1,216 patients from two separate institutions who underwent CT for the presence
of saccular IA>=2.5 mm. A two-step model was implemented: a 3D region proposal
network for initial aneurysm detection and 3D DenseNetsfor false-positive
reduction and further determination of suspicious IA. Free-response receiver
operative characteristics (FROC) curve and lesion-/patient-level performance at
established false positive per volume (FPPV) were also performed. Fisher's
exact test was used to compare with a similar available model. Results: CADIA's
sensitivities at 0.25 and 1 FPPV were 63.9% and 77.5%, respectively. Our
model's performance varied with size and location, and the best performance was
achieved in IA between 5-10 mm and in those at anterior communicating artery,
with sensitivities at 1 FPPV of 95.8% and 94%, respectively. Our model showed
statistically higher patient-level accuracy, sensitivity, and specificity when
compared to the available model at 0.25 FPPV and the best F-1 score (P<=0.001).
At 1 FPPV threshold, our model showed better accuracy and specificity
(P<=0.001) and equivalent sensitivity. Conclusions: CADIA outperformed a
comparable network in the detection task of IA. The addition of a
false-positive reduction module is a feasible step to improve the IA detection
models.

    

### [[2006.03814] The Impact of Global Structural Information in Graph Neural Networks Applications](http://arxiv.org/abs/2006.03814)


  Graph Neural Networks (GNNs) rely on the graph structure to define an
aggregation strategy where each node updates its representation by combining
information from its neighbours. A known limitation of GNNs is that, as the
number of layers increases, information gets smoothed and squashed and node
embeddings become indistinguishable, negatively affecting performance.
Therefore, practical GNN models employ few layers and only leverage the graph
structure in terms of limited, small neighbourhoods around each node.
Inevitably, practical GNNs do not capture information depending on the global
structure of the graph. While there have been several works studying the
limitations and expressivity of GNNs, the question of whether practical
applications on graph structured data require global structural knowledge or
not, remains unanswered. In this work, we empirically address this question by
giving access to global information to several GNN models, and observing the
impact it has on downstream performance. Our results show that global
information can in fact provide significant benefits for common graph-related
tasks. We further identify a novel regularization strategy that leads to an
average accuracy improvement of more than 5% on all considered tasks.

    

### [[2006.06581] Asymptotic Errors for Teacher-Student Convex Generalized Linear Models (or : How to Prove Kabashima's Replica Formula)](http://arxiv.org/abs/2006.06581)


  There has been a recent surge of interest in the study of asymptotic
reconstruction performance in various cases of generalized linear estimation
problems in the teacher-student setting, especially for the case of i.i.d
standard normal matrices. Here, we go beyond these matrices, and prove an
analytical formula for the reconstruction performance of convex generalized
linear models with rotationally-invariant data matrices with arbitrary bounded
spectrum, rigorously confirming a conjecture originally derived using the
replica method from statistical physics. The formula includes many problems
such as compressed sensing or sparse logistic classification. The proof is
achieved by leveraging on message passing algorithms and the statistical
properties of their iterates, allowing to characterize the asymptotic empirical
distribution of the estimator. Our proof is crucially based on the construction
of converging sequences of an oracle multi-layer vector approximate message
passing algorithm, where the convergence analysis is done by checking the
stability of an equivalent dynamical system. We illustrate our claim with
numerical examples on mainstream learning methods such as sparse logistic
regression and linear support vector classifiers, showing excellent agreement
between moderate size simulation and the asymptotic prediction.

    

### [[2008.08617] MTHetGNN: A Heterogeneous Graph Embedding Framework for Multivariate Time Series Forecasting](http://arxiv.org/abs/2008.08617)


  Multivariate time series forecasting, which analyzes historical time series
to predict future trends, can effectively help decision-making. Complex
relations among variables in MTS, including static, dynamic, predictable, and
latent relations, have made it possible to mining more features of MTS.
Modeling complex relations are not only essential in characterizing latent
dependency as well as modeling temporal dependence but also brings great
challenges in the MTS forecasting task. However, existing methods mainly focus
on modeling certain relations among MTS variables. In this paper, we propose a
novel end-to-end deep learning model, termed Multivariate Time Series
Forecasting via Heterogeneous Graph Neural Networks (MTHetGNN). To characterize
complex relations among variables, a relation embedding module is designed in
MTHetGNN, where each variable is regarded as a graph node, and each type of
edge represents a specific static or dynamic relationship. Meanwhile, a
temporal embedding module is introduced for time series features extraction,
where involving convolutional neural network (CNN) filters with different
perception scales. Finally, a heterogeneous graph embedding module is adopted
to handle the complex structural information generated by the two modules.
Three benchmark datasets from the real world are used to evaluate the proposed
MTHetGNN. The comprehensive experiments show that MTHetGNN achieves
state-of-the-art results in the MTS forecasting task.

    

### [[2009.00934] SAIL: Self-Augmented Graph Contrastive Learning](http://arxiv.org/abs/2009.00934)


  This paper studies learning node representations with graph neural networks
(GNNs) for unsupervised scenario. Specifically, we derive a theoretical
analysis and provide an empirical demonstration about the non-steady
performance of GNNs over different graph datasets, when the supervision signals
are not appropriately defined. The performance of GNNs depends on both the node
feature smoothness and the locality of graph structure. To smooth the
discrepancy of node proximity measured by graph topology and node feature, we
proposed SAIL - a novel \underline{S}elf-\underline{A}ugmented graph
contrast\underline{i}ve \underline{L}earning framework, with two complementary
self-distilling regularization modules, \emph{i.e.}, intra- and inter-graph
knowledge distillation. We demonstrate the competitive performance of SAIL on a
variety of graph applications. Even with a single GNN layer, SAIL has
consistently competitive or even better performance on various benchmark
datasets, comparing with state-of-the-art baselines.

    

### [[2010.01851] On the Universality of the Double Descent Peak in Ridgeless Regression](http://arxiv.org/abs/2010.01851)


  We prove a non-asymptotic distribution-independent lower bound for the
expected mean squared generalization error caused by label noise in ridgeless
linear regression. Our lower bound generalizes a similar known result to the
overparameterized (interpolating) regime. In contrast to most previous works,
our analysis applies to a broad class of input distributions with almost surely
full-rank feature matrices, which allows us to cover various types of
deterministic or random feature maps. Our lower bound is asymptotically sharp
and implies that in the presence of label noise, ridgeless linear regression
does not perform well around the interpolation threshold for any of these
feature maps. We analyze the imposed assumptions in detail and provide a theory
for analytic (random) feature maps. Using this theory, we can show that our
assumptions are satisfied for input distributions with a (Lebesgue) density and
feature maps given by random deep neural networks with analytic activation
functions like sigmoid, tanh, softplus or GELU. As further examples, we show
that feature maps from random Fourier features and polynomial kernels also
satisfy our assumptions. We complement our theory with further experimental and
analytic results.

    

### [[2010.04855] Generalized Kernel Ridge Regression for Nonparametric Structural Functions and Semiparametric Treatment Effects](http://arxiv.org/abs/2010.04855)


  We propose a family of estimators based on kernel ridge regression for
nonparametric structural functions (also called dose response curves) and
semiparametric treatment effects. Treatment and covariates may be discrete or
continuous, and low, high, or infinite dimensional. We reduce causal estimation
and inference to combinations of kernel ridge regressions, which have closed
form solutions and are easily computed by matrix operations, unlike other
machine learning paradigms. This computational simplicity allows us to extend
the framework in two directions: from means to increments and distributions of
counterfactual outcomes; and from parameters of the full population to those of
subpopulations and alternative populations. For structural functions, we prove
uniform consistency with finite sample rates. For treatment effects, we prove
$\sqrt{n}$ consistency, Gaussian approximation, and semiparametric efficiency
with a new double spectral robustness property. We conduct simulations and
estimate average, heterogeneous, and incremental structural functions of the US
Jobs Corps training program.

    

### [[2010.09536] What About Inputing Policy in Value Function: Policy Representation and Policy-extended Value Function Approximator](http://arxiv.org/abs/2010.09536)


  We study Policy-extended Value Function Approximator (PeVFA) in Reinforcement
Learning (RL), which extends conventional value function approximator (VFA) to
take as input not only the state (and action) but also an explicit policy
representation. Such an extension enables PeVFA to preserve values of multiple
policies at the same time and brings an appealing characteristic, i.e.,
\emph{value generalization among policies}. We formally analyze the value
generalization under Generalized Policy Iteration (GPI). From theoretical and
empirical lens, we show that generalized value estimates offered by PeVFA may
have lower initial approximation error to true values of successive policies,
which is expected to improve consecutive value approximation during GPI. Based
on above clues, we introduce a new form of GPI with PeVFA which leverages the
value generalization along policy improvement path. Moreover, we propose a
representation learning framework for RL policy, providing several approaches
to learn effective policy embeddings from policy network parameters or
state-action pairs. In our experiments, we evaluate the efficacy of value
generalization offered by PeVFA and policy representation learning in several
OpenAI Gym continuous control tasks. For a representative instance of algorithm
implementation, Proximal Policy Optimization (PPO) re-implemented under the
paradigm of GPI with PeVFA achieves about 40\% performance improvement on its
vanilla counterpart in most environments.

    

### [[2010.15571] Learning Sub-Patterns in Piecewise Continuous Functions](http://arxiv.org/abs/2010.15571)


  Most stochastic gradient descent algorithms can optimize neural networks that
are sub-differentiable in their parameters; however, this implies that the
neural network's activation function must exhibit a degree of continuity which
limits the neural network model's uniform approximation capacity to continuous
functions. This paper focuses on the case where the discontinuities arise from
distinct sub-patterns, each defined on different parts of the input space. We
propose a new discontinuous deep neural network model trainable via a decoupled
two-step procedure that avoids passing gradient updates through the network's
only and strategically placed, discontinuous unit. We provide approximation
guarantees for our architecture in the space of bounded continuous functions
and universal approximation guarantees in the space of piecewise continuous
functions which we introduced herein. We present a novel semi-supervised
two-step training procedure for our discontinuous deep learning model, tailored
to its structure, and we provide theoretical support for its effectiveness. The
performance of our model and trained with the propose procedure is evaluated
experimentally on both real-world financial datasets and synthetic datasets.

    

### [[2011.03885] Performative Prediction in a Stateful World](http://arxiv.org/abs/2011.03885)


  Deployed supervised machine learning models make predictions that interact
with and influence the world. This phenomenon is called performative prediction
by Perdomo et al. (ICML 2020). It is an ongoing challenge to understand the
influence of such predictions as well as design tools so as to control that
influence. We propose a theoretical framework where the response of a target
population to the deployed classifier is modeled as a function of the
classifier and the current state (distribution) of the population. We show
necessary and sufficient conditions for convergence to an equilibrium point of
two retraining algorithms, repeated risk minimization and a lazier variant.
Furthermore, convergence is near an optimal classifier. We thus generalize
results of Perdomo et al., whose performativity framework does not assume any
dependence on the state of the target population. A particular phenomenon
captured by our model is that of distinct groups that acquire information and
resources at different rates to be able to respond to the latest deployed
classifier. We study this phenomenon theoretically and empirically.

    

### [[2012.14843] Learning Adversarial Markov Decision Processes with Delayed Feedback](http://arxiv.org/abs/2012.14843)


  Reinforcement learning typically assumes that agents observe feedback for
their actions immediately, but in many real-world applications (like
recommendation systems) feedback is observed in delay. This paper studies
online learning in episodic Markov decision processes (MDPs) with unknown
transitions, adversarially changing costs and unrestricted delayed feedback.
That is, the costs and trajectory of episode $k$ are revealed to the learner
only in the end of episode $k + d^k$, where the delays $d^k$ are neither
identical nor bounded, and are chosen by an oblivious adversary. We present
novel algorithms based on policy optimization that achieve near-optimal
high-probability regret of $\sqrt{K + D}$ under full-information feedback,
where $K$ is the number of episodes and $D = \sum_{k} d^k$ is the total delay.
Under bandit feedback, we prove similar $\sqrt{K + D}$ regret assuming the
costs are stochastic, and $(K + D)^{2/3}$ regret in the general case. We are
the first to consider regret minimization in the important setting of MDPs with
delayed feedback.

    

### [[2101.07243] Gauge Invariant Autoregressive Neural Networks for Quantum Lattice Models](http://arxiv.org/abs/2101.07243)


  Gauge invariance plays a crucial role in quantum mechanics from condensed
matter physics to high energy physics. We develop an approach to constructing
gauge invariant autoregressive neural networks for quantum lattice models.
These networks can be efficiently sampled and explicitly obey gauge symmetries.
We variationally optimize our gauge invariant autoregressive neural networks
for ground states as well as real-time dynamics for a variety of models. We
exactly represent the ground and excited states of the 2D and 3D toric codes,
and the X-cube fracton model. We simulate the dynamics and the gound states of
the quantum link model of $\text{U(1)}$ lattice gauge theory, obtain the phase
diagram for the 2D $\mathbb{Z}_2$ gauge theory, determine the phase transition
and the central charge of the $\text{SU(2)}_3$ anyonic chain, and also compute
the ground state energy of the SU(2) invariant Heisenberg spin chain. Our
approach provides powerful tools for exploring condensed matter physics, high
energy physics and quantum information science.

    

### [[2102.10809] Local Calibration: Metrics and Recalibration](http://arxiv.org/abs/2102.10809)


  Probabilistic classifiers output confidence scores along with their
predictions, and these confidence scores should be calibrated, i.e., they
should reflect the reliability of the prediction. Confidence scores that
minimize standard metrics such as the expected calibration error (ECE)
accurately measure the reliability on average across the entire population.
However, it is in general impossible to measure the reliability of an
individual prediction. In this work, we propose the local calibration error
(LCE) to span the gap between average and individual reliability. For each
individual prediction, the LCE measures the average reliability of a set of
similar predictions, where similarity is quantified by a kernel function on a
pretrained feature space and by a binning scheme over predicted model
confidences. We show theoretically that the LCE can be estimated
sample-efficiently from data, and empirically find that it reveals
miscalibration modes that are more fine-grained than the ECE can detect. Our
key result is a novel local recalibration method LoRe, to improve confidence
scores for individual predictions and decrease the LCE. Experimentally, we show
that our recalibration method produces more accurate confidence scores, which
improves downstream fairness and decision making on classification tasks with
both image and tabular data.

    

### [[2102.11717] Greedy-Step Off-Policy Reinforcement Learning](http://arxiv.org/abs/2102.11717)


  Most of the policy evaluation algorithms are based on the theories of Bellman
Expectation and Optimality Equation, which derive two popular approaches -
Policy Iteration (PI) and Value Iteration (VI). However, multi-step
bootstrapping is often at cross-purposes with and off-policy learning in
PI-based methods due to the large variance of multi-step off-policy correction.
In contrast, VI-based methods are naturally off-policy but subject to one-step
this http URL this paper, we deduce a novel multi-step Bellman Optimality
Equation by utilizing a latent structure of multi-step bootstrapping with the
optimal value function. Via this new equation, we derive a new multi-step value
iteration method that converges to the optimal value function with exponential
contraction rate $\mathcal{O}(\gamma^n)$ but only linear computational
complexity. Moreover, it can naturally derive a suite of multi-step off-policy
algorithms that can safely utilize data collected by arbitrary policies without
correction.Experiments reveal that the proposed methods are reliable, easy to
implement and achieve state-of-the-art performance on a series of standard
benchmark datasets.

    

### [[2103.03547] Structure-Enhanced Meta-Learning For Few-Shot Graph Classification](http://arxiv.org/abs/2103.03547)


  Graph classification is a highly impactful task that plays a crucial role in
a myriad of real-world applications such as molecular property prediction and
protein function prediction.Aiming to handle the new classes with limited
labeled graphs, few-shot graph classification has become a bridge of existing
graph classification solutions and practical usage.This work explores the
potential of metric-based meta-learning for solving few-shot graph
classification.We highlight the importance of considering structural
characteristics in the solution and propose a novel framework which explicitly
considers global structure and local structure of the input graph. An
implementation upon GIN, named SMF-GIN, is tested on two datasets, Chembl and
TRIANGLES, where extensive experiments validate the effectiveness of the
proposed method. The Chembl is constructed to fill in the gap of lacking
large-scale benchmark for few-shot graph classification evaluation, which is
released together with the implementation of SMF-GIN at:
this https URL.

    

### [[2103.07292] VDSM: Unsupervised Video Disentanglement with State-Space Modeling and Deep Mixtures of Experts](http://arxiv.org/abs/2103.07292)


  Disentangled representations support a range of downstream tasks including
causal reasoning, generative modeling, and fair machine learning.
Unfortunately, disentanglement has been shown to be impossible without the
incorporation of supervision or inductive bias. Given that supervision is often
expensive or infeasible to acquire, we choose to incorporate structural
inductive bias and present an unsupervised, deep State-Space-Model for Video
Disentanglement (VDSM). The model disentangles latent time-varying and dynamic
factors via the incorporation of hierarchical structure with a dynamic prior
and a Mixture of Experts decoder. VDSM learns separate disentangled
representations for the identity of the object or person in the video, and for
the action being performed. We evaluate VDSM across a range of qualitative and
quantitative tasks including identity and dynamics transfer, sequence
generation, Fréchet Inception Distance, and factor classification. VDSM
provides state-of-the-art performance and exceeds adversarial methods, even
when the methods use additional supervision.

    

### [[2103.11181] Adaptive deep density approximation for Fokker-Planck equations](http://arxiv.org/abs/2103.11181)


  In this paper we present an adaptive deep density approximation strategy
based on KRnet (ADDA-KR) for solving the steady-state Fokker-Planck (F-P)
equations. F-P equations are usually high-dimensional and defined on an
unbounded domain, which limits the application of traditional grid based
numerical methods. With the Knothe-Rosenblatt rearrangement, our newly proposed
flow-based generative model, called KRnet, provides a family of probability
density functions to serve as effective solution candidates for the
Fokker-Planck equations, which has a weaker dependence on dimensionality than
traditional computational approaches and can efficiently estimate general
high-dimensional density functions. To obtain effective stochastic collocation
points for the approximation of the F-P equation, we develop an adaptive
sampling procedure, where samples are generated iteratively using the
approximate density function at each iteration. We present a general framework
of ADDA-KR, validate its accuracy and demonstrate its efficiency with numerical
experiments.

    

### [[2104.11522] Conditional super-network weights](http://arxiv.org/abs/2104.11522)


  Modern Neural Architecture Search methods have repeatedly broken
state-of-the-art results for several disciplines. The super-network, a central
component of many such methods, enables quick estimates of accuracy or loss
statistics for any architecture in the search space. They incorporate the
network weights of all candidate architectures and can thus approximate
specific ones by applying the respective operations. However, this design
ignores potential dependencies between consecutive operations. We extend
super-networks with conditional weights that depend on combinations of choices
and analyze their effect. Experiments in NAS-Bench 201 and
NAS-Bench-Macro-based search spaces show improvements in the architecture
selection and that the resource overhead is nearly negligible for sequential
network designs.

    

### [[2104.11832] Playing Lottery Tickets with Vision and Language](http://arxiv.org/abs/2104.11832)


  Large-scale pre-training has recently revolutionized vision-and-language (VL)
research. Models such as LXMERT and UNITER have significantly lifted the state
of the art over a wide range of VL tasks. However, the large number of
parameters in such models hinders their application in practice. In parallel,
work on the lottery ticket hypothesis (LTH) has shown that deep neural networks
contain small matching subnetworks that can achieve on par or even better
performance than the dense networks when trained in isolation. In this work, we
perform the first empirical study to assess whether such trainable subnetworks
also exist in pre-trained VL models. We use UNITER as the main testbed (also
test on LXMERT and ViLT), and consolidate 7 representative VL tasks for
experiments, including visual question answering, visual commonsense reasoning,
visual entailment, referring expression comprehension, image-text retrieval,
GQA, and NLVR$^2$. Through comprehensive analysis, we summarize our main
findings as follows. ($i$) It is difficult to find subnetworks that strictly
match the performance of the full model. However, we can find "relaxed" winning
tickets at 50%-70% sparsity that maintain 99% of the full accuracy. ($ii$)
Subnetworks found by task-specific pruning transfer reasonably well to the
other tasks, while those found on the pre-training tasks at 60%/70% sparsity
transfer universally, matching 98%/96% of the full accuracy on average over all
the tasks. ($iii$) Besides UNITER, other models such as LXMERT and ViLT can
also play lottery tickets. However, the highest sparsity we can achieve for
ViLT is far lower than LXMERT and UNITER (30% vs. 70%). ($iv$) LTH also remains
relevant when using other training methods (e.g., adversarial training).

    

### [[2104.14403] Do Feature Attribution Methods Correctly Attribute Features?](http://arxiv.org/abs/2104.14403)


  Feature attribution methods are popular in interpretable machine learning.
These methods compute the attribution of each input feature to represent its
importance, but there is no consensus on the definition of "attribution",
leading to many competing methods with little systematic evaluation,
complicated in particular by the lack of ground truth attribution. To address
this, we propose a dataset modification procedure to induce such ground truth.
Using this procedure, we evaluate three common methods: saliency maps,
rationales, and attentions. We identify several deficiencies and add new
perspectives to the growing body of evidence questioning the correctness and
reliability of these methods applied on datasets in the wild. We further
discuss possible avenues for remedy and recommend new attribution methods to be
tested against ground truth before deployment. The code is available at
\url{this https URL}.

    

### [[2105.02556] Metric Entropy Limits on Recurrent Neural Network Learning of Linear Dynamical Systems](http://arxiv.org/abs/2105.02556)


  One of the most influential results in neural network theory is the universal
approximation theorem [1, 2, 3] which states that continuous functions can be
approximated to within arbitrary accuracy by single-hidden-layer feedforward
neural networks. The purpose of this paper is to establish a result in this
spirit for the approximation of general discrete-time linear dynamical systems
- including time-varying systems - by recurrent neural networks (RNNs). For the
subclass of linear time-invariant (LTI) systems, we devise a quantitative
version of this statement. Specifically, measuring the complexity of the
considered class of LTI systems through metric entropy according to [4], we
show that RNNs can optimally learn - or identify in system-theory parlance -
stable LTI systems. For LTI systems whose input-output relation is
characterized through a difference equation, this means that RNNs can learn the
difference equation from input-output traces in a metric-entropy optimal
manner.

    

### [[2105.08007] Understanding and Improvement of Adversarial Training for Network Embedding from an Optimization Perspective](http://arxiv.org/abs/2105.08007)


  Network Embedding aims to learn a function mapping the nodes to Euclidean
space contribute to multiple learning analysis tasks on networks. However, the
noisy information behind the real-world networks and the overfitting problem
both negatively impact the quality of embedding vectors. To tackle these
problems, researchers utilize Adversarial Training for Network Embedding
(AdvTNE) and achieve state-of-the-art performance. Unlike the mainstream
methods introducing perturbations on the network structure or the data feature,
AdvTNE directly perturbs the model parameters, which provides a new chance to
understand the mechanism behind. In this paper, we explain AdvTNE theoretically
from an optimization perspective. Considering the Power-law property of
networks and the optimization objective, we analyze the reason for its
excellent results. According to the above analysis, we propose a new activation
to enhance the performance of AdvTNE. We conduct extensive experiments on four
real networks to validate the effectiveness of our method in node
classification and link prediction. The results demonstrate that our method is
superior to the state-of-the-art baseline methods.

    

### [[2105.10699] Denoising Noisy Neural Networks: A Bayesian Approach with Compensation](http://arxiv.org/abs/2105.10699)


  Deep neural networks (DNNs) with noisy weights, which we refer to as noisy
neural networks (NoisyNNs), arise from the training and inference of DNNs in
the presence of noise. NoisyNNs emerge in many new applications, including the
wireless transmission of DNNs, the efficient deployment or storage of DNNs in
analog devices, and the truncation or quantization of DNN weights. This paper
studies a fundamental problem of NoisyNNs: how to reconstruct the DNN weights
from their noisy manifestations. While all prior works relied on the maximum
likelihood (ML) estimation, this paper puts forth a denoising approach to
reconstruct DNNs with the aim of maximizing the inference accuracy of the
reconstructed models. The superiority of our denoiser is rigorously proven in
two small-scale problems, wherein we consider a quadratic neural network
function and a shallow feedforward neural network, respectively. When applied
to advanced learning tasks with modern DNN architectures, our denoiser exhibits
significantly better performance than the ML estimator. Consider the average
test accuracy of the denoised DNN model versus the weight variance to noise
power ratio (WNR) performance. When denoising a noisy BERT model arising from
noisy inference, our denoiser outperforms ML estimation by 1.1 dB to achieve a
test accuracy of 75%. When denoising a noisy ResNet18 model arising from noisy
training, our denoiser outperforms ML estimation by 13.4 dB and 8.3 dB to
achieve a test accuracy of 60% and 80%, respectively.

    

### [[2105.15004] Generalization Error Rates in Kernel Regression: The Crossover from the Noiseless to Noisy Regime](http://arxiv.org/abs/2105.15004)


  In this manuscript we consider Kernel Ridge Regression (KRR) under the
Gaussian design. Exponents for the decay of the excess generalization error of
KRR have been reported in various works under the assumption of power-law decay
of eigenvalues of the features co-variance. These decays were, however,
provided for sizeably different setups, namely in the noiseless case with
constant regularization and in the noisy optimally regularized case.
Intermediary settings have been left substantially uncharted. In this work, we
unify and extend this line of work, providing characterization of all regimes
and excess error decay rates that can be observed in terms of the interplay of
noise and regularization. In particular, we show the existence of a transition
in the noisy setting between the noiseless exponents to its noisy values as the
sample complexity is increased. Finally, we illustrate how this crossover can
also be observed on real data sets.

    

### [[2106.01613] NODE-GAM: Neural Generalized Additive Model for Interpretable Deep Learning](http://arxiv.org/abs/2106.01613)


  Deployment of machine learning models in real high-risk settings (e.g.
healthcare) often depends not only on model's accuracy but also on its
fairness, robustness and interpretability. Generalized Additive Models (GAMs)
are a class of interpretable models with a long history of use in these
high-risk domains, but they lack desirable features of deep learning such as
differentiability and scalability. In this work, we propose a neural GAM
(NODE-GAM) and neural GA$^2$M (NODE-GA$^2$M) that scale well and perform better
than other GAMs on large datasets, while remaining interpretable compared to
other ensemble and deep learning models. We demonstrate that our models find
interesting patterns in the data. Lastly, we show that we improve model
accuracy via self-supervised pre-training, an improvement that is not possible
for non-differentiable GAMs.

    

### [[2106.03589] Nonparametric adaptive control and prediction: theory and randomized algorithms](http://arxiv.org/abs/2106.03589)


  A key assumption in the theory of nonlinear adaptive control is that the
uncertainty of the system can be expressed in the linear span of a set of known
basis functions. While this assumption leads to efficient algorithms, it limits
applications to very specific classes of systems. We introduce a novel
nonparametric adaptive algorithm that learns an infinite-dimensional density
over parameters to cancel an unknown disturbance in a reproducing kernel
Hilbert space. Surprisingly, the resulting control input admits an analytical
expression that enables its implementation despite its underlying
infinite-dimensional structure. While this adaptive input is rich and
expressive -- subsuming, for example, traditional linear parameterizations --
its computational complexity grows linearly with time, making it comparatively
more expensive than its parametric counterparts. Leveraging the theory of
random Fourier features, we provide an efficient randomized implementation that
recovers the complexity of classical parametric methods while provably
retaining the expressivity of the nonparametric input. In particular, our
explicit bounds only depend polynomially on the underlying parameters of the
system, allowing our proposed algorithms to efficiently scale to
high-dimensional systems. As an illustration of the method, we demonstrate the
ability of the randomized approximation algorithm to learn a predictive model
of a 60-dimensional system consisting of ten point masses interacting through
Newtonian gravitation.

    

### [[2106.08085] Natural continual learning: success is a journey, not (just) a destination](http://arxiv.org/abs/2106.08085)


  Biological agents are known to learn many different tasks over the course of
their lives, and to be able to revisit previous tasks and behaviors with little
to no loss in performance. In contrast, artificial agents are prone to
'catastrophic forgetting' whereby performance on previous tasks deteriorates
rapidly as new ones are acquired. This shortcoming has recently been addressed
using methods that encourage parameters to stay close to those used for
previous tasks. This can be done by (i) using specific parameter regularizers
that map out suitable destinations in parameter space, or (ii) guiding the
optimization journey by projecting gradients into subspaces that do not
interfere with previous tasks. However, these methods often exhibit subpar
performance in both feedforward and recurrent neural networks, with recurrent
networks being of interest to the study of neural dynamics supporting
biological continual learning. In this work, we propose Natural Continual
Learning (NCL), a new method that unifies weight regularization and projected
gradient descent. NCL uses Bayesian weight regularization to encourage good
performance on all tasks at convergence and combines this with gradient
projection using the prior precision, which prevents catastrophic forgetting
during optimization. Our method outperforms both standard weight regularization
techniques and projection based approaches when applied to continual learning
problems in feedforward and recurrent networks. Finally, the trained networks
evolve task-specific dynamics that are strongly preserved as new tasks are
learned, similar to experimental findings in biological circuits.

    

### [[2106.12379] AC/DC: Alternating Compressed/DeCompressed Training of Deep Neural Networks](http://arxiv.org/abs/2106.12379)


  The increasing computational requirements of deep neural networks (DNNs) have
led to significant interest in obtaining DNN models that are sparse, yet
accurate. Recent work has investigated the even harder case of sparse training,
where the DNN weights are, for as much as possible, already sparse to reduce
computational costs during training. Existing sparse training methods are often
empirical and can have lower accuracy relative to the dense baseline. In this
paper, we present a general approach called Alternating Compressed/DeCompressed
(AC/DC) training of DNNs, demonstrate convergence for a variant of the
algorithm, and show that AC/DC outperforms existing sparse training methods in
accuracy at similar computational budgets; at high sparsity levels, AC/DC even
outperforms existing methods that rely on accurate pre-trained dense models. An
important property of AC/DC is that it allows co-training of dense and sparse
models, yielding accurate sparse-dense model pairs at the end of the training
process. This is useful in practice, where compressed variants may be desirable
for deployment in resource-constrained settings without re-doing the entire
training flow, and also provides us with insights into the accuracy gap between
dense and compressed models. The code is available at:
this https URL .

    

### [[2112.03254] Human Parity on CommonsenseQA: Augmenting Self-Attention with External Attention](http://arxiv.org/abs/2112.03254)


  Most of today's AI systems focus on using self-attention mechanisms and
transformer architectures on large amounts of diverse data to achieve
impressive performance gains. In this paper, we propose to augment the
transformer architecture with an external attention mechanism to bring external
knowledge and context to bear. By integrating external information into the
prediction process, we hope to reduce the need for ever-larger models and
increase the democratization of AI systems. We find that the proposed external
attention mechanism can significantly improve the performance of existing AI
systems, allowing practitioners to easily customize foundation AI models to
many diverse downstream applications. In particular, we focus on the task of
Commonsense Reasoning, demonstrating that the proposed external attention
mechanism can augment existing transformer models and significantly improve the
model's reasoning capabilities. The proposed system, Knowledgeable External
Attention for commonsense Reasoning (KEAR), reaches human parity on the open
CommonsenseQA research benchmark with an accuracy of 89.4\% in comparison to
the human accuracy of 88.9\%.

    

### [[2112.07789] FLOWER: A comprehensive dataflow compiler for high-level synthesis](http://arxiv.org/abs/2112.07789)


  FPGAs have found their way into data centers as accelerator cards, making
reconfigurable computing more accessible for high-performance applications. At
the same time, new high-level synthesis compilers like Xilinx Vitis and runtime
libraries such as XRT attract software programmers into the reconfigurable
domain. While software programmers are familiar with task-level and
data-parallel programming, FPGAs often require different types of parallelism.
For example, data-driven parallelism is mandatory to obtain satisfactory
hardware designs for pipelined dataflow architectures. However, software
programmers are often not acquainted with dataflow architectures - resulting in
poor hardware designs.
In this work we present FLOWER, a comprehensive compiler infrastructure that
provides automatic canonical transformations for high-level synthesis from a
domain-specific library. This allows programmers to focus on algorithm
implementations rather than low-level optimizations for dataflow architectures.
We show that FLOWER allows to synthesize efficient implementations for
high-performance streaming applications targeting System-on-Chip and FPGA
accelerator cards, in the context of image processing and computer vision.

    

### [[2112.08193] N3H-Core: Neuron-designed Neural Network Accelerator via FPGA-based Heterogeneous Computing Cores](http://arxiv.org/abs/2112.08193)


  Accelerating the neural network inference by FPGA has emerged as a popular
option, since the reconfigurability and high performance computing capability
of FPGA intrinsically satisfies the computation demand of the fast-evolving
neural algorithms. However, the popular neural accelerators on FPGA (e.g.,
Xilinx DPU) mainly utilize the DSP resources for constructing their processing
units, while the rich LUT resources are not well exploited. Via the
software-hardware co-design approach, in this work, we develop an FPGA-based
heterogeneous computing system for neural network acceleration. From the
hardware perspective, the proposed accelerator consists of DSP- and LUT-based
GEneral Matrix-Multiplication (GEMM) computing cores, which forms the entire
computing system in a heterogeneous fashion. The DSP- and LUT-based GEMM cores
are computed w.r.t a unified Instruction Set Architecture (ISA) and unified
buffers. Along the data flow of the neural network inference path, the
computation of the convolution/fully-connected layer is split into two
portions, handled by the DSP- and LUT-based GEMM cores asynchronously. From the
software perspective, we mathematically and systematically model the latency
and resource utilization of the proposed heterogeneous accelerator, regarding
varying system design configurations. Through leveraging the reinforcement
learning technique, we construct a framework to achieve end-to-end selection
and optimization of the design specification of target heterogeneous
accelerator, including workload split strategy, mixed-precision quantization
scheme, and resource allocation of DSP- and LUT-core. In virtue of the proposed
design framework and heterogeneous computing system, our design outperforms the
state-of-the-art Mix&Match design with latency reduced by 1.12-1.32x with
higher inference accuracy. The N3H-core is open-sourced at:
this https URL.

    

### [[2112.07980] Data Placement for Multi-Tenant Data Federation on the Cloud](http://arxiv.org/abs/2112.07980)


  Due to privacy concerns of users and law enforcement in data security and
privacy, it becomes more and more difficult to share data among organizations.
Data federation brings new opportunities to the data-related cooperation among
organizations by providing abstract data interfaces. With the development of
cloud computing, organizations store data on the cloud to achieve elasticity
and scalability for data processing. The existing data placement approaches
generally only consider one aspect, which is either execution time or monetary
cost, and do not consider data partitioning for hard constraints. In this
paper, we propose an approach to enable data processing on the cloud with the
data from different organizations. The approach consists of a data federation
platform named FedCube and a Lyapunov-based data placement algorithm. FedCube
enables data processing on the cloud. We use the data placement algorithm to
create a plan in order to partition and store data on the cloud so as to
achieve multiple objectives while satisfying the constraints based on a
multi-objective cost model. The cost model is composed of two objectives, i.e.,
reducing monetary cost and execution time. We present an experimental
evaluation to show our proposed algorithm significantly reduces the total cost
(up to 69.8\%) compared with existing approaches.

    

### [[2112.08012] Service Oriented Architecture in Enterprise Application](http://arxiv.org/abs/2112.08012)


  At present organizations try to achieve competitive advantages using
information technology (IT). Organizations not only use Information technology
to manage their internal operations but also to collaborate with their
customers and suppliers. For these organizations use enterprise applications.
Also, organizations expect IT to address their shifting needs in on demand
environment. So currently IT face challenges in integrating the various system
into a function that can address the organization's on-demand needs and span
over the organizational boundaries. Now Service-Oriented Architecture (SOA)
imagine as an architectural framework that addresses the issues on previous
enterprise applications. This paper represents the Service-Oriented
Architecture in Enterprise Applications. The enterprise application gives a
brief idea about enterprise applications. Afterward, this paper addresses the
main problems faced by enterprise applications on the evolution of enterprise
applications and the needs of service-oriented architecture in enterprise
applications. Afterward, discuss Service Oriented Architecture and challenges
on Service Oriented Architecture.

    

### [[2112.08282] Distributed Applications in Gamification of the Learning Process](http://arxiv.org/abs/2112.08282)


  Driven by the fact that many of us experienced softer or not-so-soft
lockdown, the intention of a couple of instructors at our university was to
develop a collaborative tool that could help in online delivery and
gamification on two courses that are delivered in the Business and IT
curriculums we are offering to our students. That tool could be described as a
decentralized web application that simulates Internet marketing principles and
helps in gamification of the learning process for our students. We planned our
web application for Internet marketing simulation as the gamification of the
learning process, which is one of the basics for active learning for Internet
Marketing course for International Business students, to gain new class
activities by online simulation competing in the field of Internet marketing
principles; and for IT students in developing the Web application and also on
adopting Blockchain technologies for the distributed reports which need to have
a consensus of all teams included in the simulation. The proposed solution
includes the design of business logic simulation and using four main digital
marketing tools social networking, content creating and sharing, search engine
marketing, and display advertising in use of such application for hands-on
online class exercises.

    

### [[2112.08338] Application of Blockchain Technology for Educational Platform](http://arxiv.org/abs/2112.08338)


  Nowadays, huge amounts of data are generated every second, and a quantity of
that data can be defined as sensitive. Blockchain technology has private,
secure, transparent and decentralized exchange of data as native. It is
adaptable and can be used in a wide range of Internet-based interactive systems
in academic and industrial settings. The essential part of programmable
distributed ledgers such as Ethereum, Polkadot, Cardano and other Web 3.0
technologies are smart contracts. Smart contracts are programs executed on the
global blockchain, the code is public as well as all of the data managed within
the transactions, thus creating a system that is reliable and cannot be cheated
if designed properly. In this paper, in order to make the educational system
more transparent and versatile we will describe an educational learning
platform designed as a distributed system.

    

### [[2105.06145] Efficient Stepping Algorithms and Implementations for Parallel Shortest Paths](http://arxiv.org/abs/2105.06145)


  In this paper, we study the single-source shortest-path (SSSP) problem with
positive edge weights, which is a notoriously hard problem in the parallel
context. In practice, the $\Delta$-stepping algorithm proposed by Meyer and
Sanders has been widely adopted. However, $\Delta$-stepping has no known
worst-case bounds for general graphs. The performance of $\Delta$-stepping also
highly relies on the parameter $\Delta$. There have also been lots of
algorithms with theoretical bounds, such as Radius-stepping, but they either
have no implementations available or are much slower than $\Delta$-stepping in
practice.
We propose a stepping algorithm framework that generalizes existing
algorithms such as $\Delta$-stepping and Radius-stepping. The framework allows
for similar analysis and implementations of all stepping algorithms. We also
propose a new ADT, lazy-batched priority queue (LaB-PQ), that abstracts the
semantics of the priority queue needed by the stepping algorithms. We provide
two data structures for LaB-PQ, focusing on theoretical and practical
efficiency, respectively. Based on the new framework and LaB-PQ, we show two
new stepping algorithms, $\rho$-stepping and $\Delta^*$-stepping, that are
simple, with non-trivial worst-case bounds, and fast in practice.
The stepping algorithm framework also provides almost identical
implementations for three algorithms: Bellman-Ford, $\Delta^*$-stepping, and
$\rho$-stepping. We compare our code with four state-of-the-art
implementations. On five social and web graphs, $\rho$-stepping is 1.3--2.5x
faster than all the existing implementations. On two road graphs, our
$\Delta^*$-stepping is at least 14\% faster than existing implementations,
while $\rho$-stepping is also competitive. The almost identical implementations
for stepping algorithms also allow for in-depth analyses and comparisons among
the stepping algorithms in practice.

    

### [[2110.04865] Parallel Minimum Spanning Forest Computation using Sparse Matrix Kernels](http://arxiv.org/abs/2110.04865)


  Formulations of graph algorithms using sparse linear algebra have yielded
highly scalable distributed algorithms for problems such as connectivity and
shortest path computation. We develop the first formulation of the
Awerbuch-Shiloach parallel minimum spanning forest (MSF) algorithm using linear
algebra primitives. We introduce a multilinear kernel that operates on an
adjacency matrix and two vectors. This kernel updates graph vertices by
simultaneously using information from both adjacent edges and vertices. In
addition, we explore optimizations to accelerate the shortcutting step in the
Awerbuch-Shiloach algorithm. We implement this MSF algorithm with Cyclops, a
distributed-memory library for generalized sparse tensor algebra. We analyze
the parallel scalability of our implementation on the Stampede2 supercomputer.

    

### [[2112.07669] AI and extreme scale computing to learn and infer the physics of higher order gravitational wave modes of quasi-circular, spinning, non-precessing binary black hole mergers](http://arxiv.org/abs/2112.07669)


  We use artificial intelligence (AI) to learn and infer the physics of higher
order gravitational wave modes of quasi-circular, spinning, non precessing
binary black hole mergers. We trained AI models using 14 million waveforms,
produced with the surrogate model NRHybSur3dq8, that include modes up to $\ell
\leq 4$ and $(5,5)$, except for $(4,0)$ and $(4,1)$, that describe binaries
with mass-ratios $q\leq8$ and individual spins $s^z_{\{1,2\}}\in[-0.8, 0.8]$.
We use our AI models to obtain deterministic and probabilistic estimates of the
mass-ratio, individual spins, effective spin, and inclination angle of
numerical relativity waveforms that describe such signal manifold. Our studies
indicate that AI provides informative estimates for these physical parameters.
This work marks the first time AI is capable of characterizing this
high-dimensional signal manifold. Our AI models were trained within 3.4 hours
using distributed training on 256 nodes (1,536 NVIDIA V100 GPUs) in the Summit
supercomputer.

    

### [[2112.07761] Split Moves for Monte-Carlo Tree Search](http://arxiv.org/abs/2112.07761)


  In many games, moves consist of several decisions made by the player. These
decisions can be viewed as separate moves, which is already a common practice
in multi-action games for efficiency reasons. Such division of a player move
into a sequence of simpler / lower level moves is called \emph{splitting}. So
far, split moves have been applied only in forementioned straightforward cases,
and furthermore, there was almost no study revealing its impact on agents'
playing strength. Taking the knowledge-free perspective, we aim to answer how
to effectively use split moves within Monte-Carlo Tree Search (MCTS) and what
is the practical impact of split design on agents' strength. This paper
proposes a generalization of MCTS that works with arbitrarily split moves. We
design several variations of the algorithm and try to measure the impact of
split moves separately on efficiency, quality of MCTS, simulations, and
action-based heuristics. The tests are carried out on a set of board games and
performed using the Regular Boardgames General Game Playing formalism, where
split strategies of different granularity can be automatically derived based on
an abstract description of the game. The results give an overview of the
behavior of agents using split design in different ways. We conclude that split
design can be greatly beneficial for single- as well as multi-action games.

    

### [[2112.07773] Filling gaps in trustworthy development of AI](http://arxiv.org/abs/2112.07773)


  The range of application of artificial intelligence (AI) is vast, as is the
potential for harm. Growing awareness of potential risks from AI systems has
spurred action to address those risks, while eroding confidence in AI systems
and the organizations that develop them. A 2019 study found over 80
organizations that published and adopted "AI ethics principles'', and more have
joined since. But the principles often leave a gap between the "what" and the
"how" of trustworthy AI development. Such gaps have enabled questionable or
ethically dubious behavior, which casts doubts on the trustworthiness of
specific organizations, and the field more broadly. There is thus an urgent
need for concrete methods that both enable AI developers to prevent harm and
allow them to demonstrate their trustworthiness through verifiable behavior.
Below, we explore mechanisms (drawn from arXiv:2004.07213) for creating an
ecosystem where AI developers can earn trust - if they are trustworthy. Better
assessment of developer trustworthiness could inform user choice, employee
actions, investment decisions, legal recourse, and emerging governance regimes.

    

### [[2112.07774] Assessing Human Interaction in Virtual Reality With Continually Learning Prediction Agents Based on Reinforcement Learning Algorithms: A Pilot Study](http://arxiv.org/abs/2112.07774)


  Artificial intelligence systems increasingly involve continual learning to
enable flexibility in general situations that are not encountered during system
training. Human interaction with autonomous systems is broadly studied, but
research has hitherto under-explored interactions that occur while the system
is actively learning, and can noticeably change its behaviour in minutes. In
this pilot study, we investigate how the interaction between a human and a
continually learning prediction agent develops as the agent develops
competency. Additionally, we compare two different agent architectures to
assess how representational choices in agent design affect the human-agent
interaction. We develop a virtual reality environment and a time-based
prediction task wherein learned predictions from a reinforcement learning (RL)
algorithm augment human predictions. We assess how a participant's performance
and behaviour in this task differs across agent types, using both quantitative
and qualitative analyses. Our findings suggest that human trust of the system
may be influenced by early interactions with the agent, and that trust in turn
affects strategic behaviour, but limitations of the pilot study rule out any
conclusive statement. We identify trust as a key feature of interaction to
focus on when considering RL-based technologies, and make several
recommendations for modification to this study in preparation for a
larger-scale investigation. A video summary of this paper can be found at
this https URL .

    

### [[2112.07790] Maximum Bayes Smatch Ensemble Distillation for AMR Parsing](http://arxiv.org/abs/2112.07790)


  AMR parsing has experienced an unprecendented increase in performance in the
last three years, due to a mixture of effects including architecture
improvements and transfer learning. Self-learning techniques have also played a
role in pushing performance forward. However, for most recent high performant
parsers, the effect of self-learning and silver data generation seems to be
fading. In this paper we show that it is possible to overcome this diminishing
returns of silver data by combining Smatch-based ensembling techniques with
ensemble distillation. In an extensive experimental setup, we push single model
English parser performance above 85 Smatch for the first time and return to
substantial gains. We also attain a new state-of-the-art for cross-lingual AMR
parsing for Chinese, German, Italian and Spanish. Finally we explore the impact
of the proposed distillation technique on domain adaptation, and show that it
can produce gains rivaling those of human annotated data for QALD-9 and achieve
a new state-of-the-art for BioAMR.

    

### [[2112.07819] Weed Recognition using Deep Learning Techniques on Class-imbalanced Imagery](http://arxiv.org/abs/2112.07819)


  Most weed species can adversely impact agricultural productivity by competing
for nutrients required by high-value crops. Manual weeding is not practical for
large cropping areas. Many studies have been undertaken to develop automatic
weed management systems for agricultural crops. In this process, one of the
major tasks is to recognise the weeds from images. However, weed recognition is
a challenging task. It is because weed and crop plants can be similar in
colour, texture and shape which can be exacerbated further by the imaging
conditions, geographic or weather conditions when the images are recorded.
Advanced machine learning techniques can be used to recognise weeds from
imagery. In this paper, we have investigated five state-of-the-art deep neural
networks, namely VGG16, ResNet-50, Inception-V3, Inception-ResNet-v2 and
MobileNetV2, and evaluated their performance for weed recognition. We have used
several experimental settings and multiple dataset combinations. In particular,
we constructed a large weed-crop dataset by combining several smaller datasets,
mitigating class imbalance by data augmentation, and using this dataset in
benchmarking the deep neural networks. We investigated the use of transfer
learning techniques by preserving the pre-trained weights for extracting the
features and fine-tuning them using the images of crop and weed datasets. We
found that VGG16 performed better than others on small-scale datasets, while
ResNet-50 performed better than other deep networks on the large combined
dataset.

    

### [[2112.07820] Value Retrieval with Arbitrary Queries for Form-like Documents](http://arxiv.org/abs/2112.07820)


  We propose value retrieval with arbitrary queries for form-like documents to
reduce human effort of processing forms. Unlike previous methods that only
address a fixed set of field items, our method predicts target value for an
arbitrary query based on the understanding of layout and semantics of a form.
To further boost model performance, we propose a simple document language
modeling (simpleDLM) strategy to improve document understanding on large-scale
model pre-training. Experimental results show that our method outperforms our
baselines significantly and the simpleDLM further improves our performance on
value retrieval by around 17\% F1 score compared with the state-of-the-art
pre-training method. Code will be made publicly available.

    

### [[2112.07835] Mining Minority-class Examples With Uncertainty Estimates](http://arxiv.org/abs/2112.07835)


  In the real world, the frequency of occurrence of objects is naturally skewed
forming long-tail class distributions, which results in poor performance on the
statistically rare classes. A promising solution is to mine tail-class examples
to balance the training dataset. However, mining tail-class examples is a very
challenging task. For instance, most of the otherwise successful
uncertainty-based mining approaches struggle due to distortion of class
probabilities resulting from skewness in data. In this work, we propose an
effective, yet simple, approach to overcome these challenges. Our framework
enhances the subdued tail-class activations and, thereafter, uses a one-class
data-centric approach to effectively identify tail-class examples. We carry out
an exhaustive evaluation of our framework on three datasets spanning over two
computer vision tasks. Substantial improvements in the minority-class mining
and fine-tuned model's performance strongly corroborate the value of our
proposed solution.

    

### [[2112.07846] Probabilistic Logic Gate in Asynchronous Game of Life with Critical Property](http://arxiv.org/abs/2112.07846)


  Metaheuristic and self-organizing criticality (SOC) could contribute to
robust computation under perturbed environments. Implementing a logic gate in a
computing system in a critical state is one of the intriguing ways to study the
role of metaheuristics and SOCs. Here, we study the behavior of cellular
automaton, game of life (GL), in asynchronous updating and implement
probabilistic logic gates by using asynchronous GL. We find that asynchronous
GL shows a phase transition, that the density of the state of 1 decays with the
power law at the critical point, and that systems at the critical point have
the most computability in asynchronous GL. We implement AND and OR gates in
asynchronous GL with criticality, which shows good performance. Since tuning
perturbations play an essential role in operating logic gates, our study
reveals the interference between manipulation and perturbation in probabilistic
logic gates.

    

### [[2112.07850] HyObscure: Hybrid Obscuring for Privacy-Preserving Data Publishing](http://arxiv.org/abs/2112.07850)


  Minimizing privacy leakage while ensuring data utility is a critical problem
to data holders in a privacy-preserving data publishing task. Most prior
research concerns only with one type of data and resorts to a single obscuring
method, \eg, obfuscation or generalization, to achieve a privacy-utility
tradeoff, which is inadequate for protecting real-life heterogeneous data and
is hard to defend ever-growing machine learning based inference attacks. This
work takes a pilot study on privacy-preserving data publishing when both
generalization and obfuscation operations are employed for heterogeneous data
protection. To this end, we first propose novel measures for privacy and
utility quantification and formulate the hybrid privacy-preserving data
obscuring problem to account for the joint effect of generalization and
obfuscation. We then design a novel hybrid protection mechanism called
HyObscure, to cross-iteratively optimize the generalization and obfuscation
operations for maximum privacy protection under a certain utility guarantee.
The convergence of the iterative process and the privacy leakage bound of
HyObscure are also provided in theory. Extensive experiments demonstrate that
HyObscure significantly outperforms a variety of state-of-the-art baseline
methods when facing various inference attacks under different scenarios.
HyObscure also scales linearly to the data size and behaves robustly with
varying key parameters.

    

### [[2112.07867] Interscript: A dataset for interactive learning of scripts through error feedback](http://arxiv.org/abs/2112.07867)


  How can an end-user provide feedback if a deployed structured prediction
model generates inconsistent output, ignoring the structural complexity of
human language? This is an emerging topic with recent progress in synthetic or
constrained settings, and the next big leap would require testing and tuning
models in real-world settings. We present a new dataset, Interscript,
containing user feedback on a deployed model that generates complex everyday
tasks. Interscript contains 8,466 data points -- the input is a possibly
erroneous script and a user feedback, and the output is a modified script. We
posit two use-cases of \ours that might significantly advance the
state-of-the-art in interactive learning. The dataset is available at:
this https URL.

    

### [[2112.07868] Few-shot Instruction Prompts for Pretrained Language Models to Detect Social Biases](http://arxiv.org/abs/2112.07868)


  Detecting social bias in text is challenging due to nuance, subjectivity, and
difficulty in obtaining good quality labeled datasets at scale, especially
given the evolving nature of social biases and society. To address these
challenges, we propose a few-shot instruction-based method for prompting
pre-trained language models (LMs). We select a few label-balanced exemplars
from a small support repository that are closest to the query to be labeled in
the embedding space. We then provide the LM with instruction that consists of
this subset of labeled exemplars, the query text to be classified, a definition
of bias, and prompt it to make a decision. We demonstrate that large LMs used
in a few-shot context can detect different types of fine-grained biases with
similar and sometimes superior accuracy to fine-tuned models. We observe that
the largest 530B parameter model is significantly more effective in detecting
social bias compared to smaller models (achieving at least 20% improvement in
AUC metric compared to other models). It also maintains a high AUC (dropping
less than 5%) in a few-shot setting with a labeled repository reduced to as few
as 100 samples. Large pretrained language models thus make it easier and
quicker to build new bias detectors.

    

### [[2112.07969] Predicting Media Memorability: Comparing Visual, Textual and Auditory Features](http://arxiv.org/abs/2112.07969)


  This paper describes our approach to the Predicting Media Memorability task
in MediaEval 2021, which aims to address the question of media memorability by
setting the task of automatically predicting video memorability. This year we
tackle the task from a comparative standpoint, looking to gain deeper insights
into each of three explored modalities, and using our results from last year's
submission (2020) as a point of reference. Our best performing short-term
memorability model (0.132) tested on the TRECVid2019 dataset -- just like last
year -- was a frame based CNN that was not trained on any TRECVid data, and our
best short-term memorability model (0.524) tested on the Memento10k dataset,
was a Bayesian Ride Regressor fit with DenseNet121 visual features.

    

### [[2112.08022] Segmentation-Reconstruction-Guided Facial Image De-occlusion](http://arxiv.org/abs/2112.08022)


  Occlusions are very common in face images in the wild, leading to the
degraded performance of face-related tasks. Although much effort has been
devoted to removing occlusions from face images, the varying shapes and
textures of occlusions still challenge the robustness of current methods. As a
result, current methods either rely on manual occlusion masks or only apply to
specific occlusions. This paper proposes a novel face de-occlusion model based
on face segmentation and 3D face reconstruction, which automatically removes
all kinds of face occlusions with even blurred boundaries,e.g., hairs. The
proposed model consists of a 3D face reconstruction module, a face segmentation
module, and an image generation module. With the face prior and the occlusion
mask predicted by the first two, respectively, the image generation module can
faithfully recover the missing facial textures. To supervise the training, we
further build a large occlusion dataset, with both manually labeled and
synthetic occlusions. Qualitative and quantitative results demonstrate the
effectiveness and robustness of the proposed method.

    

### [[2112.08087] Cognition-aware Cognate Detection](http://arxiv.org/abs/2112.08087)


  Automatic detection of cognates helps downstream NLP tasks of Machine
Translation, Cross-lingual Information Retrieval, Computational Phylogenetics
and Cross-lingual Named Entity Recognition. Previous approaches for the task of
cognate detection use orthographic, phonetic and semantic similarity based
features sets. In this paper, we propose a novel method for enriching the
feature sets, with cognitive features extracted from human readers' gaze
behaviour. We collect gaze behaviour data for a small sample of cognates and
show that extracted cognitive features help the task of cognate detection.
However, gaze data collection and annotation is a costly task. We use the
collected gaze behaviour data to predict cognitive features for a larger sample
and show that predicted cognitive features, also, significantly improve the
task performance. We report improvements of 10% with the collected gaze
features, and 12% using the predicted gaze features, over the previously
proposed approaches. Furthermore, we release the collected gaze behaviour data
along with our code and cross-lingual models.

    

### [[2112.08136] Characterizing the Program Expressive Power of Existential Rule Languages](http://arxiv.org/abs/2112.08136)


  Existential rule languages are a family of ontology languages that have been
widely used in ontology-mediated query answering (OMQA). However, for most of
them, the expressive power of representing domain knowledge for OMQA, known as
the program expressive power, is not well-understood yet. In this paper, we
establish a number of novel characterizations for the program expressive power
of several important existential rule languages, including tuple-generating
dependencies (TGDs), linear TGDs, as well as disjunctive TGDs. The
characterizations employ natural model-theoretic properties, and
automata-theoretic properties sometimes, which thus provide powerful tools for
identifying the definability of domain knowledge for OMQA in these languages.

    

### [[2112.08140] Improving Conversational Recommendation Systems' Quality with Context-Aware Item Meta Information](http://arxiv.org/abs/2112.08140)


  Conversational recommendation systems (CRS) engage with users by inferring
user preferences from dialog history, providing accurate recommendations, and
generating appropriate responses. Previous CRSs use knowledge graph (KG) based
recommendation modules and integrate KG with language models for response
generation. Although KG-based approaches prove effective, two issues remain to
be solved. First, KG-based approaches ignore the information in the
conversational context but only rely on entity relations and bag of words to
recommend items. Second, it requires substantial engineering efforts to
maintain KGs that model domain-specific relations, thus leading to less
flexibility. In this paper, we propose a simple yet effective architecture
comprising a pre-trained language model (PLM) and an item metadata encoder. The
encoder learns to map item metadata to embeddings that can reflect the semantic
information in the dialog context. The PLM then consumes the semantic-aligned
item embeddings together with dialog context to generate high-quality
recommendations and responses. Instead of modeling entity relations with KGs,
our model reduces engineering complexity by directly converting each item to an
embedding. Experimental results on the benchmark dataset ReDial show that our
model obtains state-of-the-art results on both recommendation and response
generation tasks.

    

### [[2112.08171] Text Gestalt: Stroke-Aware Scene Text Image Super-Resolution](http://arxiv.org/abs/2112.08171)


  In the last decade, the blossom of deep learning has witnessed the rapid
development of scene text recognition. However, the recognition of
low-resolution scene text images remains a challenge. Even though some
super-resolution methods have been proposed to tackle this problem, they
usually treat text images as general images while ignoring the fact that the
visual quality of strokes (the atomic unit of text) plays an essential role for
text recognition. According to Gestalt Psychology, humans are capable of
composing parts of details into the most similar objects guided by prior
knowledge. Likewise, when humans observe a low-resolution text image, they will
inherently use partial stroke-level details to recover the appearance of
holistic characters. Inspired by Gestalt Psychology, we put forward a
Stroke-Aware Scene Text Image Super-Resolution method containing a
Stroke-Focused Module (SFM) to concentrate on stroke-level internal structures
of characters in text images. Specifically, we attempt to design rules for
decomposing English characters and digits at stroke-level, then pre-train a
text recognizer to provide stroke-level attention maps as positional clues with
the purpose of controlling the consistency between the generated
super-resolution image and high-resolution ground truth. The extensive
experimental results validate that the proposed method can indeed generate more
distinguishable images on TextZoom and manually constructed Chinese character
dataset Degraded-IC13. Furthermore, since the proposed SFM is only used to
provide stroke-level guidance when training, it will not bring any time
overhead during the test phase. Code is available at
this https URL.

    

### [[2112.08178] Interpretable Feature Learning Framework for Smoking Behavior Detection](http://arxiv.org/abs/2112.08178)


  Smoking in public has been proven to be more harmful to nonsmokers, making it
a huge public health concern with urgent need for proactive measures and
attention by authorities. With the world moving towards the 4th Industrial
Revolution, there is a need for reliable eco-friendly detective measures
towards this harmful intoxicating behavior to public health in and out of smart
cities. We developed an Interpretable feature learning framework for smoking
behavior detection which utilizes a Deep Learning VGG-16 pretrained network to
predict and classify the input Image class and a Layer-wise Relevance
Propagation (LRP) to explain the network detection or prediction of smoking
behavior based on the most relevant learned features or pixels or neurons. The
network's classification decision is based mainly on features located at the
mouth especially the smoke seems to be of high importance to the network's
decision. The outline of the smoke is highlighted as evidence for the
corresponding class. Some elements are seen as having a negative effect on the
smoke neuron and are consequently highlighted differently. It is interesting to
see that the network distinguishes important from unimportant features based on
the image regions. The technology can also detect other smokeable drugs like
weed, shisha, marijuana etc. The framework allows for reliable identification
of action-based smokers in unsafe zones like schools, shopping malls, bus
stops, railway compartments or other violated places for smoking as per the
government's regulatory health policies. With installation clearly defined in
smoking zones, this technology can detect smokers out of range.

    

### [[2112.08185] Learning Cross-Lingual IR from an English Retriever](http://arxiv.org/abs/2112.08185)


  We present a new cross-lingual information retrieval (CLIR) model trained
using multi-stage knowledge distillation (KD). The teacher and the student are
heterogeneous systems-the former is a pipeline that relies on machine
translation and monolingual IR, while the latter executes a single CLIR
operation. We show that the student can learn both multilingual representations
and CLIR by optimizing two corresponding KD objectives. Learning multilingual
representations from an English-only retriever is accomplished using a novel
cross-lingual alignment algorithm that greedily re-positions the teacher tokens
for alignment. Evaluation on the XOR-TyDi benchmark shows that the proposed
model is far more effective than the existing approach of fine-tuning with
cross-lingual labeled IR data, with a gain in accuracy of 25.4 Recall@5kt.

    

### [[2112.08186] Planning with Biological Neurons and Synapses](http://arxiv.org/abs/2112.08186)


  We revisit the planning problem in the blocks world, and we implement a known
heuristic for this task. Importantly, our implementation is biologically
plausible, in the sense that it is carried out exclusively through the spiking
of neurons. Even though much has been accomplished in the blocks world over the
past five decades, we believe that this is the first algorithm of its kind. The
input is a sequence of symbols encoding an initial set of block stacks as well
as a target set, and the output is a sequence of motion commands such as ``put
the top block in stack 1 on the table''. The program is written in the Assembly
Calculus, a recently proposed computational framework meant to model
computation in the brain by bridging the gap between neural activity and
cognitive function. Its elementary objects are assemblies of neurons (stable
sets of neurons whose simultaneous firing signifies that the subject is
thinking of an object, concept, word, etc.), its commands include project and
merge, and its execution model is based on widely accepted tenets of
neuroscience. A program in this framework essentially sets up a dynamical
system of neurons and synapses that eventually, with high probability,
accomplishes the task. The purpose of this work is to establish empirically
that reasonably large programs in the Assembly Calculus can execute correctly
and reliably; and that rather realistic -- if idealized -- higher cognitive
functions, such as planning in the blocks world, can be implemented
successfully by such programs.

    

### [[2112.08198] Single Image Automatic Radial Distortion Compensation Using Deep Convolutional Network](http://arxiv.org/abs/2112.08198)


  In many computer vision domains, the input images must conform with the
pinhole camera model, where straight lines in the real world are projected as
straight lines in the image. Performing computer vision tasks on live sports
broadcast footage imposes challenging requirements where the algorithms cannot
rely on a specific calibration pattern must be able to cope with unknown and
uncalibrated cameras, radial distortion originating from complex television
lenses, few visual clues to compensate distortion by, and the necessity for
real-time performance. We present a novel method for single-image automatic
lens distortion compensation based on deep convolutional neural networks,
capable of real-time performance and accuracy using two highest-order
coefficients of the polynomial distortion model operating in the application
domain of sports broadcast. Keywords: Deep Convolutional Neural Network, Radial
Distortion, Single Image Rectification

    

### [[2112.08227] An Experimental Study of the Impact of Pre-training on the Pruning of a Convolutional Neural Network](http://arxiv.org/abs/2112.08227)


  In recent years, deep neural networks have known a wide success in various
application domains. However, they require important computational and memory
resources, which severely hinders their deployment, notably on mobile devices
or for real-time applications. Neural networks usually involve a large number
of parameters, which correspond to the weights of the network. Such parameters,
obtained with the help of a training process, are determinant for the
performance of the network. However, they are also highly redundant. The
pruning methods notably attempt to reduce the size of the parameter set, by
identifying and removing the irrelevant weights. In this paper, we examine the
impact of the training strategy on the pruning efficiency. Two training
modalities are considered and compared: (1) fine-tuned and (2) from scratch.
The experimental results obtained on four datasets (CIFAR10, CIFAR100, SVHN and
Caltech101) and for two different CNNs (VGG16 and MobileNet) demonstrate that a
network that has been pre-trained on a large corpus (e.g. ImageNet) and then
fine-tuned on a particular dataset can be pruned much more efficiently (up to
80% of parameter reduction) than the same network trained from scratch.

    

### [[2112.08256] Est-ce que vous compute? Code-switching, cultural identity, and AI](http://arxiv.org/abs/2112.08256)


  Cultural code-switching concerns how we adjust our overall behaviours,
manners of speaking, and appearance in response to a perceived change in our
social environment. We defend the need to investigate cultural code-switching
capacities in artificial intelligence systems. We explore a series of ethical
and epistemic issues that arise when bringing cultural code-switching to bear
on artificial intelligence. Building upon Dotson's (2014) analysis of
testimonial smothering, we discuss how emerging technologies in AI can give
rise to epistemic oppression, and specifically, a form of self-silencing that
we call 'cultural smothering'. By leaving the socio-dynamic features of
cultural code-switching unaddressed, AI systems risk negatively impacting
already-marginalised social groups by widening opportunity gaps and further
entrenching social inequalities.

    

### [[2112.08273] Programming Knowledge Tracing: A Comprehensive Dataset and A New Model](http://arxiv.org/abs/2112.08273)


  In this paper, we study knowledge tracing in the domain of programming
education and make two important contributions. First, we harvest and publish
so far the most comprehensive dataset, namely BePKT, which covers various
online behaviors in an OJ system, including programming text problems,
knowledge annotations, user-submitted code and system-logged events. Second, we
propose a new model PDKT to exploit the enriched context for accurate student
behavior prediction. More specifically, we construct a bipartite graph for
programming problem embedding, and design an improved pre-training model
PLCodeBERT for code embedding, as well as a double-sequence RNN model with
exponential decay attention for effective feature fusion. Experimental results
on the new dataset BePKT show that our proposed model establishes
state-of-the-art performance in programming knowledge tracing. In addition, we
verify that our code embedding strategy based on PLCodeBERT is complementary to
existing knowledge tracing models to further enhance their accuracy. As a side
product, PLCodeBERT also results in better performance in other
programming-related tasks such as code clone detection.

    

### [[2105.09418] iTelos -- Purpose Driven Knowledge Graph Generation](http://arxiv.org/abs/2105.09418)


  When building a new application we are more and more confronted with the need
of reusing and integrating pre-existing knowledge, e.g., ontologies, schemas,
data of any kind, from multiple sources. Nevertheless, it is a fact that this
prior knowledge is virtually impossible to reuse as-is. This difficulty is the
cause of high costs, with the further drawback that the resulting application
will again be hardly reusable. It is a negative loop which consistently
reinforces itself. iTelos is a general purpose methodology aiming at minimizing
as much as possible the effects of this loop. iTelos is based on the intuition
that the data level and the schema level of an application should be developed
independently, thus allowing for maximum flexibility in the reuse of the prior
knowledge, but under the overall guidance of the needs to be satisfied,
formalized as competence queries. This intuition is implemented by codifying
all the requirements, including those concerning reuse, as part of an a-priori
defined purpose, which is then used to drive a middle-out development process
where the application schema and data are continuously aligned.

    

### [[2108.04108] Team Power Dynamics and Team Impact: New Perspectives on Scientific Collaboration using Career Age as a Proxy for Team Power](http://arxiv.org/abs/2108.04108)


  Power is an unavoidable yet unrecognized element of collaboration. Power
dynamics influence every aspect of scientific collaboration. Team power
dynamics can be measured by team power level and team power hierarchy. Team
power level is conceptualized as the average level of the possession of
resources, expertise, or decision-making authorities of a team. Team power
hierarchy represents the vertical differences of the possessions of resources
in a team. In Science of Science, few studies have looked at scientific
collaboration from the perspective of team power dynamics. This research
examines how team power dynamics affect team impact to fill the research gap.
In this research, all co-authors of one publication are treated as one team.
Team power level and team power hierarchy of one team are measured by the mean
and Gini index of career age of co-authors in this team. Team impact is
quantified by citations of a paper authored by this team. By analyzing over 7.7
million teams from Science (e.g., Computer Science, Physics), Social Sciences
(e.g., Sociology, Library & Information Science), and Arts & Humanities (e.g.,
Art), we find that flat team structure is associated with higher team impact.
When team power level increases, teams with low team power hierarchy get cited
more than teams with high team power hierarchy. These findings have been
repeated in all five disciplines except Art, and are consistent in various
types of teams from Computer Science including teams from industry or academia,
teams with different gender groups, teams with geographical contrast, and teams
with distinct size.

    

### [[2112.03203] An unsupervised extractive summarization method based on multi-round computation](http://arxiv.org/abs/2112.03203)


  Text summarization methods have attracted much attention all the time. In
recent years, deep learning has been applied to text summarization, and it
turned out to be pretty effective. However, most of the current text
summarization methods based on deep learning need large-scale datasets, which
is difficult to achieve in practical applications. In this paper, an
unsupervised extractive text summarization method based on multi-round
calculation is proposed. Based on the directed graph algorithm, we change the
traditional method of calculating the sentence ranking at one time to
multi-round calculation, and the summary sentences are dynamically optimized
after each round of calculation to better match the characteristics of the
text. In this paper, experiments are carried out on four data sets, each
separately containing Chinese, English, long and short texts. The experiment
results show that our method has better performance than both baseline methods
and other unsupervised methods and is robust on different datasets.

    

### [[2112.07880] An Empirical Lower Bound on the Overheads of Production Garbage Collectors](http://arxiv.org/abs/2112.07880)


  Despite the long history of garbage collection (GC) and its prevalence in
modern programming languages, there is surprisingly little clarity about its
true overheads. Even when evaluated using modern, well-established
methodologies, crucial tradeoffs made by GCs can go unnoticed, which leads to
misinterpretation of evaluation results. In this paper, we 1) develop a
methodology that allows us to place a lower bound on the absolute overhead
(LBO) of GCs, and 2) expose key performance tradeoffs of five production GCs in
OpenJDK 17, a high-performance Java runtime. We find that with a modest heap
size and across a diverse suite of modern benchmarks, production GCs incur
substantial overheads, spending 7-82% more wall-clock time and 6-92% more CPU
cycles relative to a zero-cost GC scheme. We show that these overheads can be
masked by concurrency and generous provision of memory/compute. In addition, we
find that newer low-pause GCs are significantly more expensive than older GCs,
and sometimes even deliver worse application latency than stop-the-world GCs.
Our findings reaffirm that GC is by no means a solved problem and that a
low-cost, low-latency GC remains elusive. We recommend adopting the LBO
methodology and using a wider range of cost metrics for future GC evaluations.
This will not only help the community more comprehensively understand the
performance characteristics of different GCs, but also reveal opportunities for
future GC optimizations.

    

### [[2112.07817] Simulating Large Eliminations in Cedille](http://arxiv.org/abs/2112.07817)


  Large eliminations provide an expressive mechanism for arity- and
type-generic programming. However, as large eliminations are closely tied to a
type theory's primitive notion of inductive type, this expressivity is not
expected within polymorphic lambda calculi in which datatypes are encoded using
impredicative quantification. We report progress on simulating large
eliminations for datatype encodings in one such type theory, the calculus of
dependent lambda eliminations (CDLE). Specifically, we show that the expected
computation rules for large eliminations, expressed using a derived type of
extensional equality of types, can be proven within CDLE. We present several
case studies, demonstrating the adequacy of this simulation for a variety of
generic programming tasks, and a generic formulation of the simulation allowing
its use for any datatype. All results have been mechanically checked by
Cedille, an implementation of CDLE.

    

### [[2112.08272] Reflective Metagraph Rewriting as a Foundation for an AGI "Language of Thought"](http://arxiv.org/abs/2112.08272)


  MeTTa (Meta Type Talk) is a novel programming language created for use in the
OpenCog Hyperon AGI system. It is designed as a meta-language with very basic
and general facilities for handling symbols, groundings, variables, types,
substitutions and pattern matching. Primitives exist for creating new type
systems and associated DSLs. IInformally, MeTTa is Hyperon's lowest-level
"language of thought" -- the meta-language in which algorithms for learning
more particular knowledge representations, will operate, and in which these
algorithms themselves may be represented. Here we explain how one might go
about formalizing the MeTTa language as a system of metagraph rewrite rules, an
approach that fits in naturally to the Hyperon framework given that the
latter's core component is a distributed metagraph knowledge store (the
Atomspace). The metagraph rewrite rules constituting MeTTa programs can also be
represented as metagraphs, giving a natural model for MeTTa reflection and
self-modifying code. Considering MeTTa programs that compute equivalences
between execution traces of other MeTTa programs allows us to model spaces of
MeTTa execution traces using Homotopy Type Theory. Considering the limit of
MeTTa programs mapping between execution traces of MeTTa programs that map
between execution traces of MeTTa programs that ..., we find that a given MeTTa
codebase is effectively modeled as an infinity-groupoid, and the space of all
MeTTa codebases as an (infinity,1)-topis This topos is basically the same as
the so-called "Ruliad" previously derived from rewrite rules on hypergraphs, in
a discrete physics context. The formalization of MeTTA as metagraph rewrite
rules may also provide a useful framework for structuring the implementation of
efficient methods for pattern matching and equality inference within the MeTTa
interpreter.

    

### [[2102.07515] Sequence Types and Infinitary Semantics](http://arxiv.org/abs/2102.07515)


  We introduce a new representation of non-idempotent intersection types, using
\textbf{sequences} (families indexed with natural numbers) instead of lists or
multisets. This allows scaling up \textbf{intersection type} theory to the
infinitary $\lambda$-calculus. We thus characterize hereditary head
normalization, which gives a positive answer to a question known as
\textbf{Klop's Problem}. On our way, we use \textbf{non-idempotent
intersection} to retrieve some well-known results on infinitary terms.

    