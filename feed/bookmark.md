
## 2021-12-2

### [[2112.00143] A Comprehensive Survey on the Convergence of Vehicular Social Networks and Fog Computing](http://arxiv.org/abs/2112.00143)


  In recent years, the number of IoT devices has been growing fast which leads
to a challenging task for managing, storing, analyzing, and making decisions
about raw data from different IoT devices, especially for delay-sensitive
applications. In a vehicular network (VANET) environment, the dynamic nature of
vehicles makes the current open research issues even more challenging due to
the frequent topology changes that can lead to disconnections between vehicles.
To this end, a number of research works have been proposed in the context of
cloud and fog computing over the 5G infrastructure. On the other hand, there
are a variety of research proposals that aim to extend the connection time
between vehicles. Vehicular Social Networks (VSNs) have been defined to
decrease the burden of connection time between the vehicles. This survey paper
first provides the necessary background information and definitions about fog,
cloud and related paradigms such as 5G and SDN. Then, it introduces the reader
to Vehicular Social Networks, the different metrics and the main differences
between VSNs and Online Social Networks. Finally, this survey investigates the
related works in the context of VANETs that have demonstrated different
architectures to address the different issues in fog computing. Moreover, it
provides a categorization of the different approaches and discusses the
required metrics in the context of fog and cloud and compares them to Vehicular
social networks. A comparison of the relevant related works is discussed along
with new research challenges and trends in the domain of VSNs and fog
computing.

    

### [[2112.00147] Slicing Scheduling for Supporting Critical Traffic in Beyond 5G](http://arxiv.org/abs/2112.00147)


  One of the most challenging services fifth-generation (5G) mobile network is
designed to support, is the critical services in-need of very low latency,
and/or high reliability. It is now clear that such critical services will also
be at the core of beyond 5G (B5G) networks. While 5G radio design accommodates
such supports by introducing more flexibility in timing, how efficiently those
services could be scheduled over a shared network with other broadband services
remains as a challenge. In this paper, we use network slicing as an enabler for
network sharing and propose an optimization framework to schedule resources to
critical services via puncturing technique with minimal impact on the regular
broadband services. We then thoroughly examine the performance of the framework
in terms of throughput and reliability through simulation.

    

### [[2112.00273] Concurrent Transmission for Multi-Robot Coordination](http://arxiv.org/abs/2112.00273)


  An efficient communication mechanism forms the backbone for any multi-robot
system to achieve fruitful collaboration and coordination. Limitation in the
existing asynchronous transmission based strategies in fast dissemination and
aggregation compels the designers to prune down such requirements as much as
possible. This also restricts the possible application areas of mobile
multi-robot systems. In this work, we introduce concurrent transmission based
strategy as an alternative. Despite the commonly found difficulties in
concurrent transmission such as microsecond level time synchronization,
hardware heterogeneity, etc., we demonstrate how it can be exploited for
multi-robot systems. We propose a split architecture where the two major
activities - communication and computation are carried out independently and
coordinate through periodic interactions. The proposed split architecture is
applied on a custom build full networked control system consisting of five
two-wheel differential drive mobile robots having heterogeneous architecture.
We use the proposed design in a leader-follower setting for coordinated dynamic
speed variation as well as the independent formation of various shapes.
Experiments show a centimeter-level spatial and millisecond-level temporal
accuracy while spending very low radio duty-cycling over multi-hop
communication under a wide testing area.

    

### [[2112.00449] Frequent-Pattern Based Broadcast Scheduling for Conflict Avoidance in Multi-Channel Data Dissemination Systems](http://arxiv.org/abs/2112.00449)


  With the popularity of mobile devices, using the traditional client-server
model to handle a large number of requests is very challenging. Wireless data
broadcasting can be used to provide services to many users at the same time, so
reducing the average access time has become a popular research topic. For
example, some location-based services (LBS) consider using multiple channels to
disseminate information to reduce access time. However, data conflicts may
occur when multiple channels are used, where multiple data items associated
with the request are broadcast at about the same time. In this article, we
consider the channel switching time and identify the data conflict issue in an
on-demand multi-channel dissemination system. We model the considered problem
as a Data Broadcast with Conflict Avoidance (DBCA) problem and prove it is
NP-complete. We hence propose the frequent-pattern based broadcast scheduling
(FPBS), which provides a new variant of the frequent pattern tree, FP*-tree, to
schedule the requested data. Using FPBS, the system can avoid data conflicts
when assigning data items to time slots in the channels. In the simulation, we
discussed two modes of FPBS: online and offline. The results show that,
compared with the existing heuristic methods, FPBS can shorten the average
access time by 30%.

    

### [[2112.00515] TXOP sharing with Coordinated Spatial Reuse in Multi-AP Cooperative IEEE 802.11be WLANs](http://arxiv.org/abs/2112.00515)


  IEEE 802.11be networks (aka Wi-Fi 7) will have to cope with new
bandwidth-hungry and low-latency services such as eXtended Reality and
multi-party cloud gaming. With this goal in mind, transmit opportunity (TXOP)
sharing between coordinated access points (APs) may contribute to alleviating
inter-AP contention, hence increasing the overall network throughput. This
paper evaluates two coordinated TXOP sharing strategies: coordinated time
division multiple access (c-TDMA) and coordinated-TDMA with spatial reuse
(c-TDMA/SR). We show that, while c-TDMA alone does not result in any
significant improvement in terms of the WLAN throughput, it lays the groundwork
to implement coordinated SR (c-SR) techniques. To evaluate the performance of
c-TDMA/SR, we propose a fair scheduler able to select the best subset of
parallel transmissions in WLAN deployments, as well as the appropriate power
levels to be used by APs and stations (STAs), leading to maximum performance.
The results obtained for c-TDMA/SR show significant throughput gains compared
with c-TDMA, with values higher than 140% in 90% of the considered scenarios.

    

### [[2112.00543] (Causal)-Activation of Complex Entanglement Structures in Quantum Networks](http://arxiv.org/abs/2112.00543)


  Entanglement represents "the" key resource for several applications of
quantum information processing, ranging from quantum communications to
distributed quantum computing. Despite its fundamental importance,
deterministic generation of maximally entangled qubits represents an on-going
open problem. Here, we design a novel generation scheme exhibiting two
attractive features, namely, i) deterministically generating genuinely
multipartite entangled states, ii) without requiring any direct interaction
between the qubits. Indeed, the only necessary condition is the possibility of
coherently controlling -- according to the indefinite causal order framework --
the causal order among some unitaries acting on the qubits. Through the paper,
we analyze and derive the conditions on the unitaries for deterministic
generation, and we provide examples for unitaries practical implementation. We
conclude the paper by discussing the scalability of the proposed scheme to
higher dimensional GME states and by introducing some possible applications of
the proposal for quantum networks.

    

### [[2112.00619] Edge computing for cyber-physical systems: A systematic mapping study emphasizing trustworthiness](http://arxiv.org/abs/2112.00619)


  Edge computing is projected to have profound implications in the coming
decades, proposed to provide solutions for applications such as augmented
reality, predictive functionalities, and collaborative Cyber-Physical Systems
(CPS). For such applications, edge computing addresses the new computational
needs, as well as privacy, availability, and real-time constraints, by
providing local high-performance computing capabilities to deal with the
limitations and constraints of cloud and embedded systems. Our interests lie in
the applications of edge computing as part of CPS, where several properties (or
attributes) of trustworthiness, including safety, security, and
predictability/availability are of particular concern, each facing challenges
for the introduction of edge-based CPS. We present the results of a systematic
mapping study, a kind of systematic literature survey, investigating the use of
edge computing for CPS with a special emphasis on trustworthiness. The main
contributions of this study are a detailed description of the current research
efforts in edge-based CPS and the identification and discussion of trends and
research gaps. The results show that the main body of research in edge-based
CPS only to a very limited extent consider key attributes of system
trustworthiness, despite many efforts referring to critical CPS and
applications like intelligent transportation. More research and industrial
efforts will be needed on aspects of trustworthiness of future edge-based CPS
including their experimental evaluation. Such research needs to consider the
multiple interrelated attributes of trustworthiness including safety, security,
and predictability, and new methodologies and architectures to address them. It
is further important to provide bridges and collaboration between edge
computing and CPS disciplines.

    

### [[2112.00633] TEDGE-Caching: Transformer-based Edge Caching Towards 6G Networks](http://arxiv.org/abs/2112.00633)


  As a consequence of the COVID-19 pandemic, the demand for telecommunication
for remote learning/working and telemedicine has significantly increased.
Mobile Edge Caching (MEC) in the 6G networks has been evolved as an efficient
solution to meet the phenomenal growth of the global mobile data traffic by
bringing multimedia content closer to the users. Although massive connectivity
enabled by MEC networks will significantly increase the quality of
communications, there are several key challenges ahead. The limited storage of
edge nodes, the large size of multimedia content, and the time-variant users'
preferences make it critical to efficiently and dynamically predict the
popularity of content to store the most upcoming requested ones before being
requested. Recent advancements in Deep Neural Networks (DNNs) have drawn much
research attention to predict the content popularity in proactive caching
schemes. Existing DNN models in this context, however, suffer from longterm
dependencies, computational complexity, and unsuitability for parallel
computing. To tackle these challenges, we propose an edge caching framework
incorporated with the attention-based Vision Transformer (ViT) neural network,
referred to as the Transformer-based Edge (TEDGE) caching, which to the best of
our knowledge, is being studied for the first time. Moreover, the TEDGE caching
framework requires no data pre-processing and additional contextual
information. Simulation results corroborate the effectiveness of the proposed
TEDGE caching framework in comparison to its counterparts.

    

### [[2003.10094] Penalized and Decentralized Contextual Bandit Learning for WLAN Channel Allocation with Contention-Driven Feature Extraction](http://arxiv.org/abs/2003.10094)


  In this study, a contextual multi-armed bandit (CMAB)-based decentralized
channel exploration framework disentangling a channel utility function (i.e.,
reward) with respect to contending neighboring access points (APs) is proposed.
The proposed framework enables APs to evaluate observed rewards compositionally
for contending APs, allowing both robustness against reward fluctuation due to
neighboring APs' varying channels and assessment of even unexplored channels.
To realize this framework, we propose contention-driven feature extraction
(CDFE), which extracts the adjacency relation among APs under contention and
forms the basis for expressing reward functions in the disentangled form, that
is, a linear combination of parameters associated with neighboring APs under
contention). This allows the CMAB to be leveraged with joint a linear upper
confidence bound (JLinUCB) exploration and to delve into the effectiveness of
the proposed framework. Moreover, we address the problem of non-convergence --
the channel exploration cycle -- by proposing a penalized JLinUCB (P-JLinUCB)
based on the key idea of introducing a discount parameter to the reward for
exploiting a different channel before and after the learning round. Numerical
evaluations confirm that the proposed method allows APs to assess the channel
quality robustly against reward fluctuations by CDFE and achieves better
convergence properties by P-JLinUCB.

    

### [[2006.12242] Exploiting topology awareness for routing in LEO satellite constellations](http://arxiv.org/abs/2006.12242)


  Low Earth Orbit (LEO) satellite constellations combine great flexibility and
global coverage with short propagation delays when compared to satellites
deployed in higher orbits. However, the fast movement of the individual
satellites makes inter-satellite routing a complex and dynamic problem. In this
paper, we investigate the limits of unipath routing in a scenario where ground
stations (GSs) communicate with each other through a LEO constellation. For
this, we present a lightweight and topology-aware routing metric that favors
the selection of paths with high data rate inter-satellite links (ISLs).
Furthermore, we analyze the overall routing latency in terms of propagation,
transmission, and queueing times and calculate the maximum traffic load that
can be supported by the constellation. In our setup, the traffic is injected by
a network of GSs with real locations and is routed through adaptive multi-rate
inter-satellite links (ISLs). Our results illustrate the benefits of exploiting
the network topology, as the proposed metric can support up to 53% more traffic
when compared to the selected benchmarks, and consistently achieves the
shortest queueing times at the satellites and, ultimately, the shortest
end-to-end latency.

    

### [[2104.12147] Learning Aided Auctioning based Spectrum Access System in a Wireless Optical Network](http://arxiv.org/abs/2104.12147)


  This paper focusses on Service Level Agreement (SLA) based end-to-end Quality
of Service (QoS) maintenance across a wireless optical integrated network. We
use long term evolution (LTE) based spectrum access system (SAS) in the
wireless network and the optical network is comprised of an Ethernet Passive
Optical Network (EPON). The proposal targets a learning-based intelligent SAS
where opportunistic allocation of any available bandwidth is done after meeting
the SLA requirements. Such an opportunistic allocation is particularly
beneficial for nomadic users with varying QoS requirements. The opportunistic
allocation is carried out with the help of Vickrey-Clarke-Groves (VCG) auction.
The proposal allows the users of the integrated network to decide the payment
they want to make in order to opportunistically avail bandwidth. Learning
automata is used for the users to intelligently converge to the optimal payment
value based on the network load. The payment made by the users is later used by
the optical network units of the EPON to prepare the bids for the auction. The
proposal has been verified through extensive simulations.

    

### [[2112.00007] Sound-Guided Semantic Image Manipulation](http://arxiv.org/abs/2112.00007)


  The recent success of the generative model shows that leveraging the
multi-modal embedding space can manipulate an image using text information.
However, manipulating an image with other sources rather than text, such as
sound, is not easy due to the dynamic characteristics of the sources.
Especially, sound can convey vivid emotions and dynamic expressions of the real
world. Here, we propose a framework that directly encodes sound into the
multi-modal (image-text) embedding space and manipulates an image from the
space. Our audio encoder is trained to produce a latent representation from an
audio input, which is forced to be aligned with image and text representations
in the multi-modal embedding space. We use a direct latent optimization method
based on aligned embeddings for sound-guided image manipulation. We also show
that our method can mix text and audio modalities, which enrich the variety of
the image modification. We verify the effectiveness of our sound-guided image
manipulation quantitatively and qualitatively. We also show that our method can
mix different modalities, i.e., text and audio, which enrich the variety of the
image modification. The experiments on zero-shot audio classification and
semantic-level image classification show that our proposed model outperforms
other text and sound-guided state-of-the-art methods.

    

### [[2112.00016] Learning knot invariants across dimensions](http://arxiv.org/abs/2112.00016)


  We use deep neural networks to machine learn correlations between knot
invariants in various dimensions. The three-dimensional invariant of interest
is the Jones polynomial $J(q)$, and the four-dimensional invariants are the
Khovanov polynomial $\text{Kh}(q,t)$, smooth slice genus $g$, and Rasmussen's
$s$-invariant. We find that a two-layer feed-forward neural network can predict
$s$ from $\text{Kh}(q,-q^{-4})$ with greater than $99\%$ accuracy. A
theoretical explanation for this performance exists in knot theory via the now
disproven knight move conjecture, which is obeyed by all knots in our dataset.
More surprisingly, we find similar performance for the prediction of $s$ from
$\text{Kh}(q,-q^{-2})$, which suggests a novel relationship between the
Khovanov and Lee homology theories of a knot. The network predicts $g$ from
$\text{Kh}(q,t)$ with similarly high accuracy, and we discuss the extent to
which the machine is learning $s$ as opposed to $g$, since there is a general
inequality $|s| \leq 2g$. The Jones polynomial, as a three-dimensional
invariant, is not obviously related to $s$ or $g$, but the network achieves
greater than $95\%$ accuracy in predicting either from $J(q)$. Moreover,
similar accuracy can be achieved by evaluating $J(q)$ at roots of unity. This
suggests a relationship with $SU(2)$ Chern--Simons theory, and we review the
gauge theory construction of Khovanov homology which may be relevant for
explaining the network's performance.

    

### [[2112.00029] Pixelated Butterfly: Simple and Efficient Sparse training for Neural Network Models](http://arxiv.org/abs/2112.00029)


  Overparameterized neural networks generalize well but are expensive to train.
Ideally, one would like to reduce their computational cost while retaining
their generalization benefits. Sparse model training is a simple and promising
approach to achieve this, but there remain challenges as existing methods
struggle with accuracy loss, slow training runtime, or difficulty in
sparsifying all model components. The core problem is that searching for a
sparsity mask over a discrete set of sparse matrices is difficult and
expensive. To address this, our main insight is to optimize over a continuous
superset of sparse matrices with a fixed structure known as products of
butterfly matrices. As butterfly matrices are not hardware efficient, we
propose simple variants of butterfly (block and flat) to take advantage of
modern hardware. Our method (Pixelated Butterfly) uses a simple fixed sparsity
pattern based on flat block butterfly and low-rank matrices to sparsify most
network layers (e.g., attention, MLP). We empirically validate that Pixelated
Butterfly is 3x faster than butterfly and speeds up training to achieve
favorable accuracy--efficiency tradeoffs. On the ImageNet classification and
WikiText-103 language modeling tasks, our sparse models train up to 2.5x faster
than the dense MLP-Mixer, Vision Transformer, and GPT-2 medium with no drop in
accuracy.

    

### [[2112.00038] Robust and Provably Monotonic Networks](http://arxiv.org/abs/2112.00038)


  The Lipschitz constant of the map between the input and output space
represented by a neural network is a natural metric for assessing the
robustness of the model. We present a new method to constrain the Lipschitz
constant of dense deep learning models that can also be generalized to other
architectures. The method relies on a simple weight normalization scheme during
training that ensures the Lipschitz constant of every layer is below an upper
limit specified by the analyst. A simple residual connection can then be used
to make the model monotonic in any subset of its inputs, which is useful in
scenarios where domain knowledge dictates such dependence. Examples can be
found in algorithmic fairness requirements or, as presented here, in the
classification of the decays of subatomic particles produced at the CERN Large
Hadron Collider. Our normalization is minimally constraining and allows the
underlying architecture to maintain higher expressiveness compared to other
techniques which aim to either control the Lipschitz constant of the model or
ensure its monotonicity. We show how the algorithm was used to train a
powerful, robust, and interpretable discriminator for heavy-flavor decays in
the LHCb realtime data-processing system.

    

### [[2112.00054] Task2Sim : Towards Effective Pre-training and Transfer from Synthetic Data](http://arxiv.org/abs/2112.00054)


  Pre-training models on Imagenet or other massive datasets of real images has
led to major advances in computer vision, albeit accompanied with shortcomings
related to curation cost, privacy, usage rights, and ethical issues. In this
paper, for the first time, we study the transferability of pre-trained models
based on synthetic data generated by graphics simulators to downstream tasks
from very different domains. In using such synthetic data for pre-training, we
find that downstream performance on different tasks are favored by different
configurations of simulation parameters (e.g. lighting, object pose,
backgrounds, etc.), and that there is no one-size-fits-all solution. It is thus
better to tailor synthetic pre-training data to a specific downstream task, for
best performance. We introduce Task2Sim, a unified model mapping downstream
task representations to optimal simulation parameters to generate synthetic
pre-training data for them. Task2Sim learns this mapping by training to find
the set of best parameters on a set of "seen" tasks. Once trained, it can then
be used to predict best simulation parameters for novel "unseen" tasks in one
shot, without requiring additional training. Given a budget in number of images
per class, our extensive experiments with 20 diverse downstream tasks show
Task2Sim's task-adaptive pre-training data results in significantly better
downstream performance than non-adaptively choosing simulation parameters on
both seen and unseen tasks. It is even competitive with pre-training on real
images from Imagenet.

    

### [[2112.00059] Evaluating Gradient Inversion Attacks and Defenses in Federated Learning](http://arxiv.org/abs/2112.00059)


  Gradient inversion attack (or input recovery from gradient) is an emerging
threat to the security and privacy preservation of Federated learning, whereby
malicious eavesdroppers or participants in the protocol can recover (partially)
the clients' private data. This paper evaluates existing attacks and defenses.
We find that some attacks make strong assumptions about the setup. Relaxing
such assumptions can substantially weaken these attacks. We then evaluate the
benefits of three proposed defense mechanisms against gradient inversion
attacks. We show the trade-offs of privacy leakage and data utility of these
defense methods, and find that combining them in an appropriate manner makes
the attack less effective, even under the original strong assumptions. We also
estimate the computation cost of end-to-end recovery of a single image under
each evaluated defense. Our findings suggest that the state-of-the-art attacks
can currently be defended against with minor data utility loss, as summarized
in a list of potential strategies. Our code is available at:
this https URL.

    

### [[2112.00061] Open-Domain, Content-based, Multi-modal Fact-checking of Out-of-Context Images via Online Resources](http://arxiv.org/abs/2112.00061)


  Misinformation is now a major problem due to its potential high risks to our
core democratic and societal values and orders. Out-of-context misinformation
is one of the easiest and effective ways used by adversaries to spread viral
false stories. In this threat, a real image is re-purposed to support other
narratives by misrepresenting its context and/or elements. The internet is
being used as the go-to way to verify information using different sources and
modalities. Our goal is an inspectable method that automates this
time-consuming and reasoning-intensive process by fact-checking the
image-caption pairing using Web evidence. To integrate evidence and cues from
both modalities, we introduce the concept of 'multi-modal cycle-consistency
check'; starting from the image/caption, we gather textual/visual evidence,
which will be compared against the other paired caption/image, respectively.
Moreover, we propose a novel architecture, Consistency-Checking Network (CCN),
that mimics the layered human reasoning across the same and different
modalities: the caption vs. textual evidence, the image vs. visual evidence,
and the image vs. caption. Our work offers the first step and benchmark for
open-domain, content-based, multi-modal fact-checking, and significantly
outperforms previous baselines that did not leverage external evidence.

    

### [[2112.00065] Boosting EfficientNets Ensemble Performance via Pseudo-Labels and Synthetic Images by pix2pixHD for Infection and Ischaemia Classification in Diabetic Foot Ulcers](http://arxiv.org/abs/2112.00065)


  Diabetic foot ulcers are a common manifestation of lesions on the diabetic
foot, a syndrome acquired as a long-term complication of diabetes mellitus.
Accompanying neuropathy and vascular damage promote acquisition of pressure
injuries and tissue death due to ischaemia. Affected areas are prone to
infections, hindering the healing progress. The research at hand investigates
an approach on classification of infection and ischaemia, conducted as part of
the Diabetic Foot Ulcer Challenge (DFUC) 2021. Different models of the
EfficientNet family are utilized in ensembles. An extension strategy for the
training data is applied, involving pseudo-labeling for unlabeled images, and
extensive generation of synthetic images via pix2pixHD to cope with severe
class imbalances. The resulting extended training dataset features $8.68$ times
the size of the baseline and shows a real to synthetic image ratio of $1:3$.
Performances of models and ensembles trained on the baseline and extended
training dataset are compared. Synthetic images featured a broad qualitative
variety. Results show that models trained on the extended training dataset as
well as their ensemble benefit from the large extension. F1-Scores for rare
classes receive outstanding boosts, while those for common classes are either
not harmed or boosted moderately. A critical discussion concretizes benefits
and identifies limitations, suggesting improvements. The work concludes that
classification performance of individual models as well as that of ensembles
can be boosted utilizing synthetic images. Especially performance for rare
classes benefits notably.

    

### [[2112.00071] What to Learn, and How: Toward Effective Learning from Rationales](http://arxiv.org/abs/2112.00071)


  Learning from rationales seeks to augment model training with human-provided
rationales (i.e., a subset of input tokens) that justify those labels. While
intuitive, this idea has proven elusive in practice. We make two observations
about human rationales via empirical analyses: 1) maximizing predicted
rationale accuracy is not necessarily the optimal objective for improving model
performance; 2) human rationales vary in whether they provide sufficient
information for the model to exploit for prediction, and we can use this
variance to assess a dataset's potential improvement from learning from
rationales. Building on these insights, we propose loss functions and learning
strategies, and evaluate their effectiveness on three datasets with human
rationales. Our results demonstrate consistent improvements over baselines in
both label performance and rationale performance, including a 3% accuracy
improvement on MultiRC. Our work highlights the importance of understanding
properties of human explanations and exploiting them accordingly in model
training.

    

### [[2112.00075] A Multi-purposed Unsupervised Framework for Comparing Embeddings of Undirected and Directed Graphs](http://arxiv.org/abs/2112.00075)


  Graph embedding is a transformation of nodes of a network into a set of
vectors. A good embedding should capture the underlying graph topology and
structure, node-to-node relationship, and other relevant information about the
graph, its subgraphs, and nodes themselves. If these objectives are achieved,
an embedding is a meaningful, understandable, and often compressed
representation of a network. Unfortunately, selecting the best embedding is a
challenging task and very often requires domain experts. In this paper, we
extend the framework for evaluating graph embeddings that was recently
introduced by the authors. Now, the framework assigns two scores, local and
global, to each embedding that measure the quality of an evaluated embedding
for tasks that require good representation of local and, respectively, global
properties of the network. The best embedding, if needed, can be selected in an
unsupervised way, or the framework can identify a few embeddings that are worth
further investigation. The framework is flexible, scalable, and can deal with
undirected/directed, weighted/unweighted graphs.

    

### [[2112.00094] Leveraging Intrinsic Gradient Information for Machine Learning Model Training](http://arxiv.org/abs/2112.00094)


  Designing models that produce accurate predictions is the fundamental
objective of machine learning. This work presents methods demonstrating that
when the derivatives of target variables with respect to inputs can be
extracted from processes of interest, they can be leveraged to improve the
accuracy of differentiable machine learning models. Four key ideas are
explored: (1) Improving the predictive accuracy of linear regression models and
feed-forward neural networks (NNs); (2) Using the difference between the
performance of feedforward NNs trained with and without gradient information to
tune NN complexity (in the form of hidden node number); (3) Using gradient
information to regularise linear regression; and (4) Using gradient information
to improve generative image models. Across this variety of applications,
gradient information is shown to enhance each predictive model, demonstrating
its value for a variety of applications.

    

### [[2112.00101] Fast Topological Clustering with Wasserstein Distance](http://arxiv.org/abs/2112.00101)


  The topological patterns exhibited by many real-world networks motivate the
development of topology-based methods for assessing the similarity of networks.
However, extracting topological structure is difficult, especially for large
and dense networks whose node degrees range over multiple orders of magnitude.
In this paper, we propose a novel and computationally practical topological
clustering method that clusters complex networks with intricate topology using
principled theory from persistent homology and optimal transport. Such networks
are aggregated into clusters through a centroid-based clustering strategy based
on both their topological and geometric structure, preserving correspondence
between nodes in different networks. The notions of topological proximity and
centroid are characterized using a novel and efficient approach to computation
of the Wasserstein distance and barycenter for persistence barcodes associated
with connected components and cycles. The proposed method is demonstrated to be
effective using both simulated networks and measured functional brain networks.

    

### [[2112.00114] Show Your Work: Scratchpads for Intermediate Computation with Language Models](http://arxiv.org/abs/2112.00114)


  Large pre-trained language models perform remarkably well on tasks that can
be done "in one pass", such as generating realistic text or synthesizing
computer programs. However, they struggle with tasks that require unbounded
multi-step computation, such as adding integers or executing programs.
Surprisingly, we find that these same models are able to perform complex
multi-step computations -- even in the few-shot regime -- when asked to perform
the operation "step by step", showing the results of intermediate computations.
In particular, we train transformers to perform multi-step computations by
asking them to emit intermediate computation steps into a "scratchpad". On a
series of increasingly complex tasks ranging from long addition to the
execution of arbitrary programs, we show that scratchpads dramatically improve
the ability of language models to perform multi-step computations.

    

### [[2112.00131] CovidAlert -- A Wristwatch-based System to Alert Users from Face Touching](http://arxiv.org/abs/2112.00131)


  Worldwide 2019 million people have been infected and 4.5 million have lost
their lives in the ongoing Covid-19 pandemic. Until vaccines became widely
available, precautions and safety measures like wearing masks, physical
distancing, avoiding face touching were some of the primary means to curb the
spread of virus. Face touching is a compulsive human begavior that can not be
prevented without making a continuous consious effort, even then it is
inevitable. To address this problem, we have designed a smartwatch-based
solution, CovidAlert, that leverages Random Forest algorithm trained on
accelerometer and gyroscope data from the smartwatch to detects hand transition
to face and sends a quick haptic alert to the users. CovidALert is highly
energy efficient as it employs STA/LTA algorithm as a gatekeeper to curtail the
usage of Random Forest model on the watch when user is inactive. The overall
accuracy of our system is 88.4% with low false negatives and false positives.
We also demonstrated the system viability by implementing it on a commercial
Fossil Gen 5 smartwatch.

    

### [[2112.00133] PokeBNN: A Binary Pursuit of Lightweight Accuracy](http://arxiv.org/abs/2112.00133)


  Top-1 ImageNet optimization promotes enormous networks that may be
impractical in inference settings. Binary neural networks (BNNs) have the
potential to significantly lower the compute intensity but existing models
suffer from low quality. To overcome this deficiency, we propose PokeConv, a
binary convolution block which improves quality of BNNs by techniques such as
adding multiple residual paths, and tuning the activation function. We apply it
to ResNet-50 and optimize ResNet's initial convolutional layer which is hard to
binarize. We name the resulting network family PokeBNN. These techniques are
chosen to yield favorable improvements in both top-1 accuracy and the network's
cost. In order to enable joint optimization of the cost together with accuracy,
we define arithmetic computation effort (ACE), a hardware- and energy-inspired
cost metric for quantized and binarized networks. We also identify a need to
optimize an under-explored hyper-parameter controlling the binarization
gradient approximation.
We establish a new, strong state-of-the-art (SOTA) on top-1 accuracy together
with commonly-used CPU64 cost, ACE cost and network size metrics.
ReActNet-Adam, the previous SOTA in BNNs, achieved a 70.5% top-1 accuracy with
7.9 ACE. A small variant of PokeBNN achieves 70.5% top-1 with 2.6 ACE, more
than 3x reduction in cost; a larger PokeBNN achieves 75.6% top-1 with 7.8 ACE,
more than 5% improvement in accuracy without increasing the cost. PokeBNN
implementation in JAX/Flax and reproduction instructions are open sourced.

    

### [[2112.00141] Solving reward-collecting problems with UAVs: a comparison of online optimization and Q-learning](http://arxiv.org/abs/2112.00141)


  Uncrewed autonomous vehicles (UAVs) have made significant contributions to
reconnaissance and surveillance missions in past US military campaigns. As the
prevalence of UAVs increases, there has also been improvements in counter-UAV
technology that makes it difficult for them to successfully obtain valuable
intelligence within an area of interest. Hence, it has become important that
modern UAVs can accomplish their missions while maximizing their chances of
survival. In this work, we specifically study the problem of identifying a
short path from a designated start to a goal, while collecting all rewards and
avoiding adversaries that move randomly on the grid. We also provide a possible
application of the framework in a military setting, that of autonomous casualty
evacuation. We present a comparison of three methods to solve this problem:
namely we implement a Deep Q-Learning model, an $\varepsilon$-greedy tabular
Q-Learning model, and an online optimization framework. Our computational
experiments, designed using simple grid-world environments with random
adversaries showcase how these approaches work and compare them in terms of
performance, accuracy, and computational time.

    

### [[2112.00171] Improving Differentiable Architecture Search with a Generative Model](http://arxiv.org/abs/2112.00171)


  In differentiable neural architecture search (NAS) algorithms like DARTS, the
training set used to update model weight and the validation set used to update
model architectures are sampled from the same data distribution. Thus, the
uncommon features in the dataset fail to receive enough attention during
training. In this paper, instead of introducing more complex NAS algorithms, we
explore the idea that adding quality synthesized datasets into training can
help the classification model identify its weakness and improve recognition
accuracy. We introduce a training strategy called ``Differentiable Architecture
Search with a Generative Model(DASGM)." In DASGM, the training set is used to
update the classification model weight, while a synthesized dataset is used to
train its architecture. The generated images have different distributions from
the training set, which can help the classification model learn better features
to identify its weakness. We formulate DASGM into a multi-level optimization
framework and develop an effective algorithm to solve it. Experiments on
CIFAR-10, CIFAR-100, and ImageNet have demonstrated the effectiveness of DASGM.
Code will be made available.

    

### [[2112.00174] Adaptive Optimization with Examplewise Gradients](http://arxiv.org/abs/2112.00174)


  We propose a new, more general approach to the design of stochastic
gradient-based optimization methods for machine learning. In this new
framework, optimizers assume access to a batch of gradient estimates per
iteration, rather than a single estimate. This better reflects the information
that is actually available in typical machine learning setups. To demonstrate
the usefulness of this generalized approach, we develop Eve, an adaptation of
the Adam optimizer which uses examplewise gradients to obtain more accurate
second-moment estimates. We provide preliminary experiments, without
hyperparameter tuning, which show that the new optimizer slightly outperforms
Adam on a small scale benchmark and performs the same or worse on larger scale
benchmarks. Further work is needed to refine the algorithm and tune
hyperparameters.

    

### [[2112.00179] A collection of the accepted abstracts for the Machine Learning for Health (ML4H) symposium 2021](http://arxiv.org/abs/2112.00179)


  A collection of the accepted abstracts for the Machine Learning for Health
(ML4H) symposium 2021. This index is not complete, as some accepted abstracts
chose to opt-out of inclusion.

    

### [[2112.00190] Is the use of Deep Learning and Artificial Intelligence an appropriate means to locate debris in the ocean without harming aquatic wildlife?](http://arxiv.org/abs/2112.00190)


  With the global issue of plastic debris ever expanding, it is about time that
the technology industry stepped in. This study aims to assess whether deep
learning can successfully distinguish between marine life and man-made debris
underwater. The aim is to find if we are safely able to clean up our oceans
with Artificial Intelligence without disrupting the delicate balance of the
aquatic ecosystems. The research explores the use of Convolutional Neural
Networks from the perspective of protecting the ecosystem, rather than
primarily collecting rubbish. We did this by building a custom-built, deep
learning model, with an original database including 1,644 underwater images and
used a binary classification to sort synthesised material from aquatic life. We
concluded that although it is possible to safely distinguish between debris and
life, further exploration with a larger database and stronger CNN structure has
the potential for much more promising results.

    

### [[2112.00193] Public Data-Assisted Mirror Descent for Private Model Training](http://arxiv.org/abs/2112.00193)


  We revisit the problem of using public data to improve the privacy/utility
trade-offs for differentially private (DP) model training. Here, public data
refers to auxiliary data sets that have no privacy concerns. We consider public
data that is from the same distribution as the private training data.
For convex losses, we show that a variant of Mirror Descent provides
population risk guarantees which are independent of the dimension of the model
($p$). Specifically, we apply Mirror Descent with the loss generated by the
public data as the mirror map, and using DP gradients of the loss generated by
the private (sensitive) data. To obtain dimension independence, we require
$G_Q^2 \leq p$ public data samples, where $G_Q$ is a measure of the isotropy of
the loss function. We further show that our algorithm has a natural ``noise
stability'' property: If around the current iterate the public loss satisfies
$\alpha_v$-strong convexity in a direction $v$, then using noisy gradients
instead of the exact gradients shifts our next iterate in the direction $v$ by
an amount proportional to $1/\alpha_v$ (in contrast with DP-SGD, where the
shift is isotropic). Analogous results in prior works had to explicitly learn
the geometry using the public data in the form of preconditioner matrices. Our
method is also applicable to non-convex losses, as it does not rely on
convexity assumptions to ensure DP guarantees.
We demonstrate the empirical efficacy of our algorithm by showing
privacy/utility trade-offs on linear regression, deep learning benchmark
datasets (WikiText-2, CIFAR-10, and EMNIST), and in federated learning
(StackOverflow). We show that our algorithm not only significantly improves
over traditional DP-SGD and DP-FedAvg, which do not have access to public data,
but also improves over DP-SGD and DP-FedAvg on models that have been
pre-trained with the public data to begin with.

    

### [[2112.00195] Efficient Online Bayesian Inference for Neural Bandits](http://arxiv.org/abs/2112.00195)


  In this paper we present a new algorithm for online (sequential) inference in
Bayesian neural networks, and show its suitability for tackling contextual
bandit problems. The key idea is to combine the extended Kalman filter (which
locally linearizes the likelihood function at each time step) with a (learned
or random) low-dimensional affine subspace for the parameters; the use of a
subspace enables us to scale our algorithm to models with $\sim 1M$ parameters.
While most other neural bandit methods need to store the entire past dataset in
order to avoid the problem of "catastrophic forgetting", our approach uses
constant memory. This is possible because we represent uncertainty about all
the parameters in the model, not just the final linear layer. We show good
results on the "Deep Bayesian Bandit Showdown" benchmark, as well as MNIST and
a recommender system.

    

### [[2112.00207] Improved sparse PCA method for face and image recognition](http://arxiv.org/abs/2112.00207)


  Face recognition is the very significant field in pattern recognition area.
It has multiple applications in military and finance, to name a few. In this
paper, the combination of the sparse PCA with the nearest-neighbor method (and
with the kernel ridge regression method) will be proposed and will be applied
to solve the face recognition problem. Experimental results illustrate that the
accuracy of the combination of the sparse PCA method (using the proximal
gradient method and the FISTA method) and one specific classification system
may be lower than the accuracy of the combination of the PCA method and one
specific classification system but sometimes the combination of the sparse PCA
method (using the proximal gradient method or the FISTA method) and one
specific classification system leads to better accuracy. Moreover, we recognize
that the process computing the sparse PCA algorithm using the FISTA method is
always faster than the process computing the sparse PCA algorithm using the
proximal gradient method.

    

### [[2112.00209] Environmental Sound Extraction Using Onomatopoeia](http://arxiv.org/abs/2112.00209)


  Onomatopoeia, which is a character sequence that phonetically imitates a
sound, is effective in expressing characteristics of sound such as duration,
pitch, and timbre. We propose an environmental-sound-extraction method using
onomatopoeia to specify the target sound to be extracted. With this method, we
estimate a time-frequency mask from an input mixture spectrogram and
onomatopoeia by using U-Net architecture then extract the corresponding target
sound by masking the spectrogram. Experimental results indicate that the
proposed method can extract only the target sound corresponding to onomatopoeia
and performs better than conventional methods that use sound-event classes to
specify the target sound.

    

### [[2112.00220] A generic physics-informed neural network-based framework for reliability assessment of multi-state systems](http://arxiv.org/abs/2112.00220)


  In this paper, we leverage the recent advances in physics-informed neural
network (PINN) and develop a generic PINN-based framework to assess the
reliability of multi-state systems (MSSs). The proposed methodology consists of
two major steps. In the first step, we recast the reliability assessment of MSS
as a machine learning problem using the framework of PINN. A feedforward neural
network with two individual loss groups are constructed to encode the initial
condition and state transitions governed by ordinary differential equations
(ODEs) in MSS. Next, we tackle the problem of high imbalance in the magnitude
of the back-propagated gradients in PINN from a multi-task learning
perspective. Particularly, we treat each element in the loss function as an
individual task, and adopt a gradient surgery approach named projecting
conflicting gradients (PCGrad), where a task's gradient is projected onto the
norm plane of any other task that has a conflicting gradient. The gradient
projection operation significantly mitigates the detrimental effects caused by
the gradient interference when training PINN, thus accelerating the convergence
speed of PINN to high-precision solutions to MSS reliability assessment. With
the proposed PINN-based framework, we investigate its applications for MSS
reliability assessment in several different contexts in terms of
time-independent or dependent state transitions and system scales varying from
small to medium. The results demonstrate that the proposed PINN-based framework
shows generic and remarkable performance in MSS reliability assessment, and the
incorporation of PCGrad in PINN leads to substantial improvement in solution
quality and convergence speed.

    

### [[2112.00222] Convergence of GANs Training: A Game and Stochastic Control Methodology](http://arxiv.org/abs/2112.00222)


  Training of generative adversarial networks (GANs) is known for its
difficulty to converge. This paper first confirms analytically one of the
culprits behind this convergence issue: the lack of convexity in GANs objective
functions, hence the well-posedness problem of GANs models. Then, it proposes a
stochastic control approach for hyper-parameters tuning in GANs training. In
particular, it presents an optimal solution for adaptive learning rate which
depends on the convexity of the objective function, and builds a precise
relation between improper choices of learning rate and explosion in GANs
training. Finally, empirical studies demonstrate that training algorithms
incorporating this selection methodology outperform standard ones.

    

### [[2112.00227] A Machine Learning Analysis of COVID-19 Mental Health Data](http://arxiv.org/abs/2112.00227)


  In late December 2019, the novel coronavirus (Sars-Cov-2) and the resulting
disease COVID-19 were first identified in Wuhan China. The disease slipped
through containment measures, with the first known case in the United States
being identified on January 20th, 2020. In this paper, we utilize survey data
from the Inter-university Consortium for Political and Social Research and
apply several statistical and machine learning models and techniques such as
Decision Trees, Multinomial Logistic Regression, Naive Bayes, k-Nearest
Neighbors, Support Vector Machines, Neural Networks, Random Forests, Gradient
Tree Boosting, XGBoost, CatBoost, LightGBM, Synthetic Minority Oversampling,
and Chi-Squared Test to analyze the impacts the COVID-19 pandemic has had on
the mental health of frontline workers in the United States. Through the
interpretation of the many models applied to the mental health survey data, we
have concluded that the most important factor in predicting the mental health
decline of a frontline worker is the healthcare role the individual is in
(Nurse, Emergency Room Staff, Surgeon, etc.), followed by the amount of sleep
the individual has had in the last week, the amount of COVID-19 related news an
individual has consumed on average in a day, the age of the worker, and the
usage of alcohol and cannabis.

    

### [[2112.00236] VoRTX: Volumetric 3D Reconstruction With Transformers for Voxelwise View Selection and Fusion](http://arxiv.org/abs/2112.00236)


  Recent volumetric 3D reconstruction methods can produce very accurate
results, with plausible geometry even for unobserved surfaces. However, they
face an undesirable trade-off when it comes to multi-view fusion. They can fuse
all available view information by global averaging, thus losing fine detail, or
they can heuristically cluster views for local fusion, thus restricting their
ability to consider all views jointly. Our key insight is that greater detail
can be retained without restricting view diversity by learning a view-fusion
function conditioned on camera pose and image content. We propose to learn this
multi-view fusion using a transformer. To this end, we introduce VoRTX, an
end-to-end volumetric 3D reconstruction network using transformers for
wide-baseline, multi-view feature fusion. Our model is occlusion-aware,
leveraging the transformer architecture to predict an initial, projective scene
geometry estimate. This estimate is used to avoid backprojecting image features
through surfaces into occluded regions. We train our model on ScanNet and show
that it produces better reconstructions than state-of-the-art methods. We also
demonstrate generalization without any fine-tuning, outperforming the same
state-of-the-art methods on two other datasets, TUM-RGBD and ICL-NUIM.

    

### [[2112.00238] Imbalanced Graph Classification via Graph-of-Graph Neural Networks](http://arxiv.org/abs/2112.00238)


  Graph Neural Networks (GNNs) have achieved unprecedented success in learning
graph representations to identify categorical labels of graphs. However, most
existing graph classification problems with GNNs follow a balanced data
splitting protocol, which is misaligned with many real-world scenarios in which
some classes have much fewer labels than others. Directly training GNNs under
this imbalanced situation may lead to uninformative representations of graphs
in minority classes, and compromise the overall performance of downstream
classification, which signifies the importance of developing effective GNNs for
handling imbalanced graph classification. Existing methods are either tailored
for non-graph structured data or designed specifically for imbalance node
classification while few focus on imbalance graph classification. To this end,
we introduce a novel framework, Graph-of-Graph Neural Networks (G$^2$GNN),
which alleviates the graph imbalance issue by deriving extra supervision
globally from neighboring graphs and locally from graphs themselves. Globally,
we construct a graph of graphs (GoG) based on kernel similarity and perform GoG
propagation to aggregate neighboring graph representations, which are initially
obtained by node-level propagation with pooling via a GNN encoder. Locally, we
employ topological augmentation via masking nodes or dropping edges to improve
the model generalizability in discerning topology of unseen testing graphs.
Extensive graph classification experiments conducted on seven benchmark
datasets demonstrate our proposed G$^2$GNN outperforms numerous baselines by
roughly 5\% in both F1-macro and F1-micro scores. The implementation of
G$^2$GNN is available at
\href{this https URL}{this https URL}.

    

### [[2112.00260] Ranking Distance Calibration for Cross-Domain Few-Shot Learning](http://arxiv.org/abs/2112.00260)


  Recent progress in few-shot learning promotes a more realistic cross-domain
setting, where the source and target datasets are from different domains. Due
to the domain gap and disjoint label spaces between source and target datasets,
their shared knowledge is extremely limited. This encourages us to explore more
information in the target domain rather than to overly elaborate training
strategies on the source domain as in many existing methods. Hence, we start
from a generic representation pre-trained by a cross-entropy loss and a
conventional distance-based classifier, along with an image retrieval view, to
employ a re-ranking process for calibrating a target distance matrix by
discovering the reciprocal k-nearest neighbours within the task. Assuming the
pre-trained representation is biased towards the source, we construct a
non-linear subspace to minimise task-irrelevant features therewithin while keep
more transferrable discriminative information by a hyperbolic tangent
transformation. The calibrated distance in this target-aware non-linear
subspace is complementary to that in the pre-trained representation. To impose
such distance calibration information onto the pre-trained representation, a
Kullback-Leibler divergence loss is employed to gradually guide the model
towards the calibrated distance-based distribution. Extensive evaluations on
eight target domains show that this target ranking calibration process can
improve conventional distance-based classifiers in few-shot learning.

    

### [[2112.00265] Training BatchNorm Only in Neural Architecture Search and Beyond](http://arxiv.org/abs/2112.00265)


  This work investigates the usage of batch normalization in neural
architecture search (NAS). Specifically, Frankle et al. find that training
BatchNorm only can achieve nontrivial performance. Furthermore, Chen et al.
claim that training BatchNorm only can speed up the training of the one-shot
NAS supernet over ten times. Critically, there is no effort to understand 1)
why training BatchNorm only can find the perform-well architectures with the
reduced supernet-training time, and 2) what is the difference between the
train-BN-only supernet and the standard-train supernet. We begin by showing
that the train-BN-only networks converge to the neural tangent kernel regime,
obtain the same training dynamics as train all parameters theoretically. Our
proof supports the claim to train BatchNorm only on supernet with less training
time. Then, we empirically disclose that train-BN-only supernet provides an
advantage on convolutions over other operators, cause unfair competition
between architectures. This is due to only the convolution operator being
attached with BatchNorm. Through experiments, we show that such unfairness
makes the search algorithm prone to select models with convolutions. To solve
this issue, we introduce fairness in the search space by placing a BatchNorm
layer on every operator. However, we observe that the performance predictor in
Chen et al. is inapplicable on the new search space. To this end, we propose a
novel composite performance indicator to evaluate networks from three
perspectives: expressivity, trainability, and uncertainty, derived from the
theoretical property of BatchNorm. We demonstrate the effectiveness of our
approach on multiple NAS-benchmarks (NAS-Bench101, NAS-Bench-201) and search
spaces (DARTS search space and MobileNet search space).

    

### [[2112.00275] Learning from Mistakes based on Class Weighting with Application to Neural Architecture Search](http://arxiv.org/abs/2112.00275)


  Learning from mistakes is an effective learning approach widely used in human
learning, where a learner pays greater focus on mistakes to circumvent them in
the future. It aids in improving the overall learning outcomes. In this work,
we aim to investigate how effectively this exceptional learning ability can be
used to improve machine learning models as well. We propose a simple and
effective multi-level optimization framework called learning from mistakes
(LFM), inspired by mistake-driven learning to train better machine learning
models. Our LFM framework consists of a formulation involving three learning
stages. The primary objective is to train a model to perform effectively on
target tasks by using a re-weighting technique to prevent similar mistakes in
the future. In this formulation, we learn the class weights by minimizing the
validation loss of the model and re-train the model with the synthetic data
from the image generator weighted by class-wise performance and real data. We
apply our LFM framework for differential architecture search methods on image
classification datasets such as CIFAR and ImageNet, where the results
demonstrate the effectiveness of our proposed strategy.

    

### [[2112.00298] Exploring Social Posterior Collapse in Variational Autoencoder for Interaction Modeling](http://arxiv.org/abs/2112.00298)


  Multi-agent behavior modeling and trajectory forecasting are crucial for the
safe navigation of autonomous agents in interactive scenarios. Variational
Autoencoder (VAE) has been widely applied in multi-agent interaction modeling
to generate diverse behavior and learn a low-dimensional representation for
interacting systems. However, existing literature did not formally discuss if a
VAE-based model can properly encode interaction into its latent space. In this
work, we argue that one of the typical formulations of VAEs in multi-agent
modeling suffers from an issue we refer to as social posterior collapse, i.e.,
the model is prone to ignoring historical social context when predicting the
future trajectory of an agent. It could cause significant prediction errors and
poor generalization performance. We analyze the reason behind this
under-explored phenomenon and propose several measures to tackle it. Afterward,
we implement the proposed framework and experiment on real-world datasets for
multi-agent trajectory prediction. In particular, we propose a novel sparse
graph attention message-passing (sparse-GAMP) layer, which helps us detect
social posterior collapse in our experiments. In the experiments, we verify
that social posterior collapse indeed occurs. Also, the proposed measures are
effective in alleviating the issue. As a result, the model attains better
generalization performance when historical social context is informative for
prediction.

    

### [[2112.00305] Forward Operator Estimation in Generative Models with Kernel Transfer Operators](http://arxiv.org/abs/2112.00305)


  Generative models which use explicit density modeling (e.g., variational
autoencoders, flow-based generative models) involve finding a mapping from a
known distribution, e.g. Gaussian, to the unknown input distribution. This
often requires searching over a class of non-linear functions (e.g.,
representable by a deep neural network). While effective in practice, the
associated runtime/memory costs can increase rapidly, usually as a function of
the performance desired in an application. We propose a much cheaper (and
simpler) strategy to estimate this mapping based on adapting known results in
kernel transfer operators. We show that our formulation enables highly
efficient distribution approximation and sampling, and offers surprisingly good
empirical performance that compares favorably with powerful baselines, but with
significant runtime savings. We show that the algorithm also performs well in
small sample size settings (in brain imaging).

    

### [[2112.00313] Discriminating Quantum States with Quantum Machine Learning](http://arxiv.org/abs/2112.00313)


  Quantum machine learning (QML) algorithms have obtained great relevance in
the machine learning (ML) field due to the promise of quantum speedups when
performing basic linear algebra subroutines (BLAS), a fundamental element in
most ML algorithms. By making use of BLAS operations, we propose, implement and
analyze a quantum k-means (qk-means) algorithm with a low time complexity of
$\mathcal{O}(NKlog(D)I/C)$ to apply it to the fundamental problem of
discriminating quantum states at readout. Discriminating quantum states allows
the identification of quantum states $|0\rangle$ and $|1\rangle$ from low-level
in-phase and quadrature signal (IQ) data, and can be done using custom ML
models. In order to reduce dependency on a classical computer, we use the
qk-means to perform state discrimination on the IBMQ Bogota device and managed
to find assignment fidelities of up to 98.7% that were only marginally lower
than that of the k-means algorithm. Inspection of assignment fidelity scores
resulting from applying both algorithms to a combination of quantum states
showed concordance to our correlation analysis using Pearson Correlation
coefficients, where evidence shows cross-talk in the (1, 2) and (2, 3)
neighboring qubit couples for the analyzed device.

    

### [[2112.00314] Asymmetric error control under imperfect supervision: a label-noise-adjusted Neyman-Pearson umbrella algorithm](http://arxiv.org/abs/2112.00314)


  Label noise in data has long been an important problem in supervised learning
applications as it affects the effectiveness of many widely used classification
methods. Recently, important real-world applications, such as medical diagnosis
and cybersecurity, have generated renewed interest in the Neyman-Pearson (NP)
classification paradigm, which constrains the more severe type of error (e.g.,
the type I error) under a preferred level while minimizing the other (e.g., the
type II error). However, there has been little research on the NP paradigm
under label noise. It is somewhat surprising that even when common NP
classifiers ignore the label noise in the training stage, they are still able
to control the type I error with high probability. However, the price they pay
is excessive conservativeness of the type I error and hence a significant drop
in power (i.e., $1 - $ type II error). Assuming that domain experts provide
lower bounds on the corruption severity, we propose the first theory-backed
algorithm that adapts most state-of-the-art classification methods to the
training label noise under the NP paradigm. The resulting classifiers not only
control the type I error with high probability under the desired level but also
improve power.

    

### [[2112.00319] Object-Aware Cropping for Self-Supervised Learning](http://arxiv.org/abs/2112.00319)


  A core component of the recent success of self-supervised learning is
cropping data augmentation, which selects sub-regions of an image to be used as
positive views in the self-supervised loss. The underlying assumption is that
randomly cropped and resized regions of a given image share information about
the objects of interest, which the learned representation will capture. This
assumption is mostly satisfied in datasets such as ImageNet where there is a
large, centered object, which is highly likely to be present in random crops of
the full image. However, in other datasets such as OpenImages or COCO, which
are more representative of real world uncurated data, there are typically
multiple small objects in an image. In this work, we show that self-supervised
learning based on the usual random cropping performs poorly on such datasets.
We propose replacing one or both of the random crops with crops obtained from
an object proposal algorithm. This encourages the model to learn both object
and scene level semantic representations. Using this approach, which we call
object-aware cropping, results in significant improvements over scene cropping
on classification and object detection benchmarks. For example, on OpenImages,
our approach achieves an improvement of 8.8% mAP over random scene-level
cropping using MoCo-v2 based pre-training. We also show significant
improvements on COCO and PASCAL-VOC object detection and segmentation tasks
over the state-of-the-art self-supervised learning approaches. Our approach is
efficient, simple and general, and can be used in most existing contrastive and
non-contrastive self-supervised learning frameworks.

    

### [[2112.00324] Optimizing for In-memory Deep Learning with Emerging Memory Technology](http://arxiv.org/abs/2112.00324)


  In-memory deep learning computes neural network models where they are stored,
thus avoiding long distance communication between memory and computation units,
resulting in considerable savings in energy and time. In-memory deep learning
has already demonstrated orders of magnitude higher performance density and
energy efficiency. The use of emerging memory technology promises to increase
the gains in density, energy, and performance even further. However, emerging
memory technology is intrinsically unstable, resulting in random fluctuations
of data reads. This can translate to non-negligible accuracy loss, potentially
nullifying the gains. In this paper, we propose three optimization techniques
that can mathematically overcome the instability problem of emerging memory
technology. They can improve the accuracy of the in-memory deep learning model
while maximizing its energy efficiency. Experiments show that our solution can
fully recover most models' state-of-the-art accuracy, and achieves at least an
order of magnitude higher energy efficiency than the state-of-the-art.

    

### [[2112.00328] A Daily Tourism Demand Prediction Framework Based on Multi-head Attention CNN: The Case of The Foreign Entrant in South Korea](http://arxiv.org/abs/2112.00328)


  Developing an accurate tourism forecasting model is essential for making
desirable policy decisions for tourism management. Early studies on tourism
management focus on discovering external factors related to tourism demand.
Recent studies utilize deep learning in demand forecasting along with these
external factors. They mainly use recursive neural network models such as LSTM
and RNN for their frameworks. However, these models are not suitable for use in
forecasting tourism demand. This is because tourism demand is strongly affected
by changes in various external factors, and recursive neural network models
have limitations in handling these multivariate inputs. We propose a multi-head
attention CNN model (MHAC) for addressing these limitations. The MHAC uses
1D-convolutional neural network to analyze temporal patterns and the attention
mechanism to reflect correlations between input variables. This model makes it
possible to extract spatiotemporal characteristics from time-series data of
various variables. We apply our forecasting framework to predict inbound
tourist changes in South Korea by considering external factors such as
politics, disease, season, and attraction of Korean culture. The performance
results of extensive experiments show that our method outperforms other
deep-learning-based prediction frameworks in South Korea tourism forecasting.

    

### [[2112.00333] Joint Cluster Head Selection and Trajectory Planning in UAV-Aided IoT Networks by Reinforcement Learning with Sequential Model](http://arxiv.org/abs/2112.00333)


  Employing unmanned aerial vehicles (UAVs) has attracted growing interests and
emerged as the state-of-the-art technology for data collection in
Internet-of-Things (IoT) networks. In this paper, with the objective of
minimizing the total energy consumption of the UAV-IoT system, we formulate the
problem of jointly designing the UAV's trajectory and selecting cluster heads
in the IoT network as a constrained combinatorial optimization problem which is
classified as NP-hard and challenging to solve. We propose a novel deep
reinforcement learning (DRL) with a sequential model strategy that can
effectively learn the policy represented by a sequence-to-sequence neural
network for the UAV's trajectory design in an unsupervised manner. Through
extensive simulations, the obtained results show that the proposed DRL method
can find the UAV's trajectory that requires much less energy consumption when
compared to other baseline algorithms and achieves close-to-optimal
performance. In addition, simulation results show that the trained model by our
proposed DRL algorithm has an excellent generalization ability to larger
problem sizes without the need to retrain the model.

    

### [[2112.00334] VisRuler: Visual Analytics for Extracting Decision Rules from Bagged and Boosted Decision Trees](http://arxiv.org/abs/2112.00334)


  Bagging and boosting are two popular ensemble methods in machine learning
(ML) that produce many individual decision trees. Due to the inherent ensemble
characteristic of these methods, they typically outperform single decision
trees or other ML models in predictive performance. However, numerous decision
paths are generated for each decision tree, increasing the overall complexity
of the model and hindering its use in domains that require trustworthy and
explainable decisions, such as finance, social care, and health care. Thus, the
interpretability of bagging and boosting algorithms, such as random forests and
adaptive boosting, reduces as the number of decisions rises. In this paper, we
propose a visual analytics tool that aims to assist users in extracting
decisions from such ML models via a thorough visual inspection workflow that
includes selecting a set of robust and diverse models (originating from
different ensemble learning algorithms), choosing important features according
to their global contribution, and deciding which decisions are essential for
global explanation (or locally, for specific cases). The outcome is a final
decision based on the class agreement of several models and the explored manual
decisions exported by users. Finally, we evaluate the applicability and
effectiveness of VisRuler via a use case, a usage scenario, and a user study.

    

### [[2112.00337] A Unified Benchmark for the Unknown Detection Capability of Deep Neural Networks](http://arxiv.org/abs/2112.00337)


  Deep neural networks have achieved outstanding performance over various
tasks, but they have a critical issue: over-confident predictions even for
completely unknown samples. Many studies have been proposed to successfully
filter out these unknown samples, but they only considered narrow and specific
tasks, referred to as misclassification detection, open-set recognition, or
out-of-distribution detection. In this work, we argue that these tasks should
be treated as fundamentally an identical problem because an ideal model should
possess detection capability for all those tasks. Therefore, we introduce the
unknown detection task, an integration of previous individual tasks, for a
rigorous examination of the detection capability of deep neural networks on a
wide spectrum of unknown samples. To this end, unified benchmark datasets on
different scales were constructed and the unknown detection capabilities of
existing popular methods were subject to comparison. We found that Deep
Ensemble consistently outperforms the other approaches in detecting unknowns;
however, all methods are only successful for a specific type of unknown. The
reproducible code and benchmark datasets are available at
this https URL .

    

### [[2112.00344] Leveraging Sequence Embedding and Convolutional Neural Network for Protein Function Prediction](http://arxiv.org/abs/2112.00344)


  The capability of accurate prediction of protein functions and properties is
essential in the biotechnology industry, e.g. drug development and artificial
protein synthesis, etc. The main challenges of protein function prediction are
the large label space and the lack of labeled training data. Our method
leverages unsupervised sequence embedding and the success of deep convolutional
neural network to overcome these challenges. In contrast, most of the existing
methods delete the rare protein functions to reduce the label space.
Furthermore, some existing methods require additional bio-information (e.g.,
the 3-dimensional structure of the proteins) which is difficult to be
determined in biochemical experiments. Our proposed method significantly
outperforms the other methods on the publicly available benchmark using only
protein sequences as input. This allows the process of identifying protein
functions to be sped up.

    

### [[2112.00350] Investigation of Training Label Error Impact on RNN-T](http://arxiv.org/abs/2112.00350)


  In this paper, we propose an approach to quantitatively analyze impacts of
different training label errors to RNN-T based ASR models. The result shows
deletion errors are more harmful than substitution and insertion label errors
in RNN-T training data. We also examined label error impact mitigation
approaches on RNN-T and found that, though all the methods mitigate the
label-error-caused degradation to some extent, they could not remove the
performance gap between the models trained with and without the presence of
label errors. Based on the analysis results, we suggest to design data
pipelines for RNN-T with higher priority on reducing deletion label errors. We
also find that ensuring high-quality training labels remains important, despite
of the existence of the label error mitigation approaches.

    

### [[2112.00355] Score Transformer: Generating Musical Score from Note-level Representation](http://arxiv.org/abs/2112.00355)


  In this paper, we explore the tokenized representation of musical scores
using the Transformer model to automatically generate musical scores. Thus far,
sequence models have yielded fruitful results with note-level (MIDI-equivalent)
symbolic representations of music. Although the note-level representations can
comprise sufficient information to reproduce music aurally, they cannot contain
adequate information to represent music visually in terms of notation. Musical
scores contain various musical symbols (e.g., clef, key signature, and notes)
and attributes (e.g., stem direction, beam, and tie) that enable us to visually
comprehend musical content. However, automated estimation of these elements has
yet to be comprehensively addressed. In this paper, we first design score token
representation corresponding to the various musical elements. We then train the
Transformer model to transcribe note-level representation into appropriate
music notation. Evaluations of popular piano scores show that the proposed
method significantly outperforms existing methods on all 12 musical aspects
that were investigated. We also explore an effective notation-level token
representation to work with the model and determine that our proposed
representation produces the steadiest results.

    

### [[2112.00362] Dimensionality Reduction for Categorical Data](http://arxiv.org/abs/2112.00362)


  Categorical attributes are those that can take a discrete set of values,
e.g., colours. This work is about compressing vectors over categorical
attributes to low-dimension discrete vectors. The current hash-based methods
compressing vectors over categorical attributes to low-dimension discrete
vectors do not provide any guarantee on the Hamming distances between the
compressed representations. Here we present FSketch to create sketches for
sparse categorical data and an estimator to estimate the pairwise Hamming
distances among the uncompressed data only from their sketches. We claim that
these sketches can be used in the usual data mining tasks in place of the
original data without compromising the quality of the task. For that, we ensure
that the sketches also are categorical, sparse, and the Hamming distance
estimates are reasonably precise. Both the sketch construction and the Hamming
distance estimation algorithms require just a single-pass; furthermore, changes
to a data point can be incorporated into its sketch in an efficient manner. The
compressibility depends upon how sparse the data is and is independent of the
original dimension -- making our algorithm attractive for many real-life
scenarios. Our claims are backed by rigorous theoretical analysis of the
properties of FSketch and supplemented by extensive comparative evaluations
with related algorithms on some real-world datasets. We show that FSketch is
significantly faster, and the accuracy obtained by using its sketches are among
the top for the standard unsupervised tasks of RMSE, clustering and similarity
search.

    

### [[2112.00365] Mixed neural network Gaussian processes](http://arxiv.org/abs/2112.00365)


  This paper makes two contributions. Firstly, it introduces mixed
compositional kernels and mixed neural network Gaussian processes (NGGPs).
Mixed compositional kernels are generated by composition of probability
generating functions (PGFs). A mixed NNGP is a Gaussian process (GP) with a
mixed compositional kernel, arising in the infinite-width limit of multilayer
perceptrons (MLPs) that have a different activation function for each layer.
Secondly, $\theta$ activation functions for neural networks and $\theta$
compositional kernels are introduced by building upon the theory of branching
processes, and more specifically upon $\theta$ PGFs. While $\theta$
compositional kernels are recursive, they are expressed in closed form. It is
shown that $\theta$ compositional kernels have non-degenerate asymptotic
properties under certain conditions. Thus, GPs with $\theta$ compositional
kernels do not require non-explicit recursive kernel evaluations and have
controllable infinite-depth asymptotic properties. An open research question is
whether GPs with $\theta$ compositional kernels are limits of infinitely-wide
MLPs with $\theta$ activation functions.

    

### [[2112.00378] $\ell_\infty$-Robustness and Beyond: Unleashing Efficient Adversarial Training](http://arxiv.org/abs/2112.00378)


  Neural networks are vulnerable to adversarial attacks: adding well-crafted,
imperceptible perturbations to their input can modify their output. Adversarial
training is one of the most effective approaches in training robust models
against such attacks. However, it is much slower than vanilla training of
neural networks since it needs to construct adversarial examples for the entire
training data at every iteration, which has hampered its effectiveness.
Recently, Fast Adversarial Training was proposed that can obtain robust models
efficiently. However, the reasons behind its success are not fully understood,
and more importantly, it can only train robust models for $\ell_\infty$-bounded
attacks as it uses FGSM during training. In this paper, by leveraging the
theory of coreset selection we show how selecting a small subset of training
data provides a more principled approach towards reducing the time complexity
of robust training. Unlike existing methods, our approach can be adapted to a
wide variety of training objectives, including TRADES, $\ell_p$-PGD, and
Perceptual Adversarial Training. Our experimental results indicate that our
approach speeds up adversarial training by 2-3 times, while experiencing a
small reduction in the clean and robust accuracy.

    

### [[2112.00380] Deep Measurement Updates for Bayes Filters](http://arxiv.org/abs/2112.00380)


  Measurement update rules for Bayes filters often contain hand-crafted
heuristics to compute observation probabilities for high-dimensional sensor
data, like images. In this work, we propose the novel approach Deep Measurement
Update (DMU) as a general update rule for a wide range of systems. DMU has a
conditional encoder-decoder neural network structure to process depth images as
raw inputs. Even though the network is trained only on synthetic data, the
model shows good performance at evaluation time on real-world data. With our
proposed training scheme primed data training , we demonstrate how the DMU
models can be trained efficiently to be sensitive to condition variables
without having to rely on a stochastic information bottleneck. We validate the
proposed methods in multiple scenarios of increasing complexity, beginning with
the pose estimation of a single object to the joint estimation of the pose and
the internal state of an articulated system. Moreover, we provide a benchmark
against Articulated Signed Distance Functions(A-SDF) on the RBO dataset as a
baseline comparison for articulation state estimation.

    

### [[2112.00384] Translation-equivariant Image Quantizer for Bi-directional Image-Text Generation](http://arxiv.org/abs/2112.00384)


  Recently, vector-quantized image modeling has demonstrated impressive
performance on generation tasks such as text-to-image generation. However, we
discover that the current image quantizers do not satisfy translation
equivariance in the quantized space due to aliasing, degrading performance in
the downstream text-to-image generation and image-to-text generation, even in
simple experimental setups. Instead of focusing on anti-aliasing, we take a
direct approach to encourage translation equivariance in the quantized space.
In particular, we explore a desirable property of image quantizers, called
'Translation Equivariance in the Quantized Space' and propose a simple but
effective way to achieve translation equivariance by regularizing orthogonality
in the codebook embedding vectors. Using this method, we improve accuracy by
+22% in text-to-image generation and +26% in image-to-text generation,
outperforming the VQGAN.

    

### [[2112.00390] SegDiff: Image Segmentation with Diffusion Probabilistic Models](http://arxiv.org/abs/2112.00390)


  Diffusion Probabilistic Methods are employed for state-of-the-art image
generation. In this work, we present a method for extending such models for
performing image segmentation. The method learns end-to-end, without relying on
a pre-trained backbone. The information in the input image and in the current
estimation of the segmentation map is merged by summing the output of two
encoders. Additional encoding layers and a decoder are then used to iteratively
refine the segmentation map using a diffusion model. Since the diffusion model
is probabilistic, it is applied multiple times and the results are merged into
a final segmentation map. The new method obtains state-of-the-art results on
the Cityscapes validation set, the Vaihingen building segmentation benchmark,
and the MoNuSeg dataset.

    

### [[2112.00395] A Comprehensive Study on Various Statistical Techniques for Prediction of Movie Success](http://arxiv.org/abs/2112.00395)


  The film industry is one of the most popular entertainment industries and one
of the biggest markets for business. Among the contributing factors to this
would be the success of a movie in terms of its popularity as well as its box
office performance. Hence, we create a comprehensive comparison between the
various machine learning models to predict the rate of success of a movie. The
effectiveness of these models along with their statistical significance is
studied to conclude which of these models is the best predictor. Some insights
regarding factors that affect the success of the movies are also found. The
models studied include some Regression models, Machine Learning models, a Time
Series model and a Neural Network with the Neural Network being the best
performing model with an accuracy of about 86%. Additionally, as part of the
testing data for the movies released in 2020 are analysed.

    

### [[2112.00398] Effective and efficient structure learning with pruning and model averaging strategies](http://arxiv.org/abs/2112.00398)


  Learning the structure of a Bayesian Network (BN) with score-based solutions
involves exploring the search space of possible graphs and moving towards the
graph that maximises a given objective function. Some algorithms offer exact
solutions that guarantee to return the graph with the highest objective score,
while others offer approximate solutions in exchange for reduced computational
complexity. This paper describes an approximate BN structure learning
algorithm, which we call Model Averaging Hill-Climbing (MAHC), that combines
two novel strategies with hill-climbing search. The algorithm starts by pruning
the search space of graphs, where the pruning strategy can be viewed as an
aggressive version of the pruning strategies that are typically applied to
combinatorial optimisation structure learning problems. It then performs model
averaging in the hill-climbing search process and moves to the neighbouring
graph that maximises the objective function, on average, for that neighbouring
graph and over all its valid neighbouring graphs. Comparisons with other
algorithms spanning different classes of learning suggest that the combination
of aggressive pruning with model averaging is both effective and efficient,
particularly in the presence of data noise.

    

### [[2112.00407] Compare Where It Matters: Using Layer-Wise Regularization To Improve Federated Learning on Heterogeneous Data](http://arxiv.org/abs/2112.00407)


  Federated Learning is a widely adopted method to train neural networks over
distributed data. One main limitation is the performance degradation that
occurs when data is heterogeneously distributed. While many works have
attempted to address this problem, these methods under-perform because they are
founded on a limited understanding of neural networks. In this work, we verify
that only certain important layers in a neural network require regularization
for effective training. We additionally verify that Centered Kernel Alignment
(CKA) most accurately calculates similarity between layers of neural networks
trained on different data. By applying CKA-based regularization to important
layers during training, we significantly improve performance in heterogeneous
settings. We present FedCKA: a simple framework that out-performs previous
state-of-the-art methods on various deep learning tasks while also improving
efficiency and scalability.

    

### [[2112.00410] Rethink, Revisit, Revise: A Spiral Reinforced Self-Revised Network for Zero-Shot Learning](http://arxiv.org/abs/2112.00410)


  Current approaches to Zero-Shot Learning (ZSL) struggle to learn
generalizable semantic knowledge capable of capturing complex correlations.
Inspired by \emph{Spiral Curriculum}, which enhances learning processes by
revisiting knowledge, we propose a form of spiral learning which revisits
visual representations based on a sequence of attribute groups (e.g., a
combined group of \emph{color} and \emph{shape}). Spiral learning aims to learn
generalized local correlations, enabling models to gradually enhance global
learning and thus understand complex correlations. Our implementation is based
on a 2-stage \emph{Reinforced Self-Revised (RSR)} framework: \emph{preview} and
\emph{review}. RSR first previews visual information to construct diverse
attribute groups in a weakly-supervised manner. Then, it spirally learns
refined localities based on attribute groups and uses localities to revise
global semantic correlations. Our framework outperforms state-of-the-art
algorithms on four benchmark datasets in both zero-shot and generalized
zero-shot settings, which demonstrates the effectiveness of spiral learning in
learning generalizable and complex correlations. We also conduct extensive
analysis to show that attribute groups and reinforced decision processes can
capture complementary semantic information to improve predictions and aid
explainability.

    

### [[2112.00423] Controlling Wasserstein distances by Kernel norms with application to Compressive Statistical Learning](http://arxiv.org/abs/2112.00423)


  Comparing probability distributions is at the crux of many machine learning
algorithms. Maximum Mean Discrepancies (MMD) and Optimal Transport distances
(OT) are two classes of distances between probability measures that have
attracted abundant attention in past years. This paper establishes some
conditions under which the Wasserstein distance can be controlled by MMD norms.
Our work is motivated by the compressive statistical learning (CSL) theory, a
general framework for resource-efficient large scale learning in which the
training data is summarized in a single vector (called sketch) that captures
the information relevant to the considered learning task. Inspired by existing
results in CSL, we introduce the Hlder Lower Restricted Isometric Property
(Hlder LRIP) and show that this property comes with interesting guarantees
for compressive statistical learning. Based on the relations between the MMD
and the Wasserstein distance, we provide guarantees for compressive statistical
learning by introducing and studying the concept of Wasserstein learnability of
the learning task, that is when some task-specific metric between probability
distributions can be bounded by a Wasserstein distance.

    

### [[2112.00424] Multi-Agent Transfer Learning in Reinforcement Learning-Based Ride-Sharing Systems](http://arxiv.org/abs/2112.00424)


  Reinforcement learning (RL) has been used in a range of simulated real-world
tasks, e.g., sensor coordination, traffic light control, and on-demand mobility
services. However, real world deployments are rare, as RL struggles with
dynamic nature of real world environments, requiring time for learning a task
and adapting to changes in the environment. Transfer Learning (TL) can help
lower these adaptation times. In particular, there is a significant potential
of applying TL in multi-agent RL systems, where multiple agents can share
knowledge with each other, as well as with new agents that join the system. To
obtain the most from inter-agent transfer, transfer roles (i.e., determining
which agents act as sources and which as targets), as well as relevant transfer
content parameters (e.g., transfer size) should be selected dynamically in each
particular situation. As a first step towards fully dynamic transfers, in this
paper we investigate the impact of TL transfer parameters with fixed source and
target roles. Specifically, we label every agent-environment interaction with
agent's epistemic confidence, and we filter the shared examples using varying
threshold levels and sample sizes. We investigate impact of these parameters in
two scenarios, a standard predator-prey RL benchmark and a simulation of a
ride-sharing system with 200 vehicle agents and 10,000 ride-requests.

    

### [[2112.00432] A benchmark with decomposed distribution shifts for 360 monocular depth estimation](http://arxiv.org/abs/2112.00432)


  In this work we contribute a distribution shift benchmark for a computer
vision task; monocular depth estimation. Our differentiation is the
decomposition of the wider distribution shift of uncontrolled testing on
in-the-wild data, to three distinct distribution shifts. Specifically, we
generate data via synthesis and analyze them to produce covariate (color
input), prior (depth output) and concept (their relationship) distribution
shifts. We also synthesize combinations and show how each one is indeed a
different challenge to address, as stacking them produces increased performance
drops and cannot be addressed horizontally using standard approaches.

    

### [[2112.00434] Training Experimentally Robust and Interpretable Binarized Regression Models Using Mixed-Integer Programming](http://arxiv.org/abs/2112.00434)


  In this paper, we explore model-based approach to training robust and
interpretable binarized regression models for multiclass classification tasks
using Mixed-Integer Programming (MIP). Our MIP model balances the optimization
of prediction margin and model size by using a weighted objective that:
minimizes the total margin of incorrectly classified training instances,
maximizes the total margin of correctly classified training instances, and
maximizes the overall model regularization. We conduct two sets of experiments
to test the classification accuracy of our MIP model over standard and
corrupted versions of multiple classification datasets, respectively. In the
first set of experiments, we show that our MIP model outperforms an equivalent
Pseudo-Boolean Optimization (PBO) model and achieves competitive results to
Logistic Regression (LR) and Gradient Descent (GD) in terms of classification
accuracy over the standard datasets. In the second set of experiments, we show
that our MIP model outperforms the other models (i.e., GD and LR) in terms of
classification accuracy over majority of the corrupted datasets. Finally, we
visually demonstrate the interpretability of our MIP model in terms of its
learned parameters over the MNIST dataset. Overall, we show the effectiveness
of training robust and interpretable binarized regression models using MIP.

    

### [[2112.00460] Machine learning Hadron Spectral Functions in Lattice QCD](http://arxiv.org/abs/2112.00460)


  Hadron spectral functions carry all the information of hadrons and are
encoded in the Euclidean two-point correlation functions. The extraction of
hadron spectral functions from the correlator is a typical ill-posed inverse
problem and infinite number of solutions to this problem exists. We propose a
novel neural network (sVAE) based on the Variation Auto-Encoder (VAE) and
Bayesian theorem. Inspired by the maximum entropy method (MEM) we construct the
loss function of the neural work such that it includes a Shannon-Jaynes entropy
term and a likelihood term. The sVAE is then trained to provide the most
probable spectral functions. For the training samples of spectral function we
used general spectral functions produced from the Gaussian Mixture Model. After
the training is done we performed the mock data tests with input spectral
functions consisting 1) only a free continuum, 2) only a resonance peak, 3) a
resonance peak plus a free continuum and 4) a NRQCD motivated spectral
function. From the mock data test we find that the sVAE in most cases is
comparable to the maximum entropy method in the quality of reconstructing
spectral functions and even outperforms the MEM in the case where the spectral
function has sharp peaks with insufficient number of data points in the
correlator. By applying to temporal correlation functions of charmonium in the
pseudoscalar channel obtained in the quenched lattice QCD at 0.75 $T_c$ on
$128^3\times96$ lattices and $1.5$ $T_c$ on $128^3\times48$ lattices, we find
that the resonance peak of $\eta_c$ extracted from both the sVAE and MEM has a
substantial dependence on the number of points in the temporal direction
($N_\tau$) adopted in the lattice simulation and $N_\tau$ larger than 48 is
needed to resolve the fate of $\eta_c$ at 1.5 $T_c$.

    

### [[2112.00468] Seeking Sinhala Sentiment: Predicting Facebook Reactions of Sinhala Posts](http://arxiv.org/abs/2112.00468)


  The Facebook network allows its users to record their reactions to text via a
typology of emotions. This network, taken at scale, is therefore a prime data
set of annotated sentiment data. This paper uses millions of such reactions,
derived from a decade worth of Facebook post data centred around a Sri Lankan
context, to model an eye of the beholder approach to sentiment detection for
online Sinhala textual content. Three different sentiment analysis models are
built, taking into account a limited subset of reactions, all reactions, and
another that derives a positive/negative star rating value. The efficacy of
these models in capturing the reactions of the observers are then computed and
discussed. The analysis reveals that binary classification of reactions, for
Sinhala content, is significantly more accurate than the other approaches.
Furthermore, the inclusion of the like reaction hinders the capability of
accurately predicting other reactions.

    

### [[2112.00478] On the Practical Consistency of Meta-Reinforcement Learning Algorithms](http://arxiv.org/abs/2112.00478)


  Consistency is the theoretical property of a meta learning algorithm that
ensures that, under certain assumptions, it can adapt to any task at test time.
An open question is whether and how theoretical consistency translates into
practice, in comparison to inconsistent algorithms. In this paper, we
empirically investigate this question on a set of representative meta-RL
algorithms. We find that theoretically consistent algorithms can indeed usually
adapt to out-of-distribution (OOD) tasks, while inconsistent ones cannot,
although they can still fail in practice for reasons like poor exploration. We
further find that theoretically inconsistent algorithms can be made consistent
by continuing to update all agent components on the OOD tasks, and adapt as
well or better than originally consistent ones. We conclude that theoretical
consistency is indeed a desirable property, and inconsistent meta-RL algorithms
can easily be made consistent to enjoy the same benefits.

    

### [[2112.00499] Structure-Aware Label Smoothing for Graph Neural Networks](http://arxiv.org/abs/2112.00499)


  Representing a label distribution as a one-hot vector is a common practice in
training node classification models. However, the one-hot representation may
not adequately reflect the semantic characteristics of a node in different
classes, as some nodes may be semantically close to their neighbors in other
classes. It would cause over-confidence since the models are encouraged to
assign full probabilities when classifying every node. While training models
with label smoothing can ease this problem to some degree, it still fails to
capture the nodes' semantic characteristics implied by the graph structures. In
this work, we propose a novel SALS (\textit{Structure-Aware Label Smoothing})
method as an enhancement component to popular node classification models. SALS
leverages the graph structures to capture the semantic correlations between the
connected nodes and generate the structure-aware label distribution to replace
the original one-hot label vectors, thus improving the node classification
performance without inference costs. Extensive experiments on seven node
classification benchmark datasets reveal the effectiveness of our SALS on
improving both transductive and inductive node classification. Empirical
results show that SALS is superior to the label smoothing method and enhances
the node classification models to outperform the baseline methods.

    

### [[2112.00503] Zero-Shot Cross-Lingual Machine Reading Comprehension via Inter-Sentence Dependency Graph](http://arxiv.org/abs/2112.00503)


  We target the task of cross-lingual Machine Reading Comprehension (MRC) in
the direct zero-shot setting, by incorporating syntactic features from
Universal Dependencies (UD), and the key features we use are the syntactic
relations within each sentence. While previous work has demonstrated effective
syntax-guided MRC models, we propose to adopt the inter-sentence syntactic
relations, in addition to the rudimentary intra-sentence relations, to further
utilize the syntactic dependencies in the multi-sentence input of the MRC task.
In our approach, we build the Inter-Sentence Dependency Graph (ISDG) connecting
dependency trees to form global syntactic relations across sentences. We then
propose the ISDG encoder that encodes the global dependency graph, addressing
the inter-sentence relations via both one-hop and multi-hop dependency paths
explicitly. Experiments on three multilingual MRC datasets (XQuAD, MLQA,
TyDiQA-GoldP) show that our encoder that is only trained on English is able to
improve the zero-shot performance on all 14 test sets covering 8 languages,
with up to 3.8 F1 / 5.2 EM improvement on-average, and 5.2 F1 / 11.2 EM on
certain languages. Further analysis shows the improvement can be attributed to
the attention on the cross-linguistically consistent syntactic path.

    

### [[2112.00534] Empirical evaluation of shallow and deep learning classifiers for Arabic sentiment analysis](http://arxiv.org/abs/2112.00534)


  This work presents a detailed comparison of the performance of deep learning
models such as convolutional neural networks (CNN), long short-term memory
(LSTM), gated recurrent units (GRU), their hybrids, and a selection of shallow
learning classifiers for sentiment analysis of Arabic reviews. Additionally,
the comparison includes state-of-the-art models such as the transformer
architecture and the araBERT pre-trained model. The datasets used in this study
are multi-dialect Arabic hotel and book review datasets, which are some of the
largest publicly available datasets for Arabic reviews. Results showed deep
learning outperforming shallow learning for binary and multi-label
classification, in contrast with the results of similar work reported in the
literature. This discrepancy in outcome was caused by dataset size as we found
it to be proportional to the performance of deep learning models. The
performance of deep and shallow learning techniques was analyzed in terms of
accuracy and F1 score. The best performing shallow learning technique was
Random Forest followed by Decision Tree, and AdaBoost. The deep learning models
performed similarly using a default embedding layer, while the transformer
model performed best when augmented with araBERT.

    

### [[2112.00544] Molecular Contrastive Learning with Chemical Element Knowledge Graph](http://arxiv.org/abs/2112.00544)


  Molecular representation learning contributes to multiple downstream tasks
such as molecular property prediction and drug design. To properly represent
molecules, graph contrastive learning is a promising paradigm as it utilizes
self-supervision signals and has no requirements for human annotations.
However, prior works fail to incorporate fundamental domain knowledge into
graph semantics and thus ignore the correlations between atoms that have common
attributes but are not directly connected by bonds. To address these issues, we
construct a Chemical Element Knowledge Graph (KG) to summarize microscopic
associations between elements and propose a novel Knowledge-enhanced
Contrastive Learning (KCL) framework for molecular representation learning. KCL
framework consists of three modules. The first module, knowledge-guided graph
augmentation, augments the original molecular graph based on the Chemical
Element KG. The second module, knowledge-aware graph representation, extracts
molecular representations with a common graph encoder for the original
molecular graph and a Knowledge-aware Message Passing Neural Network (KMPNN) to
encode complex information in the augmented molecular graph. The final module
is a contrastive objective, where we maximize agreement between these two views
of molecular graphs. Extensive experiments demonstrated that KCL obtained
superior performances against state-of-the-art baselines on eight molecular
datasets. Visualization experiments properly interpret what KCL has learned
from atoms and attributes in the augmented molecular graphs. Our codes and data
are available in supplementary materials.

    

### [[2112.00552] SaDe: Learning Models that Provably Satisfy Domain Constraints](http://arxiv.org/abs/2112.00552)


  With increasing real world applications of machine learning, models are often
required to comply with certain domain based requirements, e.g., safety
guarantees in aircraft systems, legal constraints in a loan approval model. A
natural way to represent these properties is in the form of constraints.
Including such constraints in machine learning is typically done by the means
of regularization, which does not guarantee satisfaction of the constraints. In
this paper, we present a machine learning approach that can handle a wide
variety of constraints, and guarantee that these constraints will be satisfied
by the model even on unseen data. We cast machine learning as a maximum
satisfiability problem, and solve it using a novel algorithm SaDe which
combines constraint satisfaction with gradient descent. We demonstrate on three
use cases that this approach learns models that provably satisfy the given
constraints.

    

### [[2112.00557] 3D Reconstruction Using a Linear Laser Scanner and a Camera](http://arxiv.org/abs/2112.00557)


  With the rapid development of computer graphics and vision, several
three-dimensional (3D) reconstruction techniques have been proposed and used to
obtain the 3D representation of objects in the form of point cloud models, mesh
models, and geometric models. The cost of 3D reconstruction is declining due to
the maturing of this technology, however, the inexpensive 3D reconstruction
scanners on the market may not be able to generate a clear point cloud model as
expected. This study systematically reviews some basic types of 3D
reconstruction technology and introduces an easy implementation using a linear
laser scanner, a camera, and a turntable. The implementation is based on the
monovision with laser and has tested several objects like wiki and mug. The
accuracy and resolution of the point cloud result are quite satisfying. It
turns out everyone can build such a 3D reconstruction system with appropriate
procedures.

    

### [[2112.00565] On Mixing Times of Metropolized Algorithm With Optimization Step (MAO) : A New Framework](http://arxiv.org/abs/2112.00565)


  In this paper, we consider sampling from a class of distributions with thin
tails supported on $\mathbb{R}^d$ and make two primary contributions. First, we
propose a new Metropolized Algorithm With Optimization Step (MAO), which is
well suited for such targets. Our algorithm is capable of sampling from
distributions where the Metropolis-adjusted Langevin algorithm (MALA) is not
converging or lacking in theoretical guarantees. Second, we derive upper bounds
on the mixing time of MAO. Our results are supported by simulations on multiple
target distributions.

    

### [[2112.00567] DPRK-BERT: The Supreme Language Model](http://arxiv.org/abs/2112.00567)


  Deep language models have achieved remarkable success in the NLP domain. The
standard way to train a deep language model is to employ unsupervised learning
from scratch on a large unlabeled corpus. However, such large corpora are only
available for widely-adopted and high-resource languages and domains. This
study presents the first deep language model, DPRK-BERT, for the DPRK language.
We achieve this by compiling the first unlabeled corpus for the DPRK language
and fine-tuning a preexisting the ROK language model. We compare the proposed
model with existing approaches and show significant improvements on two DPRK
datasets. We also present a cross-lingual version of this model which yields
better generalization across the two Korean languages. Finally, we provide
various NLP tools related to the DPRK language that would foster future
research.

    

### [[2112.00570] Toward Foundation Models for Earth Monitoring: Proposal for a Climate Change Benchmark](http://arxiv.org/abs/2112.00570)


  Recent progress in self-supervision shows that pre-training large neural
networks on vast amounts of unsupervised data can lead to impressive increases
in generalisation for downstream tasks. Such models, recently coined as
foundation models, have been transformational to the field of natural language
processing. While similar models have also been trained on large corpuses of
images, they are not well suited for remote sensing data. To stimulate the
development of foundation models for Earth monitoring, we propose to develop a
new benchmark comprised of a variety of downstream tasks related to climate
change. We believe that this can lead to substantial improvements in many
existing applications and facilitate the development of new applications. This
proposal is also a call for collaboration with the aim of developing a better
evaluation process to mitigate potential downsides of foundation models for
Earth monitoring.

    

### [[2112.00578] Systematic Generalization with Edge Transformers](http://arxiv.org/abs/2112.00578)


  Recent research suggests that systematic generalization in natural language
understanding remains a challenge for state-of-the-art neural models such as
Transformers and Graph Neural Networks. To tackle this challenge, we propose
Edge Transformer, a new model that combines inspiration from Transformers and
rule-based symbolic AI. The first key idea in Edge Transformers is to associate
vector states with every edge, that is, with every pair of input nodes -- as
opposed to just every node, as it is done in the Transformer model. The second
major innovation is a triangular attention mechanism that updates edge
representations in a way that is inspired by unification from logic
programming. We evaluate Edge Transformer on compositional generalization
benchmarks in relational reasoning, semantic parsing, and dependency parsing.
In all three settings, the Edge Transformer outperforms Relation-aware,
Universal and classical Transformer baselines.

    

### [[2112.00579] Conditional Expectation based Value Decomposition for Scalable On-Demand Ride Pooling](http://arxiv.org/abs/2112.00579)


  Owing to the benefits for customers (lower prices), drivers (higher
revenues), aggregation companies (higher revenues) and the environment (fewer
vehicles), on-demand ride pooling (e.g., Uber pool, Grab Share) has become
quite popular. The significant computational complexity of matching vehicles to
combinations of requests has meant that traditional ride pooling approaches are
myopic in that they do not consider the impact of current matches on future
value for vehicles/drivers. Recently, Neural Approximate Dynamic Programming
(NeurADP) has employed value decomposition with Approximate Dynamic Programming
(ADP) to outperform leading approaches by considering the impact of an
individual agent's (vehicle) chosen actions on the future value of that agent.
However, in order to ensure scalability and facilitate city-scale ride pooling,
NeurADP completely ignores the impact of other agents actions on individual
agent/vehicle value. As demonstrated in our experimental results, ignoring the
impact of other agents actions on individual value can have a significant
impact on the overall performance when there is increased competition among
vehicles for demand. Our key contribution is a novel mechanism based on
computing conditional expectations through joint conditional probabilities for
capturing dependencies on other agents actions without increasing the
complexity of training or decision making. We show that our new approach,
Conditional Expectation based Value Decomposition (CEVD) outperforms NeurADP by
up to 9.76% in terms of overall requests served, which is a significant
improvement on a city wide benchmark taxi dataset.

    

### [[2112.00583] Meta Arcade: A Configurable Environment Suite for Meta-Learning](http://arxiv.org/abs/2112.00583)


  Most approaches to deep reinforcement learning (DRL) attempt to solve a
single task at a time. As a result, most existing research benchmarks consist
of individual games or suites of games that have common interfaces but little
overlap in their perceptual features, objectives, or reward structures. To
facilitate research into knowledge transfer among trained agents (e.g. via
multi-task and meta-learning), more environment suites that provide
configurable tasks with enough commonality to be studied collectively are
needed. In this paper we present Meta Arcade, a tool to easily define and
configure custom 2D arcade games that share common visuals, state spaces,
action spaces, game components, and scoring mechanisms. Meta Arcade differs
from prior environments in that both task commonality and configurability are
prioritized: entire sets of games can be constructed from common elements, and
these elements are adjustable through exposed parameters. We include a suite of
24 predefined games that collectively illustrate the possibilities of this
framework and discuss how these games can be configured for research
applications. We provide several experiments that illustrate how Meta Arcade
could be used, including single-task benchmarks of predefined games, sample
curriculum-based approaches that change game parameters over a set schedule,
and an exploration of transfer learning between games.

    

### [[2112.00584] The Shape Part Slot Machine: Contact-based Reasoning for Generating 3D Shapes from Parts](http://arxiv.org/abs/2112.00584)


  We present the Shape Part Slot Machine, a new method for assembling novel 3D
shapes from existing parts by performing contact-based reasoning. Our method
represents each shape as a graph of "slots," where each slot is a region of
contact between two shape parts. Based on this representation, we design a
graph-neural-network-based model for generating new slot graphs and retrieving
compatible parts, as well as a gradient-descent-based optimization scheme for
assembling the retrieved parts into a complete shape that respects the
generated slot graph. This approach does not require any semantic part labels;
interestingly, it also does not require complete part geometries -- reasoning
about the regions where parts connect proves sufficient to generate novel,
high-quality 3D shapes. We demonstrate that our method generates shapes that
outperform existing modeling-by-assembly approaches in terms of quality,
diversity, and structural complexity.

    

### [[2112.00588] Outlier Detection using AI: A Survey](http://arxiv.org/abs/2112.00588)


  An outlier is an event or observation that is defined as an unusual activity,
intrusion, or a suspicious data point that lies at an irregular distance from a
population. The definition of an outlier event, however, is subjective and
depends on the application and the domain (Energy, Health, Wireless Network,
etc.). It is important to detect outlier events as carefully as possible to
avoid infrastructure failures because anomalous events can cause minor to
severe damage to infrastructure. For instance, an attack on a cyber-physical
system such as a microgrid may initiate voltage or frequency instability,
thereby damaging a smart inverter which involves very expensive repairing.
Unusual activities in microgrids can be mechanical faults, behavior changes in
the system, human or instrument errors or a malicious attack. Accordingly, and
due to its variability, Outlier Detection (OD) is an ever-growing research
field. In this chapter, we discuss the progress of OD methods using AI
techniques. For that, the fundamental concepts of each OD model are introduced
via multiple categories. Broad range of OD methods are categorized into six
major categories: Statistical-based, Distance-based, Density-based,
Clustering-based, Learning-based, and Ensemble methods. For every category, we
discuss recent state-of-the-art approaches, their application areas, and
performances. After that, a brief discussion regarding the advantages,
disadvantages, and challenges of each technique is provided with
recommendations on future research directions. This survey aims to guide the
reader to better understand recent progress of OD methods for the assurance of
AI.

    

### [[2112.00599] An implementation of the "Guess who?" game using CLIP](http://arxiv.org/abs/2112.00599)


  CLIP (Contrastive Language-Image Pretraining) is an efficient method for
learning computer vision tasks from natural language supervision that has
powered a recent breakthrough in deep learning due to its zero-shot transfer
capabilities. By training from image-text pairs available on the internet, the
CLIP model transfers non-trivially to most tasks without the need for any data
set specific training. In this work, we use CLIP to implement the engine of the
popular game "Guess who?", so that the player interacts with the game using
natural language prompts and CLIP automatically decides whether an image in the
game board fulfills that prompt or not. We study the performance of this
approach by benchmarking on different ways of prompting the questions to CLIP,
and show the limitations of its zero-shot capabilites.

    

### [[2112.00600] Towards Futuristic Autonomous Experimentation--A Surprise-Reacting Sequential Experiment Policy](http://arxiv.org/abs/2112.00600)


  An autonomous experimentation platform in manufacturing is supposedly capable
of conducting a sequential search for finding suitable manufacturing conditions
for advanced materials by itself or even for discovering new materials with
minimal human intervention. The core of the intelligent control of such
platforms is the policy directing sequential experiments, namely, to decide
where to conduct the next experiment based on what has been done thus far. Such
policy inevitably trades off exploitation versus exploration and the current
practice is under the Bayesian optimization framework using the expected
improvement criterion or its variants. We discuss whether it is beneficial to
trade off exploitation versus exploration by measuring the element and degree
of surprise associated with the immediate past observation. We devise a
surprise-reacting policy using two existing surprise metrics, known as the
Shannon surprise and Bayesian surprise. Our analysis shows that the
surprise-reacting policy appears to be better suited for quickly characterizing
the overall landscape of a response surface or a design place under resource
constraints. We argue that such capability is much needed for futuristic
autonomous experimentation platforms. We do not claim that we have a fully
autonomous experimentation platform, but believe that our current effort sheds
new lights or provides a different view angle as researchers are racing to
elevate the autonomy of various primitive autonomous experimentation systems.

    

### [[2112.00639] Robustness in Deep Learning for Computer Vision: Mind the gap?](http://arxiv.org/abs/2112.00639)


  Deep neural networks for computer vision tasks are deployed in increasingly
safety-critical and socially-impactful applications, motivating the need to
close the gap in model performance under varied, naturally occurring imaging
conditions. Robustness, ambiguously used in multiple contexts including
adversarial machine learning, here then refers to preserving model performance
under naturally-induced image corruptions or alterations.
We perform a systematic review to identify, analyze, and summarize current
definitions and progress towards non-adversarial robustness in deep learning
for computer vision. We find that this area of research has received
disproportionately little attention relative to adversarial machine learning,
yet a significant robustness gap exists that often manifests in performance
degradation similar in magnitude to adversarial conditions.
To provide a more transparent definition of robustness across contexts, we
introduce a structural causal model of the data generating process and
interpret non-adversarial robustness as pertaining to a model's behavior on
corrupted images which correspond to low-probability samples from the unaltered
data distribution. We then identify key architecture-, data augmentation-, and
optimization tactics for improving neural network robustness. This causal view
of robustness reveals that common practices in the current literature, both in
regards to robustness tactics and evaluations, correspond to causal concepts,
such as soft interventions resulting in a counterfactually-altered distribution
of imaging conditions. Through our findings and analysis, we offer perspectives
on how future research may mind this evident and significant non-adversarial
robustness gap.

    

### [[2112.00646] Reliability Assessment and Safety Arguments for Machine Learning Components in Assuring Learning-Enabled Autonomous Systems](http://arxiv.org/abs/2112.00646)


  The increasing use of Machine Learning (ML) components embedded in autonomous
systems -- so-called Learning-Enabled Systems (LES) -- has resulted in the
pressing need to assure their functional safety. As for traditional functional
safety, the emerging consensus within both, industry and academia, is to use
assurance cases for this purpose. Typically assurance cases support claims of
reliability in support of safety, and can be viewed as a structured way of
organising arguments and evidence generated from safety analysis and
reliability modelling activities. While such assurance activities are
traditionally guided by consensus-based standards developed from vast
engineering experience, LES pose new challenges in safety-critical application
due to the characteristics and design of ML models. In this article, we first
present an overall assurance framework for LES with an emphasis on quantitative
aspects, e.g., breaking down system-level safety targets to component-level
requirements and supporting claims stated in reliability metrics. We then
introduce a novel model-agnostic Reliability Assessment Model (RAM) for ML
classifiers that utilises the operational profile and robustness verification
evidence. We discuss the model assumptions and the inherent challenges of
assessing ML reliability uncovered by our RAM and propose practical solutions.
Probabilistic safety arguments at the lower ML component-level are also
developed based on the RAM. Finally, to evaluate and demonstrate our methods,
we not only conduct experiments on synthetic/benchmark datasets but also
demonstrate the scope of our methods with a comprehensive case study on
Autonomous Underwater Vehicles in simulation.

    

### [[2112.00648] Remixing Functionally Graded Structures: Data-Driven Topology Optimization with Multiclass Shape Blending](http://arxiv.org/abs/2112.00648)


  To create heterogeneous, multiscale structures with unprecedented
functionalities, recent topology optimization approaches design either fully
aperiodic systems or functionally graded structures, which compete in terms of
design freedom and efficiency. We propose to inherit the advantages of both
through a data-driven framework for multiclass functionally graded structures
that mixes several families, i.e., classes, of microstructure topologies to
create spatially-varying designs with guaranteed feasibility. The key is a new
multiclass shape blending scheme that generates smoothly graded microstructures
without requiring compatible classes or connectivity and feasibility
constraints. Moreover, it transforms the microscale problem into an efficient,
low-dimensional one without confining the design to predefined shapes.
Compliance and shape matching examples using common truss geometries and
diversity-based freeform topologies demonstrate the versatility of our
framework, while studies on the effect of the number and diversity of classes
illustrate the effectiveness. The generality of the proposed methods supports
future extensions beyond the linear applications presented.

    

### [[2112.00653] Variational Learning for Unsupervised Knowledge Grounded Dialogs](http://arxiv.org/abs/2112.00653)


  Recent methods for knowledge grounded dialogs generate responses by
incorporating information from an external textual document. These methods do
not require the exact document to be known during training and rely on the use
of a retrieval system to fetch relevant documents from a large index. The
documents used to generate the responses are modeled as latent variables whose
prior probabilities need to be estimated. Models such as RAG , marginalize the
document probabilities over the documents retrieved from the index to define
the log likelihood loss function which is optimized end-to-end.
In this paper, we develop a variational approach to the above technique
wherein, we instead maximize the Evidence Lower bound (ELBO). Using a
collection of three publicly available open-conversation datasets, we
demonstrate how the posterior distribution, that has information from the
ground-truth response, allows for a better approximation of the objective
function during training. To overcome the challenges associated with sampling
over a large knowledge collection, we develop an efficient approach to
approximate the ELBO. To the best of our knowledge we are the first to apply
variational training for open-scale unsupervised knowledge grounded dialog
systems.

    

### [[2112.00654] Siamese Neural Encoders for Long-Term Indoor Localization with Mobile Devices](http://arxiv.org/abs/2112.00654)


  Fingerprinting-based indoor localization is an emerging application domain
for enhanced positioning and tracking of people and assets within indoor
locales. The superior pairing of ubiquitously available WiFi signals with
computationally capable smartphones is set to revolutionize the area of indoor
localization. However, the observed signal characteristics from independently
maintained WiFi access points vary greatly over time. Moreover, some of the
WiFi access points visible at the initial deployment phase may be replaced or
removed over time. These factors are often ignored in indoor localization
frameworks and cause gradual and catastrophic degradation of localization
accuracy post-deployment (over weeks and months). To overcome these challenges,
we propose a Siamese neural encoder-based framework that offers up to 40%
reduction in degradation of localization accuracy over time compared to the
state-of-the-art in the area, without requiring any retraining.

    

### [[2112.00655] Efficient and Local Parallel Random Walks](http://arxiv.org/abs/2112.00655)


  Random walks are a fundamental primitive used in many machine learning
algorithms with several applications in clustering and semi-supervised
learning. Despite their relevance, the first efficient parallel algorithm to
compute random walks has been introduced very recently (Lacki et al.).
Unfortunately their method has a fundamental shortcoming: their algorithm is
non-local in that it heavily relies on computing random walks out of all nodes
in the input graph, even though in many practical applications one is
interested in computing random walks only from a small subset of nodes in the
graph. In this paper, we present a new algorithm that overcomes this limitation
by building random walk efficiently and locally at the same time. We show that
our technique is both memory and round efficient, and in particular yields an
efficient parallel local clustering algorithm. Finally, we complement our
theoretical analysis with experimental results showing that our algorithm is
significantly more scalable than previous approaches.

    

### [[2112.00659] Certified Adversarial Defenses Meet Out-of-Distribution Corruptions: Benchmarking Robustness and Simple Baselines](http://arxiv.org/abs/2112.00659)


  Certified robustness guarantee gauges a model's robustness to test-time
attacks and can assess the model's readiness for deployment in the real world.
In this work, we critically examine how the adversarial robustness guarantees
from randomized smoothing-based certification methods change when
state-of-the-art certifiably robust models encounter out-of-distribution (OOD)
data. Our analysis demonstrates a previously unknown vulnerability of these
models to low-frequency OOD data such as weather-related corruptions, rendering
these models unfit for deployment in the wild. To alleviate this issue, we
propose a novel data augmentation scheme, FourierMix, that produces
augmentations to improve the spectral coverage of the training data.
Furthermore, we propose a new regularizer that encourages consistent
predictions on noise perturbations of the augmented data to improve the quality
of the smoothed models. We find that FourierMix augmentations help eliminate
the spectral bias of certifiably robust models enabling them to achieve
significantly better robustness guarantees on a range of OOD benchmarks. Our
evaluation also uncovers the inability of current OOD benchmarks at
highlighting the spectral biases of the models. To this end, we propose a
comprehensive benchmarking suite that contains corruptions from different
regions in the spectral domain. Evaluation of models trained with popular
augmentation methods on the proposed suite highlights their spectral biases and
establishes the superiority of FourierMix trained models at achieving
better-certified robustness guarantees under OOD shifts over the entire
frequency spectrum.

    

### [[2112.00661] MOMO -- Deep Learning-driven classification of external DICOM studies for PACS archivation](http://arxiv.org/abs/2112.00661)


  Patients regularly continue assessment or treatment in other facilities than
they began them in, receiving their previous imaging studies as a CD-ROM and
requiring clinical staff at the new hospital to import these studies into their
local database. However, between different facilities, standards for
nomenclature, contents, or even medical procedures may vary, often requiring
human intervention to accurately classify the received studies in the context
of the recipient hospital's standards. In this study, the authors present MOMO
(MOdality Mapping and Orchestration), a deep learning-based approach to
automate this mapping process utilizing metadata substring matching and a
neural network ensemble, which is trained to recognize the 76 most common
imaging studies across seven different modalities. A retrospective study is
performed to measure the accuracy that this algorithm can provide. To this end,
a set of 11,934 imaging series with existing labels was retrieved from the
local hospital's PACS database to train the neural networks. A set of 843
completely anonymized external studies was hand-labeled to assess the
performance of our algorithm. Additionally, an ablation study was performed to
measure the performance impact of the network ensemble in the algorithm, and a
comparative performance test with a commercial product was conducted. In
comparison to a commercial product (96.20% predictive power, 82.86% accuracy,
1.36% minor errors), a neural network ensemble alone performs the
classification task with less accuracy (99.05% predictive power, 72.69%
accuracy, 10.3% minor errors). However, MOMO outperforms either by a large
margin in accuracy and with increased predictive power (99.29% predictive
power, 92.71% accuracy, 2.63% minor errors).

    

### [[2112.00663] Graph Conditioned Sparse-Attention for Improved Source Code Understanding](http://arxiv.org/abs/2112.00663)


  Transformer architectures have been successfully used in learning source code
representations. The fusion between a graph representation like Abstract Syntax
Tree (AST) and a source code sequence makes the use of current approaches
computationally intractable for large input sequence lengths. Source code can
have long-range dependencies that require larger sequence lengths to model
effectively. Current approaches have a quadratic growth in computational and
memory costs with respect to the sequence length. Using such models in
practical scenarios is difficult. In this work, we propose the conditioning of
a source code snippet with its graph modality by using the graph adjacency
matrix as an attention mask for a sparse self-attention mechanism and the use
of a graph diffusion mechanism to model longer-range token dependencies. Our
model reaches state-of-the-art results in BLEU, METEOR, and ROUGE-L metrics for
the code summarization task and near state-of-the-art accuracy in the variable
misuse task. The memory use and inference time of our model have linear growth
with respect to the input sequence length as compared to the quadratic growth
of previous works.

    

### [[2112.00690] MDFM: Multi-Decision Fusing Model for Few-Shot Learning](http://arxiv.org/abs/2112.00690)


  In recent years, researchers pay growing attention to the few-shot learning
(FSL) task to address the data-scarce problem. A standard FSL framework is
composed of two components: i) Pre-train. Employ the base data to generate a
CNN-based feature extraction model (FEM). ii) Meta-test. Apply the trained FEM
to the novel data (category is different from base data) to acquire the feature
embeddings and recognize them. Although researchers have made remarkable
breakthroughs in FSL, there still exists a fundamental problem. Since the
trained FEM with base data usually cannot adapt to the novel class flawlessly,
the novel data's feature may lead to the distribution shift problem. To address
this challenge, we hypothesize that even if most of the decisions based on
different FEMs are viewed as \textit{weak decisions}, which are not available
for all classes, they still perform decently in some specific categories.
Inspired by this assumption, we propose a novel method Multi-Decision Fusing
Model (MDFM), which comprehensively considers the decisions based on multiple
FEMs to enhance the efficacy and robustness of the model. MDFM is a simple,
flexible, non-parametric method that can directly apply to the existing FEMs.
Besides, we extend the proposed MDFM to two FSL settings (i.e., supervised and
semi-supervised settings). We evaluate the proposed method on five benchmark
datasets and achieve significant improvements of 3.4%-7.3\% compared with
state-of-the-arts.

    

### [[2112.00695] DeepAoANet: Learning Angle of Arrival from Software Defined Radios with Deep Neural Networks](http://arxiv.org/abs/2112.00695)


  Direction finding and positioning systems based on RF signals are
significantly impacted by multipath propagation, particularly in indoor
environments. Existing algorithms (e.g MUSIC) perform poorly in resolving Angle
of Arrival (AoA) in the presence of multipath or when operating in a weak
signal regime. We note that digitally sampled RF frontends allow for the easy
analysis of signals, and their delayed components. Low-cost Software-Defined
Radio (SDR) modules enable Channel State Information (CSI) extraction across a
wide spectrum, motivating the design of an enhanced Angle-of-Arrival (AoA)
solution. We propose a Deep Learning approach to deriving AoA from a single
snapshot of the SDR multichannel data. We compare and contrast deep-learning
based angle classification and regression models, to estimate up to two AoAs
accurately. We have implemented the inference engines on different platforms to
extract AoAs in real-time, demonstrating the computational tractability of our
approach. To demonstrate the utility of our approach we have collected IQ
(In-phase and Quadrature components) samples from a four-element Universal
Linear Array (ULA) in various Light-of-Sight (LOS) and Non-Line-of-Sight (NLOS)
environments, and published the dataset. Our proposed method demonstrates
excellent reliability in determining number of impinging signals and realized
mean absolute AoA errors less than $2^{\circ}$.

    

### [[2112.00698] CondenseNeXt: An Ultra-Efficient Deep Neural Network for Embedded Systems](http://arxiv.org/abs/2112.00698)


  Due to the advent of modern embedded systems and mobile devices with
constrained resources, there is a great demand for incredibly efficient deep
neural networks for machine learning purposes. There is also a growing concern
of privacy and confidentiality of user data within the general public when
their data is processed and stored in an external server which has further
fueled the need for developing such efficient neural networks for real-time
inference on local embedded systems. The scope of our work presented in this
paper is limited to image classification using a convolutional neural network.
A Convolutional Neural Network (CNN) is a class of Deep Neural Network (DNN)
widely used in the analysis of visual images captured by an image sensor,
designed to extract information and convert it into meaningful representations
for real-time inference of the input data. In this paper, we propose a neoteric
variant of deep convolutional neural network architecture to ameliorate the
performance of existing CNN architectures for real-time inference on embedded
systems. We show that this architecture, dubbed CondenseNeXt, is remarkably
efficient in comparison to the baseline neural network architecture,
CondenseNet, by reducing trainable parameters and FLOPs required to train the
network whilst maintaining a balance between the trained model size of less
than 3.0 MB and accuracy trade-off resulting in an unprecedented computational
efficiency.

    

### [[2112.00702] Semi-supervised music emotion recognition using noisy student training and harmonic pitch class profiles](http://arxiv.org/abs/2112.00702)


  We present Mirable's submission to the 2021 Emotions and Themes in Music
challenge. In this work, we intend to address the question: can we leverage
semi-supervised learning techniques on music emotion recognition? With that, we
experiment with noisy student training, which has improved model performance in
the image classification domain. As the noisy student method requires a strong
teacher model, we further delve into the factors including (i) input training
length and (ii) complementary music representations to further boost the
performance of the teacher model. For (i), we find that models trained with
short input length perform better in PR-AUC, whereas those trained with long
input length perform better in ROC-AUC. For (ii), we find that using harmonic
pitch class profiles (HPCP) consistently improve tagging performance, which
suggests that harmonic representation is useful for music emotion tagging.
Finally, we find that noisy student method only improves tagging results for
the case of long training length. Additionally, we find that ensembling
representations trained with different training lengths can improve tagging
results significantly, which suggest a possible direction to explore
incorporating multiple temporal resolutions in the network architecture for
future work.

    

### [[2112.00706] Clustering Mixtures with Almost Optimal Separation in Polynomial Time](http://arxiv.org/abs/2112.00706)


  We consider the problem of clustering mixtures of mean-separated Gaussians in
high dimensions. We are given samples from a mixture of $k$ identity covariance
Gaussians, so that the minimum pairwise distance between any two pairs of means
is at least $\Delta$, for some parameter $\Delta > 0$, and the goal is to
recover the ground truth clustering of these samples. It is folklore that
separation $\Delta = \Theta (\sqrt{\log k})$ is both necessary and sufficient
to recover a good clustering, at least information theoretically. However, the
estimators which achieve this guarantee are inefficient. We give the first
algorithm which runs in polynomial time, and which almost matches this
guarantee. More precisely, we give an algorithm which takes polynomially many
samples and time, and which can successfully recover a good clustering, so long
as the separation is $\Delta = \Omega (\log^{1/2 + c} k)$, for any $c > 0$.
Previously, polynomial time algorithms were only known for this problem when
the separation was polynomial in $k$, and all algorithms which could tolerate
$\textsf{poly}( \log k )$ separation required quasipolynomial time. We also
extend our result to mixtures of translations of a distribution which satisfies
the Poincar inequality, under additional mild assumptions. Our main
technical tool, which we believe is of independent interest, is a novel way to
implicitly represent and estimate high degree moments of a distribution, which
allows us to extract important information about high-degree moments without
ever writing down the full moment tensors explicitly.

    

### [[2112.00723] Infinite Neural Network Quantum States](http://arxiv.org/abs/2112.00723)


  We study infinite limits of neural network quantum states ($\infty$-NNQS),
which exhibit representation power through ensemble statistics, and also
tractable gradient descent dynamics. Ensemble averages of Renyi entropies are
expressed in terms of neural network correlators, and architectures that
exhibit volume-law entanglement are presented. A general framework is developed
for studying the gradient descent dynamics of neural network quantum states
(NNQS), using a quantum state neural tangent kernel (QS-NTK). For $\infty$-NNQS
the training dynamics is simplified, since the QS-NTK becomes deterministic and
constant. An analytic solution is derived for quantum state supervised
learning, which allows an $\infty$-NNQS to recover any target wavefunction.
Numerical experiments on finite and infinite NNQS in the transverse field Ising
model and Fermi Hubbard model demonstrate excellent agreement with theory.
$\infty$-NNQS opens up new opportunities for studying entanglement and training
dynamics in other physics applications, such as in finding ground states.

    

### [[1905.12707] Heterogeneous causal effects with imperfect compliance: a Bayesian machine learning approach](http://arxiv.org/abs/1905.12707)


  This paper introduces an innovative Bayesian machine learning algorithm to
draw interpretable inference on heterogeneous causal effects in the presence of
imperfect compliance (e.g., under an irregular assignment mechanism). We show,
through Monte Carlo simulations, that the proposed Bayesian Causal Forest with
Instrumental Variable (BCF-IV) methodology outperforms other machine learning
techniques tailored for causal inference in discovering and estimating the
heterogeneous causal effects while controlling for the familywise error rate
(or - less stringently - for the false discovery rate) at leaves' level. BCF-IV
sheds a light on the heterogeneity of causal effects in instrumental variable
scenarios and, in turn, provides the policy-makers with a relevant tool for
targeted policies. Its empirical application evaluates the effects of
additional funding on students' performances. The results indicate that BCF-IV
could be used to enhance the effectiveness of school funding on students'
performance.

    

### [[1906.01756] Slack Channels Ecology in Enterprises: How Employees Collaborate Through Group Chat](http://arxiv.org/abs/1906.01756)


  Despite the long history of studying instant messaging usage in
organizations, we know very little about how today's people participate in
group chat channels and interact with others. In this short note, we aim to
update the existing knowledge on how group chat is used in the context of
today's organizations. We have the privilege of collecting a total of 4300
publicly available group chat channels in Slack from an R\&D department in a
multinational IT company. Through qualitative coding of 100 channels, we
identified 9 channel categories such as project based channels and event
channels. We further defined a feature metric with 21 features to depict the
group communication style for these group chat channels, with which we
successfully trained a machine learning model that can automatically classify a
given group channel into one of the 9 categories. In addition, we illustrated
how these communication metrics could be used for analyzing teams'
collaboration activities. We focused on 117 project teams as we have their
performance data, and further collected 54 out of the 117 teams' Slack group
data and generated the communication style metrics for each of them. With these
data, we are able to build a regression model to reveal the relationship
between these group communication styles and one indicator of the project team
performance.

    

### [[1911.02883] Graph Domain Adaptation with Localized Graph Signal Representations](http://arxiv.org/abs/1911.02883)


  In this paper we propose a domain adaptation algorithm designed for graph
domains. Given a source graph with many labeled nodes and a target graph with
few or no labeled nodes, we aim to estimate the target labels by making use of
the similarity between the characteristics of the variation of the label
functions on the two graphs. Our assumption about the source and the target
domains is that the local behaviour of the label function, such as its spread
and speed of variation on the graph, bears resemblance between the two graphs.
We estimate the unknown target labels by solving an optimization problem where
the label information is transferred from the source graph to the target graph
based on the prior that the projections of the label functions onto localized
graph bases be similar between the source and the target graphs. In order to
efficiently capture the local variation of the label functions on the graphs,
spectral graph wavelets are used as the graph bases. Experimentation on various
data sets shows that the proposed method yields quite satisfactory
classification accuracy compared to reference domain adaptation methods.

    

### [[1912.10036] A Family of Deep Learning Architectures for Channel Estimation and Hybrid Beamforming in Multi-Carrier mm-Wave Massive MIMO](http://arxiv.org/abs/1912.10036)


  Hybrid analog and digital beamforming transceivers are instrumental in
addressing the challenge of expensive hardware and high training overheads in
the next generation millimeter-wave (mm-Wave) massive MIMO (multiple-input
multiple-output) systems. However, lack of fully digital beamforming in hybrid
architectures and short coherence times at mm-Wave impose additional
constraints on the channel estimation. Prior works on addressing these
challenges have focused largely on narrowband channels wherein
optimization-based or greedy algorithms were employed to derive hybrid
beamformers. In this paper, we introduce a deep learning (DL) approach for
channel estimation and hybrid beamforming for frequency-selective, wideband
mm-Wave systems. In particular, we consider a massive MIMO Orthogonal Frequency
Division Multiplexing (MIMO-OFDM) system and propose three different DL
frameworks comprising convolutional neural networks (CNNs), which accept the
raw data of received signal as input and yield channel estimates and the hybrid
beamformers at the output. We also introduce both offline and online prediction
schemes. Numerical experiments demonstrate that, compared to the current
state-of-the-art optimization and DL methods, our approach provides higher
spectral efficiency, lesser computational cost and fewer number of pilot
signals, and higher tolerance against the deviations in the received pilot
data, corrupted channel matrix, and propagation environment.

    

### [[2003.01416] An Online Learning Framework for Energy-Efficient Navigation of Electric Vehicles](http://arxiv.org/abs/2003.01416)


  Energy-efficient navigation constitutes an important challenge in electric
vehicles, due to their limited battery capacity. We employ a Bayesian approach
to model the energy consumption at road segments for efficient navigation. In
order to learn the model parameters, we develop an online learning framework
and investigate several exploration strategies such as Thompson Sampling and
Upper Confidence Bound. We then extend our online learning framework to
multi-agent setting, where multiple vehicles adaptively navigate and learn the
parameters of the energy model. We analyze Thompson Sampling and establish
rigorous regret bounds on its performance. Finally, we demonstrate the
performance of our methods via several real-world experiments on Luxembourg
SUMO Traffic dataset.

    

### [[2003.13607] Non-asymptotic Superlinear Convergence of Standard Quasi-Newton Methods](http://arxiv.org/abs/2003.13607)


  In this paper, we study and prove the non-asymptotic superlinear convergence
rate of the Broyden class of quasi-Newton algorithms which includes the
Davidon--Fletcher--Powell (DFP) method and the
Broyden--Fletcher--Goldfarb--Shanno (BFGS) method. The asymptotic superlinear
convergence rate of these quasi-Newton methods has been extensively studied in
the literature, but their explicit finite-time local convergence rate is not
fully investigated. In this paper, we provide a finite-time (non-asymptotic)
convergence analysis for Broyden quasi-Newton algorithms under the assumptions
that the objective function is strongly convex, its gradient is Lipschitz
continuous, and its Hessian is Lipschitz continuous at the optimal solution. We
show that in a local neighborhood of the optimal solution, the iterates
generated by both DFP and BFGS converge to the optimal solution at a
superlinear rate of $(1/k)^{k/2}$, where $k$ is the number of iterations. We
also prove a similar local superlinear convergence result holds for the case
that the objective function is self-concordant. Numerical experiments on
several datasets confirm our explicit convergence rate bounds. Our theoretical
guarantee is one of the first results that provide a non-asymptotic superlinear
convergence rate for quasi-Newton methods.

    

### [[2003.13966] Individual Fairness in Advertising Auctions through Inverse Proportionality](http://arxiv.org/abs/2003.13966)


  Recent empirical work demonstrates that online advertisement can exhibit bias
in the delivery of ads across users even when all advertisers bid in a
non-discriminatory manner. We study the design of ad auctions that, given fair
bids, are guaranteed to produce fair outcomes. Following the works of Dwork and
Ilvento (2019) and Chawla et al. (2020), our goal is to design a truthful
auction that satisfies ``individual fairness'' in its outcomes: informally
speaking, users that are similar to each other should obtain similar
allocations of ads. Within this framework we quantify the tradeoff between
social welfare maximization and fairness.
This work makes two conceptual contributions. First, we express the fairness
constraint as a kind of stability condition: any two users that are assigned
multiplicatively similar values by all the advertisers must receive additively
similar allocations for each advertiser. This value stability constraint is
expressed as a function that maps the multiplicative distance between value
vectors to the maximum allowable $\ell_{\infty}$ distance between the
corresponding allocations. Standard auctions do not satisfy this kind of value
stability.
Second, we introduce a new class of allocation algorithms called Inverse
Proportional Allocation that achieve a near optimal tradeoff between fairness
and social welfare for a broad and expressive class of value stability
conditions. These allocation algorithms are truthful and prior-free, and
achieve a constant factor approximation to the optimal (unconstrained) social
welfare. In particular, the approximation ratio is independent of the number of
advertisers in the system. In this respect, these allocation algorithms greatly
surpass the guarantees achieved in previous work. We also extend our results to
broader notions of fairness that we call subset fairness.

    

### [[2005.08942] Which scaling rule applies to Artificial Neural Networks](http://arxiv.org/abs/2005.08942)


  The experience shows that cooperating and communicating computing systems,
comprising segregated single processors, have severe performance limitations.
In his classic "First Draft" von Neumann warned that using a "too fast
processor" vitiates his simple "procedure" (but not his computing model!);
furthermore, that using the classic computing paradigm for imitating neuronal
operations, is unsound. Amdahl added that large machines, comprising many
processors, have an inherent disadvantage. Given that ANN's components are
heavily communicating with each other, they are built from a large number of
components designed/fabricated for use in conventional computing, furthermore
they attempt to mimic biological operation using improper technological
solutions, their achievable payload computing performance is conceptually
modest. The type of workload that AI-based systems generate leads to an
exceptionally low payload computational performance, and their
design/technology limits their size to just above the "toy" level systems: the
scaling of processor-based ANN systems is strongly nonlinear. Given the
proliferation and growing size of ANN systems, we suggest ideas to estimate in
advance the efficiency of the device or application. Through analyzing
published measurements we provide evidence that the role of data transfer time
drastically influences both ANNs performance and feasibility. It is discussed
how some major theoretical limiting factors, ANN's layer structure and their
methods of technical implementation of communication affect their efficiency.
The paper starts from von Neumann's original model, without neglecting the
transfer time apart from processing time; derives an appropriate interpretation
and handling for Amdahl's law. It shows that, in that interpretation, Amdahl's
Law correctly describes ANNs.

    

### [[2007.02931] Adaptive Risk Minimization: Learning to Adapt to Domain Shift](http://arxiv.org/abs/2007.02931)


  A fundamental assumption of most machine learning algorithms is that the
training and test data are drawn from the same underlying distribution.
However, this assumption is violated in almost all practical applications:
machine learning systems are regularly tested under distribution shift, due to
changing temporal correlations, atypical end users, or other factors. In this
work, we consider the problem setting of domain generalization, where the
training data are structured into domains and there may be multiple test time
shifts, corresponding to new domains or domain distributions. Most prior
methods aim to learn a single robust model or invariant feature space that
performs well on all domains. In contrast, we aim to learn models that adapt at
test time to domain shift using unlabeled test points. Our primary contribution
is to introduce the framework of adaptive risk minimization (ARM), in which
models are directly optimized for effective adaptation to shift by learning to
adapt on the training domains. Compared to prior methods for robustness,
invariance, and adaptation, ARM methods provide performance gains of 1-4% test
accuracy on a number of image classification problems exhibiting domain shift.

    

### [[2007.14390] Flower: A Friendly Federated Learning Research Framework](http://arxiv.org/abs/2007.14390)


  Federated Learning (FL) has emerged as a promising technique for edge devices
to collaboratively learn a shared prediction model, while keeping their
training data on the device, thereby decoupling the ability to do machine
learning from the need to store the data in the cloud. However, FL is difficult
to implement realistically, both in terms of scale and systems heterogeneity.
Although there are a number of research frameworks available to simulate FL
algorithms, they do not support the study of scalable FL workloads on
heterogeneous edge devices.
In this paper, we present Flower -- a comprehensive FL framework that
distinguishes itself from existing platforms by offering new facilities to
execute large-scale FL experiments and consider richly heterogeneous FL device
scenarios. Our experiments show Flower can perform FL experiments up to 15M in
client size using only a pair of high-end GPUs. Researchers can then seamlessly
migrate experiments to real devices to examine other parts of the design space.
We believe Flower provides the community with a critical new tool for FL study
and development.

    

### [[2007.14823] Theory of gating in recurrent neural networks](http://arxiv.org/abs/2007.14823)


  Recurrent neural networks (RNNs) are powerful dynamical models, widely used
in machine learning (ML) and neuroscience. Prior theoretical work has focused
on RNNs with additive interactions. However, gating - i.e. multiplicative -
interactions are ubiquitous in real neurons and also the central feature of the
best-performing RNNs in ML. Here, we show that gating offers flexible control
of two salient features of the collective dynamics: i) timescales and ii)
dimensionality. The gate controlling timescales leads to a novel, marginally
stable state, where the network functions as a flexible integrator. Unlike
previous approaches, gating permits this important function without parameter
fine-tuning or special symmetries. Gates also provide a flexible,
context-dependent mechanism to reset the memory trace, thus complementing the
memory function. The gate modulating the dimensionality can induce a novel,
discontinuous chaotic transition, where inputs push a stable system to strong
chaotic activity, in contrast to the typically stabilizing effect of inputs. At
this transition, unlike additive RNNs, the proliferation of critical points
(topological complexity) is decoupled from the appearance of chaotic dynamics
(dynamical complexity).
The rich dynamics are summarized in phase diagrams, thus providing a map for
principled parameter initialization choices to ML practitioners.

    

### [[2008.00742] Collaborative Learning in the Jungle (Decentralized, Byzantine, Heterogeneous, Asynchronous and Nonconvex Learning)](http://arxiv.org/abs/2008.00742)


  We study Byzantine collaborative learning, where $n$ nodes seek to
collectively learn from each others' local data. The data distribution may vary
from one node to another. No node is trusted, and $f < n$ nodes can behave
arbitrarily. We prove that collaborative learning is equivalent to a new form
of agreement, which we call averaging agreement. In this problem, nodes start
each with an initial vector and seek to approximately agree on a common vector,
which is close to the average of honest nodes' initial vectors. We present two
asynchronous solutions to averaging agreement, each we prove optimal according
to some dimension. The first, based on the minimum-diameter averaging, requires
$ n \geq 6f+1$, but achieves asymptotically the best-possible averaging
constant up to a multiplicative constant. The second, based on reliable
broadcast and coordinate-wise trimmed mean, achieves optimal Byzantine
resilience, i.e., $n \geq 3f+1$. Each of these algorithms induces an optimal
Byzantine collaborative learning protocol. In particular, our equivalence
yields new impossibility theorems on what any collaborative learning algorithm
can achieve in adversarial and heterogeneous environments.

    

### [[2008.07527] Music Boundary Detection using Convolutional Neural Networks: A comparative analysis of combined input features](http://arxiv.org/abs/2008.07527)


  The analysis of the structure of musical pieces is a task that remains a
challenge for Artificial Intelligence, especially in the field of Deep
Learning. It requires prior identification of structural boundaries of the
music pieces. This structural boundary analysis has recently been studied with
unsupervised methods and \textit{end-to-end} techniques such as Convolutional
Neural Networks (CNN) using Mel-Scaled Log-magnitude Spectograms features
(MLS), Self-Similarity Matrices (SSM) or Self-Similarity Lag Matrices (SSLM) as
inputs and trained with human annotations. Several studies have been published
divided into unsupervised and \textit{end-to-end} methods in which
pre-processing is done in different ways, using different distance metrics and
audio characteristics, so a generalized pre-processing method to compute model
inputs is missing. The objective of this work is to establish a general method
of pre-processing these inputs by comparing the inputs calculated from
different pooling strategies, distance metrics and audio characteristics, also
taking into account the computing time to obtain them. We also establish the
most effective combination of inputs to be delivered to the CNN in order to
establish the most efficient way to extract the limits of the structure of the
music pieces. With an adequate combination of input matrices and pooling
strategies we obtain a measurement accuracy $F_1$ of 0.411 that outperforms the
current one obtained under the same conditions.

    

### [[2009.09899] Clustering COVID-19 Lung Scans](http://arxiv.org/abs/2009.09899)


  With the ongoing COVID-19 pandemic, understanding the characteristics of the
virus has become an important and challenging task in the scientific community.
While tests do exist for COVID-19, the goal of our research is to explore other
methods of identifying infected individuals. Our group applied unsupervised
clustering techniques to explore a dataset of lungscans of COVID-19 infected,
Viral Pneumonia infected, and healthy individuals. This is an important area to
explore as COVID-19 is a novel disease that is currently being studied in
detail. Our methodology explores the potential that unsupervised clustering
algorithms have to reveal important hidden differences between COVID-19 and
other respiratory illnesses. Our experiments use: Principal Component Analysis
(PCA), K-Means++ (KM++) and the recently developed Robust Continuous Clustering
algorithm (RCC). We evaluate the performance of KM++ and RCC in clustering
COVID-19 lung scans using the Adjusted Mutual Information (AMI) score.

    

### [[2010.09453] Fast accuracy estimation of deep learning based multi-class musical source separation](http://arxiv.org/abs/2010.09453)


  Music source separation represents the task of extracting all the instruments
from a given song. Recent breakthroughs on this challenge have gravitated
around a single dataset, MUSDB, only limited to four instrument classes. Larger
datasets and more instruments are costly and time-consuming in collecting data
and training deep neural networks (DNNs). In this work, we propose a fast
method to evaluate the separability of instruments in any dataset without
training and tuning a DNN. This separability measure helps to select
appropriate samples for the efficient training of neural networks. Based on the
oracle principle with an ideal ratio mask, our approach is an excellent proxy
to estimate the separation performances of state-of-the-art deep learning
approaches such as TasNet or Open-Unmix. Our results contribute to revealing
two essential points for audio source separation: 1) the ideal ratio mask,
although light and straightforward, provides an accurate measure of the audio
separability performance of recent neural nets, and 2) new end-to-end learning
methods such as Tasnet, that operate directly on waveforms, are, in fact,
internally building a Time-Frequency (TF) representation, so that they
encounter the same limitations as the TF based-methods when separating audio
pattern overlapping in the TF plane.

    

### [[2010.12751] Model Extraction Attacks on Graph Neural Networks: Taxonomy and Realization](http://arxiv.org/abs/2010.12751)


  Machine learning models are shown to face a severe threat from Model
Extraction Attacks, where a well-trained private model owned by a service
provider can be stolen by an attacker pretending as a client. Unfortunately,
prior works focus on the models trained over the Euclidean space, e.g., images
and texts, while how to extract a GNN model that contains a graph structure and
node features is yet to be explored. In this paper, for the first time, we
comprehensively investigate and develop model extraction attacks against GNN
models. We first systematically formalise the threat modelling in the context
of GNN model extraction and classify the adversarial threats into seven
categories by considering different background knowledge of the attacker, e.g.,
attributes and/or neighbour connections of the nodes obtained by the attacker.
Then we present detailed methods which utilise the accessible knowledge in each
threat to implement the attacks. By evaluating over three real-world datasets,
our attacks are shown to extract duplicated models effectively, i.e., 84% - 89%
of the inputs in the target domain have the same output predictions as the
victim model.

    

### [[2011.06782] A Nested Bi-level Optimization Framework for Robust Few Shot Learning](http://arxiv.org/abs/2011.06782)


  Model-Agnostic Meta-Learning (MAML), a popular gradient-based meta-learning
framework, assumes that the contribution of each task or instance to the
meta-learner is equal. Hence, it fails to address the domain shift between base
and novel classes in few-shot learning. In this work, we propose a novel robust
meta-learning algorithm, NestedMAML, which learns to assign weights to training
tasks or instances. We consider weights as hyper-parameters and iteratively
optimize them using a small set of validation tasks set in a nested bi-level
optimization approach (in contrast to the standard bi-level optimization in
MAML). We then apply NestedMAML in the meta-training stage, which involves (1)
several tasks sampled from a distribution different from the meta-test task
distribution, or (2) some data samples with noisy labels. Extensive experiments
on synthetic and real-world datasets demonstrate that NestedMAML efficiently
mitigates the effects of "unwanted" tasks or instances, leading to significant
improvement over the state-of-the-art robust meta-learning methods.

    

### [[2011.15014] Learning from Human Directional Corrections](http://arxiv.org/abs/2011.15014)


  This paper proposes an approach which enables a robot to learn a control
objective function incrementally from human's directional corrections. Existing
methods learn from human's magnitude corrections and require a human to
carefully choose correction magnitudes, which otherwise can easily lead to
over-correction and learning inefficiency. The proposed method only requires
human's directional corrections -- corrections that only indicate the direction
of a control change without indicating its magnitude -- applied at some time
instances during the robot's motion. We only assume that each of human's
corrections, regardless of its magnitude, points in a direction that improves
the robot's current motion relative to an implicit control objective function.
Thus, human's valid corrections always account for half of the correction
space. The proposed method uses the direction of a correction to update the
estimate of the objective function based on a cutting plane technique. We have
established the theoretical results to show that this process guarantees the
convergence of the learned objective function to the implicit one. The proposed
approach has been examined by numerical examples, a user study on two
human-robot games, and a real-world quadrotor experiment. The results confirm
the convergence of the approach and show that the approach is significantly
more effective (higher success rate), efficient/effortless (less human
corrections needed), and accessible (fewer early wasted trials) than the
state-of-the-art robot interactive learning schemes.

    

### [[2012.03646] A novel dataset for the identification of computer generated melodies in the CSMT challenge](http://arxiv.org/abs/2012.03646)


  In this paper, the dataset used for the data challenge organised by
Conference on Sound and Music Technology (CSMT) is introduced. The CSMT data
challenge requires participants to identify whether a given piece of melody is
generated by computer or is composed by human. The dataset is formed by two
parts: development dataset and evaluation dataset. The development dataset
contains only computer generated melodies whereas the evaluation dataset
contain both computer generated melodies and human composed melodies. The aim
of the dataset is to examine whether it is possible to distinguish computer
generated melodies by learning the feature of generated melodies.

    

### [[2101.11970] AHMoSe: A Knowledge-Based Visual Support System for Selecting Regression Machine Learning Models](http://arxiv.org/abs/2101.11970)


  Decision support systems have become increasingly popular in the domain of
agriculture. With the development of automated machine learning, agricultural
experts are now able to train, evaluate and make predictions using cutting edge
machine learning (ML) models without the need for much ML knowledge. Although
this automated approach has led to successful results in many scenarios, in
certain cases (e.g., when few labeled datasets are available) choosing among
different models with similar performance metrics is a difficult task.
Furthermore, these systems do not commonly allow users to incorporate their
domain knowledge that could facilitate the task of model selection, and to gain
insight into the prediction system for eventual decision making. To address
these issues, in this paper we present AHMoSe, a visual support system that
allows domain experts to better understand, diagnose and compare different
regression models, primarily by enriching model-agnostic explanations with
domain knowledge. To validate AHMoSe, we describe a use case scenario in the
viticulture domain, grape quality prediction, where the system enables users to
diagnose and select prediction models that perform better. We also discuss
feedback concerning the design of the tool from both ML and viticulture
experts.

    

### [[2103.09577] Theoretical bounds on data requirements for the ray-based classification](http://arxiv.org/abs/2103.09577)


  The problem of classifying high-dimensional shapes in real-world data grows
in complexity as the dimension of the space increases. For the case of
identifying convex shapes of different geometries, a new classification
framework has recently been proposed in which the intersections of a set of
one-dimensional representations, called rays, with the boundaries of the shape
are used to identify the specific geometry. This ray-based classification (RBC)
has been empirically verified using a synthetic dataset of two- and
three-dimensional shapes (Zwolak et al. in Proceedings of Third Workshop on
Machine Learning and the Physical Sciences (NeurIPS 2020), Vancouver, Canada
[December 11, 2020], arXiv:2010.00500, 2020) and, more recently, has also been
validated experimentally (Zwolak et al., PRX Quantum 2:020335, 2021). Here, we
establish a bound on the number of rays necessary for shape classification,
defined by key angular metrics, for arbitrary convex shapes. For two
dimensions, we derive a lower bound on the number of rays in terms of the
shape's length, diameter, and exterior angles. For convex polytopes in
$\mathbb{R}^N$, we generalize this result to a similar bound given as a
function of the dihedral angle and the geometrical parameters of polygonal
faces. This result enables a different approach for estimating high-dimensional
shapes using substantially fewer data elements than volumetric or surface-based
approaches.

    

### [[2104.01632] Isconna: Streaming Anomaly Detection with Frequency and Patterns](http://arxiv.org/abs/2104.01632)


  An edge stream is a common form of presentation of dynamic networks. It can
evolve with time, with new types of nodes or edges being continuously added.
Existing methods for anomaly detection rely on edge occurrence counts or
compare pattern snippets found in historical records. In this work, we propose
Isconna, which focuses on both the frequency and the pattern of edge records.
The burst detection component targets anomalies between individual timestamps,
while the pattern detection component highlights anomalies across segments of
timestamps. These two components together produce three intermediate scores,
which are aggregated into the final anomaly score. Isconna does not actively
explore or maintain pattern snippets; it instead measures the consecutive
presence and absence of edge records. Isconna is an online algorithm, it does
not keep the original information of edge records; only statistical values are
maintained in a few count-min sketches (CMS). Isconna's space complexity
$O(rc)$ is determined by two user-specific parameters, the size of CMSs. In
worst case, Isconna's time complexity can be up to $O(rc)$, but it can be
amortized in practice. Experiments show that Isconna outperforms five
state-of-the-art frequency- and/or pattern-based baselines on six real-world
datasets with up to 20 million edge records.

    

### [[2105.04738] Lightweight Distributed Gaussian Process Regression for Online Machine Learning](http://arxiv.org/abs/2105.04738)


  In this paper, we study the problem where a group of agents aim to
collaboratively learn a common static latent function through streaming data.
We propose a lightweight distributed Gaussian process regression (GPR)
algorithm that is cognizant of agents' limited capabilities in communication,
computation and memory. Each agent independently runs agent-based GPR using
local streaming data to predict test points of interest; then the agents
collaboratively execute distributed GPR to obtain global predictions over a
common sparse set of test points; finally, each agent fuses results from
distributed GPR with agent-based GPR to refine its predictions. By quantifying
the transient and steady-state performances in predictive variance and error,
we show that limited inter-agent communication improves learning performances
in the sense of Pareto. Monte Carlo simulation is conducted to evaluate the
developed algorithm.

    

### [[2105.09163] High-Performance FPGA-based Accelerator for Bayesian Neural Networks](http://arxiv.org/abs/2105.09163)


  Neural networks (NNs) have demonstrated their potential in a wide range of
applications such as image recognition, decision making or recommendation
systems. However, standard NNs are unable to capture their model uncertainty
which is crucial for many safety-critical applications including healthcare and
autonomous vehicles. In comparison, Bayesian neural networks (BNNs) are able to
express uncertainty in their prediction via a mathematical grounding.
Nevertheless, BNNs have not been as widely used in industrial practice, mainly
because of their expensive computational cost and limited hardware performance.
This work proposes a novel FPGA-based hardware architecture to accelerate BNNs
inferred through Monte Carlo Dropout. Compared with other state-of-the-art BNN
accelerators, the proposed accelerator can achieve up to 4 times higher energy
efficiency and 9 times better compute efficiency. Considering partial Bayesian
inference, an automatic framework is proposed, which explores the trade-off
between hardware and algorithmic performance. Extensive experiments are
conducted to demonstrate that our proposed framework can effectively find the
optimal points in the design space.

    

### [[2105.11521] Deep neural network enabled corrective source term approach to hybrid analysis and modeling](http://arxiv.org/abs/2105.11521)


  In this work, we introduce, justify and demonstrate the Corrective Source
Term Approach (CoSTA) -- a novel approach to Hybrid Analysis and Modeling
(HAM). The objective of HAM is to combine physics-based modeling (PBM) and
data-driven modeling (DDM) to create generalizable, trustworthy, accurate,
computationally efficient and self-evolving models. CoSTA achieves this
objective by augmenting the governing equation of a PBM model with a corrective
source term generated using a deep neural network. In a series of numerical
experiments on one-dimensional heat diffusion, CoSTA is found to outperform
comparable DDM and PBM models in terms of accuracy -- often reducing predictive
errors by several orders of magnitude -- while also generalizing better than
pure DDM. Due to its flexible but solid theoretical foundation, CoSTA provides
a modular framework for leveraging novel developments within both PBM and DDM.
Its theoretical foundation also ensures that CoSTA can be used to model any
system governed by (deterministic) partial differential equations. Moreover,
CoSTA facilitates interpretation of the DNN-generated source term within the
context of PBM, which results in improved explainability of the DNN. These
factors make CoSTA a potential door-opener for data-driven techniques to enter
high-stakes applications previously reserved for pure PBM.

    

### [[2105.14656] Human-level COVID-19 Diagnosis from Low-dose CT Scans Using a Two-stage Time-distributed Capsule Network](http://arxiv.org/abs/2105.14656)


  Reverse transcription-polymerase chain reaction (RT-PCR) is currently the
gold standard in COVID-19 diagnosis. It can, however, take days to provide the
diagnosis, and false negative rate is relatively high. Imaging, in particular
chest computed tomography (CT), can assist with diagnosis and assessment of
this disease. Nevertheless, it is shown that standard dose CT scan gives
significant radiation burden to patients, especially those in need of multiple
scans. In this study, we consider low-dose and ultra-low-dose (LDCT and ULDCT)
scan protocols that reduce the radiation exposure close to that of a single
X-Ray, while maintaining an acceptable resolution for diagnosis purposes. Since
thoracic radiology expertise may not be widely available during the pandemic,
we develop an Artificial Intelligence (AI)-based framework using a collected
dataset of LDCT/ULDCT scans, to study the hypothesis that the AI model can
provide human-level performance. The AI model uses a two stage capsule network
architecture and can rapidly classify COVID-19, community acquired pneumonia
(CAP), and normal cases, using LDCT/ULDCT scans. The AI model achieves COVID-19
sensitivity of 89.5% +\- 0.11, CAP sensitivity of 95% +\- 0.11, normal cases
sensitivity (specificity) of 85.7% +\- 0.16, and accuracy of 90% +\- 0.06. By
incorporating clinical data (demographic and symptoms), the performance further
improves to COVID-19 sensitivity of 94.3% +\- pm 0.05, CAP sensitivity of 96.7%
+\- 0.07, normal cases sensitivity (specificity) of 91% +\- 0.09 , and accuracy
of 94.1% +\- 0.03. The proposed AI model achieves human-level diagnosis based
on the LDCT/ULDCT scans with reduced radiation exposure. We believe that the
proposed AI model has the potential to assist the radiologists to accurately
and promptly diagnose COVID-19 infection and help control the transmission
chain during the pandemic.

    

### [[2105.15168] MSG-Transformer: Exchanging Local Spatial Information by Manipulating Messenger Tokens](http://arxiv.org/abs/2105.15168)


  Transformers have offered a new methodology of designing neural networks for
visual recognition. Compared to convolutional networks, Transformers enjoy the
ability of referring to global features at each stage, yet the attention module
brings higher computational overhead that obstructs the application of
Transformers to process high-resolution visual data. This paper aims to
alleviate the conflict between efficiency and flexibility, for which we propose
a specialized token for each region that serves as a messenger (MSG). Hence, by
manipulating these MSG tokens, one can flexibly exchange visual information
across regions and the computational complexity is reduced. We then integrate
the MSG token into a multi-scale architecture named MSG-Transformer. In
standard image classification and object detection, MSG-Transformer achieves
competitive performance and the inference on both GPU and CPU is accelerated.
Code is available at this https URL.

    

### [[2106.00774] Optimizing Functionals on the Space of Probabilities with Input Convex Neural Networks](http://arxiv.org/abs/2106.00774)


  Gradient flows are a powerful tool for optimizing functionals in general
metric spaces, including the space of probabilities endowed with the
Wasserstein metric. A typical approach to solving this optimization problem
relies on its connection to the dynamic formulation of optimal transport and
the celebrated Jordan-Kinderlehrer-Otto (JKO) scheme. However, this formulation
involves optimization over convex functions, which is challenging, especially
in high dimensions. In this work, we propose an approach that relies on the
recently introduced input-convex neural networks (ICNN) to parametrize the
space of convex functions in order to approximate the JKO scheme, as well as in
designing functionals over measures that enjoy convergence guarantees. We
derive a computationally efficient implementation of this JKO-ICNN framework
and experimentally demonstrate its feasibility and validity in approximating
solutions of low-dimensional partial differential equations with known
solutions. We also demonstrate its viability in high-dimensional applications
through an experiment in controlled generation for molecular discovery.

    

### [[2106.03996] Lessons learned developing and using a machine learning model to automatically transcribe 2.3 million handwritten occupation codes](http://arxiv.org/abs/2106.03996)


  Machine learning approaches achieve high accuracy for text recognition and
are therefore increasingly used for the transcription of handwritten historical
sources. However, using machine learning in production requires a streamlined
end-to-end pipeline that scales to the dataset size and a model that achieves
high accuracy with few manual transcriptions. The correctness of the model
results must also be verified. This paper describes our lessons learned
developing, tuning and using the Occode end-to-end machine learning pipeline
for transcribing 2.3 million handwritten occupation codes from the Norwegian
1950 population census. We achieve an accuracy of 97% for the automatically
transcribed codes, and we send 3% of the codes for manual verification. We
verify that the occupation code distribution found in our results matches the
distribution found in our training data, which should be representative for the
census as a whole. We believe our approach and lessons learned may be useful
for other transcription projects that plan to use machine learning in
production. The source code is available at:
this https URL


### [[2106.07995] Learning of feature points without additional supervision improves reinforcement learning from images](http://arxiv.org/abs/2106.07995)


  In many control problems that include vision, optimal controls can be
inferred from the location of the objects in the scene. This information can be
represented using feature points, which is a list of spatial locations in
learned feature maps of an input image. Previous works show that feature points
learned using unsupervised pre-training or human supervision can provide good
features for control tasks. In this paper, we show that it is possible to learn
efficient feature point representations end-to-end, without the need for
unsupervised pre-training, decoders, or additional losses. Our proposed
architecture consists of a differentiable feature point extractor that feeds
the coordinates of the estimated feature points directly to a soft actor-critic
agent. The proposed algorithm yields performance competitive to the
state-of-the art on DeepMind Control Suite tasks.

    

### [[2106.12066] It's All in the Heads: Using Attention Heads as a Baseline for Cross-Lingual Transfer in Commonsense Reasoning](http://arxiv.org/abs/2106.12066)


  Commonsense reasoning is one of the key problems in natural language
processing, but the relative scarcity of labeled data holds back the progress
for languages other than English. Pretrained cross-lingual models are a source
of powerful language-agnostic representations, yet their inherent reasoning
capabilities are still actively studied. In this work, we design a simple
approach to commonsense reasoning which trains a linear classifier with weights
of multi-head attention as features. To evaluate this approach, we create a
multilingual Winograd Schema corpus by processing several datasets from prior
work within a standardized pipeline and measure cross-lingual generalization
ability in terms of out-of-sample performance. The method performs
competitively with recent supervised and unsupervised approaches for
commonsense reasoning, even when applied to other languages in a zero-shot
manner. Also, we demonstrate that most of the performance is given by the same
small subset of attention heads for all studied languages, which provides
evidence of universal reasoning capabilities in multilingual encoders.

    

### [[2106.15860] Understanding Adversarial Attacks on Observations in Deep Reinforcement Learning](http://arxiv.org/abs/2106.15860)


  Deep reinforcement learning models are vulnerable to adversarial attacks that
can decrease a victim's cumulative expected reward by manipulating the victim's
observations. Despite the efficiency of previous optimization-based methods for
generating adversarial noise in supervised learning, such methods might not be
able to achieve the lowest cumulative reward since they do not explore the
environmental dynamics in general. In this paper, we provide a framework to
better understand the existing methods by reformulating the problem of
adversarial attacks on reinforcement learning in the function space. Our
reformulation generates an optimal adversary in the function space of the
targeted attacks, repelling them via a generic two-stage framework. In the
first stage, we train a deceptive policy by hacking the environment, and
discover a set of trajectories routing to the lowest reward or the worst-case
performance. Next, the adversary misleads the victim to imitate the deceptive
policy by perturbing the observations. Compared to existing approaches, we
theoretically show that our adversary is stronger under an appropriate noise
level. Extensive experiments demonstrate our method's superiority in terms of
efficiency and effectiveness, achieving the state-of-the-art performance in
both Atari and MuJoCo environments.

    

### [[2110.08721] CAE-Transformer: Transformer-based Model to Predict Invasiveness of Lung Adenocarcinoma Subsolid Nodules from Non-thin Section 3D CT Scans](http://arxiv.org/abs/2110.08721)


  Lung cancer is the leading cause of mortality from cancer worldwide and has
various histologic types, among which Lung Adenocarcinoma (LUAC) has recently
been the most prevalent. Lung adenocarcinomas are classified as pre invasive,
minimally invasive, and invasive adenocarcinomas. Timely and accurate knowledge
of the invasiveness of lung nodules leads to a proper treatment plan and
reduces the risk of unnecessary or late surgeries. Currently, the primary
imaging modality to assess and predict the invasiveness of LUACs is the chest
CT. The results based on CT images, however, are subjective and suffer from a
low accuracy compared to the ground truth pathological reviews provided after
surgical resections. In this paper, a predictive transformer-based framework,
referred to as the "CAE-Transformer", is developed to classify LUACs. The
CAE-Transformer utilizes a Convolutional Auto-Encoder (CAE) to automatically
extract informative features from CT slices, which are then fed to a modified
transformer model to capture global inter-slice relations. Experimental results
on our in-house dataset of 114 pathologically proven Sub Solid Nodules (SSNs)
demonstrate the superiority of the CAE-Transformer over the
histogram/radiomics-based models and its deep learning-based counterparts,
achieving an accuracy of 87.73%, sensitivity of 88.67%, specificity of 86.33%,
and AUC of 0.913, using a 10-fold cross-validation.

    

### [[2110.08851] Unsupervised Representation Learning for Binary Networks by Joint Classifier Learning](http://arxiv.org/abs/2110.08851)


  Self-supervised learning is a promising unsupervised learning framework that
has achieved success with large floating point networks. But such networks are
not readily deployable to edge devices. To accelerate deployment of models with
the benefit of unsupervised representation learning to such resource limited
devices for various downstream tasks, we propose a self-supervised learning
method for binary networks that uses a moving target network. In particular, we
propose to jointly train a randomly initialized classifier, attached to a
pretrained floating point feature extractor, with a binary network.
Additionally, we propose a feature similarity loss, a dynamic loss balancing
and modified multi-stage training to further improve the accuracy, and call our
method BURN. Our empirical validations over five downstream tasks using seven
datasets show that BURN outperforms self-supervised baselines for binary
networks and sometimes outperforms supervised pretraining.

    

### [[2110.09193] Topologically Regularized Data Embeddings](http://arxiv.org/abs/2110.09193)


  Unsupervised feature learning often finds low-dimensional embeddings that
capture the structure of complex data. For tasks for which expert prior
topological knowledge is available, incorporating this into the learned
representation may lead to higher quality embeddings. For example, this may
help one to embed the data into a given number of clusters, or to accommodate
for noise that prevents one from deriving the distribution of the data over the
model directly, which can then be learned more effectively. However, a general
tool for integrating different prior topological knowledge into embeddings is
lacking. Although differentiable topology layers have been recently developed
that can (re)shape embeddings into prespecified topological models, they have
two important limitations for representation learning, which we address in this
paper. First, the currently suggested topological losses fail to represent
simple models such as clusters and flares in a natural manner. Second, these
losses neglect all original structural (such as neighborhood) information in
the data that is useful for learning. We overcome these limitations by
introducing a new set of topological losses, and proposing their usage as a way
for topologically regularizing data embeddings to naturally represent a
prespecified model. We include thorough experiments on synthetic and real data
that highlight the usefulness and versatility of this approach, with
applications ranging from modeling high-dimensional single cell data, to graph
embedding.

    

### [[2110.12667] Mixture-of-Variational-Experts for Continual Learning](http://arxiv.org/abs/2110.12667)


  One significant shortcoming of machine learning is the poor ability of models
to solve new problems quicker and without forgetting acquired knowledge. To
better understand this issue, continual learning has emerged to systematically
investigate learning protocols where the model sequentially observes samples
generated by a series of tasks. First, we propose an optimality principle that
facilitates a trade-off between learning and forgetting. We derive this
principle from an information-theoretic formulation of bounded rationality and
show its connections to other continual learning methods. Second, based on this
principle, we propose a neural network layer for continual learning, called
Mixture-of-Variational-Experts (MoVE), that alleviates forgetting while
enabling the beneficial transfer of knowledge to new tasks. Our experiments on
variants of the MNIST and CIFAR10 datasets demonstrate the competitive
performance of MoVE layers when compared to state-of-the-art approaches.

    

### [[2111.00765] Validate on Sim, Detect on Real -- Model Selection for Domain Randomization](http://arxiv.org/abs/2111.00765)


  A practical approach to learning robot skills, often termed sim2real, is to
train control policies in simulation and then deploy them on a real robot.
Popular techniques to improve the sim2real transfer build on domain
randomization (DR): Training the policy on a diverse set of randomly generated
domains with the hope of better generalization to the real world. Due to the
large number of hyper-parameters in both the policy learning and DR algorithms,
one often ends up with a large number of trained models, where choosing the
best model among them demands costly evaluation on the real robot. In this work
we ask: Can we rank the policies without running them in the real world? Our
main idea is that a predefined set of real world data can be used to evaluate
all policies, using out-of-distribution detection (OOD) techniques. In a sense,
this approach can be seen as a "unit test" to evaluate policies before any real
world execution. However, we find that by itself, the OOD score can be
inaccurate and very sensitive to the particular OOD method. Our main
contribution is a simple-yet-effective policy score that combines OOD with an
evaluation in simulation. We show that our score - VSDR - can significantly
improve the accuracy of policy ranking without requiring additional real world
data. We evaluate the effectiveness of VSDR on sim2real transfer in a robotic
grasping task with image inputs. We extensively evaluate different DR
parameters and OOD methods, and show that VSDR improves policy selection across
the board. More importantly, our method achieves significantly better ranking,
and uses significantly less data compared to baselines.

    

### [[2111.11294] Scaling Law for Recommendation Models: Towards General-purpose User Representations](http://arxiv.org/abs/2111.11294)


  A recent trend shows that a general class of models, e.g., BERT, GPT-3, CLIP,
trained on broad data at scale have shown a great variety of functionalities
with a single learning architecture. In this work, we explore the possibility
of general-purpose user representation learning by training a universal user
encoder at large scales. We demonstrate that the scaling law holds in the user
modeling areas, where the training error scales as a power-law with the amount
of compute. Our Contrastive Learning User Encoder (CLUE), optimizes
task-agnostic objectives, and the resulting user embeddings stretches our
expectation of what is possible to do in various downstream tasks. CLUE also
shows great transferability to other domains and systems, as performances on an
online experiment shows significant improvements in online Click-Through-Rate
(CTR). Furthermore, we also investigate how the performance changes according
to the scale-up factors, i.e., model capacity, sequence length and batch size.
Finally, we discuss the broader impacts of CLUE in general.

    

### [[2111.11305] Universal Efficient Variable-rate Neural Image Compression](http://arxiv.org/abs/2111.11305)


  Recently, Learning-based image compression has reached comparable performance
with traditional image codecs(such as JPEG, BPG, WebP). However, computational
complexity and rate flexibility are still two major challenges for its
practical deployment. To tackle these problems, this paper proposes two
universal modules named Energy-based Channel Gating(ECG) and Bit-rate
Modulator(BM), which can be directly embedded into existing end-to-end image
compression models. ECG uses dynamic pruning to reduce FLOPs for more than 50\%
in convolution layers, and a BM pair can modulate the latent representation to
control the bit-rate in a channel-wise manner. By implementing these two
modules, existing learning-based image codecs can obtain ability to output
arbitrary bit-rate with a single model and reduced computation.

    

### [[2111.14347] Efficient Federated Learning for AIoT Applications Using Knowledge Distillation](http://arxiv.org/abs/2111.14347)


  As a promising distributed machine learning paradigm, Federated Learning (FL)
trains a central model with decentralized data without compromising user
privacy, which has made it widely used by Artificial Intelligence Internet of
Things (AIoT) applications. However, the traditional FL suffers from model
inaccuracy since it trains local models using hard labels of data and ignores
useful information of incorrect predictions with small probabilities. Although
various solutions try to tackle the bottleneck of the traditional FL, most of
them introduce significant communication and memory overhead, making the
deployment of large-scale AIoT devices a great challenge. To address the above
problem, this paper presents a novel Distillation-based Federated Learning
(DFL) architecture that enables efficient and accurate FL for AIoT
applications. Inspired by Knowledge Distillation (KD) that can increase the
model accuracy, our approach adds the soft targets used by KD to the FL model
training, which occupies negligible network resources. The soft targets are
generated by local sample predictions of each AIoT device after each round of
local training and used for the next round of model training. During the local
training of DFL, both soft targets and hard labels are used as approximation
objectives of model predictions to improve model accuracy by supplementing the
knowledge of soft targets. To further improve the performance of our DFL model,
we design a dynamic adjustment strategy for tuning the ratio of two loss
functions used in KD, which can maximize the use of both soft targets and hard
labels. Comprehensive experimental results on well-known benchmarks show that
our approach can significantly improve the model accuracy of FL with both
Independent and Identically Distributed (IID) and non-IID data.

    

### [[2112.00117] CIDAN: Computing in DRAM with\\Artificial Neurons](http://arxiv.org/abs/2112.00117)


  Numerous applications such as graph processing, cryptography, databases,
bioinformatics, etc., involve the repeated evaluation of Boolean functions on
large bit vectors. In-memory architectures which perform processing in memory
(PIM) are tailored for such applications. This paper describes a different
architecture for in-memory computation called CIDAN, that achieves a 3X
improvement in performance and a 2X improvement in energy for a representative
set of algorithms over the state-of-the-art in-memory architectures. CIDAN uses
a new basic processing element called a TLPE, which comprises a threshold logic
gate (TLG) (a.k.a artificial neuron or perceptron). The implementation of a TLG
within a TLPE is equivalent to a multi-input, edge-triggered flipflop that
computes a subset of threshold functions of its inputs. The specific threshold
function is selected on each cycle by enabling/disabling a subset of the
weights associated with the threshold function, by using logic signals. In
addition to the TLG, a TLPE realizes some non-threshold functions by a sequence
of TLG evaluations. An equivalent CMOS implementation of a TLPE requires a
substantially higher area and power. CIDAN has an array of TLPE(s) that is
integrated with a DRAM, to allow fast evaluation of any one of its set of
functions on large bit vectors. Results of running several common in-memory
applications in graph processing and cryptography are presented.

    

### [[2112.00142] ZCSD: a Computational Storage Device over Zoned Namespaces (ZNS) SSDs](http://arxiv.org/abs/2112.00142)


  The Big Data trend is putting strain on modern storage systems, which have to
support high-performance I/O accesses for the large quantities of data. With
the prevalent Von Neumann computing architecture, this data is constantly moved
back and forth between the computing (i.e., CPU) and storage entities (DRAM,
Non-Volatile Memory NVM storage). Hence, as the data volume grows, this
constant data movement between the CPU and storage devices has emerged as a key
performance bottleneck. To improve the situation, researchers have advocated to
leverage computational storage devices (CSDs), which offer a programmable
interface to run user-defined data processing operations close to the storage
without excessive data movement, thus offering performance improvements.
However, despite its potential, building CSD-aware applications remains a
challenging task due to the lack of exploration and experimentation with the
right API and abstraction. This is due to the limited accessibility to latest
CSD/NVM devices, emerging device interfaces, and closed-source software
internals of the devices. To remedy the situation, in this work we present an
open-source CSD prototype over emerging NVMe Zoned Namespaces (ZNS) SSDs and an
interface that can be used to explore application designs for CSD/NVM storage
devices. In this paper we summarize the current state of the practice with CSD
devices, make a case for designing a CSD prototype with the ZNS interface and
eBPF (ZCSD), and present our initial findings. The prototype is available at
this https URL.

    

### [[2112.00170] SAMO: Optimised Mapping of Convolutional Neural Networks to Streaming Architectures](http://arxiv.org/abs/2112.00170)


  Toolflows that map Convolutional Neural Network (CNN) models to Field
Programmable Gate Arrays (FPGAs) have been an important tool in accelerating a
range of applications across different deployment settings. However, the
significance of the problem of finding an optimal mapping is often overlooked,
with the expectation that the end user will tune their generated hardware to
their desired platform. This is particularly prominent within Streaming
Architectures toolflows, where there is a large design space to explore. There
have been many Streaming Architectures proposed, however apart from
fpgaConvNet, there is limited support for optimisation methods that explore
both performance objectives and platform constraints. In this work, we
establish a framework, SAMO: a Streaming Architecture Mapping Optimiser, which
generalises the optimisation problem of mapping Streaming Architectures to FPGA
platforms. We also implement both Brute Force and Simulated Annealing
optimisation methods in order to generate valid, high performance designs for a
range of target platforms and CNN models. We are able to observe a 4x increase
in performance compared to example designs for the popular Streaming
Architecture framework FINN.

    

### [[2112.00267] CAMA: Energy and Memory Efficient Automata Processing in Content-Addressable Memories](http://arxiv.org/abs/2112.00267)


  Accelerating finite automata processing is critical for advancing real-time
analytic in pattern matching, data mining, bioinformatics, intrusion detection,
and machine learning. Recent in-memory automata accelerators leveraging SRAMs
and DRAMs have shown exciting improvements over conventional digital designs.
However, the bit-vector representation of state transitions used by all SOTA
designs is only optimal in processing worst-case completely random patterns,
while a significant amount of memory and energy is wasted in running most
real-world benchmarks. We present CAMA, a Content-Addressable Memory (CAM)
enabled Automata accelerator for processing homogeneous non-deterministic
finite automata (NFA). A radically different state representation scheme, along
with co-designed novel circuits and data encoding schemes, greatly reduces
energy, memory, and chip area for most realistic NFAs. CAMA is holistically
optimized with the following major contributions: (1) a 16x256 8-transistor
(8T) CAM array for state matching, replacing the 256x256 6T SRAM array or two
16x256 6T SRAM banks in SOTA designs; (2) a novel encoding scheme that enables
content searching within 8T SRAMs and adapts to different applications; (3) a
reconfigurable and scalable architecture that improves efficiency on all tested
benchmarks, without losing support for any NFA that is compatible with SOTA
designs; (4) an optimization framework that automates the choice of encoding
schemes and maps a given NFA to the proposed hardware. Two versions of CAMA,
one optimized for energy (CAMA-E) and the other for throughput (CAMA-T), are
comprehensively evaluated in a 28nm CMOS process, and across 21 real-world and
synthetic benchmarks. CAMA-E achieves 2.1x, 2.8x, and 2.04x lower energy than
CA, 2-stride Impala, and eAP. CAMA-T shows 2.68x, 3.87x and 2.62x higher
average compute density than 2-stride Impala, CA, and eAP.

    

### [[2112.00304] Software Variants for Hardware Trojan Detection and Resilience in COTS Processors](http://arxiv.org/abs/2112.00304)


  The commercial off-the-shelf (COTS) component based ecosystem provides an
attractive system design paradigm due to the drastic reduction in development
time and cost compared to custom solutions. However, it brings in a growing
concern of trustworthiness arising from the possibility of embedded malicious
logic, or hardware Trojans in COTS components. Existing trust-verification
approaches are typically not applicable to COTS hardware due to the absence of
golden models and the lack of observability of internal signals. In this work,
we propose a novel approach for runtime Trojan detection and resilience in
untrusted COTS processors through judicious modifications in software. The
proposed approach does not rely on any hardware redundancy or architectural
modification and hence seamlessly integrates with the COTS-based system design
process. Trojan resilience is achieved through the execution of multiple
functionally equivalent software variants. We have developed and implemented a
solution for compiler-based automatic generation of program variants,
metric-guided selection of variants, and their integration in a single
executable. To evaluate the proposed approach, we first analyzed the
effectiveness of program variants in avoiding the activation of a random pool
of Trojans. By implementing several Trojans in an OpenRISC 1000 processor, we
analyzed the detectability and resilience during Trojan activation in both
single and multiple variants. We also present delay and code size overhead for
the automatically generated variants for several programs and discuss future
research directions to reduce the overhead.

    

### [[2112.00387] How Parallel Circuit Execution Can Be Useful for NISQ Computing?](http://arxiv.org/abs/2112.00387)


  Quantum computing is performed on Noisy Intermediate-Scale Quantum (NISQ)
hardware in the short term. Only small circuits can be executed reliably on a
quantum machine due to the unavoidable noisy quantum operations on NISQ
devices, leading to the under-utilization of hardware resources. With the
growing demand to access quantum hardware, how to utilize it more efficiently
while maintaining output fidelity is becoming a timely issue. A parallel
circuit execution technique has been proposed to address this problem by
executing multiple programs on hardware simultaneously. It can improve the
hardware throughput and reduce the overall runtime. However, accumulative
noises such as crosstalk can decrease the output fidelity in parallel workload
execution. In this paper, we first give an in-depth overview of stateof-the-art
parallel circuit execution methods. Second, we propose a Quantum
Crosstalk-aware Parallel workload execution method (QuCP) without the overhead
of crosstalk characterization. Third, we investigate the trade-off between
hardware throughput and fidelity loss to explore the hardware limitation with
parallel circuit execution. Finally, we apply parallel circuit execution to VQE
and zero-noise extrapolation error mitigation method to showcase its various
applications on advancing NISQ computing.

    

### [[2112.00471] Triangle Counting Accelerations: From Algorithm to In-Memory Computing Architecture](http://arxiv.org/abs/2112.00471)


  Triangles are the basic substructure of networks and triangle counting (TC)
has been a fundamental graph computing problem in numerous fields such as
social network analysis. Nevertheless, like other graph computing problems, due
to the high memory-computation ratio and random memory access pattern, TC
involves a large amount of data transfers thus suffers from the bandwidth
bottleneck in the traditional Von-Neumann architecture. To overcome this
challenge, in this paper, we propose to accelerate TC with the emerging
processing-in-memory (PIM) architecture through an algorithm-architecture
co-optimization manner. To enable the efficient in-memory implementations, we
come up to reformulate TC with bitwise logic operations (such as AND), and
develop customized graph compression and mapping techniques for efficient data
flow management. With the emerging computational Spin-Transfer Torque Magnetic
RAM (STT-MRAM) array, which is one of the most promising PIM enabling
techniques, the device-to-architecture co-simulation results demonstrate that
the proposed TC in-memory accelerator outperforms the state-of-the-art GPU and
FPGA accelerations by 12.2x and 31.8x, respectively, and achieves a 34x energy
efficiency improvement over the FPGA accelerator.

    

### [[2112.00621] A Two-Level Approximate Logic Synthesis Combining Cube Insertion and Removal](http://arxiv.org/abs/2112.00621)


  Approximate computing is an attractive paradigm for reducing the design
complexity of error-resilient systems, therefore improving performance and
saving power consumption. In this work, we propose a new two-level approximate
logic synthesis method based on cube insertion and removal procedures.
Experimental results have shown significant literal count and runtime reduction
compared to the state-of-the-art approach. The method scalability is
illustrated for a high error threshold over large benchmark circuits. The
obtained solutions have presented a literal number reduction up to 38%, 56% and
93% with respect to an error rate of 1%, 3% and 5%, respectively.

    

### [[2112.00053] Task Assignment in Distributed Systems based on PSO Approach](http://arxiv.org/abs/2112.00053)


  In a distributed system, Task Assignment Problem (TAP) is a key factor for
obtaining efficiency. TAP illustrates the appropriate allocation of tasks to
the processor of each computer. In this problem, the proposed methods up to now
try to minimize Makespan and maximizing CPU utilization. Since this problem is
NP-complete, many genetic algorithms have been proposed to search optimal
solutions from the entire solution space. Disregarding the techniques which can
reduce the complexity of optimization, the existing approaches scan the entire
solution space. On the other hand, this approach is time-consuming in
scheduling which is considered a shortcoming. Therefore, in this paper, a
hybrid genetic algorithm has been proposed to overcome this shortcoming.
Particle Swarm Optimization (PSO) has been applied as local search in the
proposed genetic algorithm in this paper. The results obtained from simulation
can prove that, in terms of CPU utilization and Makespan, the proposed approach
outperforms the GA-based approach.

    

### [[2112.00068] Scaling Shared-Memory Data Structures as Distributed Global-View Data Structures in the Partitioned Global Address Space model](http://arxiv.org/abs/2112.00068)


  The Partitioned Global Address Space (PGAS), a memory model in which the
global address space is explicitly partitioned across compute nodes in a
cluster, strives to bridge the gap between shared-memory and distributed-memory
programming. To further bridge this gap, there has been an adoption of
global-view distributed data structures, such as 'global arrays' or
'distributed arrays'. This work demonstrates how shared-memory data structures
can be modified to scale in distributed memory. Presented in this work is the
Distributed Interlocked Hash Table (DIHT), a global-view distributed map data
structure inpired by the Interlocked Hash Table (IHT). At 64 nodes with 44
cores per node, DIHT provides upto 110x the performance of the Chapel
standard-library HashedDist.

    

### [[2112.00087] Coupling and Simulation of Fluid-Structure Interaction Problems for Automotive Sun-roof on Graphics Processing Unit](http://arxiv.org/abs/2112.00087)


  In this paper, the authors propose an analysis of the frequency response
function in a car compartment, subject to some fluctuating pressure
distribution along the open cavity of the sun-roof at the top of a car.
Coupling of a computational fluid dynamics and of a computational acoustics
code is considered to simulate the acoustic fluid-structure interaction
problem. Iterative Krylov methods and domain decomposition methods, tuned on
Graphic Processing Unit (GPU), are considered to solve the acoustic problem
with complex number arithmetics with double precision. Numerical simulations
illustrate the efficiency, robustness and accuracy of the proposed approaches.

    

### [[2112.00116] A Review on Parallel Virtual Screening Softwares for High Performance Computers](http://arxiv.org/abs/2112.00116)


  Drug discovery is the most expensive, time demanding and challenging project
in biopharmaceutical companies which aims at the identification and
optimization of lead compounds from large-sized chemical libraries. The lead
compounds should have high affinity binding and specificity for a target
associated with a disease and in addition they should have favorable
pharmacodynamic and pharmacokinetic properties (grouped as ADMET properties).
Overall, drug discovery is a multivariable optimization and can be carried out
in supercomputers using a reliable scoring function which is a measure of
binding affinity or inhibition potential of the drug-like compound. The major
problem is that the number of compounds in the chemical spaces is huge making
the computational drug discovery very demanding. However, it is cheaper and
less time consuming when compared to experimental high throughput screening. As
the problem is to find the most stable (global) minima for numerous
protein-ligand complexes (at the order of 10$^6$ to 10$^{12}$), the parallel
implementation of in-silico virtual screening can be exploited to make the drug
discovery in affordable time. In this review, we discuss such implementations
of parallelization algorithms in virtual screening programs. The nature of
different scoring functions and search algorithms are discussed, together with
a performance analysis of several docking softwares ported on high-performance
computing architectures.

    

### [[2112.00132] Atos: A Task-Parallel GPU Dynamic Scheduling Framework for Dynamic Irregular Computations](http://arxiv.org/abs/2112.00132)


  We present Atos, a task-parallel GPU dynamic scheduling framework that is
especially suited to dynamic irregular applications. Compared to the dominant
Bulk Synchronous Parallel (BSP) frameworks, Atos exposes additional concurrency
by supporting task-parallel formulations of applications with relaxed
dependencies, achieving higher GPU utilization, which is particularly
significant for problems with concurrency bottlenecks. Atos also offers
implicit task-parallel load balancing in addition to data-parallel load
balancing, providing users the flexibility to balance between them to achieve
optimal performance. Finally, Atos allows users to adapt to different use cases
by controlling the kernel strategy and task-parallel granularity. We
demonstrate that each of these controls is important in practice. We evaluate
and analyze the performance of Atos vs. BSP on three applications:
breadth-first search, PageRank, and graph coloring. Atos implementations
achieve geomean speedups of 3.44x, 2.1x, and 2.77x and peak speedups of 12.8x,
3.2x, and 9.08x across three case studies, compared to a state-of-the-art BSP
GPU implementation. Beyond simply quantifying the speedup, we extensively
analyze the reasons behind each speedup. This deeper understanding allows us to
derive general guidelines for how to select the optimal Atos configuration for
different applications. Finally, our analysis provides insights for future
dynamic scheduling framework designs.

    

### [[2112.00200] Efficient Big Text Data Clustering Algorithms using Hadoop and Spark](http://arxiv.org/abs/2112.00200)


  Document clustering is a traditional, efficient and yet quite effective, text
mining technique when we need to get a better insight of the documents of a
collection that could be grouped together. The K-Means algorithm and the
Hierarchical Agglomerative Clustering (HAC) algorithm are two of the most known
and commonly used clustering algorithms; the former due to its low time cost
and the latter due to its accuracy. However, even the use of K-Means in text
clustering over large-scale collections can lead to unacceptable time costs. In
this paper we first address some of the most valuable approaches for document
clustering over such 'big data' (large-scale) collections. We then present two
very promising alternatives: (a) a variation of an existing K-Means-based fast
clustering technique (known as BigKClustering - BKC) so that it can be applied
in document clustering, and (b) a hybrid clustering approach based on a
customized version of the Buckshot algorithm, which first applies a
hierarchical clustering procedure on a sample of the input dataset and then it
uses the results as the initial centers for a K-Means based assignment of the
rest of the documents, with very few iterations. We also give highly efficient
adaptations of the proposed techniques in the MapReduce model which are then
experimentally tested using Apache Hadoop and Spark over a real cluster
environment. As it comes out of the experiments, they both lead to acceptable
clustering quality as well as to significant time improvements (compared to
K-Means - especially the Buckshot-based algorithm), thus constituting very
promising alternatives for big document collections.

    

### [[2112.00262] A Blockchain-Enabled Incentivised Framework for Cyber Threat Intelligence Sharing in ICS](http://arxiv.org/abs/2112.00262)


  In recent years Industrial Control Systems (ICS) have been targeted
increasingly by sophisticated cyberattacks. Improving ICS security has drawn
significant attention in the literature that emphasises the importance of Cyber
Threat Intelligence (CTI) sharing in accelerating detection, mitigation, and
prevention of cyberattacks. However, organisations are reluctant to exchange
CTI due to fear of exposure, reputational damage, and lack of incentives.
Furthermore, there has been limited discussion about the factors influencing
participation in sharing CTI about ICS. The existing CTI-sharing platforms rely
on centralised trusted architectures that suffer from a single point of failure
and risk companies' privacy as the central node maintains CTI details. In this
paper, we address the needs of organisations involved in the management and
protection of ICS and present a novel framework that facilitates secure,
private, and incentivised exchange of CTI related to ICS using blockchain. We
propose a new blockchain-enabled framework that facilitates the secure
dissemination of CTI data among multiple stakeholders in ICS. We provide the
framework design, technical development and evaluate the framework's
feasibility in a real-world application environment using practical use-case
scenarios. Our proposed design shows a more practical and efficient framework
for a CTI sharing network for ICS, including the bestowal and acknowledgment of
data privacy, trust barriers, and security issues ingrained in this domain.

    

### [[2112.00274] Distributed Forward-Backward Methods without Central Coordination](http://arxiv.org/abs/2112.00274)


  In this work, we propose and analyse forward-backward-type algorithms for
finding a zero in the sum of finitely many monotone operators, which are not
based on reduction to a two operator inclusion in the product space. Each
iteration of the studied algorithms requires one resolvent evaluation per
set-valued operator, one forward evaluation per cocoercive operator, and two
forward evaluations per monotone operator. Unlike existing methods, the
structure of the proposed algorithms are suitable for distributed,
decentralised implementation in ring networks without the need for a central
coordinator to enforce consensus between nodes.

    

### [[2112.00286] Conflict-free Collaborative Set Sharing for Distributed Systems](http://arxiv.org/abs/2112.00286)


  Collaborative Data Sharing is widely noticed to be essential for distributed
systems. Among several proposed strategies, conflict-free techniques are
considered useful for serverless concurrent systems. They aim at making shared
data be consistent between peers in such a way that their local data do not
become equal at once, but they arrive at the same data eventually when no
updates occur in any peer. Although the Conflict-free Replicated Data Type
(CRDT) approach could be used in data sharing as well, it puts restrictions on
available operations so as to concurrent updates never cause conflicts. Even
for sets, popular operations such as insertion and deletion are not freely
used, for example. We propose a novel scheme for Conflict-free Collaborative
Set Sharing that allows both insertion and deletion operations. It will provide
a new synchronization method for data sharing and gives a fresh insight into
designing conflict-free replicated data types. We might consider that this
becomes a substitute for CRDTs.

    

### [[2112.00288] Operation-based Collaborative Data Sharing for Distributed Systems](http://arxiv.org/abs/2112.00288)


  Collaborative Data Sharing raises a fundamental issue in distributed systems.
Several strategies have been proposed for making shared data consistent between
peers in such a way that the shared part of their local data become equal. Most
of the proposals rely on state-based semantics. But this suffers from a lack of
descriptiveness in conflict-free features of synchronization required for
flexible network connections. Recent applications tend to use non-permanent
connection with mobile devices or allow temporary breakaways from the system,
for example. To settle ourselves in conflict-free data sharing, we propose a
novel scheme "Operation-based Collaborative Data Sharing" that enables
conflict-free strategies for synchronization based on operational semantics.

    

### [[2112.00467] A unified framework to improve the interoperability between HPC and Big Data languages and programming models](http://arxiv.org/abs/2112.00467)


  One of the most important issues in the path to the convergence of HPC and
Big Data is caused by the differences in their software stacks. Despite some
research efforts, the interoperability between their programming models and
languages is still limited. To deal with this problem we introduce a new
computing framework called IgnisHPC, whose main objective is to unify the
execution of Big Data and HPC workloads in the same framework. IgnisHPC has
native support for multi-language applications using JVM and non-JVM-based
languages. Since MPI was used as its backbone technology, IgnisHPC takes
advantage of many communication models and network architectures. Moreover, MPI
applications can be directly executed in a efficient way in the framework. The
main consequence is that users could combine in the same multi-language code
HPC tasks (using MPI) with Big Data tasks (using MapReduce operations). The
experimental evaluation demonstrates the benefits of our proposal in terms of
performance and productivity with respect to other frameworks such as Apache
Spark. IgnisHPC is publicly available for the Big Data and HPC research
community.

    

### [[2112.00604] Near-Optimal Distributed Degree+1 Coloring](http://arxiv.org/abs/2112.00604)


  We present a new approach to randomized distributed graph coloring that is
simpler and more efficient than previous ones. In particular, it allows us to
tackle the $(\operatorname{deg}+1)$-list-coloring (D1LC) problem, where each
node $v$ of degree $d_v$ is assigned a palette of $d_v+1$ colors, and the
objective is to find a proper coloring using these palettes. While for
$(\Delta+1)$-coloring (where $\Delta$ is the maximum degree), there is a fast
randomized distributed $O(\log^3\log n)$-round algorithm (Chang, Li, and Pettie
[SIAM J. Comp. 2020]), no $o(\log n)$-round algorithms are known for the D1LC
problem.
We give a randomized distributed algorithm for D1LC that is optimal under
plausible assumptions about the deterministic complexity of the problem. Using
the recent deterministic algorithm of Ghaffari and Kuhn [FOCS2021], our
algorithm runs in $O(\log^3 \log n)$ time, matching the best bound known for
$(\Delta+1)$-coloring. In addition, it colors all nodes of degree
$\Omega(\log^7 n)$ in $O(\log^* n)$ rounds.
A key contribution is a subroutine to generate slack for D1LC. When placed
into the framework of Assadi, Chen, and Khanna [SODA2019] and Alon and Assadi
[APPROX/RANDOM2020], this almost immediately leads to a palette sparsification
theorem for D1LC, generalizing previous results. That gives fast algorithms for
D1LC in three different models: an $O(1)$-round algorithm in the MPC model with
$\tilde{O}(n)$ memory per machine; a single-pass semi-streaming algorithm in
dynamic streams; and an $\tilde{O}(n\sqrt{n})$-time algorithm in the standard
query model.

    

### [[2112.00616] Roadmap for Edge AI: A Dagstuhl Perspective](http://arxiv.org/abs/2112.00616)


  Based on the collective input of Dagstuhl Seminar (21342), this paper
presents a comprehensive discussion on AI methods and capabilities in the
context of edge computing, referred as Edge AI. In a nutshell, we envision Edge
AI to provide adaptation for data-driven applications, enhance network and
radio access, and allow the creation, optimization, and deployment of
distributed AI/ML pipelines with given quality of experience, trust, security
and privacy targets. The Edge AI community investigates novel ML methods for
the edge computing environment, spanning multiple sub-fields of computer
science, engineering and ICT. The goal is to share an envisioned roadmap that
can bring together key actors and enablers to further advance the domain of
Edge AI.

    

### [[2112.00708] Optimal Resource Scheduling and Allocation in Distributed Computing Systems](http://arxiv.org/abs/2112.00708)


  The essence of distributed computing systems is how to schedule incoming
requests and how to allocate all computing nodes to minimize both time and
computation costs. In this paper, we propose a cost-aware optimal scheduling
and allocation strategy for distributed computing systems while minimizing the
cost function including response time and service cost. First, based on the
proposed cost function, we derive the optimal request scheduling policy and the
optimal resource allocation policy synchronously. Second, considering the
effects of incoming requests on the scheduling policy, the additive increase
multiplicative decrease (AIMD) mechanism is implemented to model the relation
between the request arrival and scheduling. In particular, the AIMD parameters
can be designed such that the derived optimal strategy is still valid. Finally,
a numerical example is presented to illustrate the derived results.

    

### [[2112.00709] GPU-Accelerated Forward-Backward algorithm with Application to Lattice-Free MMI](http://arxiv.org/abs/2112.00709)


  We propose to express the forward-backward algorithm in terms of operations
between sparse matrices in a specific semiring. This new perspective naturally
leads to a GPU-friendly algorithm which is easy to implement in Julia or any
programming languages with native support of semiring algebra. We use this new
implementation to train a TDNN with the LF-MMI objective function and we
compare the training time of our system with PyChain - a recently introduced
C++/CUDA implementation of the LF-MMI loss. Our implementation is about two
times faster while not having to use any approximation such as the "leaky-HMM".

    

### [[2112.00710] Stateful Entities: Object-oriented Cloud Applications as Distributed Dataflows](http://arxiv.org/abs/2112.00710)


  Programming stateful cloud applications remains a very painful experience.
Instead of focusing on the business logic, programmers spend most of their time
dealing with distributed systems considerations, with the most important being
consistency, load balancing, failure management, recovery, and scalability. At
the same time, we witness an unprecedented adoption of modern dataflow systems
such as Apache Flink, Google Dataflow, and Timely Dataflow. These systems are
now performant and fault-tolerant, and they offer excellent state management
primitives.
With this line of work, we aim at investigating the opportunities and limits
of compiling general-purpose programs into stateful dataflows. Given a set of
easy-to-follow code conventions, programmers can author stateful entities, a
programming abstraction embedded in Python. We present a compiler pipeline
named StateFlow, to analyze the abstract syntax tree of a Python application
and rewrite it into an intermediate representation based on stateful dataflow
graphs. StateFlow compiles that intermediate representation to a target
execution system: Apache Flink and Beam, AWS Lambda, Flink's Statefun, and
Cloudburst. Through an experimental evaluation, we demonstrate that the code
generated by StateFlow incurs minimal overhead. While developing and deploying
our prototype, we came to observe important limitations of current dataflow
systems in executing cloud applications at scale.

    

### [[2001.08510] Bibliography of distributed approximation beyond bounded degree](http://arxiv.org/abs/2001.08510)


  This document is an informal bibliography of the papers dealing with
distributed approximation algorithms. A classic setting for such algorithms is
bounded degree graphs, but there is a whole set of techniques that have been
developed for other classes. These later classes are the focus of the current
work. These classes have a geometric nature (planar, bounded genus and
unit-disk graphs) and/or have bounded parameters (arboricity, expansion,
growth, independence) or forbidden structures (forbidden minors).

    

### [[2112.00086] Dyna-bAbI: unlocking bAbI's potential with dynamic synthetic benchmarking](http://arxiv.org/abs/2112.00086)


  While neural language models often perform surprisingly well on natural
language understanding (NLU) tasks, their strengths and limitations remain
poorly understood. Controlled synthetic tasks are thus an increasingly
important resource for diagnosing model behavior. In this work we focus on
story understanding, a core competency for NLU systems. However, the main
synthetic resource for story understanding, the bAbI benchmark, lacks such a
systematic mechanism for controllable task generation. We develop Dyna-bAbI, a
dynamic framework providing fine-grained control over task generation in bAbI.
We demonstrate our ideas by constructing three new tasks requiring
compositional generalization, an important evaluation setting absent from the
original benchmark. We tested both special-purpose models developed for bAbI as
well as state-of-the-art pre-trained methods, and found that while both
approaches solve the original tasks (>99% accuracy), neither approach succeeded
in the compositional generalization setting, indicating the limitations of the
original training data. We explored ways to augment the original data, and
found that though diversifying training data was far more useful than simply
increasing dataset size, it was still insufficient for driving robust
compositional generalization (with <70% accuracy for complex compositions). Our
results underscore the importance of highly controllable task generators for
creating robust NLU systems through a virtuous cycle of model and data
development.

    

### [[2112.00113] Beyond Flatland: Pre-training with a Strong 3D Inductive Bias](http://arxiv.org/abs/2112.00113)


  Pre-training on large-scale databases consisting of natural images and then
fine-tuning them to fit the application at hand, or transfer-learning, is a
popular strategy in computer vision. However, Kataoka et al., 2020 introduced a
technique to eliminate the need for natural images in supervised deep learning
by proposing a novel synthetic, formula-based method to generate 2D fractals as
training corpus. Using one synthetically generated fractal for each class, they
achieved transfer learning results comparable to models pre-trained on natural
images. In this project, we take inspiration from their work and build on this
idea -- using 3D procedural object renders. Since the image formation process
in the natural world is based on its 3D structure, we expect pre-training with
3D mesh renders to provide an implicit bias leading to better generalization
capabilities in a transfer learning setting and that invariances to 3D rotation
and illumination are easier to be learned based on 3D data. Similar to the
previous work, our training corpus will be fully synthetic and derived from
simple procedural strategies; we will go beyond classic data augmentation and
also vary illumination and pose which are controllable in our setting and study
their effect on transfer learning capabilities in context to prior work. In
addition, we will compare the 2D fractal and 3D procedural object networks to
human and non-human primate brain data to learn more about the 2D vs. 3D nature
of biological vision.

    

### [[2112.00115] Risk-based implementation of COLREGs for autonomous surface vehicles using deep reinforcement learning](http://arxiv.org/abs/2112.00115)


  Autonomous systems are becoming ubiquitous and gaining momentum within the
marine sector. Since the electrification of transport is happening
simultaneously, autonomous marine vessels can reduce environmental impact,
lower costs, and increase efficiency. Although close monitoring is still
required to ensure safety, the ultimate goal is full autonomy. One major
milestone is to develop a control system that is versatile enough to handle any
weather and encounter that is also robust and reliable. Additionally, the
control system must adhere to the International Regulations for Preventing
Collisions at Sea (COLREGs) for successful interaction with human sailors.
Since the COLREGs were written for the human mind to interpret, they are
written in ambiguous prose and therefore not machine-readable or verifiable.
Due to these challenges and the wide variety of situations to be tackled,
classical model-based approaches prove complicated to implement and
computationally heavy. Within machine learning (ML), deep reinforcement
learning (DRL) has shown great potential for a wide range of applications. The
model-free and self-learning properties of DRL make it a promising candidate
for autonomous vessels. In this work, a subset of the COLREGs is incorporated
into a DRL-based path following and obstacle avoidance system using collision
risk theory. The resulting autonomous agent dynamically interpolates between
path following and COLREG-compliant collision avoidance in the training
scenario, isolated encounter situations, and AIS-based simulations of
real-world scenarios.

    

### [[2112.00206] Querying Labelled Data with Scenario Programs for Sim-to-Real Validation](http://arxiv.org/abs/2112.00206)


  Simulation-based testing of autonomous vehicles (AVs) has become an essential
complement to road testing to ensure safety. Consequently, substantial research
has focused on searching for failure scenarios in simulation. However, a
fundamental question remains: are AV failure scenarios identified in simulation
meaningful in reality, i.e., are they reproducible on the real system? Due to
the sim-to-real gap arising from discrepancies between simulated and real
sensor data, a failure scenario identified in simulation can be either a
spurious artifact of the synthetic sensor data or an actual failure that
persists with real sensor data. An approach to validate simulated failure
scenarios is to identify instances of the scenario in a corpus of real data,
and check if the failure persists on the real data. To this end, we propose a
formal definition of what it means for a labelled data item to match an
abstract scenario, encoded as a scenario program using the SCENIC probabilistic
programming language. Using this definition, we develop a querying algorithm
which, given a scenario program and a labelled dataset, finds the subset of
data matching the scenario. Experiments demonstrate that our algorithm is
accurate and efficient on a variety of realistic traffic scenarios, and scales
to a reasonable number of agents.

    

### [[2112.00229] Frequency Fitness Assignment: Optimization without a Bias for Good Solutions can be Efficient](http://arxiv.org/abs/2112.00229)


  A fitness assignment process transforms the features (such as the objective
value) of a candidate solution to a scalar fitness, which then is the basis for
selection. Under Frequency Fitness Assignment (FFA), the fitness corresponding
to an objective value is its encounter frequency and is subject to
minimization. FFA creates algorithms that are not biased towards better
solutions and are invariant under all bijections of the objective function
value. We investigate the impact of FFA on the performance of two
theory-inspired, state-of-the-art EAs, the Greedy (2+1) GA and the
Self-Adjusting (1+lambda,lambda)) GA. FFA improves their performance
significantly on some problems that are hard for them. We empirically find that
one FFA-based algorithm can solve all theory-based benchmark problems in this
study, including traps, jumps, and plateaus, in polynomial time. We propose two
hybrid approaches that use both direct and FFA-based optimization and find that
they perform well. All FFA-based algorithms also perform better on
satisfiability problems than all pure algorithm variants.

    

### [[2112.00284] Interactive Model with Structural Loss for Language-based Abductive Reasoning](http://arxiv.org/abs/2112.00284)


  The abductive natural language inference task ($\alpha$NLI) is proposed to
infer the most plausible explanation between the cause and the event. In the
$\alpha$NLI task, two observations are given, and the most plausible hypothesis
is asked to pick out from the candidates. Existing methods model the relation
between each candidate hypothesis separately and penalize the inference network
uniformly. In this paper, we argue that it is unnecessary to distinguish the
reasoning abilities among correct hypotheses; and similarly, all wrong
hypotheses contribute the same when explaining the reasons of the observations.
Therefore, we propose to group instead of ranking the hypotheses and design a
structural loss called ``joint softmax focal loss'' in this paper. Based on the
observation that the hypotheses are generally semantically related, we have
designed a novel interactive language model aiming at exploiting the rich
interaction among competing hypotheses. We name this new model for $\alpha$NLI:
Interactive Model with Structural Loss (IMSL). The experimental results show
that our IMSL has achieved the highest performance on the RoBERTa-large
pretrained model, with ACC and AUC results increased by about 1\% and 5\%
respectively.

    

### [[2112.00289] Point Cloud Segmentation Using Sparse Temporal Local Attention](http://arxiv.org/abs/2112.00289)


  Point clouds are a key modality used for perception in autonomous vehicles,
providing the means for a robust geometric understanding of the surrounding
environment. However despite the sensor outputs from autonomous vehicles being
naturally temporal in nature, there is still limited exploration of exploiting
point cloud sequences for 3D seman-tic segmentation. In this paper we propose a
novel Sparse Temporal Local Attention (STELA) module which aggregates
intermediate features from a local neighbourhood in previous point cloud frames
to provide a rich temporal context to the decoder. Using the sparse local
neighbourhood enables our approach to gather features more flexibly than those
which directly match point features, and more efficiently than those which
perform expensive global attention over the whole point cloud frame. We achieve
a competitive mIoU of 64.3% on the SemanticKitti dataset, and demonstrate
significant improvement over the single-frame baseline in our ablation studies.

    

### [[2112.00405] NER-BERT: A Pre-trained Model for Low-Resource Entity Tagging](http://arxiv.org/abs/2112.00405)


  Named entity recognition (NER) models generally perform poorly when large
training datasets are unavailable for low-resource domains. Recently,
pre-training a large-scale language model has become a promising direction for
coping with the data scarcity issue. However, the underlying discrepancies
between the language modeling and NER task could limit the models' performance,
and pre-training for the NER task has rarely been studied since the collected
NER datasets are generally small or large but with low quality. In this paper,
we construct a massive NER corpus with a relatively high quality, and we
pre-train a NER-BERT model based on the created dataset. Experimental results
show that our pre-trained model can significantly outperform BERT as well as
other strong baselines in low-resource scenarios across nine diverse domains.
Moreover, a visualization of entity representations further indicates the
effectiveness of NER-BERT for categorizing a variety of entities.

    

### [[2112.00412] The Majority Can Help The Minority: Context-rich Minority Oversampling for Long-tailed Classification](http://arxiv.org/abs/2112.00412)


  The problem of class imbalanced data lies in that the generalization
performance of the classifier is deteriorated due to the lack of data of the
minority classes. In this paper, we propose a novel minority over-sampling
method to augment diversified minority samples by leveraging the rich context
of the majority classes as background images. To diversify the minority
samples, our key idea is to paste a foreground patch from a minority class to a
background image from a majority class having affluent contexts. Our method is
simple and can be easily combined with the existing long-tailed recognition
methods. We empirically prove the effectiveness of the proposed oversampling
method through extensive experiments and ablation studies. Without any
architectural changes or complex algorithms, our method achieves
state-of-the-art performance on various long-tailed classification benchmarks.
Our code will be publicly available at link.

    

### [[2112.00427] Research on Event Accumulator Settings for Event-Based SLAM](http://arxiv.org/abs/2112.00427)


  Event cameras are a new type of sensors that are different from traditional
cameras. Each pixel is triggered asynchronously by event. The trigger event is
the change of the brightness irradiated on the pixel. If the increment or
decrement of brightness is higher than a certain threshold, an event is output.
Compared with traditional cameras, event cameras have the advantages of high
dynamic range and no motion blur. Accumulating events to frames and using
traditional SLAM algorithm is a direct and efficient way for event-based SLAM.
Different event accumulator settings, such as slice method of event stream,
processing method for no motion, using polarity or not, decay function and
event contribution, can cause quite different accumulating results. We
conducted the research on how to accumulate event frames to achieve a better
event-based SLAM performance. For experiment verification, accumulated event
frames are fed to the traditional SLAM system to construct an event-based SLAM
system. Our strategy of setting event accumulator has been evaluated on the
public dataset. The experiment results show that our method can achieve better
performance in most sequences compared with the state-of-the-art event frame
based SLAM algorithm. In addition, the proposed approach has been tested on a
quadrotor UAV to show the potential of applications in real scenario. Code and
results are open sourced to benefit the research community of event cameras

    

### [[2112.00428] Adv-4-Adv: Thwarting Changing Adversarial Perturbations via Adversarial Domain Adaptation](http://arxiv.org/abs/2112.00428)


  Whereas adversarial training can be useful against specific adversarial
perturbations, they have also proven ineffective in generalizing towards
attacks deviating from those used for training. However, we observe that this
ineffectiveness is intrinsically connected to domain adaptability, another
crucial issue in deep learning for which adversarial domain adaptation appears
to be a promising solution. Consequently, we proposed Adv-4-Adv as a novel
adversarial training method that aims to retain robustness against unseen
adversarial perturbations. Essentially, Adv-4-Adv treats attacks incurring
different perturbations as distinct domains, and by leveraging the power of
adversarial domain adaptation, it aims to remove the domain/attack-specific
features. This forces a trained model to learn a robust domain-invariant
representation, which in turn enhances its generalization ability. Extensive
evaluations on Fashion-MNIST, SVHN, CIFAR-10, and CIFAR-100 demonstrate that a
model trained by Adv-4-Adv based on samples crafted by simple attacks (e.g.,
FGSM) can be generalized to more advanced attacks (e.g., PGD), and the
performance exceeds state-of-the-art proposals on these datasets.

    

### [[2112.00431] MAD: A Scalable Dataset for Language Grounding in Videos from Movie Audio Descriptions](http://arxiv.org/abs/2112.00431)


  The recent and increasing interest in video-language research has driven the
development of large-scale datasets that enable data-intensive machine learning
techniques. In comparison, limited effort has been made at assessing the
fitness of these datasets for the video-language grounding task. Recent works
have begun to discover significant limitations in these datasets, suggesting
that state-of-the-art techniques commonly overfit to hidden dataset biases. In
this work, we present MAD (Movie Audio Descriptions), a novel benchmark that
departs from the paradigm of augmenting existing video datasets with text
annotations and focuses on crawling and aligning available audio descriptions
of mainstream movies. MAD contains over 384,000 natural language sentences
grounded in over 1,200 hours of video and exhibits a significant reduction in
the currently diagnosed biases for video-language grounding datasets. MAD's
collection strategy enables a novel and more challenging version of
video-language grounding, where short temporal moments (typically seconds long)
must be accurately grounded in diverse long-form videos that can last up to
three hours.

    

### [[2112.00494] Closeness Centrality via the Condorcet Principle](http://arxiv.org/abs/2112.00494)


  We uncover a new relation between Closeness centrality and the Condorcet
principle. We define a Condorcet winner in a graph as a node that compared to
any other node is closer to more nodes. In other words, if we assume that nodes
vote on a closer candidate, a Condorcet winner would win a two-candidate
election against any other node in a plurality vote. We show that Closeness
centrality and its random-walk version, Random-Walk Closeness centrality, are
the only classic centrality measures that are Condorcet consistent on trees,
i.e., if a Condorcet winner exists, they rank it first. While they are not
Condorcet consistent in general graphs, we show that Closeness centrality
satisfies the Condorcet Comparison property that states that out of two
adjacent nodes, the one preferred by more nodes has higher centrality. We show
that Closeness centrality is the only regular distance-based centrality with
such a property.

    

### [[2112.00574] Collective discrete optimisation as judgment aggregation](http://arxiv.org/abs/2112.00574)


  Many important collective decision-making problems can be seen as multi-agent
versions of discrete optimisation problems. Participatory budgeting, for
instance, is the collective version of the knapsack problem; other examples
include collective scheduling, and collective spanning trees. Rather than
developing a specific model, as well as specific algorithmic techniques, for
each of these problems, we propose to represent and solve them in the unifying
framework of judgment aggregation with weighted issues. We provide a modular
definition of collective discrete optimisation (CDO) rules based on coupling a
set scoring function with an operator, and we show how they generalise several
existing procedures developed for specific CDO problems. We also give an
implementation based on integer linear programming (ILP) and test it on the
problem of collective spanning trees.

    

### [[2112.00591] AI Assurance using Causal Inference: Application to Public Policy](http://arxiv.org/abs/2112.00591)


  Developing and implementing AI-based solutions help state and federal
government agencies, research institutions, and commercial companies enhance
decision-making processes, automate chain operations, and reduce the
consumption of natural and human resources. At the same time, most AI
approaches used in practice can only be represented as "black boxes" and suffer
from the lack of transparency. This can eventually lead to unexpected outcomes
and undermine trust in such systems. Therefore, it is crucial not only to
develop effective and robust AI systems, but to make sure their internal
processes are explainable and fair. Our goal in this chapter is to introduce
the topic of designing assurance methods for AI systems with high-impact
decisions using the example of the technology sector of the US economy. We
explain how these fields would benefit from revealing cause-effect
relationships between key metrics in the dataset by providing the causal
experiment on technology economics dataset. Several causal inference approaches
and AI assurance techniques are reviewed and the transformation of the data
into a graph-structured dataset is demonstrated.

    

### [[2112.00649] Digital Twinning Remote Laboratories for Online Practical Learning](http://arxiv.org/abs/2112.00649)


  The COVID19 pandemic has demonstrated a need for remote learning and virtual
learning applications such as virtual reality (VR) and tablet-based solutions.
Creating complex learning scenarios by developers is highly time-consuming and
can take over a year. It is also costly to employ teams of system analysts,
developers and 3D artists. There is a requirement to provide a simple method to
enable lecturers to create their own content for their laboratory tutorials.
Research has been undertaken into developing generic models to enable the
semi-automatic creation of a virtual learning tools for subjects that require
practical interactions with the lab resources. In addition to the system for
creating digital twins, a case study describing the creation of a virtual
learning application for an electrical laboratory tutorial has been presented.

    

### [[2112.00712] STEM: Unsupervised STructural EMbedding for Stance Detection](http://arxiv.org/abs/2112.00712)


  Stance detection is an important task, supporting many downstream tasks such
as discourse parsing and modeling the propagation of fake news, rumors, and
science denial. In this paper, we propose a novel framework for stance
detection. Our framework is unsupervised and domain-independent. Given a claim
and a multi-participant discussion - we construct the interaction network from
which we derive topological embeddings for each speaker. These speaker
embeddings enjoy the following property: speakers with the same stance tend to
be represented by similar vectors, while antipodal vectors represent speakers
with opposing stances. These embeddings are then used to divide the speakers
into stance-partitions. We evaluate our method on three different datasets from
different platforms. Our method outperforms or is comparable with supervised
models while providing confidence levels for its output. Furthermore, we
demonstrate how the structural embeddings relate to the valence expressed by
the speakers. Finally, we discuss some limitations inherent to the framework.

    

### [[2112.00724] RegNeRF: Regularizing Neural Radiance Fields for View Synthesis from Sparse Inputs](http://arxiv.org/abs/2112.00724)


  Neural Radiance Fields (NeRF) have emerged as a powerful representation for
the task of novel view synthesis due to their simplicity and state-of-the-art
performance. Though NeRF can produce photorealistic renderings of unseen
viewpoints when many input views are available, its performance drops
significantly when this number is reduced. We observe that the majority of
artifacts in sparse input scenarios are caused by errors in the estimated scene
geometry, and by divergent behavior at the start of training. We address this
by regularizing the geometry and appearance of patches rendered from unobserved
viewpoints, and annealing the ray sampling space during training. We
additionally use a normalizing flow model to regularize the color of unobserved
viewpoints. Our model outperforms not only other methods that optimize over a
single scene, but in many cases also conditional models that are extensively
pre-trained on large multi-view datasets.

    

### [[2112.00726] MonoScene: Monocular 3D Semantic Scene Completion](http://arxiv.org/abs/2112.00726)


  MonoScene proposes a 3D Semantic Scene Completion (SSC) framework, where the
dense geometry and semantics of a scene are inferred from a single monocular
RGB image. Different from the SSC literature, relying on 2.5 or 3D input, we
solve the complex problem of 2D to 3D scene reconstruction while jointly
inferring its semantics. Our framework relies on successive 2D and 3D UNets
bridged by a novel 2D-3D features projection inspiring from optics and
introduces a 3D context relation prior to enforce spatio-semantic consistency.
Along with architectural contributions, we introduce novel global scene and
local frustums losses. Experiments show we outperform the literature on all
metrics and datasets while hallucinating plausible scenery even beyond the
camera field of view. Our code and trained models are available at
this https URL


### [[1906.01820] Risks from Learned Optimization in Advanced Machine Learning Systems](http://arxiv.org/abs/1906.01820)


  We analyze the type of learned optimization that occurs when a learned model
(such as a neural network) is itself an optimizer - a situation we refer to as
mesa-optimization, a neologism we introduce in this paper. We believe that the
possibility of mesa-optimization raises two important questions for the safety
and transparency of advanced machine learning systems. First, under what
circumstances will learned models be optimizers, including when they should not
be? Second, when a learned model is an optimizer, what will its objective be -
how will it differ from the loss function it was trained under - and how can it
be aligned? In this paper, we provide an in-depth analysis of these two primary
questions and provide an overview of topics for future research.

    

### [[2102.01381] Generalized Facial Manipulation Detection with Edge Region Feature Extraction](http://arxiv.org/abs/2102.01381)


  This paper presents a generalized and robust face manipulation detection
method based on the edge region features appearing in images. Most contemporary
face synthesis processes include color awkwardness reduction but damage the
natural fingerprint in the edge region. In addition, these color correction
processes do not proceed in the non-face background region. We also observe
that the synthesis process does not consider the natural properties of the
image appearing in the time domain. Considering these observations, we propose
a facial forensic framework that utilizes pixel-level color features appearing
in the edge region of the whole image. Furthermore, our framework includes a
3D-CNN classification model that interprets the extracted color features
spatially and temporally. Unlike other existing studies, we conduct
authenticity determination by considering all features extracted from multiple
frames within one video. Through extensive experiments, including real-world
scenarios to evaluate generalized detection ability, we show that our framework
outperforms state-of-the-art facial manipulation detection technologies in
terms of accuracy and robustness.

    

### [[2107.02156] Do Different Tracking Tasks Require Different Appearance Models?](http://arxiv.org/abs/2107.02156)


  Tracking objects of interest in a video is one of the most popular and widely
applicable problems in computer vision. However, with the years, a Cambrian
explosion of use cases and benchmarks has fragmented the problem in a multitude
of different experimental setups. As a consequence, the literature has
fragmented too, and now novel approaches proposed by the community are usually
specialised to fit only one specific setup. To understand to what extent this
specialisation is necessary, in this work we present UniTrack, a solution to
address five different tasks within the same framework. UniTrack consists of a
single and task-agnostic appearance model, which can be learned in a supervised
or self-supervised fashion, and multiple ``heads'' that address individual
tasks and do not require training. We show how most tracking tasks can be
solved within this framework, and that the same appearance model can be
successfully used to obtain results that are competitive against specialised
methods for most of the tasks considered. The framework also allows us to
analyse appearance models obtained with the most recent self-supervised
methods, thus extending their evaluation and comparison to a larger variety of
important problems.

    

### [[2112.00228] Efficient loading of reduced data ensembles produced at ORNL SNS/HFIR neutron time-of-flight facilities](http://arxiv.org/abs/2112.00228)


  We present algorithmic improvements to the loading operations of certain
reduced data ensembles produced from neutron scattering experiments at Oak
Ridge National Laboratory (ORNL) facilities. Ensembles from multiple
measurements are required to cover a wide range of the phase space of a sample
material of interest. They are stored using the standard NeXus schema on
individual HDF5 files. This makes it a scalability challenge, as the number of
experiments stored increases in a single ensemble file. The present work
follows up on our previous efforts on data management algorithms, to address
identified input output (I/O) bottlenecks in Mantid, an open-source data
analysis framework used across several neutron science facilities around the
world. We reuse an in-memory binary-tree metadata index that resembles data
access patterns, to provide a scalable search and extraction mechanism. In
addition, several memory operations are refactored and optimized for the
current common use cases, ranging most frequently from 10 to 180, and up to 360
separate measurement configurations. Results from this work show consistent
speed ups in wall-clock time on the Mantid LoadMD routine, ranging from 19\% to
23\% on average, on ORNL production computing systems. The latter depends on
the complexity of the targeted instrument-specific data and the system I/O and
compute variability for the shared computational resources available to users
of ORNL's Spallation Neutron Source (SNS) and the High Flux Isotope Reactor
(HFIR) instruments. Nevertheless, we continue to highlight the need for more
research to address reduction challenges as experimental data volumes, user
time and processing costs increase.

    

### [[2001.01266] Finally, how many efficiencies supercomputers have? And, what do they measure?](http://arxiv.org/abs/2001.01266)


  Using an extremely large number of processing elements in computing systems
leads to unexpected phenomena, such as different efficiencies of the same
system for different tasks, that cannot be explained in the frame of classical
computing paradigm. The simple non-technical (but considering the temporal
behavior of the components) model, introduced here, enables us to set up a
frame and formalism, needed to explain those unexpected experiences around
supercomputing. Introducing temporal behavior into computer science also
explains why only the extreme scale computing enabled us to reveal the
experienced limitations. The paper shows, that degradation of efficiency of
parallelized sequential systems is a natural consequence of the classical
computing paradigm, instead of being an engineering imperfectness. The
workload, that supercomputers run, is much responsible for wasting energy, as
well as limiting the size and type of tasks. Case studies provide insight, how
different contributions compete for dominating the resulting payload
performance of a computing system, and how enhancing the interconnection
technology made computing+communication to dominate in defining the efficiency
of supercomputers. Our model also enables to derive predictions about
supercomputer performance limitations for the near future, as well as it
provides hints for enhancing supercomputer components. Phenomena experienced in
large-scale computing show interesting parallels with phenomena experienced in
science, more than a century ago, and through their studying a modern science
was developed.

    

### [[2112.00364] Universal Probabilistic Programming Language Compilation with Parallel Efficient Sequential Monte Carlo Inference](http://arxiv.org/abs/2112.00364)


  Probabilistic programming languages (PPLs) allow for natural encoding of
arbitrary inference problems, and PPL implementations can provide automatic
general-purpose inference for these problems. However, constructing inference
implementations that are efficient enough is challenging for many real-world
problems. Often, this is due to PPLs not fully exploiting available
parallelization and optimization opportunities. For example, handling of
probabilistic checkpoints in PPLs through the use of continuation-passing style
transformations or non-preemptive multitasking -- as is done in many popular
PPLs -- often disallows compilation to low-level languages required for
high-performance platforms such as graphics processing units (GPUs). As a
solution to this checkpoint problem, we introduce the concept of PPL
control-flow graphs (PCFGs), providing a simple and efficient approach that can
be used for handling checkpoints in such languages. We use this approach to
implement RootPPL: a low-level PPL built on CUDA and C++ with OpenMP, providing
highly efficient and massively parallel SMC inference. We also introduce a
general method of compiling universal high-level PPLs to PCFGs, and illustrate
its application when compiling Miking CorePPL -- a high-level universal PPL --
to RootPPL. This is the first time a universal PPL has been compiled to GPUs
with SMC inference. Both RootPPL and the CorePPL compiler are evaluated through
a set of real-world experiments in the domains of phylogenetics and
epidemiology, demonstrating up to 6x speedups over state-of-the-art PPLs
implementing SMC inference.

    

### [[2011.14989] The $\aleph$ Calculus](http://arxiv.org/abs/2011.14989)


  Motivated by a need for a model of reversible computation appropriate for a
Brownian molecular architecture, the $\aleph$ calculus is introduced. This
novel model is declarative, concurrent, and term-based--encapsulating all
information about the program data and state within a single structure in order
to obviate the need for a von Neumann-style discrete computational 'machine', a
challenge in a molecular environment. The name is inspired by the Greek for
'not forgotten', due to the emphasis on (reversibly) learning and un-learning
knowledge of different variables. To demonstrate its utility for this purpose,
as well as its elegance as a programming language, a number of examples are
presented; two of these examples, addition/subtraction and
squaring/square-rooting, are furnished with designs for abstract molecular
implementations. A natural by-product of these examples and accompanying
syntactic sugar is the design of a fully-fledged programming language, alethe,
which is also presented along with an interpreter. Efficiently simulating
$\aleph$ on a deterministic computer necessitates some static analysis of
programs within the alethe interpreter in order to render the declarative
programs sequential. Finally, work towards a type system appropriate for such a
reversible, declarative model of computation is presented.

    

### [<title>Survival Analysis with Accelerated Failure Time - XGBoost</title>](https://discuss.xgboost.ai/t/survival-analysis-with-accelerated-failure-time/2572/2)

### [<title>Survival Analysis with Accelerated Failure Time - XGBoost</title>](https://discuss.xgboost.ai/t/survival-analysis-with-accelerated-failure-time/2572/1)