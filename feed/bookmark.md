
## 2021-11-30

### [[2111.13754] Toward Next Generation Open Radio Access Network--What O-RAN Can and Cannot Do!](http://arxiv.org/abs/2111.13754)


  The open radio access network (O-RAN) describes an industry-driven open
architecture and interfaces for building next generation RANs with artificial
intelligence (AI) based control. This paper dissects the O-RAN framework and
provides a unique assessment of what it enables and where its current
shortcomings are. This was motivated by a user survey that we conducted with
responses from representative wireless researchers and developers from
industry, academia, and government. The survey shows that the physical and
medium access control layers are of most relevance to researchers and that
nearly 80% believe that the O-RAN architecture will drive future cellular
network innovations and deployments. The major contributions of O-RAN are the
open interfaces among RAN modules and the specifications of different control
loops that facilitate data driven network configuration and operation using AI.
These AI controllers are encapsulated as xApps that can be introduced to
control communications, networking, or service functions. The important O-RAN
limitations relate to security, scope of AI control, and latency. We conclude
this paper by indicating how specific limitations can be overcome in future
O-RAN architecture revisions.

    

### [[2111.13846] The Transdimensional Poisson Process for Vehicular Network Analysis](http://arxiv.org/abs/2111.13846)


  A comprehensive vehicular network analysis requires modeling the street
system and vehicle locations. Even when Poisson point processes (PPPs) are used
to model the vehicle locations on each street, the analysis is barely
tractable. That holds for even a simple average-based performance metric -- the
success probability, which is a special case of the fine-grained metric, the
meta distribution (MD) of the signal-to-interference ratio (SIR). To address
this issue, we propose the transdimensional approach as an alternative. Here,
the union of 1D PPPs on the streets is simplified to the transdimensional PPP
(TPPP), a superposition of 1D and 2D PPPs. The TPPP includes the 1D PPPs on the
streets passing through the receiving vehicle and models the remaining vehicles
as a 2D PPP ignoring their street geometry. Through the SIR MD analysis, we
show that the TPPP provides good approximations to the more cumbrous models
with streets characterized by Poisson line/stick processes; and we prove that
the accuracy of the TPPP further improves under shadowing. Lastly, we use the
MD results to control network congestion by adjusting the transmit rate while
maintaining a target fraction of reliable links. A key insight is that the
success probability is an inadequate measure of congestion as it does not
capture the reliabilities of the individual links.

    

### [[2111.13879] ML-based Handover Prediction and AP Selection in Cognitive Wi-Fi Networks](http://arxiv.org/abs/2111.13879)


  Device mobility in dense Wi-Fi networks offers several challenges. Two
well-known problems related to device mobility are handover prediction and
access point selection. Due to the complex nature of the radio environment,
analytical models may not characterize the wireless channel, which makes the
solution of these problems very difficult. Recently, cognitive network
architectures using sophisticated learning techniques are increasingly being
applied to such problems. In this paper, we propose a data-driven machine
learning (ML) schemes to efficiently solve these problems in WLAN networks. The
proposed schemes are evaluated and results are compared with traditional
approaches to the aforementioned problems. The results report significant
improvement in network performance by applying the proposed schemes. For
instance, the proposed scheme for handover prediction outperforms traditional
methods i.e. RSS method and traveling distance method by reducing the number of
unnecessary handovers by 60% and 50% respectively. Similarly, in AP selection,
the proposed scheme outperforms the SSF and LLF algorithms by achieving higher
throughput gains upto 9.2% and 8% respectively.

    

### [[2111.14018] How Can Applications of Blockchain and Artificial Intelligence Improve Performance of Internet of Things? -- A Survey](http://arxiv.org/abs/2111.14018)


  In the era of the Internet of Things (IoT), massive computing devices
surrounding us operate and interact with each other to provide several
significant services in industries, medical as well as in daily life activities
at home, office, education sectors, and so on. The participating devices in an
IoT network usually have resource constraints and the devices are prone to
different cyber attacks, leading to the loopholes in the security and
authentication. As a revolutionized and innovated technology, blockchain, that
is applied in cryptocurrency, market prediction, etc., uses a distributed
ledger that records transactions securely and efficiently. To utilize the great
potential of blockchain, both industries and academia have paid a significant
attention to integrate it with the IoT, as reported by several existing
literature. On the other hand, Artificial Intelligence (AI) is able to embed
intelligence in a system, and thus the AI can be integrated with IoT devices in
order to automatically cope with different environments according to the
demands. Furthermore, both blockchain and AI can be integrated with the IoT to
design an automated secure and robust IoT model, as mentioned by numerous
existing works. In this survey, we present a discussion on the IoT, blockchain,
and AI, along with the descriptions of several research works that apply
blockchain and AI in the IoT. In this direction, we point out strengths and
limitations of the related existing researches. We also discuss different open
challenges to exploit the full capacities of blockchain and AI in designing an
IoT-based model. Therefore, the highlighted challenging issues can open the
door for the development of future IoT models which will be intelligent and
secure based on the integration of blockchain and AI with the IoT.

    

### [[2111.14044] Joint Sensing and Communication for Situational Awareness in Wireless THz Systems](http://arxiv.org/abs/2111.14044)


  Next-generation wireless systems are rapidly evolving from communication-only
systems to multi-modal systems with integrated sensing and communications. In
this paper a novel joint sensing and communication framework is proposed for
enabling wireless extended reality (XR) at terahertz (THz) bands. To gather
rich sensing information and a higher line-of-sight (LoS) availability,
THz-operated reconfigurable intelligent surfaces (RISs) acting as base stations
are deployed. The sensing parameters are extracted by leveraging THz's
quasi-opticality and opportunistically utilizing uplink communication
waveforms. This enables the use of the same waveform, spectrum, and hardware
for both sensing and communication purposes. The environmental sensing
parameters are then derived by exploiting the sparsity of THz channels via
tensor decomposition. Hence, a high-resolution indoor mapping is derived so as
to characterize the spatial availability of communications and the mobility of
users. Simulation results show that in the proposed framework, the resolution
and data rate of the overall system are positively correlated, thus allowing a
joint optimization between these metrics with no tradeoffs. Results also show
that the proposed framework improves the system reliability in static and
mobile systems. In particular, the highest reliability gains of 10% in
reliability are achieved in a walking speed mobile environment compared to
communication only systems with beam tracking.

    

### [[2111.14080] Empirical Conditional Mean: A New Method of Predicting Throughput in Uplink Data Network](http://arxiv.org/abs/2111.14080)


  Experience of live video streaming can be improved if future bandwidth can be
predicted more accurately at the video uploader side. Thus follows a natural
question, how to make predictions both easily and precisely in real networks?
Researchers have developed many prediction algorithms in the literature, from
where a simple algorithm comes out, Arithmetic Mean (AM). Based on that, we are
purposing a new method called Empirical Conditional Mean (ECM), hoping to
utilize more information in the past data to get a more accurate prediction
without loss of practicality. Our experiments found that when the frequency of
measuring the network throughput is low, the ECM algorithm performs better than
the commonly used AM one, which means ECM can send more data out while not
having higher loss rate. Hence ECM can be helpful for those who especially have
relatively limited networks to reach a more considerable a balance between
frame loss rate and video quality.

    

### [[2111.14123] Improving the Resilience of Fast Failover Routing: TREE (Tree Routing to Extend Edge disjoint paths)](http://arxiv.org/abs/2111.14123)


  Today's communication networks have stringent availability requirements and
hence need to rapidly restore connectivity after failures. Modern networks thus
implement various forms of fast reroute mechanisms in the data plane, to bridge
the gap to slow global control plane convergence. State-of-the-art fast reroute
commonly relies on disjoint route structures, to offer multiple independent
paths to the destination. We propose to leverage the network's path diversity
to extend edge disjoint path mechanisms to tree routing, in order to improve
the performance of fast rerouting. We present two such tree-mechanisms in
detail and show that they boost resilience by up to 12% and 25% respectively on
real-world, synthetic, and data center topologies, while still retaining good
path length qualities.

    

### [[2111.14125] AirSPEC: An IoT-empowered Air Quality Monitoring System integrated with a Machine Learning Framework to Detect and Predict defined Air Quality parameters](http://arxiv.org/abs/2111.14125)


  The air that surrounds us is the cardinal source of respiration of all
life-forms. Therefore, it is undoubtedly vital to highlight that balanced air
quality is utmost important to the respiratory health of all living beings,
environmental homeostasis, and even economical equilibrium. Nevertheless, a
gradual deterioration of air quality has been observed in the last few decades,
due to the continuous increment of polluted emissions from automobiles and
industries into the atmosphere. Even though many people have scarcely
acknowledged the depth of the problem, the persistent efforts of determined
parties, including the World Health Organization, have consistently pushed the
boundaries for a qualitatively better global air homeostasis, by facilitating
technology-driven initiatives to timely detect and predict air quality in
regional and global scales. However, the existing frameworks for air quality
monitoring lack the capability of real-time responsiveness and flexible
semantic distribution. In this paper, a novel Internet of Things framework is
proposed which is easily implementable, semantically distributive, and
empowered by a machine learning model. The proposed system is equipped with a
NodeRED dashboard which processes, visualizes, and stores the primary sensor
data that are acquired through a public air quality sensor network, and
further, the dashboard is integrated with a machine-learning model to obtain
temporal and geo-spatial air quality predictions. ESP8266 NodeMCU is
incorporated as a subscriber to the NodeRED dashboard via a message queuing
telemetry transport broker to communicate quantitative air quality data or
alarming emails to the end-users through the developed web and mobile
applications. Therefore, the proposed system could become highly beneficial in
empowering public engagement in air quality through an unoppressive,
data-driven, and semantic framework.

    

### [[2111.14281] Passive Indoor Localization with WiFi Fingerprints](http://arxiv.org/abs/2111.14281)


  This paper proposes passive WiFi indoor localization. Instead of using WiFi
signals received by mobile devices as fingerprints, we use signals received by
routers to locate the mobile carrier. Consequently, software installation on
the mobile device is not required. To resolve the data insufficiency problem,
flow control signals such as request to send (RTS) and clear to send (CTS) are
utilized. In our model, received signal strength indicator (RSSI) and channel
state information (CSI) are used as fingerprints for several algorithms,
including deterministic, probabilistic and neural networks localization
algorithms. We further investigated localization algorithms performance through
extensive on-site experiments with various models of phones at hundreds of
testing locations. We demonstrate that our passive scheme achieves an average
localization error of 0.8 m when the phone is actively transmitting data frames
and 1.5 m when it is not transmitting data frames.

    

### [[2105.11727] Impatient Queuing for Intelligent Task Offloading in Multi-Access Edge Computing](http://arxiv.org/abs/2105.11727)


  Multi-access edge computing (MEC) emerges as an essential part of the
upcoming Fifth Generation (5G) and future beyond-5G mobile communication
systems. It brings computation power to the edge of cellular networks, which is
close to the energy-constrained user devices, and therewith allows the users to
offload tasks to the edge computing nodes for a low-latency computation with
low battery consumption. However, due to the high dynamics of user demand and
server load, task congestion may occur at the edge nodes, leading to long
queuing delay. Such delays can significantly degrade the quality of experience
(QoE) of some latency-sensitive applications, raise the risk of service outage,
and cannot be efficiently resolved by conventional queue management solutions.
In this article, we study an latency-outage critical scenario, where the
users intend to reduce the risk of latency outage. We propose an
impatience-based queuing strategy for such users to intelligently choose
between MEC offloading and local computation, allowing them to rationally
renege from the task queue. The proposed approach is demonstrated by numerical
simulations as efficient for generic service model, when a perfect queue
information is available. For the practical case where the users obtain no
perfect queue information, we design a optimal online learning strategy to
enable its application in Poisson service scenarios.

    

### [[2105.13172] On the Complexity of Weight-Dynamic Network Algorithms](http://arxiv.org/abs/2105.13172)


  While operating communication networks adaptively may improve utilization and
performance, frequent adjustments also introduce an algorithmic challenge: the
re-optimization of traffic engineering solutions is time-consuming and may
limit the granularity at which a network can be adjusted. This paper is
motivated by question whether the reactivity of a network can be improved by
re-optimizing solutions dynamically rather than from scratch, especially if
inputs such as link weights do not change significantly. This paper explores to
what extent dynamic algorithms can be used to speed up fundamental tasks in
network operations. We specifically investigate optimizations related to
traffic engineering (namely shortest paths and maximum flow computations), but
also consider spanning tree and matching applications. While prior work on
dynamic graph algorithms focuses on link insertions and deletions, we are
interested in the practical problem of link weight changes. We revisit existing
upper bounds in the weight-dynamic model, and present several novel lower
bounds on the amortized runtime for recomputing solutions. In general, we find
that the potential performance gains depend on the application, and there are
also strict limitations on what can be achieved, even if link weights change
only slightly.

    

### [[2111.13684] Spatio-Temporal Joint Graph Convolutional Networks for Traffic Forecasting](http://arxiv.org/abs/2111.13684)


  Recent studies focus on formulating the traffic forecasting as a
spatio-temporal graph modeling problem. They typically construct a static
spatial graph at each time step and then connect each node with itself between
adjacent time steps to construct the spatio-temporal graph. In such a graph,
the correlations between different nodes at different time steps are not
explicitly reflected, which may restrict the learning ability of graph neural
networks. Meanwhile, those models ignore the dynamic spatio-temporal
correlations among nodes as they use the same adjacency matrix at different
time steps. To overcome these limitations, we propose a Spatio-Temporal Joint
Graph Convolutional Networks (STJGCN) for traffic forecasting over several time
steps ahead on a road network. Specifically, we construct both pre-defined and
adaptive spatio-temporal joint graphs (STJGs) between any two time steps, which
represent comprehensive and dynamic spatio-temporal correlations. We further
design dilated causal spatio-temporal joint graph convolution layers on STJG to
capture the spatio-temporal dependencies from distinct perspectives with
multiple ranges. A multi-range attention mechanism is proposed to aggregate the
information of different ranges. Experiments on four public traffic datasets
demonstrate that STJGCN is computationally efficient and outperforms 11
state-of-the-art baseline methods.

    

### [[2111.13693] Enforcing and Discovering Structure in Machine Learning](http://arxiv.org/abs/2111.13693)


  The world is structured in countless ways. It may be prudent to enforce
corresponding structural properties to a learning algorithm's solution, such as
incorporating prior beliefs, natural constraints, or causal structures. Doing
so may translate to faster, more accurate, and more flexible models, which may
directly relate to real-world impact. In this dissertation, we consider two
different research areas that concern structuring a learning algorithm's
solution: when the structure is known and when it has to be discovered.

    

### [[2111.13694] Speaker Embedding-aware Neural Diarization for Flexible Number of Speakers with Textual Information](http://arxiv.org/abs/2111.13694)


  Overlapping speech diarization is always treated as a multi-label
classification problem. In this paper, we reformulate this task as a
single-label prediction problem by encoding the multi-speaker labels with power
set. Specifically, we propose the speaker embedding-aware neural diarization
(SEND) method, which predicts the power set encoded labels according to the
similarities between speech features and given speaker embeddings. Our method
is further extended and integrated with downstream tasks by utilizing the
textual information, which has not been well studied in previous literature.
The experimental results show that our method achieves lower diarization error
rate than the target-speaker voice activity detection. When textual information
is involved, the diarization errors can be further reduced. For the real
meeting scenario, our method can achieve 34.11% relative improvement compared
with the Bayesian hidden Markov model based clustering algorithm.

    

### [[2111.13696] A Ubiquitous Unifying Degeneracy in 2-body Microlensing Systems](http://arxiv.org/abs/2111.13696)


  While gravitational microlensing by planetary systems can provide unique
vistas on the properties of exoplanets, observations of such 2-body
microlensing events can often be explained with multiple and distinct physical
configurations, so-called model degeneracies. An understanding of the intrinsic
and exogenous origins of different classes of degeneracy provides a foundation
for phenomenological interpretation. Here, leveraging a fast machine-learning
based inference framework, we present the discovery of a new regime of
degeneracy--the offset degeneracy--which unifies the previously known
close-wide and inner-outer degeneracies, generalises to resonant caustics, and
upon reanalysis, is ubiquitous in previously published planetary events with
2-fold degenerate solutions. Importantly, our discovery suggests that the
commonly reported close-wide degeneracy essentially never arises in actual
events and should, instead, be more suitably viewed as a transition point of
the offset degeneracy. While previous studies of microlensing degeneracies are
largely studies of degenerate caustics, our discovery demonstrates that
degenerate caustics do not necessarily result in degenerate events, which for
the latter it is more relevant to study magnifications at the location of the
source. This discovery fundamentally changes the way in which degeneracies in
planetary microlensing events should be interpreted, suggests a deeper symmetry
in the mathematics of 2-body lenses than has previously been recognised, and
will increasingly manifest itself in data from new generations of microlensing
surveys.

    

### [[2111.13723] SARS-CoV-2 Dissemination using a Network of the United States Counties](http://arxiv.org/abs/2111.13723)


  During 2020 and 2021, severe acute respiratory syndrome coronavirus 2
(SARS-CoV-2) transmission has been increasing amongst the world's population at
an alarming rate. Reducing the spread of SARS-CoV-2 and other diseases that are
spread in similar manners is paramount for public health officials as they seek
to effectively manage resources and potential population control measures such
as social distancing and quarantines. By analyzing the United States' county
network structure, one can model and interdict potential higher infection
areas. County officials can provide targeted information, preparedness
training, as well as increase testing in these areas. While these approaches
may provide adequate countermeasures for localized areas, they are inadequate
for the holistic United States. We solve this problem by collecting coronavirus
disease 2019 (COVID-19) infections and deaths from the Center for Disease
Control and Prevention and a network adjacency structure from the United States
Census Bureau. Generalized network autoregressive (GNAR) time series models
have been proposed as an efficient learning algorithm for networked datasets.
This work fuses network science and operations research techniques to
univariately model COVID-19 cases, deaths, and current survivors across the
United States' county network structure.

    

### [[2111.13726] BCH-NLP at BioCreative VII Track 3: medications detection in tweets using transformer networks and multi-task learning](http://arxiv.org/abs/2111.13726)


  In this paper, we present our work participating in the BioCreative VII Track
3 - automatic extraction of medication names in tweets, where we implemented a
multi-task learning model that is jointly trained on text classification and
sequence labelling. Our best system run achieved a strict F1 of 80.4, ranking
first and more than 10 points higher than the average score of all
participants. Our analyses show that the ensemble technique, multi-task
learning, and data augmentation are all beneficial for medication detection in
tweets.

    

### [[2111.13730] Towards Efficient Ansatz Architecture for Variational Quantum Algorithms](http://arxiv.org/abs/2111.13730)


  Variational quantum algorithms are expected to demonstrate the advantage of
quantum computing on near-term noisy quantum computers. However, training such
variational quantum algorithms suffers from gradient vanishing as the size of
the algorithm increases. Previous work cannot handle the gradient vanishing
induced by the inevitable noise effects on realistic quantum hardware. In this
paper, we propose a novel training scheme to mitigate such noise-induced
gradient vanishing. We first introduce a new cost function of which the
gradients are significantly augmented by employing traceless observables in
truncated subspace. We then prove that the same minimum can be reached by
optimizing the original cost function with the gradients from the new cost
function. Experiments show that our new training scheme is highly effective for
major variational quantum algorithms of various tasks.

    

### [[2111.13733] Failure Modes of Domain Generalization Algorithms](http://arxiv.org/abs/2111.13733)


  Domain generalization algorithms use training data from multiple domains to
learn models that generalize well to unseen domains. While recently proposed
benchmarks demonstrate that most of the existing algorithms do not outperform
simple baselines, the established evaluation methods fail to expose the impact
of various factors that contribute to the poor performance. In this paper we
propose an evaluation framework for domain generalization algorithms that
allows decomposition of the error into components capturing distinct aspects of
generalization. Inspired by the prevalence of algorithms based on the idea of
domain-invariant representation learning, we extend the evaluation framework to
capture various types of failures in achieving invariance. We show that the
largest contributor to the generalization error varies across methods,
datasets, regularization strengths and even training lengths. We observe two
problems associated with the strategy of learning domain-invariant
representations. On Colored MNIST, most domain generalization algorithms fail
because they reach domain-invariance only on the training domains. On
Camelyon-17, domain-invariance degrades the quality of representations on
unseen domains. We hypothesize that focusing instead on tuning the classifier
on top of a rich representation can be a promising direction.

    

### [[2111.13755] A survey on multi-objective hyperparameter optimization algorithms for Machine Learning](http://arxiv.org/abs/2111.13755)


  Hyperparameter optimization (HPO) is a necessary step to ensure the best
possible performance of Machine Learning (ML) algorithms. Several methods have
been developed to perform HPO; most of these are focused on optimizing one
performance measure (usually an error-based measure), and the literature on
such single-objective HPO problems is vast. Recently, though, algorithms have
appeared which focus on optimizing multiple conflicting objectives
simultaneously. This article presents a systematic survey of the literature
published between 2014 and 2020 on multi-objective HPO algorithms,
distinguishing between metaheuristic-based algorithms, metamodel-based
algorithms, and approaches using a mixture of both. We also discuss the quality
metrics used to compare multi-objective HPO procedures and present future
research directions.

    

### [[2111.13756] Demystifying Ten Big Ideas and Rules Every Fire Scientist & Engineer Should Know About Blackbox, Whitebox & Causal Artificial Intelligence](http://arxiv.org/abs/2111.13756)


  Artificial intelligence (AI) is paving the way towards the fourth industrial
revolution with the fire domain (Fire 4.0). As a matter of fact, the next few
years will be elemental to how this technology will shape our academia,
practice, and entrepreneurship. Despite the growing interest between fire
research groups, AI remains absent of our curriculum, and we continue to lack a
methodical framework to adopt, apply and create AI solutions suitable for our
problems. The above is also true for parallel engineering domains (i.e.,
civil/mechanical engineering), and in order to negate the notion of history
repeats itself (e.g., look at the continued debate with regard to modernizing
standardized fire testing, etc.), it is the motivation behind this letter to
the Editor to demystify some of the big ideas behind AI to jump-start prolific
and strategic discussions on the front of AI & Fire. In addition, this letter
intends to explain some of the most fundamental concepts and clear common
misconceptions specific to the adoption of AI in fire engineering. This short
letter is a companion to the Smart Systems in Fire Engineering special issue
sponsored by Fire Technology. An in-depth review of AI algorithms [1] and
success stories to the proper implementations of such algorithms can be found
in the aforenoted special issue and collection of papers. This letter comprises
two sections. The first section outlines big ideas pertaining to AI, and
answers some of the burning questions with regard to the merit of adopting AI
in our domain. The second section presents a set of rules or technical
recommendations an AI user may deem helpful to practice whenever AI is used as
an investigation methodology. The presented set of rules are complementary to
the big ideas.

    

### [[2111.13759] Dynamic Analysis of Nonlinear Civil Engineering Structures using Artificial Neural Network with Adaptive Training](http://arxiv.org/abs/2111.13759)


  Dynamic analysis of structures subjected to earthquake excitation is a
time-consuming process, particularly in the case of extremely small time step
required, or in the presence of high geometric and material nonlinearity.
Performing parametric studies in such cases is even more tedious. The
advancement of computer graphics hardware in recent years enables efficient
training of artificial neural networks that are well-known to be capable of
learning highly nonlinear mappings. In this study, artificial neural networks
are developed with adaptive training algorithms, which enables automatic nodes
generation and layers addition. The hyperbolic tangent function is selected as
the activation function. Stochastic Gradient Descent and Back Propagation
algorithms are adopted to train the networks. The neural networks initiate with
a small number of hidden layers and nodes. During training, the performance of
the network is continuously tracked, and new nodes or layers are added to the
hidden layers if the neural network reaches its capacity. At the end of the
training process, the network with appropriate architecture is automatically
formed. The performance of the networks has been validated for inelastic shear
frames, as well as rocking structures, of which both are first built in finite
element program for dynamic analysis to generate training data. Results have
shown the developed networks can successfully predict the time-history response
of the shear frame and the rock structure subjected to real ground motion
records. The efficiency of the proposed neural networks is also examined, which
shows the computational time can be reduced by 43% by the neural networks
method than FE models. This indicates the trained networks can be utilized to
generate rocking spectrums of structures more efficiently which demands a large
number of time-history analyses.

    

### [[2111.13760] Interpreting Machine Learning Models for Room Temperature Prediction in Non-domestic Buildings](http://arxiv.org/abs/2111.13760)


  An ensuing challenge in Artificial Intelligence (AI) is the perceived
difficulty in interpreting sophisticated machine learning models, whose
ever-increasing complexity makes it hard for such models to be understood,
trusted and thus accepted by human beings. The lack, if not complete absence,
of interpretability for these so-called black-box models can lead to serious
economic and ethical consequences, thereby hindering the development and
deployment of AI in wider fields, particularly in those involving critical and
regulatory applications. Yet, the building services industry is a
highly-regulated domain requiring transparency and decision-making processes
that can be understood and trusted by humans. To this end, the design and
implementation of autonomous Heating, Ventilation and Air Conditioning systems
for the automatic but concurrently interpretable optimisation of energy
efficiency and room thermal comfort is of topical interest. This work therefore
presents an interpretable machine learning model aimed at predicting room
temperature in non-domestic buildings, for the purpose of optimising the use of
the installed HVAC system. We demonstrate experimentally that the proposed
model can accurately forecast room temperatures eight hours ahead in real-time
by taking into account historical RT information, as well as additional
environmental and time-series features. In this paper, an enhanced feature
engineering process is conducted based on the Exploratory Data Analysis
results. Furthermore, beyond the commonly used Interpretable Machine Learning
techniques, we propose a Permutation Feature-based Frequency Response Analysis
(PF-FRA) method for quantifying the contributions of the different predictors
in the frequency domain. Based on the generated reason codes, we find that the
historical RT feature is the dominant factor that has most impact on the model
prediction.

    

### [[2111.13769] Unsupervised MKL in Multi-layer Kernel Machines](http://arxiv.org/abs/2111.13769)


  Kernel based Deep Learning using multi-layer kernel machines(MKMs) was
proposed by Y.Cho and L.K. Saul in \cite{saul}. In MKMs they used only one
kernel(arc-cosine kernel) at a layer for the kernel PCA-based feature
extraction. We propose to use multiple kernels in each layer by taking a convex
combination of many kernels following an unsupervised learning strategy.
Empirical study is conducted on \textit{mnist-back-rand},
\textit{mnist-back-image} and \textit{mnist-rot-back-image} datasets generated
by adding random noise in the image background of MNIST dataset. Experimental
results indicate that using MKL in MKMs earns a better representation of the
raw data and improves the classifier performance.

    

### [[2111.13772] Particle Dynamics for Learning EBMs](http://arxiv.org/abs/2111.13772)


  Energy-based modeling is a promising approach to unsupervised learning, which
yields many downstream applications from a single model. The main difficulty in
learning energy-based models with the "contrastive approaches" is the
generation of samples from the current energy function at each iteration. Many
advances have been made to accomplish this subroutine cheaply. Nevertheless,
all such sampling paradigms run MCMC targeting the current model, which
requires infinitely long chains to generate samples from the true energy
distribution and is problematic in practice. This paper proposes an alternative
approach to getting these samples and avoiding crude MCMC sampling from the
current model. We accomplish this by viewing the evolution of the modeling
distribution as (i) the evolution of the energy function, and (ii) the
evolution of the samples from this distribution along some vector field. We
subsequently derive this time-dependent vector field such that the particles
following this field are approximately distributed as the current density
model. Thereby we match the evolution of the particles with the evolution of
the energy function prescribed by the learning procedure. Importantly, unlike
Monte Carlo sampling, our method targets to match the current distribution in a
finite time. Finally, we demonstrate its effectiveness empirically compared to
MCMC-based learning methods.

    

### [[2111.13785] OmiTrans: generative adversarial networks based omics-to-omics translation framework](http://arxiv.org/abs/2111.13785)


  With the rapid development of high-throughput experimental technologies,
different types of omics (e.g., genomics, epigenomics, transcriptomics,
proteomics, and metabolomics) data can be produced from clinical samples. The
correlations between different omics types attracts a lot of research interest,
whereas the stduy on genome-wide omcis data translation (i.e, generation and
prediction of one type of omics data from another type of omics data) is almost
blank. Generative adversarial networks and the variants are one of the most
state-of-the-art deep learning technologies, which have shown great success in
image-to-image translation, text-to-image translation, etc. Here we proposed
OmiTrans, a deep learning framework adopted the idea of generative adversarial
networks to achieve omics-to-omics translation with promising results. OmiTrans
was able to faithfully reconstruct gene expression profiles from DNA
methylation data with high accuracy and great model generalisation, as
demonstrated in the experiments.

    

### [[2111.13786] Learning from learning machines: a new generation of AI technology to meet the needs of science](http://arxiv.org/abs/2111.13786)


  We outline emerging opportunities and challenges to enhance the utility of AI
for scientific discovery. The distinct goals of AI for industry versus the
goals of AI for science create tension between identifying patterns in data
versus discovering patterns in the world from data. If we address the
fundamental challenges associated with "bridging the gap" between domain-driven
scientific models and data-driven AI learning machines, then we expect that
these AI models can transform hypothesis generation, scientific discovery, and
the scientific process itself.

    

### [[2111.13792] LAFITE: Towards Language-Free Training for Text-to-Image Generation](http://arxiv.org/abs/2111.13792)


  One of the major challenges in training text-to-image generation models is
the need of a large number of high-quality image-text pairs. While image
samples are often easily accessible, the associated text descriptions typically
require careful human captioning, which is particularly time- and
cost-consuming. In this paper, we propose the first work to train text-to-image
generation models without any text data. Our method leverages the well-aligned
multi-modal semantic space of the powerful pre-trained CLIP model: the
requirement of text-conditioning is seamlessly alleviated via generating text
features from image features. Extensive experiments are conducted to illustrate
the effectiveness of the proposed method. We obtain state-of-the-art results in
the standard text-to-image generation tasks. Importantly, the proposed
language-free model outperforms most existing models trained with full
image-text pairs. Furthermore, our method can be applied in fine-tuning
pre-trained models, which saves both training time and cost in training
text-to-image generation models. Our pre-trained model obtains competitive
results in zero-shot text-to-image generation on the MS-COCO dataset, yet with
around only 1% of the model size and training data size relative to the
recently proposed large DALL-E model.

    

### [[2111.13800] Feature Selection for Causal Inference from High Dimensional Observational Data with Outcome Adaptive Elastic Net](http://arxiv.org/abs/2111.13800)


  Feature selection is an extensively studied technique in the machine learning
literature where the main objective is to identify the subset of features that
provides the highest predictive power. However, in causal inference, our goal
is to identify the set of variables that are associated with both the treatment
variable and outcome (i.e., the confounders). While controlling for the
confounding variables helps us to achieve an unbiased estimate of causal
effect, recent research shows that controlling for purely outcome predictors
along with the confounders can reduce the variance of the estimate. In this
paper, we propose an Outcome Adaptive Elastic-Net (OAENet) method specifically
designed for causal inference to select the confounders and outcome predictors
for inclusion in the propensity score model or in the matching mechanism.
OAENet provides two major advantages over existing methods: it performs
superiorly on correlated data, and it can be applied to any matching method and
any estimates. In addition, OAENet is computationally efficient compared to
state-of-the-art methods.

    

### [[2111.13802] Factorized Fourier Neural Operators](http://arxiv.org/abs/2111.13802)


  The Fourier Neural Operator (FNO) is a learning-based method for efficiently
simulating partial differential equations. We propose the Factorized Fourier
Neural Operator (F-FNO) that allows much better generalization with deeper
networks. With a careful combination of the Fourier factorization, a shared
kernel integral operator across all layers, the Markov property, and residual
connections, F-FNOs achieve a six-fold reduction in error on the most turbulent
setting of the Navier- Stokes benchmark dataset. We show that our model
maintains an error rate of 2% while still running an order of magnitude faster
than a numerical solver, even when the problem setting is extended to include
additional contexts such as viscosity and time-varying forces. This enables the
same pretrained neural network to model vastly different conditions.

    

### [[2111.13807] Offline Neural Contextual Bandits: Pessimism, Optimization and Generalization](http://arxiv.org/abs/2111.13807)


  Offline policy learning (OPL) leverages existing data collected a priori for
policy optimization without any active exploration. Despite the prevalence and
recent interest in this problem, its theoretical and algorithmic foundations in
function approximation settings remain under-developed. In this paper, we
consider this problem on the axes of distributional shift, optimization, and
generalization in offline contextual bandits with neural networks. In
particular, we propose a provably efficient offline contextual bandit with
neural network function approximation that does not require any functional
assumption on the reward. We show that our method provably generalizes over
unseen contexts under a milder condition for distributional shift than the
existing OPL works. Notably, unlike any other OPL method, our method learns
from the offline data in an online manner using stochastic gradient descent,
allowing us to leverage the benefits of online learning into an offline
setting. Moreover, we show that our method is more computationally efficient
and has a better dependence on the effective dimension of the neural network
than an online counterpart. Finally, we demonstrate the empirical effectiveness
of our method in a range of synthetic and real-world OPL problems.

    

### [[2111.13812] Achieving an Accurate Random Process Model for PV Power using Cheap Data: Leveraging the SDE and Public Weather Reports](http://arxiv.org/abs/2111.13812)


  The stochastic differential equation (SDE)-based random process models of
volatile renewable energy sources (RESs) jointly capture the evolving
probability distribution and temporal correlation in continuous time. It has
enabled recent studies to remarkably improve the performance of power system
dynamic uncertainty quantification and optimization. However, considering the
non-homogeneous random process nature of PV, there still remains a challenging
question: how can a realistic and accurate SDE model for PV power be obtained
that reflects its weather-dependent uncertainty in online operation, especially
when high-resolution numerical weather prediction (NWP) is unavailable for many
distributed plants? To fill this gap, this article finds that an accurate SDE
model for PV power can be constructed by only using the cheap data from
low-resolution public weather reports. Specifically, an hourly parameterized
Jacobi diffusion process is constructed to recreate the temporal patterns of PV
volatility during a day. Its parameters are mapped from the public weather
report using an ensemble of extreme learning machines (ELMs) to reflect the
varying weather conditions. The SDE model jointly captures intraday and
intrahour volatility. Statistical examination based on real-world data
collected in Macau shows the proposed approach outperforms a selection of
state-of-the-art deep learning-based time-series forecast methods.

    

### [[2111.13822] On Learning Domain-Invariant Representations for Transfer Learning with Multiple Sources](http://arxiv.org/abs/2111.13822)


  Domain adaptation (DA) benefits from the rigorous theoretical works that
study its insightful characteristics and various aspects, e.g., learning
domain-invariant representations and its trade-off. However, it seems not the
case for the multiple source DA and domain generalization (DG) settings which
are remarkably more complicated and sophisticated due to the involvement of
multiple source domains and potential unavailability of target domain during
training. In this paper, we develop novel upper-bounds for the target general
loss which appeal to us to define two kinds of domain-invariant
representations. We further study the pros and cons as well as the trade-offs
of enforcing learning each domain-invariant representation. Finally, we conduct
experiments to inspect the trade-off of these representations for offering
practical hints regarding how to use them in practice and explore other
interesting properties of our developed theory.

    

### [[2111.13834] Deep Learning with Multiple Data Set: A Weighted Goal Programming Approach](http://arxiv.org/abs/2111.13834)


  Large-scale data analysis is growing at an exponential rate as data
proliferates in our societies. This abundance of data has the advantage of
allowing the decision-maker to implement complex models in scenarios that were
prohibitive before. At the same time, such an amount of data requires a
distributed thinking approach. In fact, Deep Learning models require plenty of
resources, and distributed training is needed. This paper presents a
Multicriteria approach for distributed learning. Our approach uses the Weighted
Goal Programming approach in its Chebyshev formulation to build an ensemble of
decision rules that optimize aprioristically defined performance metrics. Such
a formulation is beneficial because it is both model and metric agnostic and
provides an interpretable output for the decision-maker. We test our approach
by showing a practical application in electricity demand forecasting. Our
results suggest that when we allow for dataset split overlapping, the
performances of our methodology are consistently above the baseline model
trained on the whole dataset.

    

### [[2111.13839] Towards Principled Disentanglement for Domain Generalization](http://arxiv.org/abs/2111.13839)


  A fundamental challenge for machine learning models is generalizing to
out-of-distribution (OOD) data, in part due to spurious correlations. To tackle
this challenge, we first formalize the OOD generalization problem as
constrained optimization, called Disentanglement-constrained Domain
Generalization (DDG). We relax this non-trivial constrained optimization to a
tractable form with finite-dimensional parameterization and empirical
approximation. Then a theoretical analysis of the extent to which the above
transformations deviates from the original problem is provided. Based on the
transformation, we propose a primal-dual algorithm for joint representation
disentanglement and domain generalization. In contrast to traditional
approaches based on domain adversarial training and domain labels, DDG jointly
learns semantic and variation encoders for disentanglement, enabling flexible
manipulation and augmentation on training data. DDG aims to learn intrinsic
representations of semantic concepts that are invariant to nuisance factors and
generalizable across different domains. Comprehensive experiments on popular
benchmarks show that DDG can achieve competitive OOD performance and uncover
interpretable salient structures within data.

    

### [[2111.13850] Temporal Context Mining for Learned Video Compression](http://arxiv.org/abs/2111.13850)


  We address end-to-end learned video compression with a special focus on
better learning and utilizing temporal contexts. For temporal context mining,
we propose to store not only the previously reconstructed frames, but also the
propagated features into the generalized decoded picture buffer. From the
stored propagated features, we propose to learn multi-scale temporal contexts,
and re-fill the learned temporal contexts into the modules of our compression
scheme, including the contextual encoder-decoder, the frame generator, and the
temporal context encoder. Our scheme discards the parallelization-unfriendly
auto-regressive entropy model to pursue a more practical decoding time. We
compare our scheme with x264 and x265 (representing industrial software for
H.264 and H.265, respectively) as well as the official reference software for
H.264, H.265, and H.266 (JM, HM, and VTM, respectively). When intra period is
32 and oriented to PSNR, our scheme outperforms H.265--HM by 14.4% bit rate
saving; when oriented to MS-SSIM, our scheme outperforms H.266--VTM by 21.1%
bit rate saving.

    

### [[2111.13858] Why MDAC? A Multi-domain Activation Function](http://arxiv.org/abs/2111.13858)


  In this study, a novel, general and ingenious activation function termed MDAC
is proposed to surmount the troubles of gradient vanishing and
non-differentiable existence. MDAC approximately inherits the properties of
exponential activation function (such as Tanh family) and piecewise linear
activation function (such as ReLU family). Specifically, in the positive
region, the adaptive linear structure is designed to respond to various domain
distributions. In the negative region, the combination of exponent and
linearity is considered to conquer the obstacle of gradient vanishing.
Furthermore, the non-differentiable existence is eliminated by smooth
approximation. Experiments show that MDAC improves performance on both
classical models and pre-training optimization models in six domain datasets by
simply changing the activation function, which indicates MDAC's effectiveness
and pro-gressiveness. MDAC is superior to other prevalent activation functions
in robustness and generalization, and can reflect excellent activation
performance in multiple domains.

    

### [[2111.13861] AIS: A nonlinear activation function for industrial safety engineering](http://arxiv.org/abs/2111.13861)


  In the task of Chinese named entity recognition based on deep learning,
activation function plays an irreplaceable role, it introduces nonlinear
characteristics into neural network, so that the fitted model can be applied to
various tasks. However, the information density of industrial safety analysis
text is relatively high, and the correlation and similarity between the
information are large, which is easy to cause the problem of high deviation and
high standard deviation of the model, no specific activation function has been
designed in previous studies, and the traditional activation function has the
problems of gradient vanishing and negative region, which also lead to the
recognition accuracy of the model can not be further improved. To solve these
problems, a novel activation function AIS is proposed in this paper. AIS is an
activation function applied in industrial safety engineering, which is composed
of two piecewise nonlinear functions. In the positive region, the structure
combining exponential function and quadratic function is used to alleviate the
problem of deviation and standard deviation, and the linear function is added
to modify it, which makes the whole activation function smoother and overcomes
the problem of gradient vanishing. In the negative region, the cubic function
structure is used to solve the negative region problem and accelerate the
convergence of the model. Based on the deep learning model of BERT-BiLSTM-CRF,
the performance of AIS is evaluated. The results show that, compared with other
activation functions, AIS overcomes the problems of gradient vanishing and
negative region, reduces the deviation of the model, speeds up the model
fitting, and improves the extraction ability of the model for industrial
entities.

    

### [[2111.13872] Normative Disagreement as a Challenge for Cooperative AI](http://arxiv.org/abs/2111.13872)


  Cooperation in settings where agents have both common and conflicting
interests (mixed-motive environments) has recently received considerable
attention in multi-agent learning. However, the mixed-motive environments
typically studied have a single cooperative outcome on which all agents can
agree. Many real-world multi-agent environments are instead bargaining problems
(BPs): they have several Pareto-optimal payoff profiles over which agents have
conflicting preferences. We argue that typical cooperation-inducing learning
algorithms fail to cooperate in BPs when there is room for normative
disagreement resulting in the existence of multiple competing cooperative
equilibria, and illustrate this problem empirically. To remedy the issue, we
introduce the notion of norm-adaptive policies. Norm-adaptive policies are
capable of behaving according to different norms in different circumstances,
creating opportunities for resolving normative disagreement. We develop a class
of norm-adaptive policies and show in experiments that these significantly
increase cooperation. However, norm-adaptiveness cannot address residual
bargaining failure arising from a fundamental tradeoff between exploitability
and cooperative robustness.

    

### [[2111.13878] A dual semismooth Newton based augmented Lagrangian method for large-scale linearly constrained sparse group square-root Lasso problems](http://arxiv.org/abs/2111.13878)


  Square-root Lasso problems are proven robust regression problems.
Furthermore, square-root regression problems with structured sparsity also
plays an important role in statistics and machine learning. In this paper, we
focus on the numerical computation of large-scale linearly constrained sparse
group square-root Lasso problems. In order to overcome the difficulty that
there are two nonsmooth terms in the objective function, we propose a dual
semismooth Newton (SSN) based augmented Lagrangian method (ALM) for it. That
is, we apply the ALM to the dual problem with the subproblem solved by the SSN
method. To apply the SSN method, the positive definiteness of the generalized
Jacobian is very important. Hence we characterize the equivalence of its
positive definiteness and the constraint nondegeneracy condition of the
corresponding primal problem. In numerical implementation, we fully employ the
second order sparsity so that the Newton direction can be efficiently obtained.
Numerical experiments demonstrate the efficiency of the proposed algorithm.

    

### [[2111.13884] Automated Antenna Testing Using Encoder-Decoder-based Anomaly Detection](http://arxiv.org/abs/2111.13884)


  We propose a new method for testing antenna arrays that records the radiating
electromagnetic (EM) field using an absorbing material and evaluating the
resulting thermal image series through an AI using a conditional
encoder-decoder model. Given the power and phase of the signals fed into each
array element, we are able to reconstruct normal sequences through our trained
model and compare it to the real sequences observed by a thermal camera. These
thermograms only contain low-level patterns such as blobs of various shapes. A
contour-based anomaly detector can then map the reconstruction error matrix to
an anomaly score to identify faulty antenna arrays and increase the
classification F-measure (F-M) by up to 46%. We show our approach on the time
series thermograms collected by our antenna testing system. Conventionally, a
variational autoencoder (VAE) learning observation noise may yield better
results than a VAE with a constant noise assumption. However, we demonstrate
that this is not the case for anomaly detection on such low-level patterns for
two reasons. First, the baseline metric reconstruction probability, which
incorporates the learned observation noise, fails to differentiate anomalous
patterns. Second, the area under the receiver operating characteristic (ROC)
curve of a VAE with a lower observation noise assumption achieves 11.83% higher
than that of a VAE with learned noise.

    

### [[2111.13895] Towards Understanding the Impact of Model Size on Differential Private Classification](http://arxiv.org/abs/2111.13895)


  Differential privacy (DP) is an essential technique for privacy-preserving.
It was found that a large model trained for privacy preserving performs worse
than a smaller model (e.g. ResNet50 performs worse than ResNet18). To better
understand this phenomenon, we study high dimensional DP learning from the
viewpoint of generalization. Theoretically, we show that for the simple
Gaussian model with even small DP noise, if the dimension is large enough, then
the classification error can be as bad as the random guessing. Then we propose
a feature selection method to reduce the size of the model, based on a new
metric which trades off the classification accuracy and privacy preserving.
Experiments on real data support our theoretical results and demonstrate the
advantage of the proposed method.

    

### [[2111.13902] A Quantum-like Model for Predicting Human Decisions in the Entangled Social Systems](http://arxiv.org/abs/2111.13902)


  Human-centered systems of systems such as social networks, Internet of
Things, or healthcare systems are growingly becoming major facets of modern
life. Realistic models of human behavior in such systems play a significant
role in their accurate modeling and prediction. Yet, human behavior under
uncertainty often violates the predictions by the conventional probabilistic
models. Recently, quantum-like decision theories have shown a considerable
potential to explain the contradictions in human behavior by applying quantum
probability. But providing a quantum-like decision theory that could predict,
rather than describe the current, state of human behavior is still one of the
unsolved challenges. The main novelty of our approach is introducing an
entangled Bayesian network inspired by the entanglement concept in quantum
information theory, in which each human is a part of the entire society.
Accordingly, society's effect on the dynamic evolution of the decision-making
process, which is less often considered in decision theories, is modeled by the
entanglement measures. The proposed predictive entangled quantum-like Bayesian
network (PEQBN) is evaluated on 22 experimental tasks. Results confirm that
PEQBN provides more realistic predictions of human decisions under uncertainty,
when compared with classical Bayesian networks and three recent quantum-like
approaches.

    

### [[2111.13905] AdaDM: Enabling Normalization for Image Super-Resolution](http://arxiv.org/abs/2111.13905)


  Normalization like Batch Normalization (BN) is a milestone technique to
normalize the distributions of intermediate layers in deep learning, enabling
faster training and better generalization accuracy. However, in fidelity image
Super-Resolution (SR), it is believed that normalization layers get rid of
range flexibility by normalizing the features and they are simply removed from
modern SR networks. In this paper, we study this phenomenon quantitatively and
qualitatively. We found that the standard deviation of the residual feature
shrinks a lot after normalization layers, which causes the performance
degradation in SR networks. Standard deviation reflects the amount of variation
of pixel values. When the variation becomes smaller, the edges will become less
discriminative for the network to resolve. To address this problem, we propose
an Adaptive Deviation Modulator (AdaDM), in which a modulation factor is
adaptively predicted to amplify the pixel deviation. For better generalization
performance, we apply BN in state-of-the-art SR networks with the proposed
AdaDM. Meanwhile, the deviation amplification strategy in AdaDM makes the edge
information in the feature more distinguishable. As a consequence, SR networks
with BN and our AdaDM can get substantial performance improvements on benchmark
datasets. Extensive experiments have been conducted to show the effectiveness
of our method.

    

### [[2111.13914] Fast and Informative Model Selection using Learning Curve Cross-Validation](http://arxiv.org/abs/2111.13914)


  Common cross-validation (CV) methods like k-fold cross-validation or
Monte-Carlo cross-validation estimate the predictive performance of a learner
by repeatedly training it on a large portion of the given data and testing on
the remaining data. These techniques have two major drawbacks. First, they can
be unnecessarily slow on large datasets. Second, beyond an estimation of the
final performance, they give almost no insights into the learning process of
the validated algorithm. In this paper, we present a new approach for
validation based on learning curves (LCCV). Instead of creating train-test
splits with a large portion of training data, LCCV iteratively increases the
number of instances used for training. In the context of model selection, it
discards models that are very unlikely to become competitive. We run a large
scale experiment on the 67 datasets from the AutoML benchmark and empirically
show that in over 90% of the cases using LCCV leads to similar performance (at
most 1.5% difference) as using 5/10-fold CV. However, it yields substantial
runtime reductions of over 20% on average. Additionally, it provides important
insights, which for example allow assessing the benefits of acquiring more
data. These results are orthogonal to other advances in the field of AutoML.

    

### [[2111.13920] Sparse Subspace Clustering Friendly Deep Dictionary Learning for Hyperspectral Image Classification](http://arxiv.org/abs/2111.13920)


  Subspace clustering techniques have shown promise in hyperspectral image
segmentation. The fundamental assumption in subspace clustering is that the
samples belonging to different clusters/segments lie in separable subspaces.
What if this condition does not hold? We surmise that even if the condition
does not hold in the original space, the data may be nonlinearly transformed to
a space where it will be separable into subspaces. In this work, we propose a
transformation based on the tenets of deep dictionary learning (DDL). In
particular, we incorporate the sparse subspace clustering (SSC) loss in the DDL
formulation. Here DDL nonlinearly transforms the data such that the transformed
representation (of the data) is separable into subspaces. We show that the
proposed formulation improves over the state-of-the-art deep learning
techniques in hyperspectral image clustering.

    

### [[2111.13921] Transformed K-means Clustering](http://arxiv.org/abs/2111.13921)


  In this work we propose a clustering framework based on the paradigm of
transform learning. In simple terms the representation from transform learning
is used for K-means clustering; however, the problem is not solved in such a
naïve piecemeal fashion. The K-means clustering loss is embedded into the
transform learning framework and the joint problem is solved using the
alternating direction method of multipliers. Results on document clustering
show that our proposed approach improves over the state-of-the-art.

    

### [[2111.13931] Resource-Aware Asynchronous Online Federated Learning for Nonlinear Regression](http://arxiv.org/abs/2111.13931)


  Many assumptions in the federated learning literature present a best-case
scenario that can not be satisfied in most real-world applications. An
asynchronous setting reflects the realistic environment in which federated
learning methods must be able to operate reliably. Besides varying amounts of
non-IID data at participants, the asynchronous setting models heterogeneous
client participation due to available computational power and battery
constraints and also accounts for delayed communications between clients and
the server. To reduce the communication overhead associated with asynchronous
online federated learning (ASO-Fed), we use the principles of
partial-sharing-based communication. In this manner, we reduce the
communication load of the participants and, therefore, render participation in
the learning task more accessible. We prove the convergence of the proposed
ASO-Fed and provide simulations to analyze its behavior further. The
simulations reveal that, in the asynchronous setting, it is possible to achieve
the same convergence as the federated stochastic gradient (Online-FedSGD) while
reducing the communication tenfold.

    

### [[2111.13955] A Recommender System-Inspired Cloud Data Filling Scheme for Satellite-based Coastal Observation](http://arxiv.org/abs/2111.13955)


  Filling missing data in cloud-covered areas of satellite imaging is an
important task to improve data quantity and quality for enhanced earth
observation. Traditional cloud filling studies focused on continuous numerical
data such as temperature and cyanobacterial concentration in the open ocean.
Cloud data filling issues in coastal imaging is far less studied because of the
complex landscape. Inspired by the success of data imputation methods in
recommender systems that are designed for online shopping, the present study
explored their application to satellite cloud data filling tasks. A numerical
experiment was designed and conducted for a LandSat dataset with a range of
synthetic cloud covers to examine the performance of different data filling
schemes. The recommender system-inspired matrix factorization algorithm called
Funk-SVD showed superior performance in computational accuracy and efficiency
for the task of recovering landscape types in a complex coastal area than the
traditional data filling scheme of DINEOF (Data Interpolating Empirical
Orthogonal Functions) and the deep learning method of Datawig. The new method
achieved the best filling accuracy and reached a speed comparable to DINEOF and
much faster than deep learning. A theoretical framework was created to analyze
the error propagation in DINEOF and found the algorithm needs to be modified to
converge to the ground truth. The present study showed that Funk-SVD has great
potential to enhance cloud data filling performance and connects the fields of
recommender systems and cloud filling to promote the improvement and sharing of
useful algorithms.

    

### [[2111.13956] Understanding Anharmonic Effects on Hydrogen Desorption Characteristics of Mg$_n$H$_{2n}$ Nanoclusters by ab initio trained Deep Neural Network](http://arxiv.org/abs/2111.13956)


  Magnesium hydride (MgH$_2$) has been widely studied for effective hydrogen
storage. However, its bulk desorption temperature (553 K) is deemed too high
for practical applications. Besides doping, a strategy to decrease such
reaction energy for releasing hydrogen is the use of MgH$_2$-based
nanoparticles (NPs). Here, we investigate first the thermodynamic properties of
Mg$_n$H$_{2n}$ NPs ($n<10$) from first-principles, in particular by assessing
the anharmonic effects on the enthalpy, entropy and thermal expansion by means
of the Stochastic Self Consistent Harmonic Approximation (SSCHA). The latter
method goes beyond previous approaches, typically based on molecular mechanics
and the quasi-harmonic approximation, allowing the ab initio calculation of the
fully-anharmonic free energy. We find an almost linear dependence on
temperature of the interatomic bond lengths - with a relative variation of few
percent over 300K -, alongside with a bond distance decrease of the Mg-H bonds.
In order to increase the size of NPs toward experiments of hydrogen desorption
from MgH$_2$ we devise a computationally effective Machine Learning model
trained to accurately determine the forces and total energies (i.e. the
potential energy surfaces), integrating the latter with the SSCHA model to
fully include the anharmonic effects. We find a significative decrease of the
H-desorption temperature for sub-nanometric clusters Mg$_n$H$_{2n}$ with $n
\leq 10$, with a non-negligible, although little effect due to anharmonicities
(up to 10%).

    

### [[2111.13958] Safe Screening for Sparse Conditional Random Fields](http://arxiv.org/abs/2111.13958)


  Sparse Conditional Random Field (CRF) is a powerful technique in computer
vision and natural language processing for structured prediction. However,
solving sparse CRFs in large-scale applications remains challenging. In this
paper, we propose a novel safe dynamic screening method that exploits an
accurate dual optimum estimation to identify and remove the irrelevant features
during the training process. Thus, the problem size can be reduced
continuously, leading to great savings in the computational cost without
sacrificing any accuracy on the finally learned model. To the best of our
knowledge, this is the first screening method which introduces the dual optimum
estimation technique -- by carefully exploring and exploiting the strong
convexity and the complex structure of the dual problem -- in static screening
methods to dynamic screening. In this way, we can absorb the advantages of both
the static and dynamic screening methods and avoid their drawbacks. Our
estimation would be much more accurate than those developed based on the
duality gap, which contributes to a much stronger screening rule. Moreover, our
method is also the first screening method in sparse CRFs and even structure
prediction models. Experimental results on both synthetic and real-world
datasets demonstrate that the speedup gained by our method is significant.

    

### [[2111.13962] Leveraging Unsupervised Learning to Summarize APIs Discussed in Stack Overflow](http://arxiv.org/abs/2111.13962)


  Automated source code summarization is a task that generates summarized
information about the purpose, usage, and--or implementation of methods and
classes to support understanding of these code entities. Multiple approaches
and techniques have been proposed for supervised and unsupervised learning in
code summarization, however, they were mostly focused on generating a summary
for a piece of code. In addition, very few works have leveraged unofficial
documentation. This paper proposes an automatic and novel approach for
summarizing Android API methods discussed in Stack Overflow that we consider as
unofficial documentation in this research. Our approach takes the API method's
name as an input and generates a natural language summary based on Stack
Overflow discussions of that API method. We have conducted a survey that
involves 16 Android developers to evaluate the quality of our automatically
generated summaries and compare them with the official Android documentation.
Our results demonstrate that while developers find the official documentation
more useful in general, the generated summaries are also competitive, in
particular for offering implementation details, and can be used as a
complementary source for guiding developers in software development and
maintenance tasks.

    

### [[2111.13964] Benchmarking Accuracy and Generalizability of Four Graph Neural Networks Using Large In Vitro ADME Datasets from Different Chemical Spaces](http://arxiv.org/abs/2111.13964)


  In this work, we benchmark a variety of single- and multi-task graph neural
network (GNN) models against lower-bar and higher-bar traditional machine
learning approaches employing human engineered molecular features. We consider
four GNN variants -- Graph Convolutional Network (GCN), Graph Attention Network
(GAT), Message Passing Neural Network (MPNN), and Attentive Fingerprint
(AttentiveFP). So far deep learning models have been primarily benchmarked
using lower-bar traditional models solely based on fingerprints, while more
realistic benchmarks employing fingerprints, whole-molecule descriptors and
predictions from other related endpoints (e.g., LogD7.4) appear to be scarce
for industrial ADME datasets. In addition to time-split test sets based on
Genentech data, this study benefits from the availability of measurements from
an external chemical space (Roche data). We identify GAT as a promising
approach to implementing deep learning models. While all GNN models
significantly outperform lower-bar benchmark traditional models solely based on
fingerprints, only GATs seem to offer a small but consistent improvement over
higher-bar benchmark traditional models. Finally, the accuracy of in vitro
assays from different laboratories predicting the same experimental endpoints
appears to be comparable with the accuracy of GAT single-task models,
suggesting that most of the observed error from the models is a function of the
experimental error propagation.

    

### [[2111.13980] Forecasting Daily COVID-19 Related Calls in VA Health Care System: Predictive Model Development](http://arxiv.org/abs/2111.13980)


  Background: COVID-19 has become a challenge worldwide and properly planning
of medical resources is the key to combating COVID-19. In the US Veteran
Affairs Health Care System (VA), many of the enrollees are susceptible to
COVID-19. Predicting the COVID-19 to allocate medical resources promptly
becomes a critical issue. When the VA enrollees have COVID-19 symptoms, it is
recommended that their first step should be to call the VA Call Center. For
confirmed COVID-19 patients, the median time from the first symptom to hospital
admission was seven days. By predicting the number of COVID-19 related calls,
we could predict imminent surges in healthcare use and plan medical resources
ahead. Objective: The study aims to develop a method to forecast the daily
number of COVID-19 related calls for each of the 110 VA medical centers.
Methods: In the proposed method, we pre-trained a model using a cluster of
medical centers and fine-tuned it for individual medical centers. At the
cluster level, we performed feature selection to select significant features
and automatic hyper-parameter search to select optimal hyper-parameter value
combinations for the model. Conclusions: This study proposed an accurate method
to forecast the daily number of COVID-19 related calls for VA medical centers.
The proposed method was able to overcome modeling challenges by grouping
similar medical centers into clusters to enlarge the dataset for training
models, and using hyper-parameter search to automatically find optimal
hyper-parameter value combinations for models. With the proposed method, surges
in health care can be predicted ahead. This allows health care practitioners to
better plan medical resources and combat COVID-19.

    

### [[2111.13984] NCVX: A User-Friendly and Scalable Package for Nonconvex Optimization in Machine Learning](http://arxiv.org/abs/2111.13984)


  Optimizing nonconvex (NCVX) problems, especially those nonsmooth (NSMT) and
constrained (CSTR), is an essential part of machine learning and deep learning.
But it is hard to reliably solve this type of problems without optimization
expertise. Existing general-purpose NCVX optimization packages are powerful,
but typically cannot handle nonsmoothness. GRANSO is among the first packages
targeting NCVX, NSMT, CSTR problems. However, it has several limitations such
as the lack of auto-differentiation and GPU acceleration, which preclude the
potential broad deployment by non-experts. To lower the technical barrier for
the machine learning community, we revamp GRANSO into a user-friendly and
scalable python package named NCVX, featuring auto-differentiation, GPU
acceleration, tensor input, scalable QP solver, and zero dependency on
proprietary packages. As a highlight, NCVX can solve general CSTR deep learning
problems, the first of its kind. NCVX is available at this https URL, with
detailed documentation and numerous examples from machine learning and other
fields.

    

### [[2111.13987] Multi-modality fusion using canonical correlation analysis methods: Application in breast cancer survival prediction from histology and genomics](http://arxiv.org/abs/2111.13987)


  The availability of multi-modality datasets provides a unique opportunity to
characterize the same object of interest using multiple viewpoints more
comprehensively. In this work, we investigate the use of canonical correlation
analysis (CCA) and penalized variants of CCA (pCCA) for the fusion of two
modalities. We study a simple graphical model for the generation of
two-modality data. We analytically show that, with known model parameters,
posterior mean estimators that jointly use both modalities outperform arbitrary
linear mixing of single modality posterior estimators in latent variable
prediction. Penalized extensions of CCA (pCCA) that incorporate domain
knowledge can discover correlations with high-dimensional, low-sample data,
whereas traditional CCA is inapplicable. To facilitate the generation of
multi-dimensional embeddings with pCCA, we propose two matrix deflation schemes
that enforce desirable properties exhibited by CCA. We propose a two-stage
prediction pipeline using pCCA embeddings generated with deflation for latent
variable prediction by combining all the above. On simulated data, our proposed
model drastically reduces the mean-squared error in latent variable prediction.
When applied to publicly available histopathology data and RNA-sequencing data
from The Cancer Genome Atlas (TCGA) breast cancer patients, our model can
outperform principal components analysis (PCA) embeddings of the same dimension
in survival prediction.

    

### [[2111.14000] Factor-augmented tree ensembles](http://arxiv.org/abs/2111.14000)


  This article proposes an extension for standard time-series regression tree
modelling to handle predictors that show irregularities such as missing
observations, periodic patterns in the form of seasonality and cycles, and
non-stationary trends. In doing so, this approach permits also to enrich the
information set used in tree-based autoregressions via unobserved components.
Furthermore, this manuscript also illustrates a relevant approach to control
over-fitting based on ensemble learning and recent developments in the
jackknife literature. This is strongly beneficial when the number of observed
time periods is small and advantageous compared to benchmark resampling
methods. Empirical results show the benefits of predicting equity squared
returns as a function of their own past and a set of macroeconomic data via
factor-augmented tree ensembles, with respect to simpler benchmarks. As a
by-product, this approach allows to study the real-time importance of economic
news on equity volatility.

    

### [[2111.14003] Answer Generation for Questions With Multiple Information Sources in E-Commerce](http://arxiv.org/abs/2111.14003)


  Automatic question answering is an important yet challenging task in
E-commerce given the millions of questions posted by users about the product
that they are interested in purchasing. Hence, there is a great demand for
automatic answer generation systems that provide quick responses using related
information about the product. There are three sources of knowledge available
for answering a user posted query, they are reviews, duplicate or similar
questions, and specifications. Effectively utilizing these information sources
will greatly aid us in answering complex questions. However, there are two main
challenges present in exploiting these sources: (i) The presence of irrelevant
information and (ii) the presence of ambiguity of sentiment present in reviews
and similar questions. Through this work we propose a novel pipeline (MSQAP)
that utilizes the rich information present in the aforementioned sources by
separately performing relevancy and ambiguity prediction before generating a
response.
Experimental results show that our relevancy prediction model (BERT-QA)
outperforms all other variants and has an improvement of 12.36% in F1 score
compared to the BERT-base baseline. Our generation model (T5-QA) outperforms
the baselines in all content preservation metrics such as BLEU, ROUGE and has
an average improvement of 35.02% in ROUGE and 198.75% in BLEU compared to the
highest performing baseline (HSSC-q). Human evaluation of our pipeline shows us
that our method has an overall improvement in accuracy of 30.7% over the
generation model (T5-QA), resulting in our full pipeline-based approach (MSQAP)
providing more accurate answers. To the best of our knowledge, this is the
first work in the e-commerce domain that automatically generates natural
language answers combining the information present in diverse sources such as
specifications, similar questions, and reviews data.

    

### [[2111.14007] An Entropy Weighted Nonnegative Matrix Factorization Algorithm for Feature Representation](http://arxiv.org/abs/2111.14007)


  Nonnegative matrix factorization (NMF) has been widely used to learn
low-dimensional representations of data. However, NMF pays the same attention
to all attributes of a data point, which inevitably leads to inaccurate
representation. For example, in a human-face data set, if an image contains a
hat on the head, the hat should be removed or the importance of its
corresponding attributes should be decreased during matrix factorizing. This
paper proposes a new type of NMF called entropy weighted NMF (EWNMF), which
uses an optimizable weight for each attribute of each data point to emphasize
their importance. This process is achieved by adding an entropy regularizer to
the cost function and then using the Lagrange multiplier method to solve the
problem. Experimental results with several data sets demonstrate the
feasibility and effectiveness of the proposed method. We make our code
available at this https URL.

    

### [[2111.14008] Federated Gaussian Process: Convergence, Automatic Personalization and Multi-fidelity Modeling](http://arxiv.org/abs/2111.14008)


  In this paper, we propose \texttt{FGPR}: a Federated Gaussian process
($\mathcal{GP}$) regression framework that uses an averaging strategy for model
aggregation and stochastic gradient descent for local client computations.
Notably, the resulting global model excels in personalization as \texttt{FGPR}
jointly learns a global $\mathcal{GP}$ prior across all clients. The predictive
posterior then is obtained by exploiting this prior and conditioning on local
data which encodes personalized features from a specific client. Theoretically,
we show that \texttt{FGPR} converges to a critical point of the full
log-likelihood function, subject to statistical error. Through extensive case
studies we show that \texttt{FGPR} excels in a wide range of applications and
is a promising approach for privacy-preserving multi-fidelity data modeling.

    

### [[2111.14031] FastTrees: Parallel Latent Tree-Induction for Faster Sequence Encoding](http://arxiv.org/abs/2111.14031)


  Inducing latent tree structures from sequential data is an emerging trend in
the NLP research landscape today, largely popularized by recent methods such as
Gumbel LSTM and Ordered Neurons (ON-LSTM). This paper proposes FASTTREES, a new
general purpose neural module for fast sequence encoding. Unlike most previous
works that consider recurrence to be necessary for tree induction, our work
explores the notion of parallel tree induction, i.e., imbuing our model with
hierarchical inductive biases in a parallelizable, non-autoregressive fashion.
To this end, our proposed FASTTREES achieves competitive or superior
performance to ON-LSTM on four well-established sequence modeling tasks, i.e.,
language modeling, logical inference, sentiment analysis and natural language
inference. Moreover, we show that the FASTTREES module can be applied to
enhance Transformer models, achieving performance gains on three sequence
transduction tasks (machine translation, subject-verb agreement and
mathematical language understanding), paving the way for modular tree induction
modules. Overall, we outperform existing state-of-the-art models on logical
inference tasks by +4% and mathematical language understanding by +8%.

    

### [[2111.14034] ORCHARD: A Benchmark For Measuring Systematic Generalization of Multi-Hierarchical Reasoning](http://arxiv.org/abs/2111.14034)


  The ability to reason with multiple hierarchical structures is an attractive
and desirable property of sequential inductive biases for natural language
processing. Do the state-of-the-art Transformers and LSTM architectures
implicitly encode for these biases? To answer this, we propose ORCHARD, a
diagnostic dataset for systematically evaluating hierarchical reasoning in
state-of-the-art neural sequence models. While there have been prior evaluation
frameworks such as ListOps or Logical Inference, our work presents a novel and
more natural setting where our models learn to reason with multiple explicit
hierarchical structures instead of only one, i.e., requiring the ability to do
both long-term sequence memorizing, relational reasoning while reasoning with
hierarchical structure. Consequently, backed by a set of rigorous experiments,
we show that (1) Transformer and LSTM models surprisingly fail in systematic
generalization, and (2) with increased references between hierarchies,
Transformer performs no better than random.

    

### [[2111.14038] Learning Wildfire Model from Incomplete State Observations](http://arxiv.org/abs/2111.14038)


  As wildfires are expected to become more frequent and severe, improved
prediction models are vital to mitigating risk and allocating resources. With
remote sensing data, valuable spatiotemporal statistical models can be created
and used for resource management practices. In this paper, we create a dynamic
model for future wildfire predictions of five locations within the western
United States through a deep neural network via historical burned area and
climate data. The proposed model has distinct features that address the
characteristic need in prediction evaluations, including dynamic online
estimation and time-series modeling. Between locations, local fire event
triggers are not isolated, and there are confounding factors when local data is
analyzed due to incomplete state observations. When compared to existing
approaches that do not account for incomplete state observation within wildfire
time-series data, on average, we are able to achieve higher prediction
performances.

    

### [[2111.14039] Generalization Performance of Empirical Risk Minimization on Over-parameterized Deep ReLU Nets](http://arxiv.org/abs/2111.14039)


  In this paper, we study the generalization performance of global minima for
implementing empirical risk minimization (ERM) on over-parameterized deep ReLU
nets. Using a novel deepening scheme for deep ReLU nets, we rigorously prove
that there exist perfect global minima achieving almost optimal generalization
error bounds for numerous types of data under mild conditions. Since
over-parameterization is crucial to guarantee that the global minima of ERM on
deep ReLU nets can be realized by the widely used stochastic gradient descent
(SGD) algorithm, our results indeed fill a gap between optimization and
generalization.

    

### [[2111.14046] Neural Tangent Kernel of Matrix Product States: Convergence and Applications](http://arxiv.org/abs/2111.14046)


  In this work, we study the Neural Tangent Kernel (NTK) of Matrix Product
States (MPS) and the convergence of its NTK in the infinite bond dimensional
limit. We prove that the NTK of MPS asymptotically converges to a constant
matrix during the gradient descent (training) process (and also the
initialization phase) as the bond dimensions of MPS go to infinity by the
observation that the variation of the tensors in MPS asymptotically goes to
zero during training in the infinite limit. By showing the
positive-definiteness of the NTK of MPS, the convergence of MPS during the
training in the function space (space of functions represented by MPS) is
guaranteed without any extra assumptions of the data set. We then consider the
settings of (supervised) Regression with Mean Square Error (RMSE) and
(unsupervised) Born Machines (BM) and analyze their dynamics in the infinite
bond dimensional limit. The ordinary differential equations (ODEs) which
describe the dynamics of the responses of MPS in the RMSE and BM are derived
and solved in the closed-form. For the Regression, we consider Mercer Kernels
(Gaussian Kernels) and find that the evolution of the mean of the responses of
MPS follows the largest eigenvalue of the NTK. Due to the orthogonality of the
kernel functions in BM, the evolution of different modes (samples) decouples
and the "characteristic time" of convergence in training is obtained.

    

### [[2111.14051] Enabling Super-Fast Deep Learning on Tiny Energy-Harvesting IoT Devices](http://arxiv.org/abs/2111.14051)


  Energy harvesting (EH) IoT devices that operate intermittently without
batteries, coupled with advances in deep neural networks (DNNs), have opened up
new opportunities for enabling sustainable smart applications. Nevertheless,
implementing those computation and memory-intensive intelligent algorithms on
EH devices is extremely difficult due to the challenges of limited resources
and intermittent power supply that causes frequent failures. To address those
challenges, this paper proposes a methodology that enables super-fast deep
learning with low-energy accelerators for tiny energy harvesting devices. We
first propose RAD, a resource-aware structured DNN training framework, which
employs block circulant matrix with ADMM to achieve high compression and model
quantization for leveraging the advantage of various vector operation
accelerators. A DNN implementation method, ACE, is then proposed that employs
low-energy accelerators to profit maximum performance with minor energy
consumption. Finally, we further design FLEX, the system support for
intermittent computation in energy harvesting situations. Experimental results
from three different DNN models demonstrate that RAD, ACE, and FLEX can enable
super-fast and correct inference on energy harvesting devices with up to 4.26X
runtime reduction, up to 7.7X energy reduction with higher accuracy over the
state-of-the-art.

    

### [[2111.14053] Towards Conditional Generation of Minimal Action Potential Pathways for Molecular Dynamics](http://arxiv.org/abs/2111.14053)


  In this paper, we utilized generative models, and reformulate it for problems
in molecular dynamics (MD) simulation, by introducing an MD potential energy
component to our generative model. By incorporating potential energy as
calculated from TorchMD into a conditional generative framework, we attempt to
construct a low-potential energy route of transformation between the
helix~$\rightarrow$~coil structures of a protein. We show how to add an
additional loss function to conditional generative models, motivated by
potential energy of molecular configurations, and also present an optimization
technique for such an augmented loss function. Our results show the benefit of
this additional loss term on synthesizing realistic molecular trajectories.

    

### [[2111.14056] Towards Robust and Automatic Hyper-Parameter Tunning](http://arxiv.org/abs/2111.14056)


  The task of hyper-parameter optimization (HPO) is burdened with heavy
computational costs due to the intractability of optimizing both a model's
weights and its hyper-parameters simultaneously. In this work, we introduce a
new class of HPO method and explore how the low-rank factorization of the
convolutional weights of intermediate layers of a convolutional neural network
can be used to define an analytical response surface for optimizing
hyper-parameters, using only training data. We quantify how this surface
behaves as a surrogate to model performance and can be solved using a
trust-region search algorithm, which we call autoHyper. The algorithm
outperforms state-of-the-art such as Bayesian Optimization and generalizes
across model, optimizer, and dataset selection. The PyTorch codes can be found
in \url{this https URL}.

    

### [[2111.14059] NoFADE: Analyzing Diminishing Returns on CO2 Investment](http://arxiv.org/abs/2111.14059)


  Climate change continues to be a pressing issue that currently affects
society at-large. It is important that we as a society, including the Computer
Vision (CV) community take steps to limit our impact on the environment. In
this paper, we (a) analyze the effect of diminishing returns on CV methods, and
(b) propose a \textit{``NoFADE''}: a novel entropy-based metric to quantify
model--dataset--complexity relationships. We show that some CV tasks are
reaching saturation, while others are almost fully saturated. In this light,
NoFADE allows the CV community to compare models and datasets on a similar
basis, establishing an agnostic platform.

    

### [[2111.14062] P4AI: Approaching AI Ethics through Principlism](http://arxiv.org/abs/2111.14062)


  The field of computer vision is rapidly evolving, particularly in the context
of new methods of neural architecture design. These models contribute to (1)
the Climate Crisis - increased CO2 emissions and (2) the Privacy Crisis - data
leakage concerns. To address the often overlooked impact the Computer Vision
(CV) community has on these crises, we outline a novel ethical framework,
\textit{P4AI}: Principlism for AI, an augmented principlistic view of ethical
dilemmas within AI. We then suggest using P4AI to make concrete recommendations
to the community to mitigate the climate and privacy crises.

    

### [[2111.14069] Escape saddle points by a simple gradient-descent based algorithm](http://arxiv.org/abs/2111.14069)


  Escaping saddle points is a central research topic in nonconvex optimization.
In this paper, we propose a simple gradient-based algorithm such that for a
smooth function $f\colon\mathbb{R}^n\to\mathbb{R}$, it outputs an
$\epsilon$-approximate second-order stationary point in $\tilde{O}(\log
n/\epsilon^{1.75})$ iterations. Compared to the previous state-of-the-art
algorithms by Jin et al. with $\tilde{O}((\log n)^{4}/\epsilon^{2})$ or
$\tilde{O}((\log n)^{6}/\epsilon^{1.75})$ iterations, our algorithm is
polynomially better in terms of $\log n$ and matches their complexities in
terms of $1/\epsilon$. For the stochastic setting, our algorithm outputs an
$\epsilon$-approximate second-order stationary point in $\tilde{O}((\log
n)^{2}/\epsilon^{4})$ iterations. Technically, our main contribution is an idea
of implementing a robust Hessian power method using only gradients, which can
find negative curvature near saddle points and achieve the polynomial speedup
in $\log n$ compared to the perturbed gradient descent methods. Finally, we
also perform numerical experiments that support our results.

    

### [[2111.14088] Multicriteria interpretability driven Deep Learning](http://arxiv.org/abs/2111.14088)


  Deep Learning methods are renowned for their performances, yet their lack of
interpretability prevents them from high-stakes contexts. Recent model agnostic
methods address this problem by providing post-hoc interpretability methods by
reverse-engineering the model's inner workings. However, in many regulated
fields, interpretability should be kept in mind from the start, which means
that post-hoc methods are valid only as a sanity check after model training.
Interpretability from the start, in an abstract setting, means posing a set of
soft constraints on the model's behavior by injecting knowledge and
annihilating possible biases. We propose a Multicriteria technique that allows
to control the feature effects on the model's outcome by injecting knowledge in
the objective function. We then extend the technique by including a non-linear
knowledge function to account for more complex effects and local lack of
knowledge. The result is a Deep Learning model that embodies interpretability
from the start and aligns with the recent regulations. A practical empirical
example based on credit risk, suggests that our approach creates performant yet
robust models capable of overcoming biases derived from data scarcity.

    

### [[2111.14120] Imbalanced data preprocessing techniques utilizing local data characteristics](http://arxiv.org/abs/2111.14120)


  Data imbalance, that is the disproportion between the number of training
observations coming from different classes, remains one of the most significant
challenges affecting contemporary machine learning. The negative impact of data
imbalance on traditional classification algorithms can be reduced by the data
preprocessing techniques, methods that manipulate the training data to
artificially reduce the degree of imbalance. However, the existing data
preprocessing techniques, in particular SMOTE and its derivatives, which
constitute the most prevalent paradigm of imbalanced data preprocessing, tend
to be susceptible to various data difficulty factors. This is in part due to
the fact that the original SMOTE algorithm does not utilize the information
about majority class observations. The focus of this thesis is development of
novel data resampling strategies natively utilizing the information about the
distribution of both minority and majority class. The thesis summarizes the
content of 12 research papers focused on the proposed binary data resampling
strategies, their translation to the multi-class setting, and the practical
application to the problem of histopathological data classification.

    

### [[2111.14148] Computational Complexity of Normalizing Constants for the Product of Determinantal Point Processes](http://arxiv.org/abs/2111.14148)


  We consider the product of determinantal point processes (DPPs), a point
process whose probability mass is proportional to the product of principal
minors of multiple matrices, as a natural, promising generalization of DPPs. We
study the computational complexity of computing its normalizing constant, which
is among the most essential probabilistic inference tasks. Our
complexity-theoretic results (almost) rule out the existence of efficient
algorithms for this task unless the input matrices are forced to have favorable
structures. In particular, we prove the following:
(1) Computing $\sum_S\det({\bf A}_{S,S})^p$ exactly for every (fixed)
positive even integer $p$ is UP-hard and Mod$_3$P-hard, which gives a negative
answer to an open question posed by Kulesza and Taskar.
(2) $\sum_S\det({\bf A}_{S,S})\det({\bf B}_{S,S})\det({\bf C}_{S,S})$ is
NP-hard to approximate within a factor of $2^{O(|I|^{1-\epsilon})}$ or
$2^{O(n^{1/\epsilon})}$ for any $\epsilon>0$, where $|I|$ is the input size and
$n$ is the order of the input matrix. This result is stronger than the
#P-hardness for the case of two matrices derived by Gillenwater.
(3) There exists a $k^{O(k)}n^{O(1)}$-time algorithm for computing
$\sum_S\det({\bf A}_{S,S})\det({\bf B}_{S,S})$, where $k$ is the maximum rank
of $\bf A$ and $\bf B$ or the treewidth of the graph formed by nonzero entries
of $\bf A$ and $\bf B$. Such parameterized algorithms are said to be
fixed-parameter tractable.
These results can be extended to the fixed-size case. Further, we present two
applications of fixed-parameter tractable algorithms given a matrix $\bf A$ of
treewidth $w$:
(4) We can compute a $2^{\frac{n}{2p-1}}$-approximation to $\sum_S\det({\bf
A}_{S,S})^p$ for any fractional number $p>1$ in $w^{O(wp)}n^{O(1)}$ time.
(5) We can find a $2^{\sqrt n}$-approximation to unconstrained MAP inference
in $w^{O(w\sqrt n)}n^{O(1)}$ time.

    

### [[2111.14151] Learning Physical Concepts in Cyber-Physical Systems: A Case Study](http://arxiv.org/abs/2111.14151)


  Machine Learning (ML) has achieved great successes in recent decades, both in
research and in practice. In Cyber-Physical Systems (CPS), ML can for example
be used to optimize systems, to detect anomalies or to identify root causes of
system failures. However, existing algorithms suffer from two major drawbacks:
(i) They are hard to interpret by human experts. (ii) Transferring results from
one systems to another (similar) system is often a challenge. Concept learning,
or Representation Learning (RepL), is a solution to both of these drawbacks;
mimicking the human solution approach to explain-ability and transfer-ability:
By learning general concepts such as physical quantities or system states, the
model becomes interpretable by humans. Furthermore concepts on this abstract
level can normally be applied to a wide range of different systems. Modern ML
methods are already widely used in CPS, but concept learning and transfer
learning are hardly used so far. In this paper, we provide an overview of the
current state of research regarding methods for learning physical concepts in
time series data, which is the primary form of sensor data of CPS. We also
analyze the most important methods from the current state of the art using the
example of a three-tank system. Based on these concrete implementations1, we
discuss the advantages and disadvantages of the methods and show for which
purpose and under which conditions they can be used.

    

### [[2111.14159] Dimensionality Reduction of Longitudinal 'Omics Data using Modern Tensor Factorization](http://arxiv.org/abs/2111.14159)


  Precision medicine is a clinical approach for disease prevention, detection
and treatment, which considers each individual's genetic background,
environment and lifestyle. The development of this tailored avenue has been
driven by the increased availability of omics methods, large cohorts of
temporal samples, and their integration with clinical data. Despite the immense
progression, existing computational methods for data analysis fail to provide
appropriate solutions for this complex, high-dimensional and longitudinal data.
In this work we have developed a new method termed TCAM, a dimensionality
reduction technique for multi-way data, that overcomes major limitations when
doing trajectory analysis of longitudinal omics data. Using real-world data, we
show that TCAM outperforms traditional methods, as well as state-of-the-art
tensor-based approaches for longitudinal microbiome data analysis. Moreover, we
demonstrate the versatility of TCAM by applying it to several different omics
datasets, and the applicability of it as a drop-in replacement within
straightforward ML tasks.

    

### [[2111.14177] Evaluating Generalization and Transfer Capacity of Multi-Agent Reinforcement Learning Across Variable Number of Agents](http://arxiv.org/abs/2111.14177)


  Multi-agent Reinforcement Learning (MARL) problems often require cooperation
among agents in order to solve a task. Centralization and decentralization are
two approaches used for cooperation in MARL. While fully decentralized methods
are prone to converge to suboptimal solutions due to partial observability and
nonstationarity, the methods involving centralization suffer from scalability
limitations and lazy agent problem. Centralized training decentralized
execution paradigm brings out the best of these two approaches; however,
centralized training still has an upper limit of scalability not only for
acquired coordination performance but also for model size and training time. In
this work, we adopt the centralized training with decentralized execution
paradigm and investigate the generalization and transfer capacity of the
trained models across variable number of agents. This capacity is assessed by
training variable number of agents in a specific MARL problem and then
performing greedy evaluations with variable number of agents for each training
configuration. Thus, we analyze the evaluation performance for each combination
of agent count for training versus evaluation. We perform experimental
evaluations on predator prey and traffic junction environments and demonstrate
that it is possible to obtain similar or higher evaluation performance by
training with less agents. We conclude that optimal number of agents to perform
training may differ from the target number of agents and argue that transfer
across large number of agents can be a more efficient solution to scaling up
than directly increasing number of agents during training.

    

### [[2111.14182] Make an Omelette with Breaking Eggs: Zero-Shot Learning for Novel Attribute Synthesis](http://arxiv.org/abs/2111.14182)


  Most of the existing algorithms for zero-shot classification problems
typically rely on the attribute-based semantic relations among categories to
realize the classification of novel categories without observing any of their
instances. However, training the zero-shot classification models still requires
attribute labeling for each class (or even instance) in the training dataset,
which is also expensive. To this end, in this paper, we bring up a new problem
scenario: "Are we able to derive zero-shot learning for novel attribute
detectors/classifiers and use them to automatically annotate the dataset for
labeling efficiency?" Basically, given only a small set of detectors that are
learned to recognize some manually annotated attributes (i.e., the seen
attributes), we aim to synthesize the detectors of novel attributes in a
zero-shot learning manner. Our proposed method, Zero Shot Learning for
Attributes (ZSLA), which is the first of its kind to the best of our knowledge,
tackles this new research problem by applying the set operations to first
decompose the seen attributes into their basic attributes and then recombine
these basic attributes into the novel ones. Extensive experiments are conducted
to verify the capacity of our synthesized detectors for accurately capturing
the semantics of the novel attributes and show their superior performance in
terms of detection and localization compared to other baseline approaches.
Moreover, with using only 32 seen attributes on the Caltech-UCSD Birds-200-2011
dataset, our proposed method is able to synthesize other 207 novel attributes,
while various generalized zero-shot classification algorithms trained upon the
dataset re-annotated by our synthesized attribute detectors are able to provide
comparable performance with those trained with the manual ground-truth
annotations.

    

### [[2111.14200] Transfer Learning with Jukebox for Music Source Separation](http://arxiv.org/abs/2111.14200)


  In this work, we demonstrate how to adapt a publicly available pre-trained
Jukebox model for the problem of audio source separation from a single mixed
audio channel. Our neural network architecture for transfer learning is fast to
train and results demonstrate comparable performance to other state-of-the-art
approaches. We provide an open-source code implementation of our architecture
(this https URL).

    

### [[2111.14204] Count-Based Temperature Scheduling for Maximum Entropy Reinforcement Learning](http://arxiv.org/abs/2111.14204)


  Maximum Entropy Reinforcement Learning (MaxEnt RL) algorithms such as Soft
Q-Learning (SQL) and Soft Actor-Critic trade off reward and policy entropy,
which has the potential to improve training stability and robustness. Most
MaxEnt RL methods, however, use a constant tradeoff coefficient (temperature),
contrary to the intuition that the temperature should be high early in training
to avoid overfitting to noisy value estimates and decrease later in training as
we increasingly trust high value estimates to truly lead to good rewards.
Moreover, our confidence in value estimates is state-dependent, increasing
every time we use more evidence to update an estimate. In this paper, we
present a simple state-based temperature scheduling approach, and instantiate
it for SQL as Count-Based Soft Q-Learning (CBSQL). We evaluate our approach on
a toy domain as well as in several Atari 2600 domains and show promising
results.

    

### [[2111.14212] On Predicting Generalization using GANs](http://arxiv.org/abs/2111.14212)


  Research on generalization bounds for deep networks seeks to give ways to
predict test error using just the training dataset and the network parameters.
While generalization bounds can give many insights about architecture design,
training algorithms etc., what they do not currently do is yield good
predictions for actual test error. A recently introduced Predicting
Generalization in Deep Learning competition aims to encourage discovery of
methods to better predict test error. The current paper investigates a simple
idea: can test error be predicted using 'synthetic data' produced using a
Generative Adversarial Network (GAN) that was trained on the same training
dataset? Upon investigating several GAN models and architectures, we find that
this turns out to be the case. In fact, using GANs pre-trained on standard
datasets, the test error can be predicted without requiring any additional
hyper-parameter tuning. This result is surprising because GANs have well-known
limitations (e.g. mode collapse) and are known to not learn the data
distribution accurately. Yet the generated samples are good enough to
substitute for test data. Several additional experiments are presented to
explore reasons why GANs do well at this task. In addition to a new approach
for predicting generalization, the counter-intuitive phenomena presented in our
work may also call for a better understanding of GANs' strengths and
limitations.

    

### [[2111.14213] Local Learning Matters: Rethinking Data Heterogeneity in Federated Learning](http://arxiv.org/abs/2111.14213)


  Federated learning (FL) is a promising strategy for performing
privacy-preserving, distributed learning with a network of clients (i.e., edge
devices). However, the data distribution among clients is often non-IID in
nature, making efficient optimization difficult. To alleviate this issue, many
FL algorithms focus on mitigating the effects of data heterogeneity across
clients by introducing a variety of proximal terms, some incurring considerable
compute and/or memory overheads, to restrain local updates with respect to the
global model. Instead, we consider rethinking solutions to data heterogeneity
in FL with a focus on local learning generality rather than proximal
restriction. To this end, we first present a systematic study informed by
second-order indicators to better understand algorithm effectiveness in FL.
Interestingly, we find that standard regularization methods are surprisingly
strong performers in mitigating data heterogeneity effects. Based on our
findings, we further propose a simple and effective method, FedAlign, to
overcome data heterogeneity and the pitfalls of previous methods. FedAlign
achieves competitive accuracy with state-of-the-art FL methods across a variety
of settings while minimizing computation and memory overhead. Code will be
publicly available.

    

### [[2111.14219] Approximate Inference via Clustering](http://arxiv.org/abs/2111.14219)


  In recent years, large-scale Bayesian learning draws a great deal of
attention. However, in big-data era, the amount of data we face is growing much
faster than our ability to deal with it. Fortunately, it is observed that
large-scale datasets usually own rich internal structure and is somewhat
redundant. In this paper, we attempt to simplify the Bayesian posterior via
exploiting this structure. Specifically, we restrict our interest to the
so-called well-clustered datasets and construct an \emph{approximate posterior}
according to the clustering information. Fortunately, the clustering structure
can be efficiently obtained via a particular clustering algorithm. When
constructing the approximate posterior, the data points in the same cluster are
all replaced by the centroid of the cluster. As a result, the posterior can be
significantly simplified. Theoretically, we show that under certain conditions
the approximate posterior we construct is close (measured by KL divergence) to
the exact posterior. Furthermore, thorough experiments are conducted to
validate the fact that the constructed posterior is a good approximation to the
true posterior and much easier to sample from.

    

### [[2111.14220] On the Robustness and Generalization of Deep Learning Driven Full Waveform Inversion](http://arxiv.org/abs/2111.14220)


  The data-driven approach has been demonstrated as a promising technique to
solve complicated scientific problems. Full Waveform Inversion (FWI) is
commonly epitomized as an image-to-image translation task, which motivates the
use of deep neural networks as an end-to-end solution. Despite being trained
with synthetic data, the deep learning-driven FWI is expected to perform well
when evaluated with sufficient real-world data. In this paper, we study such
properties by asking: how robust are these deep neural networks and how do they
generalize? For robustness, we prove the upper bounds of the deviation between
the predictions from clean and noisy data. Moreover, we demonstrate an
interplay between the noise level and the additional gain of loss. For
generalization, we prove a norm-based generalization error upper bound via a
stability-generalization framework. Experimental results on seismic FWI
datasets corroborate with the theoretical results, shedding light on a better
understanding of utilizing Deep Learning for complicated scientific
applications.

    

### [[2111.14232] Long-range and hierarchical language predictions in brains and algorithms](http://arxiv.org/abs/2111.14232)


  Deep learning has recently made remarkable progress in natural language
processing. Yet, the resulting algorithms remain far from competing with the
language abilities of the human brain. Predictive coding theory offers a
potential explanation to this discrepancy: while deep language algorithms are
optimized to predict adjacent words, the human brain would be tuned to make
long-range and hierarchical predictions. To test this hypothesis, we analyze
the fMRI brain signals of 304 subjects each listening to 70min of short
stories. After confirming that the activations of deep language algorithms
linearly map onto those of the brain, we show that enhancing these models with
long-range forecast representations improves their brain-mapping. The results
further reveal a hierarchy of predictions in the brain, whereby the
fronto-parietal cortices forecast more abstract and more distant
representations than the temporal cortices. Overall, this study strengthens
predictive coding theory and suggests a critical role of long-range and
hierarchical predictions in natural language processing.

    

### [[2111.14243] EffCNet: An Efficient CondenseNet for Image Classification on NXP BlueBox](http://arxiv.org/abs/2111.14243)


  Intelligent edge devices with built-in processors vary widely in terms of
capability and physical form to perform advanced Computer Vision (CV) tasks
such as image classification and object detection, for example. With constant
advances in the field of autonomous cars and UAVs, embedded systems and mobile
devices, there has been an ever-growing demand for extremely efficient
Artificial Neural Networks (ANN) for real-time inference on these smart edge
devices with constrained computational resources. With unreliable network
connections in remote regions and an added complexity of data transmission, it
is of an utmost importance to capture and process data locally instead of
sending the data to cloud servers for remote processing. Edge devices on the
other hand, offer limited processing power due to their inexpensive hardware,
and limited cooling and computational resources. In this paper, we propose a
novel deep convolutional neural network architecture called EffCNet which is an
improved and an efficient version of CondenseNet Convolutional Neural Network
(CNN) for edge devices utilizing self-querying data augmentation and depthwise
separable convolutional strategies to improve real-time inference performance
as well as reduce the final trained model size, trainable parameters, and
Floating-Point Operations (FLOPs) of EffCNet CNN. Furthermore, extensive
supervised image classification analyses are conducted on two benchmarking
datasets: CIFAR-10 and CIFAR-100, to verify real-time inference performance of
our proposed CNN. Finally, we deploy these trained weights on NXP BlueBox which
is an intelligent edge development platform designed for self-driving vehicles
and UAVs, and conclusions will be extrapolated accordingly.

    

### [[2111.14244] Schema matching using Gaussian mixture models with Wasserstein distance](http://arxiv.org/abs/2111.14244)


  Gaussian mixture models find their place as a powerful tool, mostly in the
clustering problem, but with proper preparation also in feature extraction,
pattern recognition, image segmentation and in general machine learning. When
faced with the problem of schema matching, different mixture models computed on
different pieces of data can maintain crucial information about the structure
of the dataset. In order to measure or compare results from mixture models, the
Wasserstein distance can be very useful, however it is not easy to calculate
for mixture distributions. In this paper we derive one of possible
approximations for the Wasserstein distance between Gaussian mixture models and
reduce it to linear problem. Furthermore, application examples concerning real
world data are shown.

    

### [[2111.14247] A Survey of Large-Scale Deep Learning Serving System Optimization: Challenges and Opportunities](http://arxiv.org/abs/2111.14247)


  Deep Learning (DL) models have achieved superior performance in many
application domains, including vision, language, medical, commercial ads,
entertainment, etc. With the fast development, both DL applications and the
underlying serving hardware have demonstrated strong scaling trends, i.e.,
Model Scaling and Compute Scaling, for example, the recent pre-trained model
with hundreds of billions of parameters with ~TB level memory consumption, as
well as the newest GPU accelerators providing hundreds of TFLOPS. With both
scaling trends, new problems and challenges emerge in DL inference serving
systems, which gradually trends towards Large-scale Deep learning Serving
systems (LDS). This survey aims to summarize and categorize the emerging
challenges and optimization opportunities for large-scale deep learning serving
systems. By providing a novel taxonomy, summarizing the computing paradigms,
and elaborating the recent technique advances, we hope that this survey could
shed light on new optimization perspectives and motivate novel works in
large-scale deep learning system optimization.

    

### [[2111.14248] Fed2: Feature-Aligned Federated Learning](http://arxiv.org/abs/2111.14248)


  Federated learning learns from scattered data by fusing collaborative models
from local nodes. However, the conventional coordinate-based model averaging by
FedAvg ignored the random information encoded per parameter and may suffer from
structural feature misalignment. In this work, we propose Fed2, a
feature-aligned federated learning framework to resolve this issue by
establishing a firm structure-feature alignment across the collaborative
models. Fed2 is composed of two major designs: First, we design a
feature-oriented model structure adaptation method to ensure explicit feature
allocation in different neural network structures. Applying the structure
adaptation to collaborative models, matchable structures with similar feature
information can be initialized at the very early training stage. During the
federated learning process, we then propose a feature paired averaging scheme
to guarantee aligned feature distribution and maintain no feature fusion
conflicts under either IID or non-IID scenarios. Eventually, Fed2 could
effectively enhance the federated learning convergence performance under
extensive homo- and heterogeneous settings, providing excellent convergence
speed, accuracy, and computation/communication efficiency.

    

### [[2111.14251] False Data Injection Threats in Active Distribution Systems: A Comprehensive Survey](http://arxiv.org/abs/2111.14251)


  With the proliferation of smart devices and revolutions in communications,
electrical distribution systems are gradually shifting from passive,
manually-operated and inflexible ones, to a massively interconnected
cyber-physical smart grid to address the energy challenges of the future.
However, the integration of several cutting-edge technologies has introduced
several security and privacy vulnerabilities due to the large-scale complexity
and resource limitations of deployments. Recent research trends have shown that
False Data Injection (FDI) attacks are becoming one of the most malicious cyber
threats within the entire smart grid paradigm. Therefore, this paper presents a
comprehensive survey of the recent advances in FDI attacks within active
distribution systems and proposes a taxonomy to classify the FDI threats with
respect to smart grid targets. The related studies are contrasted and
summarized in terms of the attack methodologies and implications on the
electrical power distribution networks. Finally, we identify some research gaps
and recommend a number of future research directions to guide and motivate
prospective researchers.

    

### [[2111.14260] A Practical Tutorial on Explainable AI Techniques](http://arxiv.org/abs/2111.14260)


  Last years have been characterized by an upsurge of opaque automatic decision
support systems, such as Deep Neural Networks (DNNs). Although they have great
generalization and prediction skills, their functioning does not allow
obtaining detailed explanations of their behaviour. As opaque machine learning
models are increasingly being employed to make important predictions in
critical environments, the danger is to create and use decisions that are not
justifiable or legitimate. Therefore, there is a general agreement on the
importance of endowing machine learning models with explainability. The reason
is that EXplainable Artificial Intelligence (XAI) techniques can serve to
verify and certify model outputs and enhance them with desirable notions such
as trustworthiness, accountability, transparency and fairness. This tutorial is
meant to be the go-to handbook for any audience with a computer science
background aiming at getting intuitive insights of machine learning models,
accompanied with straight, fast, and intuitive explanations out of the box. We
believe that these methods provide a valuable contribution for applying XAI
techniques in their particular day-to-day models, datasets and use-cases.
Figure \ref{fig:Flowchart} acts as a flowchart/map for the reader and should
help him to find the ideal method to use according to his type of data. The
reader will find a description of the proposed method as well as an example of
use and a Python notebook that he can easily modify as he pleases in order to
apply it to his own case of application.

    

### [[2111.14262] Customizing an Affective Tutoring System Based on Facial Expression and Head Pose Estimation](http://arxiv.org/abs/2111.14262)


  In recent years, the main problem in e-learning has shifted from analyzing
content to personalization of learning environment by Intelligence Tutoring
Systems (ITSs). Therefore, by designing personalized teaching models, learners
are able to have a successful and satisfying experience in achieving their
learning goals. Affective Tutoring Systems (ATSs) are some kinds of ITS that
can recognize and respond to affective states of learner. In this study, we
designed, implemented, and evaluated a system to personalize the learning
environment based on the facial emotions recognition, head pose estimation, and
cognitive style of learners. First, a unit called Intelligent Analyzer (AI)
created which was responsible for recognizing facial expression and head angles
of learners. Next, the ATS was built which mainly made of two units: ITS, IA.
Results indicated that with the ATS, participants needed less efforts to pass
the tests. In other words, we observed when the IA unit was activated, learners
could pass the final tests in fewer attempts than those for whom the IA unit
was deactivated. Additionally, they showed an improvement in terms of the mean
passing score and academic satisfaction.

    

### [[2111.14267] Explore the Potential Performance of Vision-and-Language Navigation Model: a Snapshot Ensemble Method](http://arxiv.org/abs/2111.14267)


  Vision-and-Language Navigation (VLN) is a challenging task in the field of
artificial intelligence. Although massive progress has been made in this task
over the past few years attributed to breakthroughs in deep vision and language
models, it remains tough to build VLN models that can generalize as well as
humans. In this paper, we provide a new perspective to improve VLN models.
Based on our discovery that snapshots of the same VLN model behave
significantly differently even when their success rates are relatively the
same, we propose a snapshot-based ensemble solution that leverages predictions
among multiple snapshots. Constructed on the snapshots of the existing
state-of-the-art (SOTA) model $\circlearrowright$BERT and our past-action-aware
modification, our proposed ensemble achieves the new SOTA performance in the
R2R dataset challenge in Navigation Error (NE) and Success weighted by Path
Length (SPL).

    

### [[2111.14271] ExCon: Explanation-driven Supervised Contrastive Learning for Image Classification](http://arxiv.org/abs/2111.14271)


  Contrastive learning has led to substantial improvements in the quality of
learned embedding representations for tasks such as image classification.
However, a key drawback of existing contrastive augmentation methods is that
they may lead to the modification of the image content which can yield
undesired alterations of its semantics. This can affect the performance of the
model on downstream tasks. Hence, in this paper, we ask whether we can augment
image data in contrastive learning such that the task-relevant semantic content
of an image is preserved. For this purpose, we propose to leverage
saliency-based explanation methods to create content-preserving masked
augmentations for contrastive learning. Our novel explanation-driven supervised
contrastive learning (ExCon) methodology critically serves the dual goals of
encouraging nearby image embeddings to have similar content and explanation. To
quantify the impact of ExCon, we conduct experiments on the CIFAR-100 and the
Tiny ImageNet datasets. We demonstrate that ExCon outperforms vanilla
supervised contrastive learning in terms of classification, explanation
quality, adversarial robustness as well as calibration of probabilistic
predictions of the model in the context of distributional shift.

    

### [[2111.14272] Identification of Subgroups With Similar Benefits in Off-Policy Policy Evaluation](http://arxiv.org/abs/2111.14272)


  Off-policy policy evaluation methods for sequential decision making can be
used to help identify if a proposed decision policy is better than a current
baseline policy. However, a new decision policy may be better than a baseline
policy for some individuals but not others. This has motivated a push towards
personalization and accurate per-state estimates of heterogeneous treatment
effects (HTEs). Given the limited data present in many important applications,
individual predictions can come at a cost to accuracy and confidence in such
predictions. We develop a method to balance the need for personalization with
confident predictions by identifying subgroups where it is possible to
confidently estimate the expected difference in a new decision policy relative
to a baseline. We propose a novel loss function that accounts for uncertainty
during the subgroup partitioning phase. In experiments, we show that our method
can be used to form accurate predictions of HTEs where other methods struggle.

    

### [[2111.14282] Customer Sentiment Analysis using Weak Supervision for Customer-Agent Chat](http://arxiv.org/abs/2111.14282)


  Prior work on sentiment analysis using weak supervision primarily focuses on
different reviews such as movies (IMDB), restaurants (Yelp), products
(Amazon).~One under-explored field in this regard is customer chat data for a
customer-agent chat in customer support due to the lack of availability of free
public data. Here, we perform sentiment analysis on customer chat using weak
supervision on our in-house dataset. We fine-tune the pre-trained language
model (LM) RoBERTa as a sentiment classifier using weak supervision. Our
contribution is as follows:1) We show that by using weak sentiment classifiers
along with domain-specific lexicon-based rules as Labeling Functions (LF), we
can train a fairly accurate customer chat sentiment classifier using weak
supervision. 2) We compare the performance of our custom-trained model with
off-the-shelf google cloud NLP API for sentiment analysis. We show that by
injecting domain-specific knowledge using LFs, even with weak supervision, we
can train a model to handle some domain-specific use cases better than
off-the-shelf google cloud NLP API. 3) We also present an analysis of how
customer sentiment in a chat relates to problem resolution.

    

### [[2111.14283] Exploration of Dark Chemical Genomics Space via Portal Learning: Applied to Targeting the Undruggable Genome and COVID-19 Anti-Infective Polypharmacology](http://arxiv.org/abs/2111.14283)


  Advances in biomedicine are largely fueled by exploring uncharted territories
of human biology. Machine learning can both enable and accelerate discovery,
but faces a fundamental hurdle when applied to unseen data with distributions
that differ from previously observed ones -- a common dilemma in scientific
inquiry. We have developed a new deep learning framework, called
{\textit{Portal Learning}}, to explore dark chemical and biological space.
Three key, novel components of our approach include: (i) end-to-end, step-wise
transfer learning, in recognition of biology's sequence-structure-function
paradigm, (ii) out-of-cluster meta-learning, and (iii) stress model selection.
Portal Learning provides a practical solution to the out-of-distribution (OOD)
problem in statistical machine learning. Here, we have implemented Portal
Learning to predict chemical-protein interactions on a genome-wide scale.
Systematic studies demonstrate that Portal Learning can effectively assign
ligands to unexplored gene families (unknown functions), versus existing
state-of-the-art methods, thereby allowing us to target previously
"undruggable" proteins and design novel polypharmacological agents for
disrupting interactions between SARS-CoV-2 and human proteins. Portal Learning
is general-purpose and can be further applied to other areas of scientific
inquiry.

    

### [[2111.14293] A category theory framework for Bayesian learning](http://arxiv.org/abs/2111.14293)


  Inspired by the foundational works by Spivak and Fong and Cruttwell et al.,
we introduce a categorical framework to formalize Bayesian inference and
learning. The two key ideas at play here are the notions of Bayesian inversions
and the functor GL as constructed by Cruttwell et al.. In this context, we find
that Bayesian learning is the simplest case of the learning paradigm. We then
obtain categorical formulations of batch and sequential Bayes updates while
also verifying that the two coincide in a specific example.

    

### [[2111.14294] Towards Autonomous Driving of Personal Mobility with Small and Noisy Dataset using Tsallis-statistics-based Behavioral Cloning](http://arxiv.org/abs/2111.14294)


  Autonomous driving has made great progress and been introduced in practical
use step by step. On the other hand, the concept of personal mobility is also
getting popular, and its autonomous driving specialized for individual drivers
is expected for a new step. However, it is difficult to collect a large driving
dataset, which is basically required for the learning of autonomous driving,
from the individual driver of the personal mobility. In addition, when the
driver is not familiar with the operation of the personal mobility, the dataset
will contain non-optimal data. This study therefore focuses on an autonomous
driving method for the personal mobility with such a small and noisy, so-called
personal, dataset. Specifically, we introduce a new loss function based on
Tsallis statistics that weights gradients depending on the original loss
function and allows us to exclude noisy data in the optimization phase. In
addition, we improve the visualization technique to verify whether the driver
and the controller have the same region of interest. From the experimental
results, we found that the conventional autonomous driving failed to drive
properly due to the wrong operations in the personal dataset, and the region of
interest was different from that of the driver. In contrast, the proposed
method learned robustly against the errors and successfully drove automatically
while paying attention to the similar region to the driver. Attached video is
also uploaded on youtube: this https URL


### [[2111.14297] Data Augmentation For Medical MR Image Using Generative Adversarial Networks](http://arxiv.org/abs/2111.14297)


  Computer-assisted diagnosis (CAD) based on deep learning has become a crucial
diagnostic technology in the medical industry, effectively improving diagnosis
accuracy. However, the scarcity of brain tumor Magnetic Resonance (MR) image
datasets causes the low performance of deep learning algorithms. The
distribution of transformed images generated by traditional data augmentation
(DA) intrinsically resembles the original ones, resulting in a limited
performance in terms of generalization ability. This work improves Progressive
Growing of GANs with a structural similarity loss function (PGGAN-SSIM) to
solve image blurriness problems and model collapse. We also explore other
GAN-based data augmentation to demonstrate the effectiveness of the proposed
model. Our results show that PGGAN-SSIM successfully generates 256x256
realistic brain tumor MR images which fill the real image distribution
uncovered by the original dataset. Furthermore, PGGAN-SSIM exceeds other
GAN-based methods, achieving promising performance improvement in Frechet
Inception Distance (FID) and Multi-scale Structural Similarity (MS-SSIM).

    

### [[2111.14309] A General Framework for Defending Against Backdoor Attacks via Influence Graph](http://arxiv.org/abs/2111.14309)


  In this work, we propose a new and general framework to defend against
backdoor attacks, inspired by the fact that attack triggers usually follow a
\textsc{specific} type of attacking pattern, and therefore, poisoned training
examples have greater impacts on each other during training. We introduce the
notion of the {\it influence graph}, which consists of nodes and edges
respectively representative of individual training points and associated
pair-wise influences. The influence between a pair of training points
represents the impact of removing one training point on the prediction of
another, approximated by the influence function \citep{koh2017understanding}.
Malicious training points are extracted by finding the maximum average
sub-graph subject to a particular size. Extensive experiments on computer
vision and natural language processing tasks demonstrate the effectiveness and
generality of the proposed framework.

    

### [[2111.14310] How Can Creativity Occur in Multi-Agent Systems?](http://arxiv.org/abs/2111.14310)


  Complex systems show how surprising and beautiful phenomena can emerge from
structures or agents following simple rules. With the recent success of deep
reinforcement learning (RL), a natural path forward would be to use the
capabilities of multiple deep RL agents to produce emergent behavior of greater
benefit and sophistication. In general, this has proved to be an unreliable
strategy without significant computation due to the difficulties inherent in
multi-agent RL training. In this paper, we propose some criteria for creativity
in multi-agent RL. We hope this proposal will give artists applying multi-agent
RL a starting point, and provide a catalyst for further investigation guided by
philosophical discussion.

    

### [[2111.14311] The CSIRO Crown-of-Thorn Starfish Detection Dataset](http://arxiv.org/abs/2111.14311)


  Crown-of-Thorn Starfish (COTS) outbreaks are a major cause of coral loss on
the Great Barrier Reef (GBR) and substantial surveillance and control programs
are underway in an attempt to manage COTS populations to ecologically
sustainable levels. We release a large-scale, annotated underwater image
dataset from a COTS outbreak area on the GBR, to encourage research on Machine
Learning and AI-driven technologies to improve the detection, monitoring, and
management of COTS populations at reef scale. The dataset is released and
hosted in a Kaggle competition that challenges the international Machine
Learning community with the task of COTS detection from these underwater
images.

    

### [[2111.14319] TinyDefectNet: Highly Compact Deep Neural Network Architecture for High-Throughput Manufacturing Visual Quality Inspection](http://arxiv.org/abs/2111.14319)


  A critical aspect in the manufacturing process is the visual quality
inspection of manufactured components for defects and flaws. Human-only visual
inspection can be very time-consuming and laborious, and is a significant
bottleneck especially for high-throughput manufacturing scenarios. Given
significant advances in the field of deep learning, automated visual quality
inspection can lead to highly efficient and reliable detection of defects and
flaws during the manufacturing process. However, deep learning-driven visual
inspection methods often necessitate significant computational resources, thus
limiting throughput and act as a bottleneck to widespread adoption for enabling
smart factories. In this study, we investigated the utilization of a
machine-driven design exploration approach to create TinyDefectNet, a highly
compact deep convolutional network architecture tailored for high-throughput
manufacturing visual quality inspection. TinyDefectNet comprises of just ~427K
parameters and has a computational complexity of ~97M FLOPs, yet achieving a
detection accuracy of a state-of-the-art architecture for the task of surface
defect detection on the NEU defect benchmark dataset. As such, TinyDefectNet
can achieve the same level of detection performance at 52$\times$ lower
architectural complexity and 11x lower computational complexity. Furthermore,
TinyDefectNet was deployed on an AMD EPYC 7R32, and achieved 7.6x faster
throughput using the native Tensorflow environment and 9x faster throughput
using AMD ZenDNN accelerator library. Finally, explainability-driven
performance validation strategy was conducted to ensure correct decision-making
behaviour was exhibited by TinyDefectNet to improve trust in its usage by
operators and inspectors.

    

### [[2111.14324] Is the Rush to Machine Learning Jeopardizing Safety? Results of a Survey](http://arxiv.org/abs/2111.14324)


  Machine learning (ML) is finding its way into safety-critical systems (SCS).
Current safety standards and practice were not designed to cope with ML
techniques, and it is difficult to be confident that SCSs that contain ML
components are safe. Our hypothesis was that there has been a rush to deploy ML
techniques at the expense of a thorough examination as to whether the use of ML
techniques introduces safety problems that we are not yet adequately able to
detect and mitigate against. We thus conducted a targeted literature survey to
determine the research effort that has been expended in applying ML to SCS
compared with that spent on evaluating the safety of SCSs that deploy ML
components. This paper presents the (surprising) results of the survey.

    

### [[2111.14330] Sparse DETR: Efficient End-to-End Object Detection with Learnable Sparsity](http://arxiv.org/abs/2111.14330)


  DETR is the first end-to-end object detector using a transformer
encoder-decoder architecture and demonstrates competitive performance but low
computational efficiency on high resolution feature maps. The subsequent work,
Deformable DETR, enhances the efficiency of DETR by replacing dense attention
with deformable attention, which achieves 10x faster convergence and improved
performance. Deformable DETR uses the multiscale feature to ameliorate
performance, however, the number of encoder tokens increases by 20x compared to
DETR, and the computation cost of the encoder attention remains a bottleneck.
In our preliminary experiment, we observe that the detection performance hardly
deteriorates even if only a part of the encoder token is updated. Inspired by
this observation, we propose Sparse DETR that selectively updates only the
tokens expected to be referenced by the decoder, thus help the model
effectively detect objects. In addition, we show that applying an auxiliary
detection loss on the selected tokens in the encoder improves the performance
while minimizing computational overhead. We validate that Sparse DETR achieves
better performance than Deformable DETR even with only 10% encoder tokens on
the COCO dataset. Albeit only the encoder tokens are sparsified, the total
computation cost decreases by 38% and the frames per second (FPS) increases by
42% compared to Deformable DETR.
Code is available at this https URL


### [[2111.14331] Improving Experience Replay with Successor Representation](http://arxiv.org/abs/2111.14331)


  Prioritized experience replay is a reinforcement learning technique shown to
speed up learning by allowing agents to replay useful past experiences more
frequently. This usefulness is quantified as the expected gain from replaying
the experience, and is often approximated as the prediction error (TD-error)
observed during the corresponding experience. However, prediction error is only
one possible prioritization metric. Recent work in neuroscience suggests that,
in biological organisms, replay is prioritized by both gain and need. The need
term measures the expected relevance of each experience with respect to the
current situation, and more importantly, this term is not currently considered
in algorithms such as deep Q-network (DQN). Thus, in this paper we present a
new approach for prioritizing experiences for replay that considers both gain
and need. We test our approach by considering the need term, quantified as the
Successor Representation, into the sampling process of different reinforcement
learning algorithms. Our proposed algorithms show a significant increase in
performance in benchmarks including the Dyna-Q maze and a selection of Atari
games.

    

### [[2111.14338] Improving Deep Learning Interpretability by Saliency Guided Training](http://arxiv.org/abs/2111.14338)


  Saliency methods have been widely used to highlight important input features
in model predictions. Most existing methods use backpropagation on a modified
gradient function to generate saliency maps. Thus, noisy gradients can result
in unfaithful feature attributions. In this paper, we tackle this issue and
introduce a {\it saliency guided training}procedure for neural networks to
reduce noisy gradients used in predictions while retaining the predictive
performance of the model. Our saliency guided training procedure iteratively
masks features with small and potentially noisy gradients while maximizing the
similarity of model outputs for both masked and unmasked inputs. We apply the
saliency guided training procedure to various synthetic and real data sets from
computer vision, natural language processing, and time series across diverse
neural architectures, including Recurrent Neural Networks, Convolutional
Networks, and Transformers. Through qualitative and quantitative evaluations,
we show that saliency guided training procedure significantly improves model
interpretability across various domains while preserving its predictive
performance.

    

### [[1711.07271] Positive semi-definite embedding for dimensionality reduction and out-of-sample extensions](http://arxiv.org/abs/1711.07271)


  In machine learning or statistics, it is often desirable to reduce the
dimensionality of a sample of data points in a high dimensional space
$\mathbb{R}^d$. This paper introduces a dimensionality reduction method where
the embedding coordinates are the eigenvectors of a positive semi-definite
kernel obtained as the solution of an infinite dimensional analogue of a
semi-definite program. This embedding is adaptive and non-linear. We discuss
this problem both with weak and strong smoothness assumptions about the learned
kernel. A main feature of our approach is the existence of an out-of-sample
extension formula of the embedding coordinates in both cases. This
extrapolation formula yields an extension of the kernel matrix to a
data-dependent Mercer kernel function. Our empirical results indicate that this
embedding method is more robust with respect to the influence of outliers,
compared with a spectral embedding method.

    

### [[1811.05927] Improvements on SCORE, Especially for Weak Signals](http://arxiv.org/abs/1811.05927)


  A network may have weak signals and severe degree heterogeneity, and may be
very sparse in one occurrence but very dense in another. SCORE (Jin, 2015) is a
recent approach to network community detection. It accommodates severe degree
heterogeneity and is adaptive to different levels of sparsity, but its
performance for networks with weak signals is unclear. In this paper, we show
that in a broad class of network settings where we allow for weak signals,
severe degree heterogeneity, and a wide range of network sparsity, SCORE
achieves prefect clustering and has the so-called "exponential rate" in Hamming
clustering errors. The proof uses the most recent advancement on entry-wise
bounds for the leading eigenvectors of the network adjacency matrix.
The theoretical analysis assures us that SCORE continues to work well in the
weak signal settings, but it does not rule out the possibility that SCORE may
be further improved to have better performance in real applications, especially
for networks with weak signals. As a second contribution of the paper, we
propose SCORE+ as an improved version of SCORE. We investigate SCORE+ with 8
network data sets and found that it outperforms several representative
approaches. In particular, for the 6 data sets with relatively strong signals,
SCORE+ has similar performance as that of SCORE, but for the 2 data sets
(Simmons, Caltech) with possibly weak signals, SCORE+ has much lower error
rates. SCORE+ proposes several changes to SCORE. We carefully explain the
rationale underlying each of these changes, using a mixture of theoretical and
numerical study.

    

### [[1901.00456] Cost-sensitive Selection of Variables by Ensemble of Model Sequences](http://arxiv.org/abs/1901.00456)


  Many applications require the collection of data on different variables or
measurements over many system performance metrics. We term those broadly as
measures or variables. Often data collection along each measure incurs a cost,
thus it is desirable to consider the cost of measures in modeling. This is a
fairly new class of problems in the area of cost-sensitive learning. A few
attempts have been made to incorporate costs in combining and selecting
measures. However, existing studies either do not strictly enforce a budget
constraint, or are not the `most' cost effective. With a focus on
classification problem, we propose a computationally efficient approach that
could find a near optimal model under a given budget by exploring the most
`promising' part of the solution space. Instead of outputting a single model,
we produce a model schedule -- a list of models, sorted by model costs and
expected predictive accuracy. This could be used to choose the model with the
best predictive accuracy under a given budget, or to trade off between the
budget and the predictive accuracy. Experiments on some benchmark datasets show
that our approach compares favorably to competing methods.

    

### [[1906.06784] Interpolated Adversarial Training: Achieving Robust Neural Networks without Sacrificing Too Much Accuracy](http://arxiv.org/abs/1906.06784)


  Adversarial robustness has become a central goal in deep learning, both in
the theory and the practice. However, successful methods to improve the
adversarial robustness (such as adversarial training) greatly hurt
generalization performance on the unperturbed data. This could have a major
impact on how the adversarial robustness affects real world systems (i.e. many
may opt to forego robustness if it can improve accuracy on the unperturbed
data). We propose Interpolated Adversarial Training, which employs recently
proposed interpolation based training methods in the framework of adversarial
training. On CIFAR-10,adversarial training increases the standard test error
(when there is no adversary) from 4.43% to 12.32%, whereas with our
Interpolated adversarial training we retain the adversarial robustness while
achieving a standard test error of only 6.45%. With our technique, the relative
increase in the standard error for the robust model is reduced from 178.1% to
just 45.5%. Moreover, we provide mathematical analysis of Interpolated
Adversarial Training to confirm its efficiencies and demonstrate its advantages
in terms of robustness and generalization.

    

### [[1906.08253] When to Trust Your Model: Model-Based Policy Optimization](http://arxiv.org/abs/1906.08253)


  Designing effective model-based reinforcement learning algorithms is
difficult because the ease of data generation must be weighed against the bias
of model-generated data. In this paper, we study the role of model usage in
policy optimization both theoretically and empirically. We first formulate and
analyze a model-based reinforcement learning algorithm with a guarantee of
monotonic improvement at each step. In practice, this analysis is overly
pessimistic and suggests that real off-policy data is always preferable to
model-generated on-policy data, but we show that an empirical estimate of model
generalization can be incorporated into such analysis to justify model usage.
Motivated by this analysis, we then demonstrate that a simple procedure of
using short model-generated rollouts branched from real data has the benefits
of more complicated model-based algorithms without the usual pitfalls. In
particular, this approach surpasses the sample efficiency of prior model-based
methods, matches the asymptotic performance of the best model-free algorithms,
and scales to horizons that cause other model-based methods to fail entirely.

    

### [[1909.11015] diffGrad: An Optimization Method for Convolutional Neural Networks](http://arxiv.org/abs/1909.11015)


  Stochastic Gradient Decent (SGD) is one of the core techniques behind the
success of deep neural networks. The gradient provides information on the
direction in which a function has the steepest rate of change. The main problem
with basic SGD is to change by equal sized steps for all parameters,
irrespective of gradient behavior. Hence, an efficient way of deep network
optimization is to make adaptive step sizes for each parameter. Recently,
several attempts have been made to improve gradient descent methods such as
AdaGrad, AdaDelta, RMSProp and Adam. These methods rely on the square roots of
exponential moving averages of squared past gradients. Thus, these methods do
not take advantage of local change in gradients. In this paper, a novel
optimizer is proposed based on the difference between the present and the
immediate past gradient (i.e., diffGrad). In the proposed diffGrad optimization
technique, the step size is adjusted for each parameter in such a way that it
should have a larger step size for faster gradient changing parameters and a
lower step size for lower gradient changing parameters. The convergence
analysis is done using the regret bound approach of online learning framework.
Rigorous analysis is made in this paper over three synthetic complex non-convex
functions. The image categorization experiments are also conducted over the
CIFAR10 and CIFAR100 datasets to observe the performance of diffGrad with
respect to the state-of-the-art optimizers such as SGDM, AdaGrad, AdaDelta,
RMSProp, AMSGrad, and Adam. The residual unit (ResNet) based Convolutional
Neural Networks (CNN) architecture is used in the experiments. The experiments
show that diffGrad outperforms other optimizers. Also, we show that diffGrad
performs uniformly well for training CNN using different activation functions.
The source code is made publicly available at
this https URL.

    

### [[1910.01055] QuaRL: Quantization for Sustainable Reinforcement Learning](http://arxiv.org/abs/1910.01055)


  Deep reinforcement learning has achieved significant milestones, however, the
computational demands of reinforcement learning training and inference remain
substantial. Quantization is an effective method to reduce the computational
overheads of neural networks, though in the context of reinforcement learning,
it is unknown whether quantization's computational benefits outweigh the
accuracy costs introduced by the corresponding quantization error. To quantify
this tradeoff we perform a broad study applying quantization to reinforcement
learning. We apply standard quantization techniques such as post-training
quantization (PTQ) and quantization aware training (QAT) to a comprehensive set
of reinforcement learning tasks (Atari, Gym), algorithms (A2C, DDPG, DQN, D4PG,
PPO), and models (MLPs, CNNs) and show that policies may be quantized to 8-bits
without degrading reward, enabling significant inference speedups on
resource-constrained edge devices. Motivated by the effectiveness of standard
quantization techniques on reinforcement learning policies, we introduce a
novel quantization algorithm, \textit{ActorQ}, for quantized actor-learner
distributed reinforcement learning training. By leveraging full precision
optimization on the learner and quantized execution on the actors,
\textit{ActorQ} enables 8-bit inference while maintaining convergence. We
develop a system for quantized reinforcement learning training around
\textit{ActorQ} and demonstrate end to end speedups of $>$ 1.5 $\times$ - 2.5
$\times$ over full precision training on a range of tasks (Deepmind Control
Suite). Finally, we break down the various runtime costs of distributed
reinforcement learning training (such as communication time, inference time,
model load time, etc) and evaluate the effects of quantization on these system
attributes.

    

### [[1910.09185] Exploring Simple and Transferable Recognition-Aware Image Processing](http://arxiv.org/abs/1910.09185)


  Recent progress in image recognition has stimulated the deployment of vision
systems at an unprecedented scale. As a result, visual data are now often
consumed not only by humans but also by machines. Existing image processing
methods only optimize for better human perception, yet the resulting images may
not be accurately recognized by machines. This can be undesirable, e.g., the
images can be improperly handled by search engines or recommendation systems.
In this work, we examine simple approaches to improve machine recognition of
processed images: optimizing the recognition loss directly on the image
processing network or through an intermediate transforming model.
Interestingly, the processing model's ability to enhance recognition quality
can transfer when evaluated on models of different architectures, recognized
categories, tasks and training datasets. This makes the methods applicable even
when we do not have the knowledge of future recognition models, e.g., if we
upload processed images to the Internet. We conduct experiments on multiple
image processing tasks, with ImageNet classification and PASCAL VOC detection
as recognition tasks. With these simple yet effective methods, substantial
accuracy gain can be achieved with strong transferability and minimal image
quality loss. Through a user study we further show that the accuracy gain can
transfer to a black-box cloud model. Finally, we try to explain this
transferability phenomenon by demonstrating the similarities of different
models' decision boundaries. Code is available at
this https URL .

    

### [[1910.09191] Regularization Matters in Policy Optimization](http://arxiv.org/abs/1910.09191)


  Deep Reinforcement Learning (Deep RL) has been receiving increasingly more
attention thanks to its encouraging performance on a variety of control tasks.
Yet, conventional regularization techniques in training neural networks (e.g.,
$L_2$ regularization, dropout) have been largely ignored in RL methods,
possibly because agents are typically trained and evaluated in the same
environment, and because the deep RL community focuses more on high-level
algorithm designs. In this work, we present the first comprehensive study of
regularization techniques with multiple policy optimization algorithms on
continuous control tasks. Interestingly, we find conventional regularization
techniques on the policy networks can often bring large improvement, especially
on harder tasks. Our findings are shown to be robust against training
hyperparameter variations. We also compare these techniques with the more
widely used entropy regularization. In addition, we study regularizing
different components and find that only regularizing the policy network is
typically the best. We further analyze why regularization may help
generalization in RL from four perspectives - sample complexity, reward
distribution, weight norm, and noise robustness. We hope our study provides
guidance for future practices in regularizing policy optimization algorithms.
Our code is available at this https URL .

    

### [[1911.06854] Empirical Study of Off-Policy Policy Evaluation for Reinforcement Learning](http://arxiv.org/abs/1911.06854)


  We offer an experimental benchmark and empirical study for off-policy policy
evaluation (OPE) in reinforcement learning, which is a key problem in many
safety critical applications. Given the increasing interest in deploying
learning-based methods, there has been a flurry of recent proposals for OPE
method, leading to a need for standardized empirical analyses. Our work takes a
strong focus on diversity of experimental design to enable stress testing of
OPE methods. We provide a comprehensive benchmarking suite to study the
interplay of different attributes on method performance. We distill the results
into a summarized set of guidelines for OPE in practice. Our software package,
the Caltech OPE Benchmarking Suite (COBS), is open-sourced and we invite
interested researchers to further contribute to the benchmark.

    

### [[2001.05489] CDGAN: Cyclic Discriminative Generative Adversarial Networks for Image-to-Image Transformation](http://arxiv.org/abs/2001.05489)


  Generative Adversarial Networks (GANs) have facilitated a new direction to
tackle the image-to-image transformation problem. Different GANs use generator
and discriminator networks with different losses in the objective function.
Still there is a gap to fill in terms of both the quality of the generated
images and close to the ground truth images. In this work, we introduce a new
Image-to-Image Transformation network named Cyclic Discriminative Generative
Adversarial Networks (CDGAN) that fills the above mentioned gaps. The proposed
CDGAN generates high quality and more realistic images by incorporating the
additional discriminator networks for cycled images in addition to the original
architecture of the CycleGAN. The proposed CDGAN is tested over three
image-to-image transformation datasets. The quantitative and qualitative
results are analyzed and compared with the state-of-the-art methods. The
proposed CDGAN method outperforms the state-of-the-art methods when compared
over the three baseline Image-to-Image transformation datasets. The code is
available at this https URL.

    

### [[2002.05936] Human Perception of Intrinsically Motivated Autonomy in Human-Robot Interaction](http://arxiv.org/abs/2002.05936)


  A challenge in using robots in human-inhabited environments is to design
behavior that is engaging, yet robust to the perturbations induced by human
interaction. Our idea is to imbue the robot with intrinsic motivation (IM) so
that it can handle new situations and appears as a genuine social other to
humans and thus be of more interest to a human interaction partner. Human-robot
interaction (HRI) experiments mainly focus on scripted or teleoperated robots,
that mimic characteristics such as IM to control isolated behavior factors.
This article presents a "robotologist" study design that allows comparing
autonomously generated behaviors with each other, and, for the first time,
evaluates the human perception of IM-based generated behavior in robots. We
conducted a within-subjects user study (N=24) where participants interacted
with a fully autonomous Sphero BB8 robot with different behavioral regimes: one
realizing an adaptive, intrinsically motivated behavior and the other being
reactive, but not adaptive. The robot and its behaviors are intentionally kept
minimal to concentrate on the effect induced by IM. A quantitative analysis of
post-interaction questionnaires showed a significantly higher perception of the
dimension "Warmth" compared to the reactive baseline behavior. Warmth is
considered a primary dimension for social attitude formation in human social
cognition. A human perceived as warm (friendly, trustworthy) experiences more
positive social interactions.

    

### [[2003.09960] Efficient Clustering for Stretched Mixtures: Landscape and Optimality](http://arxiv.org/abs/2003.09960)


  This paper considers a canonical clustering problem where one receives
unlabeled samples drawn from a balanced mixture of two elliptical distributions
and aims for a classifier to estimate the labels. Many popular methods
including PCA and k-means require individual components of the mixture to be
somewhat spherical, and perform poorly when they are stretched. To overcome
this issue, we propose a non-convex program seeking for an affine transform to
turn the data into a one-dimensional point cloud concentrating around $-1$ and
$1$, after which clustering becomes easy. Our theoretical contributions are
two-fold: (1) we show that the non-convex loss function exhibits desirable
geometric properties when the sample size exceeds some constant multiple of the
dimension, and (2) we leverage this to prove that an efficient first-order
algorithm achieves near-optimal statistical precision without good
initialization. We also propose a general methodology for clustering with
flexible choices of feature transforms and loss objectives.

    

### [[2003.12659] Semiparametric Inference For Causal Effects In Graphical Models With Hidden Variables](http://arxiv.org/abs/2003.12659)


  Identification theory for causal effects in causal models associated with
hidden variable directed acyclic graphs (DAGs) is well studied. However, the
corresponding algorithms are underused due to the complexity of estimating the
identifying functionals they output. In this work, we bridge the gap between
identification and estimation of population-level causal effects involving a
single treatment and a single outcome. We derive influence function based
estimators that exhibit double robustness for the identified effects in a large
class of hidden variable DAGs where the treatment satisfies a simple graphical
criterion; this class includes models yielding the adjustment and front-door
functionals as special cases. We also provide necessary and sufficient
conditions under which the statistical model of a hidden variable DAG is
nonparametrically saturated and implies no equality constraints on the observed
data distribution. Further, we derive an important class of hidden variable
DAGs that imply observed data distributions observationally equivalent (up to
equality constraints) to fully observed DAGs. In these classes of DAGs, we
derive estimators that achieve the semiparametric efficiency bounds for the
target of interest where the treatment satisfies our graphical criterion.
Finally, we provide a sound and complete identification algorithm that directly
yields a weight based estimation strategy for any identifiable effect in hidden
variable causal models.

    

### [[2007.01289] Image Shape Manipulation from a Single Augmented Training Sample](http://arxiv.org/abs/2007.01289)


  In this paper, we present DeepSIM, a generative model for conditional image
manipulation based on a single image. We find that extensive augmentation is
key for enabling single image training, and incorporate the use of
thin-plate-spline (TPS) as an effective augmentation. Our network learns to map
between a primitive representation of the image to the image itself. The choice
of a primitive representation has an impact on the ease and expressiveness of
the manipulations and can be automatic (e.g. edges), manual (e.g. segmentation)
or hybrid such as edges on top of segmentations. At manipulation time, our
generator allows for making complex image changes by modifying the primitive
input representation and mapping it through the network. Our method is shown to
achieve remarkable performance on image manipulation tasks.

    

### [[2007.03047] Leveraging Class Hierarchies with Metric-Guided Prototype Learning](http://arxiv.org/abs/2007.03047)


  In many classification tasks, the set of target classes can be organized into
a hierarchy. This structure induces a semantic distance between classes, and
can be summarised under the form of a cost matrix, which defines a finite
metric on the class set. In this paper, we propose to model the hierarchical
class structure by integrating this metric in the supervision of a prototypical
network. Our method relies on jointly learning a feature-extracting network and
a set of class prototypes whose relative arrangement in the embedding space
follows an hierarchical metric. We show that this approach allows for a
consistent improvement of the error rate weighted by the cost matrix when
compared to traditional methods and other prototype-based strategies.
Furthermore, when the induced metric contains insight on the data structure,
our method improves the overall precision as well. Experiments on four
different public datasets - from agricultural time series classification to
depth image semantic segmentation - validate our approach.

    

### [[2007.03834] Language Modeling with Reduced Densities](http://arxiv.org/abs/2007.03834)


  This work originates from the observation that today's state-of-the-art
statistical language models are impressive not only for their performance, but
also - and quite crucially - because they are built entirely from correlations
in unstructured text data. The latter observation prompts a fundamental
question that lies at the heart of this paper: What mathematical structure
exists in unstructured text data? We put forth enriched category theory as a
natural answer. We show that sequences of symbols from a finite alphabet, such
as those found in a corpus of text, form a category enriched over
probabilities. We then address a second fundamental question: How can this
information be stored and modeled in a way that preserves the categorical
structure? We answer this by constructing a functor from our enriched category
of text to a particular enriched category of reduced density operators. The
latter leverages the Loewner order on positive semidefinite operators, which
can further be interpreted as a toy example of entailment.

    

### [[2007.04462] Scalable Computations of Wasserstein Barycenter via Input Convex Neural Networks](http://arxiv.org/abs/2007.04462)


  Wasserstein Barycenter is a principled approach to represent the weighted
mean of a given set of probability distributions, utilizing the geometry
induced by optimal transport. In this work, we present a novel scalable
algorithm to approximate the Wasserstein Barycenters aiming at high-dimensional
applications in machine learning. Our proposed algorithm is based on the
Kantorovich dual formulation of the Wasserstein-2 distance as well as a recent
neural network architecture, input convex neural network, that is known to
parametrize convex functions. The distinguishing features of our method are: i)
it only requires samples from the marginal distributions; ii) unlike the
existing approaches, it represents the Barycenter with a generative model and
can thus generate infinite samples from the barycenter without querying the
marginal distributions; iii) it works similar to Generative Adversarial Model
in one marginal case. We demonstrate the efficacy of our algorithm by comparing
it with the state-of-art methods in multiple experiments.

    

### [[2007.06075] Identifying Latent Stochastic Differential Equations](http://arxiv.org/abs/2007.06075)


  We present a method for learning latent stochastic differential equations
(SDEs) from high-dimensional time series data. Given a high-dimensional time
series generated from a lower dimensional latent unknown Itô process, the
proposed method learns the mapping from ambient to latent space, and the
underlying SDE coefficients, through a self-supervised learning approach. Using
the framework of variational autoencoders, we consider a conditional generative
model for the data based on the Euler-Maruyama approximation of SDE solutions.
Furthermore, we use recent results on identifiability of latent variable models
to show that the proposed model can recover not only the underlying SDE
coefficients, but also the original latent variables, up to an isometry, in the
limit of infinite data. We validate the method through several simulated video
processing tasks, where the underlying SDE is known, and through real world
datasets.

    

### [[2009.04324] Overcoming the curse of dimensionality with Laplacian regularization in semi-supervised learning](http://arxiv.org/abs/2009.04324)


  As annotations of data can be scarce in large-scale practical problems,
leveraging unlabelled examples is one of the most important aspects of machine
learning. This is the aim of semi-supervised learning. To benefit from the
access to unlabelled data, it is natural to diffuse smoothly knowledge of
labelled data to unlabelled one. This induces to the use of Laplacian
regularization. Yet, current implementations of Laplacian regularization suffer
from several drawbacks, notably the well-known curse of dimensionality. In this
paper, we provide a statistical analysis to overcome those issues, and unveil a
large body of spectral filtering methods that exhibit desirable behaviors. They
are implemented through (reproducing) kernel methods, for which we provide
realistic computational guidelines in order to make our method usable with
large amounts of data.

    

### [[2010.01823] Quantifying Statistical Significance of Neural Network-based Image Segmentation by Selective Inference](http://arxiv.org/abs/2010.01823)


  Although a vast body of literature relates to image segmentation methods that
use deep neural networks (DNNs), less attention has been paid to assessing the
statistical reliability of segmentation results. In this study, we interpret
the segmentation results as hypotheses driven by DNN (called DNN-driven
hypotheses) and propose a method by which to quantify the reliability of these
hypotheses within a statistical hypothesis testing framework. Specifically, we
consider a statistical hypothesis test for the difference between the object
and background regions. This problem is challenging, as the difference would be
falsely large because of the adaptation of the DNN to the data. To overcome
this difficulty, we introduce a conditional selective inference (SI) framework
-- a new statistical inference framework for data-driven hypotheses that has
recently received considerable attention -- to compute exact (non-asymptotic)
valid p-values for the segmentation results. To use the conditional SI
framework for DNN-based segmentation, we develop a new SI algorithm based on
the homotopy method, which enables us to derive the exact (non-asymptotic)
sampling distribution of DNN-driven hypothesis. We conduct experiments on both
synthetic and real-world datasets, through which we offer evidence that our
proposed method can successfully control the false positive rate, has good
performance in terms of computational efficiency, and provides good results
when applied to medical image data.

    

### [[2010.04894] HAMLET: A Hierarchical Agent-based Machine Learning Platform](http://arxiv.org/abs/2010.04894)


  Hierarchical Multi-Agent Systems provide convenient and relevant ways to
analyze, model, and simulate complex systems composed of a large number of
entities that interact at different levels of abstraction. In this paper, we
introduce HAMLET (Hierarchical Agent-based Machine LEarning plaTform), a hybrid
machine learning platform based on hierarchical multi-agent systems, to
facilitate the research and democratization of geographically and/or locally
distributed machine learning entities. The proposed system models a machine
learning solutions as a hypergraph and autonomously sets up a multi-level
structure of heterogeneous agents based on their innate capabilities and
learned skills. HAMLET aids the design and management of machine learning
systems and provides analytical capabilities for research communities to assess
the existing and/or new algorithms/datasets through flexible and customizable
queries. The proposed hybrid machine learning platform does not assume
restrictions on the type of learning algorithms/datasets and is theoretically
proven to be sound and complete with polynomial computational requirements.
Additionally, it is examined empirically on 120 training and four generalized
batch testing tasks performed on 24 machine learning algorithms and 9 standard
datasets. The provided experimental results not only establish confidence in
the platform's consistency and correctness but also demonstrate its testing and
analytical capacity.

    

### [[2010.05627] Towards Theoretically Understanding Why SGD Generalizes Better Than ADAM in Deep Learning](http://arxiv.org/abs/2010.05627)


  It is not clear yet why ADAM-alike adaptive gradient algorithms suffer from
worse generalization performance than SGD despite their faster training speed.
This work aims to provide understandings on this generalization gap by
analyzing their local convergence behaviors. Specifically, we observe the heavy
tails of gradient noise in these algorithms. This motivates us to analyze these
algorithms through their Levy-driven stochastic differential equations (SDEs)
because of the similar convergence behaviors of an algorithm and its SDE. Then
we establish the escaping time of these SDEs from a local basin. The result
shows that (1) the escaping time of both SGD and ADAM~depends on the Radon
measure of the basin positively and the heaviness of gradient noise negatively;
(2) for the same basin, SGD enjoys smaller escaping time than ADAM, mainly
because (a) the geometry adaptation in ADAM~via adaptively scaling each
gradient coordinate well diminishes the anisotropic structure in gradient noise
and results in larger Radon measure of a basin; (b) the exponential gradient
average in ADAM~smooths its gradient and leads to lighter gradient noise tails
than SGD. So SGD is more locally unstable than ADAM~at sharp minima defined as
the minima whose local basins have small Radon measure, and can better escape
from them to flatter ones with larger Radon measure. As flat minima here which
often refer to the minima at flat or asymmetric basins/valleys often generalize
better than sharp ones , our result explains the better generalization
performance of SGD over ADAM. Finally, experimental results confirm our
heavy-tailed gradient noise assumption and theoretical affirmation.

    

### [[2010.06855] GreedyFool: Multi-Factor Imperceptibility and Its Application to Designing a Black-box Adversarial Attack](http://arxiv.org/abs/2010.06855)


  Adversarial examples are well-designed input samples, in which perturbations
are imperceptible to the human eyes, but easily mislead the output of deep
neural networks (DNNs). Existing works synthesize adversarial examples by
leveraging simple metrics to penalize perturbations, that lack sufficient
consideration of the human visual system (HVS), which produces noticeable
artifacts. To explore why the perturbations are visible, this paper summarizes
four primary factors affecting the perceptibility of human eyes. Based on this
investigation, we design a multi-factor metric MulFactorLoss for measuring the
perceptual loss between benign examples and adversarial ones. In order to test
the imperceptibility of the multi-factor metric, we propose a novel black-box
adversarial attack that is referred to as GreedyFool. GreedyFool applies
differential evolution to evaluate the effects of perturbed pixels on the
confidence of a target DNN, and introduces greedy approximation to
automatically generate adversarial perturbations. We conduct extensive
experiments on the ImageNet and CIFRA-10 datasets and a comprehensive user
study with 60 participants. The experimental results demonstrate that
MulFactorLoss is a more imperceptible metric than the existing pixelwise
metrics, and GreedyFool achieves a 100% success rate in a black-box manner.

    

### [[2012.03772] Continuum Limit of Lipschitz Learning on Graphs](http://arxiv.org/abs/2012.03772)


  Tackling semi-supervised learning problems with graph-based methods has
become a trend in recent years since graphs can represent all kinds of data and
provide a suitable framework for studying continuum limits, e.g., of
differential operators. A popular strategy here is $p$-Laplacian learning,
which poses a smoothness condition on the sought inference function on the set
of unlabeled data. For $p<\infty$ continuum limits of this approach were
studied using tools from $\Gamma$-convergence. For the case $p=\infty$, which
is referred to as Lipschitz learning, continuum limits of the related
infinity-Laplacian equation were studied using the concept of viscosity
solutions.
In this work, we prove continuum limits of Lipschitz learning using
$\Gamma$-convergence. In particular, we define a sequence of functionals which
approximate the largest local Lipschitz constant of a graph function and prove
$\Gamma$-convergence in the $L^\infty$-topology to the supremum norm of the
gradient as the graph becomes denser. Furthermore, we show compactness of the
functionals which implies convergence of minimizers. In our analysis we allow a
varying set of labeled data which converges to a general closed set in the
Hausdorff distance. We apply our results to nonlinear ground states, i.e.,
minimizers with constrained $L^p$-norm, and, as a by-product, prove convergence
of graph distance functions to geodesic distance functions.

    

### [[2012.06731] PiRank: Scalable Learning To Rank via Differentiable Sorting](http://arxiv.org/abs/2012.06731)


  A key challenge with machine learning approaches for ranking is the gap
between the performance metrics of interest and the surrogate loss functions
that can be optimized with gradient-based methods. This gap arises because
ranking metrics typically involve a sorting operation which is not
differentiable w.r.t. the model parameters. Prior works have proposed
surrogates that are loosely related to ranking metrics or simple smoothed
versions thereof, and often fail to scale to real-world applications. We
propose PiRank, a new class of differentiable surrogates for ranking, which
employ a continuous, temperature-controlled relaxation to the sorting operator
based on NeuralSort [1]. We show that PiRank exactly recovers the desired
metrics in the limit of zero temperature and further propose a divide
and-conquer extension that scales favorably to large list sizes, both in theory
and practice. Empirically, we demonstrate the role of larger list sizes during
training and show that PiRank significantly improves over comparable approaches
on publicly available internet-scale learning-to-rank benchmarks.

    

### [[2012.12896] How Does a Neural Network's Architecture Impact Its Robustness to Noisy Labels?](http://arxiv.org/abs/2012.12896)


  Noisy labels are inevitable in large real-world datasets. In this work, we
explore an area understudied by previous works -- how the network's
architecture impacts its robustness to noisy labels. We provide a formal
framework connecting the robustness of a network to the alignments between its
architecture and target/noise functions. Our framework measures a network's
robustness via the predictive power in its representations -- the test
performance of a linear model trained on the learned representations using a
small set of clean labels. We hypothesize that a network is more robust to
noisy labels if its architecture is more aligned with the target function than
the noise. To support our hypothesis, we provide both theoretical and empirical
evidence across various neural network architectures and different domains. We
also find that when the network is well-aligned with the target function, its
predictive power in representations could improve upon state-of-the-art (SOTA)
noisy-label-training methods in terms of test accuracy and even outperform
sophisticated methods that use clean labels.

    

### [[2101.03989] Technology Readiness Levels for Machine Learning Systems](http://arxiv.org/abs/2101.03989)


  The development and deployment of machine learning (ML) systems can be
executed easily with modern tools, but the process is typically rushed and
means-to-an-end. The lack of diligence can lead to technical debt, scope creep
and misaligned objectives, model misuse and failures, and expensive
consequences. Engineering systems, on the other hand, follow well-defined
processes and testing standards to streamline development for high-quality,
reliable results. The extreme is spacecraft systems, where mission critical
measures and robustness are ingrained in the development process. Drawing on
experience in both spacecraft engineering and ML (from research through product
across domain areas), we have developed a proven systems engineering approach
for machine learning development and deployment. Our "Machine Learning
Technology Readiness Levels" (MLTRL) framework defines a principled process to
ensure robust, reliable, and responsible systems while being streamlined for ML
workflows, including key distinctions from traditional software engineering.
Even more, MLTRL defines a lingua franca for people across teams and
organizations to work collaboratively on artificial intelligence and machine
learning technologies. Here we describe the framework and elucidate it with
several real world use-cases of developing ML methods from basic research
through productization and deployment, in areas such as medical diagnostics,
consumer computer vision, satellite imagery, and particle physics.

    

### [[2102.06548] Is Q-Learning Minimax Optimal? A Tight Sample Complexity Analysis](http://arxiv.org/abs/2102.06548)


  Q-learning, which seeks to learn the optimal Q-function of a Markov decision
process (MDP) in a model-free fashion, lies at the heart of reinforcement
learning. When it comes to the synchronous setting (such that independent
samples for all state-action pairs are drawn from a generative model in each
iteration), substantial progress has been made towards understanding the sample
efficiency of Q-learning. Consider a $\gamma$-discounted infinite-horizon MDP
with state space $\mathcal{S}$ and action space $\mathcal{A}$: to yield an
entrywise $\varepsilon$-approximation of the optimal Q-function,
state-of-the-art theory for Q-learning requires a sample size exceeding the
order of $\frac{|\mathcal{S}||\mathcal{A}|}{(1-\gamma)^5\varepsilon^{2}}$,
which fails to match existing minimax lower bounds. This gives rise to natural
questions: what is the sharp sample complexity of Q-learning? Is Q-learning
provably sub-optimal? This paper addresses these questions for the synchronous
setting: (1) when $|\mathcal{A}|=1$ (so that Q-learning reduces to TD
learning), we prove that the sample complexity of TD learning is minimax
optimal and scales as $\frac{|\mathcal{S}|}{(1-\gamma)^3\varepsilon^2}$ (up to
log factor); (2) when $|\mathcal{A}|\geq 2$, we settle the sample complexity of
Q-learning to be on the order of
$\frac{|\mathcal{S}||\mathcal{A}|}{(1-\gamma)^4\varepsilon^2}$ (up to log
factor). Our theory unveils the strict sub-optimality of Q-learning when
$|\mathcal{A}|\geq 2$, and rigorizes the negative impact of over-estimation in
Q-learning. Finally, we extend our analysis to accommodate asynchronous
Q-learning (i.e., the case with Markovian samples), sharpening the horizon
dependency of its sample complexity to be $\frac{1}{(1-\gamma)^4}$.

    

### [[2102.08111] Multivariable Fractional Polynomials for lithium-ion batteries degradation models under dynamic conditions](http://arxiv.org/abs/2102.08111)


  Longevity and safety of lithium-ion batteries are facilitated by efficient
monitoring and adjustment of the battery operating conditions. Hence, it is
crucial to implement fast and accurate algorithms for State of Health (SoH)
monitoring on the Battery Management System. The task is challenging due to the
complexity and multitude of the factors contributing to the battery
degradation, especially because the different degradation processes occur at
various timescales and their interactions play an important role. Data-driven
methods bypass this issue by approximating the complex processes with
statistical or machine learning models. This paper proposes a data-driven
approach which is understudied in the context of battery degradation, despite
its simplicity and ease of computation: the Multivariable Fractional Polynomial
(MFP) regression. Models are trained from historical data of one exhausted cell
and used to predict the SoH of other cells. The data are characterised by
varying loads simulating dynamic operating conditions. Two hypothetical
scenarios are considered: one assumes that a recent capacity measurement is
known, the other is based only on the nominal capacity. It was shown that the
degradation behaviour of the batteries under examination is influenced by their
historical data, as supported by the low prediction errors achieved (root mean
squared errors from 1.2% to 7.22% when considering data up to the battery End
of Life). Moreover, we offer a multi-factor perspective where the degree of
impact of each different factor is analysed. Finally, we compare with a Long
Short-Term Memory Neural Network and other works from the literature on the
same dataset. We conclude that the MFP regression is effective and competitive
with contemporary works, and provides several additional advantages e.g. in
terms of interpretability, generalisability, and implementability.

    

### [[2102.12192] Multiplicative Reweighting for Robust Neural Network Optimization](http://arxiv.org/abs/2102.12192)


  Yet, their performance degrades in the presence of noisy labels at train
time. Inspired by the setting of learning with expert advice, where
multiplicative weights (MW) updates were recently shown to be robust to
moderate data corruptions in expert advice, we propose to use MW for
reweighting examples during neural networks optimization. We theoretically
establish the convergence of our method when used with gradient descent and
prove its advantage for label noise in 1d cases. We then validate empirically
our findings for the general case by showing that MW improves neural networks
accuracy in the presence of label noise on CIFAR-10, CIFAR-100 and Clothing1M.
We also show the impact of our approach on adversarial robustness.

    

### [[2103.13686] Robust subgroup discovery](http://arxiv.org/abs/2103.13686)


  We introduce the problem of robust subgroup discovery, i.e., finding a set of
interpretable descriptions of subsets that 1) stand out with respect to one or
more target attributes, 2) are statistically robust, and 3) non-redundant. Many
attempts have been made to mine either locally robust subgroups or to tackle
the pattern explosion, but we are the first to address both challenges at the
same time from a global modelling perspective. First, we formulate the broad
model class of subgroup lists, i.e., ordered sets of subgroups, for univariate
and multivariate targets that can consist of nominal or numeric variables, and
that includes traditional top-1 subgroup discovery in its definition. This
novel model class allows us to formalise the problem of optimal robust subgroup
discovery using the Minimum Description Length (MDL) principle, where we resort
to optimal Normalised Maximum Likelihood and Bayesian encodings for nominal and
numeric targets, respectively. Second, as finding optimal subgroup lists is
NP-hard, we propose SSD++, a greedy heuristic that finds good subgroup lists
and guarantees that the most significant subgroup found according to the MDL
criterion is added in each iteration, which is shown to be equivalent to a
Bayesian one-sample proportions, multinomial, or t-test between the subgroup
and dataset marginal target distributions plus a multiple hypothesis testing
penalty. We empirically show on 54 datasets that SSD++ outperforms previous
subgroup set discovery methods in terms of quality and subgroup list size.

    

### [[2103.14572] Sparse Object-level Supervision for Instance Segmentation with Pixel Embeddings](http://arxiv.org/abs/2103.14572)


  Most state-of-the-art instance segmentation methods have to be trained on
densely annotated images. While difficult in general, this requirement is
especially daunting for biomedical images, where domain expertise is often
required for annotation and no large public data collections are available for
pre-training. We propose to address the dense annotation bottleneck by
introducing a proposal-free segmentation approach based on non-spatial
embeddings, which exploits the structure of the learned embedding space to
extract individual instances in a differentiable way. The segmentation loss can
then be applied directly to instances and the overall pipeline can be trained
in a fully- or weakly supervised manner, including the challenging case of
positive-unlabeled supervision, where a novel self-supervised consistency loss
is introduced for the unlabeled parts of the training data. We evaluate the
proposed method on 2D and 3D segmentation problems in different microscopy
modalities as well as on the Cityscapes and CVPPP instance segmentation
benchmarks, achieving state-of-the-art results on the latter. The code is
available at: this https URL


### [[2103.14776] Scalable and Efficient Neural Speech Coding: A Hybrid Design](http://arxiv.org/abs/2103.14776)


  We present a scalable and efficient neural waveform coding system for speech
compression. We formulate the speech coding problem as an autoencoding task,
where a convolutional neural network (CNN) performs encoding and decoding as a
neural waveform codec (NWC) during its feedforward routine. The proposed NWC
also defines quantization and entropy coding as a trainable module, so the
coding artifacts and bitrate control are handled during the optimization
process. We achieve efficiency by introducing compact model components to NWC,
such as gated residual networks and depthwise separable convolution.
Furthermore, the proposed models are with a scalable architecture, cross-module
residual learning (CMRL), to cover a wide range of bitrates. To this end, we
employ the residual coding concept to concatenate multiple NWC autoencoding
modules, where each NWC module performs residual coding to restore any
reconstruction loss that its preceding modules have created. CMRL can scale
down to cover lower bitrates as well, for which it employs linear predictive
coding (LPC) module as its first autoencoder. The hybrid design integrates LPC
and NWC by redefining LPC's quantization as a differentiable process, making
the system training an end-to-end manner. The decoder of proposed system is
with either one NWC (0.12 million parameters) in low to medium bitrate ranges
(12 to 20 kbps) or two NWCs in the high bitrate (32 kbps). Although the
decoding complexity is not yet as low as that of conventional speech codecs, it
is significantly reduced from that of other neural speech coders, such as a
WaveNet-based vocoder. For wide-band speech coding quality, our system yields
comparable or superior performance to AMR-WB and Opus on TIMIT test utterances
at low and medium bitrates. The proposed system can scale up to higher bitrates
to achieve near transparent performance.

    

### [[2104.01568] Information-theoretic regularization for Multi-source Domain Adaptation](http://arxiv.org/abs/2104.01568)


  Adversarial learning strategy has demonstrated remarkable performance in
dealing with single-source Domain Adaptation (DA) problems, and it has recently
been applied to Multi-source DA (MDA) problems. Although most existing MDA
strategies rely on a multiple domain discriminator setting, its effect on the
latent space representations has been poorly understood. Here we adopt an
information-theoretic approach to identify and resolve the potential adverse
effect of the multiple domain discriminators on MDA: disintegration of
domain-discriminative information, limited computational scalability, and a
large variance in the gradient of the loss during training. We examine the
above issues by situating adversarial DA in the context of information
regularization. This also provides a theoretical justification for using a
single and unified domain discriminator. Based on this idea, we implement a
novel neural architecture called a Multi-source Information-regularized
Adaptation Networks (MIAN). Large-scale experiments demonstrate that MIAN,
despite its structural simplicity, reliably and significantly outperforms other
state-of-the-art methods.

    

### [[2104.01575] Reliably fast adversarial training via latent adversarial perturbation](http://arxiv.org/abs/2104.01575)


  While multi-step adversarial training is widely popular as an effective
defense method against strong adversarial attacks, its computational cost is
notoriously expensive, compared to standard training. Several single-step
adversarial training methods have been proposed to mitigate the above-mentioned
overhead cost; however, their performance is not sufficiently reliable
depending on the optimization setting. To overcome such limitations, we deviate
from the existing input-space-based adversarial training regime and propose a
single-step latent adversarial training method (SLAT), which leverages the
gradients of latent representation as the latent adversarial perturbation. We
demonstrate that the L1 norm of feature gradients is implicitly regularized
through the adopted latent perturbation, thereby recovering local linearity and
ensuring reliable performance, compared to the existing single-step adversarial
training methods. Because latent perturbation is based on the gradients of the
latent representations which can be obtained for free in the process of input
gradients computation, the proposed method costs roughly the same time as the
fast gradient sign method. Experiment results demonstrate that the proposed
method, despite its structural simplicity, outperforms state-of-the-art
accelerated adversarial training methods.

    

### [[2104.12419] ECLIPSE : Envisioning Cloud Induced Perturbations in Solar Energy](http://arxiv.org/abs/2104.12419)


  Efficient integration of solar energy into the electricity mix depends on a
reliable anticipation of its intermittency. A promising approach to forecast
the temporal variability of solar irradiance resulting from the cloud cover
dynamics is based on the analysis of sequences of ground-taken sky images or
satellite images. Despite encouraging results, a recurrent limitation of
existing deep learning approaches lies in the ubiquitous tendency of reacting
to past observations rather than actively anticipating future events. This
leads to a frequent temporal lag and limited ability to predict sudden events.
To address this challenge, we introduce ECLIPSE, a spatio-temporal neural
network architecture that models cloud motion from sky images to not only
predict future irradiance levels but also segmented images, which provide
richer information on the local irradiance map. We show that ECLIPSE
anticipates critical events and reduces temporal delay while generating
visually realistic futures.

    

### [[2105.00373] Investigating the Impact of Multi-LiDAR Placement on Object Detection for Autonomous Driving](http://arxiv.org/abs/2105.00373)


  The past few years have witnessed an increasing interest in improving the
perception performance of LiDARs on autonomous vehicles. While most of the
existing works focus on developing new deep learning algorithms or model
architectures, we study the problem from the physical design perspective, i.e.,
how different placements of multiple LiDARs influence the learning-based
perception. To this end, we introduce an easy-to-compute information-theoretic
surrogate metric to quantitatively and fast evaluate LiDAR placement for 3D
detection of different types of objects. We also present a new data collection,
detection model training and evaluation framework in the realistic CARLA
simulator to evaluate disparate multi-LiDAR configurations. Using several
prevalent placements inspired by the designs of self-driving companies, we show
the correlation between our surrogate metric and object detection performance
of different representative algorithms on KITTI through extensive experiments,
validating the effectiveness of our LiDAR placement evaluation approach. Our
results show that sensor placement is non-negligible in 3D point cloud-based
object detection, which will contribute to 5% ~ 10% performance discrepancy in
terms of average precision in challenging 3D object detection settings. We
believe that this is one of the first studies to quantitatively investigate the
influence of LiDAR placement on perception performance.

    

### [[2105.03480] A semigroup method for high dimensional elliptic PDEs and eigenvalue problems based on neural networks](http://arxiv.org/abs/2105.03480)


  In this paper, we propose a semigroup method for solving high-dimensional
elliptic partial differential equations (PDEs) and the associated eigenvalue
problems based on neural networks. For the PDE problems, we reformulate the
original equations as variational problems with the help of semigroup operators
and then solve the variational problems with neural network (NN)
parameterization. The main advantages are that no mixed second-order derivative
computation is needed during the stochastic gradient descent training and that
the boundary conditions are taken into account automatically by the semigroup
operator. Unlike popular methods like PINN \cite{raissi2019physics} and Deep
Ritz \cite{weinan2018deep} where the Dirichlet boundary condition is enforced
solely through penalty functions and thus changes the true solution, the
proposed method is able to address the boundary conditions without penalty
functions and it gives the correct true solution even when penalty functions
are added, thanks to the semigroup operator. For eigenvalue problems, a
primal-dual method is proposed, efficiently resolving the constraint with a
simple scalar dual variable and resulting in a faster algorithm compared with
the BSDE solver \cite{han2020solving} in certain problems such as the
eigenvalue problem associated with the linear Schrödinger operator. Numerical
results are provided to demonstrate the performance of the proposed methods.

    

### [[2105.07519] Graph-Free Knowledge Distillation for Graph Neural Networks](http://arxiv.org/abs/2105.07519)


  Knowledge distillation (KD) transfers knowledge from a teacher network to a
student by enforcing the student to mimic the outputs of the pretrained teacher
on training data. However, data samples are not always accessible in many cases
due to large data sizes, privacy, or confidentiality. Many efforts have been
made on addressing this problem for convolutional neural networks (CNNs) whose
inputs lie in a grid domain within a continuous space such as images and
videos, but largely overlook graph neural networks (GNNs) that handle non-grid
data with different topology structures within a discrete space. The inherent
differences between their inputs make these CNN-based approaches not applicable
to GNNs. In this paper, we propose to our best knowledge the first dedicated
approach to distilling knowledge from a GNN without graph data. The proposed
graph-free KD (GFKD) learns graph topology structures for knowledge transfer by
modeling them with multivariate Bernoulli distribution. We then introduce a
gradient estimator to optimize this framework. Essentially, the gradients
w.r.t. graph structures are obtained by only using GNN forward-propagation
without back-propagation, which means that GFKD is compatible with modern GNN
libraries such as DGL and Geometric. Moreover, we provide the strategies for
handling different types of prior knowledge in the graph data or the GNNs.
Extensive experiments demonstrate that GFKD achieves the state-of-the-art
performance for distilling knowledge from GNNs without training data.

    

### [[2105.10446] ReduNet: A White-box Deep Network from the Principle of Maximizing Rate Reduction](http://arxiv.org/abs/2105.10446)


  This work attempts to provide a plausible theoretical framework that aims to
interpret modern deep (convolutional) networks from the principles of data
compression and discriminative representation. We argue that for
high-dimensional multi-class data, the optimal linear discriminative
representation maximizes the coding rate difference between the whole dataset
and the average of all the subsets. We show that the basic iterative gradient
ascent scheme for optimizing the rate reduction objective naturally leads to a
multi-layer deep network, named ReduNet, which shares common characteristics of
modern deep networks. The deep layered architectures, linear and nonlinear
operators, and even parameters of the network are all explicitly constructed
layer-by-layer via forward propagation, although they are amenable to
fine-tuning via back propagation. All components of so-obtained "white-box"
network have precise optimization, statistical, and geometric interpretation.
Moreover, all linear operators of the so-derived network naturally become
multi-channel convolutions when we enforce classification to be rigorously
shift-invariant. The derivation in the invariant setting suggests a trade-off
between sparsity and invariance, and also indicates that such a deep
convolution network is significantly more efficient to construct and learn in
the spectral domain. Our preliminary simulations and experiments clearly verify
the effectiveness of both the rate reduction objective and the associated
ReduNet. All code and data are available at
\url{this https URL}.

    

### [[2105.12247] GraphVICRegHSIC: Towards improved self-supervised representation learning for graphs with a hyrbid loss function](http://arxiv.org/abs/2105.12247)


  Self-supervised learning and pre-training strategieshave developed over the
last few years especiallyfor Convolutional Neural Networks (CNNs). Re-cently
application of such methods can also be no-ticed for Graph Neural Networks
(GNNs) . In thispaper, we have used a graph based self-supervisedlearning
strategy with different loss functions (Bar-low Twins[Zbontaret al., 2021],
HSIC[Tsaiet al.,2021], VICReg[Bardeset al., 2021]) which haveshown promising
results when applied with CNNspreviously. We have also proposed a hybrid
lossfunction combining the advantages of VICReg andHSIC and called it as
VICRegHSIC. The perfor-mance of these aforementioned methods have beencompared
when applied to 7 different datasets suchas MUTAG, PROTEINS, IMDB-Binary, etc.
Ex-periments showed that our hybrid loss function per-formed better than the
remaining ones in 4 out of7 cases. Moreover, the impact of different
batchsizes, projector dimensions and data augmentationstrategies have also been
explored.

    

### [[2105.13462] On Linear Stability of SGD and Input-Smoothness of Neural Networks](http://arxiv.org/abs/2105.13462)


  The multiplicative structure of parameters and input data in the first layer
of neural networks is explored to build connection between the landscape of the
loss function with respect to parameters and the landscape of the model
function with respect to input data. By this connection, it is shown that flat
minima regularize the gradient of the model function, which explains the good
generalization performance of flat minima. Then, we go beyond the flatness and
consider high-order moments of the gradient noise, and show that Stochastic
Gradient Descent (SGD) tends to impose constraints on these moments by a linear
stability analysis of SGD around global minima. Together with the
multiplicative structure, we identify the Sobolev regularization effect of SGD,
i.e. SGD regularizes the Sobolev seminorms of the model function with respect
to the input data. Finally, bounds for generalization error and adversarial
robustness are provided for solutions found by SGD under assumptions of the
data distribution.

    

### [[2106.04729] A Markov Decision Process Approach for Managing Medical Drone Deliveries](http://arxiv.org/abs/2106.04729)


  We consider the problem of optimizing the distribution operations at a drone
hub that dispatches drones to different geographic locations generating
stochastic demands for medical supplies. Drone delivery is an innovative method
that introduces many benefits, such as low-contact delivery, thereby reducing
the spread of pandemic and vaccine-preventable diseases. While we focus on
medical supply delivery for this work, drone delivery is suitable for many
other items, including food, postal parcels, and e-commerce. In this paper, our
goal is to address drone delivery challenges related to the stochastic demands
of different geographic locations. We consider different classes of demand
related to geographic locations that require different flight ranges, which is
directly related to the amount of charge held in a drone battery. We classify
the stochastic demands based on their distance from the drone hub, use a Markov
decision process to model the problem, and perform computational tests using
realistic data representing a prominent drone delivery company. We solve the
problem using a reinforcement learning method and show its high performance
compared with the exact solution found using dynamic programming. Finally, we
analyze the results and provide insights for managing the drone hub operations.

    

### [[2106.10516] DiffLoop: Tuning PID controllers by differentiating through the feedback loop](http://arxiv.org/abs/2106.10516)


  Since most industrial control applications use PID controllers, PID tuning
and anti-windup measures are significant problems. This paper investigates
tuning the feedback gains of a PID controller via back-calculation and
automatic differentiation tools. In particular, we episodically use a cost
function to generate gradients and perform gradient descent to improve
controller performance. We provide a theoretical framework for analyzing this
non-convex optimization and establish a relationship between back-calculation
and disturbance feedback policies. We include numerical experiments on linear
systems with actuator saturation to show the efficacy of this approach.

    

### [[2106.11921] Not All Labels Are Equal: Rationalizing The Labeling Costs for Training Object Detection](http://arxiv.org/abs/2106.11921)


  Deep neural networks have reached high accuracy on object detection but their
success hinges on large amounts of labeled data. To reduce the labels
dependency, various active learning strategies have been proposed, typically
based on the confidence of the detector. However, these methods are biased
towards high-performing classes and can lead to acquired datasets that are not
good representatives of the testing set data. In this work, we propose a
unified framework for active learning, that considers both the uncertainty and
the robustness of the detector, ensuring that the network performs well in all
classes. Furthermore, our method leverages auto-labeling to suppress a
potential distribution drift while boosting the performance of the model.
Experiments on PASCAL VOC07+12 and MS-COCO show that our method consistently
outperforms a wide range of active learning methods, yielding up to a 7.7%
improvement in mAP, or up to 82% reduction in labeling cost. Code will be
released upon acceptance of the paper.

    

### [[2106.14210] Nonparametric estimation of continuous DPPs with kernel methods](http://arxiv.org/abs/2106.14210)


  Determinantal Point Process (DPPs) are statistical models for repulsive point
patterns. Both sampling and inference are tractable for DPPs, a rare feature
among models with negative dependence that explains their popularity in machine
learning and spatial statistics. Parametric and nonparametric inference methods
have been proposed in the finite case, i.e. when the point patterns live in a
finite ground set. In the continuous case, only parametric methods have been
investigated, while nonparametric maximum likelihood for DPPs -- an
optimization problem over trace-class operators -- has remained an open
question. In this paper, we show that a restricted version of this maximum
likelihood (MLE) problem falls within the scope of a recent representer theorem
for nonnegative functions in an RKHS. This leads to a finite-dimensional
problem, with strong statistical ties to the original MLE. Moreover, we
propose, analyze, and demonstrate a fixed point algorithm to solve this
finite-dimensional problem. Finally, we also provide a controlled estimate of
the correlation kernel of the DPP, thus providing more interpretability.

    

### [[2107.00306] MHER: Model-based Hindsight Experience Replay](http://arxiv.org/abs/2107.00306)


  Solving multi-goal reinforcement learning (RL) problems with sparse rewards
is generally challenging. Existing approaches have utilized goal relabeling on
collected experiences to alleviate issues raised from sparse rewards. However,
these methods are still limited in efficiency and cannot make full use of
experiences. In this paper, we propose Model-based Hindsight Experience Replay
(MHER), which exploits experiences more efficiently by leveraging environmental
dynamics to generate virtual achieved goals. Replacing original goals with
virtual goals generated from interaction with a trained dynamics model leads to
a novel relabeling method, model-based relabeling (MBR). Based on MBR, MHER
performs both reinforcement learning and supervised learning for efficient
policy improvement. Theoretically, we also prove the supervised part in MHER,
i.e., goal-conditioned supervised learning with MBR data, optimizes a lower
bound on the multi-goal RL objective. Experimental results in several
point-based tasks and simulated robotics environments show that MHER achieves
significantly higher sample efficiency than previous model-free and model-based
multi-goal methods.

    

### [[2108.00941] A Survey of Human-in-the-loop for Machine Learning](http://arxiv.org/abs/2108.00941)


  Human-in-the-loop aims to train an accurate prediction model with minimum
cost by integrating human knowledge and experience. Humans can provide training
data for machine learning applications and directly accomplish tasks that are
hard for computers in the pipeline with the help of machine-based approaches.
In this paper, we survey existing works on human-in-the-loop from a data
perspective and classify them into three categories with a progressive
relationship: (1) the work of improving model performance from data processing,
(2) the work of improving model performance through interventional model
training, and (3) the design of the system independent human-in-the-loop. Using
the above categorization, we summarize major approaches in the field; along
with their technical strengths/ weaknesses, we have simple classification and
discussion in natural language processing, computer vision, and others.
Besides, we provide some open challenges and opportunities. This survey intends
to provide a high-level summarization for human-in-the-loop and motivates
interested readers to consider approaches for designing effective
human-in-the-loop solutions.

    

### [[2110.01602] Clustering a Mixture of Gaussians with Unknown Covariance](http://arxiv.org/abs/2110.01602)


  We investigate a clustering problem with data from a mixture of Gaussians
that share a common but unknown, and potentially ill-conditioned, covariance
matrix. We start by considering Gaussian mixtures with two equally-sized
components and derive a Max-Cut integer program based on maximum likelihood
estimation. We prove its solutions achieve the optimal misclassification rate
when the number of samples grows linearly in the dimension, up to a logarithmic
factor. However, solving the Max-cut problem appears to be computationally
intractable. To overcome this, we develop an efficient spectral algorithm that
attains the optimal rate but requires a quadratic sample size. Although this
sample complexity is worse than that of the Max-cut problem, we conjecture that
no polynomial-time method can perform better. Furthermore, we gather numerical
and theoretical evidence that supports the existence of a
statistical-computational gap. Finally, we generalize the Max-Cut program to a
$k$-means program that handles multi-component mixtures with possibly unequal
weights. It enjoys similar optimality guarantees for mixtures of distributions
that satisfy a transportation-cost inequality, encompassing Gaussian and
strongly log-concave distributions.

    

### [[2111.11418] MetaFormer is Actually What You Need for Vision](http://arxiv.org/abs/2111.11418)


  Transformers have shown great potential in computer vision tasks. A common
belief is their attention-based token mixer module contributes most to their
competence. However, recent works show the attention-based module in
transformers can be replaced by spatial MLPs and the resulted models still
perform quite well. Based on this observation, we hypothesize that the general
architecture of the transformers, instead of the specific token mixer module,
is more essential to the model's performance. To verify this, we deliberately
replace the attention module in transformers with an embarrassingly simple
spatial pooling operator to conduct only the most basic token mixing.
Surprisingly, we observe that the derived model, termed as PoolFormer, achieves
competitive performance on multiple computer vision tasks. For example, on
ImageNet-1K, PoolFormer achieves 82.1% top-1 accuracy, surpassing well-tuned
vision transformer/MLP-like baselines DeiT-B/ResMLP-B24 by 0.3%/1.1% accuracy
with 35%/52% fewer parameters and 48%/60% fewer MACs. The effectiveness of
PoolFormer verifies our hypothesis and urges us to initiate the concept of
"MetaFormer", a general architecture abstracted from transformers without
specifying the token mixer. Based on the extensive experiments, we argue that
MetaFormer is the key player in achieving superior results for recent
transformer and MLP-like models on vision tasks. This work calls for more
future research dedicated to improving MetaFormer instead of focusing on the
token mixer modules. Additionally, our proposed PoolFormer could serve as a
starting baseline for future MetaFormer architecture design. Code is available
at this https URL


### [[2111.13729] Mapping Surface Code to Superconducting Quantum Processors](http://arxiv.org/abs/2111.13729)


  In this paper, we formally describe the three challenges of mapping surface
code on superconducting devices, and present a comprehensive synthesis
framework to overcome these challenges. The proposed framework consists of
three optimizations. First, we adopt a geometrical method to allocate data
qubits which ensures the existence of shallow syndrome extraction circuit. The
proposed data qubit layout optimization reduces the overhead of syndrome
extraction and serves as a good initial point for following optimizations.
Second, we only use bridge qubits enclosed by data qubits and reduce the number
of bridge qubits by merging short path between data qubits. The proposed bridge
qubit optimization reduces the probability of bridge qubit conflicts and
further minimizes the syndrome extraction overhead. Third, we propose an
efficient heuristic to schedule syndrome extractions. Based on the proposed
data qubit allocation, we devise a good initial schedule of syndrome
extractions and further refine this schedule to minimize the total time needed
by a complete surface code error detection cycle. Our experiments on
mainsstream superconducting quantum architectures have demonstrated the
efficiency of the proposed framework.

    

### [[2111.14072] A Comprehensive and Cross-Platform Test Suite for Memory Safety -- Towards an Open Framework for Testing Processor Hardware Supported Security Extensions](http://arxiv.org/abs/2111.14072)


  Memory safety remains a critical and widely violated property in reality.
Numerous defense techniques have been proposed and developed but most of them
are not applied or enabled by default in production-ready environment due to
their substantial running cost. The situation might change in the near future
because the hardware supported defenses against these attacks are finally
beginning to be adopted by commercial processors, operating systems and
compilers. We then face a question as there is currently no suitable test suite
to measure the memory safety extensions supported on different processors. In
fact, the issue is not constrained only for memory safety but all aspect of
processor security. All of the existing test suites related to processor
security lack some of the key properties, such as comprehensiveness,
distinguishability and portability.
As an initial step, we propose an expandable test framework for measuring the
processor security and open source a memory safety test suite utilizing this
framework. The framework is deliberately designed to be flexible so it can be
gradually extended to all types of hardware supported security extensions in
processors. The initial test suite for memory safety currently contains 160
test cases covering spatial and temporal safety of memory, memory access
control, pointer integrity and control-flow integrity. Each type of
vulnerabilities and their related defenses have been individually evaluated by
one or more test cases. The test suite has been ported to three different
instruction set architectures (ISAs) and experimented on six different
platforms. We have also utilized the test suite to explore the security
benefits of applying different sets of compiler flags available on the latest
GNU GCC and LLVM compilers.

    

### [[2111.14249] Virtualizing Intermittent Computing](http://arxiv.org/abs/2111.14249)


  Intermittent computing requires custom programming models to ensure the
correct execution of applications despite power failures. However, existing
programming models lead to programs that are hardware-dependent and not
reusable. This paper aims at virtualizing intermittent computing to remedy
these problems. We introduce PureVM, a virtual machine that abstracts a
transiently powered computer, and PureLANG, a continuation-passing-style
programming language to develop programs that run on PureVM. This
virtualization, for the first time, paves the way for portable and reusable
transiently-powered applications.

    

### [[2111.14252] Search for Optimal Systolic Arrays: A Comprehensive Automated Exploration Framework and Lessons Learned](http://arxiv.org/abs/2111.14252)


  Systolic arrays have been widely used for accelerating HPC and deep learning
applications. There is a plethora of previous works on the performance tuning
of systolic arrays, but usually based on a number of oversimplified assumptions
(e.g., only considering divisors for loop tiling, pruning based on off-chip
data communication) to reduce the design space.
In this paper, we present a comprehensive design space exploration tool named
Odyssey for systolic array optimization. Odyssey does not rely on artificial
assumptions to limit the design space, and yet it is highly efficient and
scalable with a hybrid optimization technique. For example, for a
1024x1024x1024 matrix multiplication, it finds designs that reach 90% of the
optimal performance in 5 seconds with a single CPU thread. Moreover, using
Odyssey, we unveil and quantify the suboptimality introduced by multiple
commonly used oversimplifications in prior studies for systolic array design
space exploration. For example, Odyssey results show that limiting to divisors
for loop tiling leads to a 39% performance loss, and pruning based on off-chip
data movement results in a 45% performance loss. We applied Odyssey to explore
the architecture trade-offs for matrix multiplication and convolutional neural
network, providing inspiration into possible optimizations for these two
applications.

    

### [[2102.10031] Toward Taming the Overhead Monster for Data-Flow Integrity](http://arxiv.org/abs/2102.10031)


  Data-Flow Integrity (DFI) is a well-known approach to effectively detecting a
wide range of software attacks. However, its real-world application has been
quite limited so far because of the prohibitive performance overhead it incurs.
Moreover, the overhead is enormously difficult to overcome without
substantially lowering the DFI criterion. In this work, an analysis is
performed to understand the main factors contributing to the overhead.
Accordingly, a hardware-assisted parallel approach is proposed to tackle the
overhead challenge. Simulations on SPEC CPU 2006 benchmark show that the
proposed approach can completely enforce the DFI defined in the original
seminal work while reducing performance overhead by 4x, on average.

    

### [[2104.09502] CodeAPeel: An Integrated and Layered Learning Technology For Computer Architecture Courses](http://arxiv.org/abs/2104.09502)


  This paper introduces a versatile, multi-layered technology to help support
teaching and learning core computer architecture concepts. This technology,
called CodeAPeel is already implemented in one particular form to describe
instruction processing in compiler, assembly, and machine layers of a generic
instruction set architecture by a comprehensive simulation of its
fetch-decode-execute cycle as well as animation of the behavior of its CPU
registers, RAM, VRAM, STACK memories, various control registers, and graphics
screen. Unlike most educational CPU simulators that simulate a real processor
such as MIPS or RISC-V, CodeAPeel is designed and implemented as a generic RISC
instruction set architecture simulator with both scalar and vector instructions
to provide a dual-mode processor simulator as described by Flynn's
classification of SISD and SIMD processors. Vectorization of operations is
built into the instruction repertoire of CodeAPeel, making it straightforward
to simulate such processors with powerful vector instructions.

    

### [[2111.13877] DSAG: A mixed synchronous-asynchronous iterative method for straggler-resilient learning](http://arxiv.org/abs/2111.13877)


  We consider straggler-resilient learning. In many previous works, e.g., in
the coded computing literature, straggling is modeled as random delays that are
independent and identically distributed between workers. However, in many
practical scenarios, a given worker may straggle over an extended period of
time. We propose a latency model that captures this behavior and is
substantiated by traces collected on Microsoft Azure, Amazon Web Services
(AWS), and a small local cluster. Building on this model, we propose DSAG, a
mixed synchronous-asynchronous iterative optimization method, based on the
stochastic average gradient (SAG) method, that combines timely and stale
results. We also propose a dynamic load-balancing strategy to further reduce
the impact of straggling workers. We evaluate DSAG for principal component
analysis, cast as a finite-sum optimization problem, of a large genomics
dataset, and for logistic regression on a cluster composed of 100 workers on
AWS, and find that DSAG is up to about 50% faster than SAG, and more than twice
as fast as coded computing methods, for the particular scenario that we
consider.

    

### [[2111.13908] Artificial neural networks for online error detection](http://arxiv.org/abs/2111.13908)


  Hardware reliability is adversely affected by the downscaling of
semiconductor devices and the scale-out of systems necessitated by modern
applications. Apart from crashes, this unreliability often manifests as silent
data corruptions (SDCs), affecting application output. Therefore, we need
low-cost and low-human-effort solutions to reduce the incidence rate and the
effects of SDCs on the quality of application outputs. We propose Artificial
Neural Networks (ANNs) as an effective mechanism for online error detection. We
train ANNs using software fault injection. We find that the average overhead of
our approach, followed by a costly error correction by re-execution, is 6.45%
in terms of CPU cycles. We also report that ANNs discover 94.85% of faults
thereby resulting in minimal output quality degradation. To validate our
approach we overclock ARM Cortex A53 CPUs, execute benchmarks on them and
record the program outputs. ANNs prove to be an efficient error detection
mechanism, better than a state of the art approximate error detection mechanism
(Topaz), both in terms of performance (12.81% CPU overhead) and quality of
application output (94.11% detection coverage).

    

### [[2111.13949] Distributed Anomaly Detection in Edge Streams using Frequency based Sketch Datastructures](http://arxiv.org/abs/2111.13949)


  Often logs hosted in large data centers represent network traffic data over a
long period of time. For instance, such network traffic data logged via a TCP
dump packet sniffer (as considered in the 1998 DARPA intrusion attack) included
network packets being transmitted between computers. While an online framework
is necessary for detecting any anomalous or suspicious network activities like
denial of service attacks or unauthorized usage in real time, often such large
data centers log data over long periods of time (e.g., TCP dump) and hence an
offline framework is much more suitable in such scenarios. Given a network log
history of edges from a dynamic graph, how can we assign anomaly scores to
individual edges indicating suspicious events with high accuracy using only
constant memory and within limited time than state-of-the-art methods? We
propose MDistrib and its variants which provides (a) faster detection of
anomalous events via distributed processing with GPU support compared to other
approaches, (b) better false positive guarantees than state of the art methods
considering fixed space and (c) with collision aware based anomaly scoring for
better accuracy results than state-of-the-art approaches. We describe
experiments confirming that MDistrib is more efficient than prior work.

    

### [[2111.14255] Automated Runtime-Aware Scheduling for Multi-Tenant DNN Inference on GPU](http://arxiv.org/abs/2111.14255)


  With the fast development of deep neural networks (DNNs), many real-world
applications are adopting multiple models to conduct compound tasks, such as
co-running classification, detection, and segmentation models on autonomous
vehicles. Such multi-tenant DNN inference cases greatly exacerbate the
computational complexity and call for comprehensive collaboration for
graph-level operator scheduling, runtime-level resource awareness, as well as
hardware scheduler support. However, the current scheduling support for such
multi-tenant inference is still relatively backward. In this work, we propose a
resource-aware scheduling framework for efficient multi-tenant DNN inference on
GPU, which automatically coordinates DNN computing in different execution
levels. Leveraging the unified scheduling intermediate representation and the
automated ML-based searching algorithm, optimal schedules could be generated to
wisely adjust model concurrency and interleave DNN model operators, maintaining
a continuously balanced resource utilization across the entire inference
process, and eventually improving the runtime efficiency. Experiments show that
we could consistently achieve 1.3-1.7x speed-up, compared to regular DNN
runtime libraries (e.g., CuDNN, TVM) and particular concurrent scheduling
methods (e.g., NVIDIA Multi-Stream).

    

### [[1907.06068] Time-optimal self-stabilizing leader election in population protocols](http://arxiv.org/abs/1907.06068)


  We consider the standard population protocol model, where (a priori)
indistinguishable and anonymous agents interact in pairs according to uniformly
random scheduling. The self-stabilizing leader election problem requires the
protocol to converge on a single leader agent from any possible initial
configuration. We initiate the study of time complexity of population protocols
solving this problem in its original setting: with probability 1, in a complete
communication graph. The only previously known protocol by Cai, Izumi, and Wada
[Theor. Comput. Syst. 50] runs in expected parallel time $\Theta(n^2)$ and has
the optimal number of $n$ states in a population of $n$ agents. The existing
protocol has the additional property that it becomes silent, i.e., the agents'
states eventually stop changing.
Observing that any silent protocol solving self-stabilizing leader election
requires $\Omega(n)$ expected parallel time, we introduce a silent protocol
that uses optimal $O(n)$ parallel time and states. Without any silence
constraints, we show that it is possible to solve self-stabilizing leader
election in asymptotically optimal expected parallel time of $O(\log n)$, but
using at least exponential states (a quasi-polynomial number of bits). All of
our protocols (and also that of Cai et al.) work by solving the more difficult
ranking problem: assigning agents the ranks $1,\ldots,n$.

    

### [[2005.04935] Performance Modeling and Vertical Autoscaling of Stream Joins](http://arxiv.org/abs/2005.04935)


  Streaming analysis is widely used in cloud as well as edge infrastructures.
In these contexts, fine-grained application performance can be based on
accurate modeling of streaming operators. This is especially beneficial for
computationally expensive operators like adaptive stream joins that, being very
sensitive to rate-varying data streams, would otherwise require costly frequent
monitoring.
We propose a dynamic model for the processing throughput and latency of
adaptive stream joins that run with different parallelism degrees. The model is
presented with progressive complexity, from a centralized non-deterministic up
to a deterministic parallel stream join, describing how throughput and latency
dynamics are influenced by various configuration parameters. The model is
catalytic for understanding the behavior of stream joins against different
system deployments, as we show with our model-based autoscaling methodology to
change the parallelism degree of stream joins during the execution. Our
thorough evaluation, for a broad spectrum of parameter, confirms the model can
reliably predict throughput and latency metrics with a fairly high accuracy,
with the median error in estimation ranging from approximately 0.1% to 6.5%,
even for an overloaded system. Furthermore, we show that our model allows to
efficiently control adaptive stream joins by estimating the needed resources
solely based on the observed input load. In particular, we show it can be
employed to enable efficient autoscaling, even when big changes in the input
load happen frequently (in the realm of seconds).

    

### [[2106.13020] Zero-Cost, Arrow-Enabled Data Interface for Apache Spark](http://arxiv.org/abs/2106.13020)


  Distributed data processing ecosystems are widespread and their components
are highly specialized, such that efficient interoperability is urgent.
Recently, Apache Arrow was chosen by the community to serve as a format
mediator, providing efficient in-memory data representation. Arrow enables
efficient data movement between data processing and storage engines,
significantly improving interoperability and overall performance. In this work,
we design a new zero-cost data interoperability layer between Apache Spark and
Arrow-based data sources through the Arrow Dataset API. Our novel data
interface helps separate the computation (Spark) and data (Arrow) layers. This
enables practitioners to seamlessly use Spark to access data from all Arrow
Dataset API-enabled data sources and frameworks. To benefit our community, we
open-source our work and show that consuming data through Apache Arrow is
zero-cost: our novel data interface is either on-par or more performant than
native Spark.

    

### [[2111.13770] A Fast Evolutionary adaptation for MCTS in Pommerman](http://arxiv.org/abs/2111.13770)


  Artificial Intelligence, when amalgamated with games makes the ideal
structure for research and advancing the field. Multi-agent games have multiple
controls for each agent which generates huge amounts of data while increasing
search complexity. Thus, we need advanced search methods to find a solution and
create an artificially intelligent agent. In this paper, we propose our novel
Evolutionary Monte Carlo Tree Search (FEMCTS) agent which borrows ideas from
Evolutionary Algorthims (EA) and Monte Carlo Tree Search (MCTS) to play the
game of Pommerman. It outperforms Rolling Horizon Evolutionary Algorithm (RHEA)
significantly in high observability settings and performs almost as well as
MCTS for most game seeds, outperforming it in some cases.

    

### [[2111.13781] Common Sense Knowledge Learning for Open Vocabulary Neural Reasoning: A First View into Chronic Disease Literature](http://arxiv.org/abs/2111.13781)


  In this paper, we address reasoning tasks from open vocabulary Knowledge
Bases (openKBs) using state-of-the-art Neural Language Models (NLMs) with
applications in scientific literature. For this purpose, self-attention based
NLMs are trained using a common sense KB as a source task. The NLMs are then
tested on a target KB for open vocabulary reasoning tasks involving scientific
knowledge related to the most prevalent chronic diseases (also known as
non-communicable diseases, NCDs). Our results identified NLMs that performed
consistently and with significance in knowledge inference for both source and
target tasks. Furthermore, in our analysis by inspection we discussed the
semantic regularities and reasoning capabilities learned by the models, while
showing a first insight into the potential benefits of our approach to aid NCD
research.

    

### [[2111.13813] Video Content Classification using Deep Learning](http://arxiv.org/abs/2111.13813)


  Video content classification is an important research content in computer
vision, which is widely used in many fields, such as image and video retrieval,
computer vision. This paper presents a model that is a combination of
Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN) which
develops, trains, and optimizes a deep learning network that can identify the
type of video content and classify them into categories such as "Animation,
Gaming, natural content, flat content, etc". To enhance the performance of the
model novel keyframe extraction method is included to classify only the
keyframes, thereby reducing the overall processing time without sacrificing any
significant performance.

    

### [[2111.13827] Natural Language Processing in-and-for Design Research](http://arxiv.org/abs/2111.13827)


  We review the scholarly contributions that utilise Natural Language
Processing (NLP) methods to support the design process. Using a heuristic
approach, we collected 223 articles published in 32 journals and within the
period 1991-present. We present state-of-the-art NLP in-and-for design research
by reviewing these articles according to the type of natural language text
sources: internal reports, design concepts, discourse transcripts, technical
publications, consumer opinions, and others. Upon summarizing and identifying
the gaps in these contributions, we utilise an existing design innovation
framework to identify the applications that are currently being supported by
NLP. We then propose a few methodological and theoretical directions for future
NLP in-and-for design research.

    

### [[2111.13943] Computational simulation and the search for a quantitative description of simple reinforcement schedules](http://arxiv.org/abs/2111.13943)


  We aim to discuss schedules of reinforcement in its theoretical and practical
terms pointing to practical limitations on implementing those schedules while
discussing the advantages of computational simulation. In this paper, we
present a R script named Beak, built to simulate rates of behavior interacting
with schedules of reinforcement. Using Beak, we've simulated data that allows
an assessment of different reinforcement feedback functions (RFF). This was
made with unparalleled precision, since simulations provide huge samples of
data and, more importantly, simulated behavior isn't changed by the
reinforcement it produces. Therefore, we can vary it systematically. We've
compared different RFF for RI schedules, using as criteria: meaning, precision,
parsimony and generality. Our results indicate that the best feedback function
for the RI schedule was published by Baum (1981). We also propose that the
model used by Killeen (1975) is a viable feedback function for the RDRL
schedule. We argue that Beak paves the way for greater understanding of
schedules of reinforcement, addressing still open questions about quantitative
features of schedules. Also, they could guide future experiments that use
schedules as theoretical and methodological tools.

    

### [[2111.13970] Label Assistant: A Workflow for Assisted Data Annotation in Image Segmentation Tasks](http://arxiv.org/abs/2111.13970)


  Recent research in the field of computer vision strongly focuses on deep
learning architectures to tackle image processing problems. Deep neural
networks are often considered in complex image processing scenarios since
traditional computer vision approaches are expensive to develop or reach their
limits due to complex relations. However, a common criticism is the need for
large annotated datasets to determine robust parameters. Annotating images by
human experts is time-consuming, burdensome, and expensive. Thus, support is
needed to simplify annotation, increase user efficiency, and annotation
quality. In this paper, we propose a generic workflow to assist the annotation
process and discuss methods on an abstract level. Thereby, we review the
possibilities of focusing on promising samples, image pre-processing,
pre-labeling, label inspection, or post-processing of annotations. In addition,
we present an implementation of the proposal by means of a developed flexible
and extendable software prototype nested in hybrid touchscreen/laptop device.

    

### [[2111.13978] Deep Q-Learning based Reinforcement Learning Approach for Network Intrusion Detection](http://arxiv.org/abs/2111.13978)


  The rise of the new generation of cyber threats demands more sophisticated
and intelligent cyber defense solutions equipped with autonomous agents capable
of learning to make decisions without the knowledge of human experts. Several
reinforcement learning methods (e.g., Markov) for automated network intrusion
tasks have been proposed in recent years. In this paper, we introduce a new
generation of network intrusion detection methods that combines a
Q-learning-based reinforcement learning with a deep-feed forward neural network
method for network intrusion detection. Our proposed Deep Q-Learning (DQL)
model provides an ongoing auto-learning capability for a network environment
that can detect different types of network intrusions using an automated
trial-error approach and continuously enhance its detection capabilities. We
provide the details of fine-tuning different hyperparameters involved in the
DQL model for more effective self-learning. According to our extensive
experimental results based on the NSL-KDD dataset, we confirm that the lower
discount factor which is set as 0.001 under 250 episodes of training yields the
best performance results. Our experimental results also show that our proposed
DQL is highly effective in detecting different intrusion classes and
outperforms other similar machine learning approaches.

    

### [[2111.13982] Language models in word sense disambiguation for Polish](http://arxiv.org/abs/2111.13982)


  In the paper, we test two different approaches to the {unsupervised} word
sense disambiguation task for Polish. In both methods, we use neural language
models to predict words similar to those being disambiguated and, on the basis
of these words, we predict the partition of word senses in different ways. In
the first method, we cluster selected similar words, while in the second, we
cluster vectors representing their subsets. The evaluation was carried out on
texts annotated with plWordNet senses and provided a relatively good result
(F1=0.68 for all ambiguous words). The results are significantly better than
those obtained for the neural model-based unsupervised method proposed in
\cite{waw:myk:17:Sense} and are at the level of the supervised method presented
there. The proposed method may be a way of solving word sense disambiguation
problem for languages that lack sense annotated data.

    

### [[2111.14021] AI-supported Framework of Semi-Automatic Monoplotting for Monocular Oblique Visual Data Analysis](http://arxiv.org/abs/2111.14021)


  In the last decades, the development of smartphones, drones, aerial patrols,
and digital cameras enabled high-quality photographs available to large
populations and, thus, provides an opportunity to collect massive data of the
nature and society with global coverage. However, the data collected with new
photography tools is usually oblique - they are difficult to be georeferenced,
and huge amounts of data is often obsolete. Georeferencing oblique imagery data
may be solved by a technique called monoplotting, which only requires a single
image and Digital Elevation Model (DEM). In traditional monoplotting, a human
user has to manually choose a series of ground control point (GCP) pairs in the
image and DEM and then determine the extrinsic and intrinsic parameters of the
camera to establish a pixel-level correspondence between photos and the DEM to
enable the mapping and georeferencing of objects in photos. This traditional
method is difficult to scale due to several challenges including the
labor-intensive inputs, the need of rich experience to identify well-defined
GCPs, and limitations in camera pose estimation. Therefore, existing
monoplotting methods are rarely used in analyzing large-scale databases or
near-real-time warning systems. In this paper, we propose and demonstrate a
novel semi-automatic monoplotting framework that provides pixel-level
correspondence between photos and DEMs requiring minimal human interventions. A
pipeline of analyses was developed including key point detection in images and
DEM rasters, retrieving georeferenced 3D DEM GCPs, regularized gradient-based
optimization, pose estimation, ray tracing, and the correspondence
identification between image pixels and real world coordinates. Two numerical
experiments show that the framework is superior in georeferencing visual data
in 3-D coordinates, paving a way toward fully automatic monoplotting
methodology.

    

### [[2111.14094] Topic Driven Adaptive Network for Cross-Domain Sentiment Classification](http://arxiv.org/abs/2111.14094)


  Cross-domain sentiment classification has been a hot spot these years, which
aims to learn a reliable classifier using labeled data from the source domain
and evaluate it on the target domain. In this vein, most approaches utilized
domain adaptation that maps data from different domains into a common feature
space. To further improve the model performance, several methods targeted to
mine domain-specific information were proposed. However, most of them only
utilized a limited part of domain-specific information. In this study, we first
develop a method of extracting domain-specific words based on the topic
information. Then, we propose a Topic Driven Adaptive Network (TDAN) for
cross-domain sentiment classification. The network consists of two
sub-networks: semantics attention network and domain-specific word attention
network, the structures of which are based on transformers. These sub-networks
take different forms of input and their outputs are fused as the feature
vector. Experiments validate the effectiveness of our TDAN on sentiment
classification across domains.

    

### [[2111.14142] Agility in Software 2.0 -- Notebook Interfaces and MLOps with Buttresses and Rebars](http://arxiv.org/abs/2111.14142)


  Artificial intelligence through machine learning is increasingly used in the
digital society. Solutions based on machine learning bring both great
opportunities, thus coined "Software 2.0," but also great challenges for the
engineering community to tackle. Due to the experimental approach used by data
scientists when developing machine learning models, agility is an essential
characteristic. In this keynote address, we discuss two contemporary
development phenomena that are fundamental in machine learning development,
i.e., notebook interfaces and MLOps. First, we present a solution that can
remedy some of the intrinsic weaknesses of working in notebooks by supporting
easy transitions to integrated development environments. Second, we propose
reinforced engineering of AI systems by introducing metaphorical buttresses and
rebars in the MLOps context. Machine learning-based solutions are dynamic in
nature, and we argue that reinforced continuous engineering is required to
quality assure the trustworthy AI systems of tomorrow.

    

### [[2111.14145] FashionSearchNet-v2: Learning Attribute Representations with Localization for Image Retrieval with Attribute Manipulation](http://arxiv.org/abs/2111.14145)


  The focus of this paper is on the problem of image retrieval with attribute
manipulation. Our proposed work is able to manipulate the desired attributes of
the query image while maintaining its other attributes. For example, the collar
attribute of the query image can be changed from round to v-neck to retrieve
similar images from a large dataset. A key challenge in e-commerce is that
images have multiple attributes where users would like to manipulate and it is
important to estimate discriminative feature representations for each of these
attributes. The proposed FashionSearchNet-v2 architecture is able to learn
attribute specific representations by leveraging on its weakly-supervised
localization module, which ignores the unrelated features of attributes in the
feature space, thus improving the similarity learning. The network is jointly
trained with the combination of attribute classification and triplet ranking
loss to estimate local representations. These local representations are then
merged into a single global representation based on the instructed attribute
manipulation where desired images can be retrieved with a distance metric. The
proposed method also provides explainability for its retrieval process to help
provide additional information on the attention of the network. Experiments
performed on several datasets that are rich in terms of the number of
attributes show that FashionSearchNet-v2 outperforms the other state-of-the-art
attribute manipulation techniques. Different than our earlier work
(FashionSearchNet), we propose several improvements in the learning procedure
and show that the proposed FashionSearchNet-v2 can be generalized to different
domains other than fashion.

    

### [[2111.14183] Code Clone Detection based on Event Embedding and Event Dependency](http://arxiv.org/abs/2111.14183)


  The code clone detection method based on semantic similarity has important
value in software engineering tasks (e.g., software evolution, software reuse).
Traditional code clone detection technologies pay more attention to the
similarity of code at the syntax level, and less attention to the semantic
similarity of the code. As a result, candidate codes similar in semantics are
ignored. To address this issue, we propose a code clone detection method based
on semantic similarity. By treating code as a series of interdependent events
that occur continuously, we design a model namely EDAM to encode code semantic
information based on event embedding and event dependency. The EDAM model uses
the event embedding method to model the execution characteristics of program
statements and the data dependence information between all statements. In this
way, we can embed the program semantic information into a vector and use the
vector to detect codes similar in semantics. Experimental results show that the
performance of our EDAM model is superior to state of-the-art open source
models for code clone detection.

    

### [[2111.14192] Zero-Shot Cross-Lingual Transfer in Legal Domain Using Transformer models](http://arxiv.org/abs/2111.14192)


  Zero-shot cross-lingual transfer is an important feature in modern NLP models
and architectures to support low-resource languages. In this work, We study
zero-shot cross-lingual transfer from English to French and German under
Multi-Label Text Classification, where we train a classifier using English
training set, and we test using French and German test sets. We extend
EURLEX57K dataset, the English dataset for topic classification of legal
documents, with French and German official translation. We investigate the
effect of using some training techniques, namely Gradual Unfreezing and
Language Model finetuning, on the quality of zero-shot cross-lingual transfer.
We find that Language model finetuning of multi-lingual pre-trained model
(M-DistilBERT, M-BERT) leads to 32.0-34.94%, 76.15-87.54\% relative improvement
on French and German test sets correspondingly. Also, Gradual unfreezing of
pre-trained model's layers during training results in relative improvement of
38-45% for French and 58-70% for German. Compared to training a model in Joint
Training scheme using English, French and German training sets, zero-shot
BERT-based classification model reaches 86% of the performance achieved by
jointly-trained BERT-based classification model.

    

### [[2111.14203] How Deep Are the Fakes? Focusing on Audio Deepfake: A Survey](http://arxiv.org/abs/2111.14203)


  Deepfake is content or material that is synthetically generated or
manipulated using artificial intelligence (AI) methods, to be passed off as
real and can include audio, video, image, and text synthesis. This survey has
been conducted with a different perspective compared to existing survey papers,
that mostly focus on just video and image deepfakes. This survey not only
evaluates generation and detection methods in the different deepfake
categories, but mainly focuses on audio deepfakes that are overlooked in most
of the existing surveys. This paper critically analyzes and provides a unique
source of audio deepfake research, mostly ranging from 2016 to 2020. To the
best of our knowledge, this is the first survey focusing on audio deepfakes in
English. This survey provides readers with a summary of 1) different deepfake
categories 2) how they could be created and detected 3) the most recent trends
in this domain and shortcomings in detection methods 4) audio deepfakes, how
they are created and detected in more detail which is the main focus of this
paper. We found that Generative Adversarial Networks(GAN), Convolutional Neural
Networks (CNN), and Deep Neural Networks (DNN) are common ways of creating and
detecting deepfakes. In our evaluation of over 140 methods we found that the
majority of the focus is on video deepfakes and in particular in the generation
of video deepfakes. We found that for text deepfakes there are more generation
methods but very few robust methods for detection, including fake news
detection, which has become a controversial area of research because of the
potential of heavy overlaps with human generation of fake content. This paper
is an abbreviated version of the full survey and reveals a clear need to
research audio deepfakes and particularly detection of audio deepfakes.

    

### [[2111.14301] PSG: Prompt-based Sequence Generation for Acronym Extraction](http://arxiv.org/abs/2111.14301)


  Acronym extraction aims to find acronyms (i.e., short-forms) and their
meanings (i.e., long-forms) from the documents, which is important for
scientific document understanding (SDU@AAAI-22) tasks. Previous works are
devoted to modeling this task as a paragraph-level sequence labeling problem.
However, it lacks the effective use of the external knowledge, especially when
the datasets are in a low-resource setting. Recently, the prompt-based method
with the vast pre-trained language model can significantly enhance the
performance of the low-resourced downstream tasks. In this paper, we propose a
Prompt-based Sequence Generation (PSG) method for the acronym extraction task.
Specifically, we design a template for prompting the extracted acronym texts
with auto-regression. A position extraction algorithm is designed for
extracting the position of the generated answers. The results on the acronym
extraction of Vietnamese and Persian in a low-resource setting show that the
proposed method outperforms all other competitive state-of-the-art (SOTA)
methods.

    

### [[2111.14306] SimCLAD: A Simple Framework for Contrastive Learning of Acronym Disambiguation](http://arxiv.org/abs/2111.14306)


  Acronym disambiguation means finding the correct meaning of an ambiguous
acronym in a given sentence from the dictionary, which is one of the key points
for scientific document understanding (SDU@AAAI-22). Recently, many attempts
have tried to solve this problem via fine-tuning the pre-trained masked
language models (MLMs) in order to obtain a better acronym representation.
However, the acronym meaning is varied under different contexts, whose
corresponded sentence representation is the anisotropic distribution occupied
with a narrow subset of the entire representation space. Such representations
from pre-trained MLMs are not ideal for the acronym disambiguation from the
given dictionary. In this paper, we propose a Simple framework for Contrastive
Learning of Acronym Disambiguation (SimCLAD) method to better understand the
acronym meanings. Specifically, we design a novel continual contrastive
pre-training method that enhances the pre-trained model's generalization
ability by learning the isotropic and discriminative distribution of the
acronym sentence representations. The results on the acronym disambiguation of
the scientific domain in English show that the proposed method outperforms all
other competitive state-of-the-art (SOTA) methods.

    

### [[2111.14339] Heterogeneous Visible-Thermal and Visible-Infrared Face Recognition using Unit-Class Loss and Cross-Modality Discriminator](http://arxiv.org/abs/2111.14339)


  Visible-to-thermal face image matching is a challenging variate of
cross-modality recognition. The challenge lies in the large modality gap and
low correlation between visible and thermal modalities. Existing approaches
employ image preprocessing, feature extraction, or common subspace projection,
which are independent problems in themselves. In this paper, we propose an
end-to-end framework for cross-modal face recognition. The proposed algorithm
aims to learn identity-discriminative features from unprocessed facial images
and identify cross-modal image pairs. A novel Unit-Class Loss is proposed for
preserving identity information while discarding modality information. In
addition, a Cross-Modality Discriminator block is proposed for integrating
image-pair classification capability into the network. The proposed network can
be used to extract modality-independent vector representations or a
matching-pair classification for test images. Our cross-modality face
recognition experiments on five independent databases demonstrate that the
proposed method achieves marked improvement over existing state-of-the-art
methods.

    

### [[2111.14341] ROBIN : A Benchmark for Robustness to Individual Nuisancesin Real-World Out-of-Distribution Shifts](http://arxiv.org/abs/2111.14341)


  Enhancing the robustness in real-world scenarios has been proven very
challenging. One reason is that existing robustness benchmarks are limited, as
they either rely on synthetic data or they simply measure robustness as
generalization between datasets and hence ignore the effects of individual
nuisance factors. In this work, we introduce ROBIN, a benchmark dataset for
diagnosing the robustness of vision algorithms to individual nuisances in
real-world images. ROBIN builds on 10 rigid categories from the PASCAL VOC 2012
and ImageNet datasets and includes out-of-distribution examples of the objects
3D pose, shape, texture, context and weather conditions. ROBIN is richly
annotated to enable benchmark models for image classification, object
detection, and 3D pose estimation. We provide results for a number of popular
baselines and make several interesting observations: 1. Some nuisance factors
have a much stronger negative effect on the performance compared to others.
Moreover, the negative effect of an OODnuisance depends on the downstream
vision task. 2. Current approaches to enhance OOD robustness using strong data
augmentation have only marginal effects in real-world OOD scenarios, and
sometimes even reduce the OOD performance. 3. We do not observe any significant
differences between convolutional and transformer architectures in terms of OOD
robustness. We believe our dataset provides a rich testbed to study the OOD
robustness of vision algorithms and will help to significantly push forward
research in this area.

    

### [[2009.04547] Optimal Inspection and Maintenance Planning for Deteriorating Structural Components through Dynamic Bayesian Networks and Markov Decision Processes](http://arxiv.org/abs/2009.04547)


  Civil and maritime engineering systems, among others, from bridges to
offshore platforms and wind turbines, must be efficiently managed as they are
exposed to deterioration mechanisms throughout their operational life, such as
fatigue or corrosion. Identifying optimal inspection and maintenance policies
demands the solution of a complex sequential decision-making problem under
uncertainty, with the main objective of efficiently controlling the risk
associated with structural failures. Addressing this complexity, risk-based
inspection planning methodologies, supported often by dynamic Bayesian
networks, evaluate a set of pre-defined heuristic decision rules to reasonably
simplify the decision problem. However, the resulting policies may be
compromised by the limited space considered in the definition of the decision
rules. Avoiding this limitation, Partially Observable Markov Decision Processes
(POMDPs) provide a principled mathematical methodology for stochastic optimal
control under uncertain action outcomes and observations, in which the optimal
actions are prescribed as a function of the entire, dynamically updated, state
probability distribution. In this paper, we combine dynamic Bayesian networks
with POMDPs in a joint framework for optimal inspection and maintenance
planning, and we provide the formulation for developing both infinite and
finite horizon POMDPs in a structural reliability context. The proposed
methodology is implemented and tested for the case of a structural component
subject to fatigue deterioration, demonstrating the capability of
state-of-the-art point-based POMDP solvers for solving the underlying planning
optimization problem. Within the numerical experiments, POMDP and
heuristic-based policies are thoroughly compared, and results showcase that
POMDPs achieve substantially lower costs as compared to their counterparts,
even for traditional problem settings.

    

### [[2009.05794] Open Benchmarking for Click-Through Rate Prediction](http://arxiv.org/abs/2009.05794)


  Click-through rate (CTR) prediction is a critical task for many applications,
as its accuracy has a direct impact on user experience and platform revenue. In
recent years, CTR prediction has been widely studied in both academia and
industry, resulting in a wide variety of CTR prediction models. Unfortunately,
there is still a lack of standardized benchmarks and uniform evaluation
protocols for CTR prediction research. This leads to non-reproducible or even
inconsistent experimental results among existing studies, which largely limit
the practical value and potential impact of their research. In this work, we
aim to perform open benchmarking for CTR prediction and present a rigorous
comparison of different models in a reproducible manner. To this end, we ran
{over 7,000 experiments for more than 12,000 GPU hours in total to re-evaluate
24 existing models} on multiple dataset settings. Surprisingly, our experiments
show that with sufficient hyper-parameter search and model tuning, many deep
models have smaller differences than expected. The results also reveal that
making real progress on the modeling of CTR prediction is indeed a very
challenging research task. We believe that our benchmarking work could not only
allow researchers to gauge the effectiveness of new models conveniently but
also make them fairly compare with the state of the arts. We have publicly
released the benchmarking tools, evaluation protocols, and experimental
settings of our work to promote reproducible research in this field.

    

### [[2010.02278] Reasons People Want Explanations After Unrecoverable Pre-Handover Failures](http://arxiv.org/abs/2010.02278)


  Most research on human-robot handovers focuses on the development of
comfortable and efficient HRI; few have studied handover failures. If a failure
occurs in the beginning of the interaction, it prevents the whole handover
process and destroys trust. Here we analyze the underlying reasons why people
want explanations in a handover scenario where a robot cannot possess the
object. Results suggest that participants set expectations on their request and
that a robot should provide explanations rather than non-verbal cues after
failing. Participants also expect that their handover request can be done by a
robot, and, if not, would like to be able to fix the robot or change the
request based on the provided explanations.

    

### [[2012.10820] AdnFM: An Attentive DenseNet based Factorization Machine for CTR Prediction](http://arxiv.org/abs/2012.10820)


  In this paper, we consider the Click-Through-Rate (CTR) prediction problem.
Factorization Machines and their variants consider pair-wise feature
interactions, but normally we won't do high-order feature interactions using FM
due to high time complexity. Given the success of deep neural networks (DNNs)
in many fields, researchers have proposed several DNN-based models to learn
high-order feature interactions. Multi-layer perceptrons (MLP) have been widely
employed to learn reliable mappings from feature embeddings to final logits. In
this paper, we aim to explore more about these high-order features
interactions. However, high-order feature interaction deserves more attention
and further development. Inspired by the great achievements of Densely
Connected Convolutional Networks (DenseNet) in computer vision, we propose a
novel model called Attentive DenseNet based Factorization Machines (AdnFM).
AdnFM can extract more comprehensive deep features by using all the hidden
layers from a feed-forward neural network as implicit high-order features, then
selects dominant features via an attention mechanism. Also, high-order
interactions in the implicit way using DNNs are more cost-efficient than in the
explicit way, for example in FM. Extensive experiments on two real-world
datasets show that the proposed model can effectively improve the performance
of CTR prediction.

    

### [[2105.03090] An Extended Jump Functions Benchmark for the Analysis of Randomized Search Heuristics](http://arxiv.org/abs/2105.03090)


  Jump functions are the {most-studied} non-unimodal benchmark in the theory of
randomized search heuristics, in particular, evolutionary algorithms (EAs).
They have significantly improved our understanding of how EAs escape from local
optima. However, their particular structure -- to leave the local optimum one
can only jump directly to the global optimum -- raises the question of how
representative such results are.
For this reason, we propose an extended class $\textsc{Jump}_{k,\delta}$ of
jump functions that contain a valley of low fitness of width $\delta$ starting
at distance $k$ from the global optimum. We prove that several previous results
extend to this more general class: for all {$k \le \frac{n^{1/3}}{\ln{n}}$} and
$\delta < k$, the optimal mutation rate for the $(1+1)$~EA is
$\frac{\delta}{n}$, and the fast $(1+1)$~EA runs faster than the classical
$(1+1)$~EA by a factor super-exponential in $\delta$. However, we also observe
that some known results do not generalize: the randomized local search
algorithm with stagnation detection, which is faster than the fast $(1+1)$~EA
by a factor polynomial in $k$ on $\textsc{Jump}_k$, is slower by a factor
polynomial in $n$ on some $\textsc{Jump}_{k,\delta}$ instances.
Computationally, the new class allows experiments with wider fitness valleys,
especially when they lie further away from the global optimum.

    

### [[2111.13728] QECV: Quantum Error Correction Verification](http://arxiv.org/abs/2111.13728)


  Quantum Error Correction (QEC) is essential for fault-tolerant quantum
copmutation, and its implementation is a very sophisticated process involving
both quantum and classical hardware. Formulating and verifying the
decomposition of logical operations into physical ones is a challenge in
itself. In this paper, we propose QECV, a verification framework that can
efficiently verify the formal correctness of stabilizer codes, arguably the
most important class of QEC codes. QECV first comes with a concise language,
QECV-Lang, where stabilizers are treated as a first-class object, to represent
QEC programs. Stabilizers are also used as predicates in our new assertion
language, QECV-Assn, as logical and arithmetic operations of stabilizers can be
naturally defined. We derive a sound quantum Hoare logic proof system with a
set of inference rules for QECV to efficiently reason about the correctness of
QEC programs. We demonstrate the effectiveness of QECV with both theoretical
complexity analysis and in-depth case studies of two well-known stabilizer QEC
codes, the repetition code and the surface code.

    

### [[2111.13952] A Calculus for Modular Loop Acceleration and Non-Termination Proofs](http://arxiv.org/abs/2111.13952)


  Loop acceleration can be used to prove safety, reachability, runtime bounds,
and (non-)termination of programs operating on integers. To this end, a variety
of acceleration techniques has been proposed. However, all of them are
monolithic: Either they accelerate a loop successfully, or they fail
completely. In contrast, we present a calculus that allows for combining
acceleration techniques in a modular way and we show how to integrate many
existing acceleration techniques into our calculus. Moreover, we propose two
novel acceleration techniques that can be incorporated into our calculus
seamlessly. Some of these acceleration techniques apply only to non-terminating
loops. Thus, combining them with our novel calculus results in a new, modular
approach for proving non-termination. An empirical evaluation demonstrates the
applicability of our approach, both for loop acceleration and for proving
non-termination.

    