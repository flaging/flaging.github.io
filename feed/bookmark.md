
## 2021-11-9

### [[2111.03779] NSF Broadband Research 2020 Report](http://arxiv.org/abs/2111.03779)


  The internet has become a critical communications infrastructure, and access
is among the ``assets, systems, and networks, whether physical or virtual,
[that] are considered so vital to the United States that their incapacitation
or destruction would have a debilitating effect on security, national economic
security, national public health or safety, or any combination thereof.''
[CISA] But the internet is more than an issue for the nation as a whole.
Internet access affects the security, health, safety, and opportunities in life
for individuals and communities, and the economic vitality of businesses
everywhere.
On the one hand, the COVID-19 pandemic has revealed the success of broadband
access in allowing society to function, even during lockdowns. On the other
hand, the pandemic has exposed weak, unreliable, or even nonexistent, broadband
access and usability in many areas and for many individuals, including
especially rural residents and children in many school districts, urban, and
rural, as well as a compelling need for universal reach and affordability. It
was thus timely that in November 2020, the National Science Foundation (NSF)
sponsored a series of workshops to identify new research areas to drive the
broadband NSF agenda for the next five years. This request followed similar
initiatives during the past twenty years, the most recent in 2016.
This Broadband Research Workshop Report of 2021 discusses the research
questions and challenges that need to be addressed to provide robust,
affordable, and meaningful broadband access to every resident of the United
States.

    

### [[2111.03785] Centralization is about Control, not Protocols (Position Paper)](http://arxiv.org/abs/2111.03785)


  Many common ``consumer'' applications, i.e., applications widely used by
non-technical users, are now provided by a very small number of companies, even
if that set of companies differ across geographic regions, or rely on a very
small number of implementations even if the applications are largely
standards-based. While likely only a partial solution, we can draw on earlier
regulatory experiences to facilitate competition or at least lessen the impact
of the lack thereof.

    

### [[2111.04175] A Survey of Wireless Networks for Future Aerial COMmunications (FACOM)](http://arxiv.org/abs/2111.04175)


  Electrification turned over a new leaf in aviation by introducing new types
of aerial vehicles along with new means of transportation. Addressing a
plethora of use cases, drones are gaining attention and increasingly appear in
the sky. Emerging concepts of flying taxi enable passengers to be transported
over several tens of kilometers. Therefore, unmanned traffic management systems
are under development to cope with the complexity of future airspace, thereby
resulting in unprecedented communication needs. Moreover, the increase in the
number of commercial airplanes pushes the limits of voice-oriented
communications, and future options such as single-pilot operations demand
robust connectivity. In this survey, we provide a comprehensive review and
vision for enabling the connectivity applications of aerial vehicles utilizing
current and future communication technologies. We begin by categorizing the
connectivity use cases per aerial vehicle and analyzing their connectivity
requirements. By reviewing more than 500 related studies, we aim for a
comprehensive approach to cover wireless communication technologies, and
provide an overview of recent findings from the literature toward the
possibilities and challenges of employing the wireless communication standards.
After analyzing the network architectures, we list the open-source testbed
platforms to facilitate future investigations. This study helped us observe
that while numerous works focused on cellular technologies for aerial
platforms, a single wireless technology is not sufficient to meet the stringent
connectivity demands of the aerial use cases. We identified the need of further
investigations on multi-technology network architectures to enable robust
connectivity in the sky. Future works should consider suitable technology
combinations to develop unified aerial networks that can meet the diverse
quality of service demands.

    

### [[2111.04217] Access Management in Joint Sensing and Communication Systems: Efficiency versus Fairness](http://arxiv.org/abs/2111.04217)


  In this paper, we consider a distributed joint sensing and communication
(DJSC) system in which multiple radar sensors are deployed. Each sensor is
equipped with a sensing function and a communication function, and thus it is a
JSC node. The JSC nodes are able to perform sensing their surrounding
environments, e.g., weather conditions or available spectrum. Furthermore, they
can cooperatively detect and track a common target. The information, i.e., of
the environment and target, collected by the JSC nodes is transmitted to a base
station (BS) for further processing. As such, different aspects of the target
to be viewed simultaneously, which significantly improves the performance of
the target detection and tracking. However, both the sensing function and
communication function require a certain amount of bandwidth for their
operations, and deploying multiple JSC nodes may consume a large amount of
bandwidth. Therefore, we investigate the bandwidth allocation problem for the
DJSC system. In particular, we aim to optimize the bandwidth allocation to the
sensing function and the communication function of the JSC nodes. To improve
the allocation efficiency while benefiting the spatial diversity advantage of
the DJSC systems, the objective is to maximize the sum of sensing performances,
i.e., estimation rates, communication performances, i.e., communication data
rates, and fairnesses of all the users. The optimization problem is non-convex
and difficult to be solved. For this, we propose a fully polynomial time
approximation algorithm, and we prove that the approximation algorithm can
guarantee a near-optimal solution with an accuracy bound of $\epsilon$.
Furthermore, we propose to use a heuristic algorithm with lower complexity. The
simulation results show that both the proposed algorithms are able to achieve
the solutions close to the optimum in a computationally efficient fashion.

    

### [[2104.01283] A Review of AI-enabled Routing Protocols for UAV Networks: Trends, Challenges, and Future Outlook](http://arxiv.org/abs/2104.01283)


  Unmanned Aerial Vehicles (UAVs), as a recently emerging technology, enabled a
new breed of unprecedented applications in different domains. This technology's
ongoing trend is departing from large remotely-controlled drones to networks of
small autonomous drones to collectively complete intricate tasks time and
cost-effectively. An important challenge is developing efficient sensing,
communication, and control algorithms that can accommodate the requirements of
highly dynamic UAV networks with heterogeneous mobility levels. Recently, the
use of Artificial Intelligence (AI) in learning-based networking has gained
momentum to harness the learning power of cognizant nodes to make more
intelligent networking decisions by integrating computational intelligence into
UAV networks. An important example of this trend is developing learning-powered
routing protocols, where machine learning methods are used to model and predict
topology evolution, channel status, traffic mobility, and environmental factors
for enhanced routing.
This paper reviews AI-enabled routing protocols designed primarily for aerial
networks, including topology-predictive and self-adaptive learning-based
routing algorithms, with an emphasis on accommodating highly-dynamic network
topology. To this end, we justify the importance and adaptation of AI into UAV
network communications. We also address, with an AI emphasis, the closely
related topics of mobility and networking models for UAV networks, simulation
tools and public datasets, and relations to UAV swarming, which serve to choose
the right algorithm for each scenario. We conclude by presenting future trends,
and the remaining challenges in AI-based UAV networking, for different aspects
of routing, connectivity, topology control, security and privacy, energy
efficiency, and spectrum sharing.

    

### [[2110.11943] Solving N-player dynamic routing games with congestion: a mean field approach](http://arxiv.org/abs/2110.11943)


  The recent emergence of navigational tools has changed traffic patterns and
has now enabled new types of congestion-aware routing control like dynamic road
pricing. Using the fundamental diagram of traffic flows - applied in
macroscopic and mesoscopic traffic modeling - the article introduces a new
N-player dynamic routing game with explicit congestion dynamics. The model is
well-posed and can reproduce heterogeneous departure times and congestion spill
back phenomena. However, as Nash equilibrium computations are PPAD-complete,
solving the game becomes intractable for large but realistic numbers of
vehicles N. Therefore, the corresponding mean field game is also introduced.
Experiments were performed on several classical benchmark networks of the
traffic community: the Pigou, Braess, and Sioux Falls networks with
heterogeneous origin, destination and departure time tuples. The Pigou and the
Braess examples reveal that the mean field approximation is generally very
accurate and computationally efficient as soon as the number of vehicles
exceeds a few dozen. On the Sioux Falls network (76 links, 100 time steps),
this approach enables learning traffic dynamics with more than 14,000 vehicles.

    

### [[2111.03662] Predicting Mortality from Credit Reports](http://arxiv.org/abs/2111.03662)


  Data on hundreds of variables related to individual consumer finance behavior
(such as credit card and loan activity) is routinely collected in many
countries and plays an important role in lending decisions. We postulate that
the detailed nature of this data may be used to predict outcomes in seemingly
unrelated domains such as individual health. We build a series of machine
learning models to demonstrate that credit report data can be used to predict
individual mortality. Variable groups related to credit cards and various
loans, mostly unsecured loans, are shown to carry significant predictive power.
Lags of these variables are also significant thus indicating that dynamics also
matters. Improved mortality predictions based on consumer finance data can have
important economic implications in insurance markets but may also raise privacy
concerns.

    

### [[2111.03663] First steps on Gamification of Lung Fluid Cells Annotations in the Flower Domain](http://arxiv.org/abs/2111.03663)


  Annotating data, especially in the medical domain, requires expert knowledge
and a lot of effort. This limits the amount and/or usefulness of available
medical data sets for experimentation. Therefore, developing strategies to
increase the number of annotations while lowering the needed domain knowledge
is of interest. A possible strategy is the use of gamification, that is i.e.
transforming the annotation task into a game. We propose an approach to gamify
the task of annotating lung fluid cells from pathological whole slide images.
As this domain is unknown to non-expert annotators, we transform images of
cells detected with a RetinaNet architecture to the domain of flower images.
This domain transfer is performed with a CycleGAN architecture for different
cell types. In this more assessable domain, non-expert annotators can be
(t)asked to annotate different kinds of flowers in a playful setting. In order
to provide a proof of concept, this work shows that the domain transfer is
possible by evaluating an image classification network trained on real cell
images and tested on the cell images generated by the CycleGAN network. The
classification network reaches an accuracy of 97.48% and 95.16% on the original
lung fluid cells and transformed lung fluid cells, respectively. With this
study, we lay the foundation for future research on gamification using
CycleGANs.

    

### [[2111.03664] Oracle Teacher: Towards Better Knowledge Distillation](http://arxiv.org/abs/2111.03664)


  Knowledge distillation (KD), best known as an effective method for model
compression, aims at transferring the knowledge of a bigger network (teacher)
to a much smaller network (student). Conventional KD methods usually employ the
teacher model trained in a supervised manner, where output labels are treated
only as targets. Extending this supervised scheme further, we introduce a new
type of teacher model for KD, namely Oracle Teacher, that utilizes the
embeddings of both the source inputs and the output labels to extract a more
accurate knowledge to be transferred to the student. The proposed model follows
the encoder-decoder attention structure of the Transformer network, which
allows the model to attend to related information from the output labels.
Extensive experiments are conducted on three different sequence learning tasks:
speech recognition, scene text recognition, and machine translation. From the
experimental results, we empirically show that the proposed model improves the
students across these tasks while achieving a considerable speed-up in the
teacher model's training time.

    

### [[2111.03693] Disaster mapping from satellites: damage detection with crowdsourced point labels](http://arxiv.org/abs/2111.03693)


  High-resolution satellite imagery available immediately after disaster events
is crucial for response planning as it facilitates broad situational awareness
of critical infrastructure status such as building damage, flooding, and
obstructions to access routes. Damage mapping at this scale would require
hundreds of expert person-hours. However, a combination of crowdsourcing and
recent advances in deep learning reduces the effort needed to just a few hours
in real time. Asking volunteers to place point marks, as opposed to shapes of
actual damaged areas, significantly decreases the required analysis time for
response during the disaster. However, different volunteers may be inconsistent
in their marking. This work presents methods for aggregating potentially
inconsistent damage marks to train a neural network damage detector.

    

### [[2111.03702] Reconstructing Training Data from Diverse ML Models by Ensemble Inversion](http://arxiv.org/abs/2111.03702)


  Model Inversion (MI), in which an adversary abuses access to a trained
Machine Learning (ML) model attempting to infer sensitive information about its
original training data, has attracted increasing research attention. During MI,
the trained model under attack (MUA) is usually frozen and used to guide the
training of a generator, such as a Generative Adversarial Network (GAN), to
reconstruct the distribution of the original training data of that model. This
might cause leakage of original training samples, and if successful, the
privacy of dataset subjects will be at risk if the training data contains
Personally Identifiable Information (PII). Therefore, an in-depth investigation
of the potentials of MI techniques is crucial for the development of
corresponding defense techniques. High-quality reconstruction of training data
based on a single model is challenging. However, existing MI literature does
not explore targeting multiple models jointly, which may provide additional
information and diverse perspectives to the adversary.
We propose the ensemble inversion technique that estimates the distribution
of original training data by training a generator constrained by an ensemble
(or set) of trained models with shared subjects or entities. This technique
leads to noticeable improvements of the quality of the generated samples with
distinguishable features of the dataset entities compared to MI of a single ML
model. We achieve high quality results without any dataset and show how
utilizing an auxiliary dataset that's similar to the presumed training data
improves the results. The impact of model diversity in the ensemble is
thoroughly investigated and additional constraints are utilized to encourage
sharp predictions and high activations for the reconstructed samples, leading
to more accurate reconstruction of training images.

    

### [[2111.03706] Inferring untrained complex dynamics of delay systems using an adapted echo state network](http://arxiv.org/abs/2111.03706)


  Caused by finite signal propagation velocities, many complex systems feature
time delays that may induce high-dimensional chaotic behavior and make
forecasting intricate. Here, we propose an echo state network adaptable to the
physics of systems with arbitrary delays. After training the network to
forecast a system with a unique and sufficiently long delay, it already learned
to predict the system dynamics for all other delays. A simple adaptation of the
network's topology allows us to infer untrained features such as
high-dimensional chaotic attractors, bifurcations, and even multistabilities,
that emerge with shorter and longer delays. Thus, the fusion of physical
knowledge of the delay system and data-driven machine learning yields a model
with high generalization capabilities and unprecedented prediction accuracy.

    

### [[2111.03707] Feature-Level Fusion of Super-App and Telecommunication Alternative Data Sources for Credit Card Fraud Detection](http://arxiv.org/abs/2111.03707)


  Identity theft is a major problem for credit lenders when there's not enough
data to corroborate a customer's identity. Among super-apps large digital
platforms that encompass many different services this problem is even more
relevant; losing a client in one branch can often mean losing them in other
services. In this paper, we review the effectiveness of a feature-level fusion
of super-app customer information, mobile phone line data, and traditional
credit risk variables for the early detection of identity theft credit card
fraud. Through the proposed framework, we achieved better performance when
using a model whose input is a fusion of alternative data and traditional
credit bureau data, achieving a ROC AUC score of 0.81. We evaluate our approach
over approximately 90,000 users from a credit lender's digital platform
database. The evaluation was performed using not only traditional ML metrics
but the financial costs as well.

    

### [[2111.03708] Damage Estimation and Localization from Sparse Aerial Imagery](http://arxiv.org/abs/2111.03708)


  Aerial images provide important situational awareness for responding to
natural disasters such as hurricanes. They are well-suited for providing
information for damage estimation and localization (DEL); i.e., characterizing
the type and spatial extent of damage following a disaster. Despite recent
advances in sensing and unmanned aerial systems technology, much of
post-disaster aerial imagery is still taken by handheld DSLR cameras from
small, manned, fixed-wing aircraft. However, these handheld cameras lack IMU
information, and images are taken opportunistically post-event by operators. As
such, DEL from such imagery is still a highly manual and time-consuming
process. We propose an approach to both detect damage in aerial images and
localize it in world coordinates, with specific focus on detecting and
localizing flooding. The approach is based on using structure from motion to
relate image coordinates to world coordinates via a projective transformation,
using class activation mapping to detect the extent of damage in an image, and
applying the projective transformation to localize damage in world coordinates.
We evaluate the performance of our approach on post-event data from the 2016
Louisiana floods, and find that our approach achieves a precision of 88%. Given
this high precision using limited data, we argue that this approach is
currently viable for fast and effective DEL from handheld aerial imagery for
disaster response.

    

### [[2111.03715] Leveraging Sentiment Analysis Knowledge to Solve Emotion Detection Tasks](http://arxiv.org/abs/2111.03715)


  Identifying and understanding underlying sentiment or emotions in text is a
key component of multiple natural language processing applications. While
simple polarity sentiment analysis is a well-studied subject, fewer advances
have been made in identifying more complex, finer-grained emotions using only
textual data. In this paper, we present a Transformer-based model with a Fusion
of Adapter layers which leverages knowledge from more simple sentiment analysis
tasks to improve the emotion detection task on large scale dataset, such as
CMU-MOSEI, using the textual modality only. Results show that our proposed
method is competitive with other approaches. We obtained state-of-the-art
results for emotion recognition on CMU-MOSEI even while using only the textual
modality.

    

### [[2111.03727] Defect Detection on Semiconductor Wafers by Distribution Analysis](http://arxiv.org/abs/2111.03727)


  A method for object classification that is based on distribution analysis is
proposed. In addition, a method for finding relevant features and the
unification of this algorithm with another classification algorithm is
proposed. The presented classification algorithm has been applied successfully
to real-world measurement data from wafer fabrication of close to hundred
thousand chips of several product types. The presented algorithm prefers
finding the best rater in a low-dimensional search space over finding a good
rater in a high-dimensional search space. Our approach is interesting in that
it is fast (quasi-linear) and reached good to excellent prediction or detection
quality for real-world wafer data.

    

### [[2111.03729] Explaining neural network predictions of material strength](http://arxiv.org/abs/2111.03729)


  We recently developed a deep learning method that can determine the critical
peak stress of a material by looking at scanning electron microscope (SEM)
images of the material's crystals. However, it has been somewhat unclear what
kind of image features the network is keying off of when it makes its
prediction. It is common in computer vision to employ an explainable AI
saliency map to tell one what parts of an image are important to the network's
decision. One can usually deduce the important features by looking at these
salient locations. However, SEM images of crystals are more abstract to the
human observer than natural image photographs. As a result, it is not easy to
tell what features are important at the locations which are most salient. To
solve this, we developed a method that helps us map features from important
locations in SEM images to non-abstract textures that are easier to interpret.

    

### [[2111.03731] Frugal Machine Learning](http://arxiv.org/abs/2111.03731)


  Machine learning, already at the core of increasingly many systems and
applications, is set to become even more ubiquitous with the rapid rise of
wearable devices and the Internet of Things. In most machine learning
applications, the main focus is on the quality of the results achieved (e.g.,
prediction accuracy), and hence vast amounts of data are being collected,
requiring significant computational resources to build models. In many
scenarios, however, it is infeasible or impractical to set up large centralized
data repositories. In personal health, for instance, privacy issues may inhibit
the sharing of detailed personal data. In such cases, machine learning should
ideally be performed on wearable devices themselves, which raises major
computational limitations such as the battery capacity of smartwatches. This
paper thus investigates frugal learning, aimed to build the most accurate
possible models using the least amount of resources. A wide range of learning
algorithms is examined through a frugal lens, analyzing their accuracy/runtime
performance on a wide range of data sets. The most promising algorithms are
thereafter assessed in a real-world scenario by implementing them in a
smartwatch and letting them learn activity recognition models on the watch
itself.

    

### [[2111.03739] Tradeoffs of Linear Mixed Models in Genome-wide Association Studies](http://arxiv.org/abs/2111.03739)


  Motivated by empirical arguments that are well-known from the genome-wide
association studies (GWAS) literature, we study the statistical properties of
linear mixed models (LMMs) applied to GWAS. First, we study the sensitivity of
LMMs to the inclusion of a candidate SNP in the kinship matrix, which is often
done in practice to speed up computations. Our results shed light on the size
of the error incurred by including a candidate SNP, providing a justification
to this technique in order to trade-off velocity against veracity. Second, we
investigate how mixed models can correct confounders in GWAS, which is widely
accepted as an advantage of LMMs over traditional methods. We consider two
sources of confounding factors, population stratification and environmental
confounding factors, and study how different methods that are commonly used in
practice trade-off these two confounding factors differently.

    

### [[2111.03740] Toward Learning Human-aligned Cross-domain Robust Models by Countering Misaligned Features](http://arxiv.org/abs/2111.03740)


  Machine learning has demonstrated remarkable prediction accuracy over i.i.d
data, but the accuracy often drops when tested with data from another
distribution. In this paper, we aim to offer another view of this problem in a
perspective assuming the reason behind this accuracy drop is the reliance of
models on the features that are not aligned well with how a data annotator
considers similar across these two datasets. We refer to these features as
misaligned features. We extend the conventional generalization error bound to a
new one for this setup with the knowledge of how the misaligned features are
associated with the label. Our analysis offers a set of techniques for this
problem, and these techniques are naturally linked to many previous methods in
robust machine learning literature. We also compared the empirical strength of
these methods demonstrated the performance when these previous techniques are
combined.

    

### [[2111.03741] Sharp Bounds for Federated Averaging (Local SGD) and Continuous Perspective](http://arxiv.org/abs/2111.03741)


  Federated Averaging (FedAvg), also known as Local SGD, is one of the most
popular algorithms in Federated Learning (FL). Despite its simplicity and
popularity, the convergence rate of FedAvg has thus far been undetermined. Even
under the simplest assumptions (convex, smooth, homogeneous, and bounded
covariance), the best-known upper and lower bounds do not match, and it is not
clear whether the existing analysis captures the capacity of the algorithm. In
this work, we first resolve this question by providing a lower bound for FedAvg
that matches the existing upper bound, which shows the existing FedAvg upper
bound analysis is not improvable. Additionally, we establish a lower bound in a
heterogeneous setting that nearly matches the existing upper bound. While our
lower bounds show the limitations of FedAvg, under an additional assumption of
third-order smoothness, we prove more optimistic state-of-the-art convergence
results in both convex and non-convex settings. Our analysis stems from a
notion we call iterate bias, which is defined by the deviation of the
expectation of the SGD trajectory from the noiseless gradient descent
trajectory with the same initialization. We prove novel sharp bounds on this
quantity, and show intuitively how to analyze this quantity from a Stochastic
Differential Equation (SDE) perspective.

    

### [[2111.03743] Increasing Data Diversity with Iterative Sampling to Improve Performance](http://arxiv.org/abs/2111.03743)


  As a part of the Data-Centric AI Competition, we propose a data-centric
approach to improve the diversity of the training samples by iterative
sampling. The method itself relies strongly on the fidelity of augmented
samples and the diversity of the augmentation methods. Moreover, we improve the
performance further by introducing more samples for the difficult classes
especially providing closer samples to edge cases potentially those the model
at hand misclassifies.

    

### [[2111.03745] An Algorithmic Theory of Metacognition in Minds and Machines](http://arxiv.org/abs/2111.03745)


  Humans sometimes choose actions that they themselves can identify as
sub-optimal, or wrong, even in the absence of additional information. How is
this possible? We present an algorithmic theory of metacognition based on a
well-understood trade-off in reinforcement learning (RL) between value-based RL
and policy-based RL. To the cognitive (neuro)science community, our theory
answers the outstanding question of why information can be used for error
detection but not for action selection. To the machine learning community, our
proposed theory creates a novel interaction between the Actor and Critic in
Actor-Critic agents and notes a novel connection between RL and Bayesian
Optimization. We call our proposed agent the Metacognitive Actor Critic (MAC).
We conclude with showing how to create metacognition in machines by
implementing a deep MAC and showing that it can detect (some of) its own
suboptimal actions without external information or delay.

    

### [[2111.03753] CloudRCA: A Root Cause Analysis Framework for Cloud Computing Platforms](http://arxiv.org/abs/2111.03753)


  As business of Alibaba expands across the world among various industries,
higher standards are imposed on the service quality and reliability of big data
cloud computing platforms which constitute the infrastructure of Alibaba Cloud.
However, root cause analysis in these platforms is non-trivial due to the
complicated system architecture. In this paper, we propose a root cause
analysis framework called CloudRCA which makes use of heterogeneous
multi-source data including Key Performance Indicators (KPIs), logs, as well as
topology, and extracts important features via state-of-the-art anomaly
detection and log analysis techniques. The engineered features are then
utilized in a Knowledge-informed Hierarchical Bayesian Network (KHBN) model to
infer root causes with high accuracy and efficiency. Ablation study and
comprehensive experimental comparisons demonstrate that, compared to existing
frameworks, CloudRCA 1) consistently outperforms existing approaches in
f1-score across different cloud systems; 2) can handle novel types of root
causes thanks to the hierarchical structure of KHBN; 3) performs more robustly
with respect to algorithmic configurations; and 4) scales more favorably in the
data and feature sizes. Experiments also show that a cross-platform transfer
learning mechanism can be adopted to further improve the accuracy by more than
10\%. CloudRCA has been integrated into the diagnosis system of Alibaba Cloud
and employed in three typical cloud computing platforms including MaxCompute,
Realtime Compute and Hologres. It saves Site Reliability Engineers (SREs) more
than $20\%$ in the time spent on resolving failures in the past twelve months
and improves service reliability significantly.

    

### [[2111.03759] MQBench: Towards Reproducible and Deployable Model Quantization Benchmark](http://arxiv.org/abs/2111.03759)


  Model quantization has emerged as an indispensable technique to accelerate
deep learning inference. While researchers continue to push the frontier of
quantization algorithms, existing quantization work is often unreproducible and
undeployable. This is because researchers do not choose consistent training
pipelines and ignore the requirements for hardware deployments. In this work,
we propose Model Quantization Benchmark (MQBench), a first attempt to evaluate,
analyze, and benchmark the reproducibility and deployability for model
quantization algorithms. We choose multiple different platforms for real-world
deployments, including CPU, GPU, ASIC, DSP, and evaluate extensive
state-of-the-art quantization algorithms under a unified training pipeline.
MQBench acts like a bridge to connect the algorithm and the hardware. We
conduct a comprehensive analysis and find considerable intuitive or
counter-intuitive insights. By aligning the training settings, we find existing
algorithms have about the same performance on the conventional academic track.
While for the hardware-deployable quantization, there is a huge accuracy gap
which remains unsettled. Surprisingly, no existing algorithm wins every
challenge in MQBench, and we hope this work could inspire future research
directions.

    

### [[2111.03772] Dynamic Regret Minimization for Control of Non-stationary Linear Dynamical Systems](http://arxiv.org/abs/2111.03772)


  We consider the problem of controlling a Linear Quadratic Regulator (LQR)
system over a finite horizon $T$ with fixed and known cost matrices $Q,R$, but
unknown and non-stationary dynamics $\{A_t, B_t\}$. The sequence of dynamics
matrices can be arbitrary, but with a total variation, $V_T$, assumed to be
$o(T)$ and unknown to the controller. Under the assumption that a sequence of
stabilizing, but potentially sub-optimal controllers is available for all $t$,
we present an algorithm that achieves the optimal dynamic regret of
$\tilde{\mathcal{O}}\left(V_T^{2/5}T^{3/5}\right)$. With piece-wise constant
dynamics, our algorithm achieves the optimal regret of
$\tilde{\mathcal{O}}(\sqrt{ST})$ where $S$ is the number of switches. The crux
of our algorithm is an adaptive non-stationarity detection strategy, which
builds on an approach recently developed for contextual Multi-armed Bandit
problems. We also argue that non-adaptive forgetting (e.g., restarting or using
sliding window learning with a static window size) may not be regret optimal
for the LQR problem, even when the window size is optimally tuned with the
knowledge of $V_T$. The main technical challenge in the analysis of our
algorithm is to prove that the ordinary least squares (OLS) estimator has a
small bias when the parameter to be estimated is non-stationary. Our analysis
also highlights that the key motif driving the regret is that the LQR problem
is in spirit a bandit problem with linear feedback and locally quadratic cost.
This motif is more universal than the LQR problem itself, and therefore we
believe our results should find wider application.

    

### [[2111.03788] d3rlpy: An Offline Deep Reinforcement Learning Library](http://arxiv.org/abs/2111.03788)


  In this paper, we introduce d3rlpy, an open-sourced offline deep
reinforcement learning (RL) library for Python. d3rlpy supports a number of
offline deep RL algorithms as well as online algorithms via a user-friendly
API. To assist deep RL research and development projects, d3rlpy provides
practical and unique features such as data collection, exporting policies for
deployment, preprocessing and postprocessing, distributional Q-functions,
multi-step learning and a convenient command-line interface. Furthermore,
d3rlpy additionally provides a novel graphical interface that enables users to
train offline RL algorithms without coding programs. Lastly, the implemented
algorithms are benchmarked with D4RL datasets to ensure the implementation
quality. The d3rlpy source code can be found on GitHub:
\url{this https URL}.

    

### [[2111.03794] Physics-Informed Neural Operator for Learning Partial Differential Equations](http://arxiv.org/abs/2111.03794)


  Machine learning methods have recently shown promise in solving partial
differential equations (PDEs). They can be classified into two broad
categories: approximating the solution function and learning the solution
operator. The Physics-Informed Neural Network (PINN) is an example of the
former while the Fourier neural operator (FNO) is an example of the latter.
Both these approaches have shortcomings. The optimization in PINN is
challenging and prone to failure, especially on multi-scale dynamic systems.
FNO does not suffer from this optimization issue since it carries out
supervised learning on a given dataset, but obtaining such data may be too
expensive or infeasible. In this work, we propose the physics-informed neural
operator (PINO), where we combine the operating-learning and
function-optimization frameworks. This integrated approach improves convergence
rates and accuracy over both PINN and FNO models. In the operator-learning
phase, PINO learns the solution operator over multiple instances of the
parametric PDE family. In the test-time optimization phase, PINO optimizes the
pre-trained operator ansatz for the querying instance of the PDE. Experiments
show PINO outperforms previous ML methods on many popular PDE families while
retaining the extraordinary speed-up of FNO compared to solvers. In particular,
PINO accurately solves challenging long temporal transient flows and Kolmogorov
flows where other baseline ML methods fail to converge.

    

### [[2111.03808] Contextual Unsupervised Outlier Detection in Sequences](http://arxiv.org/abs/2111.03808)


  This work proposes an unsupervised learning framework for trajectory
(sequence) outlier detection that combines ranking tests with user sequence
models. The overall framework identifies sequence outliers at a desired false
positive rate (FPR), in an otherwise parameter-free manner. We evaluate our
methodology on a collection of real and simulated datasets based on user
actions at the websites this http URL and this http URL, where we know ground truth, and
demonstrate improved accuracy over existing approaches. We also apply our
approach to a large real-world dataset of Pinterest and Facebook users, where
we find that users tend to re-share Pinterest posts of Facebook friends
significantly more than other types of users, pointing to a potential influence
of Facebook friendship on sharing behavior on Pinterest.

    

### [[2111.03820] Distributed stochastic proximal algorithm with random reshuffling for non-smooth finite-sum optimization](http://arxiv.org/abs/2111.03820)


  The non-smooth finite-sum minimization is a fundamental problem in machine
learning. This paper develops a distributed stochastic proximal-gradient
algorithm with random reshuffling to solve the finite-sum minimization over
time-varying multi-agent networks. The objective function is a sum of
differentiable convex functions and non-smooth regularization. Each agent in
the network updates local variables with a constant step-size by local
information and cooperates to seek an optimal solution. We prove that local
variable estimates generated by the proposed algorithm achieve consensus and
are attracted to a neighborhood of the optimal solution in expectation with an
$\mathcal{O}(\frac{1}{T}+\frac{1}{\sqrt{T}})$ convergence rate. In addition,
this paper shows that the steady-state error of the objective function can be
arbitrarily small by choosing small enough step-sizes. Finally, some
comparative simulations are provided to verify the convergence performance of
the proposed algorithm.

    

### [[2111.03837] Focusing on Possible Named Entities in Active Named Entity Label Acquisition](http://arxiv.org/abs/2111.03837)


  Named entity recognition (NER) aims to identify mentions of named entities in
an unstructured text and classify them into the predefined named entity
classes. Even though deep learning-based pre-trained language models achieve
good predictive performances, many domain-specific NERtasks still require a
sufficient amount of labeled data. Active learning (AL), a general framework
for the label acquisition problem, has been used for the NER tasks to minimize
the annotation cost without sacrificing model performance. However, heavily
imbalanced class distribution of tokens introduces challenges in designing
effective AL querying methods for NER. We propose AL sentence query evaluation
functions which pay more attention to possible positive tokens, and evaluate
these proposed functions with both sentence-based and token-based cost
evaluation strategies. We also propose a better data-driven normalization
approach to penalize too long or too short sentences. Our experiments on three
datasets from different domains reveal that the proposed approaches reduce the
number of annotated tokens while achieving better or comparable prediction
performance with conventional methods.

    

### [[2111.03842] Class Token and Knowledge Distillation for Multi-head Self-Attention Speaker Verification Systems](http://arxiv.org/abs/2111.03842)


  This paper explores three novel approaches to improve the performance of
speaker verification (SV) systems based on deep neural networks (DNN) using
Multi-head Self-Attention (MSA) mechanisms and memory layers. Firstly, we
propose the use of a learnable vector called Class token to replace the average
global pooling mechanism to extract the embeddings. Unlike global average
pooling, our proposal takes into account the temporal structure of the input
what is relevant for the text-dependent SV task. The class token is
concatenated to the input before the first MSA layer, and its state at the
output is used to predict the classes. To gain additional robustness, we
introduce two approaches. First, we have developed a Bayesian estimation of the
class token. Second, we have added a distilled representation token for
training a teacher-student pair of networks using the Knowledge Distillation
(KD) philosophy, which is combined with the class token. This distillation
token is trained to mimic the predictions from the teacher network, while the
class token replicates the true label. All the strategies have been tested on
the RSR2015-Part II and DeepMine-Part 1 databases for text-dependent SV,
providing competitive results compared to the same architecture using the
average pooling mechanism to extract average embeddings.

    

### [[2111.03853] A new baseline for retinal vessel segmentation: Numerical identification and correction of methodological inconsistencies affecting 100+ papers](http://arxiv.org/abs/2111.03853)


  In the last 15 years, the segmentation of vessels in retinal images has
become an intensively researched problem in medical imaging, with hundreds of
algorithms published. One of the de facto benchmarking data sets of vessel
segmentation techniques is the DRIVE data set. Since DRIVE contains a
predefined split of training and test images, the published performance results
of the various segmentation techniques should provide a reliable ranking of the
algorithms. Including more than 100 papers in the study, we performed a
detailed numerical analysis of the coherence of the published performance
scores. We found inconsistencies in the reported scores related to the use of
the field of view (FoV), which has a significant impact on the performance
scores. We attempted to eliminate the biases using numerical techniques to
provide a more realistic picture of the state of the art. Based on the results,
we have formulated several findings, most notably: despite the well-defined
test set of DRIVE, most rankings in published papers are based on
non-comparable figures; in contrast to the near-perfect accuracy scores
reported in the literature, the highest accuracy score achieved to date is
0.9582 in the FoV region, which is 1% higher than that of human annotators. The
methods we have developed for identifying and eliminating the evaluation biases
can be easily applied to other domains where similar problems may arise.

    

### [[2111.03854] Learning equilibria with personalized incentives in a class of nonmonotone games](http://arxiv.org/abs/2111.03854)


  We consider quadratic, nonmonotone generalized Nash equilibrium problems with
symmetric interactions among the agents, which are known to be potential. As
may happen in practical cases, we envision a scenario in which an explicit
expression of the underlying potential function is not available, and we design
a two-layer Nash equilibrium seeking algorithm. In the proposed scheme, a
coordinator iteratively integrates the noisy agents' feedback to learn the
pseudo-gradients of the agents, and then design personalized incentives for
them. On their side, the agents receive those personalized incentives, compute
a solution to an extended game, and then return feedback measures to the
coordinator. We show that our algorithm returns an equilibrium in case the
coordinator is endowed with standard learning policies, and corroborate our
results on a numerical instance of a hypomonotone game.

    

### [[2111.03861] What augmentations are sensitive to hyper-parameters and why?](http://arxiv.org/abs/2111.03861)


  We apply augmentations to our dataset to enhance the quality of our
predictions and make our final models more resilient to noisy data and domain
drifts. Yet the question remains, how are these augmentations going to perform
with different hyper-parameters? In this study we evaluate the sensitivity of
augmentations with regards to the model's hyper parameters along with their
consistency and influence by performing a Local Surrogate (LIME) interpretation
on the impact of hyper-parameters when different augmentations are applied to a
machine learning model. We have utilized Linear regression coefficients for
weighing each augmentation. Our research has proved that there are some
augmentations which are highly sensitive to hyper-parameters and others which
are more resilient and reliable.

    

### [[2111.03874] Towards Calibrated Model for Long-Tailed Visual Recognition from Prior Perspective](http://arxiv.org/abs/2111.03874)


  Real-world data universally confronts a severe class-imbalance problem and
exhibits a long-tailed distribution, i.e., most labels are associated with
limited instances. The na√Øve models supervised by such datasets would prefer
dominant labels, encounter a serious generalization challenge and become poorly
calibrated. We propose two novel methods from the prior perspective to
alleviate this dilemma. First, we deduce a balance-oriented data augmentation
named Uniform Mixup (UniMix) to promote mixup in long-tailed scenarios, which
adopts advanced mixing factor and sampler in favor of the minority. Second,
motivated by the Bayesian theory, we figure out the Bayes Bias (Bayias), an
inherent bias caused by the inconsistency of prior, and compensate it as a
modification on standard cross-entropy loss. We further prove that both the
proposed methods ensure the classification calibration theoretically and
empirically. Extensive experiments verify that our strategies contribute to a
better-calibrated model, and their combination achieves state-of-the-art
performance on CIFAR-LT, ImageNet-LT, and iNaturalist 2018.

    

### [[2111.03892] TND-NAS: Towards Non-differentiable Objectives in Progressive Differentiable NAS Framework](http://arxiv.org/abs/2111.03892)


  Differentiable architecture search has gradually become the mainstream
research topic in the field of Neural Architecture Search (NAS) for its
capability to improve efficiency compared with the early NAS (EA-based,
RL-based) methods. Recent differentiable NAS also aims at further improving
search efficiency, reducing the GPU-memory consumption, and addressing the
"depth gap" issue. However, these methods are no longer capable of tackling the
non-differentiable objectives, let alone multi-objectives, e.g., performance,
robustness, efficiency, and other metrics. We propose an end-to-end
architecture search framework towards non-differentiable objectives, TND-NAS,
with the merits of the high efficiency in differentiable NAS framework and the
compatibility among non-differentiable metrics in Multi-objective NAS (MNAS).
Under differentiable NAS framework, with the continuous relaxation of the
search space, TND-NAS has the architecture parameters ($\alpha$) been optimized
in discrete space, while resorting to the search policy of progressively
shrinking the supernetwork by $\alpha$. Our representative experiment takes two
objectives (Parameters, Accuracy) as an example, we achieve a series of
high-performance compact architectures on CIFAR10 (1.09M/3.3%, 2.4M/2.95%,
9.57M/2.54%) and CIFAR100 (2.46M/18.3%, 5.46/16.73%, 12.88/15.20%) datasets.
Favorably, under real-world scenarios (resource-constrained,
platform-specialized), the Pareto-optimal solutions can be conveniently reached
by TND-NAS.

    

### [[2111.03904] On pseudo-absence generation and machine learning for locust breeding ground prediction in Africa](http://arxiv.org/abs/2111.03904)


  Desert locust outbreaks threaten the food security of a large part of Africa
and have affected the livelihoods of millions of people over the years. Machine
learning (ML) has been demonstrated as an effective approach to locust
distribution modelling which could assist in early warning. ML requires a
significant amount of labelled data to train. Most publicly available labelled
data on locusts are presence-only data, where only the sightings of locusts
being present at a location are recorded. Therefore, prior work using ML have
resorted to pseudo-absence generation methods as a way to circumvent this
issue. The most commonly used approach is to randomly sample points in a region
of interest while ensuring that these sampled pseudo-absence points are at
least a specific distance away from true presence points. In this paper, we
compare this random sampling approach to more advanced pseudo-absence
generation methods, such as environmental profiling and optimal background
extent limitation, specifically for predicting desert locust breeding grounds
in Africa. Interestingly, we find that for the algorithms we tested, namely
logistic regression, gradient boosting, random forests and maximum entropy, all
popular in prior work, the logistic model performed significantly better than
the more sophisticated ensemble methods, both in terms of prediction accuracy
and F1 score. Although background extent limitation combined with random
sampling boosted performance for ensemble methods, for LR this was not the
case, and instead, a significant improvement was obtained when using
environmental profiling. In light of this, we conclude that a simpler ML
approach such as logistic regression combined with more advanced pseudo-absence
generation, specifically environmental profiling, can be a sensible and
effective approach to predicting locust breeding grounds across Africa.

    

### [[2111.03915] Robust Deep Reinforcement Learning for Quadcopter Control](http://arxiv.org/abs/2111.03915)


  Deep reinforcement learning (RL) has made it possible to solve complex
robotics problems using neural networks as function approximators. However, the
policies trained on stationary environments suffer in terms of generalization
when transferred from one environment to another. In this work, we use Robust
Markov Decision Processes (RMDP) to train the drone control policy, which
combines ideas from Robust Control and RL. It opts for pessimistic optimization
to handle potential gaps between policy transfer from one environment to
another. The trained control policy is tested on the task of quadcopter
positional control. RL agents were trained in a MuJoCo simulator. During
testing, different environment parameters (unseen during the training) were
used to validate the robustness of the trained policy for transfer from one
environment to another. The robust policy outperformed the standard agents in
these environments, suggesting that the added robustness increases generality
and can adapt to non-stationary environments.
Codes: this https URL


### [[2111.03917] Optimal and Efficient Dynamic Regret Algorithms for Non-Stationary Dueling Bandits](http://arxiv.org/abs/2111.03917)


  We study the problem of \emph{dynamic regret minimization} in $K$-armed
Dueling Bandits under non-stationary or time varying preferences. This is an
online learning setup where the agent chooses a pair of items at each round and
observes only a relative binary `win-loss' feedback for this pair, sampled from
an underlying preference matrix at that round. We first study the problem of
static-regret minimization for adversarial preference sequences and design an
efficient algorithm with $O(\sqrt{KT})$ high probability regret. We next use
similar algorithmic ideas to propose an efficient and provably optimal
algorithm for dynamic-regret minimization under two notions of
non-stationarities. In particular, we establish $\tO(\sqrt{SKT})$ and
$\tO({V_T^{1/3}K^{1/3}T^{2/3}})$ dynamic-regret guarantees, $S$ being the total
number of `effective-switches' in the underlying preference relations and $V_T$
being a measure of `continuous-variation' non-stationarity. The complexity of
these problems have not been studied prior to this work despite the
practicability of non-stationary environments in real world systems. We justify
the optimality of our algorithms by proving matching lower bound guarantees
under both the above-mentioned notions of non-stationarities. Finally, we
corroborate our results with extensive simulations and compare the efficacy of
our algorithms over state-of-the-art baselines.

    

### [[2111.03923] Deep Learning Based Model for Breast Cancer Subtype Classification](http://arxiv.org/abs/2111.03923)


  Breast cancer has long been a prominent cause of mortality among women.
Diagnosis, therapy, and prognosis are now possible, thanks to the availability
of RNA sequencing tools capable of recording gene expression data. Molecular
subtyping being closely related to devising clinical strategy and prognosis,
this paper focuses on the use of gene expression data for the classification of
breast cancer into four subtypes, namely, Basal, Her2, LumA, and LumB. In stage
1, we suggested a deep learning-based model that uses an autoencoder to reduce
dimensionality. The size of the feature set is reduced from 20,530 gene
expression values to 500 by using an autoencoder. This encoded representation
is passed to the deep neural network of the second stage for the classification
of patients into four molecular subtypes of breast cancer. By deploying the
combined network of stages 1 and 2, we have been able to attain a mean 10-fold
test accuracy of 0.907 on the TCGA breast cancer dataset. The proposed
framework is fairly robust throughout 10 different runs, as shown by the
boxplot for classification accuracy. Compared to related work reported in the
literature, we have achieved a competitive outcome. In conclusion, the proposed
two-stage deep learning-based model is able to accurately classify four breast
cancer subtypes, highlighting the autoencoder's capacity to deduce the compact
representation and the neural network classifier's ability to correctly label
breast cancer patients.

    

### [[2111.03932] AGGLIO: Global Optimization for Locally Convex Functions](http://arxiv.org/abs/2111.03932)


  This paper presents AGGLIO (Accelerated Graduated Generalized LInear-model
Optimization), a stage-wise, graduated optimization technique that offers
global convergence guarantees for non-convex optimization problems whose
objectives offer only local convexity and may fail to be even quasi-convex at a
global scale. In particular, this includes learning problems that utilize
popular activation functions such as sigmoid, softplus and SiLU that yield
non-convex training objectives. AGGLIO can be readily implemented using point
as well as mini-batch SGD updates and offers provable convergence to the global
optimum in general conditions. In experiments, AGGLIO outperformed several
recently proposed optimization techniques for non-convex and locally convex
objectives in terms of convergence rate as well as convergent accuracy. AGGLIO
relies on a graduation technique for generalized linear models, as well as a
novel proof strategy, both of which may be of independent interest.

    

### [[2111.03936] SOPE: Spectrum of Off-Policy Estimators](http://arxiv.org/abs/2111.03936)


  Many sequential decision making problems are high-stakes and require
off-policy evaluation (OPE) of a new policy using historical data collected
using some other policy. One of the most common OPE techniques that provides
unbiased estimates is trajectory based importance sampling (IS). However, due
to the high variance of trajectory IS estimates, importance sampling methods
based on state-action visitation distributions (SIS) have recently been
adopted. Unfortunately, while SIS often provides lower variance estimates for
long horizons, estimating the state-action distribution ratios can be
challenging and lead to biased estimates. In this paper, we present a new
perspective on this bias-variance trade-off and show the existence of a
spectrum of estimators whose endpoints are SIS and IS. Additionally, we also
establish a spectrum for doubly-robust and weighted version of these
estimators. We provide empirical evidence that estimators in this spectrum can
be used to trade-off between the bias and variance of IS and SIS and can
achieve lower mean-squared error than both IS and SIS.

    

### [[2111.03940] Convolutional Gated MLP: Combining Convolutions & gMLP](http://arxiv.org/abs/2111.03940)


  To the best of our knowledge, this is the first paper to introduce
Convolutions to Gated MultiLayer Perceptron and contributes an implementation
of this novel Deep Learning architecture. Google Brain introduced the gMLP in
May 2021. Microsoft introduced Convolutions in Vision Transformer in Mar 2021.
Inspired by both gMLP and CvT, we introduce convolutional layers in gMLP. CvT
combined the power of Convolutions and Attention. Our implementation combines
the best of Convolutional learning along with spatial gated MLP. Further, the
paper visualizes how CgMLP learns. Visualizations show how CgMLP learns from
features such as outline of a car. While Attention was the basis of much of
recent progress in Deep Learning, gMLP proposed an approach that doesn't use
Attention computation. In Transformer based approaches, a whole lot of
Attention matrixes need to be learnt using vast amount of training data. In
gMLP, the fine tunning for new tasks can be challenging by transfer learning
with smaller datasets. We implement CgMLP and compares it with gMLP on CIFAR
dataset. Experimental results explore the power of generaliza-tion of CgMLP,
while gMLP tend to drastically overfit the training data.
To summarize, the paper contributes a novel Deep Learning architecture and
demonstrates the learning mechanism of CgMLP through visualizations, for the
first time in literature.

    

### [[2111.03941] Time Discretization-Invariant Safe Action Repetition for Policy Gradient Methods](http://arxiv.org/abs/2111.03941)


  In reinforcement learning, continuous time is often discretized by a time
scale $\delta$, to which the resulting performance is known to be highly
sensitive. In this work, we seek to find a $\delta$-invariant algorithm for
policy gradient (PG) methods, which performs well regardless of the value of
$\delta$. We first identify the underlying reasons that cause PG methods to
fail as $\delta \to 0$, proving that the variance of the PG estimator can
diverge to infinity in stochastic environments under a certain assumption of
stochasticity. While durative actions or action repetition can be employed to
have $\delta$-invariance, previous action repetition methods cannot immediately
react to unexpected situations in stochastic environments. We thus propose a
novel $\delta$-invariant method named Safe Action Repetition (SAR) applicable
to any existing PG algorithm. SAR can handle the stochasticity of environments
by adaptively reacting to changes in states during action repetition. We
empirically show that our method is not only $\delta$-invariant but also robust
to stochasticity, outperforming previous $\delta$-invariant approaches on eight
MuJoCo environments with both deterministic and stochastic settings. Our code
is available at this https URL.

    

### [[2111.03943] A Probit Tensor Factorization Model For Relational Learning](http://arxiv.org/abs/2111.03943)


  With the proliferation of knowledge graphs, modeling data with complex
multirelational structure has gained increasing attention in the area of
statistical relational learning. One of the most important goals of statistical
relational learning is link prediction, i.e., predicting whether certain
relations exist in the knowledge graph. A large number of models and algorithms
have been proposed to perform link prediction, among which tensor factorization
method has proven to achieve state-of-the-art performance in terms of
computation efficiency and prediction accuracy. However, a common drawback of
the existing tensor factorization models is that the missing relations and
non-existing relations are treated in the same way, which results in a loss of
information. To address this issue, we propose a binary tensor factorization
model with probit link, which not only inherits the computation efficiency from
the classic tensor factorization model but also accounts for the binary nature
of relational data. Our proposed probit tensor factorization (PTF) model shows
advantages in both the prediction accuracy and interpretability

    

### [[2111.03947] Exponential Bellman Equation and Improved Regret Bounds for Risk-Sensitive Reinforcement Learning](http://arxiv.org/abs/2111.03947)


  We study risk-sensitive reinforcement learning (RL) based on the entropic
risk measure. Although existing works have established non-asymptotic regret
guarantees for this problem, they leave open an exponential gap between the
upper and lower bounds. We identify the deficiencies in existing algorithms and
their analysis that result in such a gap. To remedy these deficiencies, we
investigate a simple transformation of the risk-sensitive Bellman equations,
which we call the exponential Bellman equation. The exponential Bellman
equation inspires us to develop a novel analysis of Bellman backup procedures
in risk-sensitive RL algorithms, and further motivates the design of a novel
exploration mechanism. We show that these analytic and algorithmic innovations
together lead to improved regret upper bounds over existing ones.

    

### [[2111.03949] Deep Neyman-Scott Processes](http://arxiv.org/abs/2111.03949)


  A Neyman-Scott process is a special case of a Cox process. The latent and
observable stochastic processes are both Poisson processes. We consider a deep
Neyman-Scott process in this paper, for which the building components of a
network are all Poisson processes. We develop an efficient posterior sampling
via Markov chain Monte Carlo and use it for likelihood-based inference. Our
method opens up room for the inference in sophisticated hierarchical point
processes. We show in the experiments that more hidden Poisson processes brings
better performance for likelihood fitting and events types prediction. We also
compare our method with state-of-the-art models for temporal real-world
datasets and demonstrate competitive abilities for both data fitting and
prediction, using far fewer parameters.

    

### [[2111.03950] Kernel Methods for Multistage Causal Inference: Mediation Analysis and Dynamic Treatment Effects](http://arxiv.org/abs/2111.03950)


  We propose kernel ridge regression estimators for mediation analysis and
dynamic treatment effects over short horizons. We allow treatments, covariates,
and mediators to be discrete or continuous, and low, high, or infinite
dimensional. We propose estimators of means, increments, and distributions of
counterfactual outcomes with closed form solutions in terms of kernel matrix
operations. For the continuous treatment case, we prove uniform consistency
with finite sample rates. For the discrete treatment case, we prove root-n
consistency, Gaussian approximation, and semiparametric efficiency. We conduct
simulations then estimate mediated and dynamic treatment effects of the US Job
Corps program for disadvantaged youth.

    

### [[2111.03952] CALText: Contextual Attention Localization for Offline Handwritten Text](http://arxiv.org/abs/2111.03952)


  Recognition of Arabic-like scripts such as Persian and Urdu is more
challenging than Latin-based scripts. This is due to the presence of a
two-dimensional structure, context-dependent character shapes, spaces and
overlaps, and placement of diacritics. Not much research exists for offline
handwritten Urdu script which is the 10th most spoken language in the world. We
present an attention based encoder-decoder model that learns to read Urdu in
context. A novel localization penalty is introduced to encourage the model to
attend only one location at a time when recognizing the next character. In
addition, we comprehensively refine the only complete and publicly available
handwritten Urdu dataset in terms of ground-truth annotations. We evaluate the
model on both Urdu and Arabic datasets and show that contextual attention
localization outperforms both simple attention and multi-directional LSTM
models.

    

### [[2111.03967] A Deep Reinforcement Learning Approach for Composing Moving IoT Services](http://arxiv.org/abs/2111.03967)


  We develop a novel framework for efficiently and effectively discovering
crowdsourced services that move in close proximity to a user over a period of
time. We introduce a moving crowdsourced service model which is modelled as a
moving region. We propose a deep reinforcement learning-based composition
approach to select and compose moving IoT services considering quality
parameters. Additionally, we develop a parallel flock-based service discovery
algorithm as a ground-truth to measure the accuracy of the proposed approach.
The experiments on two real-world datasets verify the effectiveness and
efficiency of the deep reinforcement learning-based approach.

    

### [[2111.03971] Towards noise robust trigger-word detection with contrastive learning pre-task for fast on-boarding of new trigger-words](http://arxiv.org/abs/2111.03971)


  Trigger-word detection plays an important role as the entry point of user's
communication with voice assistants. But supporting a particular word as a
trigger-word involves huge amount of data collection, augmentation and
labelling for that word. This makes supporting new trigger-words a tedious and
time consuming process. To combat this, we explore the use of contrastive
learning as a pre-training task that helps the detection model to generalize to
different words and noise conditions. We explore supervised contrastive
techniques and also propose a self-supervised technique using chunked words
from long sentence audios. We show that the contrastive pre-training techniques
have comparable results to a traditional classification pre-training on new
trigger words with less data availability.

    

### [[2111.03972] Understanding Layer-wise Contributions in Deep Neural Networks through Spectral Analysis](http://arxiv.org/abs/2111.03972)


  Spectral analysis is a powerful tool, decomposing any function into simpler
parts. In machine learning, Mercer's theorem generalizes this idea, providing
for any kernel and input distribution a natural basis of functions of
increasing frequency. More recently, several works have extended this analysis
to deep neural networks through the framework of Neural Tangent Kernel. In this
work, we analyze the layer-wise spectral bias of Deep Neural Networks and
relate it to the contributions of different layers in the reduction of
generalization error for a given target function. We utilize the properties of
Hermite polynomials and spherical harmonics to prove that initial layers
exhibit a larger bias towards high-frequency functions defined on the unit
sphere. We further provide empirical results validating our theory in high
dimensional datasets for Deep Neural Networks.

    

### [[2111.03976] CubeLearn: End-to-end Learning for Human Motion Recognition from Raw mmWave Radar Signals](http://arxiv.org/abs/2111.03976)


  mmWave FMCW radar has attracted huge amount of research interest for
human-centered applications in recent years, such as human gesture/activity
recognition. Most existing pipelines are built upon conventional Discrete
Fourier Transform (DFT) pre-processing and deep neural network classifier
hybrid methods, with a majority of previous works focusing on designing the
downstream classifier to improve overall accuracy. In this work, we take a step
back and look at the pre-processing module. To avoid the drawbacks of
conventional DFT pre-processing, we propose a learnable pre-processing module,
named CubeLearn, to directly extract features from raw radar signal and build
an end-to-end deep neural network for mmWave FMCW radar motion recognition
applications. Extensive experiments show that our CubeLearn module consistently
improves the classification accuracies of different pipelines, especially
benefiting those previously weaker models. We provide ablation studies on
initialization methods and structure of the proposed module, as well as an
evaluation of the running time on PC and edge devices. This work also serves as
a comparison of different approaches towards data cube slicing. Through our
task agnostic design, we propose a first step towards a generic end-to-end
solution for radar recognition problems.

    

### [[2111.03984] Proposing an Interactive Audit Pipeline for Visual Privacy Research](http://arxiv.org/abs/2111.03984)


  In an ideal world, deployed machine learning models will enhance our society.
We hope that those models will provide unbiased and ethical decisions that will
benefit everyone. However, this is not always the case; issues arise from the
data curation process to the models' deployment. The continued use of biased
datasets and processes will adversely damage communities and increase the cost
to fix the problem. In this work, we walk through the decision process that a
researcher will need to make before, during, and after their project to
consider the broader impacts of research and the community. Throughout this
paper, we observe the critical decisions that are often overlooked when
deploying AI, argue for the use of fairness forensics to discover bias and
fairness issues in systems, assert the need for a responsible
human-over-the-loop to bring accountability into the deployed system, and
finally, reflect on the need to explore research agendas that have harmful
societal impacts. We examine visual privacy research and draw lessons that can
apply broadly to Artificial Intelligence. Our goal is to provide a systematic
analysis of the machine learning pipeline for visual privacy and bias issues.
With this pipeline, we hope to raise stakeholder (e.g., researchers, modelers,
corporations) awareness as these issues propagate in the various machine
learning phases.

    

### [[2111.03985] Machine Learning-Assisted E-jet Printing of Organic Flexible Biosensors](http://arxiv.org/abs/2111.03985)


  Electrohydrodynamic-jet (e-jet) printing technique enables the
high-resolution printing of complex soft electronic devices. As such, it has an
unmatched potential for becoming the conventional technique for printing soft
electronic devices. In this study, the electrical conductivity of the e-jet
printed circuits was studied as a function of key printing parameters (nozzle
speed, ink flow rate, and voltage). The collected experimental dataset was then
used to train a machine learning algorithm to establish models capable of
predicting the characteristics of the printed circuits in real-time. Precision
parameters were compared to evaluate the supervised classification models.
Since decision tree methods could not increase the accuracy higher than 71%,
more advanced algorithms are performed on our dataset to improve the precision
of model. According to F-measure values, the K-NN model (k=10) and random
forest are the best methods to classify the conductivity of electrodes. The
highest accuracy of AdaBoost ensemble learning has resulted in the range of
10-15 trees (87%).

    

### [[2111.03987] V-MAO: Generative Modeling for Multi-Arm Manipulation of Articulated Objects](http://arxiv.org/abs/2111.03987)


  Manipulating articulated objects requires multiple robot arms in general. It
is challenging to enable multiple robot arms to collaboratively complete
manipulation tasks on articulated objects. In this paper, we present
$\textbf{V-MAO}$, a framework for learning multi-arm manipulation of
articulated objects. Our framework includes a variational generative model that
learns contact point distribution over object rigid parts for each robot arm.
The training signal is obtained from interaction with the simulation
environment which is enabled by planning and a novel formulation of
object-centric control for articulated objects. We deploy our framework in a
customized MuJoCo simulation environment and demonstrate that our framework
achieves a high success rate on six different objects and two different robots.
We also show that generative modeling can effectively learn the contact point
distribution on articulated objects.

    

### [[2111.03994] NarrationBot and InfoBot: A Hybrid System for Automated Video Description](http://arxiv.org/abs/2111.03994)


  Video accessibility is crucial for blind and low vision users for equitable
engagements in education, employment, and entertainment. Despite the
availability of professional and amateur services and tools, most
human-generated descriptions are expensive and time consuming. Moreover, the
rate of human-generated descriptions cannot match the speed of video
production. To overcome the increasing gaps in video accessibility, we
developed a hybrid system of two tools to 1) automatically generate
descriptions for videos and 2) provide answers or additional descriptions in
response to user queries on a video. Results from a mixed-methods study with 26
blind and low vision individuals show that our system significantly improved
user comprehension and enjoyment of selected videos when both tools were used
in tandem. In addition, participants reported no significant difference in
their ability to understand videos when presented with autogenerated
descriptions versus human-revised autogenerated descriptions. Our results
demonstrate user enthusiasm about the developed system and its promise for
providing customized access to videos. We discuss the limitations of the
current work and provide recommendations for the future development of
automated video description tools.

    

### [[2111.04003] Predictive Model for Gross Community Production Rate of Coral Reefs using Ensemble Learning Methodologies](http://arxiv.org/abs/2111.04003)


  Coral reefs play a vital role in maintaining the ecological balance of the
marine ecosystem. Various marine organisms depend on coral reefs for their
existence and their natural processes. Coral reefs provide the necessary
habitat for reproduction and growth for various exotic species of the marine
ecosystem. In this article, we discuss the most important parameters which
influence the lifecycle of coral and coral reefs such as ocean acidification,
deoxygenation and other physical parameters such as flow rate and surface area.
Ocean acidification depends on the amount of dissolved Carbon dioxide (CO2).
This is due to the release of H+ ions upon the reaction of the dissolved CO2
gases with the calcium carbonate compounds in the ocean. Deoxygenation is
another problem that leads to hypoxia which is characterized by a lesser amount
of dissolved oxygen in water than the required amount for the existence of
marine organisms. In this article, we highlight the importance of physical
parameters such as flow rate which influence gas exchange, heat dissipation,
bleaching sensitivity, nutrient supply, feeding, waste and sediment removal,
growth and reproduction. In this paper, we also bring out these important
parameters and propose an ensemble machine learning-based model for analyzing
these parameters and provide better rates that can help us to understand and
suitably improve the ocean composition which in turn can eminently improve the
sustainability of the marine ecosystem, mainly the coral reefs

    

### [[2111.04004] Quasi-potential theory for escape problem: Quantitative sharpness effect on SGD's escape from local minima](http://arxiv.org/abs/2111.04004)


  We develop a quantitative theory on an escape problem of a stochastic
gradient descent (SGD) algorithm and investigate the effect of sharpness of
loss surfaces on the escape. Deep learning has achieved tremendous success in
various domains, however, it has opened up various theoretical open questions.
One of the typical questions is why an SGD can find parameters that generalize
well over non-convex loss surfaces. An escape problem is an approach to tackle
this question, which investigates how efficiently an SGD escapes from local
minima. In this paper, we develop a quasi-potential theory for the escape
problem, by applying a theory of stochastic dynamical systems. We show that the
quasi-potential theory can handle both geometric properties of loss surfaces
and a covariance structure of gradient noise in a unified manner, while they
have been separately studied in previous works. Our theoretical results imply
that (i) the sharpness of loss surfaces contributes to the slow escape of an
SGD, and (ii) the SGD's noise structure cancels the effect and exponentially
accelerates the escape. We also conduct experiments to empirically validate our
theory using neural networks trained with real data.

    

### [[2111.04006] A Review of Location Encoding for GeoAI: Methods and Applications](http://arxiv.org/abs/2111.04006)


  A common need for artificial intelligence models in the broader geoscience is
to represent and encode various types of spatial data, such as points (e.g.,
points of interest), polylines (e.g., trajectories), polygons (e.g.,
administrative regions), graphs (e.g., transportation networks), or rasters
(e.g., remote sensing images), in a hidden embedding space so that they can be
readily incorporated into deep learning models. One fundamental step is to
encode a single point location into an embedding space, such that this
embedding is learning-friendly for downstream machine learning models such as
support vector machines and neural networks. We call this process location
encoding. However, there lacks a systematic review on the concept of location
encoding, its potential applications, and key challenges that need to be
addressed. This paper aims to fill this gap. We first provide a formal
definition of location encoding, and discuss the necessity of location encoding
for GeoAI research from a machine learning perspective. Next, we provide a
comprehensive survey and discussion about the current landscape of location
encoding research. We classify location encoding models into different
categories based on their inputs and encoding methods, and compare them based
on whether they are parametric, multi-scale, distance preserving, and direction
aware. We demonstrate that existing location encoding models can be unified
under a shared formulation framework. We also discuss the application of
location encoding for different types of spatial data. Finally, we point out
several challenges in location encoding research that need to be solved in the
future.

    

### [[2111.04012] A-PixelHop: A Green, Robust and Explainable Fake-Image Detector](http://arxiv.org/abs/2111.04012)


  A novel method for detecting CNN-generated images, called Attentive PixelHop
(or A-PixelHop), is proposed in this work. It has three advantages: 1) low
computational complexity and a small model size, 2) high detection performance
against a wide range of generative models, and 3) mathematical transparency.
A-PixelHop is designed under the assumption that it is difficult to synthesize
high-quality, high-frequency components in local regions. It contains four
building modules: 1) selecting edge/texture blocks that contain significant
high-frequency components, 2) applying multiple filter banks to them to obtain
rich sets of spatial-spectral responses as features, 3) feeding features to
multiple binary classifiers to obtain a set of soft decisions, 4) developing an
effective ensemble scheme to fuse the soft decisions into the final decision.
Experimental results show that A-PixelHop outperforms state-of-the-art methods
in detecting CycleGAN-generated images. Furthermore, it can generalize well to
unseen generative models and datasets.

    

### [[2111.04033] Positivity Validation Detection and Explainability via Zero Fraction Multi-Hypothesis Testing and Asymmetrically Pruned Decision Trees](http://arxiv.org/abs/2111.04033)


  Positivity is one of the three conditions for causal inference from
observational data. The standard way to validate positivity is to analyze the
distribution of propensity. However, to democratize the ability to do causal
inference by non-experts, it is required to design an algorithm to (i) test
positivity and (ii) explain where in the covariate space positivity is lacking.
The latter could be used to either suggest the limitation of further causal
analysis and/or encourage experimentation where positivity is violated. The
contribution of this paper is first present the problem of automatic positivity
analysis and secondly to propose an algorithm based on a two steps process. The
first step, models the propensity condition on the covariates and then analyze
the latter distribution using multiple hypothesis testing to create positivity
violation labels. The second step uses asymmetrically pruned decision trees for
explainability. The latter is further converted into readable text a non-expert
can understand. We demonstrate our method on a proprietary data-set of a large
software enterprise.

    

### [[2111.04067] High Performance Out-of-sample Embedding Techniques for Multidimensional Scaling](http://arxiv.org/abs/2111.04067)


  The recent rapid growth of the dimension of many datasets means that many
approaches to dimension reduction (DR) have gained significant attention.
High-performance DR algorithms are required to make data analysis feasible for
big and fast data sets. However, many traditional DR techniques are challenged
by truly large data sets. In particular multidimensional scaling (MDS) does not
scale well. MDS is a popular group of DR techniques because it can perform DR
on data where the only input is a dissimilarity function. However, common
approaches are at least quadratic in memory and computation and, hence,
prohibitive for large-scale data.
We propose an out-of-sample embedding (OSE) solution to extend the MDS
algorithm for large-scale data utilising the embedding of only a subset of the
given data. We present two OSE techniques: the first based on an optimisation
approach and the second based on a neural network model. With a minor trade-off
in the approximation, the out-of-sample techniques can process large-scale data
with reasonable computation and memory requirements. While both methods perform
well, the neural network model outperforms the optimisation approach of the OSE
solution in terms of efficiency. OSE has the dual benefit that it allows fast
DR on streaming datasets as well as static databases.

    

### [[2111.04068] Crowdsourcing with Meta-Workers: A New Way to Save the Budget](http://arxiv.org/abs/2111.04068)


  Due to the unreliability of Internet workers, it's difficult to complete a
crowdsourcing project satisfactorily, especially when the tasks are multiple
and the budget is limited. Recently, meta learning has brought new vitality to
few-shot learning, making it possible to obtain a classifier with a fair
performance using only a few training samples. Here we introduce the concept of
\emph{meta-worker}, a machine annotator trained by meta learning for types of
tasks (i.e., image classification) that are well-fit for AI. Unlike regular
crowd workers, meta-workers can be reliable, stable, and more importantly,
tireless and free. We first cluster unlabeled data and ask crowd workers to
repeatedly annotate the instances nearby the cluster centers; we then leverage
the annotated data and meta-training datasets to build a cluster of
meta-workers using different meta learning algorithms. Subsequently,
meta-workers are asked to annotate the remaining crowdsourced tasks. The
Jensen-Shannon divergence is used to measure the disagreement among the
annotations provided by the meta-workers, which determines whether or not crowd
workers should be invited for further annotation of the same task. Finally, we
model meta-workers' preferences and compute the consensus annotation by
weighted majority voting. Our empirical study confirms that, by combining
machine and human intelligence, we can accomplish a crowdsourcing project with
a lower budget than state-of-the-art task assignment methods, while achieving a
superior or comparable quality.

    

### [[2111.04071] DVS: Deep Visibility Series and its Application in Construction Cost Index Forecasting](http://arxiv.org/abs/2111.04071)


  Time series forecasting has always been a hot spot in scientific research.
With the development of artificial intelligence, new time series forecasting
methods have obtained better forecasting effects and forecasting performance
through bionic research and improvements to the past methods. Visibility Graph
(VG) algorithm is often used for time series prediction in previous research,
but the prediction effect is not as good as deep learning prediction methods
such as Artificial Neural Network (ANN), Convolutional Neural Network (CNN) and
Long Short-Term Memory Network (LSTM) prediction. The VG algorithm contains a
wealth of network information, but previous studies did not effectively use the
network information to make predictions, resulting in relatively large
prediction errors. In order to solve this problem, this paper proposes the Deep
Visibility Series (DVS) module through the bionic design of VG and the
expansion of the past research, which is the first time to combine VG with
bionic design and deep network. By applying the bionic design of biological
vision to VG, the time series of DVS has obtained superior forecast accuracy,
which has made a contribution to time series forecasting. At the same time,
this paper applies the DVS forecasting method to the construction cost index
forecast, which has practical significance.

    

### [[2111.04073] Open-Set Crowdsourcing using Multiple-Source Transfer Learning](http://arxiv.org/abs/2111.04073)


  We raise and define a new crowdsourcing scenario, open set crowdsourcing,
where we only know the general theme of an unfamiliar crowdsourcing project,
and we don't know its label space, that is, the set of possible labels. This is
still a task annotating problem, but the unfamiliarity with the tasks and the
label space hampers the modelling of the task and of workers, and also the
truth inference. We propose an intuitive solution, OSCrowd. First, OSCrowd
integrates crowd theme related datasets into a large source domain to
facilitate partial transfer learning to approximate the label space inference
of these tasks. Next, it assigns weights to each source domain based on
category correlation. After this, it uses multiple-source open set transfer
learning to model crowd tasks and assign possible annotations. The label space
and annotations given by transfer learning will be used to guide and
standardize crowd workers' annotations. We validate OSCrowd in an online
scenario, and prove that OSCrowd solves the open set crowdsourcing problem,
works better than related crowdsourcing solutions.

    

### [[2111.04080] Cross-modal Zero-shot Hashing by Label Attributes Embedding](http://arxiv.org/abs/2111.04080)


  Cross-modal hashing (CMH) is one of the most promising methods in cross-modal
approximate nearest neighbor search. Most CMH solutions ideally assume the
labels of training and testing set are identical. However, the assumption is
often violated, causing a zero-shot CMH problem. Recent efforts to address this
issue focus on transferring knowledge from the seen classes to the unseen ones
using label attributes. However, the attributes are isolated from the features
of multi-modal data. To reduce the information gap, we introduce an approach
called LAEH (Label Attributes Embedding for zero-shot cross-modal Hashing).
LAEH first gets the initial semantic attribute vectors of labels by word2vec
model and then uses a transformation network to transform them into a common
subspace. Next, it leverages the hash vectors and the feature similarity matrix
to guide the feature extraction network of different modalities. At the same
time, LAEH uses the attribute similarity as the supplement of label similarity
to rectify the label embedding and common subspace. Experiments show that LAEH
outperforms related representative zero-shot and cross-modal hashing methods.

    

### [[2111.04086] Meta Cross-Modal Hashing on Long-Tailed Data](http://arxiv.org/abs/2111.04086)


  Due to the advantage of reducing storage while speeding up query time on big
heterogeneous data, cross-modal hashing has been extensively studied for
approximate nearest neighbor search of multi-modal data. Most hashing methods
assume that training data is class-balanced.However, in practice, real world
data often have a long-tailed distribution. In this paper, we introduce a
meta-learning based cross-modal hashing method (MetaCMH) to handle long-tailed
data. Due to the lack of training samples in the tail classes, MetaCMH first
learns direct features from data in different modalities, and then introduces
an associative memory module to learn the memory features of samples of the
tail classes. It then combines the direct and memory features to obtain meta
features for each sample. For samples of the head classes of the long tail
distribution, the weight of the direct features is larger, because there are
enough training data to learn them well; while for rare classes, the weight of
the memory features is larger. Finally, MetaCMH uses a likelihood loss function
to preserve the similarity in different modalities and learns hash functions in
an end-to-end fashion. Experiments on long-tailed datasets show that MetaCMH
performs significantly better than state-of-the-art methods, especially on the
tail classes.

    

### [[2111.04089] Sampling from Log-Concave Distributions with Infinity-Distance Guarantees and Applications to Differentially Private Optimization](http://arxiv.org/abs/2111.04089)


  For a $d$-dimensional log-concave distribution $\pi(\theta)\propto
e^{-f(\theta)}$ on a polytope $K$, we consider the problem of outputting
samples from a distribution $\nu$ which is $O(\varepsilon)$-close in
infinity-distance $\sup_{\theta\in K}|\log\frac{\nu(\theta)}{\pi(\theta)}|$ to
$\pi$. Such samplers with infinity-distance guarantees are specifically desired
for differentially private optimization as traditional sampling algorithms
which come with total-variation distance or KL divergence bounds are
insufficient to guarantee differential privacy. Our main result is an algorithm
that outputs a point from a distribution $O(\varepsilon)$-close to $\pi$ in
infinity-distance and requires
$O((md+dL^2R^2)\times(LR+d\log(\frac{Rd+LRd}{\varepsilon r}))\times
md^{\omega-1})$ arithmetic operations, where $f$ is $L$-Lipschitz, $K$ is
defined by $m$ inequalities, is contained in a ball of radius $R$ and contains
a ball of smaller radius $r$, and $\omega$ is the matrix-multiplication
constant. In particular this runtime is logarithmic in $\frac{1}{\varepsilon}$
and significantly improves on prior works. Technically, we depart from the
prior works that construct Markov chains on a
$\frac{1}{\varepsilon^2}$-discretization of $K$ to achieve a sample with
$O(\varepsilon)$ infinity-distance error, and present a method to convert
continuous samples from $K$ with total-variation bounds to samples with
infinity bounds. To achieve improved dependence on $d$, we present a
"soft-threshold" version of the Dikin walk which may be of independent
interest. Plugging our algorithm into the framework of the exponential
mechanism yields similar improvements in the running time of $\varepsilon$-pure
differentially private algorithms for optimization problems such as empirical
risk minimization of Lipschitz-convex functions and low-rank approximation,
while still achieving the tightest known utility bounds.

    

### [[2111.04090] Learn-Morph-Infer: a new way of solving the inverse problem for brain tumor modeling](http://arxiv.org/abs/2111.04090)


  Current treatment planning of patients diagnosed with brain tumor could
significantly benefit by accessing the spatial distribution of tumor cell
concentration. Existing diagnostic modalities, such as magnetic-resonance
imaging (MRI), contrast sufficiently well areas of high cell density. However,
they do not portray areas of low concentration, which can often serve as a
source for the secondary appearance of the tumor after treatment. Numerical
simulations of tumor growth could complement imaging information by providing
estimates of full spatial distributions of tumor cells. Over recent years a
corpus of literature on medical image-based tumor modeling was published. It
includes different mathematical formalisms describing the forward tumor growth
model. Alongside, various parametric inference schemes were developed to
perform an efficient tumor model personalization, i.e. solving the inverse
problem. However, the unifying drawback of all existing approaches is the time
complexity of the model personalization that prohibits a potential integration
of the modeling into clinical settings. In this work, we introduce a
methodology for inferring patient-specific spatial distribution of brain tumor
from T1Gd and FLAIR MRI medical scans. Coined as \textit{Learn-Morph-Infer} the
method achieves real-time performance in the order of minutes on widely
available hardware and the compute time is stable across tumor models of
different complexity, such as reaction-diffusion and
reaction-advection-diffusion models. We believe the proposed inverse solution
approach not only bridges the way for clinical translation of brain tumor
personalization but can also be adopted to other scientific and engineering
domains.

    

### [[2111.04094] Acquisition-invariant brain MRI segmentation with informative uncertainties](http://arxiv.org/abs/2111.04094)


  Combining multi-site data can strengthen and uncover trends, but is a task
that is marred by the influence of site-specific covariates that can bias the
data and therefore any downstream analyses. Post-hoc multi-site correction
methods exist but have strong assumptions that often do not hold in real-world
scenarios. Algorithms should be designed in a way that can account for
site-specific effects, such as those that arise from sequence parameter
choices, and in instances where generalisation fails, should be able to
identify such a failure by means of explicit uncertainty modelling. This body
of work showcases such an algorithm, that can become robust to the physics of
acquisition in the context of segmentation tasks, while simultaneously
modelling uncertainty. We demonstrate that our method not only generalises to
complete holdout datasets, preserving segmentation quality, but does so while
also accounting for site-specific sequence choices, which also allows it to
perform as a harmonisation tool.

    

### [[2111.04095] Iterative Causal Discovery in the Possible Presence of Latent Confounders and Selection Bias](http://arxiv.org/abs/2111.04095)


  We present a sound and complete algorithm, called iterative causal discovery
(ICD), for recovering causal graphs in the presence of latent confounders and
selection bias. ICD relies on the causal Markov and faithfulness assumptions
and recovers the equivalence class of the underlying causal graph. It starts
with a complete graph, and consists of a single iterative stage that gradually
refines this graph by identifying conditional independence (CI) between
connected nodes. Independence and causal relations entailed after any iteration
are correct, rendering ICD anytime. Essentially, we tie the size of the CI
conditioning set to its distance on the graph from the tested nodes, and
increase this value in the successive iteration. Thus, each iteration refines a
graph that was recovered by previous iterations having smaller conditioning
sets -- a higher statistical power -- which contributes to stability. We
demonstrate empirically that ICD requires significantly fewer CI tests and
learns more accurate causal graphs compared to FCI, FCI+, and RFCI algorithms.

    

### [[2111.04099] Developing neural machine translation models for Hungarian-English](http://arxiv.org/abs/2111.04099)


  I train models for the task of neural machine translation for
English-Hungarian and Hungarian-English, using the Hunglish2 corpus. The main
contribution of this work is evaluating different data augmentation methods
during the training of NMT models. I propose 5 different augmentation methods
that are structure-aware, meaning that instead of randomly selecting words for
blanking or replacement, the dependency tree of sentences is used as a basis
for augmentation. I start my thesis with a detailed literature review on neural
networks, sequential modeling, neural machine translation, dependency parsing
and data augmentation. After a detailed exploratory data analysis and
preprocessing of the Hunglish2 corpus, I perform experiments with the proposed
data augmentation techniques. The best model for Hungarian-English achieves a
BLEU score of 33.9, while the best model for English-Hungarian achieves a BLEU
score of 28.6.

    

### [[2111.04104] Uncertainty Calibration for Ensemble-Based Debiasing Methods](http://arxiv.org/abs/2111.04104)


  Ensemble-based debiasing methods have been shown effective in mitigating the
reliance of classifiers on specific dataset bias, by exploiting the output of a
bias-only model to adjust the learning target. In this paper, we focus on the
bias-only model in these ensemble-based methods, which plays an important role
but has not gained much attention in the existing literature. Theoretically, we
prove that the debiasing performance can be damaged by inaccurate uncertainty
estimations of the bias-only model. Empirically, we show that existing
bias-only models fall short in producing accurate uncertainty estimations.
Motivated by these findings, we propose to conduct calibration on the bias-only
model, thus achieving a three-stage ensemble-based debiasing framework,
including bias modeling, model calibrating, and debiasing. Experimental results
on NLI and fact verification tasks show that our proposed three-stage debiasing
framework consistently outperforms the traditional two-stage one in
out-of-distribution accuracy.

    

### [[2111.04105] DQRE-SCnet: A novel hybrid approach for selecting users in Federated Learning with Deep-Q-Reinforcement Learning based on Spectral Clustering](http://arxiv.org/abs/2111.04105)


  Machine learning models based on sensitive data in the real-world promise
advances in areas ranging from medical screening to disease outbreaks,
agriculture, industry, defense science, and more. In many applications,
learning participant communication rounds benefit from collecting their own
private data sets, teaching detailed machine learning models on the real data,
and sharing the benefits of using these models. Due to existing privacy and
security concerns, most people avoid sensitive data sharing for training.
Without each user demonstrating their local data to a central server, Federated
Learning allows various parties to train a machine learning algorithm on their
shared data jointly. This method of collective privacy learning results in the
expense of important communication during training. Most large-scale
machine-learning applications require decentralized learning based on data sets
generated on various devices and places. Such datasets represent an essential
obstacle to decentralized learning, as their diverse contexts contribute to
significant differences in the delivery of data across devices and locations.
Researchers have proposed several ways to achieve data privacy in Federated
Learning systems. However, there are still challenges with homogeneous local
data. This research approach is to select nodes (users) to share their data in
Federated Learning for independent data-based equilibrium to improve accuracy,
reduce training time, and increase convergence. Therefore, this research
presents a combined Deep-QReinforcement Learning Ensemble based on Spectral
Clustering called DQRE-SCnet to choose a subset of devices in each
communication round. Based on the results, it has been displayed that it is
possible to decrease the number of communication rounds needed in Federated
Learning.

    

### [[2111.04107] Structure-aware generation of drug-like molecules](http://arxiv.org/abs/2111.04107)


  Structure-based drug design involves finding ligand molecules that exhibit
structural and chemical complementarity to protein pockets. Deep generative
methods have shown promise in proposing novel molecules from scratch (de-novo
design), avoiding exhaustive virtual screening of chemical space. Most
generative de-novo models fail to incorporate detailed ligand-protein
interactions and 3D pocket structures. We propose a novel supervised model that
generates molecular graphs jointly with 3D pose in a discretised molecular
space. Molecules are built atom-by-atom inside pockets, guided by structural
information from crystallographic data. We evaluate our model using a docking
benchmark and find that guided generation improves predicted binding affinities
by 8% and drug-likeness scores by 10% over the baseline. Furthermore, our model
proposes molecules with binding scores exceeding some known ligands, which
could be useful in future wet-lab studies.

    

### [[2111.04112] MetaMIML: Meta Multi-Instance Multi-Label Learning](http://arxiv.org/abs/2111.04112)


  Multi-Instance Multi-Label learning (MIML) models complex objects (bags),
each of which is associated with a set of interrelated labels and composed with
a set of instances. Current MIML solutions still focus on a single-type of
objects and assumes an IID distribution of training data. But these objects are
linked with objects of other types, %(i.e., pictures in Facebook link with
various users), which also encode the semantics of target objects. In addition,
they generally need abundant labeled data for training. To effectively mine
interdependent MIML objects of different types, we propose a network embedding
and meta learning based approach (MetaMIML). MetaMIML introduces the context
learner with network embedding to capture semantic information of objects of
different types, and the task learner to extract the meta knowledge for fast
adapting to new tasks. In this way, MetaMIML can naturally deal with MIML
objects at data level improving, but also exploit the power of meta-learning at
the model enhancing. Experiments on benchmark datasets demonstrate that
MetaMIML achieves a significantly better performance than state-of-the-art
algorithms.

    

### [[2111.04123] NeurInt : Learning to Interpolate through Neural ODEs](http://arxiv.org/abs/2111.04123)


  A wide range of applications require learning image generation models whose
latent space effectively captures the high-level factors of variation present
in the data distribution. The extent to which a model represents such
variations through its latent space can be judged by its ability to interpolate
between images smoothly. However, most generative models mapping a fixed prior
to the generated images lead to interpolation trajectories lacking smoothness
and containing images of reduced quality. In this work, we propose a novel
generative model that learns a flexible non-parametric prior over interpolation
trajectories, conditioned on a pair of source and target images. Instead of
relying on deterministic interpolation methods (such as linear or spherical
interpolation in latent space), we devise a framework that learns a
distribution of trajectories between two given images using Latent Second-Order
Neural Ordinary Differential Equations. Through a hybrid combination of
reconstruction and adversarial losses, the generator is trained to map the
sampled points from these trajectories to sequences of realistic images that
smoothly transition from the source to the target image. Through comprehensive
qualitative and quantitative experiments, we demonstrate our approach's
effectiveness in generating images of improved quality as well as its ability
to learn a diverse distribution over smooth interpolation trajectories for any
pair of real source and target images.

    

### [[2111.04130] NLP From Scratch Without Large-Scale Pretraining: A Simple and Efficient Framework](http://arxiv.org/abs/2111.04130)


  Pretrained language models have become the standard approach for many NLP
tasks due to strong performance, but they are very expensive to train. We
propose a simple and efficient learning framework, TLM, that does not rely on
large-scale pretraining. Given some labeled task data and a large general
corpus, TLM uses task data as queries to retrieve a tiny subset of the general
corpus and jointly optimizes the task objective and the language modeling
objective from scratch. On eight classification datasets in four domains, TLM
achieves results better than or similar to pretrained language models (e.g.,
RoBERTa-Large) while reducing the training FLOPs by two orders of magnitude.
With high accuracy and efficiency, we hope TLM will contribute to democratizing
NLP and expediting its development.

    

### [[2111.04131] Plumber: Diagnosing and Removing Performance Bottlenecks in Machine Learning Data Pipelines](http://arxiv.org/abs/2111.04131)


  Input pipelines, which ingest and transform input data, are an essential part
of training Machine Learning (ML) models. However, it is challenging to
implement efficient input pipelines, as it requires reasoning about
parallelism, asynchrony, and variability in fine-grained profiling information.
Our analysis of over 2 million ML jobs in Google datacenters reveals that a
significant fraction of model training jobs could benefit from faster input
data pipelines. At the same time, our analysis reveals that most jobs do not
saturate host hardware, pointing in the direction of software-based
bottlenecks. Motivated by these findings, we propose Plumber, a tool for
finding bottlenecks in ML input pipelines. Plumber uses an extensible and
interprettable operational analysis analytical model to automatically tune
parallelism, prefetching, and caching under host resource constraints. Across
five representative ML pipelines, Plumber obtains speedups of up to 46x for
misconfigured pipelines. By automating caching, Plumber obtains end-to-end
speedups of over 40% compared to state-of-the-art tuners.

    

### [[2111.04138] Look at the Variance! Efficient Black-box Explanations with Sobol-based Sensitivity Analysis](http://arxiv.org/abs/2111.04138)


  We describe a novel attribution method which is grounded in Sensitivity
Analysis and uses Sobol indices. Beyond modeling the individual contributions
of image regions, Sobol indices provide an efficient way to capture
higher-order interactions between image regions and their contributions to a
neural network's prediction through the lens of variance. We describe an
approach that makes the computation of these indices efficient for
high-dimensional problems by using perturbation masks coupled with efficient
estimators to handle the high dimensionality of images. Importantly, we show
that the proposed method leads to favorable scores on standard benchmarks for
vision (and language models) while drastically reducing the computing time
compared to other black-box methods -- even surpassing the accuracy of
state-of-the-art white-box methods which require access to internal
representations. Our code is freely available:
this https URL


### [[2111.04146] Optimization of the Model Predictive Control Meta-Parameters Through Reinforcement Learning](http://arxiv.org/abs/2111.04146)


  Model predictive control (MPC) is increasingly being considered for control
of fast systems and embedded applications. However, the MPC has some
significant challenges for such systems. Its high computational complexity
results in high power consumption from the control algorithm, which could
account for a significant share of the energy resources in battery-powered
embedded systems. The MPC parameters must be tuned, which is largely a
trial-and-error process that affects the control performance, the robustness
and the computational complexity of the controller to a high degree. In this
paper, we propose a novel framework in which any parameter of the control
algorithm can be jointly tuned using reinforcement learning(RL), with the goal
of simultaneously optimizing the control performance and the power usage of the
control algorithm. We propose the novel idea of optimizing the meta-parameters
of MPCwith RL, i.e. parameters affecting the structure of the MPCproblem as
opposed to the solution to a given problem. Our control algorithm is based on
an event-triggered MPC where we learn when the MPC should be re-computed, and a
dual mode MPC and linear state feedback control law applied in between MPC
computations. We formulate a novel mixture-distribution policy and show that
with joint optimization we achieve improvements that do not present themselves
when optimizing the same parameters in isolation. We demonstrate our framework
on the inverted pendulum control task, reducing the total computation time of
the control system by 36% while also improving the control performance by 18.4%
over the best-performing MPC baseline.

    

### [[2111.04153] Data-Efficient Deep Reinforcement Learning for Attitude Control of Fixed-Wing UAVs: Field Experiments](http://arxiv.org/abs/2111.04153)


  Attitude control of fixed-wing unmanned aerial vehicles (UAVs)is a difficult
control problem in part due to uncertain nonlinear dynamics, actuator
constraints, and coupled longitudinal and lateral motions. Current
state-of-the-art autopilots are based on linear control and are thus limited in
their effectiveness and performance. Deep reinforcement learning (DRL) is a
machine learning method to automatically discover optimal control laws through
interaction with the controlled system, that can handle complex nonlinear
dynamics. We show in this paper that DRL can successfully learn to perform
attitude control of a fixed-wing UAV operating directly on the original
nonlinear dynamics, requiring as little as three minutes of flight data. We
initially train our model in a simulation environment and then deploy the
learned controller on the UAV in flight tests, demonstrating comparable
performance to the state-of-the-art ArduPlaneproportional-integral-derivative
(PID) attitude controller with no further online learning required. To better
understand the operation of the learned controller we present an analysis of
its behaviour, including a comparison to the existing well-tuned PID
controller.

    

### [[2111.04178] Teamwork makes von Neumann work: Min-Max Optimization in Two-Team Zero-Sum Games](http://arxiv.org/abs/2111.04178)


  Motivated by recent advances in both theoretical and applied aspects of
multiplayer games, spanning from e-sports to multi-agent generative adversarial
networks, we focus on min-max optimization in team zero-sum games. In this
class of games, players are split in two teams with payoffs equal within the
same team and of opposite sign across the opponent team. Unlike the textbook
two-player zero-sum games, finding a Nash equilibrium in our class can be shown
to be CLS-hard, i.e., it is unlikely to have a polynomial time algorithm for
computing Nash equilibria. Moreover in this generalized framework, we establish
that even asymptotic last iterate or time average convergence to a Nash
Equilibrium is not possible using Gradient Descent Ascent (GDA), its optimistic
variant and extra gradient. Specifically, we present a family of team games
whose induced utility is \emph{non} multi-linear with \emph{non} attractive
\emph{per-se} mixed Nash Equilibria, as strict saddle points of the underlying
optimization landscape. Leveraging techniques from control theory, we
complement these negative results by designing a modified GDA that converges
locally to Nash equilibria. Finally, we discuss connections of our framework
with AI architectures with team competition structure like multi-agent
generative adversarial networks.

    

### [[2111.04185] CoughTrigger: Earbuds IMU Based Cough Detection Activator Using An Energy-efficient Sensitivity-prioritized Time Series Classifier](http://arxiv.org/abs/2111.04185)


  Persistent coughs are a major symptom of respiratory-related diseases.
Increasing research attention has been paid to detecting coughs using
wearables, especially during the COVID-19 pandemic. Among all types of sensors
utilized, microphone is most widely used to detect coughs. However, the intense
power consumption needed to process audio signals hinders continuous
audio-based cough detection on battery-limited commercial wearable products,
such as earbuds. We present CoughTrigger, which utilizes a lower-power sensor,
an inertial measurement unit (IMU), in earbuds as a cough detection activator
to trigger a higher-power sensor for audio processing and classification. It is
able to run all-the-time as a standby service with minimal battery consumption
and trigger the audio-based cough detection when a candidate cough is detected
from IMU. Besides, the use of IMU brings the benefit of improved specificity of
cough detection. Experiments are conducted on 45 subjects and our IMU-based
model achieved 0.77 AUC score under leave one subject out evaluation. We also
validated its effectiveness on free-living data and through on-device
implementation.

    

### [[2111.04190] VizAI : Selecting Accurate Visualizations of Numerical Data](http://arxiv.org/abs/2111.04190)


  A good data visualization is not only a distortion-free graphical
representation of data but also a way to reveal underlying statistical
properties of the data. Despite its common use across various stages of data
analysis, selecting a good visualization often is a manual process involving
many iterations. Recently there has been interest in reducing this effort by
developing models that can recommend visualizations, but they are of limited
use since they require large training samples (data and visualization pairs)
and focus primarily on the design aspects rather than on assessing the
effectiveness of the selected visualization.
In this paper, we present VizAI, a generative-discriminative framework that
first generates various statistical properties of the data from a number of
alternative visualizations of the data. It is linked to a discriminative model
that selects the visualization that best matches the true statistics of the
data being visualized. VizAI can easily be trained with minimal supervision and
adapts to settings with varying degrees of supervision easily. Using
crowd-sourced judgements and a large repository of publicly available
visualizations, we demonstrate that VizAI outperforms the state of the art
methods that learn to recommend visualizations.

    

### [[2111.04207] Uncertainty Quantification in Neural Differential Equations](http://arxiv.org/abs/2111.04207)


  Uncertainty quantification (UQ) helps to make trustworthy predictions based
on collected observations and uncertain domain knowledge. With increased usage
of deep learning in various applications, the need for efficient UQ methods
that can make deep models more reliable has increased as well. Among
applications that can benefit from effective handling of uncertainty are the
deep learning based differential equation (DE) solvers. We adapt several
state-of-the-art UQ methods to get the predictive uncertainty for DE solutions
and show the results on four different DE types.

    

### [[2111.04208] AI challenges for predicting the impact of mutations on protein stability](http://arxiv.org/abs/2111.04208)


  Stability is a key ingredient of protein fitness and its modification through
targeted mutations has applications in various fields such as protein
engineering, drug design and deleterious variant interpretation. Many studies
have been devoted over the past decades to building new, more effective methods
for predicting the impact of mutations on protein stability, based on the
latest developments in artificial intelligence (AI). We discuss their features,
algorithms, computational efficiency, and accuracy estimated on an independent
test set. We focus on a critical analysis of their limitations, the recurrent
biases towards the training set, their generalizability and interpretability.
We found that the accuracy of the predictors has stagnated at around 1 kcal/mol
for over 15 years. We conclude by discussing the challenges that need to be
addressed to reach improved performance.

    

### [[2111.04225] Representation Learning via Quantum Neural Tangent Kernels](http://arxiv.org/abs/2111.04225)


  Variational quantum circuits are used in quantum machine learning and
variational quantum simulation tasks. Designing good variational circuits or
predicting how well they perform for given learning or optimization tasks is
still unclear. Here we discuss these problems, analyzing variational quantum
circuits using the theory of neural tangent kernels. We define quantum neural
tangent kernels, and derive dynamical equations for their associated loss
function in optimization and learning tasks. We analytically solve the dynamics
in the frozen limit, or lazy training regime, where variational angles change
slowly and a linear perturbation is good enough. We extend the analysis to a
dynamical setting, including quadratic corrections in the variational angles.
We then consider hybrid quantum-classical architecture and define a large width
limit for hybrid kernels, showing that a hybrid quantum-classical neural
network can be approximately Gaussian. The results presented here show limits
for which analytical understandings of the training dynamics for variational
quantum circuits, used for quantum machine learning and optimization problems,
are possible. These analytical results are supported by numerical simulations
of quantum machine learning experiments.

    

### [[2111.04239] Learning to Rectify for Robust Learning with Noisy Labels](http://arxiv.org/abs/2111.04239)


  Label noise significantly degrades the generalization ability of deep models
in applications. Effective strategies and approaches, \textit{e.g.}
re-weighting, or loss correction, are designed to alleviate the negative impact
of label noise when training a neural network. Those existing works usually
rely on the pre-specified architecture and manually tuning the additional
hyper-parameters. In this paper, we propose warped probabilistic inference
(WarPI) to achieve adaptively rectifying the training procedure for the
classification network within the meta-learning scenario. In contrast to the
deterministic models, WarPI is formulated as a hierarchical probabilistic model
by learning an amortization meta-network, which can resolve sample ambiguity
and be therefore more robust to serious label noise. Unlike the existing
approximated weighting function of directly generating weight values from
losses, our meta-network is learned to estimate a rectifying vector from the
input of the logits and labels, which has the capability of leveraging
sufficient information lying in them. This provides an effective way to rectify
the learning procedure for the classification network, demonstrating a
significant improvement of the generalization ability. Besides, modeling the
rectifying vector as a latent variable and learning the meta-network can be
seamlessly integrated into the SGD optimization of the classification network.
We evaluate WarPI on four benchmarks of robust learning with noisy labels and
achieve the new state-of-the-art under variant noise types. Extensive study and
analysis also demonstrate the effectiveness of our model.

    

### [[2111.04253] A Novel Data Pre-processing Technique: Making Data Mining Robust to Different Units and Scales of Measurement](http://arxiv.org/abs/2111.04253)


  Many existing data mining algorithms use feature values directly in their
model, making them sensitive to units/scales used to measure/represent data.
Pre-processing of data based on rank transformation has been suggested as a
potential solution to overcome this issue. However, the resulting data after
pre-processing with rank transformation is uniformly distributed, which may not
be very useful in many data mining applications. In this paper, we present a
better and effective alternative based on ranks over multiple sub-samples of
data. We call the proposed pre-processing technique as ARES | Average Rank over
an Ensemble of Sub-samples. Our empirical results of widely used data mining
algorithms for classification and anomaly detection in a wide range of data
sets suggest that ARES results in more consistent task specific? outcome across
various algorithms and data sets. In addition to this, it results in better or
competitive outcome most of the time compared to the most widely used min-max
normalisation and the traditional rank transformation.

    

### [[2111.04260] Personalized Benchmarking with the Ludwig Benchmarking Toolkit](http://arxiv.org/abs/2111.04260)


  The rapid proliferation of machine learning models across domains and
deployment settings has given rise to various communities (e.g. industry
practitioners) which seek to benchmark models across tasks and objectives of
personal value. Unfortunately, these users cannot use standard benchmark
results to perform such value-driven comparisons as traditional benchmarks
evaluate models on a single objective (e.g. average accuracy) and fail to
facilitate a standardized training framework that controls for confounding
variables (e.g. computational budget), making fair comparisons difficult. To
address these challenges, we introduce the open-source Ludwig Benchmarking
Toolkit (LBT), a personalized benchmarking toolkit for running end-to-end
benchmark studies (from hyperparameter optimization to evaluation) across an
easily extensible set of tasks, deep learning models, datasets and evaluation
metrics. LBT provides a configurable interface for controlling training and
customizing evaluation, a standardized training framework for eliminating
confounding variables, and support for multi-objective evaluation. We
demonstrate how LBT can be used to create personalized benchmark studies with a
large-scale comparative analysis for text classification across 7 models and 9
datasets. We explore the trade-offs between inference latency and performance,
relationships between dataset attributes and performance, and the effects of
pretraining on convergence and robustness, showing how LBT can be used to
satisfy various benchmarking objectives.

    

### [[2111.04263] Federated Learning Based on Dynamic Regularization](http://arxiv.org/abs/2111.04263)


  We propose a novel federated learning method for distributively training
neural network models, where the server orchestrates cooperation between a
subset of randomly chosen devices in each round. We view Federated Learning
problem primarily from a communication perspective and allow more device level
computations to save transmission costs. We point out a fundamental dilemma, in
that the minima of the local-device level empirical loss are inconsistent with
those of the global empirical loss. Different from recent prior works, that
either attempt inexact minimization or utilize devices for parallelizing
gradient computation, we propose a dynamic regularizer for each device at each
round, so that in the limit the global and device solutions are aligned. We
demonstrate both through empirical results on real and synthetic data as well
as analytical results that our scheme leads to efficient training, in both
convex and non-convex settings, while being fully agnostic to device
heterogeneity and robust to large number of devices, partial participation and
unbalanced data.

    

### [[2111.04271] Group-Aware Threshold Adaptation for Fair Classification](http://arxiv.org/abs/2111.04271)


  The fairness in machine learning is getting increasing attention, as its
applications in different fields continue to expand and diversify. To mitigate
the discriminated model behaviors between different demographic groups, we
introduce a novel post-processing method to optimize over multiple fairness
constraints through group-aware threshold adaptation. We propose to learn
adaptive classification thresholds for each demographic group by optimizing the
confusion matrix estimated from the probability distribution of a
classification model output. As we only need an estimated probability
distribution of model output instead of the classification model structure, our
post-processing model can be applied to a wide range of classification models
and improve fairness in a model-agnostic manner and ensure privacy. This even
allows us to post-process existing fairness methods to further improve the
trade-off between accuracy and fairness. Moreover, our model has low
computational cost. We provide rigorous theoretical analysis on the convergence
of our optimization algorithm and the trade-off between accuracy and fairness
of our method. Our method theoretically enables a better upper bound in near
optimality than existing method under same condition. Experimental results
demonstrate that our method outperforms state-of-the-art methods and obtains
the result that is closest to the theoretical accuracy-fairness trade-off
boundary.

    

### [[2111.04272] Identifying Best Fair Intervention](http://arxiv.org/abs/2111.04272)


  We study the problem of best arm identification with a fairness constraint in
a given causal model. The goal is to find a soft intervention on a given node
to maximize the outcome while meeting a fairness constraint by counterfactual
estimation with only partial knowledge of the causal model. The problem is
motivated by ensuring fairness on an online marketplace. We provide theoretical
guarantees on the probability of error and empirically examine the
effectiveness of our algorithm with a two-stage baseline.

    

### [[2111.04273] Mimic: An adaptive algorithm for multivariate time series classification](http://arxiv.org/abs/2111.04273)


  Time series data are valuable but are often inscrutable. Gaining trust in
time series classifiers for finance, healthcare, and other critical
applications may rely on creating interpretable models. Researchers have
previously been forced to decide between interpretable methods that lack
predictive power and deep learning methods that lack transparency. In this
paper, we propose a novel Mimic algorithm that retains the predictive accuracy
of the strongest classifiers while introducing interpretability. Mimic mirrors
the learning method of an existing multivariate time series classifier while
simultaneously producing a visual representation that enhances user
understanding of the learned model. Experiments on 26 time series datasets
support Mimic's ability to imitate a variety of time series classifiers
visually and accurately.

    

### [[2111.04276] Deep Marching Tetrahedra: a Hybrid Representation for High-Resolution 3D Shape Synthesis](http://arxiv.org/abs/2111.04276)


  We introduce DMTet, a deep 3D conditional generative model that can
synthesize high-resolution 3D shapes using simple user guides such as coarse
voxels. It marries the merits of implicit and explicit 3D representations by
leveraging a novel hybrid 3D representation. Compared to the current implicit
approaches, which are trained to regress the signed distance values, DMTet
directly optimizes for the reconstructed surface, which enables us to
synthesize finer geometric details with fewer artifacts. Unlike deep 3D
generative models that directly generate explicit representations such as
meshes, our model can synthesize shapes with arbitrary topology. The core of
DMTet includes a deformable tetrahedral grid that encodes a discretized signed
distance function and a differentiable marching tetrahedra layer that converts
the implicit signed distance representation to the explicit surface mesh
representation. This combination allows joint optimization of the surface
geometry and topology as well as generation of the hierarchy of subdivisions
using reconstruction and adversarial losses defined explicitly on the surface
mesh. Our approach significantly outperforms existing work on conditional shape
synthesis from coarse voxel inputs, trained on a dataset of complex 3D animal
shapes. Project page: this https URL.

    

### [[2111.04279] Batch Reinforcement Learning from Crowds](http://arxiv.org/abs/2111.04279)


  A shortcoming of batch reinforcement learning is its requirement for rewards
in data, thus not applicable to tasks without reward functions. Existing
settings for lack of reward, such as behavioral cloning, rely on optimal
demonstrations collected from humans. Unfortunately, extensive expertise is
required for ensuring optimality, which hinder the acquisition of large-scale
data for complex tasks. This paper addresses the lack of reward in a batch
reinforcement learning setting by learning a reward function from preferences.
Generating preferences only requires a basic understanding of a task. Being a
mental process, generating preferences is faster than performing
demonstrations. So preferences can be collected at scale from non-expert humans
using crowdsourcing. This paper tackles a critical challenge that emerged when
collecting data from non-expert humans: the noise in preferences. A novel
probabilistic model is proposed for modelling the reliability of labels, which
utilizes labels collaboratively. Moreover, the proposed model smooths the
estimation with a learned reward function. Evaluation on Atari datasets
demonstrates the effectiveness of the proposed model, followed by an ablation
study to analyze the relative importance of the proposed ideas.

    

### [[2111.04286] Deep Unsupervised Active Learning on Learnable Graphs](http://arxiv.org/abs/2111.04286)


  Recently deep learning has been successfully applied to unsupervised active
learning. However, current method attempts to learn a nonlinear transformation
via an auto-encoder while ignoring the sample relation, leaving huge room to
design more effective representation learning mechanisms for unsupervised
active learning. In this paper, we propose a novel deep unsupervised Active
Learning model via Learnable Graphs, named ALLG. ALLG benefits from learning
optimal graph structures to acquire better sample representation and select
representative samples. To make the learnt graph structure more stable and
effective, we take into account $k$-nearest neighbor graph as a priori, and
learn a relation propagation graph structure. We also incorporate shortcut
connections among different layers, which can alleviate the well-known
over-smoothing problem to some extent. To the best of our knowledge, this is
the first attempt to leverage graph structure learning for unsupervised active
learning. Extensive experiments performed on six datasets demonstrate the
efficacy of our method.

    

### [[2111.04287] BlueFog: Make Decentralized Algorithms Practical for Optimization and Deep Learning](http://arxiv.org/abs/2111.04287)


  Decentralized algorithm is a form of computation that achieves a global goal
through local dynamics that relies on low-cost communication between
directly-connected agents. On large-scale optimization tasks involving
distributed datasets, decentralized algorithms have shown strong, sometimes
superior, performance over distributed algorithms with a central node.
Recently, developing decentralized algorithms for deep learning has attracted
great attention. They are considered as low-communication-overhead alternatives
to those using a parameter server or the Ring-Allreduce protocol. However, the
lack of an easy-to-use and efficient software package has kept most
decentralized algorithms merely on paper. To fill the gap, we introduce
BlueFog, a python library for straightforward, high-performance implementations
of diverse decentralized algorithms. Based on a unified abstraction of various
communication operations, BlueFog offers intuitive interfaces to implement a
spectrum of decentralized algorithms, from those using a static, undirected
graph for synchronous operations to those using dynamic and directed graphs for
asynchronous operations. BlueFog also adopts several system-level acceleration
techniques to further optimize the performance on the deep learning tasks. On
mainstream DNN training tasks, BlueFog reaches a much higher throughput and
achieves an overall $1.2\times \sim 1.8\times$ speedup over Horovod, a
state-of-the-art distributed deep learning package based on Ring-Allreduce.
BlueFog is open source at this https URL.

    

### [[2111.04295] The Hardness Analysis of Thompson Sampling for Combinatorial Semi-bandits with Greedy Oracle](http://arxiv.org/abs/2111.04295)


  Thompson sampling (TS) has attracted a lot of interest in the bandit area. It
was introduced in the 1930s but has not been theoretically proven until recent
years. All of its analysis in the combinatorial multi-armed bandit (CMAB)
setting requires an exact oracle to provide optimal solutions with any input.
However, such an oracle is usually not feasible since many combinatorial
optimization problems are NP-hard and only approximation oracles are available.
An example (Wang and Chen, 2018) has shown the failure of TS to learn with an
approximation oracle. However, this oracle is uncommon and is designed only for
a specific problem instance. It is still an open question whether the
convergence analysis of TS can be extended beyond the exact oracle in CMAB. In
this paper, we study this question under the greedy oracle, which is a common
(approximation) oracle with theoretical guarantees to solve many (offline)
combinatorial optimization problems. We provide a problem-dependent regret
lower bound of order $\Omega(\log T/\Delta^2)$ to quantify the hardness of TS
to solve CMAB problems with greedy oracle, where $T$ is the time horizon and
$\Delta$ is some reward gap. We also provide an almost matching regret upper
bound. These are the first theoretical results for TS to solve CMAB with a
common approximation oracle and break the misconception that TS cannot work
with approximation oracles.

    

### [[2111.04303] Defense Against Explanation Manipulation](http://arxiv.org/abs/2111.04303)


  Explainable machine learning attracts increasing attention as it improves
transparency of models, which is helpful for machine learning to be trusted in
real applications. However, explanation methods have recently been demonstrated
to be vulnerable to manipulation, where we can easily change a model's
explanation while keeping its prediction constant. To tackle this problem, some
efforts have been paid to use more stable explanation methods or to change
model configurations. In this work, we tackle the problem from the training
perspective, and propose a new training scheme called Adversarial Training on
EXplanations (ATEX) to improve the internal explanation stability of a model
regardless of the specific explanation method being applied. Instead of
directly specifying explanation values over data instances, ATEX only puts
requirement on model predictions which avoids involving second-order
derivatives in optimization. As a further discussion, we also find that
explanation stability is closely related to another property of the model,
i.e., the risk of being exposed to adversarial attack. Through experiments,
besides showing that ATEX improves model robustness against manipulation
targeting explanation, it also brings additional benefits including smoothing
explanations and improving the efficacy of adversarial training if applied to
the model.

    

### [[2111.04308] Learning Context-Aware Representations of Subtrees](http://arxiv.org/abs/2111.04308)


  This thesis tackles the problem of learning efficient representations of
complex, structured data with a natural application to web page and element
classification. We hypothesise that the context around the element inside the
web page is of high value to the problem and is currently under exploited. This
thesis aims to solve the problem of classifying web elements as subtrees of a
DOM tree by also considering their context.
To achieve this, first we discuss current expert knowledge systems that work
on structures, such as Tree-LSTM. Then, we propose context-aware extensions to
this model. We show that the new model achieves an average F1-score of 0.7973
on a multi-class web classification task. This model generates better
representations for various subtrees and may be used for applications such
element classification, state estimators in reinforcement learning over the Web
and more.

    

### [[2111.04309] Assessing learned features of Deep Learning applied to EEG](http://arxiv.org/abs/2111.04309)


  Convolutional Neural Networks (CNNs) have achieved impressive performance on
many computer vision related tasks, such as object detection, image
recognition, image retrieval, etc. These achievements benefit from the CNNs'
outstanding capability to learn discriminative features with deep layers of
neuron structures and iterative training process. This has inspired the EEG
research community to adopt CNN in performing EEG classification tasks.
However, CNNs learned features are not immediately interpretable, causing a
lack of understanding of the CNNs' internal working mechanism. To improve CNN
interpretability, CNN visualization methods are applied to translate the
internal features into visually perceptible patterns for qualitative analysis
of CNN layers. Many CNN visualization methods have been proposed in the
Computer Vision literature to interpret the CNN network structure, operation,
and semantic concept, yet applications to EEG data analysis have been limited.
In this work we use 3 different methods to extract EEG-relevant features from a
CNN trained on raw EEG data: optimal samples for each classification category,
activation maximization, and reverse convolution. We applied these methods to a
high-performing Deep Learning model with state-of-the-art performance for an
EEG sex classification task, and show that the model features a difference in
the theta frequency band. We show that visualization of a CNN model can reveal
interesting EEG results. Using these tools, EEG researchers using Deep Learning
can better identify the learned EEG features, possibly identifying new class
relevant biomarkers.

    

### [[2111.04313] A Relational Model for One-Shot Classification](http://arxiv.org/abs/2111.04313)


  We show that a deep learning model with built-in relational inductive bias
can bring benefits to sample-efficient learning, without relying on extensive
data augmentation. The proposed one-shot classification model performs
relational matching of a pair of inputs in the form of local and pairwise
attention. Our approach solves perfectly the one-shot image classification
Omniglot challenge. Our model exceeds human level accuracy, as well as the
previous state of the art, with no data augmentation.

    

### [[2111.04314] Graph Robustness Benchmark: Benchmarking the Adversarial Robustness of Graph Machine Learning](http://arxiv.org/abs/2111.04314)


  Adversarial attacks on graphs have posed a major threat to the robustness of
graph machine learning (GML) models. Naturally, there is an ever-escalating
arms race between attackers and defenders. However, the strategies behind both
sides are often not fairly compared under the same and realistic conditions. To
bridge this gap, we present the Graph Robustness Benchmark (GRB) with the goal
of providing a scalable, unified, modular, and reproducible evaluation for the
adversarial robustness of GML models. GRB standardizes the process of attacks
and defenses by 1) developing scalable and diverse datasets, 2) modularizing
the attack and defense implementations, and 3) unifying the evaluation protocol
in refined scenarios. By leveraging the GRB pipeline, the end-users can focus
on the development of robust GML models with automated data processing and
experimental evaluations. To support open and reproducible research on graph
adversarial learning, GRB also hosts public leaderboards across different
scenarios. As a starting point, we conduct extensive experiments to benchmark
baseline techniques. GRB is open-source and welcomes contributions from the
community. Datasets, codes, leaderboards are available at
this https URL.

    

### [[2111.04315] Spirometry-based airways disease simulation and recognition using Machine Learning approaches](http://arxiv.org/abs/2111.04315)


  The purpose of this study is to provide means to physicians for automated and
fast recognition of airways diseases. In this work, we mainly focus on measures
that can be easily recorded using a spirometer. The signals used in this
framework are simulated using the linear bi-compartment model of the lungs.
This allows us to simulate ventilation under the hypothesis of ventilation at
rest (tidal breathing). By changing the resistive and elastic parameters, data
samples are realized simulating healthy, fibrosis and asthma breathing. On this
synthetic data, different machine learning models are tested and their
performance is assessed. All but the Naive bias classifier show accuracy of at
least 99%. This represents a proof of concept that Machine Learning can
accurately differentiate diseases based on manufactured spirometry data. This
paves the way for further developments on the topic, notably testing the model
on real data.

    

### [[2111.04318] Auto-Encoding Knowledge Graph for Unsupervised Medical Report Generation](http://arxiv.org/abs/2111.04318)


  Medical report generation, which aims to automatically generate a long and
coherent report of a given medical image, has been receiving growing research
interests. Existing approaches mainly adopt a supervised manner and heavily
rely on coupled image-report pairs. However, in the medical domain, building a
large-scale image-report paired dataset is both time-consuming and expensive.
To relax the dependency on paired data, we propose an unsupervised model
Knowledge Graph Auto-Encoder (KGAE) which accepts independent sets of images
and reports in training. KGAE consists of a pre-constructed knowledge graph, a
knowledge-driven encoder and a knowledge-driven decoder. The knowledge graph
works as the shared latent space to bridge the visual and textual domains; The
knowledge-driven encoder projects medical images and reports to the
corresponding coordinates in this latent space and the knowledge-driven decoder
generates a medical report given a coordinate in this space. Since the
knowledge-driven encoder and decoder can be trained with independent sets of
images and reports, KGAE is unsupervised. The experiments show that the
unsupervised KGAE generates desirable medical reports without using any
image-report training pairs. Moreover, KGAE can also work in both
semi-supervised and supervised settings, and accept paired images and reports
in training. By further fine-tuning with image-report pairs, KGAE consistently
outperforms the current state-of-the-art models on two datasets.

    

### [[2111.04330] Characterizing the adversarial vulnerability of speech self-supervised learning](http://arxiv.org/abs/2111.04330)


  A leaderboard named Speech processing Universal PERformance Benchmark
(SUPERB), which aims at benchmarking the performance of a shared
self-supervised learning (SSL) speech model across various downstream speech
tasks with minimal modification of architectures and small amount of data, has
fueled the research for speech representation learning. The SUPERB demonstrates
speech SSL upstream models improve the performance of various downstream tasks
through just minimal adaptation. As the paradigm of the self-supervised
learning upstream model followed by downstream tasks arouses more attention in
the speech community, characterizing the adversarial robustness of such
paradigm is of high priority. In this paper, we make the first attempt to
investigate the adversarial vulnerability of such paradigm under the attacks
from both zero-knowledge adversaries and limited-knowledge adversaries. The
experimental results illustrate that the paradigm proposed by SUPERB is
seriously vulnerable to limited-knowledge adversaries, and the attacks
generated by zero-knowledge adversaries are with transferability. The XAB test
verifies the imperceptibility of crafted adversarial attacks.

    

### [[1710.09953] The Error Probability of Random Fourier Features is Dimensionality Independent](http://arxiv.org/abs/1710.09953)


  We show that the error probability of reconstructing kernel matrices from
Random Fourier Features for the Gaussian kernel function is at most
$\mathcal{O}(R^{2/3} \exp(-D))$, where $D$ is the number of random features and
$R$ is the diameter of the data domain. We also provide an
information-theoretic method-independent lower bound of $\Omega((1-\exp(-R^2))
\exp(-D))$. Compared to prior work, we are the first to show that the error
probability for random Fourier features is independent of the dimensionality of
data points. As applications of our theory, we obtain dimension-independent
bounds for kernel ridge regression and support vector machines.

    

### [[1906.01549] Streaming Variational Monte Carlo](http://arxiv.org/abs/1906.01549)


  Nonlinear state-space models are powerful tools to describe dynamical
structures in complex time series. In a streaming setting where data are
processed one sample at a time, simultaneous inference of the state and its
nonlinear dynamics has posed significant challenges in practice. We develop a
novel online learning framework, leveraging variational inference and
sequential Monte Carlo, which enables flexible and accurate Bayesian joint
filtering. Our method provides an approximation of the filtering posterior
which can be made arbitrarily close to the true filtering distribution for a
wide class of dynamics models and observation models. Specifically, the
proposed framework can efficiently approximate a posterior over the dynamics
using sparse Gaussian processes, allowing for an interpretable model of the
latent dynamics. Constant time complexity per sample makes our approach
amenable to online learning scenarios and suitable for real-time applications.

    

### [[2002.12435] Learning in Markov Decision Processes under Constraints](http://arxiv.org/abs/2002.12435)


  We consider reinforcement learning (RL) in Markov Decision Processes in which
an agent repeatedly interacts with an environment that is modeled by a
controlled Markov process. At each time step $t$, it earns a reward, and also
incurs a cost-vector consisting of $M$ costs. We design learning algorithms
that maximize the cumulative reward earned over a time horizon of $T$
time-steps, while simultaneously ensuring that the average values of the $M$
cost expenditures are bounded by agent-specified thresholds
$c^{ub}_i,i=1,2,\ldots,M$. The considerations on the cumulative cost
expenditures departs from the existing literature, in that the agent now
additionally needs to balance the cost expenses in an online manner, while
simultaneously performing the exploration-exploitation trade-off that is
typically encountered in RL tasks.
In order to measure the performance of a reinforcement learning algorithm
that satisfies the average cost constraints, we define an $M+1$ dimensional
regret vector that is composed of its reward regret, and $M$ cost regrets. The
reward regret measures the sub-optimality in the cumulative reward, while the
$i$-th component of the cost regret vector is the difference between its $i$-th
cumulative cost expense and the expected cost expenditures $Tc^{ub}_i$. We
prove that with a high probablity, the regret vector of UCRL-CMDP is
upper-bounded as $O\left( S\sqrt{AT^{1.5}\log(T)}\right)$, where $S$ is the
number of states, $A$ is the number of actions, and $T$ is the time horizon. We
further show how to reduce the regret of a desired subset of the $M$ costs, at
the expense of increasing the regrets of rewards and the remaining costs. To
the best of our knowledge, ours is the only work that considers non-episodic RL
under average cost constraints, and derive algorithms that can~\emph{tune the
regret vector} according to the agent's requirements on its cost regrets.

    

### [[2003.09347] SAT: Improving Adversarial Training via Curriculum-Based Loss Smoothing](http://arxiv.org/abs/2003.09347)


  Adversarial training (AT) has become a popular choice for training robust
networks. However, it tends to sacrifice clean accuracy heavily in favor of
robustness and suffers from a large generalization error. To address these
concerns, we propose Smooth Adversarial Training (SAT), guided by our analysis
on the eigenspectrum of the loss Hessian. We find that curriculum learning, a
scheme that emphasizes on starting "easy" and gradually ramping up on the
"difficulty" of training, smooths the adversarial loss landscape for a suitably
chosen difficulty metric. We present a general formulation for curriculum
learning in the adversarial setting and propose two difficulty metrics based on
the maximal Hessian eigenvalue (H-SAT) and the softmax probability (P-SA). We
demonstrate that SAT stabilizes network training even for a large perturbation
norm and allows the network to operate at a better clean accuracy versus
robustness trade-off curve compared to AT. This leads to a significant
improvement in both clean accuracy and robustness compared to AT, TRADES, and
other baselines. To highlight a few results, our best model improves normal and
robust accuracy by 6% and 1% on CIFAR-100 compared to AT, respectively. On
Imagenette, a ten-class subset of ImageNet, our model outperforms AT by 23% and
3% on normal and robust accuracy respectively.

    

### [[2003.12020] A Separation Result Between Data-oblivious and Data-aware Poisoning Attacks](http://arxiv.org/abs/2003.12020)


  Poisoning attacks have emerged as a significant security threat to machine
learning algorithms. It has been demonstrated that adversaries who make small
changes to the training set, such as adding specially crafted data points, can
hurt the performance of the output model. Some of the stronger poisoning
attacks require the full knowledge of the training data. This leaves open the
possibility of achieving the same attack results using poisoning attacks that
do not have the full knowledge of the clean training set.
In this work, we initiate a theoretical study of the problem above.
Specifically, for the case of feature selection with LASSO, we show that
full-information adversaries (that craft poisoning examples based on the rest
of the training data) are provably stronger than the optimal attacker that is
oblivious to the training set yet has access to the distribution of the data.
Our separation result shows that the two setting of data-aware and
data-oblivious are fundamentally different and we cannot hope to always achieve
the same attack or defense results in these scenarios.

    

### [[2007.00072] Data Movement Is All You Need: A Case Study on Optimizing Transformers](http://arxiv.org/abs/2007.00072)


  Transformers are one of the most important machine learning workloads today.
Training one is a very compute-intensive task, often taking days or weeks, and
significant attention has been given to optimizing transformers. Despite this,
existing implementations do not efficiently utilize GPUs. We find that data
movement is the key bottleneck when training. Due to Amdahl's Law and massive
improvements in compute performance, training has now become memory-bound.
Further, existing frameworks use suboptimal data layouts. Using these insights,
we present a recipe for globally optimizing data movement in transformers. We
reduce data movement by up to 22.91% and overall achieve a 1.30x performance
improvement over state-of-the-art frameworks when training a BERT encoder layer
and 1.19x for the entire BERT. Our approach is applicable more broadly to
optimizing deep neural networks, and offers insight into how to tackle emerging
performance bottlenecks.

    

### [[2008.01036] Multiple Descent: Design Your Own Generalization Curve](http://arxiv.org/abs/2008.01036)


  This paper explores the generalization loss of linear regression in variably
parameterized families of models, both under-parameterized and
over-parameterized. We show that the generalization curve can have an arbitrary
number of peaks, and moreover, locations of those peaks can be explicitly
controlled. Our results highlight the fact that both classical U-shaped
generalization curve and the recently observed double descent curve are not
intrinsic properties of the model family. Instead, their emergence is due to
the interaction between the properties of the data and the inductive biases of
learning algorithms.

    

### [[2008.02577] A critical analysis of metrics used for measuring progress in artificial intelligence](http://arxiv.org/abs/2008.02577)


  Comparing model performances on benchmark datasets is an integral part of
measuring and driving progress in artificial intelligence. A model's
performance on a benchmark dataset is commonly assessed based on a single or a
small set of performance metrics. While this enables quick comparisons, it may
entail the risk of inadequately reflecting model performance if the metric does
not sufficiently cover all performance characteristics. It is unknown to what
extent this might impact benchmarking efforts.
To address this question, we analysed the current landscape of performance
metrics based on data covering 3867 machine learning model performance results
from the open repository 'Papers with Code'. Our results suggest that the large
majority of metrics currently used have properties that may result in an
inadequate reflection of a models' performance. While alternative metrics that
address problematic properties have been proposed, they are currently rarely
used.
Furthermore, we describe ambiguities in reported metrics, which may lead to
difficulties in interpreting and comparing model performances.

    

### [[2008.04213] Boosting Ant Colony Optimization via Solution Prediction and Machine Learning](http://arxiv.org/abs/2008.04213)


  This paper introduces an enhanced meta-heuristic (ML-ACO) that combines
machine learning (ML) and ant colony optimization (ACO) to solve combinatorial
optimization problems. To illustrate the underlying mechanism of our ML-ACO
algorithm, we start by describing a test problem, the orienteering problem. In
this problem, the objective is to find a route that visits a subset of vertices
in a graph within a time budget to maximize the collected score. In the first
phase of our ML-ACO algorithm, an ML model is trained using a set of small
problem instances where the optimal solution is known. Specifically,
classification models are used to classify an edge as being part of the optimal
route, or not, using problem-specific features and statistical measures. The
trained model is then used to predict the probability that an edge in the graph
of a test problem instance belongs to the corresponding optimal route. In the
second phase, we incorporate the predicted probabilities into the ACO component
of our algorithm, i.e., using the probability values as heuristic weights or to
warm start the pheromone matrix. Here, the probability values bias sampling
towards favoring those predicted high-quality edges when constructing feasible
routes. We have tested multiple classification models including graph neural
networks, logistic regression and support vector machines, and the experimental
results show that our solution prediction approach consistently boosts the
performance of ACO. Further, we empirically show that our ML model trained on
small synthetic instances generalizes well to large synthetic and real-world
instances. Our approach integrating ML with a meta-heuristic is generic and can
be applied to a wide range of optimization problems.

    

### [[2008.05948] Estimating the Magnitude and Phase of Automotive Radar Signals under Multiple Interference Sources with Fully Convolutional Networks](http://arxiv.org/abs/2008.05948)


  Radar sensors are gradually becoming a wide-spread equipment for road
vehicles, playing a crucial role in autonomous driving and road safety. The
broad adoption of radar sensors increases the chance of interference among
sensors from different vehicles, generating corrupted range profiles and
range-Doppler maps. In order to extract distance and velocity of multiple
targets from range-Doppler maps, the interference affecting each range profile
needs to be mitigated. In this paper, we propose a fully convolutional neural
network for automotive radar interference mitigation. In order to train our
network in a real-world scenario, we introduce a new data set of realistic
automotive radar signals with multiple targets and multiple interferers. To our
knowledge, we are the first to apply weight pruning in the automotive radar
domain, obtaining superior results compared to the widely-used dropout. While
most previous works successfully estimated the magnitude of automotive radar
signals, we propose a deep learning model that can accurately estimate the
phase. For instance, our novel approach reduces the phase estimation error with
respect to the commonly-adopted zeroing technique by half, from 12.55 degrees
to 6.58 degrees. Considering the lack of databases for automotive radar
interference mitigation, we release as open source our large-scale data set
that closely replicates the real-world automotive scenario for multiple
interference cases, allowing others to objectively compare their future work in
this domain. Our data set is available for download at:
this http URL.

    

### [[2009.01174] Transform Quantization for CNN (Convolutional Neural Network) Compression](http://arxiv.org/abs/2009.01174)


  In this paper, we compress convolutional neural network (CNN) weights
post-training via transform quantization. Previous CNN quantization techniques
tend to ignore the joint statistics of weights and activations, producing
sub-optimal CNN performance at a given quantization bit-rate, or consider their
joint statistics during training only and do not facilitate efficient
compression of already trained CNN models. We optimally transform (decorrelate)
and quantize the weights post-training using a rate-distortion framework to
improve compression at any given quantization bit-rate. Transform quantization
unifies quantization and dimensionality reduction (decorrelation) techniques in
a single framework to facilitate low bit-rate compression of CNNs and efficient
inference in the transform domain. We first introduce a theory of rate and
distortion for CNN quantization, and pose optimum quantization as a
rate-distortion optimization problem. We then show that this problem can be
solved using optimal bit-depth allocation following decorrelation by the
optimal End-to-end Learned Transform (ELT) we derive in this paper. Experiments
demonstrate that transform quantization advances the state of the art in CNN
compression in both retrained and non-retrained quantization scenarios. In
particular, we find that transform quantization with retraining is able to
compress CNN models such as AlexNet, ResNet and DenseNet to very low bit-rates
(1-2 bits).

    

### [[2010.04456] Augmenting Physical Models with Deep Networks for Complex Dynamics Forecasting](http://arxiv.org/abs/2010.04456)


  Forecasting complex dynamical phenomena in settings where only partial
knowledge of their dynamics is available is a prevalent problem across various
scientific fields. While purely data-driven approaches are arguably
insufficient in this context, standard physical modeling based approaches tend
to be over-simplistic, inducing non-negligible errors. In this work, we
introduce the APHYNITY framework, a principled approach for augmenting
incomplete physical dynamics described by differential equations with deep
data-driven models. It consists in decomposing the dynamics into two
components: a physical component accounting for the dynamics for which we have
some prior knowledge, and a data-driven component accounting for errors of the
physical model. The learning problem is carefully formulated such that the
physical model explains as much of the data as possible, while the data-driven
component only describes information that cannot be captured by the physical
model, no more, no less. This not only provides the existence and uniqueness
for this decomposition, but also ensures interpretability and benefits
generalization. Experiments made on three important use cases, each
representative of a different family of phenomena, i.e. reaction-diffusion
equations, wave equations and the non-linear damped pendulum, show that
APHYNITY can efficiently leverage approximate physical models to accurately
forecast the evolution of the system and correctly identify relevant physical
parameters.

    

### [[2010.05649] Multivariate Time Series Classification with Hierarchical Variational Graph Pooling](http://arxiv.org/abs/2010.05649)


  With the advancement of sensing technology, multivariate time series
classification (MTSC) has recently received considerable attention. Existing
deep learning-based MTSC techniques, which mostly rely on convolutional or
recurrent neural networks, are primarily concerned with the temporal dependency
of single time series. As a result, they struggle to express pairwise
dependencies among multivariate variables directly. Furthermore, current
spatial-temporal modeling (e.g., graph classification) methodologies based on
Graph Neural Networks (GNNs) are inherently flat and cannot aggregate hub data
in a hierarchical manner. To address these limitations, we propose a novel
graph pooling-based framework MTPool to obtain the expressive global
representation of MTS. We first convert MTS slices to graphs by utilizing
interactions of variables via graph structure learning module and attain the
spatial-temporal graph node features via temporal convolutional module. To get
global graph-level representation, we design an "encoder-decoder" based
variational graph pooling module for creating adaptive centroids for cluster
assignments. Then we combine GNNs and our proposed variational graph pooling
layers for joint graph representation learning and graph coarsening, after
which the graph is progressively coarsened to one node. At last, a
differentiable classifier takes this coarsened representation to get the final
predicted class. Experiments on ten benchmark datasets exhibit MTPool
outperforms state-of-the-art strategies in the MTSC task.

    

### [[2012.00190] Towards Label-Agnostic Emotion Embeddings](http://arxiv.org/abs/2012.00190)


  Research in emotion analysis is scattered across different label formats
(e.g., polarity types, basic emotion categories, and affective dimensions),
linguistic levels (word vs. sentence vs. discourse), and, of course, (few
well-resourced but much more under-resourced) natural languages and text genres
(e.g., product reviews, tweets, news). The resulting heterogeneity makes data
and software developed under these conflicting constraints hard to compare and
challenging to integrate. To resolve this unsatisfactory state of affairs we
here propose a training scheme that learns a shared latent representation of
emotion independent from different label formats, natural languages, and even
disparate model architectures. Experiments on a wide range of datasets indicate
that this approach yields the desired interoperability without penalizing
prediction quality. Code and data are archived under DOI
10.5281/zenodo.5466068.

    

### [[2012.00364] Pre-Trained Image Processing Transformer](http://arxiv.org/abs/2012.00364)


  As the computing power of modern hardware is increasing strongly, pre-trained
deep learning models (e.g., BERT, GPT-3) learned on large-scale datasets have
shown their effectiveness over conventional methods. The big progress is mainly
contributed to the representation ability of transformer and its variant
architectures. In this paper, we study the low-level computer vision task
(e.g., denoising, super-resolution and deraining) and develop a new pre-trained
model, namely, image processing transformer (IPT). To maximally excavate the
capability of transformer, we present to utilize the well-known ImageNet
benchmark for generating a large amount of corrupted image pairs. The IPT model
is trained on these images with multi-heads and multi-tails. In addition, the
contrastive learning is introduced for well adapting to different image
processing tasks. The pre-trained model can therefore efficiently employed on
desired task after fine-tuning. With only one pre-trained model, IPT
outperforms the current state-of-the-art methods on various low-level
benchmarks. Code is available at this https URL
and this https URL


### [[2012.03774] Learning to extrapolate using continued fractions: Predicting the critical temperature of superconductor materials](http://arxiv.org/abs/2012.03774)


  In Artificial Intelligence we often seek to identify an unknown target
function of many variables $y=f(\mathbf{x})$ giving a limited set of instances
$S=\{(\mathbf{x^{(i)}},y^{(i)})\}$ with $\mathbf{x^{(i)}} \in D$ where $D$ is a
domain of interest. We refer to $S$ as the training set and the final quest is
to identify the mathematical model that approximates this target function for
new $\mathbf{x}$; with the set $T=\{ \mathbf{x^{(j)}} \} \subset D$ with $T
\neq S$ (i.e. thus testing the model generalisation). However, for some
applications, the main interest is approximating well the unknown function on a
larger domain $D'$ that contains $D$. In cases involving the design of new
structures, for instance, we may be interested in maximizing $f$; thus, the
model derived from $S$ alone should also generalize well in $D'$ for samples
with values of $y$ larger than the largest observed in $S$. In that sense, the
AI system would provide important information that could guide the design
process, e.g., using the learned model as a surrogate function to design new
lab experiments.
We introduce a method for multivariate regression based on iterative fitting
of a continued fraction by incorporating additive spline models. We compared it
with established methods such as AdaBoost, Kernel Ridge, Linear Regression,
Lasso Lars, Linear Support Vector Regression, Multi-Layer Perceptrons, Random
Forests, Stochastic Gradient Descent and XGBoost. We tested the performance on
the important problem of predicting the critical temperature of superconductors
based on physical-chemical characteristics.

    

### [[2012.03817] A bounded-noise mechanism for differential privacy](http://arxiv.org/abs/2012.03817)


  We present an asymptotically optimal $(\epsilon,\delta)$ differentially
private mechanism for answering multiple, adaptively asked, $\Delta$-sensitive
queries, settling the conjecture of Steinke and Ullman [2020]. Our algorithm
has a significant advantage that it adds independent bounded noise to each
query, thus providing an absolute error bound. Additionally, we apply our
algorithm in adaptive data analysis, obtaining an improved guarantee for
answering multiple queries regarding some underlying distribution using a
finite sample. Numerical computations show that the bounded-noise mechanism
outperforms the Gaussian mechanism in many standard settings.

    

### [[2012.04231] Molecule Optimization via Fragment-based Generative Models](http://arxiv.org/abs/2012.04231)


  In drug discovery, molecule optimization is an important step in order to
modify drug candidates into better ones in terms of desired drug properties.
With the recent advance of Artificial Intelligence, this traditionally in vitro
process has been increasingly facilitated by in silico approaches. We present
an innovative in silico approach to computationally optimizing molecules and
formulate the problem as to generate optimized molecular graphs via deep
generative models. Our generative models follow the key idea of fragment-based
drug design, and optimize molecules by modifying their small fragments. Our
models learn how to identify the to-be-optimized fragments and how to modify
such fragments by learning from the difference of molecules that have good and
bad properties. In optimizing a new molecule, our models apply the learned
signals to decode optimized fragments at the predicted location of the
fragments. We also construct multiple such models into a pipeline such that
each of the models in the pipeline is able to optimize one fragment, and thus
the entire pipeline is able to modify multiple fragments of molecule if needed.
We compare our models with other state-of-the-art methods on benchmark datasets
and demonstrate that our methods significantly outperform others with more than
80% property improvement under moderate molecular similarity constraints, and
more than 10% property improvement under high molecular similarity constraints.

    

### [[2012.10794] Sample Complexity of Adversarially Robust Linear Classification on Separated Data](http://arxiv.org/abs/2012.10794)


  We consider the sample complexity of learning with adversarial robustness.
Most prior theoretical results for this problem have considered a setting where
different classes in the data are close together or overlapping. Motivated by
some real applications, we consider, in contrast, the well-separated case where
there exists a classifier with perfect accuracy and robustness, and show that
the sample complexity narrates an entirely different story. Specifically, for
linear classifiers, we show a large class of well-separated distributions where
the expected robust loss of any algorithm is at least $\Omega(\frac{d}{n})$,
whereas the max margin algorithm has expected standard loss $O(\frac{1}{n})$.
This shows a gap in the standard and robust losses that cannot be obtained via
prior techniques. Additionally, we present an algorithm that, given an instance
where the robustness radius is much smaller than the gap between the classes,
gives a solution with expected robust loss is $O(\frac{1}{n})$. This shows that
for very well-separated data, convergence rates of $O(\frac{1}{n})$ are
achievable, which is not the case otherwise. Our results apply to robustness
measured in any $\ell_p$ norm with $p > 1$ (including $p = \infty$).

    

### [[2012.13180] Unveiling Real-Life Effects of Online Photo Sharing](http://arxiv.org/abs/2012.13180)


  Social networks give free access to their services in exchange for the right
to exploit their users' data. Data sharing is done in an initial context which
is chosen by the users. However, data are used by social networks and third
parties in different contexts which are often not transparent. In order to
unveil such usages, we propose an approach which focuses on the effects of data
sharing in impactful real-life situations. Focus is put on visual content
because of its strong influence in shaping online user profiles. The approach
relies on three components: (1) a set of visual objects with associated
situation impact ratings obtained by crowdsourcing, (2) a corresponding set of
object detectors for mining users' photos and (3) a ground truth dataset made
of 500 visual user profiles which are manually rated per situation. These
components are combined in LERVUP, a method which learns to rate visual user
profiles in each situation. LERVUP exploits a new image descriptor which
aggregates object ratings and object detections at user level and an attention
mechanism which boosts highly-rated objects to prevent them from being
overwhelmed by low-rated ones. Performance is evaluated per situation by
measuring the correlation between the automatic ranking of profile ratings and
a manual ground truth. Results indicate that LERVUP is effective since a strong
correlation of the two rankings is obtained. A practical implementation of the
approach in a mobile app which raises user awareness about shared data usage is
also discussed.

    

### [[2101.01350] Towards an efficient approach for the nonconvex $\ell_p$-ball projection: algorithm and analysis](http://arxiv.org/abs/2101.01350)


  This paper primarily focuses on computing the Euclidean projection of a
vector onto the $\ell_{p}$ ball in which $p\in(0,1)$. Such a problem emerges as
the core building block in statistical machine learning and signal processing
tasks because of its ability to promote sparsity. However, efficient numerical
algorithms for finding the projections are still not available, particularly in
large-scale optimization. To meet this challenge, we first derive the
first-order necessary optimality conditions of this problem. Based on this
characterization, we develop a novel numerical approach for computing the
stationary point through solving a sequence of projections onto the reweighted
$\ell_{1}$-balls. This method is practically simple to implement and
computationally efficient. Moreover, the proposed algorithm is shown to
converge uniquely under mild conditions and has a worst-case $O(1/\sqrt{k})$
convergence rate. Numerical experiments demonstrate the efficiency of our
proposed algorithm.

    

### [[2101.04829] On the Effectiveness of Small Input Noise for Defending Against Query-based Black-Box Attacks](http://arxiv.org/abs/2101.04829)


  While deep neural networks show unprecedented performance in various tasks,
the vulnerability to adversarial examples hinders their deployment in
safety-critical systems. Many studies have shown that attacks are also possible
even in a black-box setting where an adversary cannot access the target model's
internal information. Most black-box attacks are based on queries, each of
which obtains the target model's output for an input, and many recent studies
focus on reducing the number of required queries. In this paper, we pay
attention to an implicit assumption of query-based black-box adversarial
attacks that the target model's output exactly corresponds to the query input.
If some randomness is introduced into the model, it can break the assumption,
and thus, query-based attacks may have tremendous difficulty in both gradient
estimation and local search, which are the core of their attack process. From
this motivation, we observe even a small additive input noise can neutralize
most query-based attacks and name this simple yet effective approach Small
Noise Defense (SND). We analyze how SND can defend against query-based
black-box attacks and demonstrate its effectiveness against eight
state-of-the-art attacks with CIFAR-10 and ImageNet datasets. Even with strong
defense ability, SND almost maintains the original classification accuracy and
computational speed. SND is readily applicable to pre-trained models by adding
only one line of code at the inference.

    

### [[2101.10841] PConv: Simple yet Effective Convolutional Layer for Generative Adversarial Network](http://arxiv.org/abs/2101.10841)


  This paper presents a novel convolutional layer, called perturbed convolution
(PConv), which focuses on achieving two goals simultaneously: improving the
generative adversarial network (GAN) performance and alleviating the
memorization problem in which the discriminator memorizes all images from a
given dataset as training progresses. In PConv, perturbed features are
generated by randomly disturbing an input tensor before performing the
convolution operation. This approach is simple but surprisingly effective.
First, to produce a similar output even with the perturbed tensor, each layer
in the discriminator should learn robust features having a small local
Lipschitz value. Second, since the input tensor is randomly perturbed during
the training procedure like the dropout in neural networks, the memorization
problem could be alleviated. To show the generalization ability of the proposed
method, we conducted extensive experiments with various loss functions and
datasets including CIFAR-10, CelebA, CelebA-HQ, LSUN, and tiny-ImageNet. The
quantitative evaluations demonstrate that PConv effectively boosts the
performance of GAN and conditional GAN in terms of Frechet inception distance
(FID).

    

### [[2101.11214] Towards Robustness to Label Noise in Text Classification via Noise Modeling](http://arxiv.org/abs/2101.11214)


  Large datasets in NLP suffer from noisy labels, due to erroneous automatic
and human annotation procedures. We study the problem of text classification
with label noise, and aim to capture this noise through an auxiliary noise
model over the classifier. We first assign a probability score to each training
sample of having a noisy label, through a beta mixture model fitted on the
losses at an early epoch of training. Then, we use this score to selectively
guide the learning of the noise model and classifier. Our empirical evaluation
on two text classification tasks shows that our approach can improve over the
baseline accuracy, and prevent over-fitting to the noise.

    

### [[2102.04419] Analysis of the Effectiveness of Face-Coverings on the Death Ratio of COVID-19 Using Machine Learning](http://arxiv.org/abs/2102.04419)


  The recent outbreak of the COVID-19 led to the death of millions of people
worldwide. To stave off the spread of the virus, the authorities in the US
employed different strategies, including the mask mandate order issued by the
states' governors. In the current work, we defined a parameter called the
average death ratio as the monthly average of the number of daily deaths to the
monthly average number of daily cases. We utilized survey data to quantify
people's abidance by the mask mandate order. Additionally, we implicitly
addressed the extent to which people abide by the mask mandate order that may
depend on some parameters like population, income, and education level. Using
different machine learning classification algorithms, we investigated how the
decrease or increase in death ratio for the counties in the US West Coast
correlates with the input parameters. The results showed that for most counties
there, the mask mandate order decreased the death ratio reflecting the
effectiveness of this preventive measure on the West Coast. Additionally, the
changes in the death ratio demonstrated a noticeable correlation with the
socio-economic condition of each county. Moreover, the results showed a
promising classification accuracy score as high as around 90%.

    

### [[2102.04897] Learning State Representations from Random Deep Action-conditional Predictions](http://arxiv.org/abs/2102.04897)


  Our main contribution in this work is an empirical finding that random
General Value Functions (GVFs), i.e., deep action-conditional predictions --
random both in what feature of observations they predict as well as in the
sequence of actions the predictions are conditioned upon -- form good auxiliary
tasks for reinforcement learning (RL) problems. In particular, we show that
random deep action-conditional predictions when used as auxiliary tasks yield
state representations that produce control performance competitive with
state-of-the-art hand-crafted auxiliary tasks like value prediction, pixel
control, and CURL in both Atari and DeepMind Lab tasks. In another set of
experiments we stop the gradients from the RL part of the network to the state
representation learning part of the network and show, perhaps surprisingly,
that the auxiliary tasks alone are sufficient to learn state representations
good enough to outperform an end-to-end trained actor-critic baseline. We
opensourced our code at this https URL.

    

### [[2102.07952] A Survey of Machine Learning for Computer Architecture and Systems](http://arxiv.org/abs/2102.07952)


  It has been a long time that computer architecture and systems are optimized
for efficient execution of machine learning (ML) models. Now, it is time to
reconsider the relationship between ML and systems, and let ML transform the
way that computer architecture and systems are designed. This embraces a
twofold meaning: improvement of designers' productivity, and completion of the
virtuous cycle. In this paper, we present a comprehensive review of the work
that applies ML for computer architecture and system design. First, we perform
a high-level taxonomy by considering the typical role that ML techniques take
in architecture/system design, i.e., either for fast predictive modeling or as
the design methodology. Then, we summarize the common problems in computer
architecture/system design that can be solved by ML techniques, and the typical
ML techniques employed to resolve each of them. In addition to emphasis on
computer architecture in a narrow sense, we adopt the concept that data centers
can be recognized as warehouse-scale computers; sketchy discussions are
provided in adjacent computer systems, such as code generation and compiler; we
also give attention to how ML techniques can aid and transform design
automation. We further provide a future vision of opportunities and potential
directions, and envision that applying ML for computer architecture and systems
would thrive in the community.

    

### [[2102.09086] Consistent Non-Parametric Methods for Maximizing Robustness](http://arxiv.org/abs/2102.09086)


  Learning classifiers that are robust to adversarial examples has received a
great deal of recent attention. A major drawback of the standard robust
learning framework is there is an artificial robustness radius $r$ that applies
to all inputs. This ignores the fact that data may be highly heterogeneous, in
which case it is plausible that robustness regions should be larger in some
regions of data, and smaller in others. In this paper, we address this
limitation by proposing a new limit classifier, called the neighborhood optimal
classifier, that extends the Bayes optimal classifier outside its support by
using the label of the closest in-support point. We then argue that this
classifier maximizes the size of its robustness regions subject to the
constraint of having accuracy equal to the Bayes optimal. We then present
sufficient conditions under which general non-parametric methods that can be
represented as weight functions converge towards this limit, and show that both
nearest neighbors and kernel classifiers satisfy them under certain conditions.

    

### [[2102.09101] No-Substitution $k$-means Clustering with Optimal Center Complexity and Low Memory](http://arxiv.org/abs/2102.09101)


  We consider $k$-means clustering in the online no-substitution setting where
one must decide whether to take each data point $x_t$ as a center immediately
upon streaming it and cannot remove centers once taken. Our work is focused on
the \emph{arbitrary-order} assumption where there are no restrictions on how
the points $X$ are ordered or generated. Algorithms in this setting are
evaluated with respect to their approximation ratio compared to optimal
clustering cost, the number of centers they select, and their memory usage.
Recently, Bhattacharjee and Moshkovitz (2020) defined a parameter,
$Lower_{\alpha, k}(X)$ that governs the minimum number of centers any
$\alpha$-approximation clustering algorithm, allowed any amount of memory, must
take given input $X$. To complement their result, we give the first algorithm
that takes $\tilde{O}(Lower_{\alpha,k}(X))$ centers (hiding factors of $k, \log
n$) while simultaneously achieving a constant approximation and using
$\tilde{O}(k)$ memory in addition to the memory required to save the centers.
Our algorithm shows that it in the no-substitution setting, it is possible to
take an order-optimal number of centers while using little additional memory.

    

### [[2102.09703] Near-Optimal Randomized Exploration for Tabular MDP](http://arxiv.org/abs/2102.09703)


  We study exploration using randomized value functions in Thompson Sampling
(TS)-like algorithms in reinforcement learning. This type of algorithms enjoys
appealing empirical performance. We show that when we use 1) a single random
seed in each episode, and 2) a Bernstein-type magnitude of noise, we obtain a
worst-case $\widetilde{O}\left(H\sqrt{SAT}\right)$ regret bound for episodic
time-inhomogeneous Markov Decision Process where $S$ is the size of state
space, $A$ is the size of action space, $H$ is the planning horizon and $T$ is
the number of interactions. This bound polynomially improves all existing
bounds for TS-like algorithms based on randomized value functions, and for the
first time, matches the $\Omega\left(H\sqrt{SAT}\right)$ lower bound up to
logarithmic factors. Our result highlights that randomized exploration can be
near-optimal, which was previously only achieved by optimistic algorithms. To
achieve the desired result, we develop 1) a new clipping operation to ensure
both the probability being optimistic and the probability being pessimistic are
lower bounded by a constant, and 2) a new recursion formula for the absolute
value of estimations errors to analyze the regret.

    

### [[2103.00221] RA-GCN: Graph Convolutional Network for Disease Prediction Problems with Imbalanced Data](http://arxiv.org/abs/2103.00221)


  Disease prediction is a well-known classification problem in medical
applications. GCNs provide a powerful tool for analyzing the patients' features
relative to each other. This can be achieved by modeling the problem as a graph
node classification task, where each node is a patient. Due to the nature of
such medical datasets, class imbalance is a prevalent issue in the field of
disease prediction, where the distribution of classes is skewed. When the class
imbalance is present in the data, the existing graph-based classifiers tend to
be biased towards the major class(es) and neglect the samples in the minor
class(es). On the other hand, the correct diagnosis of the rare positive cases
among all the patients is vital in a healthcare system. In conventional
methods, such imbalance is tackled by assigning appropriate weights to classes
in the loss function which is still dependent on the relative values of
weights, sensitive to outliers, and in some cases biased towards the minor
class(es). In this paper, we propose a Re-weighted Adversarial Graph
Convolutional Network (RA-GCN) to prevent the graph-based classifier from
emphasizing the samples of any particular class. This is accomplished by
associating a graph-based neural network to each class, which is responsible
for weighting the class samples and changing the importance of each sample for
the classifier. Therefore, the classifier adjusts itself and determines the
boundary between classes with more attention to the important samples. The
parameters of the classifier and weighting networks are trained by an
adversarial approach. We show experiments on synthetic and three publicly
available medical datasets. RA-GCN demonstrates the superiority compared to
recent methods in identifying the patient's status on all three datasets. The
detailed analysis is provided as quantitative and qualitative experiments on
synthetic datasets.

    

### [[2103.03239] Moshpit SGD: Communication-Efficient Decentralized Training on Heterogeneous Unreliable Devices](http://arxiv.org/abs/2103.03239)


  Training deep neural networks on large datasets can often be accelerated by
using multiple compute nodes. This approach, known as distributed training, can
utilize hundreds of computers via specialized message-passing protocols such as
Ring All-Reduce. However, running these protocols at scale requires reliable
high-speed networking that is only available in dedicated clusters. In
contrast, many real-world applications, such as federated learning and
cloud-based distributed training, operate on unreliable devices with unstable
network bandwidth. As a result, these applications are restricted to using
parameter servers or gossip-based averaging protocols. In this work, we lift
that restriction by proposing Moshpit All-Reduce - an iterative averaging
protocol that exponentially converges to the global average. We demonstrate the
efficiency of our protocol for distributed optimization with strong theoretical
guarantees. The experiments show 1.3x speedup for ResNet-50 training on
ImageNet compared to competitive gossip-based strategies and 1.5x speedup when
training ALBERT-large from scratch using preemptible compute nodes.

    

### [[2103.07206] Medical data wrangling with sequential variational autoencoders](http://arxiv.org/abs/2103.07206)


  Medical data sets are usually corrupted by noise and missing data. These
missing patterns are commonly assumed to be completely random, but in medical
scenarios, the reality is that these patterns occur in bursts due to sensors
that are off for some time or data collected in a misaligned uneven fashion,
among other causes. This paper proposes to model medical data records with
heterogeneous data types and bursty missing data using sequential variational
autoencoders (VAEs). In particular, we propose a new methodology, the Shi-VAE,
which extends the capabilities of VAEs to sequential streams of data with
missing observations. We compare our model against state-of-the-art solutions
in an intensive care unit database (ICU) and a dataset of passive human
monitoring. Furthermore, we find that standard error metrics such as RMSE are
not conclusive enough to assess temporal models and include in our analysis the
cross-correlation between the ground truth and the imputed signal. We show that
Shi-VAE achieves the best performance in terms of using both metrics, with
lower computational complexity than the GP-VAE model, which is the
state-of-the-art method for medical records.

    

### [[2103.07853] Membership Inference Attacks on Machine Learning: A Survey](http://arxiv.org/abs/2103.07853)


  Machine learning (ML) models have been widely applied to various
applications, including image classification, text generation, audio
recognition, and graph data analysis. However, recent studies have shown that
ML models are vulnerable to membership inference attacks (MIAs), which aim to
infer whether a data record was used to train a target model or not. MIAs on ML
models can directly lead to a privacy breach. For example, via identifying the
fact that a clinical record that has been used to train a model associated with
a certain disease, an attacker can infer that the owner of the clinical record
has the disease with a high chance. In recent years, MIAs have been shown to be
effective on various ML models, e.g., classification models and generative
models. Meanwhile, many defense methods have been proposed to mitigate MIAs.
Although MIAs on ML models form a newly emerging and rapidly growing research
area, there has been no systematic survey on this topic yet. In this paper, we
conduct the first comprehensive survey on membership inference attacks and
defenses. We provide the taxonomies for both attacks and defenses, based on
their characterizations, and discuss their pros and cons. Based on the
limitations and gaps identified in this survey, we point out several promising
future research directions to inspire the researchers who wish to follow this
area. This survey not only serves as a reference for the research community but
also brings a clear picture to researchers outside this research domain. To
further facilitate the researchers, we have created an online resource
repository and keep updating it with the future relevant works. Interested
readers can find the repository at
this https URL.

    

### [[2103.08414] Online Learning with Radial Basis Function Networks](http://arxiv.org/abs/2103.08414)


  We investigate the benefits of feature selection, nonlinear modelling and
online learning when forecasting in financial time series. We consider the
sequential and continual learning sub-genres of online learning. The
experiments we conduct show that there is a benefit to online transfer
learning, in the form of radial basis function networks, beyond the sequential
updating of recursive least-squares models. We show that the radial basis
function networks, which make use of clustering algorithms to construct a
kernel Gram matrix, are more beneficial than treating each training vector as
separate basis functions, as occurs with kernel Ridge regression. We
demonstrate quantitative procedures to determine the very structure of the
radial basis function networks. Finally, we conduct experiments on the log
returns of financial time series and show that the online learning models,
particularly the radial basis function networks, are able to outperform a
random walk baseline, whereas the offline learning models struggle to do so.

    

### [[2103.14110] Robust Data-Driven Predictive Control using Reachability Analysis](http://arxiv.org/abs/2103.14110)


  We present a robust data-driven control scheme for an unknown linear system
model with bounded process and measurement noise. Instead of depending on a
system model in traditional predictive control, a controller utilizing
data-driven reachable regions is proposed. The data-driven reachable regions
are based on a matrix zonotope recursion and are computed based on only noisy
input-output data of a trajectory of the system. We assume that measurement and
process noise are contained in bounded sets. While we assume knowledge of these
bounds, no knowledge about the statistical properties of the noise is assumed.
In the noise-free case, we prove that the presented purely data-driven control
scheme results in an equivalent closed-loop behavior to a nominal model
predictive control scheme. In the case of measurement and process noise, our
proposed scheme guarantees robust constraint satisfaction, which is essential
in safety-critical applications. Numerical experiments show the effectiveness
of the proposed data-driven controller in comparison to model-based control
schemes.

    

### [[2103.14749] Pervasive Label Errors in Test Sets Destabilize Machine Learning Benchmarks](http://arxiv.org/abs/2103.14749)


  We identify label errors in the test sets of 10 of the most commonly-used
computer vision, natural language, and audio datasets, and subsequently study
the potential for these label errors to affect benchmark results. Errors in
test sets are numerous and widespread: we estimate an average of at least 3.3%
errors across the 10 datasets, where for example label errors comprise at least
6% of the ImageNet validation set. Putative label errors are identified using
confident learning algorithms and then human-validated via crowdsourcing (51%
of the algorithmically-flagged candidates are indeed erroneously labeled, on
average across the datasets). Traditionally, machine learning practitioners
choose which model to deploy based on test accuracy - our findings advise
caution here, proposing that judging models over correctly labeled test sets
may be more useful, especially for noisy real-world datasets. Surprisingly, we
find that lower capacity models may be practically more useful than higher
capacity models in real-world datasets with high proportions of erroneously
labeled data. For example, on ImageNet with corrected labels: ResNet-18
outperforms ResNet-50 if the prevalence of originally mislabeled test examples
increases by just 6%. On CIFAR-10 with corrected labels: VGG-11 outperforms
VGG-19 if the prevalence of originally mislabeled test examples increases by
just 5%. Test set errors across the 10 datasets can be viewed at
this https URL and all label errors can be reproduced by
this https URL.

    

### [[2103.15996] Minimum complexity interpolation in random features models](http://arxiv.org/abs/2103.15996)


  Despite their many appealing properties, kernel methods are heavily affected
by the curse of dimensionality. For instance, in the case of inner product
kernels in $\mathbb{R}^d$, the Reproducing Kernel Hilbert Space (RKHS) norm is
often very large for functions that depend strongly on a small subset of
directions (ridge functions). Correspondingly, such functions are difficult to
learn using kernel methods. This observation has motivated the study of
generalizations of kernel methods, whereby the RKHS norm -- which is equivalent
to a weighted $\ell_2$ norm -- is replaced by a weighted functional $\ell_p$
norm, which we refer to as $\mathcal{F}_p$ norm. Unfortunately, tractability of
these approaches is unclear. The kernel trick is not available and minimizing
these norms requires to solve an infinite-dimensional convex problem.
We study random features approximations to these norms and show that, for
$p>1$, the number of random features required to approximate the original
learning problem is upper bounded by a polynomial in the sample size. Hence,
learning with $\mathcal{F}_p$ norms is tractable in these cases. We introduce a
proof technique based on uniform concentration in the dual, which can be of
broader interest in the study of overparametrized models. For $p= 1$, our
guarantees for the random features approximation break down. We prove instead
that learning with the $\mathcal{F}_1$ norm is $\mathsf{NP}$-hard under a
randomized reduction based on the problem of learning halfspaces with noise.

    

### [[2104.00671] TRS: Transferability Reduced Ensemble via Encouraging Gradient Diversity and Model Smoothness](http://arxiv.org/abs/2104.00671)


  Adversarial Transferability is an intriguing property - adversarial
perturbation crafted against one model is also effective against another model,
while these models are from different model families or training processes. To
better protect ML systems against adversarial attacks, several questions are
raised: what are the sufficient conditions for adversarial transferability and
how to bound it? Is there a way to reduce the adversarial transferability in
order to improve the robustness of an ensemble ML model? To answer these
questions, in this work we first theoretically analyze and outline sufficient
conditions for adversarial transferability between models; then propose a
practical algorithm to reduce the transferability between base models within an
ensemble to improve its robustness. Our theoretical analysis shows that only
promoting the orthogonality between gradients of base models is not enough to
ensure low transferability; in the meantime, the model smoothness is an
important factor to control the transferability. We also provide the lower and
upper bounds of adversarial transferability under certain conditions. Inspired
by our theoretical analysis, we propose an effective Transferability Reduced
Smooth(TRS) ensemble training strategy to train a robust ensemble with low
transferability by enforcing both gradient orthogonality and model smoothness
between base models. We conduct extensive experiments on TRS and compare with 6
state-of-the-art ensemble baselines against 8 whitebox attacks on different
datasets, demonstrating that the proposed TRS outperforms all baselines
significantly.

    

### [[2104.04883] Graph Representation Learning in Biomedicine](http://arxiv.org/abs/2104.04883)


  Biomedical networks are universal descriptors of systems of interacting
elements, from protein interactions to disease networks, all the way to
healthcare systems and scientific knowledge. With the remarkable success of
representation learning in providing powerful predictions and insights, we have
witnessed a rapid expansion of representation learning techniques into
modeling, analyzing, and learning with such networks. In this review, we put
forward an observation that long-standing principles of networks in biology and
medicine -- while often unspoken in machine learning research -- can provide
the conceptual grounding for representation learning, explain its current
successes and limitations, and inform future advances. We synthesize a spectrum
of algorithmic approaches that, at their core, leverage graph topology to embed
networks into compact vector spaces, and capture the breadth of ways in which
representation learning is proving useful. Areas of profound impact include
identifying variants underlying complex traits, disentangling behaviors of
single cells and their effects on health, assisting in diagnosis and treatment
of patients, and developing safe and effective medicines.

    

### [[2104.08762] Case-based Reasoning for Natural Language Queries over Knowledge Bases](http://arxiv.org/abs/2104.08762)


  It is often challenging to solve a complex problem from scratch, but much
easier if we can access other similar problems with their solutions -- a
paradigm known as case-based reasoning (CBR). We propose a neuro-symbolic CBR
approach (CBR-KBQA) for question answering over large knowledge bases. CBR-KBQA
consists of a nonparametric memory that stores cases (question and logical
forms) and a parametric model that can generate a logical form for a new
question by retrieving cases that are relevant to it. On several KBQA datasets
that contain complex questions, CBR-KBQA achieves competitive performance. For
example, on the ComplexWebQuestions dataset, CBR-KBQA outperforms the current
state of the art by 11\% on accuracy. Furthermore, we show that CBR-KBQA is
capable of using new cases \emph{without} any further training: by
incorporating a few human-labeled examples in the case memory, CBR-KBQA is able
to successfully generate logical forms containing unseen KB entities as well as
relations.

    

### [[2104.08955] Many-Speakers Single Channel Speech Separation with Optimal Permutation Training](http://arxiv.org/abs/2104.08955)


  Single channel speech separation has experienced great progress in the last
few years. However, training neural speech separation for a large number of
speakers (e.g., more than 10 speakers) is out of reach for the current methods,
which rely on the Permutation Invariant Loss (PIT). In this work, we present a
permutation invariant training that employs the Hungarian algorithm in order to
train with an $O(C^3)$ time complexity, where $C$ is the number of speakers, in
comparison to $O(C!)$ of PIT based methods. Furthermore, we present a modified
architecture that can handle the increased number of speakers. Our approach
separates up to $20$ speakers and improves the previous results for large $C$
by a wide margin.

    

### [[2104.13619] Reconstructing nodal pressures in water distribution systems with graph neural networks](http://arxiv.org/abs/2104.13619)


  Knowing the pressure at all times in each node of a water distribution system
(WDS) facilitates safe and efficient operation. Yet, complete measurement data
cannot be collected due to the limited number of instruments in a real-life
WDS. The data-driven methodology of reconstructing all the nodal pressures by
observing only a limited number of nodes is presented in the paper. The
reconstruction method is based on K-localized spectral graph filters, wherewith
graph convolution on water networks is possible. The effect of the number of
layers, layer depth and the degree of the Chebyshev-polynomial applied in the
kernel is discussed taking into account the peculiarities of the application.
In addition, a weighting method is shown, wherewith information on friction
loss can be embed into the spectral graph filters through the adjacency matrix.
The performance of the proposed model is presented on 3 WDSs at different
number of nodes observed compared to the total number of nodes. The weighted
connections prove no benefit over the binary connections, but the proposed
model reconstructs the nodal pressure with at most 5% relative error on average
at an observation ratio of 5% at least. The results are achieved with shallow
graph neural networks by following the considerations discussed in the paper.

    

### [[2104.14028] Analysis of Legal Documents via Non-negative Matrix Factorization Methods](http://arxiv.org/abs/2104.14028)


  The California Innocence Project (CIP), a clinical law school program aiming
to free wrongfully convicted prisoners, evaluates thousands of mails containing
new requests for assistance and corresponding case files. Processing and
interpreting this large amount of information presents a significant challenge
for CIP officials, which can be successfully aided by topic modeling
this http URL this paper, we apply Non-negative Matrix Factorization (NMF)
method and implement various offshoots of it to the important and previously
unstudied data set compiled by CIP. We identify underlying topics of existing
case files and classify request files by crime type and case status (decision
type). The results uncover the semantic structure of current case files and can
provide CIP officials with a general understanding of newly received case files
before further examinations. We also provide an exposition of popular variants
of NMF with their experimental results and discuss the benefits and drawbacks
of each variant through the real-world application.

    

### [[2105.00303] RATT: Leveraging Unlabeled Data to Guarantee Generalization](http://arxiv.org/abs/2105.00303)


  To assess generalization, machine learning scientists typically either (i)
bound the generalization gap and then (after training) plug in the empirical
risk to obtain a bound on the true risk; or (ii) validate empirically on
holdout data. However, (i) typically yields vacuous guarantees for
overparameterized models. Furthermore, (ii) shrinks the training set and its
guarantee erodes with each re-use of the holdout set. In this paper, we
introduce a method that leverages unlabeled data to produce generalization
bounds. After augmenting our (labeled) training set with randomly labeled fresh
examples, we train in the standard fashion. Whenever classifiers achieve low
error on clean data and high error on noisy data, our bound provides a tight
upper bound on the true risk. We prove that our bound is valid for 0-1
empirical risk minimization and with linear classifiers trained by gradient
descent. Our approach is especially useful in conjunction with deep learning
due to the early learning phenomenon whereby networks fit true labels before
noisy labels but requires one intuitive assumption. Empirically, on canonical
computer vision and NLP tasks, our bound provides non-vacuous generalization
guarantees that track actual performance closely. This work provides
practitioners with an option for certifying the generalization of deep nets
even when unseen labeled data is unavailable and provides theoretical insights
into the relationship between random label noise and generalization.

    

### [[2105.00336] Comprehensive Review On Twin Support Vector Machines](http://arxiv.org/abs/2105.00336)


  Twin support vector machine (TWSVM) and twin support vector regression (TSVR)
are newly emerging efficient machine learning techniques which offer promising
solutions for classification and regression challenges respectively. TWSVM is
based upon the idea to identify two nonparallel hyperplanes which classify the
data points to their respective classes. It requires to solve two small sized
quadratic programming problems (QPPs) in lieu of solving single large size QPP
in support vector machine (SVM) while TSVR is formulated on the lines of TWSVM
and requires to solve two SVM kind problems. Although there has been good
research progress on these techniques; there is limited literature on the
comparison of different variants of TSVR. Thus, this review presents a rigorous
analysis of recent research in TWSVM and TSVR simultaneously mentioning their
limitations and advantages. To begin with we first introduce the basic theory
of support vector machine, TWSVM and then focus on the various improvements and
applications of TWSVM, and then we introduce TSVR and its various enhancements.
Finally, we suggest future research and development prospects.

    

### [[2105.01281] Citadel: Protecting Data Privacy and Model Confidentiality for Collaborative Learning with SGX](http://arxiv.org/abs/2105.01281)


  With the advancement of machine learning (ML) and its growing awareness, many
organizations who own data but not ML expertise (data owner) would like to pool
their data and collaborate with those who have expertise but need data from
diverse sources to train truly generalizable models (model owner). In such
collaborative ML, the data owner wants to protect the privacy of its training
data, while the model owner desires the confidentiality of the model and the
training method which may contain intellectual properties. However, existing
private ML solutions, such as federated learning and split learning, cannot
meet the privacy requirements of both data and model owners at the same time.
This paper presents Citadel, a scalable collaborative ML system that protects
the privacy of both data owner and model owner in untrusted infrastructures
with the help of Intel SGX. Citadel performs distributed training across
multiple training enclaves running on behalf of data owners and an aggregator
enclave on behalf of the model owner. Citadel further establishes a strong
information barrier between these enclaves by means of zero-sum masking and
hierarchical aggregation to prevent data/model leakage during collaborative
training. Compared with the existing SGX-protected training systems, Citadel
enables better scalability and stronger privacy guarantees for collaborative
ML. Cloud deployment with various ML models shows that Citadel scales to a
large number of enclaves with less than 1.73X slowdown caused by SGX.

    

### [[2105.05821] SimNet: Accurate and High-Performance Computer Architecture Simulation using Machine Learning](http://arxiv.org/abs/2105.05821)


  While discrete-event simulators are essential tools for architecture
research, design, and development, their practicality is limited by an
extremely long time-to-solution for realistic applications under investigation.
This work describes a concerted effort, where machine learning (ML) is used to
accelerate discrete-event simulation. First, an ML-based instruction latency
prediction framework that accounts for both static instruction properties and
dynamic processor states is constructed. Then, a GPU-accelerated parallel
simulator is implemented based on the proposed instruction latency predictor,
and its simulation accuracy and throughput are validated and evaluated against
a state-of-the-art simulator. Leveraging modern GPUs, the ML-based simulator
outperforms traditional simulators significantly.

    

### [[2105.07033] Cause and Effect: Hierarchical Concept-based Explanation of Neural Networks](http://arxiv.org/abs/2105.07033)


  In many scenarios, human decisions are explained based on some high-level
concepts. In this work, we take a step in the interpretability of neural
networks by examining their internal representation or neuron's activations
against concepts. A concept is characterized by a set of samples that have
specific features in common. We propose a framework to check the existence of a
causal relationship between a concept (or its negation) and task classes. While
the previous methods focus on the importance of a concept to a task class, we
go further and introduce four measures to quantitatively determine the order of
causality. Moreover, we propose a method for constructing a hierarchy of
concepts in the form of a concept-based decision tree which can shed light on
how various concepts interact inside a neural network towards predicting output
classes. Through experiments, we demonstrate the effectiveness of the proposed
method in explaining the causal relationship between a concept and the
predictive behaviour of a neural network as well as determining the
interactions between different concepts through constructing a concept
hierarchy.

    

### [[2105.08339] DRIVE: One-bit Distributed Mean Estimation](http://arxiv.org/abs/2105.08339)


  We consider the problem where $n$ clients transmit $d$-dimensional
real-valued vectors using $d(1+o(1))$ bits each, in a manner that allows the
receiver to approximately reconstruct their mean. Such compression problems
naturally arise in distributed and federated learning. We provide novel
mathematical results and derive computationally efficient algorithms that are
more accurate than previous compression techniques. We evaluate our methods on
a collection of distributed and federated learning tasks, using a variety of
datasets, and show a consistent improvement over the state of the art.

    

### [[2105.08619] On the Robustness of Domain Constraints](http://arxiv.org/abs/2105.08619)


  Machine learning is vulnerable to adversarial examples-inputs designed to
cause models to perform poorly. However, it is unclear if adversarial examples
represent realistic inputs in the modeled domains. Diverse domains such as
networks and phishing have domain constraints-complex relationships between
features that an adversary must satisfy for an attack to be realized (in
addition to any adversary-specific goals). In this paper, we explore how domain
constraints limit adversarial capabilities and how adversaries can adapt their
strategies to create realistic (constraint-compliant) examples. In this, we
develop techniques to learn domain constraints from data, and show how the
learned constraints can be integrated into the adversarial crafting process. We
evaluate the efficacy of our approach in network intrusion and phishing
datasets and find: (1) up to 82% of adversarial examples produced by
state-of-the-art crafting algorithms violate domain constraints, (2) domain
constraints are robust to adversarial examples; enforcing constraints yields an
increase in model accuracy by up to 34%. We observe not only that adversaries
must alter inputs to satisfy domain constraints, but that these constraints
make the generation of valid adversarial examples far more challenging.

    

### [[2105.08923] Reinforcement Learning Assisted Oxygen Therapy for COVID-19 Patients Under Intensive Care](http://arxiv.org/abs/2105.08923)


  Patients with severe Coronavirus disease 19 (COVID-19) typically require
supplemental oxygen as an essential treatment. We developed a machine learning
algorithm, based on a deep Reinforcement Learning (RL), for continuous
management of oxygen flow rate for critical ill patients under intensive care,
which can identify the optimal personalized oxygen flow rate with strong
potentials to reduce mortality rate relative to the current clinical practice.
Basically, we modeled the oxygen flow trajectory of COVID-19 patients and their
health outcomes as a Markov decision process. Based on individual patient
characteristics and health status, a reinforcement learning based oxygen
control policy is learned and real-time recommends the oxygen flow rate to
reduce the mortality rate. We assessed the performance of proposed methods
through cross validation by using a retrospective cohort of 1,372 critically
ill patients with COVID-19 from New York University Langone Health ambulatory
care with electronic health records from April 2020 to January 2021. The mean
mortality rate under the RL algorithm is lower than standard of care by 2.57%
(95% CI: 2.08- 3.06) reduction (P<0.001) from 7.94% under the standard of care
to 5.37 % under our algorithm and the averaged recommended oxygen flow rate is
1.28 L/min (95% CI: 1.14-1.42) lower than the rate actually delivered to
patients. Thus, the RL algorithm could potentially lead to better intensive
care treatment that can reduce mortality rate, while saving the oxygen scarce
resources. It can reduce the oxygen shortage issue and improve public health
during the COVID-19 pandemic.

    

### [[2105.10389] Learning Visible Connectivity Dynamics for Cloth Smoothing](http://arxiv.org/abs/2105.10389)


  Robotic manipulation of cloth remains challenging for robotics due to the
complex dynamics of the cloth, lack of a low-dimensional state representation,
and self-occlusions. In contrast to previous model-based approaches that learn
a pixel-based dynamics model or a compressed latent vector dynamics, we propose
to learn a particle-based dynamics model from a partial point cloud
observation. To overcome the challenges of partial observability, we infer
which visible points are connected on the underlying cloth mesh. We then learn
a dynamics model over this visible connectivity graph. Compared to previous
learning-based approaches, our model poses strong inductive bias with its
particle based representation for learning the underlying cloth physics; it is
invariant to visual features; and the predictions can be more easily
visualized. We show that our method greatly outperforms previous
state-of-the-art model-based and model-free reinforcement learning methods in
simulation. Furthermore, we demonstrate zero-shot sim-to-real transfer where we
deploy the model trained in simulation on a Franka arm and show that the model
can successfully smooth different types of cloth from crumpled configurations.
Videos can be found on our project website.

    

### [[2105.11853] Quantum Embedding Search for Quantum Machine Learning](http://arxiv.org/abs/2105.11853)


  This paper introduces a novel quantum embedding search algorithm (QES,
pronounced as "quest"), enabling search for optimal quantum embedding design
for a specific dataset of interest. First, we establish the connection between
the structures of quantum embedding and the representations of directed
multi-graphs, enabling a well-defined search space. Second, we instigate the
entanglement level to reduce the cardinality of the search space to a feasible
size for practical implementations. Finally, we mitigate the cost of evaluating
the true loss function by using surrogate models via sequential model-based
optimization. We demonstrate the feasibility of our proposed approach on
synthesis and Iris datasets, which empirically shows that found quantum
embedding architecture by QES outperforms manual designs whereas achieving
comparable performance to classical machine learning models.

    

### [[2105.12564] Predicting invasive ductal carcinoma using a Reinforcement Sample Learning Strategy using Deep Learning](http://arxiv.org/abs/2105.12564)


  Invasive ductal carcinoma is a prevalent, potentially deadly disease
associated with a high rate of morbidity and mortality. Its malignancy is the
second leading cause of death from cancer in women. The mammogram is an
extremely useful resource for mass detection and invasive ductal carcinoma
diagnosis. We are proposing a method for Invasive ductal carcinoma that will
use convolutional neural networks (CNN) on mammograms to assist radiologists in
diagnosing the disease. Due to the varying image clarity and structure of
certain mammograms, it is difficult to observe major cancer characteristics
such as microcalcification and mass, and it is often difficult to interpret and
diagnose these attributes. The aim of this study is to establish a novel method
for fully automated feature extraction and classification in invasive ductal
carcinoma computer-aided diagnosis (CAD) systems. This article presents a tumor
classification algorithm that makes novel use of convolutional neural networks
on breast mammogram images to increase feature extraction and training speed.
The algorithm makes two contributions.

    

### [[2105.14035] DeepMoM: Robust Deep Learning With Median-of-Means](http://arxiv.org/abs/2105.14035)


  Data used in deep learning is notoriously problematic. For example, data are
usually combined from diverse sources, rarely cleaned and vetted thoroughly,
and sometimes corrupted on purpose. Intentional corruption that targets the
weak spots of algorithms has been studied extensively under the label of
"adversarial attacks." In contrast, the arguably much more common case of
corruption that reflects the limited quality of data has been studied much
less. Such "random" corruptions are due to measurement errors, unreliable
sources, convenience sampling, and so forth. These kinds of corruption are
common in deep learning, because data are rarely collected according to strict
protocols -- in strong contrast to the formalized data collection in some parts
of classical statistics. This paper concerns such corruption. We introduce an
approach motivated by very recent insights into median-of-means and Le Cam's
principle, we show that the approach can be readily implemented, and we
demonstrate that it performs very well in practice. In conclusion, we believe
that our approach is a very promising alternative to standard parameter
training based on least-squares and cross-entropy loss.

    

### [[2106.02700] A Discrete Variational Derivation of Accelerated Methods in Optimization](http://arxiv.org/abs/2106.02700)


  Many of the new developments in machine learning are connected with
gradient-based optimization methods. Recently, these methods have been studied
using a variational perspective. This has opened up the possibility of
introducing variational and symplectic methods using geometric integration. In
particular, in this paper, we introduce variational integrators which allow us
to derive different methods for optimization. Using both, Hamilton's and
Lagrange-d'Alembert's principle, we derive two families of respective
optimization methods in one-to-one correspondence that generalize Polyak's
heavy ball and the well known Nesterov accelerated gradient method, the second
of which mimics the behavior of the first reducing the oscillations of
classical momentum methods. However, since the systems considered are
explicitly time-dependent, the preservation of symplecticity of autonomous
systems occurs here solely on the fibers. Several experiments exemplify the
result.

    

### [[2106.03314] Measuring Generalization with Optimal Transport](http://arxiv.org/abs/2106.03314)


  Understanding the generalization of deep neural networks is one of the most
important tasks in deep learning. Although much progress has been made,
theoretical error bounds still often behave disparately from empirical
observations. In this work, we develop margin-based generalization bounds,
where the margins are normalized with optimal transport costs between
independent random subsets sampled from the training distribution. In
particular, the optimal transport cost can be interpreted as a generalization
of variance which captures the structural properties of the learned feature
space. Our bounds robustly predict the generalization error, given training
data and network parameters, on large scale datasets. Theoretically, we
demonstrate that the concentration and separation of features play crucial
roles in generalization, supporting empirical results in the literature. The
code is available at \url{this https URL}.

    

### [[2106.04166] Incorporating NODE with Pre-trained Neural Differential Operator for Learning Dynamics](http://arxiv.org/abs/2106.04166)


  Learning dynamics governed by differential equations is crucial for
predicting and controlling the systems in science and engineering. Neural
Ordinary Differential Equation (NODE), a deep learning model integrated with
differential equations, is popular in learning dynamics recently due to its
robustness to irregular samples and its flexibility to high-dimensional input.
However, the training of NODE is sensitive to the precision of the numerical
solver, which makes the convergence of NODE unstable, especially for
ill-conditioned dynamical systems. In this paper, to reduce the reliance on the
numerical solver, we propose to enhance the supervised signal in the training
of NODE. Specifically, we pre-train a neural differential operator (NDO) to
output an estimation of the derivatives to serve as an additional supervised
signal. The NDO is pre-trained on a class of basis functions and learns the
mapping between the trajectory samples of these functions to their derivatives.
To leverage both the trajectory signal and the estimated derivatives from NDO,
we propose an algorithm called NDO-NODE, in which the loss function contains
two terms: the fitness on the true trajectory samples and the fitness on the
estimated derivatives that are outputted by the pre-trained NDO. Experiments on
various kinds of dynamics show that our proposed NDO-NODE can consistently
improve the forecasting accuracy with one pre-trained NDO. Especially for the
stiff ODEs, we observe that NDO-NODE can capture the transitions in the
dynamics more accurately compared with other regularization methods.

    

### [[2106.04496] Towards a Theoretical Framework of Out-of-Distribution Generalization](http://arxiv.org/abs/2106.04496)


  Generalization to out-of-distribution (OOD) data is one of the central
problems in modern machine learning. Recently, there is a surge of attempts to
propose algorithms that mainly build upon the idea of extracting invariant
features. Although intuitively reasonable, theoretical understanding of what
kind of invariance can guarantee OOD generalization is still limited, and
generalization to arbitrary out-of-distribution is clearly impossible. In this
work, we take the first step towards rigorous and quantitative definitions of
1) what is OOD; and 2) what does it mean by saying an OOD problem is learnable.
We also introduce a new concept of expansion function, which characterizes to
what extent the variance is amplified in the test domains over the training
domains, and therefore give a quantitative meaning of invariant features. Based
on these, we prove OOD generalization error bounds. It turns out that OOD
generalization largely depends on the expansion function. As recently pointed
out by Gulrajani and Lopez-Paz (2020), any OOD learning algorithm without a
model selection module is incomplete. Our theory naturally induces a model
selection criterion. Extensive experiments on benchmark OOD datasets
demonstrate that our model selection criterion has a significant advantage over
baselines.

    

### [[2106.06529] The Limitations of Large Width in Neural Networks: A Deep Gaussian Process Perspective](http://arxiv.org/abs/2106.06529)


  Large width limits have been a recent focus of deep learning research: modulo
computational practicalities, do wider networks outperform narrower ones?
Answering this question has been challenging, as conventional networks gain
representational power with width, potentially masking any negative effects.
Our analysis in this paper decouples capacity and width via the generalization
of neural networks to Deep Gaussian Processes (Deep GP), a class of
nonparametric hierarchical models that subsume neural nets. In doing so, we aim
to understand how width affects (standard) neural networks once they have
sufficient capacity for a given modeling task. Our theoretical and empirical
results on Deep GP suggest that large width can be detrimental to hierarchical
models. Surprisingly, we prove that even nonparametric Deep GP converge to
Gaussian processes, effectively becoming shallower without any increase in
representational power. The posterior, which corresponds to a mixture of
data-adaptable basis functions, becomes less data-dependent with width. Our
tail analysis demonstrates that width and depth have opposite effects: depth
accentuates a model's non-Gaussianity, while width makes models increasingly
Gaussian. We find there is a "sweet spot" that maximizes test performance
before the limiting GP behavior prevents adaptability, occurring at width = 1
or width = 2 for nonparametric Deep GP. These results make strong predictions
about the same phenomenon in conventional neural networks trained with L2
regularization (analogous to a Gaussian prior on parameters): we show that such
neural networks may need up to 500 - 1000 hidden units for sufficient capacity
- depending on the dataset - but further width degrades performance.

    

### [[2106.07539] On the Representation of Solutions to Elliptic PDEs in Barron Spaces](http://arxiv.org/abs/2106.07539)


  Numerical solutions to high-dimensional partial differential equations (PDEs)
based on neural networks have seen exciting developments. This paper derives
complexity estimates of the solutions of $d$-dimensional second-order elliptic
PDEs in the Barron space, that is a set of functions admitting the integral of
certain parametric ridge function against a probability measure on the
parameters. We prove under some appropriate assumptions that if the
coefficients and the source term of the elliptic PDE lie in Barron spaces, then
the solution of the PDE is $\epsilon$-close with respect to the $H^1$ norm to a
Barron function. Moreover, we prove dimension-explicit bounds for the Barron
norm of this approximate solution, depending at most polynomially on the
dimension $d$ of the PDE. As a direct consequence of the complexity estimates,
the solution of the PDE can be approximated on any bounded domain by a
two-layer neural network with respect to the $H^1$ norm with a
dimension-explicit convergence rate.

    

### [[2106.07718] HUMAP: Hierarchical Uniform Manifold Approximation and Projection](http://arxiv.org/abs/2106.07718)


  Dimensionality reduction (DR) techniques help analysts to understand patterns
in high-dimensional spaces. These techniques, often represented by scatter
plots, are employed in diverse science domains and facilitate similarity
analysis among clusters and data samples. For datasets containing many
granularities or when analysis follows the information visualization mantra,
hierarchical DR techniques are the most suitable approach since they present
major structures beforehand and details on demand. However, current
hierarchical DR techniques are not fully capable of addressing literature
problems because they do not preserve the projection mental map across
hierarchical levels or are not suitable for most data types. This work presents
HUMAP, a novel hierarchical dimensionality reduction technique designed to be
flexible on preserving local and global structures and preserve the mental map
throughout hierarchical exploration. We provide empirical evidence of our
technique's superiority compared with current hierarchical approaches and show
two case studies to demonstrate its strengths.

    

### [[2106.07754] Counterfactual Explanations as Interventions in Latent Space](http://arxiv.org/abs/2106.07754)


  Explainable Artificial Intelligence (XAI) is a set of techniques that allows
the understanding of both technical and non-technical aspects of Artificial
Intelligence (AI) systems. XAI is crucial to help satisfying the increasingly
important demand of \emph{trustworthy} Artificial Intelligence, characterized
by fundamental characteristics such as respect of human autonomy, prevention of
harm, transparency, accountability, etc. Within XAI techniques, counterfactual
explanations aim to provide to end users a set of features (and their
corresponding values) that need to be changed in order to achieve a desired
outcome. Current approaches rarely take into account the feasibility of actions
needed to achieve the proposed explanations, and in particular they fall short
of considering the causal impact of such actions. In this paper, we present
Counterfactual Explanations as Interventions in Latent Space (CEILS), a
methodology to generate counterfactual explanations capturing by design the
underlying causal relations from the data, and at the same time to provide
feasible recommendations to reach the proposed profile. Moreover, our
methodology has the advantage that it can be set on top of existing
counterfactuals generator algorithms, thus minimising the complexity of
imposing additional causal constrains. We demonstrate the effectiveness of our
approach with a set of different experiments using synthetic and real datasets
(including a proprietary dataset of the financial domain).

    

### [[2106.08895] Masked Training of Neural Networks with Partial Gradients](http://arxiv.org/abs/2106.08895)


  State-of-the-art training algorithms for deep learning models are based on
stochastic gradient descent (SGD). Recently, many variations have been
explored: perturbing parameters for better accuracy (such as in Extragradient),
limiting SGD updates to a subset of parameters for increased efficiency (such
as meProp) or a combination of both (such as Dropout). However, the convergence
of these methods is often not studied in theory. We propose a unified
theoretical framework to study such SGD variants -- encompassing the
aforementioned algorithms and additionally a broad variety of methods used for
communication efficient training or model compression. Our insights can be used
as a guide to improve the efficiency of such methods and facilitate
generalization to new applications. As an example, we tackle the task of
jointly training networks, a version of which (limited to sub-networks) is used
to create Slimmable Networks. By training a low-rank Transformer jointly with a
standard one we obtain superior performance than when it is trained separately.

    

### [[2106.09129] A Winning Hand: Compressing Deep Networks Can Improve Out-Of-Distribution Robustness](http://arxiv.org/abs/2106.09129)


  Successful adoption of deep learning (DL) in the wild requires models to be:
(1) compact, (2) accurate, and (3) robust to distributional shifts.
Unfortunately, efforts towards simultaneously meeting these requirements have
mostly been unsuccessful. This raises an important question: Is the inability
to create Compact, Accurate, and Robust Deep neural networks (CARDs)
fundamental? To answer this question, we perform a large-scale analysis of
popular model compression techniques which uncovers several intriguing
patterns. Notably, in contrast to traditional pruning approaches (e.g., fine
tuning and gradual magnitude pruning), we find that "lottery ticket-style"
approaches can surprisingly be used to produce CARDs, including binary-weight
CARDs. Specifically, we are able to create extremely compact CARDs that,
compared to their larger counterparts, have similar test accuracy and matching
(or better) robustness -- simply by pruning and (optionally) quantizing.
Leveraging the compactness of CARDs, we develop a simple domain-adaptive
test-time ensembling approach (CARD-Decks) that uses a gating module to
dynamically select appropriate CARDs from the CARD-Deck based on their
spectral-similarity with test samples. The proposed approach builds a "winning
hand'' of CARDs that establishes a new state-of-the-art (on RobustBench) on
CIFAR-10-C accuracies (i.e., 96.8% standard and 92.75% robust) and CIFAR-100-C
accuracies (80.6% standard and 71.3% robust) with better memory usage than
non-compressed baselines (pretrained CARDs and CARD-Decks available at
this https URL). Finally, we provide theoretical
support for our empirical findings.

    

### [[2106.09146] Contrastive Reinforcement Learning of Symbolic Reasoning Domains](http://arxiv.org/abs/2106.09146)


  Abstract symbolic reasoning, as required in domains such as mathematics and
logic, is a key component of human intelligence. Solvers for these domains have
important applications, especially to computer-assisted education. But learning
to solve symbolic problems is challenging for machine learning algorithms.
Existing models either learn from human solutions or use hand-engineered
features, making them expensive to apply in new domains. In this paper, we
instead consider symbolic domains as simple environments where states and
actions are given as unstructured text, and binary rewards indicate whether a
problem is solved. This flexible setup makes it easy to specify new domains,
but search and planning become challenging. We introduce four environments
inspired by the Mathematics Common Core Curriculum, and observe that existing
Reinforcement Learning baselines perform poorly. We then present a novel
learning algorithm, Contrastive Policy Learning (ConPoLe) that explicitly
optimizes the InfoNCE loss, which lower bounds the mutual information between
the current state and next states that continue on a path to the solution.
ConPoLe successfully solves all four domains. Moreover, problem representations
learned by ConPoLe enable accurate prediction of the categories of problems in
a real mathematics curriculum. Our results suggest new directions for
reinforcement learning in symbolic domains, as well as applications to
mathematics education.

    

### [[2106.09524] Implicit Bias of SGD for Diagonal Linear Networks: a Provable Benefit of Stochasticity](http://arxiv.org/abs/2106.09524)


  Understanding the implicit bias of training algorithms is of crucial
importance in order to explain the success of overparametrised neural networks.
In this paper, we study the dynamics of stochastic gradient descent over
diagonal linear networks through its continuous time version, namely stochastic
gradient flow. We explicitly characterise the solution chosen by the stochastic
flow and prove that it always enjoys better generalisation properties than that
of gradient flow. Quite surprisingly, we show that the convergence speed of the
training loss controls the magnitude of the biasing effect: the slower the
convergence, the better the bias. To fully complete our analysis, we provide
convergence guarantees for the dynamics. We also give experimental results
which support our theoretical claims. Our findings highlight the fact that
structured noise can induce better generalisation and they help explain the
greater performances observed in practice of stochastic gradient descent over
gradient descent.

    

### [[2106.10207] Distributed Deep Learning in Open Collaborations](http://arxiv.org/abs/2106.10207)


  Modern deep learning applications require increasingly more compute to train
state-of-the-art models. To address this demand, large corporations and
institutions use dedicated High-Performance Computing clusters, whose
construction and maintenance are both environmentally costly and well beyond
the budget of most organizations. As a result, some research directions become
the exclusive domain of a few large industrial and even fewer academic actors.
To alleviate this disparity, smaller groups may pool their computational
resources and run collaborative experiments that benefit all participants. This
paradigm, known as grid- or volunteer computing, has seen successful
applications in numerous scientific areas. However, using this approach for
machine learning is difficult due to high latency, asymmetric bandwidth, and
several challenges unique to volunteer computing. In this work, we carefully
analyze these constraints and propose a novel algorithmic framework designed
specifically for collaborative training. We demonstrate the effectiveness of
our approach for SwAV and ALBERT pretraining in realistic conditions and
achieve performance comparable to traditional setups at a fraction of the cost.
Finally, we provide a detailed report of successful collaborative language
model pretraining with 40 participants.

    

### [[2106.10820] Stateful ODE-Nets using Basis Function Expansions](http://arxiv.org/abs/2106.10820)


  The recently-introduced class of ordinary differential equation networks
(ODE-Nets) establishes a fruitful connection between deep learning and
dynamical systems. In this work, we reconsider formulations of the weights as
continuous-in-depth functions using linear combinations of basis functions
which enables us to leverage parameter transformations such as function
projections. In turn, this view allows us to formulate a novel stateful
ODE-Block that handles stateful layers. The benefits of this new ODE-Block are
twofold: first, it enables incorporating meaningful continuous-in-depth batch
normalization layers to achieve state-of-the-art performance; second, it
enables compressing the weights through a change of basis, without retraining,
while maintaining near state-of-the-art performance and reducing both inference
time and memory footprint. Performance is demonstrated by applying our stateful
ODE-Block to (a) image classification tasks using convolutional units and (b)
sentence-tagging tasks using transformer encoder units.

    

### [[2106.10994] BernNet: Learning Arbitrary Graph Spectral Filters via Bernstein Approximation](http://arxiv.org/abs/2106.10994)


  Many representative graph neural networks, e.g., GPR-GNN and ChebNet,
approximate graph convolutions with graph spectral filters. However, existing
work either applies predefined filter weights or learns them without necessary
constraints, which may lead to oversimplified or ill-posed filters. To overcome
these issues, we propose BernNet, a novel graph neural network with theoretical
support that provides a simple but effective scheme for designing and learning
arbitrary graph spectral filters. In particular, for any filter over the
normalized Laplacian spectrum of a graph, our BernNet estimates it by an
order-$K$ Bernstein polynomial approximation and designs its spectral property
by setting the coefficients of the Bernstein basis. Moreover, we can learn the
coefficients (and the corresponding filter weights) based on observed graphs
and their associated signals and thus achieve the BernNet specialized for the
data. Our experiments demonstrate that BernNet can learn arbitrary spectral
filters, including complicated band-rejection and comb filters, and it achieves
superior performance in real-world graph modeling tasks. Code is available at
this https URL.

    

### [[2106.12242] A Unified Approach to Fair Online Learning via Blackwell Approachability](http://arxiv.org/abs/2106.12242)


  We provide a setting and a general approach to fair online learning with
stochastic sensitive and non-sensitive contexts. The setting is a repeated game
between the Player and Nature, where at each stage both pick actions based on
the contexts. Inspired by the notion of unawareness, we assume that the Player
can only access the non-sensitive context before making a decision, while we
discuss both cases of Nature accessing the sensitive contexts and Nature
unaware of the sensitive contexts. Adapting Blackwell's approachability theory
to handle the case of an unknown contexts' distribution, we provide a general
necessary and sufficient condition for learning objectives to be compatible
with some fairness constraints. This condition is instantiated on (group-wise)
no-regret and (group-wise) calibration objectives, and on demographic parity as
an additional constraint. When the objective is not compatible with the
constraint, the provided framework permits to characterise the optimal
trade-off between the two.

    

### [[2106.12839] Visualizing Graph Neural Networks with CorGIE: Corresponding a Graph to Its Embedding](http://arxiv.org/abs/2106.12839)


  Graph neural networks (GNNs) are a class of powerful machine learning tools
that model node relations for making predictions of nodes or links. GNN
developers rely on quantitative metrics of the predictions to evaluate a GNN,
but similar to many other neural networks, it is difficult for them to
understand if the GNN truly learns characteristics of a graph as expected. We
propose an approach to corresponding an input graph to its node embedding (aka
latent space), a common component of GNNs that is later used for prediction. We
abstract the data and tasks, and develop an interactive multi-view interface
called CorGIE to instantiate the abstraction. As the key function in CorGIE, we
propose the K-hop graph layout to show topological neighbors in hops and their
clustering structure. To evaluate the functionality and usability of CorGIE, we
present how to use CorGIE in two usage scenarios, and conduct a case study with
five GNN experts.

    

### [[2107.01201] Multi-user VoiceFilter-Lite via Attentive Speaker Embedding](http://arxiv.org/abs/2107.01201)


  In this paper, we propose a solution to allow speaker conditioned speech
models, such as VoiceFilter-Lite, to support an arbitrary number of enrolled
users in a single pass. This is achieved by using an attention mechanism on
multiple speaker embeddings to compute a single attentive embedding, which is
then used as a side input to the model. We implemented multi-user
VoiceFilter-Lite and evaluated it for three tasks: (1) a streaming automatic
speech recognition (ASR) task; (2) a text-independent speaker verification
task; and (3) a personalized keyphrase detection task, where ASR has to detect
keyphrases from multiple enrolled users in a noisy environment. Our experiments
show that, with up to four enrolled users, multi-user VoiceFilter-Lite is able
to significantly reduce speech recognition and speaker verification errors when
there is overlapping speech, without affecting performance under other acoustic
conditions. This attentive speaker embedding approach can also be easily
applied to other speaker-conditioned models such as personal VAD and
personalized ASR.

    

### [[2110.12778] A Deep Reinforcement Learning Approach for Audio-based Navigation and Audio Source Localization in Multi-speaker Environments](http://arxiv.org/abs/2110.12778)


  In this work we apply deep reinforcement learning to the problems of
navigating a three-dimensional environment and inferring the locations of human
speaker audio sources within, in the case where the only available information
is the raw sound from the environment, as a simulated human listener placed in
the environment would hear it. For this purpose we create two virtual
environments using the Unity game engine, one presenting an audio-based
navigation problem and one presenting an audio source localization problem. We
also create an autonomous agent based on PPO online reinforcement learning
algorithm and attempt to train it to solve these environments. Our experiments
show that our agent achieves adequate performance and generalization ability in
both environments, measured by quantitative metrics, even when a limited amount
of training data are available or the environment parameters shift in ways not
encountered during training. We also show that a degree of agent knowledge
transfer is possible between the environments.

    

### [[2111.03746] Efficient Neuromorphic Signal Processing with Loihi 2](http://arxiv.org/abs/2111.03746)


  The biologically inspired spiking neurons used in neuromorphic computing are
nonlinear filters with dynamic state variables -- very different from the
stateless neuron models used in deep learning. The next version of Intel's
neuromorphic research processor, Loihi 2, supports a wide range of stateful
spiking neuron models with fully programmable dynamics. Here we showcase
advanced spiking neuron models that can be used to efficiently process
streaming data in simulation experiments on emulated Loihi 2 hardware. In one
example, Resonate-and-Fire (RF) neurons are used to compute the Short Time
Fourier Transform (STFT) with similar computational complexity but 47x less
output bandwidth than the conventional STFT. In another example, we describe an
algorithm for optical flow estimation using spatiotemporal RF neurons that
requires over 90x fewer operations than a conventional DNN-based solution. We
also demonstrate promising preliminary results using backpropagation to train
RF neurons for audio classification tasks. Finally, we show that a cascade of
Hopf resonators - a variant of the RF neuron - replicates novel properties of
the cochlea and motivates an efficient spike-based spectrogram encoder.

    

### [[2111.04222] Not All Fabrics Are Created Equal: Exploring eFPGA Parameters For IP Redaction](http://arxiv.org/abs/2111.04222)


  Semiconductor design houses rely on third-party foundries to manufacture
their integrated circuits (IC). While this trend allows them to tackle
fabrication costs, it introduces security concerns as external (and potentially
malicious) parties can access critical parts of the designs and steal or modify
the Intellectual Property (IP). Embedded FPGA (eFPGA) redaction is a promising
technique to protect critical IPs of an ASIC by \textit{redacting} (i.e.,
removing) critical parts and mapping them onto a custom reconfigurable fabric.
Only trusted parties will receive the correct bitstream to restore the redacted
functionality. While previous studies imply that using an eFPGA is a sufficient
condition to provide security against IP threats like reverse-engineering,
whether this truly holds for all eFPGA architectures is unclear, thus
motivating the study in this paper. We examine the security of eFPGA fabrics
generated by varying different FPGA design parameters. We characterize the
power, performance, and area (PPA) characteristics and evaluate each fabric's
resistance to SAT-based bitstream recovery. Our results encourage designers to
work with custom eFPGA fabrics rather than off-the-shelf commercial FPGAs and
reveals that only considering a redaction fabric's bitstream size is inadequate
for gauging security.

    

### [[2111.03683] On Homomorphism Graphs](http://arxiv.org/abs/2111.03683)


  We introduce a new type of examples of bounded degree acyclic Borel graphs
and study their combinatorial properties in the context of descriptive
combinatorics, using a generalization of the determinacy method of Marks. The
motivation for the construction comes from the adaptation of this method to the
LOCAL model of distributed computing. Our approach unifies the previous results
in the area, as well as produces new ones. In particular, we show that for
$\Delta>2$ it is impossible to give a simple characterization of acyclic
$\Delta$-regular Borel graphs with Borel chromatic number at most $\Delta$:
such graphs form a $\mathbf{\Sigma}^1_2$-complete set. This implies a strong
failure of Brooks'-like theorems in the Borel context.

    

### [[2111.04007] Varuna: Scalable, Low-cost Training of Massive Deep Learning Models](http://arxiv.org/abs/2111.04007)


  Systems for training massive deep learning models (billions of parameters)
today assume and require specialized "hyper-clusters": hundreds or thousands of
GPUs wired with specialized high-bandwidth interconnects such as NV-Link and
Infiniband. Besides being expensive, such dependence on hyper-clusters and
custom high-speed inter-connects limits the size of such clusters, creating (a)
scalability limits on job parallelism; (b) resource fragmentation across
hyper-clusters.
In this paper, we present Varuna, a new system that enables training massive
deep learning models on commodity networking. Varuna makes thrifty use of
networking resources and automatically configures the user's training job to
efficiently use any given set of resources. Therefore, Varuna is able to
leverage "low-priority" VMs that cost about 5x cheaper than dedicated GPUs,
thus significantly reducing the cost of training massive models. We demonstrate
the efficacy of Varuna by training massive models, including a 200 billion
parameter model, on 5x cheaper "spot VMs", while maintaining high training
throughput. Even in scenarios where hyper-cluster resources are available,
Varuna improves end-to-end training time by 20-78% compared to alternative
approaches.
The code for Varuna is available at this https URL.

    

### [[2111.04289] LMStream: When Distributed Micro-Batch Stream Processing Systems Meet GPU](http://arxiv.org/abs/2111.04289)


  This paper presents LMStream, which ensures bounded latency while maximizing
the throughput on the GPU-enabled micro-batch streaming systems. The main ideas
behind LMStream's design can be summarized as two novel mechanisms: (1) dynamic
batching and (2) dynamic operation-level query planning. By controlling the
micro-batch size, LMStream significantly reduces the latency of individual
dataset because it does not perform unconditional buffering only for improving
GPU utilization. LMStream bounds the latency to an optimal value according to
the characteristics of the window operation used in the streaming application.
Dynamic mapping between a query to an execution device based on the data size
and dynamic device preference improves both the throughput and latency as much
as possible. In addition, LMStream proposes a low-overhead online cost model
parameter optimization method without interrupting the real-time stream
processing. We implemented LMStream on Apache Spark, which supports micro-batch
stream processing. Compared to the previous throughput-oriented method,
LMStream showed an average latency improvement up to a maximum of 70.7%, while
improving average throughput up to 1.74x.

    

### [[2007.01046] Proofs of Useless Work -- Positive and Negative Results for Wasteless Mining Systems](http://arxiv.org/abs/2007.01046)


  Many blockchain systems today, including Bitcoin, rely on Proof of Work
(PoW). Proof of work is crucial to the liveness and security of
cryptocurrencies. The assumption when using PoW is that a lot of trial and
error is required on average before a valid block is generated. One of the main
concerns raised with regard to this kind of system is the inherent need to
"waste" energy on "meaningless" problems. In fact, the Bitcoin system is
believed to consume more electricity than several small countries.
In this work we formally define three properties that are necessary for
wasteless PoW systems: (1) solve "meaningful" problems (2) solve them
efficiently and (3) be secure against double-spend attacks. These properties
aim to create an open market for problem-solving, in which miners produce
solutions to problems in the most efficient way (wasteless). The security of
the system stems from the economical incentive created by the demand for
solutions to these problems.
We analyze these properties, and deduce constraints that must apply to such
PoW systems. In our main result, we conclude that under realistic assumptions,
the set of allowed problems must be preimage resistant functions in order to
keep the system secure and efficient.

    

### [[2106.06841] Quantum Algorithms and Simulation for Parallel and Distributed Quantum Computing](http://arxiv.org/abs/2106.06841)


  A viable approach for building large-scale quantum computers is to interlink
small-scale quantum computers with a quantum network to create a larger
distributed quantum computer. When designing quantum algorithms for such a
distributed quantum computer, one can make use of the added parallelization and
distribution abilities inherent in the system. An added difficulty to then
overcome for distributed quantum computing is that a complex control system to
orchestrate the various components is required. In this work, we aim to address
these issues. We explicitly define what it means for a quantum algorithm to be
distributed and then present various quantum algorithms that fit the
definition. We discuss potential benefits and propose a high-level scheme for
controlling the system. With this, we present our software framework called
Interlin-q, a simulation platform that aims to simplify designing and verifying
parallel and distributed quantum algorithms. We demonstrate Interlin-q by
implementing some of the discussed algorithms using Interlin-q and layout
future steps for developing Interlin-q into a control system for distributed
quantum computers.

    

### [[2111.03687] AI and Blackness: Towards moving beyond bias and representation](http://arxiv.org/abs/2111.03687)


  In this paper, we argue that AI ethics must move beyond the concepts of
race-based representation and bias, and towards those that probe the deeper
relations that impact how these systems are designed, developed, and deployed.
Many recent discussions on ethical considerations of bias in AI systems have
centered on racial bias. We contend that antiblackness in AI requires more of
an examination of the ontological space that provides a foundation for the
design, development, and deployment of AI systems. We examine what this
contention means from the perspective of the sociocultural context in which AI
systems are designed, developed, and deployed and focus on intersections with
anti-Black racism (antiblackness). To bring these multiple perspectives
together and show an example of antiblackness in the face of attempts at
de-biasing, we discuss results from auditing an existing open-source semantic
network (ConceptNet). We use this discussion to further contextualize
antiblackness in design, development, and deployment of AI systems and suggest
questions one may ask when attempting to combat antiblackness in AI systems.

    

### [[2111.03699] A space of goals: the cognitive geometry of informationally bounded agents](http://arxiv.org/abs/2111.03699)


  Traditionally, Euclidean geometry is treated by scientists as a priori and
objective. However, when we take the position of an agent, the problem of
selecting a best route should also factor in the abilities of the agent, its
embodiment and particularly its cognitive effort. In this paper we consider
geometry in terms of travel between states within a world by incorporating
information processing costs with the appropriate spatial distances. This
induces a geometry that increasingly differs from the original geometry of the
given world, as information costs become increasingly important. We visualize
this \textit{"cognitive geometry"} by projecting it onto 2- and 3-dimensional
spaces showing distinct distortions reflecting the emergence of epistemic and
information-saving strategies as well as pivot states. The analogies between
traditional cost-based geometries and those induced by additional informational
costs invite a generalization of the traditional notion of geodesics as
cheapest routes towards the notion of \textit{infodesics}. Crucially, the
concept of infodesics approximates the usual geometric property that,
travelling from a start to a goal along a geodesic, not only the goal, but all
intermediate points are equally visited at optimal cost from the start.

    

### [[2111.03728] Shared Model of Sense-making for Human-Machine Collaboration](http://arxiv.org/abs/2111.03728)


  We present a model of sense-making that greatly facilitates the collaboration
between an intelligent analyst and a knowledge-based agent. It is a general
model grounded in the science of evidence and the scientific method of
hypothesis generation and testing, where sense-making hypotheses that explain
an observation are generated, relevant evidence is then discovered, and the
hypotheses are tested based on the discovered evidence. We illustrate how the
model enables an analyst to directly instruct the agent to understand
situations involving the possible production of weapons (e.g., chemical warfare
agents) and how the agent becomes increasingly more competent in understanding
other situations from that domain (e.g., possible production of
centrifuge-enriched uranium or of stealth fighter aircraft).

    

### [[2111.03751] Asynchronous Collaborative Localization by Integrating Spatiotemporal Graph Learning with Model-Based Estimation](http://arxiv.org/abs/2111.03751)


  Collaborative localization is an essential capability for a team of robots
such as connected vehicles to collaboratively estimate object locations from
multiple perspectives with reliant cooperation. To enable collaborative
localization, four key challenges must be addressed, including modeling complex
relationships between observed objects, fusing observations from an arbitrary
number of collaborating robots, quantifying localization uncertainty, and
addressing latency of robot communications. In this paper, we introduce a novel
approach that integrates uncertainty-aware spatiotemporal graph learning and
model-based state estimation for a team of robots to collaboratively localize
objects. Specifically, we introduce a new uncertainty-aware graph learning
model that learns spatiotemporal graphs to represent historical motions of the
objects observed by each robot over time and provides uncertainties in object
localization. Moreover, we propose a novel method for integrated learning and
model-based state estimation, which fuses asynchronous observations obtained
from an arbitrary number of robots for collaborative localization. We evaluate
our approach in two collaborative object localization scenarios in simulations
and on real robots. Experimental results show that our approach outperforms
previous methods and achieves state-of-the-art performance on asynchronous
collaborative localization.

    

### [[2111.03780] Artifact- and content-specific quality assessment for MRI with image rulers](http://arxiv.org/abs/2111.03780)


  In clinical practice MR images are often first seen by radiologists long
after the scan. If image quality is inadequate either patients have to return
for an additional scan, or a suboptimal interpretation is rendered. An
automatic image quality assessment (IQA) would enable real-time remediation.
Existing IQA works for MRI give only a general quality score, agnostic to the
cause of and solution to low-quality scans. Furthermore, radiologists' image
quality requirements vary with the scan type and diagnostic task. Therefore,
the same score may have different implications for different scans. We propose
a framework with multi-task CNN model trained with calibrated labels and
inferenced with image rulers. Labels calibrated by human inputs follow a
well-defined and efficient labeling task. Image rulers address varying quality
standards and provide a concrete way of interpreting raw scores from the CNN.
The model supports assessments of two of the most common artifacts in MRI:
noise and motion. It achieves accuracies of around 90%, 6% better than the best
previous method examined, and 3% better than human experts on noise assessment.
Our experiments show that label calibration, image rulers, and multi-task
training improve the model's performance and generalizability.

    

### [[2111.03782] Confidence Composition for Monitors of Verification Assumptions](http://arxiv.org/abs/2111.03782)


  Closed-loop verification of cyber-physical systems with neural network
controllers offers strong safety guarantees under certain assumptions. It is,
however, difficult to determine whether these guarantees apply at run time
because verification assumptions may be violated. To predict safety violations
in a verified system, we propose a three-step framework for monitoring the
confidence in verification assumptions. First, we represent the sufficient
condition for verified safety with a propositional logical formula over
assumptions. Second, we build calibrated confidence monitors that evaluate the
probability that each assumption holds. Third, we obtain the confidence in the
verification guarantees by composing the assumption monitors using a
composition function suitable for the logical formula. Our framework provides
theoretical bounds on the calibration and conservatism of compositional
monitors. In two case studies, we demonstrate that the composed monitors
improve over their constituents and successfully predict safety violations.

    

### [[2111.03789] Generation of microbial colonies dataset with deep learning style transfer](http://arxiv.org/abs/2111.03789)


  We introduce an effective strategy to generate a synthetic dataset of
microbiological images of Petri dishes that can be used to train deep learning
models. The developed generator employs traditional computer vision algorithms
together with a neural style transfer method for data augmentation. We show
that the method is able to synthesize a dataset of realistic looking images
that can be used to train a neural network model capable of localising,
segmenting, and classifying five different microbial species. Our method
requires significantly fewer resources to obtain a useful dataset than
collecting and labeling a whole large set of real images with annotations. We
show that starting with only 100 real images, we can generate data to train a
detector that achieves comparable results to the same detector but trained on a
real, several dozen times bigger dataset. We prove the usefulness of the method
in microbe detection and segmentation, but we expect that it is general and
flexible and can also be applicable in other domains of science and industry to
detect various objects.

    

### [[2111.03796] Development of collective behavior in newborn artificial agents](http://arxiv.org/abs/2111.03796)


  Collective behavior is widespread across the animal kingdom. To date,
however, the developmental and mechanistic foundations of collective behavior
have not been formally established. What learning mechanisms drive the
development of collective behavior in newborn animals? Here, we used deep
reinforcement learning and curiosity-driven learning -- two learning mechanisms
deeply rooted in psychological and neuroscientific research -- to build newborn
artificial agents that develop collective behavior. Like newborn animals, our
agents learn collective behavior from raw sensory inputs in naturalistic
environments. Our agents also learn collective behavior without external
rewards, using only intrinsic motivation (curiosity) to drive learning.
Specifically, when we raise our artificial agents in natural visual
environments with groupmates, the agents spontaneously develop ego-motion,
object recognition, and a preference for groupmates, rapidly learning all of
the core skills required for collective behavior. This work bridges the divide
between high-dimensional sensory inputs and collective action, resulting in a
pixels-to-actions model of collective animal behavior. More generally, we show
that two generic learning mechanisms -- deep reinforcement learning and
curiosity-driven learning -- are sufficient to learn collective behavior from
unsupervised natural experience.

    

### [[2111.03811] SIG-VC: A Speaker Information Guided Zero-shot Voice Conversion System for Both Human Beings and Machines](http://arxiv.org/abs/2111.03811)


  Nowadays, as more and more systems achieve good performance in traditional
voice conversion (VC) tasks, people's attention gradually turns to VC tasks
under extreme conditions. In this paper, we propose a novel method for
zero-shot voice conversion. We aim to obtain intermediate representations for
speaker-content disentanglement of speech to better remove speaker information
and get pure content information. Accordingly, our proposed framework contains
a module that removes the speaker information from the acoustic feature of the
source speaker. Moreover, speaker information control is added to our system to
maintain the voice cloning performance. The proposed system is evaluated by
subjective and objective metrics. Results show that our proposed system
significantly reduces the trade-off problem in zero-shot voice conversion,
while it also manages to have high spoofing power to the speaker verification
system.

    

### [[2111.03848] Multimodal PET/CT Tumour Segmentation and Prediction of Progression-Free Survival using a Full-Scale UNet with Attention](http://arxiv.org/abs/2111.03848)


  Segmentation of head and neck (H\&N) tumours and prediction of patient
outcome are crucial for patient's disease diagnosis and treatment monitoring.
Current developments of robust deep learning models are hindered by the lack of
large multi-centre, multi-modal data with quality annotations. The MICCAI 2021
HEad and neCK TumOR (HECKTOR) segmentation and outcome prediction challenge
creates a platform for comparing segmentation methods of the primary gross
target volume on fluoro-deoxyglucose (FDG)-PET and Computed Tomography images
and prediction of progression-free survival in H\&N oropharyngeal cancer.For
the segmentation task, we proposed a new network based on an encoder-decoder
architecture with full inter- and intra-skip connections to take advantage of
low-level and high-level semantics at full scales. Additionally, we used
Conditional Random Fields as a post-processing step to refine the predicted
segmentation maps. We trained multiple neural networks for tumor volume
segmentation, and these segmentations were ensembled achieving an average Dice
Similarity Coefficient of 0.75 in cross-validation, and 0.76 on the challenge
testing data set. For prediction of patient progression free survival task, we
propose a Cox proportional hazard regression combining clinical, radiomic, and
deep learning features. Our survival prediction model achieved a concordance
index of 0.82 in cross-validation, and 0.62 on the challenge testing data set.

    

### [[2111.03937] Transformer Based Bengali Chatbot Using General Knowledge Dataset](http://arxiv.org/abs/2111.03937)


  An AI chatbot provides an impressive response after learning from the trained
dataset. In this decade, most of the research work demonstrates that deep
neural models superior to any other model. RNN model regularly used for
determining the sequence-related problem like a question and it answers. This
approach acquainted with everyone as seq2seq learning. In a seq2seq model
mechanism, it has encoder and decoder. The encoder embedded any input sequence,
and the decoder embedded output sequence. For reinforcing the seq2seq model
performance, attention mechanism added into the encoder and decoder. After
that, the transformer model has introduced itself as a high-performance model
with multiple attention mechanism for solving the sequence-related dilemma.
This model reduces training time compared with RNN based model and also
achieved state-of-the-art performance for sequence transduction. In this
research, we applied the transformer model for Bengali general knowledge
chatbot based on the Bengali general knowledge Question Answer (QA) dataset. It
scores 85.0 BLEU on the applied QA data. To check the comparison of the
transformer model performance, we trained the seq2seq model with attention on
our dataset that scores 23.5 BLEU.

    

### [[2111.03963] Profitable Trade-Off Between Memory and Performance In Multi-Domain Chatbot Architectures](http://arxiv.org/abs/2111.03963)


  Text classification problem is a very broad field of study in the field of
natural language processing. In short, the text classification problem is to
determine which of the previously determined classes the given text belongs to.
Successful studies have been carried out in this field in the past studies. In
the study, Bidirectional Encoder Representations for Transformers (BERT), which
is a frequently preferred method for solving the classification problem in the
field of natural language processing, is used. By solving classification
problems through a single model to be used in a chatbot architecture, it is
aimed to alleviate the load on the server that will be created by more than one
model used for solving more than one classification problem. At this point,
with the masking method applied during the estimation of a single BERT model,
which was created for classification in more than one subject, the estimation
of the model was provided on a problem-based basis. Three separate data sets
covering different fields from each other are divided by various methods in
order to complicate the problem, and classification problems that are very
close to each other in terms of field are also included in this way. The
dataset used in this way consists of five classification problems with 154
classes. A BERT model containing all classification problems and other BERT
models trained specifically for the problems were compared with each other in
terms of performance and the space they occupied on the server.

    

### [[2111.03995] Explainable Deep Reinforcement Learning for Portfolio Management: An Empirical Approach](http://arxiv.org/abs/2111.03995)


  Deep reinforcement learning (DRL) has been widely studied in the portfolio
management task. However, it is challenging to understand a DRL-based trading
strategy because of the black-box nature of deep neural networks. In this
paper, we propose an empirical approach to explain the strategies of DRL agents
for the portfolio management task. First, we use a linear model in hindsight as
the reference model, which finds the best portfolio weights by assuming knowing
actual stock returns in foresight. In particular, we use the coefficients of a
linear model in hindsight as the reference feature weights. Secondly, for DRL
agents, we use integrated gradients to define the feature weights, which are
the coefficients between reward and features under a linear regression model.
Thirdly, we study the prediction power in two cases, single-step prediction and
multi-step prediction. In particular, we quantify the prediction power by
calculating the linear correlations between the feature weights of a DRL agent
and the reference feature weights, and similarly for machine learning methods.
Finally, we evaluate a portfolio management task on Dow Jones 30 constituent
stocks during 01/01/2009 to 09/01/2021. Our approach empirically reveals that a
DRL agent exhibits a stronger multi-step prediction power than machine learning
methods.

    

### [[2111.03997] The Three-Dimensional Structural Configuration of the Central Retinal Vessel Trunk and Branches as a Glaucoma Biomarker](http://arxiv.org/abs/2111.03997)


  Purpose: To assess whether the three-dimensional (3D) structural
configuration of the central retinal vessel trunk and its branches (CRVT&B)
could be used as a diagnostic marker for glaucoma. Method: We trained a deep
learning network to automatically segment the CRVT&B from the B-scans of the
optical coherence tomography (OCT) volume of the optic nerve head (ONH).
Subsequently, two different approaches were used for glaucoma diagnosis using
the structural configuration of the CRVT&B as extracted from the OCT volumes.
In the first approach, we aimed to provide a diagnosis using only 3D CNN and
the 3D structure of the CRVT&B. For the second approach, we projected the 3D
structure of the CRVT&B orthographically onto three planes to obtain 2D images,
and then a 2D CNN was used for diagnosis. The segmentation accuracy was
evaluated using the Dice coefficient, whereas the diagnostic accuracy was
assessed using the area under the receiver operating characteristic curves
(AUC). The diagnostic performance of the CRVT&B was also compared with that of
retinal nerve fiber layer (RNFL) thickness. Results: Our segmentation network
was able to efficiently segment retinal blood vessels from OCT scans. On a test
set, we achieved a Dice coefficient of 0.81\pm0.07. The 3D and 2D diagnostic
networks were able to differentiate glaucoma from non-glaucoma subjects with
accuracies of 82.7% and 83.3%, respectively. The corresponding AUCs for CRVT&B
were 0.89 and 0.90, higher than those obtained with RNFL thickness alone.
Conclusions: Our work demonstrated that the diagnostic power of the CRVT&B is
superior to that of a gold-standard glaucoma parameter, i.e., RNFL thickness.
Our work also suggested that the major retinal blood vessels form a skeleton --
the configuration of which may be representative of major ONH structural
changes as typically observed with the development and progression of glaucoma.

    

### [[2111.04051] Coordinated Proximal Policy Optimization](http://arxiv.org/abs/2111.04051)


  We present Coordinated Proximal Policy Optimization (CoPPO), an algorithm
that extends the original Proximal Policy Optimization (PPO) to the multi-agent
setting. The key idea lies in the coordinated adaptation of step size during
the policy update process among multiple agents. We prove the monotonicity of
policy improvement when optimizing a theoretically-grounded joint objective,
and derive a simplified optimization objective based on a set of
approximations. We then interpret that such an objective in CoPPO can achieve
dynamic credit assignment among agents, thereby alleviating the high variance
issue during the concurrent update of agent policies. Finally, we demonstrate
that CoPPO outperforms several strong baselines and is competitive with the
latest multi-agent PPO method (i.e. MAPPO) under typical multi-agent settings,
including cooperative matrix games and the StarCraft II micromanagement tasks.

    

### [[2111.04085] Modelling and Optimisation of Resource Usage in an IoT Enabled Smart Campus](http://arxiv.org/abs/2111.04085)


  University campuses are essentially a microcosm of a city. They comprise
diverse facilities such as residences, sport centres, lecture theatres, parking
spaces, and public transport stops. Universities are under constant pressure to
improve efficiencies while offering a better experience to various stakeholders
including students, staff, and visitors. Nonetheless, anecdotal evidence
indicates that campus assets are not being utilised efficiently, often due to
the lack of data collection and analysis, thereby limiting the ability to make
informed decisions on the allocation and management of resources. Advances in
the Internet of Things (IoT) technologies that can sense and communicate data
from the physical world, coupled with data analytics and Artificial
intelligence (AI) that can predict usage patterns, have opened up new
opportunities for organisations to lower cost and improve user experience. This
thesis explores this opportunity via theory and experimentation using UNSW
Sydney as a living laboratory.

    

### [[2111.04092] Consistency and Consensus Driven for Hesitant Fuzzy Linguistic Decision Making with Pairwise Comparisons](http://arxiv.org/abs/2111.04092)


  Hesitant fuzzy linguistic preference relation (HFLPR) is of interest because
it provides an efficient way for opinion expression under uncertainty. For
enhancing the theory of decision making with HFLPR, the paper introduces an
algorithm for group decision making with HFLPRs based on the acceptable
consistency and consensus measurements, which involves (1) defining a hesitant
fuzzy linguistic geometric consistency index (HFLGCI) and proposing a procedure
for consistency checking and inconsistency improving for HFLPR; (2) measuring
the group consensus based on the similarity between the original individual
HFLPRs and the overall perfect HFLPR, then establishing a procedure for
consensus ensuring including the determination of decision-makers weights. The
convergence and monotonicity of the proposed two procedures have been proved.
Some experiments are furtherly performed to investigate the critical values of
the defined HFLGCI, and comparative analyses are conducted to show the
effectiveness of the proposed algorithm. A case concerning the performance
evaluation of venture capital guiding funds is given to illustrate the
availability of the proposed algorithm. As an application of our work, an
online decision-making portal is finally provided for decision-makers to
utilize the proposed algorithms to solve decision-making problems.

    

### [[2111.04120] Automatic Goal Generation using Dynamical Distance Learning](http://arxiv.org/abs/2111.04120)


  Reinforcement Learning (RL) agents can learn to solve complex sequential
decision making tasks by interacting with the environment. However, sample
efficiency remains a major challenge. In the field of multi-goal RL, where
agents are required to reach multiple goals to solve complex tasks, improving
sample efficiency can be especially challenging. On the other hand, humans or
other biological agents learn such tasks in a much more strategic way,
following a curriculum where tasks are sampled with increasing difficulty level
in order to make gradual and efficient learning progress. In this work, we
propose a method for automatic goal generation using a dynamical distance
function (DDF) in a self-supervised fashion. DDF is a function which predicts
the dynamical distance between any two states within a markov decision process
(MDP). With this, we generate a curriculum of goals at the appropriate
difficulty level to facilitate efficient learning throughout the training
process. We evaluate this approach on several goal-conditioned robotic
manipulation and navigation tasks, and show improvements in sample efficiency
over a baseline method which only uses random goal sampling.

    

### [[2111.04147] Learning Finite Linear Temporal Logic Specifications with a Specialized Neural Operator](http://arxiv.org/abs/2111.04147)


  Finite linear temporal logic ($\mathsf{LTL}_f$) is a powerful formal
representation for modeling temporal sequences. We address the problem of
learning a compact $\mathsf{LTL}_f$ formula from labeled traces of system
behavior. We propose a novel neural network operator and evaluate the resulting
architecture, Neural$\mathsf{LTL}_f$. Our approach includes a specialized
recurrent filter, designed to subsume $\mathsf{LTL}_f$ temporal operators, to
learn a highly accurate classifier for traces. Then, it discretizes the
activations and extracts the truth table represented by the learned weights.
This truth table is converted to symbolic form and returned as the learned
formula. Experiments on randomly generated $\mathsf{LTL}_f$ formulas show
Neural$\mathsf{LTL}_f$ scales to larger formula sizes than existing approaches
and maintains high accuracy even in the presence of noise.

    

### [[2111.04158] A Word on Machine Ethics: A Response to Jiang et al. (2021)](http://arxiv.org/abs/2111.04158)


  Ethics is one of the longest standing intellectual endeavors of humanity. In
recent years, the fields of AI and NLP have attempted to wrangle with how
learning systems that interact with humans should be constrained to behave
ethically. One proposal in this vein is the construction of morality models
that can take in arbitrary text and output a moral judgment about the situation
described. In this work, we focus on a single case study of the recently
proposed Delphi model and offer a critique of the project's proposed method of
automating morality judgments. Through an audit of Delphi, we examine broader
issues that would be applicable to any similar attempt. We conclude with a
discussion of how machine ethics could usefully proceed, by focusing on current
and near-future uses of technology, in a way that centers around transparency,
democratic values, and allows for straightforward accountability.

    

### [[2111.04165] On the Limits of Design: What Are the Conceptual Constraints on Designing Artificial Intelligence for Social Good?](http://arxiv.org/abs/2111.04165)


  Artificial intelligence AI can bring substantial benefits to society by
helping to reduce costs, increase efficiency and enable new solutions to
complex problems. Using Floridi's notion of how to design the 'infosphere' as a
starting point, in this chapter I consider the question: what are the limits of
design, i.e. what are the conceptual constraints on designing AI for social
good? The main argument of this chapter is that while design is a useful
conceptual tool to shape technologies and societies, collective efforts towards
designing future societies are constrained by both internal and external
factors. Internal constraints on design are discussed by evoking Hardin's
thought experiment regarding 'the Tragedy of the Commons'. Further, Hayek's
classical distinction between 'cosmos' and 'taxis' is used to demarcate
external constraints on design. Finally, five design principles are presented
which are aimed at helping policymakers manage the internal and external
constraints on design. A successful approach to designing future societies
needs to account for the emergent properties of complex systems by allowing
space for serendipity and socio-technological coevolution.

    

### [[2111.04212] Dense Representative Tooth Landmark/axis Detection Network on 3D Model](http://arxiv.org/abs/2111.04212)


  Artificial intelligence (AI) technology is increasingly used for digital
orthodontics, but one of the challenges is to automatically and accurately
detect tooth landmarks and axes. This is partly because of sophisticated
geometric definitions of them, and partly due to large variations among
individual tooth and across different types of tooth. As such, we propose a
deep learning approach with a labeled dataset by professional dentists to the
tooth landmark/axis detection on tooth model that are crucial for orthodontic
treatments. Our method can extract not only tooth landmarks in the form of
point (e.g. cusps), but also axes that measure the tooth angulation and
inclination. The proposed network takes as input a 3D tooth model and predicts
various types of the tooth landmarks and axes. Specifically, we encode the
landmarks and axes as dense fields defined on the surface of the tooth model.
This design choice and a set of added components make the proposed network more
suitable for extracting sparse landmarks from a given 3D tooth model. Extensive
evaluation of the proposed method was conducted on a set of dental models
prepared by experienced dentists. Results show that our method can produce
tooth landmarks with high accuracy. Our method was examined and justified via
comparison with the state-of-the-art methods as well as the ablation studies.

    

### [[2111.04224] Automated Detection of GDPR Disclosure Requirements in Privacy Policies using Deep Active Learning](http://arxiv.org/abs/2111.04224)


  Since GDPR came into force in May 2018, companies have worked on their data
practices to comply with this privacy law. In particular, since the privacy
policy is the essential communication channel for users to understand and
control their privacy, many companies updated their privacy policies after GDPR
was enforced. However, most privacy policies are verbose, full of jargon, and
vaguely describe companies' data practices and users' rights. Therefore, it is
unclear if they comply with GDPR. In this paper, we create a privacy policy
dataset of 1,080 websites labeled with the 18 GDPR requirements and develop a
Convolutional Neural Network (CNN) based model which can classify the privacy
policies with an accuracy of 89.2%. We apply our model to perform a measurement
on the compliance in the privacy policies. Our results show that even after
GDPR went into effect, 97% of websites still fail to comply with at least one
requirement of GDPR.

    

### [[2111.04248] Trust-aware Control for Intelligent Transportation Systems](http://arxiv.org/abs/2111.04248)


  Many intelligent transportation systems are multi-agent systems, i.e., both
the traffic participants and the subsystems within the transportation
infrastructure can be modeled as interacting agents. The use of AI-based
methods to achieve coordination among the different agents systems can provide
greater safety over transportation systems containing only human-operated
vehicles, and also improve the system efficiency in terms of traffic
throughput, sensing range, and enabling collaborative tasks. However, increased
autonomy makes the transportation infrastructure vulnerable to compromised
vehicular agents or infrastructure. This paper proposes a new framework by
embedding the trust authority into transportation infrastructure to
systematically quantify the trustworthiness of agents using an epistemic logic
known as subjective logic. In this paper, we make the following novel
contributions: (i) We propose a framework for using the quantified
trustworthiness of agents to enable trust-aware coordination and control. (ii)
We demonstrate how to synthesize trust-aware controllers using an approach
based on reinforcement learning. (iii) We comprehensively analyze an autonomous
intersection management (AIM) case study and develop a trust-aware version
called AIM-Trust that leads to lower accident rates in scenarios consisting of
a mixture of trusted and untrusted agents.

    

### [[2111.04261] JaMIE: A Pipeline Japanese Medical Information Extraction System](http://arxiv.org/abs/2111.04261)


  We present an open-access natural language processing toolkit for Japanese
medical information extraction. We first propose a novel relation annotation
schema for investigating the medical and temporal relations between medical
entities in Japanese medical reports. We experiment with the practical
annotation scenarios by separately annotating two different types of reports.
We design a pipeline system with three components for recognizing medical
entities, classifying entity modalities, and extracting relations. The
empirical results show accurate analyzing performance and suggest the
satisfactory annotation quality, the effective annotation strategy for
targeting report types, and the superiority of the latest contextual embedding
models.

    

### [[2007.14490] On Accuracy and Coherence with Infinite Opinion Sets](http://arxiv.org/abs/2007.14490)


  There is a well-known equivalence between avoiding accuracy dominance and
having probabilistically coherent credences (see, e.g., de Finetti 1974, Joyce
2009, Predd et al. 2009, Schervish et al. 2009, Pettigrew 2016). However, this
equivalence has been established only when the set of propositions on which
credence functions are defined is finite. In this paper, we establish
connections between accuracy dominance and coherence when credence functions
are defined on an infinite set of propositions. In particular, we establish the
necessary results to extend the classic accuracy argument for probabilism
originally due to Joyce (1998) to certain classes of infinite sets of
propositions including countably infinite partitions.

    

### [[2105.14713] 1xN Block Pattern for Network Sparsity](http://arxiv.org/abs/2105.14713)


  Though network sparsity emerges as a promising direction to overcome the
drastically increasing size of neural networks, it remains an open problem to
concurrently maintain model accuracy as well as achieve significant speedups on
general CPUs. In this paper, we propose one novel concept of $1\times N$ block
sparsity pattern (block pruning) to break this limitation. In particular,
consecutive $N$ output kernels with the same input channel index are grouped
into one block, which serves as a basic pruning granularity of our pruning
pattern. Our $1 \times N$ sparsity pattern prunes these blocks considered
unimportant. We also provide a workflow of filter rearrangement that first
rearranges the weight matrix in the output channel dimension to derive more
influential blocks for accuracy improvements, and then applies similar
rearrangement to the next-layer weights in the input channel dimension to
ensure correct convolutional operations. Moreover, the output computation after
our $1 \times N$ block sparsity can be realized via a parallelized block-wise
vectorized operation, leading to significant speedups on general CPUs-based
platforms. The efficacy of our pruning pattern is proved with experiments on
ILSVRC-2012. For example, in the case of 50% sparsity and $N=4$, our pattern
obtains about 3.0% improvements over filter pruning in the top-1 accuracy of
MobileNet-V2. Meanwhile, it obtains 56.04ms inference savings on Cortex-A7 CPU
over weight pruning. Code is available at this https URL.

    

### [[2111.03701] Choreographies as Functions](http://arxiv.org/abs/2111.03701)


  We propose a new interpretation of choreographies as functions, whereby
coordination protocols for concurrent and distributed systems are expressed in
terms of a {\lambda}-calculus. Our language is expressive enough to enable, for
the first time, the writing of higher-order protocols that do not require
central control. Nevertheless, it retains the simplicity and elegance of the
{\lambda}-calculus, and it is possible to translate choreographies into
endpoint implementations.

    

### [[2111.03720] Exception Handling on Multiparty Interactions](http://arxiv.org/abs/2111.03720)


  In designing distributed and parallel systems there are several approaches
for programming interactions in a multiprocess environment. Usually, these
approaches take care only of synchronization or communication in two-party
interactions. This paper is concerned with a more general concept: multiparty
interactions. In a multiparty interaction, several executing threads somehow
"come together" to produce an intermediate and temporary combined state, use
this state as a well-defined starting point for some joint activity, and then
leave this interaction and continue their separate execution. The concept of
multiparty interactions has been investigated by several researchers, but to
the best of our knowledge, none have considered how faults in one or more
participants of the multiparty interaction can best be dealt with. The goal of
this paper is twofold: to show how an existing specification language can be
extended in order to allow dependable multiparty interactions (DMIs) to be
declared and to present an object-oriented framework for implementing DMIs in
distributed systems. To show how our scheme can be used to program a system in
which multiparty interactions are more than simple synchronizations or
communications, we use a case study based on an industrial production cell
model developed by Forschungszentrum Informatik, Karlsruhe, Germany.

    

### [[2111.03881] Abstraction for Crash-Resilient Objects (Extended Version)](http://arxiv.org/abs/2111.03881)


  We study abstraction for crash-resilient concurrent objects using
non-volatile memory (NVM). We develop a library correctness criterion that is
sound for ensuring contextual refinement in this setting, thus allowing clients
to reason about library behaviors in terms of their abstract specifications,
and library developers to verify their implementations against the
specifications abstracting away from particular client programs. As a semantic
foundation we employ a recent NVM model, called Persistent Sequential
Consistency, and extend its language and operational semantics with useful
specification constructs. The proposed correctness criterion accounts for
NVM-related interactions between client and library code due to explicit
persist instructions, and for calling policies enforced by libraries. We
illustrate our approach on two implementations and specifications of simple
persistent objects with different prototypical durability guarantees. Our
results provide the first approach to formal compositional reasoning under NVM.

    

### [[2111.04259] OpenMP aware MHP Analysis for Improved Static Data-Race Detection](http://arxiv.org/abs/2111.04259)


  Data races, a major source of bugs in concurrent programs, can result in loss
of manpower and time as well as data loss due to system failures. OpenMP, the
de facto shared memory parallelism framework used in the HPC community, also
suffers from data races. To detect race conditions in OpenMP programs and
improve turnaround time and/or developer productivity, we present a data flow
analysis based, fast, static data race checker in the LLVM compiler framework.
Our tool can detect races in the presence or absence of explicit barriers, with
implicit or explicit synchronization. In addition, our tool effectively works
for the OpenMP target offloading constructs and also supports the frequently
used OpenMP constructs. We formalize and provide a data flow analysis framework
to perform Phase Interval Analysis (PIA) of OpenMP programs. Phase intervals
are then used to compute the MHP (and its complement NHP) sets for the
programs, which, in turn, are used to detect data races statically. We evaluate
our work using multiple OpenMP race detection benchmarks and real world
applications. Our experiments show that the checker is comparable to the
state-of-the-art in various performance metrics with around 90% accuracy,
almost perfect recall, and significantly lower runtime and memory footprint.

    

### [[2111.04298] Solving String Constraints With Regex-Dependent Functions Through Transducers With Priorities And Variables](http://arxiv.org/abs/2111.04298)


  Regular expressions are a classical concept in formal language theory.
Regular expressions in programming languages (RegEx) such as JavaScript,
feature non-standard semantics of operators (e.g. greedy/lazy Kleene star), as
well as additional features such as capturing groups and references. While
symbolic execution of programs containing RegExes appeals to string solvers
natively supporting important features of RegEx, such a string solver is
hitherto missing. In this paper, we propose the first string theory and string
solver that natively provide such a support. The key idea of our string solver
is to introduce a new automata model, called prioritized streaming string
transducers (PSST), to formalize the semantics of RegEx-dependent string
functions. PSSTs combine priorities, which have previously been introduced in
prioritized finite-state automata to capture greedy/lazy semantics, with string
variables as in streaming string transducers to model capturing groups. We
validate the consistency of the formal semantics with the actual JavaScript
semantics by extensive experiments. Furthermore, to solve the string
constraints, we show that PSSTs enjoy nice closure and algorithmic properties,
in particular, the regularity-preserving property (i.e., pre-images of regular
constraints under PSSTs are regular), and introduce a sound sequent calculus
that exploits these properties and performs propagation of regular constraints
by means of taking post-images or pre-images. Although the satisfiability of
the string constraint language is undecidable, we show that our approach is
complete for the so-called straight-line fragment. We evaluate the performance
of our string solver on over 195000 string constraints generated from an
open-source RegEx library. The experimental results show the efficacy of our
approach, drastically improving the existing methods in both precision and
efficiency.

    