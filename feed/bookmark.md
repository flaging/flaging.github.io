
## 2021-11-23

### [<title>Xgb.shap.data not found - XGBoost</title>](https://discuss.xgboost.ai/t/xgb-shap-data-not-found/2557/1)

### [<title>What are the tradeoffs associated with the various XGBoost interfaces? - XGBoost</title>](https://discuss.xgboost.ai/t/what-are-the-tradeoffs-associated-with-the-various-xgboost-interfaces/2555/1)

### [[2111.10496] Innovative System Design for Remote Air Traffic Control Simulation Training on and beyond COVID19](http://arxiv.org/abs/2111.10496)


  Remote ATC simulation training is an emerging technology in aviation
education during COVID. Professional training institutions can learn from
others, whereas the experiences of developing remote ATC simulation
teaching/training with the Start-up company ByteProTeq will be beneficial for
the rest of the world to understand the differences and similarities of current
remote training and to improve their performance in building a safe and
efficient remote training environment. In this paper, we will present three
improvements to our remote ATC training: 1) infrastructure upgrading of
hardware and software from an existing stand-alone system to a remote network
that considers costs, cyber security, system compatibility, et cetera; 2)
quality of remote ATC simulation training, compared with traditional
face-to-face training, including students and instructors feedback; 3)
enhancement of the current remote training system beyond COVID-19 regarding
reliability, cyber security, and capacity. This foundation paper will support
understanding of the current stage of remote ATC simulation training
development with the Start-up company ByteProTeq, during and beyond the
COVID-19 pandemic, thereby providing an excellent example for the rest of the
world.

    

### [[2111.10508] Broadband Digital Over-the-Air Computation for Asynchronous Federated Edge Learning](http://arxiv.org/abs/2111.10508)


  This paper presents the first broadband digital over-the-air computation
(AirComp) system for phase asynchronous OFDM-based federated edge learning
systems. Existing analog AirComp systems often assume perfect phase alignment
via channel precoding and utilize uncoded analog modulation for model
aggregation. In contrast, our digital AirComp system leverages digital
modulation and channel codes to overcome phase asynchrony, thereby achieving
accurate model aggregation in the asynchronous multi-user OFDM systems. To
realize a digital AirComp system, we propose a non-orthogonal multiple access
protocol that allows simultaneous transmissions from multiple edge devices, and
present a joint channel decoding and aggregation (Jt-CDA) decoder (i.e.,
full-state joint decoder). To reduce the computation complexity, we further
present a reduced-complexity Jt-CDA decoder (i.e., reduced-state joint
decoder), and its arithmetic sum bit error rate performance is similar to that
of the full-state joint decoder for most signal-to-noise ratio (SNR) regimes.
Simulation results on test accuracy (of CIFAR10 dataset) versus SNR show that:
1) analog AirComp systems are sensitive to phase asynchrony under practical
setup, and the test accuracy performance exhibits an error floor even at high
SNR regime; 2) our digital AirComp system outperforms an analog AirComp system
by at least 1.5 times when SNR 9dB, demonstrating the advantage of digital
AirComp in asynchronous multi-user OFDM systems.

    

### [[2111.10548] Reliable Coded Distributed Computing for Metaverse Services: Coalition Formation and Incentive Mechanism Design](http://arxiv.org/abs/2111.10548)


  The metaverse is regarded as a new wave of technological transformation that
provides a virtual space for people to interact with each other through digital
avatars. To achieve immersive user experiences in the metaverse, real-time
rendering is the key technology. However, computing intensive tasks of
real-time graphic and audio rendering from metaverse service providers cannot
be processed efficiently on a single resource-limited mobile and Internet of
Things (IoT) device. Alternatively, such devices can adopt the collaborative
computing paradigm based on Coded Distributed Computing (CDC) to support
metaverse services. Therefore, this paper introduces a reliable collaborative
CDC framework for metaverse. In the framework, idle resources from mobile
devices, acting as CDC workers, are aggregated to handle intensive computation
tasks in the metaverse. A coalition can be formed among reliable workers based
on a reputation metric which is maintained in a double blockchains database.
The framework also considers an incentive to attract reliable workers to
participate and process computation tasks of metaverse services. Moreover, the
framework is designed with a hierarchical structure composed of coalition
formation and Stackelberg games in the lower and upper levels to determine
stable coalitions and rewards for reliable workers, respectively. The
simulation results illustrate that the proposed framework is resistant to
malicious workers. Compared with the random worker selection scheme, the
proposed coalition formation and Stackelberg game can improve the utilities of
both metaverse service providers and CDC workers.

    

### [[2111.10570] Improving Spectral Efficiency of Wireless Networks through Democratic Spectrum Sharing](http://arxiv.org/abs/2111.10570)


  Wireless devices need spectrum to communicate. With the increase in the
number of devices competing for the same spectrum, it has become nearly
impossible to support the throughput requirements of all the devices through
current spectrum sharing methods. In this work, we look at the problem of
spectrum resource contention fundamentally, taking inspiration from the
principles of globalization. We develop a distributed algorithm whereby the
wireless nodes democratically share the spectrum resources and improve their
spectral efficiency and throughput without additional power or spectrum
resources. We validate the performance of our proposed democratic spectrum
sharing (DSS) algorithm over real-world Wi-Fi networks and on synthetically
generated networks with varying design parameters. Compared to the greedy
approach, DSS achieves significant gains in throughput (~60%), area spectral
efficiency ($\sim$50\%) and fairness in datarate distribution (~20%). Due to
the distributed nature of the proposed algorithm, we can apply it to wireless
networks of any size and density.

    

### [[2111.10572] Control Analysis of Packet Transmission Algorithms: Study on Fairness and Stability](http://arxiv.org/abs/2111.10572)


  This document is a study of fairness, feedback and stability notions of
different packet transmission algorithms. We start the discussion with defining
two scalable control algorithms namely primal and dual algorithm. We discuss
the dual algorithm model and then understand the fair dual algorithm. Further,
we discuss different notions of fairness under fair dual algorithm those
correspond to TCP and RCP congestion control protocols. Feedback parameters are
analyzed in each of these fairness algorithms and thus their stability is
studied.

    

### [[2111.10658] Q-Learning Based Energy-Efficient Network Planning in IP-over-EON](http://arxiv.org/abs/2111.10658)


  During network planning phase, optimal network planning implemented through
efficient resource allocation and static traffic demand provisioning in
IP-over-elastic optical network (IP-over-EON) is significantly challenging
compared with the fixed-grid wavelength division multiplexing (WDM) network due
to increased flexibility in IP-over-EON. Mathematical optimization models used
for this purpose may not provide solution for large networks due to large
computational complexity. In this regard, a greedy heuristic may be used that
intuitively selects traffic elements in sequence from static traffic demand
matrix and attempts to find the best solution. However, in general, such greedy
heuristics offer suboptimal solutions, since appropriate traffic sequence
offering the optimal performance is rarely selected. In this regard, we propose
a reinforcement learning technique (in particular a Q-learning method),
combined with an auxiliary graph (AG)-based energy efficient greedy method to
be used for large network planning. The Q-learning method is used to decide the
suitable sequence of traffic allocation such that the overall power consumption
in the network reduces. In the proposed heuristic, each traffic from the given
static traffic demand matrix is successively selected using the Q-learning
technique and provisioned using the AG-based greedy method.

    

### [[2111.10663] Fueling the Next Quantum Leap in Cellular Networks: Embracing AI in 5G Evolution towards 6G](http://arxiv.org/abs/2111.10663)


  Cellular networks, such as 5G systems, are becoming increasingly complex for
supporting various deployment scenarios and applications. Embracing artificial
intelligence (AI) in 5G evolution is critical to managing the complexity and
fueling the next quantum leap in 6G cellular networks. In this article, we
share our experience and best practices in applying AI in cellular networks. We
first present a primer on the state of the art of AI in cellular networks,
including basic concepts and recent key advances. Then we discuss 3GPP
standardization aspects and share various design rationales influencing
standardization. We also present case studies with real network data to
showcase how AI can improve network performance and enable network automation.

    

### [[2111.10690] Network Graph Generation through Adaptive Clustering and Infection Dynamics: A Step Towards Global Connectivity](http://arxiv.org/abs/2111.10690)


  More than 40% of the world's population is not connected to the internet,
majorly due to the lack of adequate infrastructure. Our work aims to bridge
this digital divide by proposing solutions for network deployment in remote
areas. Specifically, a number of access points (APs) are deployed as an
interface between the users and backhaul nodes (BNs). The main challenges
include designing the number and location of the APs, and connecting them to
the BNs. In order to address these challenges, we first propose a metric called
connectivity ratio to assess the quality of the deployment. Next, we propose an
agile search algorithm to determine the number of APs that maximizes this
metric and perform clustering to find the optimal locations of the APs.
Furthermore, we propose a novel algorithm inspired by infection dynamics to
connect all the deployed APs to the existing BNs economically. To support the
existing terrestrial BNs, we investigate the deployment of non-terrestrial BNs,
which further improves the network performance in terms of average hop count,
traffic distribution, and backhaul length. Finally, we use real datasets from a
remote village to test our solution.

    

### [[2111.10815] A User Centric Blockage Model for Wireless Networks](http://arxiv.org/abs/2111.10815)


  This paper proposes a cascade blockage model for analyzing the vision that a
user has of a wireless network. This model, inspired by the classical
multiplicative cascade models, has a radial structure meant to analyze
blockages seen by the receiver at the origin in different angular sectors. The
main novelty is that it is based on the geometry of obstacles and takes the
joint blockage phenomenon into account. We show on a couple of simple instances
that the Laplace transforms of total interference satisfies a functional
equation that can be solved efficiently by an iterative scheme. This is used to
analyze the coverage probability of the receiver and the effect of blockage
correlation and penetration loss in both dense and sparse blockage
environments. Furthermore, this model is used to investigate the effect of
blockage correlation on user beamforming techniques. Another functional
equation and its associated iterative algorithm are proposed to derive the
coverage performance of the best beam selection in this context. In addition,
the conditional coverage probability is also derived to evaluate the effect of
beam switching. The results not only show that beam selection is quite
efficient for multi-beam terminals, but also show how the correlation brought
by blockages can be leveraged to accelerate beam sweeping and pairing.

    

### [[2111.10967] Towards a Zero-Trust Micro-segmentation Network Security Strategy: An Evaluation Framework](http://arxiv.org/abs/2111.10967)


  Micro-segmentation is an emerging security technique that separates physical
networks into isolated logical micro-segments (workloads). By tying
fine-grained security policies to individual workloads, it limits the
attacker's ability to move laterally through the network, even after
infiltrating the perimeter defences. While micro-segmentation is proved to be
effective for shrinking enterprise networks attack surface, its impact
assessment is almost absent in the literature. This research is dedicated to
developing an analytical framework to characterise and quantify the
effectiveness of micro-segmentation on enhancing networks security. We rely on
a twofold graph-feature based framework of the network connectivity and attack
graphs to evaluate the network exposure and robustness, respectively. While the
former assesses the network assets connectedness, reachability and centrality,
the latter depicts the ability of the network to resist goal-oriented
attackers. Tracking the variations of formulated metrics values post the
deployment of micro-segmentation reveals exposure reduction and robustness
improvement in the range of 60% - 90%.

    

### [[2111.10994] Entanglement Swapping for Repeater Chains with Finite Memory Sizes](http://arxiv.org/abs/2111.10994)


  We develop entanglement swapping protocols and memory allocation methods for
quantum repeater chains. Unlike most of the existing studies, the memory size
of each quantum repeater in this work is a parameter that can be optimized.
Based on Markov chain modeling of the entanglement distribution process, we
determine the trade-off between the entanglement distribution rate and the
memory size for temporal multiplexing techniques. We then propose three memory
allocation methods that achieve entanglement distribution rates decaying
polynomially with respect to the distance while using constant average memory
slots per node. We also quantify the average number of memory slots required
due to classical communication delay, as well as the delay of entanglement
distribution. Our results show that a moderate memory size suffices to achieve
a polynomial decay of entanglement distribution rate with respect to the
distance, which is the scaling achieved by the optimal protocol even with
infinite memory size at each node.

    

### [[2111.11038] Time-Critical Tasks Implementation in MEC based Multi-Robot Cooperation Systems](http://arxiv.org/abs/2111.11038)


  Mobile edge computing (MEC) deployment in a multi-robot cooperation (MRC)
system is an effective way to accomplish the tasks in terms of energy
consumption and implementation latency. However, the computation and
communication resources need to be considered jointly to fully exploit the
advantages brought by the MEC technology. In this paper, the scenario where
multi robots cooperate to accomplish the time-critical tasks is studied, where
an intelligent master robot (MR) acts as an edge server to provide services to
multiple slave robots (SRs) and the SRs are responsible for the environment
sensing and data collection. To save energy and prolong the function time of
the system, two schemes are proposed to optimize the computation and
communication resources, respectively. In the first scheme, the energy
consumption of SRs is minimized and balanced while guaranteeing that the tasks
are accomplished under a time constraint. In the second scheme, not only the
energy consumption, but also the remaining energies of the SRs are considered
to enhance the robustness of the system. Through the analysis and numerical
simulations, we demonstrate that even though the first policy may guarantee the
minimization on the total SRs' energy consumption, the function time of MRC
system by the second scheme is longer than that by the first one.

    

### [[2008.13453] CoShare: An Efficient Approach for Redundancy Allocation in NFV](http://arxiv.org/abs/2008.13453)


  An appealing feature of Network Function Virtualization (NFV) is that in an
NFV-based network, a network function (NF) instance may be placed at any node.
On the one hand this offers great flexibility in allocation of redundant
instances, but on the other hand it makes the allocation a unique and difficult
challenge. One particular concern is that there is inherent correlation among
nodes due to the structure of the network, thus requiring special care in this
allocation. To this aim, our novel approach, called CoShare, is proposed.
Firstly, its design takes into consideration the effect of network structural
dependency, which might result in the unavailability of nodes of a network
after failure of a node. Secondly, to efficiently make use of resources,
CoShare proposes the idea of shared reservation, where multiple flows may be
allowed to share the same reserved backup capacity at an NF instance.
Furthermore, CoShare factors in the heterogeneity in nodes, NF instances and
availability requirements of flows in the design. The results from a number of
experiments conducted using realistic network topologies show that the
integration of structural dependency allows meeting availability requirements
for more flows compared to a baseline approach. Specifically, CoShare is able
to meet diverse availability requirements in a resource-efficient manner,
requiring, e.g., up to 85% in some studied cases, less resource overbuild than
the baseline approach that uses the idea of dedicated reservation commonly
adopted for redundancy allocation in NFV.

    

### [[2111.10371] ColDE: A Depth Estimation Framework for Colonoscopy Reconstruction](http://arxiv.org/abs/2111.10371)


  One of the key elements of reconstructing a 3D mesh from a monocular video is
generating every frame's depth map. However, in the application of colonoscopy
video reconstruction, producing good-quality depth estimation is challenging.
Neural networks can be easily fooled by photometric distractions or fail to
capture the complex shape of the colon surface, predicting defective shapes
that result in broken meshes. Aiming to fundamentally improve the depth
estimation quality for colonoscopy 3D reconstruction, in this work we have
designed a set of training losses to deal with the special challenges of
colonoscopy data. For better training, a set of geometric consistency
objectives was developed, using both depth and surface normal information.
Also, the classic photometric loss was extended with feature matching to
compensate for illumination noise. With the training losses powerful enough,
our self-supervised framework named ColDE is able to produce better depth maps
of colonoscopy data as compared to the previous work utilizing prior depth
knowledge. Used in reconstruction, our network is able to reconstruct
good-quality colon meshes in real-time without any post-processing, making it
the first to be clinically applicable.

    

### [[2111.10376] Diabetic Foot Ulcer Grand Challenge 2021: Evaluation and Summary](http://arxiv.org/abs/2111.10376)


  Diabetic foot ulcer classification systems use the presence of wound
infection (bacteria present within the wound) and ischaemia (restricted blood
supply) as vital clinical indicators for treatment and prediction of wound
healing. Studies investigating the use of automated computerised methods of
classifying infection and ischaemia within diabetic foot wounds are limited due
to a paucity of publicly available datasets and severe data imbalance in those
few that exist. The Diabetic Foot Ulcer Challenge 2021 provided participants
with a more substantial dataset comprising a total of 15,683 diabetic foot
ulcer patches, with 5,955 used for training, 5,734 used for testing and an
additional 3,994 unlabelled patches to promote the development of
semi-supervised and weakly-supervised deep learning techniques. This paper
provides an evaluation of the methods used in the Diabetic Foot Ulcer Challenge
2021, and summarises the results obtained from each network. The best
performing network was an ensemble of the results of the top 3 models, with a
macro-average F1-score of 0.6307.

    

### [[2111.10391] Data Excellence for AI: Why Should You Care](http://arxiv.org/abs/2111.10391)


  The efficacy of machine learning (ML) models depends on both algorithms and
data. Training data defines what we want our models to learn, and testing data
provides the means by which their empirical progress is measured. Benchmark
datasets define the entire world within which models exist and operate, yet
research continues to focus on critiquing and improving the algorithmic aspect
of the models rather than critiquing and improving the data with which our
models operate. If "data is the new oil," we are still missing work on the
refineries by which the data itself could be optimized for more effective use.

    

### [[2111.10401] Community-Detection via Hashtag-Graphs for Semi-Supervised NMF Topic Models](http://arxiv.org/abs/2111.10401)


  Extracting topics from large collections of unstructured text-documents has
become a central task in current NLP applications and algorithms like NMF, LDA
as well as their generalizations are the well-established current state of the
art. However, especially when it comes to short text documents like Tweets,
these approaches often lead to unsatisfying results due to the sparsity of the
document-feature matrices.
Even though, several approaches have been proposed to overcome this sparsity
by taking additional information into account, these are merely focused on the
aggregation of similar documents and the estimation of word-co-occurrences.
This ultimately completely neglects the fact that a lot of topical-information
can be actually retrieved from so-called hashtag-graphs by applying common
community detection algorithms. Therefore, this paper outlines a novel approach
on how to integrate topic structures of hashtag graphs into the estimation of
topic models by connecting graph-based community detection and semi-supervised
NMF.
By applying this approach on recently streamed Twitter data it will be seen
that this procedure actually leads to more intuitive and humanly interpretable
topics.

    

### [[2111.10434] Machine Learning for Mechanical Ventilation Control (Extended Abstract)](http://arxiv.org/abs/2111.10434)


  Mechanical ventilation is one of the most widely used therapies in the ICU.
However, despite broad application from anaesthesia to COVID-related life
support, many injurious challenges remain. We frame these as a control problem:
ventilators must let air in and out of the patient's lungs according to a
prescribed trajectory of airway pressure. Industry-standard controllers, based
on the PID method, are neither optimal nor robust. Our data-driven approach
learns to control an invasive ventilator by training on a simulator itself
trained on data collected from the ventilator. This method outperforms popular
reinforcement learning algorithms and even controls the physical ventilator
more accurately and robustly than PID. These results underscore how effective
data-driven methodologies can be for invasive ventilation and suggest that more
general forms of ventilation (e.g., non-invasive, adaptive) may also be
amenable.

    

### [[2111.10447] Dynamic Graph Representation Learning via Graph Transformer Networks](http://arxiv.org/abs/2111.10447)


  Dynamic graph representation learning is an important task with widespread
applications. Previous methods on dynamic graph learning are usually sensitive
to noisy graph information such as missing or spurious connections, which can
yield degenerated performance and generalization. To overcome this challenge,
we propose a Transformer-based dynamic graph learning method named Dynamic
Graph Transformer (DGT) with spatial-temporal encoding to effectively learn
graph topology and capture implicit links. To improve the generalization
ability, we introduce two complementary self-supervised pre-training tasks and
show that jointly optimizing the two pre-training tasks results in a smaller
Bayesian error rate via an information-theoretic analysis. We also propose a
temporal-union graph structure and a target-context node sampling strategy for
efficient and scalable training. Extensive experiments on real-world datasets
illustrate that DGT presents superior performance compared with several
state-of-the-art baselines.

    

### [[2111.10452] MURAL: An Unsupervised Random Forest-Based Embedding for Electronic Health Record Data](http://arxiv.org/abs/2111.10452)


  A major challenge in embedding or visualizing clinical patient data is the
heterogeneity of variable types including continuous lab values, categorical
diagnostic codes, as well as missing or incomplete data. In particular, in EHR
data, some variables are {\em missing not at random (MNAR)} but deliberately
not collected and thus are a source of information. For example, lab tests may
be deemed necessary for some patients on the basis of suspected diagnosis, but
not for others. Here we present the MURAL forest -- an unsupervised random
forest for representing data with disparate variable types (e.g., categorical,
continuous, MNAR). MURAL forests consist of a set of decision trees where
node-splitting variables are chosen at random, such that the marginal entropy
of all other variables is minimized by the split. This allows us to also split
on MNAR variables and discrete variables in a way that is consistent with the
continuous variables. The end goal is to learn the MURAL embedding of patients
using average tree distances between those patients. These distances can be fed
to nonlinear dimensionality reduction method like PHATE to derive visualizable
embeddings. While such methods are ubiquitous in continuous-valued datasets
(like single cell RNA-sequencing) they have not been used extensively in mixed
variable data. We showcase the use of our method on one artificial and two
clinical datasets. We show that using our approach, we can visualize and
classify data more accurately than competing approaches. Finally, we show that
MURAL can also be used to compare cohorts of patients via the recently proposed
tree-sliced Wasserstein distances.

    

### [[2111.10459] Identifying Population Movements with Non-Negative Matrix Factorization from Wi-Fi User Counts in Smart and Connected Cities](http://arxiv.org/abs/2111.10459)


  Non-Negative Matrix Factorization (NMF) is a valuable matrix factorization
technique which produces a "parts-based" decomposition of data sets. Wi-Fi user
counts are a privacy-preserving indicator of population movements in smart and
connected urban environments. In this paper, we apply NMF with a novel matrix
embedding to Wi-Fi user count data from the University of Colorado at Boulder
Campus for the purpose of automatically identifying patterns of human movement
in a Smart and Connected infrastructure environment.

    

### [[2111.10461] Gaussian Process Inference Using Mini-batch Stochastic Gradient Descent: Convergence Guarantees and Empirical Benefits](http://arxiv.org/abs/2111.10461)


  Stochastic gradient descent (SGD) and its variants have established
themselves as the go-to algorithms for large-scale machine learning problems
with independent samples due to their generalization performance and intrinsic
computational advantage. However, the fact that the stochastic gradient is a
biased estimator of the full gradient with correlated samples has led to the
lack of theoretical understanding of how SGD behaves under correlated settings
and hindered its use in such cases. In this paper, we focus on hyperparameter
estimation for the Gaussian process (GP) and take a step forward towards
breaking the barrier by proving minibatch SGD converges to a critical point of
the full log-likelihood loss function, and recovers model hyperparameters with
rate $O(\frac{1}{K})$ for $K$ iterations, up to a statistical error term
depending on the minibatch size. Our theoretical guarantees hold provided that
the kernel functions exhibit exponential or polynomial eigendecay which is
satisfied by a wide range of kernels commonly used in GPs. Numerical studies on
both simulated and real datasets demonstrate that minibatch SGD has better
generalization over state-of-the-art GP methods while reducing the
computational burden and opening a new, previously unexplored, data size regime
for GPs.

    

### [[2111.10471] SNPs Filtered by Allele Frequency Improve the Prediction of Hypertension Subtypes](http://arxiv.org/abs/2111.10471)


  Hypertension is the leading global cause of cardiovascular disease and
premature death. Distinct hypertension subtypes may vary in their prognoses and
require different treatments. An individual's risk for hypertension is
determined by genetic and environmental factors as well as their interactions.
In this work, we studied 911 African Americans and 1,171 European Americans in
the Hypertension Genetic Epidemiology Network (HyperGEN) cohort. We built
hypertension subtype classification models using both environmental variables
and sets of genetic features selected based on different criteria. The fitted
prediction models provided insights into the genetic landscape of hypertension
subtypes, which may aid personalized diagnosis and treatment of hypertension in
the future.

    

### [[2111.10476] Towards Return Parity in Markov Decision Processes](http://arxiv.org/abs/2111.10476)


  Algorithmic decisions made by machine learning models in high-stakes domains
may have lasting impacts over time. Unfortunately, naive applications of
standard fairness criterion in static settings over temporal domains may lead
to delayed and adverse effects. To understand the dynamics of performance
disparity, we study a fairness problem in Markov decision processes (MDPs).
Specifically, we propose return parity, a fairness notion that requires MDPs
from different demographic groups that share the same state and action spaces
to achieve approximately the same expected time-discounted rewards. We first
provide a decomposition theorem for return disparity, which decomposes the
return disparity of any two MDPs into the distance between group-wise reward
functions, the discrepancy of group policies, and the discrepancy between state
visitation distributions induced by the group policies. Motivated by our
decomposition theorem, we propose algorithms to mitigate return disparity via
learning a shared group policy with state visitation distributional alignment
using integral probability metrics. We conduct experiments to corroborate our
results, showing that the proposed algorithm can successfully close the
disparity gap while maintaining the performance of policies on two real-world
recommender system benchmark datasets.

    

### [[2111.10487] Federated Learning with Domain Generalization](http://arxiv.org/abs/2111.10487)


  Federated Learning (FL) enables a group of clients to jointly train a machine
learning model with the help of a centralized server. Clients do not need to
submit their local data to the server during training, and hence the local
training data of clients is protected. In FL, distributed clients collect their
local data independently, so the dataset of each client may naturally form a
distinct source domain. In practice, the model trained over multiple source
domains may have poor generalization performance on unseen target domains. To
address this issue, we propose FedADG to equip federated learning with domain
generalization capability. FedADG employs the federated adversarial learning
approach to measure and align the distributions among different source domains
via matching each distribution to a reference distribution. The reference
distribution is adaptively generated (by accommodating all source domains) to
minimize the domain shift distance during alignment. In FedADG, the alignment
is fine-grained since each class is aligned independently. In this way, the
learned feature representation is supposed to be universal, so it can
generalize well on the unseen domains. Extensive experiments on various
datasets demonstrate that FedADG has better performance than most of the
previous solutions even if they have an additional advantage that allows
centralized data access. To support study reproducibility, the project codes
are available in this https URL


### [[2111.10489] Modeling Design and Control Problems Involving Neural Network Surrogates](http://arxiv.org/abs/2111.10489)


  We consider nonlinear optimization problems that involve surrogate models
represented by neural networks. We demonstrate first how to directly embed
neural network evaluation into optimization models, highlight a difficulty with
this approach that can prevent convergence, and then characterize stationarity
of such models. We then present two alternative formulations of these problems
in the specific case of feedforward neural networks with ReLU activation: as a
mixed-integer optimization problem and as a mathematical program with
complementarity constraints. For the latter formulation we prove that
stationarity at a point for this problem corresponds to stationarity of the
embedded formulation. Each of these formulations may be solved with
state-of-the-art optimization methods, and we show how to obtain good initial
feasible solutions for these methods. We compare our formulations on three
practical applications arising in the design and control of combustion engines,
in the generation of adversarial attacks on classifier networks, and in the
determination of optimal flows in an oil well network.

    

### [[2111.10492] Feature selection or extraction decision process for clustering using PCA and FRSD](http://arxiv.org/abs/2111.10492)


  This paper concerns the critical decision process of extracting or selecting
the features before applying a clustering algorithm. It is not obvious to
evaluate the importance of the features since the most popular methods to do it
are usually made for a supervised learning technique process. A clustering
algorithm is an unsupervised method. It means that there is no known output
label to match the input data. This paper proposes a new method to choose the
best dimensionality reduction method (selection or extraction) according to the
data scientist's parameters, aiming to apply a clustering process at the end.
It uses Feature Ranking Process Based on Silhouette Decomposition (FRSD)
algorithm, a Principal Component Analysis (PCA) algorithm, and a K-Means
algorithm along with its metric, the Silhouette Index (SI). This paper presents
5 use cases based on a smart city dataset. This research also aims to discuss
the impacts, the advantages, and the disadvantages of each choice that can be
made in this unsupervised learning process.

    

### [[2111.10501] Exploring Language Patterns in a Medical Licensure Exam Item Bank](http://arxiv.org/abs/2111.10501)


  This study examines the use of natural language processing (NLP) models to
evaluate whether language patterns used by item writers in a medical licensure
exam might contain evidence of biased or stereotypical language. This type of
bias in item language choices can be particularly impactful for items in a
medical licensure assessment, as it could pose a threat to content validity and
defensibility of test score validity evidence. To the best of our knowledge,
this is the first attempt using machine learning (ML) and NLP to explore
language bias on a large item bank. Using a prediction algorithm trained on
clusters of similar item stems, we demonstrate that our approach can be used to
review large item banks for potential biased language or stereotypical patient
characteristics in clinical science vignettes. The findings may guide the
development of methods to address stereotypical language patterns found in test
items and enable an efficient updating of those items, if needed, to reflect
contemporary norms, thereby improving the evidence to support the validity of
the test scores.

    

### [[2111.10510] Bayesian Learning via Neural Schrödinger-Föllmer Flows](http://arxiv.org/abs/2111.10510)


  In this work we explore a new framework for approximate Bayesian inference in
large datasets based on stochastic control. We advocate stochastic control as a
finite time alternative to popular steady-state methods such as stochastic
gradient Langevin dynamics (SGLD). Furthermore, we discuss and adapt the
existing theoretical guarantees of this framework and establish connections to
already existing VI routines in SDE-based models.

    

### [[2111.10536] Quaternion-Based Graph Convolution Network for Recommendation](http://arxiv.org/abs/2111.10536)


  Graph Convolution Network (GCN) has been widely applied in recommender
systems for its representation learning capability on user and item embeddings.
However, GCN is vulnerable to noisy and incomplete graphs, which are common in
real world, due to its recursive message propagation mechanism. In the
literature, some work propose to remove the feature transformation during
message propagation, but making it unable to effectively capture the graph
structural features. Moreover, they model users and items in the Euclidean
space, which has been demonstrated to have high distortion when modeling
complex graphs, further degrading the capability to capture the graph
structural features and leading to sub-optimal performance. To this end, in
this paper, we propose a simple yet effective Quaternion-based Graph
Convolution Network (QGCN) recommendation model. In the proposed model, we
utilize the hyper-complex Quaternion space to learn user and item
representations and feature transformation to improve both performance and
robustness. Specifically, we first embed all users and items into the
Quaternion space. Then, we introduce the quaternion embedding propagation
layers with quaternion feature transformation to perform message propagation.
Finally, we combine the embeddings generated at each layer with the mean
pooling strategy to obtain the final embeddings for recommendation. Extensive
experiments on three public benchmark datasets demonstrate that our proposed
QGCN model outperforms baseline methods by a large margin.

    

### [[2111.10539] Edge-Enhanced Global Disentangled Graph Neural Network for Sequential Recommendation](http://arxiv.org/abs/2111.10539)


  Sequential recommendation has been a widely popular topic of recommender
systems. Existing works have contributed to enhancing the prediction ability of
sequential recommendation systems based on various methods, such as recurrent
networks and self-attention mechanisms. However, they fail to discover and
distinguish various relationships between items, which could be underlying
factors which motivate user behaviors. In this paper, we propose an
Edge-Enhanced Global Disentangled Graph Neural Network (EGD-GNN) model to
capture the relation information between items for global item representation
and local user intention learning. At the global level, we build a global-link
graph over all sequences to model item relationships. Then a channel-aware
disentangled learning layer is designed to decompose edge information into
different channels, which can be aggregated to represent the target item from
its neighbors. At the local level, we apply a variational auto-encoder
framework to learn user intention over the current sequence. We evaluate our
proposed method on three real-world datasets. Experimental results show that
our model can get a crucial improvement over state-of-the-art baselines and is
able to distinguish item features.

    

### [[2111.10552] Accelerating non-LTE synthesis and inversions with graph networks](http://arxiv.org/abs/2111.10552)


  Context: The computational cost of fast non-LTE synthesis is one of the
challenges that limits the development of 2D and 3D inversion codes. It also
makes the interpretation of observations of lines formed in the chromosphere
and transition region a slow and computationally costly process, which limits
the inference of the physical properties on rather small fields of view. Having
access to a fast way of computing the deviation from the LTE regime through the
departure coefficients could largely alleviate this problem. Aims: We propose
to build and train a graph network that quickly predicts the atomic level
populations without solving the non-LTE problem. Methods: We find an optimal
architecture for the graph network for predicting the departure coefficients of
the levels of an atom from the physical conditions of a model atmosphere. A
suitable dataset with a representative sample of potential model atmospheres is
used for training. This dataset has been computed using existing non-LTE
synthesis codes. Results: The graph network has been integrated into existing
synthesis and inversion codes for the particular case of \caii. We demonstrate
orders of magnitude gain in computing speed. We analyze the generalization
capabilities of the graph network and demonstrate that it produces good
predicted departure coefficients for unseen models. We implement this approach
in \hazel\ and show how the inversions nicely compare with those obtained with
standard non-LTE inversion codes. Our approximate method opens up the
possibility of extracting physical information from the chromosphere on large
fields-of-view with time evolution. This allows us to understand better this
region of the Sun, where large spatial and temporal scales are crucial.

    

### [[2111.10559] Learning Non-Stationary Time-Series with Dynamic Pattern Extractions](http://arxiv.org/abs/2111.10559)


  The era of information explosion had prompted the accumulation of a
tremendous amount of time-series data, including stationary and non-stationary
time-series data. State-of-the-art algorithms have achieved a decent
performance in dealing with stationary temporal data. However, traditional
algorithms that tackle stationary time-series do not apply to non-stationary
series like Forex trading. This paper investigates applicable models that can
improve the accuracy of forecasting future trends of non-stationary time-series
sequences. In particular, we focus on identifying potential models and
investigate the effects of recognizing patterns from historical data. We
propose a combination of \rebuttal{the} seq2seq model based on RNN, along with
an attention mechanism and an enriched set features extracted via dynamic time
warping and zigzag peak valley indicators. Customized loss functions and
evaluating metrics have been designed to focus more on the predicting
sequence's peaks and valley points. Our results show that our model can predict
4-hour future trends with high accuracy in the Forex dataset, which is crucial
in realistic scenarios to assist foreign exchange trading decision making. We
further provide evaluations of the effects of various loss functions,
evaluation metrics, model variants, and components on model performance.

    

### [[2111.10561] Teacher-Student Training and Triplet Loss to Reduce the Effect of Drastic Face Occlusion](http://arxiv.org/abs/2111.10561)


  We study a series of recognition tasks in two realistic scenarios requiring
the analysis of faces under strong occlusion. On the one hand, we aim to
recognize facial expressions of people wearing Virtual Reality (VR) headsets.
On the other hand, we aim to estimate the age and identify the gender of people
wearing surgical masks. For all these tasks, the common ground is that half of
the face is occluded. In this challenging setting, we show that convolutional
neural networks (CNNs) trained on fully-visible faces exhibit very low
performance levels. While fine-tuning the deep learning models on occluded
faces is extremely useful, we show that additional performance gains can be
obtained by distilling knowledge from models trained on fully-visible faces. To
this end, we study two knowledge distillation methods, one based on
teacher-student training and one based on triplet loss. Our main contribution
consists in a novel approach for knowledge distillation based on triplet loss,
which generalizes across models and tasks. Furthermore, we consider combining
distilled models learned through conventional teacher-student training or
through our novel teacher-student training based on triplet loss. We provide
empirical evidence showing that, in most cases, both individual and combined
knowledge distillation methods bring statistically significant performance
improvements. We conduct experiments with three different neural models (VGG-f,
VGG-face, ResNet-50) on various tasks (facial expression recognition, gender
recognition, age estimation), showing consistent improvements regardless of the
model or task.

    

### [[2111.10583] Generating meta-learning tasks to evolve parametric loss for classification learning](http://arxiv.org/abs/2111.10583)


  The field of meta-learning has seen a dramatic rise in interest in recent
years. In existing meta-learning approaches, learning tasks for training
meta-models are usually collected from public datasets, which brings the
difficulty of obtaining a sufficient number of meta-learning tasks with a large
amount of training data. In this paper, we propose a meta-learning approach
based on randomly generated meta-learning tasks to obtain a parametric loss for
classification learning based on big data. The loss is represented by a deep
neural network, called meta-loss network (MLN). To train the MLN, we construct
a large number of classification learning tasks through randomly generating
training data, validation data, and corresponding ground-truth linear
classifier. Our approach has two advantages. First, sufficient meta-learning
tasks with large number of training data can be obtained easily. Second, the
ground-truth classifier is given, so that the difference between the learned
classifier and the ground-truth model can be measured to reflect the
performance of MLN more precisely than validation accuracy. Based on this
difference, we apply the evolutionary strategy algorithm to find out the
optimal MLN. The resultant MLN not only leads to satisfactory learning effects
on generated linear classifier learning tasks for testing, but also behaves
very well on generated nonlinear classifier learning tasks and various public
classification tasks. Our MLN stably surpass cross-entropy (CE) and mean square
error (MSE) in testing accuracy and generalization ability. These results
illustrate the possibility of achieving satisfactory meta-learning effects
using generated learning tasks.

    

### [[2111.10586] Satellite Based Computing Networks with Federated Learning](http://arxiv.org/abs/2111.10586)


  Driven by the ever-increasing penetration and proliferation of data-driven
applications, a new generation of wireless communication, the sixth-generation
(6G) mobile system enhanced by artificial intelligence (AI), has attracted
substantial research interests. Among various candidate technologies of 6G, low
earth orbit (LEO) satellites have appealing characteristics of ubiquitous
wireless access. However, the costs of satellite communication (SatCom) are
still high, relative to counterparts of ground mobile networks. To support
massively interconnected devices with intelligent adaptive learning and reduce
expensive traffic in SatCom, we propose federated learning (FL) in LEO-based
satellite communication networks. We first review the state-of-the-art
LEO-based SatCom and related machine learning (ML) techniques, and then analyze
four possible ways of combining ML with satellite networks. The learning
performance of the proposed strategies is evaluated by simulation and results
reveal that FL-based computing networks improve the performance of
communication overheads and latency. Finally, we discuss future research topics
along this research direction.

    

### [[2111.10588] Vehicular Visible Light Communications Noise Analysis and Autoencoder Based Denoising](http://arxiv.org/abs/2111.10588)


  Vehicular visible light communications (V-VLC) is a promising intelligent
transportation systems (ITS) technology for vehicle-to-vehicle (V2V) and
vehicle-to-infrastructure (V2I) communications with the utilization of
light-emitting diodes (LEDs). The main degrading factor for the performance of
V-VLC systems is noise. Unlike traditional radio frequency (RF) based systems,
V-VLC systems include many noise sources: solar radiation, background lighting
from vehicles, streets, parking garages, and tunnel lights. Traditional V-VLC
system noise modeling is based on the additive white Gaussian noise assumption
in the form of shot and thermal noise. In this paper, to investigate both
time-correlated and white noise components of the V-VLC channel, we propose a
noise analysis based on Allan variance (AVAR), which provides a time-series
analysis method to identify noise from the data. We also propose a generalized
Wiener process-based V-VLC channel noise synthesis methodology to generate
different noise components. We further propose a convolutional autoencoder(CAE)
based denoising scheme to reduce V-VLC signal noise, which achieves
reconstruction root mean square error (RMSE) of 0.0442 and 0.0474 for indoor
and outdoor channels, respectively.

    

### [[2111.10592] Deep Spoken Keyword Spotting: An Overview](http://arxiv.org/abs/2111.10592)


  Spoken keyword spotting (KWS) deals with the identification of keywords in
audio streams and has become a fast-growing technology thanks to the paradigm
shift introduced by deep learning a few years ago. This has allowed the rapid
embedding of deep KWS in a myriad of small electronic devices with different
purposes like the activation of voice assistants. Prospects suggest a sustained
growth in terms of social use of this technology. Thus, it is not surprising
that deep KWS has become a hot research topic among speech scientists, who
constantly look for KWS performance improvement and computational complexity
reduction. This context motivates this paper, in which we conduct a literature
review into deep spoken KWS to assist practitioners and researchers who are
interested in this technology. Specifically, this overview has a comprehensive
nature by covering a thorough analysis of deep KWS systems (which includes
speech features, acoustic modeling and posterior handling), robustness methods,
applications, datasets, evaluation metrics, performance of deep KWS systems and
audio-visual KWS. The analysis performed in this paper allows us to identify a
number of directions for future research, including directions adopted from
automatic speech recognition research and directions that are unique to the
problem of spoken KWS.

    

### [[2111.10596] Semi-supervised Impedance Inversion by Bayesian Neural Network Based on 2-d CNN Pre-training](http://arxiv.org/abs/2111.10596)


  Seismic impedance inversion can be performed with a semi-supervised learning
algorithm, which only needs a few logs as labels and is less likely to get
overfitted. However, classical semi-supervised learning algorithm usually leads
to artifacts on the predicted impedance image. In this artical, we improve the
semi-supervised learning from two aspects. First, by replacing 1-d
convolutional neural network (CNN) layers in deep learning structure with 2-d
CNN layers and 2-d maxpooling layers, the prediction accuracy is improved.
Second, prediction uncertainty can also be estimated by embedding the network
into a Bayesian inference framework. Local reparameterization trick is used
during forward propagation of the network to reduce sampling cost. Tests with
Marmousi2 model and SEAM model validate the feasibility of the proposed
strategy.

    

### [[2111.10601] Safe Multi-Task Learning](http://arxiv.org/abs/2111.10601)


  In recent years, Multi-Task Learning (MTL) attracts much attention due to its
good performance in many applications. However, many existing MTL models cannot
guarantee that its performance is no worse than its single-task counterpart on
each task. Though this phenomenon has been empirically observed by some works,
little work aims to handle the resulting problem, which is formally defined as
negative sharing in this paper. To achieve safe multi-task learning where no
\textit{negative sharing} occurs, we propose a Safe Multi-Task Learning (SMTL)
model, which consists of a public encoder shared by all the tasks, private
encoders, gates, and private decoders. Specifically, each task has a private
encoder, a gate, and a private decoder, where the gate is to learn how to
combine the private encoder and public encoder for the downstream private
decoder. To reduce the storage cost during the inference stage, a lite version
of SMTL is proposed to allow the gate to choose either the public encoder or
the corresponding private encoder. Moreover, we propose a variant of SMTL to
place all the gates after decoders of all the tasks. Experiments on several
benchmark datasets demonstrate the effectiveness of the proposed methods.

    

### [[2111.10603] A Closer Look at Loss Weighting in Multi-Task Learning](http://arxiv.org/abs/2111.10603)


  Multi-Task Learning (MTL) has achieved great success in various fields,
however, how to balance different tasks to avoid negative effects is still a
key problem. To achieve the task balancing, there exist many works to balance
task losses or gradients. In this paper, we unify eight representative task
balancing methods from the perspective of loss weighting and provide a
consistent experimental comparison. Moreover, we surprisingly find that
training a MTL model with random weights sampled from a distribution can
achieve comparable performance over state-of-the-art baselines. Based on this
finding, we propose a simple yet effective weighting strategy called Random
Loss Weighting (RLW), which can be implemented in only one additional line of
code over existing works. Theoretically, we analyze the convergence of RLW and
reveal that RLW has a higher probability to escape local minima than existing
models with fixed task weights, resulting in a better generalization ability.
Empirically, we extensively evaluate the proposed RLW method on six image
datasets and four multilingual tasks from the XTREME benchmark to show the
effectiveness of the proposed RLW strategy when compared with state-of-the-art
strategies.

    

### [[2111.10610] Constrained Deep One-Class Feature Learning For Classifying Imbalanced Medical Images](http://arxiv.org/abs/2111.10610)


  Medical image data are usually imbalanced across different classes. One-class
classification has attracted increasing attention to address the data imbalance
problem by distinguishing the samples of the minority class from the majority
class. Previous methods generally aim to either learn a new feature space to
map training samples together or to fit training samples by autoencoder-like
models. These methods mainly focus on capturing either compact or descriptive
features, where the information of the samples of a given one class is not
sufficiently utilized. In this paper, we propose a novel deep learning-based
method to learn compact features by adding constraints on the bottleneck
features, and to preserve descriptive features by training an autoencoder at
the same time. Through jointly optimizing the constraining loss and the
autoencoder's reconstruction loss, our method can learn more relevant features
associated with the given class, making the majority and minority samples more
distinguishable. Experimental results on three clinical datasets (including the
MRI breast images, FFDM breast images and chest X-ray images) obtains
state-of-art performance compared to previous methods.

    

### [[2111.10617] Extracting Deformation-Aware Local Features by Learning to Deform](http://arxiv.org/abs/2111.10617)


  Despite the advances in extracting local features achieved by handcrafted and
learning-based descriptors, they are still limited by the lack of invariance to
non-rigid transformations. In this paper, we present a new approach to compute
features from still images that are robust to non-rigid deformations to
circumvent the problem of matching deformable surfaces and objects. Our
deformation-aware local descriptor, named DEAL, leverages a polar sampling and
a spatial transformer warping to provide invariance to rotation, scale, and
image deformations. We train the model architecture end-to-end by applying
isometric non-rigid deformations to objects in a simulated environment as
guidance to provide highly discriminative local features. The experiments show
that our method outperforms state-of-the-art handcrafted, learning-based image,
and RGB-D descriptors in different datasets with both real and realistic
synthetic deformable objects in still images. The source code and trained model
of the descriptor are publicly available at
this https URL.

    

### [[2111.10622] SPINE: Soft Piecewise Interpretable Neural Equations](http://arxiv.org/abs/2111.10622)


  Relu Fully Connected Networks are ubiquitous but uninterpretable because they
fit piecewise linear functions emerging from multi-layered structures and
complex interactions of model weights. This paper takes a novel approach to
piecewise fits by using set operations on individual pieces(parts). This is
done by approximating canonical normal forms and using the resultant as a
model. This gives special advantages like (a)strong correspondence of
parameters to pieces of the fit function(High Interpretability); (b)ability to
fit any combination of continuous functions as pieces of the piecewise
function(Ease of Design); (c)ability to add new non-linearities in a targeted
region of the domain(Targeted Learning); (d)simplicity of an equation which
avoids layering. It can also be expressed in the general max-min representation
of piecewise linear functions which gives theoretical ease and credibility.
This architecture is tested on simulated regression and classification tasks
and benchmark datasets including UCI datasets, MNIST, FMNIST, and CIFAR 10.
This performance is on par with fully connected architectures. It can find a
variety of applications where fully connected layers must be replaced by
interpretable layers.

    

### [[2111.10625] Explainable Biomedical Recommendations via Reinforcement Learning Reasoning on Knowledge Graphs](http://arxiv.org/abs/2111.10625)


  For Artificial Intelligence to have a greater impact in biology and medicine,
it is crucial that recommendations are both accurate and transparent. In other
domains, a neurosymbolic approach of multi-hop reasoning on knowledge graphs
has been shown to produce transparent explanations. However, there is a lack of
research applying it to complex biomedical datasets and problems. In this
paper, the approach is explored for drug discovery to draw solid conclusions on
its applicability. For the first time, we systematically apply it to multiple
biomedical datasets and recommendation tasks with fair benchmark comparisons.
The approach is found to outperform the best baselines by 21.7% on average
whilst producing novel, biologically relevant explanations.

    

### [[2111.10626] Learning algorithms versus automatability of Frege systems](http://arxiv.org/abs/2111.10626)


  We connect learning algorithms and algorithms automating proof search in
propositional proof systems: for every sufficiently strong, well-behaved
propositional proof system $P$, we prove that the following statements are
equivalent,
1. Provable learning: $P$ proves efficiently that p-size circuits are
learnable by subexponential-size circuits over the uniform distribution with
membership queries.
2. Provable automatability: $P$ proves efficiently that $P$ is automatable by
non-uniform circuits on propositional formulas expressing p-size circuit lower
bounds.
Here, $P$ is sufficiently strong and well-behaved if I.-III. holds: I. $P$
p-simulates Jeřábek's system $WF$ (which strengthens the Extended Frege
system $EF$ by a surjective weak pigeonhole principle); II. $P$ satisfies some
basic properties of standard proof systems which p-simulate $WF$; III. $P$
proves efficiently for some Boolean function $h$ that $h$ is hard on average
for circuits of subexponential size. For example, if III. holds for $P=WF$,
then Items 1 and 2 are equivalent for $P=WF$.
If there is a function $h\in NE\cap coNE$ which is hard on average for
circuits of size $2^{n/4}$, for each sufficiently big $n$, then there is an
explicit propositional proof system $P$ satisfying properties I.-III., i.e. the
equivalence of Items 1 and 2 holds for $P$.

    

### [[2111.10635] HeterPS: Distributed Deep Learning With Reinforcement Learning Based Scheduling in Heterogeneous Environments](http://arxiv.org/abs/2111.10635)


  Deep neural networks (DNNs) exploit many layers and a large number of
parameters to achieve excellent performance. The training process of DNN models
generally handles large-scale input data with many sparse features, which
incurs high Input/Output (IO) cost, while some layers are compute-intensive.
The training process generally exploits distributed computing resources to
reduce training time. In addition, heterogeneous computing resources, e.g.,
CPUs, GPUs of multiple types, are available for the distributed training
process. Thus, the scheduling of multiple layers to diverse computing resources
is critical for the training process. To efficiently train a DNN model using
the heterogeneous computing resources, we propose a distributed framework,
i.e., Paddle-Heterogeneous Parameter Server (Paddle-HeterPS), composed of a
distributed architecture and a Reinforcement Learning (RL)-based scheduling
method. The advantages of Paddle-HeterPS are three-fold compared with existing
frameworks. First, Paddle-HeterPS enables efficient training process of diverse
workloads with heterogeneous computing resources. Second, Paddle-HeterPS
exploits an RL-based method to efficiently schedule the workload of each layer
to appropriate computing resources to minimize the cost while satisfying
throughput constraints. Third, Paddle-HeterPS manages data storage and data
communication among distributed computing resources. We carry out extensive
experiments to show that Paddle-HeterPS significantly outperforms
state-of-the-art approaches in terms of throughput (14.5 times higher) and
monetary cost (312.3% smaller). The codes of the framework are publicly
available at: this https URL.

    

### [[2111.10639] Implicit Acoustic Echo Cancellation for Keyword Spotting and Device-Directed Speech Detection](http://arxiv.org/abs/2111.10639)


  In many speech-enabled human-machine interaction scenarios, user speech can
overlap with the device playback audio. In these instances, the performance of
tasks such as keyword-spotting (KWS) and device-directed speech detection (DDD)
can degrade significantly. To address this problem, we propose an implicit
acoustic echo cancellation (iAEC) framework where a neural network is trained
to exploit the additional information from a reference microphone channel to
learn to ignore the interfering signal and improve detection performance. We
study this framework for the tasks of KWS and DDD on, respectively, an
augmented version of Google Speech Commands v2 and a real-world Alexa device
dataset. Notably, we show a $56\%$ reduction in false-reject rate for the DDD
task during device playback conditions. We also show comparable or superior
performance over a strong end-to-end neural echo cancellation + KWS baseline
for the KWS task with an order of magnitude less computational requirements.

    

### [[2111.10653] Real-time Human Detection Model for Edge Devices](http://arxiv.org/abs/2111.10653)


  Building a small-sized fast surveillance system model to fit on limited
resource devices is a challenging, yet an important task. Convolutional Neural
Networks (CNNs) have replaced traditional feature extraction and machine
learning models in detection and classification tasks. Various complex large
CNN models are proposed that achieve significant improvement in the accuracy.
Lightweight CNN models have been recently introduced for real-time tasks. This
paper suggests a CNN-based lightweight model that can fit on a limited edge
device such as Raspberry Pi. Our proposed model provides better performance
time, smaller size and comparable accuracy with existing method. The model
performance is evaluated on multiple benchmark datasets. It is also compared
with existing models in terms of size, average processing time, and F-score.
Other enhancements for future research are suggested.

    

### [[2111.10656] Simple End-to-end Deep Learning Model for CDR-H3 Loop Structure Prediction](http://arxiv.org/abs/2111.10656)


  Predicting a structure of an antibody from its sequence is important since it
allows for a better design process of synthetic antibodies that play a vital
role in the health industry. Most of the structure of an antibody is
conservative. The most variable and hard-to-predict part is the {\it third
complementarity-determining region of the antibody heavy chain} (CDR H3).
Lately, deep learning has been employed to solve the task of CDR H3 prediction.
However, current state-of-the-art methods are not end-to-end, but rather they
output inter-residue distances and orientations to the RosettaAntibody package
that uses this additional information alongside statistical and physics-based
methods to predict the 3D structure. This does not allow a fast screening
process and, therefore, inhibits the development of targeted synthetic
antibodies. In this work, we present an end-to-end model to predict CDR H3 loop
structure, that performs on par with state-of-the-art methods in terms of
accuracy but an order of magnitude faster. We also raise an issue with a
commonly used RosettaAntibody benchmark that leads to data leaks, i.e., the
presence of identical sequences in the train and test datasets.

    

### [[2111.10657] Generalizing Graph Neural Networks on Out-Of-Distribution Graphs](http://arxiv.org/abs/2111.10657)


  Graph Neural Networks (GNNs) are proposed without considering the agnostic
distribution shifts between training and testing graphs, inducing the
degeneration of the generalization ability of GNNs on Out-Of-Distribution (OOD)
settings. The fundamental reason for such degeneration is that most GNNs are
developed based on the I.I.D hypothesis. In such a setting, GNNs tend to
exploit subtle statistical correlations existing in the training set for
predictions, even though it is a spurious correlation. However, such spurious
correlations may change in testing environments, leading to the failure of
GNNs. Therefore, eliminating the impact of spurious correlations is crucial for
stable GNNs. To this end, we propose a general causal representation framework,
called StableGNN. The main idea is to extract high-level representations from
graph data first and resort to the distinguishing ability of causal inference
to help the model get rid of spurious correlations. Particularly, we exploit a
graph pooling layer to extract subgraph-based representations as high-level
representations. Furthermore, we propose a causal variable distinguishing
regularizer to correct the biased training distribution. Hence, GNNs would
concentrate more on the stable correlations. Extensive experiments on both
synthetic and real-world OOD graph datasets well verify the effectiveness,
flexibility and interpretability of the proposed framework.

    

### [[2111.10672] Doing More by Doing Less: How Structured Partial Backpropagation Improves Deep Learning Clusters](http://arxiv.org/abs/2111.10672)


  Many organizations employ compute clusters equipped with accelerators such as
GPUs and TPUs for training deep learning models in a distributed fashion.
Training is resource-intensive, consuming significant compute, memory, and
network resources. Many prior works explore how to reduce training resource
footprint without impacting quality, but their focus on a subset of the
bottlenecks (typically only the network) limits their ability to improve
overall cluster utilization. In this work, we exploit the unique
characteristics of deep learning workloads to propose Structured Partial
Backpropagation(SPB), a technique that systematically controls the amount of
backpropagation at individual workers in distributed training. This
simultaneously reduces network bandwidth, compute utilization, and memory
footprint while preserving model quality. To efficiently leverage the benefits
of SPB at cluster level, we introduce JigSaw, a SPB aware scheduler, which does
scheduling at the iteration level for Deep Learning Training(DLT) jobs. We find
that JigSaw can improve large scale cluster efficiency by as high as 28\%.

    

### [[2111.10683] A Review on The Division of Magnetic Resonant Prostate Images with Deep Learning](http://arxiv.org/abs/2111.10683)


  Deep learning; it is often used in dividing processes on images in the
biomedical field. In recent years, it has been observed that there is an
increase in the division procedures performed on prostate images using deep
learning compared to other methods of image division. Looking at the
literature; It is seen that the process of dividing prostate images, which are
carried out with deep learning, is an important step for the diagnosis and
treatment of prostate cancer. For this reason, in this study; to be a source
for future splitting operations; deep learning splitting procedures on prostate
images obtained from magnetic resonance (MRI) imaging devices were examined.

    

### [[2111.10686] Representing Prior Knowledge Using Randomly, Weighted Feature Networks for Visual Relationship Detection](http://arxiv.org/abs/2111.10686)


  The single-hidden-layer Randomly Weighted Feature Network (RWFN) introduced
by Hong and Pavlic (2021) was developed as an alternative to neural tensor
network approaches for relational learning tasks. Its relatively small
footprint combined with the use of two randomized input projections -- an
insect-brain-inspired input representation and random Fourier features -- allow
it to achieve rich expressiveness for relational learning with relatively low
training cost. In particular, when Hong and Pavlic compared RWFN to Logic
Tensor Networks (LTNs) for Semantic Image Interpretation (SII) tasks to extract
structured semantic descriptions from images, they showed that the RWFN
integration of the two hidden, randomized representations better captures
relationships among inputs with a faster training process even though it uses
far fewer learnable parameters. In this paper, we use RWFNs to perform Visual
Relationship Detection (VRD) tasks, which are more challenging SII tasks. A
zero-shot learning approach is used with RWFN that can exploit similarities
with other seen relationships and background knowledge -- expressed with
logical constraints between subjects, relations, and objects -- to achieve the
ability to predict triples that do not appear in the training set. The
experiments on the Visual Relationship Dataset to compare the performance
between RWFNs and LTNs, one of the leading Statistical Relational Learning
frameworks, show that RWFNs outperform LTNs for the predicate-detection task
while using fewer number of adaptable parameters (1:56 ratio). Furthermore,
background knowledge represented by RWFNs can be used to alleviate the
incompleteness of training sets even though the space complexity of RWFNs is
much smaller than LTNs (1:27 ratio).

    

### [[2111.10695] Image-Like Graph Representations for Improved Molecular Property Prediction](http://arxiv.org/abs/2111.10695)


  Research into deep learning models for molecular property prediction has
primarily focused on the development of better Graph Neural Network (GNN)
architectures. Though new GNN variants continue to improve performance, their
modifications share a common theme of alleviating problems intrinsic to their
fundamental graph-to-graph nature. In this work, we examine these limitations
and propose a new molecular representation that bypasses the need for GNNs
entirely, dubbed CubeMol. Our fixed-dimensional stochastic representation, when
paired with a transformer model, exceeds the performance of state-of-the-art
GNN models and provides a path for scalability.

    

### [[2111.10698] Towards Graph Self-Supervised Learning with Contrastive Adjusted Zooming](http://arxiv.org/abs/2111.10698)


  Graph representation learning (GRL) is critical for graph-structured data
analysis. However, most of the existing graph neural networks (GNNs) heavily
rely on labeling information, which is normally expensive to obtain in the real
world. Existing unsupervised GRL methods suffer from certain limitations, such
as the heavy reliance on monotone contrastiveness and limited scalability. To
overcome the aforementioned problems, in light of the recent advancements in
graph contrastive learning, we introduce a novel self-supervised graph
representation learning algorithm via Graph Contrastive Adjusted Zooming,
namely G-Zoom, to learn node representations by leveraging the proposed
adjusted zooming scheme. Specifically, this mechanism enables G-Zoom to explore
and extract self-supervision signals from a graph from multiple scales: micro
(i.e., node-level), meso (i.e., neighbourhood-level), and macro (i.e.,
subgraph-level). Firstly, we generate two augmented views of the input graph
via two different graph augmentations. Then, we establish three different
contrastiveness on the above three scales progressively, from node,
neighbouring, to subgraph level, where we maximize the agreement between graph
representations across scales. While we can extract valuable clues from a given
graph on the micro and macro perspectives, the neighbourhood-level
contrastiveness offers G-Zoom the capability of a customizable option based on
our adjusted zooming scheme to manually choose an optimal viewpoint that lies
between the micro and macro perspectives to better understand the graph data.
Additionally, to make our model scalable to large graphs, we employ a parallel
graph diffusion approach to decouple model training from the graph size. We
have conducted extensive experiments on real-world datasets, and the results
demonstrate that our proposed model outperforms state-of-the-art methods
consistently.

    

### [[2111.10699] Faster Deterministic Approximation Algorithms for Correlation Clustering and Cluster Deletion](http://arxiv.org/abs/2111.10699)


  Correlation clustering is a framework for partitioning datasets based on
pairwise similarity and dissimilarity scores, and has been used for diverse
applications in bioinformatics, social network analysis, and computer vision.
Although many approximation algorithms have been designed for this problem, the
best theoretical results rely on obtaining lower bounds via expensive linear
programming relaxations. In this paper we prove new relationships between
correlation clustering problems and edge labeling problems related to the
principle of strong triadic closure. We use these connections to develop new
approximation algorithms for correlation clustering that have deterministic
constant factor approximation guarantees and avoid the canonical linear
programming relaxation. Our approach also extends to a variant of correlation
clustering called cluster deletion, that strictly prohibits placing negative
edges inside clusters. Our results include 4-approximation algorithms for
cluster deletion and correlation clustering, based on simplified linear
programs with far fewer constraints than the canonical relaxations. More
importantly, we develop faster techniques that are purely combinatorial, based
on computing maximal matchings in certain auxiliary graphs and hypergraphs.
This leads to a combinatorial 6-approximation for complete unweighted
correlation clustering, which is the best deterministic result for any method
that does not rely on linear programming. We also present the first
combinatorial constant factor approximation for cluster deletion.

    

### [[2111.10701] Self-Supervised Point Cloud Completion via Inpainting](http://arxiv.org/abs/2111.10701)


  When navigating in urban environments, many of the objects that need to be
tracked and avoided are heavily occluded. Planning and tracking using these
partial scans can be challenging. The aim of this work is to learn to complete
these partial point clouds, giving us a full understanding of the object's
geometry using only partial observations. Previous methods achieve this with
the help of complete, ground-truth annotations of the target objects, which are
available only for simulated datasets. However, such ground truth is
unavailable for real-world LiDAR data. In this work, we present a
self-supervised point cloud completion algorithm, PointPnCNet, which is trained
only on partial scans without assuming access to complete, ground-truth
annotations. Our method achieves this via inpainting. We remove a portion of
the input data and train the network to complete the missing region. As it is
difficult to determine which regions were occluded in the initial cloud and
which were synthetically removed, our network learns to complete the full
cloud, including the missing regions in the initial partial cloud. We show that
our method outperforms previous unsupervised and weakly-supervised methods on
both the synthetic dataset, ShapeNet, and real-world LiDAR dataset, Semantic
KITTI.

    

### [[2111.10708] PAC-Learning Uniform Ergodic Communicative Networks](http://arxiv.org/abs/2111.10708)


  This work addressed the problem of learning a network with communication
between vertices. The communication between vertices is presented in the form
of perturbation on the measure. We studied the scenario where samples are drawn
from a uniform ergodic Random Graph Process (RGPs for short), which provides a
natural mathematical context for the problem of interest. For the binary
classification problem, the result we obtained gives uniform learn-ability as
the worst-case theoretical limits. We introduced the structural Rademacher
complexity, which naturally fused into the VC theory to upperbound the first
moment. With the martingale method and Marton's coupling, we establish the tail
bound for uniform convergence and give consistency guarantee for empirical risk
minimizer. The technique used in this work to obtain high probability bounds is
of independent interest to other mixing processes with and without network
structure.

    

### [[2111.10722] Low-Discrepancy Points via Energetic Variational Inference](http://arxiv.org/abs/2111.10722)


  In this paper, we propose a deterministic variational inference approach and
generate low-discrepancy points by minimizing the kernel discrepancy, also
known as the Maximum Mean Discrepancy or MMD. Based on the general energetic
variational inference framework by Wang et. al. (2021), minimizing the kernel
discrepancy is transformed to solving a dynamic ODE system via the explicit
Euler scheme. We name the resulting algorithm EVI-MMD and demonstrate it
through examples in which the target distribution is fully specified, partially
specified up to the normalizing constant, and empirically known in the form of
training data. Its performances are satisfactory compared to alternative
methods in the applications of distribution approximation, numerical
integration, and generative learning. The EVI-MMD algorithm overcomes the
bottleneck of the existing MMD-descent algorithms, which are mostly applicable
to two-sample problems. Algorithms with more sophisticated structures and
potential advantages can be developed under the EVI framework.

    

### [[2111.10723] End-to-end Learning for Fair Ranking Systems](http://arxiv.org/abs/2111.10723)


  The learning-to-rank problem aims at ranking items to maximize exposure of
those most relevant to a user query. A desirable property of such ranking
systems is to guarantee some notion of fairness among specified item groups.
While fairness has recently been considered in the context of learning-to-rank
systems, current methods cannot provide guarantees on the fairness of the
proposed ranking policies.
This paper addresses this gap and introduces Smart Predict and Optimize for
Fair Ranking (SPOFR), an integrated optimization and learning framework for
fairness-constrained learning to rank. The end-to-end SPOFR framework includes
a constrained optimization sub-model and produces ranking policies that are
guaranteed to satisfy fairness constraints while allowing for fine control of
the fairness-utility tradeoff. SPOFR is shown to significantly improve current
state-of-the-art fair learning-to-rank systems with respect to established
performance metrics.

    

### [[2111.10734] Deep Probability Estimation](http://arxiv.org/abs/2111.10734)


  Reliable probability estimation is of crucial importance in many real-world
applications where there is inherent uncertainty, such as weather forecasting,
medical prognosis, or collision avoidance in autonomous vehicles.
Probability-estimation models are trained on observed outcomes (e.g. whether it
has rained or not, or whether a patient has died or not), because the
ground-truth probabilities of the events of interest are typically unknown. The
problem is therefore analogous to binary classification, with the important
difference that the objective is to estimate probabilities rather than
predicting the specific outcome. The goal of this work is to investigate
probability estimation from high-dimensional data using deep neural networks.
There exist several methods to improve the probabilities generated by these
models but they mostly focus on classification problems where the probabilities
are related to model uncertainty. In the case of problems with inherent
uncertainty, it is challenging to evaluate performance without access to
ground-truth probabilities. To address this, we build a synthetic dataset to
study and compare different computable metrics. We evaluate existing methods on
the synthetic data as well as on three real-world probability estimation tasks,
all of which involve inherent uncertainty: precipitation forecasting from radar
images, predicting cancer patient survival from histopathology images, and
predicting car crashes from dashcam videos. Finally, we also propose a new
method for probability estimation using neural networks, which modifies the
training process to promote output probabilities that are consistent with
empirical probabilities computed from the data. The method outperforms existing
approaches on most metrics on the simulated as well as real-world data.

    

### [[2111.10752] Stochastic Variance Reduced Ensemble Adversarial Attack for Boosting the Adversarial Transferability](http://arxiv.org/abs/2111.10752)


  The black-box adversarial attack has attracted impressive attention for its
practical use in the field of deep learning security, meanwhile, it is very
challenging as there is no access to the network architecture or internal
weights of the target model. Based on the hypothesis that if an example remains
adversarial for multiple models, then it is more likely to transfer the attack
capability to other models, the ensemble-based adversarial attack methods are
efficient and widely used for black-box attacks. However, ways of ensemble
attack are rather less investigated, and existing ensemble attacks simply fuse
the outputs of all the models evenly. In this work, we treat the iterative
ensemble attack as a stochastic gradient descent optimization process, in which
the variance of the gradients on different models may lead to poor local
optima. To this end, we propose a novel attack method called the stochastic
variance reduced ensemble (SVRE) attack, which could reduce the gradient
variance of the ensemble models and take full advantage of the ensemble attack.
Empirical results on the standard ImageNet dataset demonstrate that the
proposed method could boost the adversarial transferability and outperforms
existing ensemble attacks significantly.

    

### [[2111.10754] Local Linearity and Double Descent in Catastrophic Overfitting](http://arxiv.org/abs/2111.10754)


  Catastrophic overfitting is a phenomenon observed during Adversarial Training
(AT) with the Fast Gradient Sign Method (FGSM) where the test robustness
steeply declines over just one epoch in the training stage. Prior work has
attributed this loss in robustness to a sharp decrease in $\textit{local
linearity}$ of the neural network with respect to the input space, and has
demonstrated that introducing a local linearity measure as a regularization
term prevents catastrophic overfitting. Using a simple neural network
architecture, we experimentally demonstrate that maintaining high local
linearity might be $\textit{sufficient}$ to prevent catastrophic overfitting
but is not $\textit{necessary.}$ Further, inspired by Parseval networks, we
introduce a regularization term to AT with FGSM to make the weight matrices of
the network orthogonal and study the connection between orthogonality of the
network weights and local linearity. Lastly, we identify the $\textit{double
descent}$ phenomenon during the adversarial training process.

    

### [[2111.10755] A Pseudo-Inverse for Nonlinear Operators](http://arxiv.org/abs/2111.10755)


  The Moore-Penrose inverse is widely used in physics, statistics and various
fields of engineering. Among other characteristics, it captures well the notion
of inversion of linear operators in the case of overcomplete data. In data
science, nonlinear operators are extensively used. In this paper we define and
characterize the fundamental properties of a pseudo-inverse for nonlinear
operators.
The concept is defined broadly. First for general sets, and then a refinement
for normed spaces. Our pseudo-inverse for normed spaces yields the
Moore-Penrose inverse when the operator is a matrix. We present conditions for
existence and uniqueness of a pseudo-inverse and establish theoretical results
investigating its properties, such as continuity, its value for operator
compositions and projection operators, and others. Analytic expressions are
given for the pseudo-inverse of some well-known, non-invertible, nonlinear
operators, such as hard- or soft-thresholding and ReLU. Finally, we analyze a
neural layer and discuss relations to wavelet thresholding and to regularized
loss minimization.

    

### [[2111.10756] TraVLR: Now You See It, Now You Don't! Evaluating Cross-Modal Transfer of Visio-Linguistic Reasoning](http://arxiv.org/abs/2111.10756)


  Numerous visio-linguistic (V+L) representation learning methods have been
developed, yet existing datasets do not evaluate the extent to which they
represent visual and linguistic concepts in a unified space. Inspired by the
crosslingual transfer and psycholinguistics literature, we propose a novel
evaluation setting for V+L models: zero-shot cross-modal transfer. Existing V+L
benchmarks also often report global accuracy scores on the entire dataset,
rendering it difficult to pinpoint the specific reasoning tasks that models
fail and succeed at. To address this issue and enable the evaluation of
cross-modal transfer, we present TraVLR, a synthetic dataset comprising four
V+L reasoning tasks. Each example encodes the scene bimodally such that either
modality can be dropped during training/testing with no loss of relevant
information. TraVLR's training and testing distributions are also constrained
along task-relevant dimensions, enabling the evaluation of out-of-distribution
generalisation. We evaluate four state-of-the-art V+L models and find that
although they perform well on the test set from the same modality, all models
fail to transfer cross-modally and have limited success accommodating the
addition or deletion of one modality. In alignment with prior work, we also
find these models to require large amounts of data to learn simple spatial
relationships. We release TraVLR as an open challenge for the research
community.

    

### [[2111.10759] Adversarial Mask: Real-World Adversarial Attack Against Face Recognition Models](http://arxiv.org/abs/2111.10759)


  Deep learning-based facial recognition (FR) models have demonstrated
state-of-the-art performance in the past few years, even when wearing
protective medical face masks became commonplace during the COVID-19 pandemic.
Given the outstanding performance of these models, the machine learning
research community has shown increasing interest in challenging their
robustness. Initially, researchers presented adversarial attacks in the digital
domain, and later the attacks were transferred to the physical domain. However,
in many cases, attacks in the physical domain are conspicuous, requiring, for
example, the placement of a sticker on the face, and thus may raise suspicion
in real-world environments (e.g., airports). In this paper, we propose
Adversarial Mask, a physical adversarial universal perturbation (UAP) against
state-of-the-art FR models that is applied on face masks in the form of a
carefully crafted pattern. In our experiments, we examined the transferability
of our adversarial mask to a wide range of FR model architectures and datasets.
In addition, we validated our adversarial mask effectiveness in real-world
experiments by printing the adversarial pattern on a fabric medical face mask,
causing the FR system to identify only 3.34% of the participants wearing the
mask (compared to a minimum of 83.34% with other evaluated masks).

    

### [[2111.10762] COVID-19 Detection through Deep Feature Extraction](http://arxiv.org/abs/2111.10762)


  The SARS-CoV2 virus has caused a lot of tribulation to the human population.
Predictive modeling that can accurately determine whether a person is infected
with COVID-19 is imperative. The study proposes a novel approach that utilizes
deep feature extraction technique, pre-trained ResNet50 acting as the backbone
of the network, combined with Logistic Regression as the head model. The
proposed model has been trained on Kaggle COVID-19 Radiography Dataset. The
proposed model achieves a cross-validation accuracy of 100% on the COVID-19 and
Normal X-Ray image classes. Similarly, when tested on combined three classes,
the proposed model achieves 98.84% accuracy.

    

### [[2111.10763] Distributed Unsupervised Visual Representation Learning with Fused Features](http://arxiv.org/abs/2111.10763)


  Federated learning (FL) enables distributed clients to learn a shared model
for prediction while keeping the training data local on each client. However,
existing FL requires fully-labeled data for training, which is inconvenient or
sometimes infeasible to obtain due to the high labeling cost and the
requirement of expertise. The lack of labels makes FL impractical in many
realistic settings. Self-supervised learning can address this challenge by
learning from unlabeled data such that FL can be widely used. Contrastive
learning (CL), a self-supervised learning approach, can effectively learn data
representations from unlabeled data. However, the distributed data collected on
clients are usually not independent and identically distributed (non-IID) among
clients, and each client may only have few classes of data, which degrades the
performance of CL and learned representations. To tackle this problem, we
propose a federated contrastive learning framework consisting of two
approaches: feature fusion and neighborhood matching, by which a unified
feature space among clients is learned for better data representations. Feature
fusion provides remote features as accurate contrastive information to each
client for better local learning. Neighborhood matching further aligns each
client's local features to the remote features such that well-clustered
features among clients can be learned. Extensive experiments show the
effectiveness of the proposed framework. It outperforms other methods by 11\%
on IID data and matches the performance of centralized learning.

    

### [[2111.10769] Design of an Novel Spectrum Sensing Scheme Based on Long Short-Term Memory and Experimental Validation](http://arxiv.org/abs/2111.10769)


  Spectrum sensing allows cognitive radio systems to detect relevant signals in
despite the presence of severe interference. Most of the existing spectrum
sensing techniques use a particular signal-noise model with certain assumptions
and derive certain detection performance. To deal with this uncertainty,
learning based approaches are being adopted and more recently deep learning
based tools have become popular. Here, we propose an approach of spectrum
sensing which is based on long short term memory (LSTM) which is a critical
element of deep learning networks (DLN). Use of LSTM facilitates implicit
feature learning from spectrum data. The DLN is trained using several features
and the performance of the proposed sensing technique is validated with the
help of an empirical testbed setup using Adalm Pluto. The testbed is trained to
acquire the primary signal of a real world radio broadcast taking place using
FM. Experimental data show that even at low signal to noise ratio, our approach
performs well in terms of detection and classification accuracies, as compared
to current spectrum sensing methods.

    

### [[2111.10770] Efficient Softmax Approximation for Deep Neural Networks with Attention Mechanism](http://arxiv.org/abs/2111.10770)


  There has been a rapid advance of custom hardware (HW) for accelerating the
inference speed of deep neural networks (DNNs). Previously, the softmax layer
was not a main concern of DNN accelerating HW, because its portion is
relatively small in multi-layer perceptron or convolutional neural networks.
However, as the attention mechanisms are widely used in various modern DNNs, a
cost-efficient implementation of softmax layer is becoming very important. In
this paper, we propose two methods to approximate softmax computation, which
are based on the usage of LookUp Tables (LUTs). The required size of LUT is
quite small (about 700 Bytes) because ranges of numerators and denominators of
softmax are stable if normalization is applied to the input. We have validated
the proposed technique over different AI tasks (object detection, machine
translation, sentiment analysis, and semantic equivalence) and DNN models
(DETR, Transformer, BERT) by a variety of benchmarks (COCO17, WMT14, WMT17,
GLUE). We showed that 8-bit approximation allows to obtain acceptable accuracy
loss below $1.0\%$.

    

### [[2111.10772] Network representation learning: A macro and micro view](http://arxiv.org/abs/2111.10772)


  Graph is a universe data structure that is widely used to organize data in
real-world. Various real-word networks like the transportation network, social
and academic network can be represented by graphs. Recent years have witnessed
the quick development on representing vertices in the network into a
low-dimensional vector space, referred to as network representation learning.
Representation learning can facilitate the design of new algorithms on the
graph data. In this survey, we conduct a comprehensive review of current
literature on network representation learning. Existing algorithms can be
categorized into three groups: shallow embedding models, heterogeneous network
embedding models, graph neural network based models. We review state-of-the-art
algorithms for each category and discuss the essential differences between
these algorithms. One advantage of the survey is that we systematically study
the underlying theoretical foundations underlying the different categories of
algorithms, which offers deep insights for better understanding the development
of the network representation learning field.

    

### [[2111.10776] Is Speech Emotion Recognition Language-Independent? Analysis of English and Bangla Languages using Language-Independent Vocal Features](http://arxiv.org/abs/2111.10776)


  A language agnostic approach to recognizing emotions from speech remains an
incomplete and challenging task. In this paper, we used Bangla and English
languages to assess whether distinguishing emotions from speech is independent
of language. The following emotions were categorized for this study: happiness,
anger, neutral, sadness, disgust, and fear. We employed three Emotional Speech
Sets, of which the first two were developed by native Bengali speakers in
Bangla and English languages separately. The third was the Toronto Emotional
Speech Set (TESS), which was developed by native English speakers from Canada.
We carefully selected language-independent prosodic features, adopted a Support
Vector Machine (SVM) model, and conducted three experiments to carry out our
proposition. In the first experiment, we measured the performance of the three
speech sets individually. This was followed by the second experiment, where we
recorded the classification rate by combining the speech sets. Finally, in the
third experiment we measured the recognition rate by training and testing the
model with different speech sets. Although this study reveals that Speech
Emotion Recognition (SER) is mostly language-independent, there is some
disparity while recognizing emotional states like disgust and fear in these two
languages. Moreover, our investigations inferred that non-native speakers
convey emotions through speech, much like expressing themselves in their native
tongue.

    

### [[2111.10778] Federated Social Recommendation with Graph Neural Network](http://arxiv.org/abs/2111.10778)


  Recommender systems have become prosperous nowadays, designed to predict
users' potential interests in items by learning embeddings. Recent developments
of the Graph Neural Networks~(GNNs) also provide recommender systems with
powerful backbones to learn embeddings from a user-item graph. However, only
leveraging the user-item interactions suffers from the cold-start issue due to
the difficulty in data collection. Hence, current endeavors propose fusing
social information with user-item interactions to alleviate it, which is the
social recommendation problem. Existing work employs GNNs to aggregate both
social links and user-item interactions simultaneously. However, they all
require centralized storage of the social links and item interactions of users,
which leads to privacy concerns. Additionally, according to strict privacy
protection under General Data Protection Regulation, centralized data storage
may not be feasible in the future, urging a decentralized framework of social
recommendation. To this end, we devise a novel framework \textbf{Fe}drated
\textbf{So}cial recommendation with \textbf{G}raph neural network (FeSoG).
Firstly, FeSoG adopts relational attention and aggregation to handle
heterogeneity. Secondly, FeSoG infers user embeddings using local data to
retain personalization. Last but not least, the proposed model employs
pseudo-labeling techniques with item sampling to protect the privacy and
enhance training. Extensive experiments on three real-world datasets justify
the effectiveness of FeSoG in completing social recommendation and privacy
protection. We are the first work proposing a federated learning framework for
social recommendation to the best of our knowledge.

    

### [[2111.10785] Differentiable Projection for Constrained Deep Learning](http://arxiv.org/abs/2111.10785)


  Deep neural networks (DNNs) have achieved extraordinary performance in
solving different tasks in various fields. However, the conventional DNN model
is steadily approaching the ground-truth value through loss backpropagation. In
some applications, some prior knowledge could be easily obtained, such as
constraints which the ground truth observation follows. Here, we try to give a
general approach to incorporate information from these constraints to enhance
the performance of the DNNs. Theoretically, we could formulate these kinds of
problems as constrained optimization problems that KKT conditions could solve.
In this paper, we propose to use a differentiable projection layer in DNN
instead of directly solving time-consuming KKT conditions. The proposed
projection method is differentiable, and no heavy computation is required.
Finally, we also conducted some experiments using a randomly generated
synthetic dataset and image segmentation task using the PASCAL VOC dataset to
evaluate the performance of the proposed projection method. Experimental
results show that the projection method is sufficient and outperforms baseline
methods.

    

### [[2111.10806] A Data-Driven Line Search Rule for Support Recovery in High-dimensional Data Analysis](http://arxiv.org/abs/2111.10806)


  In this work, we consider the algorithm to the (nonlinear) regression
problems with $\ell_0$ penalty. The existing algorithms for $\ell_0$ based
optimization problem are often carried out with a fixed step size, and the
selection of an appropriate step size depends on the restricted strong
convexity and smoothness for the loss function, hence it is difficult to
compute in practical calculation. In sprite of the ideas of support detection
and root finding \cite{HJK2020}, we proposes a novel and efficient data-driven
line search rule to adaptively determine the appropriate step size. We prove
the $\ell_2$ error bound to the proposed algorithm without much restrictions
for the cost functional. A large number of numerical comparisons with
state-of-the-art algorithms in linear and logistic regression problems show the
stability, effectiveness and superiority of the proposed algorithms.

    

### [[2111.10810] Vulcan: Solving the Steiner Tree Problem with Graph Neural Networks and Deep Reinforcement Learning](http://arxiv.org/abs/2111.10810)


  Steiner Tree Problem (STP) in graphs aims to find a tree of minimum weight in
the graph that connects a given set of vertices. It is a classic NP-hard
combinatorial optimization problem and has many real-world applications (e.g.,
VLSI chip design, transportation network planning and wireless sensor
networks). Many exact and approximate algorithms have been developed for STP,
but they suffer from high computational complexity and weak worst-case solution
guarantees, respectively. Heuristic algorithms are also developed. However,
each of them requires application domain knowledge to design and is only
suitable for specific scenarios. Motivated by the recently reported observation
that instances of the same NP-hard combinatorial problem may maintain the same
or similar combinatorial structure but mainly differ in their data, we
investigate the feasibility and benefits of applying machine learning
techniques to solving STP. To this end, we design a novel model Vulcan based on
novel graph neural networks and deep reinforcement learning. The core of Vulcan
is a novel, compact graph embedding that transforms highdimensional graph
structure data (i.e., path-changed information) into a low-dimensional vector
representation. Given an STP instance, Vulcan uses this embedding to encode its
pathrelated information and sends the encoded graph to a deep reinforcement
learning component based on a double deep Q network (DDQN) to find solutions.
In addition to STP, Vulcan can also find solutions to a wide range of NP-hard
problems (e.g., SAT, MVC and X3C) by reducing them to STP. We implement a
prototype of Vulcan and demonstrate its efficacy and efficiency with extensive
experiments using real-world and synthetic datasets.

    

### [[2111.10831] Learning by Active Forgetting for Neural Networks](http://arxiv.org/abs/2111.10831)


  Remembering and forgetting mechanisms are two sides of the same coin in a
human learning-memory system. Inspired by human brain memory mechanisms, modern
machine learning systems have been working to endow machine with lifelong
learning capability through better remembering while pushing the forgetting as
the antagonist to overcome. Nevertheless, this idea might only see the half
picture. Up until very recently, increasing researchers argue that a brain is
born to forget, i.e., forgetting is a natural and active process for abstract,
rich, and flexible representations. This paper presents a learning model by
active forgetting mechanism with artificial neural networks. The active
forgetting mechanism (AFM) is introduced to a neural network via a
"plug-and-play" forgetting layer (P\&PF), consisting of groups of inhibitory
neurons with Internal Regulation Strategy (IRS) to adjust the extinction rate
of themselves via lateral inhibition mechanism and External Regulation Strategy
(ERS) to adjust the extinction rate of excitatory neurons via inhibition
mechanism. Experimental studies have shown that the P\&PF offers surprising
benefits: self-adaptive structure, strong generalization, long-term learning
and memory, and robustness to data and parameter perturbation. This work sheds
light on the importance of forgetting in the learning process and offers new
perspectives to understand the underlying mechanisms of neural networks.

    

### [[2111.10832] Automated Controller Calibration by Kalman Filtering](http://arxiv.org/abs/2111.10832)


  This paper proposes a method for calibrating control parameters. Examples of
such control parameters are gains of PID controllers, weights of a cost
function for optimal control, filter coefficients, the sliding surface of a
sliding mode controller, or weights of a neural network. Hence, the proposed
method can be applied to a wide range of controllers. The method uses a Kalman
filter that estimates control parameters rather than the system's state, using
data of closed-loop system operation. The control parameter calibration is
driven by a training objective, which encompasses specifications on the
performance of the dynamical system. The calibration method tunes the
parameters online and robustly, is computationally efficient, has low data
storage requirements, and is easy to implement making it appealing for many
real-time applications. Simulation results show that the method is able to
learn control parameters quickly (approximately 24% average decay factor of
closed-loop cost), is able to tune the parameters to compensate for
disturbances (approximately 29% improvement on tracking precision), and is
robust to noise. Further, a simulation study with the high-fidelity vehicle
simulator CarSim shows that the method can calibrate controllers of a complex
dynamical system online, which indicates its applicability to a real-world
system.

    

### [[2111.10847] Calibrated Diffusion Tensor Estimation](http://arxiv.org/abs/2111.10847)


  It is highly desirable to know how uncertain a model's predictions are,
especially for models that are complex and hard to understand as in deep
learning. Although there has been a growing interest in using deep learning
methods in diffusion-weighted MRI, prior works have not addressed the issue of
model uncertainty. Here, we propose a deep learning method to estimate the
diffusion tensor and compute the estimation uncertainty. Data-dependent
uncertainty is computed directly by the network and learned via loss
attenuation. Model uncertainty is computed using Monte Carlo dropout. We also
propose a new method for evaluating the quality of predicted uncertainties. We
compare the new method with the standard least-squares tensor estimation and
bootstrap-based uncertainty computation techniques. Our experiments show that
when the number of measurements is small the deep learning method is more
accurate and its uncertainty predictions are better calibrated than the
standard methods. We show that the estimation uncertainties computed by the new
method can highlight the model's biases, detect domain shift, and reflect the
strength of noise in the measurements. Our study shows the importance and
practical value of modeling prediction uncertainties in deep learning-based
diffusion MRI analysis.

    

### [[2111.10854] XnODR and XnIDR: Two Accurate and Fast Fully Connected Layers For Convolutional Neural Networks](http://arxiv.org/abs/2111.10854)


  Although Capsule Networks show great abilities in defining the position
relationship between features in deep neural networks for visual recognition
tasks, they are computationally expensive and not suitable for running on
mobile devices. The bottleneck is in the computational complexity of the
Dynamic Routing mechanism used between capsules. On the other hand, neural
networks such as XNOR-Net are fast and computationally efficient but have
relatively low accuracy because of their information loss in the binarization
process. This paper proposes a new class of Fully Connected (FC) Layers by
xnorizing the linear projector outside or inside the Dynamic Routing within the
CapsFC layer. Specifically, our proposed FC layers have two versions, XnODR
(Xnorizing Linear Projector Outside Dynamic Routing) and XnIDR (Xnorizing
Linear Projector Inside Dynamic Routing). To test their generalization, we
insert them into MobileNet V2 and ResNet-50 separately. Experiments on three
datasets, MNIST, CIFAR-10, MultiMNIST validate their effectiveness. Our
experimental results demonstrate that both XnODR and XnIDR help networks to
have high accuracy with lower FLOPs and fewer parameters (e.g., 95.32\%
accuracy with 2.99M parameters and 311.22M FLOPs on CIFAR-10).

    

### [[2111.10857] Accretionary Learning with Deep Neural Networks](http://arxiv.org/abs/2111.10857)


  One of the fundamental limitations of Deep Neural Networks (DNN) is its
inability to acquire and accumulate new cognitive capabilities. When some new
data appears, such as new object classes that are not in the prescribed set of
objects being recognized, a conventional DNN would not be able to recognize
them due to the fundamental formulation that it takes. The current solution is
typically to re-design and re-learn the entire network, perhaps with a new
configuration, from a newly expanded dataset to accommodate new knowledge. This
process is quite different from that of a human learner. In this paper, we
propose a new learning method named Accretionary Learning (AL) to emulate human
learning, in that the set of objects to be recognized may not be pre-specified.
The corresponding learning structure is modularized, which can dynamically
expand to register and use new knowledge. During accretionary learning, the
learning process does not require the system to be totally re-designed and
re-trained as the set of objects grows in size. The proposed DNN structure does
not forget previous knowledge when learning to recognize new data classes. We
show that the new structure and the design methodology lead to a system that
can grow to cope with increased cognitive complexity while providing stable and
superior overall performance.

    

### [[2111.10858] Bilevel learning of l1-regularizers with closed-form gradients(BLORC)](http://arxiv.org/abs/2111.10858)


  We present a method for supervised learning of sparsity-promoting
regularizers, a key ingredient in many modern signal reconstruction problems.
The parameters of the regularizer are learned to minimize the mean squared
error of reconstruction on a training set of ground truth signal and
measurement pairs. Training involves solving a challenging bilevel optimization
problem with a nonsmooth lower-level objective. We derive an expression for the
gradient of the training loss using the implicit closed-form solution of the
lower-level variational problem given by its dual problem, and provide an
accompanying gradient descent algorithm (dubbed BLORC) to minimize the loss.
Our experiments on simple natural images and for denoising 1D signals show that
the proposed method can learn meaningful operators and the analytical gradients
calculated are faster than standard automatic differentiation methods. While
the approach we present is applied to denoising, we believe that it can be
adapted to a wide-variety of inverse problems with linear measurement models,
thus giving it applicability in a wide range of scenarios.

    

### [[2111.10891] ARMAS: Active Reconstruction of Missing Audio Segments](http://arxiv.org/abs/2111.10891)


  Digital audio signal reconstruction of lost or corrupt segment using deep
learning algorithms has been explored intensively in the recent years.
Nevertheless, prior traditional methods with linear interpolation, phase coding
and tone insertion techniques are still in vogue. However, we found no research
work on the reconstruction of audio signals with the fusion of dithering,
steganography, and machine learning regressors. Therefore, this paper proposes
the combination of steganography, halftoning (dithering), and state-of-the-art
shallow (RF- Random Forest and SVR- Support Vector Regression) and deep
learning (LSTM- Long Short-Term Memory) methods. The results (including
comparison to the SPAIN and Autoregressive methods) are evaluated with four
different metrics. The observations from the results show that the proposed
solution is effective and can enhance the reconstruction of audio signals
performed by the side information (noisy-latent representation) steganography
provides. This work may trigger interest in the optimization of this approach
and/or in transferring it to different domains (i.e., image reconstruction).

    

### [[2111.10897] Health Monitoring of Industrial machines using Scene-Aware Threshold Selection](http://arxiv.org/abs/2111.10897)


  This paper presents an autoencoder based unsupervised approach to identify
anomaly in an industrial machine using sounds produced by the machine. The
proposed framework is trained using log-melspectrogram representations of the
sound signal. In classification, our hypothesis is that the reconstruction
error computed for an abnormal machine is larger than that of the a normal
machine, since only normal machine sounds are being used to train the
autoencoder. A threshold is chosen to discriminate between normal and abnormal
machines. However, the threshold changes as surrounding conditions vary. To
select an appropriate threshold irrespective of the surrounding, we propose a
scene classification framework, which can classify the underlying surrounding.
Hence, the threshold can be selected adaptively irrespective of the
surrounding. The experiment evaluation is performed on MIMII dataset for
industrial machines namely fan, pump, valve and slide rail. Our experiment
analysis shows that utilizing adaptive threshold, the performance improves
significantly as that obtained using the fixed threshold computed for a given
surrounding only.

    

### [[2111.10898] Renewable energy integration and microgrid energy trading using multi-agent deep reinforcement learning](http://arxiv.org/abs/2111.10898)


  In this paper, multi-agent reinforcement learning is used to control a hybrid
energy storage system working collaboratively to reduce the energy costs of a
microgrid through maximising the value of renewable energy and trading. The
agents must learn to control three different types of energy storage system
suited for short, medium, and long-term storage under fluctuating demand,
dynamic wholesale energy prices, and unpredictable renewable energy generation.
Two case studies are considered: the first looking at how the energy storage
systems can better integrate renewable energy generation under dynamic pricing,
and the second with how those same agents can be used alongside an aggregator
agent to sell energy to self-interested external microgrids looking to reduce
their own energy bills. This work found that the centralised learning with
decentralised execution of the multi-agent deep deterministic policy gradient
and its state-of-the-art variants allowed the multi-agent methods to perform
significantly better than the control from a single global agent. It was also
found that using separate reward functions in the multi-agent approach
performed much better than using a single control agent. Being able to trade
with the other microgrids, rather than just selling back to the utility grid,
also was found to greatly increase the grid's savings.

    

### [[2111.10912] Johnson Coverage Hypothesis: Inapproximability of k-means and k-median in L_p metrics](http://arxiv.org/abs/2111.10912)


  K-median and k-means are the two most popular objectives for clustering
algorithms. Despite intensive effort, a good understanding of the
approximability of these objectives, particularly in $\ell_p$-metrics, remains
a major open problem. In this paper, we significantly improve upon the hardness
of approximation factors known in literature for these objectives in
$\ell_p$-metrics.
We introduce a new hypothesis called the Johnson Coverage Hypothesis (JCH),
which roughly asserts that the well-studied max k-coverage problem on set
systems is hard to approximate to a factor greater than 1-1/e, even when the
membership graph of the set system is a subgraph of the Johnson graph. We then
show that together with generalizations of the embedding techniques introduced
by Cohen-Addad and Karthik (FOCS '19), JCH implies hardness of approximation
results for k-median and k-means in $\ell_p$-metrics for factors which are
close to the ones obtained for general metrics. In particular, assuming JCH we
show that it is hard to approximate the k-means objective:
$\bullet$ Discrete case: To a factor of 3.94 in the $\ell_1$-metric and to a
factor of 1.73 in the $\ell_2$-metric; this improves upon the previous factor
of 1.56 and 1.17 respectively, obtained under UGC.
$\bullet$ Continuous case: To a factor of 2.10 in the $\ell_1$-metric and to
a factor of 1.36 in the $\ell_2$-metric; this improves upon the previous factor
of 1.07 in the $\ell_2$-metric obtained under UGC.
We also obtain similar improvements under JCH for the k-median objective.
Additionally, we prove a weak version of JCH using the work of Dinur et al.
(SICOMP '05) on Hypergraph Vertex Cover, and recover all the results stated
above of Cohen-Addad and Karthik (FOCS '19) to (nearly) the same
inapproximability factors but now under the standard NP$\neq$P assumption
(instead of UGC).

    

### [[2111.10916] Video Content Swapping Using GAN](http://arxiv.org/abs/2111.10916)


  Video generation is an interesting problem in computer vision. It is quite
popular for data augmentation, special effect in move, AR/VR and so on. With
the advances of deep learning, many deep generative models have been proposed
to solve this task. These deep generative models provide away to utilize all
the unlabeled images and videos online, since it can learn deep feature
representations with unsupervised manner. These models can also generate
different kinds of images, which have great value for visual application.
However generating a video would be much more challenging since we need to
model not only the appearances of objects in the video but also their temporal
motion. In this work, we will break down any frame in the video into content
and pose. We first extract the pose information from a video using a
pre-trained human pose detection and use a generative model to synthesize the
video based on the content code and pose code.

    

### [[2111.10917] Deep Reinforced Attention Regression for Partial Sketch Based Image Retrieval](http://arxiv.org/abs/2111.10917)


  Fine-Grained Sketch-Based Image Retrieval (FG-SBIR) aims at finding a
specific image from a large gallery given a query sketch. Despite the
widespread applicability of FG-SBIR in many critical domains (e.g., crime
activity tracking), existing approaches still suffer from a low accuracy while
being sensitive to external noises such as unnecessary strokes in the sketch.
The retrieval performance will further deteriorate under a more practical
on-the-fly setting, where only a partially complete sketch with only a few
(noisy) strokes are available to retrieve corresponding images. We propose a
novel framework that leverages a uniquely designed deep reinforcement learning
model that performs a dual-level exploration to deal with partial sketch
training and attention region selection. By enforcing the model's attention on
the important regions of the original sketches, it remains robust to
unnecessary stroke noises and improve the retrieval accuracy by a large margin.
To sufficiently explore partial sketches and locate the important regions to
attend, the model performs bootstrapped policy gradient for global exploration
while adjusting a standard deviation term that governs a locator network for
local exploration. The training process is guided by a hybrid loss that
integrates a reinforcement loss and a supervised loss. A dynamic ranking reward
is developed to fit the on-the-fly image retrieval process using partial
sketches. The extensive experimentation performed on three public datasets
shows that our proposed approach achieves the state-of-the-art performance on
partial sketch based image retrieval.

    

### [[2111.10919] Offline Reinforcement Learning: Fundamental Barriers for Value Function Approximation](http://arxiv.org/abs/2111.10919)


  We consider the offline reinforcement learning problem, where the aim is to
learn a decision making policy from logged data. Offline RL -- particularly
when coupled with (value) function approximation to allow for generalization in
large or continuous state spaces -- is becoming increasingly relevant in
practice, because it avoids costly and time-consuming online data collection
and is well suited to safety-critical domains. Existing sample complexity
guarantees for offline value function approximation methods typically require
both (1) distributional assumptions (i.e., good coverage) and (2)
representational assumptions (i.e., ability to represent some or all $Q$-value
functions) stronger than what is required for supervised learning. However, the
necessity of these conditions and the fundamental limits of offline RL are not
well understood in spite of decades of research. This led Chen and Jiang (2019)
to conjecture that concentrability (the most standard notion of coverage) and
realizability (the weakest representation condition) alone are not sufficient
for sample-efficient offline RL. We resolve this conjecture in the positive by
proving that in general, even if both concentrability and realizability are
satisfied, any algorithm requires sample complexity polynomial in the size of
the state space to learn a non-trivial policy.
Our results show that sample-efficient offline reinforcement learning
requires either restrictive coverage conditions or representation conditions
that go beyond supervised learning, and highlight a phenomenon called
over-coverage which serves as a fundamental barrier for offline value function
approximation methods. A consequence of our results for reinforcement learning
with linear function approximation is that the separation between online and
offline RL can be arbitrarily large, even in constant dimension.

    

### [[2111.10928] WalkingTime: Dynamic Graph Embedding Using Temporal-Topological Flows](http://arxiv.org/abs/2111.10928)


  Increased attention has been paid over the last four years to dynamic network
embedding. Existing dynamic embedding methods, however, consider the problem as
limited to the evolution of a topology over a sequence of global, discrete
states. We propose a novel embedding algorithm, WalkingTime, based on a
fundamentally different handling of time, allowing for the local consideration
of continuously occurring phenomena; while others consider global time-steps to
be first-order citizens of the dynamic environment, we hold flows comprised of
temporally and topologically local interactions as our primitives, without any
discretization or alignment of time-related attributes being necessary.
Keywords: dynamic networks , representation learning , dynamic graph
embedding , time-respecting paths , temporal-topological flows , temporal
random walks , temporal networks , real-attributed knowledge graphs , streaming
graphs , online networks , asynchronous graphs , asynchronous networks , graph
algorithms , deep learning , network analysis , datamining , network science

    

### [[2111.10932] Self-supervised Semi-supervised Learning for Data Labeling and Quality Evaluation](http://arxiv.org/abs/2111.10932)


  As the adoption of deep learning techniques in industrial applications grows
with increasing speed and scale, successful deployment of deep learning models
often hinges on the availability, volume, and quality of annotated data. In
this paper, we tackle the problems of efficient data labeling and annotation
verification under the human-in-the-loop setting. We showcase that the latest
advancements in the field of self-supervised visual representation learning can
lead to tools and methods that benefit the curation and engineering of natural
image datasets, reducing annotation cost and increasing annotation quality. We
propose a unifying framework by leveraging self-supervised semi-supervised
learning and use it to construct workflows for data labeling and annotation
verification tasks. We demonstrate the effectiveness of our workflows over
existing methodologies. On active learning task, our method achieves 97.0%
Top-1 Accuracy on CIFAR10 with 0.1% annotated data, and 83.9% Top-1 Accuracy on
CIFAR100 with 10% annotated data. When learning with 50% of wrong labels, our
method achieves 97.4% Top-1 Accuracy on CIFAR10 and 85.5% Top-1 Accuracy on
CIFAR100.

    

### [[2111.10933] Decentralized Multi-Armed Bandit Can Outperform Classic Upper Confidence Bound](http://arxiv.org/abs/2111.10933)


  This paper studies a decentralized multi-armed bandit problem in a
multi-agent network. The problem is simultaneously solved by N agents assuming
they face a common set of M arms and share the same mean of each arm's reward.
Each agent can receive information only from its neighbors, where the neighbor
relations among the agents are described by a directed graph whose vertices
represent agents and whose directed edges depict neighbor relations. A fully
decentralized multi-armed bandit algorithm is proposed for each agent, which
twists the classic consensus algorithm and upper confidence bound (UCB)
algorithm. It is shown that the algorithm guarantees each agent to achieve a
better logarithmic asymptotic regret than the classic UCB provided the neighbor
graph is strongly connected. The regret can be further improved if the neighbor
graph is undirected.

    

### [[2111.10934] Privacy-preserving Federated Adversarial Domain Adaption over Feature Groups for Interpretability](http://arxiv.org/abs/2111.10934)


  We present a novel privacy-preserving federated adversarial domain adaptation
approach ($\textbf{PrADA}$) to address an under-studied but practical
cross-silo federated domain adaptation problem, in which the party of the
target domain is insufficient in both samples and features. We address the
lack-of-feature issue by extending the feature space through vertical federated
learning with a feature-rich party and tackle the sample-scarce issue by
performing adversarial domain adaptation from the sample-rich source party to
the target party. In this work, we focus on financial applications where
interpretability is critical. However, existing adversarial domain adaptation
methods typically apply a single feature extractor to learn feature
representations that are low-interpretable with respect to the target task. To
improve interpretability, we exploit domain expertise to split the feature
space into multiple groups that each holds relevant features, and we learn a
semantically meaningful high-order feature from each feature group. In
addition, we apply a feature extractor (along with a domain discriminator) for
each feature group to enable a fine-grained domain adaptation. We design a
secure protocol that enables performing the PrADA in a secure and efficient
manner. We evaluate our approach on two tabular datasets. Experiments
demonstrate both the effectiveness and practicality of our approach.

    

### [[2111.10937] Adaptive Transfer Learning: a simple but effective transfer learning](http://arxiv.org/abs/2111.10937)


  Transfer learning (TL) leverages previously obtained knowledge to learn new
tasks efficiently and has been used to train deep learning (DL) models with
limited amount of data. When TL is applied to DL, pretrained (teacher) models
are fine-tuned to build domain specific (student) models. This fine-tuning
relies on the fact that DL model can be decomposed to classifiers and feature
extractors, and a line of studies showed that the same feature extractors can
be used to train classifiers on multiple tasks. Furthermore, recent studies
proposed multiple algorithms that can fine-tune teacher models' feature
extractors to train student models more efficiently. We note that regardless of
the fine-tuning of feature extractors, the classifiers of student models are
trained with final outputs of feature extractors (i.e., the outputs of
penultimate layers). However, a recent study suggested that feature maps in
ResNets across layers could be functionally equivalent, raising the possibility
that feature maps inside the feature extractors can also be used to train
student models' classifiers. Inspired by this study, we tested if feature maps
in the hidden layers of the teacher models can be used to improve the student
models' accuracy (i.e., TL's efficiency). Specifically, we developed 'adaptive
transfer learning (ATL)', which can choose an optimal set of feature maps for
TL, and tested it in the few-shot learning setting. Our empirical evaluations
suggest that ATL can help DL models learn more efficiently, especially when
available examples are limited.

    

### [[2111.10940] How do kernel-based sensor fusion algorithms behave under high dimensional noise?](http://arxiv.org/abs/2111.10940)


  We study the behavior of two kernel based sensor fusion algorithms,
nonparametric canonical correlation analysis (NCCA) and alternating diffusion
(AD), under the nonnull setting that the clean datasets collected from two
sensors are modeled by a common low dimensional manifold embedded in a high
dimensional Euclidean space and the datasets are corrupted by high dimensional
noise. We establish the asymptotic limits and convergence rates for the
eigenvalues of the associated kernel matrices assuming that the sample
dimension and sample size are comparably large, where NCCA and AD are conducted
using the Gaussian kernel. It turns out that both the asymptotic limits and
convergence rates depend on the signal-to-noise ratio (SNR) of each sensor and
selected bandwidths. On one hand, we show that if NCCA and AD are directly
applied to the noisy point clouds without any sanity check, it may generate
artificial information that misleads scientists' interpretation. On the other
hand, we prove that if the bandwidths are selected adequately, both NCCA and AD
can be made robust to high dimensional noise when the SNRs are relatively
large.

    

### [[2111.10952] ExT5: Towards Extreme Multi-Task Scaling for Transfer Learning](http://arxiv.org/abs/2111.10952)


  Despite the recent success of multi-task learning and transfer learning for
natural language processing (NLP), few works have systematically studied the
effect of scaling up the number of tasks during pre-training. Towards this
goal, this paper introduces ExMix (Extreme Mixture): a massive collection of
107 supervised NLP tasks across diverse domains and task-families. Using ExMix,
we study the effect of multi-task pre-training at the largest scale to date,
and analyze co-training transfer amongst common families of tasks. Through this
analysis, we show that manually curating an ideal set of tasks for multi-task
pre-training is not straightforward, and that multi-task scaling can vastly
improve models on its own. Finally, we propose ExT5: a model pre-trained using
a multi-task objective of self-supervised span denoising and supervised ExMix.
Via extensive experiments, we show that ExT5 outperforms strong T5 baselines on
SuperGLUE, GEM, Rainbow, Closed-Book QA tasks, and several tasks outside of
ExMix. ExT5 also significantly improves sample efficiency while pre-training.

    

### [[2111.10954] Generation Drawing/Grinding Trajectoy Based on Hierarchical CVAE](http://arxiv.org/abs/2111.10954)


  In this study, we propose a method to model the local and global features of
the drawing/grinding trajectory with hierarchical Variational Autoencoders
(VAEs). By combining two separately trained VAE models in a hierarchical
structure, it is possible to generate trajectories with high reproducibility
for both local and global features. The hierarchical generation network enables
the generation of higher-order trajectories with a relatively small amount of
training data. The simulation and experimental results demonstrate the
generalization performance of the proposed method. In addition, we confirmed
that it is possible to generate new trajectories, which have never been learned
in the past, by changing the combination of the learned models.

    

### [[2111.10957] Hierarchical Knowledge Distillation for Dialogue Sequence Labeling](http://arxiv.org/abs/2111.10957)


  This paper presents a novel knowledge distillation method for dialogue
sequence labeling. Dialogue sequence labeling is a supervised learning task
that estimates labels for each utterance in the target dialogue document, and
is useful for many applications such as dialogue act estimation. Accurate
labeling is often realized by a hierarchically-structured large model
consisting of utterance-level and dialogue-level networks that capture the
contexts within an utterance and between utterances, respectively. However, due
to its large model size, such a model cannot be deployed on
resource-constrained devices. To overcome this difficulty, we focus on
knowledge distillation which trains a small model by distilling the knowledge
of a large and high performance teacher model. Our key idea is to distill the
knowledge while keeping the complex contexts captured by the teacher model. To
this end, the proposed method, hierarchical knowledge distillation, trains the
small model by distilling not only the probability distribution of the label
classification, but also the knowledge of utterance-level and dialogue-level
contexts trained in the teacher model by training the model to mimic the
teacher model's output in each level. Experiments on dialogue act estimation
and call scene segmentation demonstrate the effectiveness of the proposed
method.

    

### [[2111.10983] A Semi-Supervised Adaptive Discriminative Discretization Method Improving Discrimination Power of Regularized Naive Bayes](http://arxiv.org/abs/2111.10983)


  Recently, many improved naive Bayes methods have been developed with enhanced
discrimination capabilities. Among them, regularized naive Bayes (RNB) produces
excellent performance by balancing the discrimination power and generalization
capability. Data discretization is important in naive Bayes. By grouping
similar values into one interval, the data distribution could be better
estimated. However, existing methods including RNB often discretize the data
into too few intervals, which may result in a significant information loss. To
address this problem, we propose a semi-supervised adaptive discriminative
discretization framework for naive Bayes, which could better estimate the data
distribution by utilizing both labeled data and unlabeled data through
pseudo-labeling techniques. The proposed method also significantly reduces the
information loss during discretization by utilizing an adaptive discriminative
discretization scheme, and hence greatly improves the discrimination power of
classifiers. The proposed RNB+, i.e., regularized naive Bayes utilizing the
proposed discretization framework, is systematically evaluated on a wide range
of machine-learning datasets. It significantly and consistently outperforms
state-of-the-art NB classifiers.

    

### [[2111.11004] Gradient Temporal Difference with Momentum: Stability and Convergence](http://arxiv.org/abs/2111.11004)


  Gradient temporal difference (Gradient TD) algorithms are a popular class of
stochastic approximation (SA) algorithms used for policy evaluation in
reinforcement learning. Here, we consider Gradient TD algorithms with an
additional heavy ball momentum term and provide choice of step size and
momentum parameter that ensures almost sure convergence of these algorithms
asymptotically. In doing so, we decompose the heavy ball Gradient TD iterates
into three separate iterates with different step sizes. We first analyze these
iterates under one-timescale SA setting using results from current literature.
However, the one-timescale case is restrictive and a more general analysis can
be provided by looking at a three-timescale decomposition of the iterates. In
the process, we provide the first conditions for stability and convergence of
general three-timescale SA. We then prove that the heavy ball Gradient TD
algorithm is convergent using our three-timescale SA analysis. Finally, we
evaluate these algorithms on standard RL problems and report improvement in
performance over the vanilla algorithms.

    

### [[2111.11010] Density Ratio Estimation via Infinitesimal Classification](http://arxiv.org/abs/2111.11010)


  Density ratio estimation (DRE) is a fundamental machine learning technique
for comparing two probability distributions. However, existing methods struggle
in high-dimensional settings, as it is difficult to accurately compare
probability distributions based on finite samples. In this work we propose
DRE-\infty, a divide-and-conquer approach to reduce DRE to a series of easier
subproblems. Inspired by Monte Carlo methods, we smoothly interpolate between
the two distributions via an infinite continuum of intermediate bridge
distributions. We then estimate the instantaneous rate of change of the bridge
distributions indexed by time (the "time score") -- a quantity defined
analogously to data (Stein) scores -- with a novel time score matching
objective. Crucially, the learned time scores can then be integrated to compute
the desired density ratio. In addition, we show that traditional (Stein) scores
can be used to obtain integration paths that connect regions of high density in
both distributions, improving performance in practice. Empirically, we
demonstrate that our approach performs well on downstream tasks such as mutual
information estimation and energy-based modeling on complex, high-dimensional
datasets.

    

### [[2111.11017] Benchmarking Predictive Risk Models for Emergency Departments with Large Public Electronic Health Records](http://arxiv.org/abs/2111.11017)


  There is a continuously growing demand for emergency department (ED) services
across the world, especially under the COVID-19 pandemic. Risk triaging plays a
crucial role in prioritizing limited medical resources for patients who need
them most. Recently the pervasive use of Electronic Health Records (EHR) has
generated a large volume of stored data, accompanied by vast opportunities for
the development of predictive models which could improve emergency care.
However, there is an absence of widely accepted ED benchmarks based on
large-scale public EHR, which new researchers could easily access. Success in
filling in this gap could enable researchers to start studies on ED more
quickly and conveniently without verbose data preprocessing and facilitate
comparisons among different studies and methodologies. In this paper, based on
the Medical Information Mart for Intensive Care IV Emergency Department
(MIMIC-IV-ED) database, we proposed a public ED benchmark suite and obtained a
benchmark dataset containing over 500,000 ED visits episodes from 2011 to 2019.
Three ED-based prediction tasks (hospitalization, critical outcomes, and
72-hour ED revisit) were introduced, where various popular methodologies, from
machine learning methods to clinical scoring systems, were implemented. The
results of their performance were evaluated and compared. Our codes are
open-source so that anyone with access to MIMIC-IV-ED could follow the same
steps of data processing, build the benchmarks, and reproduce the experiments.
This study provided insights, suggestions, as well as protocols for future
researchers to process the raw data and quickly build up models for emergency
care.

    

### [[2111.11023] Multi-Channel Multi-Speaker ASR Using 3D Spatial Feature](http://arxiv.org/abs/2111.11023)


  Automatic speech recognition (ASR) of multi-channel multi-speaker overlapped
speech remains one of the most challenging tasks to the speech community. In
this paper, we look into this challenge by utilizing the location information
of target speakers in the 3D space for the first time. To explore the strength
of proposed the 3D spatial feature, two paradigms are investigated. 1) a
pipelined system with a multi-channel speech separation module followed by the
state-of-the-art single-channel ASR module; 2) a "All-In-One" model where the
3D spatial feature is directly used as an input to ASR system without explicit
separation modules. Both of them are fully differentiable and can be
back-propagated end-to-end. We test them on simulated overlapped speech and
real recordings. Experimental results show that 1) the proposed ALL-In-One
model achieved a comparable error rate to the pipelined system while reducing
the inference time by half; 2) the proposed 3D spatial feature significantly
outperformed (31\% CERR) all previous works of using the 1D directional
information in both paradigms.

    

### [[2111.11032] Episodic Multi-agent Reinforcement Learning with Curiosity-Driven Exploration](http://arxiv.org/abs/2111.11032)


  Efficient exploration in deep cooperative multi-agent reinforcement learning
(MARL) still remains challenging in complex coordination problems. In this
paper, we introduce a novel Episodic Multi-agent reinforcement learning with
Curiosity-driven exploration, called EMC. We leverage an insight of popular
factorized MARL algorithms that the "induced" individual Q-values, i.e., the
individual utility functions used for local execution, are the embeddings of
local action-observation histories, and can capture the interaction between
agents due to reward backpropagation during centralized training. Therefore, we
use prediction errors of individual Q-values as intrinsic rewards for
coordinated exploration and utilize episodic memory to exploit explored
informative experience to boost policy training. As the dynamics of an agent's
individual Q-value function captures the novelty of states and the influence
from other agents, our intrinsic reward can induce coordinated exploration to
new or promising states. We illustrate the advantages of our method by didactic
examples, and demonstrate its significant outperformance over state-of-the-art
MARL baselines on challenging tasks in the StarCraft II micromanagement
benchmark.

    

### [[2111.11053] DAPPER: Performance Estimation of Domain Adaptation in Mobile Sensing](http://arxiv.org/abs/2111.11053)


  Many applications that utilize sensors in mobile devices and apply machine
learning to provide novel services have emerged. However, various factors such
as different users, devices, environments, and hyperparameters, affect the
performance for such applications, thus making the domain shift (i.e.,
distribution shift of a target user from the training source dataset) an
important problem. Although recent domain adaptation techniques attempt to
solve this problem, the complex interplay between the diverse factors often
limits their effectiveness. We argue that accurately estimating the performance
in untrained domains could significantly reduce performance uncertainty. We
present DAPPER (Domain AdaPtation Performance EstimatoR) that estimates the
adaptation performance in a target domain with only unlabeled target data. Our
intuition is that the outputs of a model on the target data provide clues for
the model's actual performance in the target domain. DAPPER does not require
expensive labeling costs nor involve additional training after deployment. Our
evaluation with four real-world sensing datasets compared against four
baselines shows that DAPPER outperforms the baselines by on average 17% in
estimation accuracy. Moreover, our on-device experiment shows that DAPPER
achieves up to 216X less computation overhead compared with the baselines.

    

### [[2111.11056] Evaluating Adversarial Attacks on ImageNet: A Reality Check on Misclassification Classes](http://arxiv.org/abs/2111.11056)


  Although ImageNet was initially proposed as a dataset for performance
benchmarking in the domain of computer vision, it also enabled a variety of
other research efforts. Adversarial machine learning is one such research
effort, employing deceptive inputs to fool models in making wrong predictions.
To evaluate attacks and defenses in the field of adversarial machine learning,
ImageNet remains one of the most frequently used datasets. However, a topic
that is yet to be investigated is the nature of the classes into which
adversarial examples are misclassified. In this paper, we perform a detailed
analysis of these misclassification classes, leveraging the ImageNet class
hierarchy and measuring the relative positions of the aforementioned type of
classes in the unperturbed origins of the adversarial examples. We find that
$71\%$ of the adversarial examples that achieve model-to-model adversarial
transferability are misclassified into one of the top-5 classes predicted for
the underlying source images. We also find that a large subset of untargeted
misclassifications are, in fact, misclassifications into semantically similar
classes. Based on these findings, we discuss the need to take into account the
ImageNet class hierarchy when evaluating untargeted adversarial successes.
Furthermore, we advocate for future research efforts to incorporate categorical
information.

    

### [[2111.11063] Comparing the Accuracy of Deep Neural Networks (DNN) and Convolutional Neural Network (CNN) in Music Genre Recognition (MGR): Experiments on Kurdish Music](http://arxiv.org/abs/2111.11063)


  Musicologists use various labels to classify similar music styles under a
shared title. But, non-specialists may categorize music differently. That could
be through finding patterns in harmony, instruments, and form of the music.
People usually identify a music genre solely by listening, but now computers
and Artificial Intelligence (AI) can automate this process. The work on
applying AI in the classification of types of music has been growing recently,
but there is no evidence of such research on the Kurdish music genres. In this
research, we developed a dataset that contains 880 samples from eight different
Kurdish music genres. We evaluated two machine learning approaches, a Deep
Neural Network (DNN) and a Convolutional Neural Network (CNN), to recognize the
genres. The results showed that the CNN model outperformed the DNN by achieving
92% versus 90% accuracy.

    

### [[2111.11066] FedCV: A Federated Learning Framework for Diverse Computer Vision Tasks](http://arxiv.org/abs/2111.11066)


  Federated Learning (FL) is a distributed learning paradigm that can learn a
global or personalized model from decentralized datasets on edge devices.
However, in the computer vision domain, model performance in FL is far behind
centralized training due to the lack of exploration in diverse tasks with a
unified FL framework. FL has rarely been demonstrated effectively in advanced
computer vision tasks such as object detection and image segmentation. To
bridge the gap and facilitate the development of FL for computer vision tasks,
in this work, we propose a federated learning library and benchmarking
framework, named FedCV, to evaluate FL on the three most representative
computer vision tasks: image classification, image segmentation, and object
detection. We provide non-I.I.D. benchmarking datasets, models, and various
reference FL algorithms. Our benchmark study suggests that there are multiple
challenges that deserve future exploration: centralized training tricks may not
be directly applied to FL; the non-I.I.D. dataset actually downgrades the model
accuracy to some degree in different tasks; improving the system efficiency of
federated training is challenging given the huge number of parameters and the
per-client memory cost. We believe that such a library and benchmark, along
with comparable evaluation settings, is necessary to make meaningful progress
in FL on computer vision tasks. FedCV is publicly available:
this https URL.

    

### [[2111.11090] Optimistic Temporal Difference Learning for 2048](http://arxiv.org/abs/2111.11090)


  Temporal difference (TD) learning and its variants, such as multistage TD
(MS-TD) learning and temporal coherence (TC) learning, have been successfully
applied to 2048. These methods rely on the stochasticity of the environment of
2048 for exploration. In this paper, we propose to employ optimistic
initialization (OI) to encourage exploration for 2048, and empirically show
that the learning quality is significantly improved. This approach
optimistically initializes the feature weights to very large values. Since
weights tend to be reduced once the states are visited, agents tend to explore
those states which are unvisited or visited few times. Our experiments show
that both TD and TC learning with OI significantly improve the performance. As
a result, the network size required to achieve the same performance is
significantly reduced. With additional tunings such as expectimax search,
multistage learning, and tile-downgrading technique, our design achieves the
state-of-the-art performance, namely an average score of 625 377 and a rate of
72% reaching 32768 tiles. In addition, for sufficiently large tests, 65536
tiles are reached at a rate of 0.02%.

    

### [[2111.11097] UMBRELLA: Uncertainty-Aware Model-Based Offline Reinforcement Learning Leveraging Planning](http://arxiv.org/abs/2111.11097)


  Offline reinforcement learning (RL) provides a framework for learning
decision-making from offline data and therefore constitutes a promising
approach for real-world applications as automated driving. Self-driving
vehicles (SDV) learn a policy, which potentially even outperforms the behavior
in the sub-optimal data set. Especially in safety-critical applications as
automated driving, explainability and transferability are key to success. This
motivates the use of model-based offline RL approaches, which leverage
planning. However, current state-of-the-art methods often neglect the influence
of aleatoric uncertainty arising from the stochastic behavior of multi-agent
systems. This work proposes a novel approach for Uncertainty-aware Model-Based
Offline REinforcement Learning Leveraging plAnning (UMBRELLA), which solves the
prediction, planning, and control problem of the SDV jointly in an
interpretable learning-based fashion. A trained action-conditioned stochastic
dynamics model captures distinctively different future evolutions of the
traffic scene. The analysis provides empirical evidence for the effectiveness
of our approach in challenging automated driving simulations and based on a
real-world public dataset.

    

### [[1406.5143] The Sample Complexity of Learning Linear Predictors with the Squared Loss](http://arxiv.org/abs/1406.5143)


  In this short note, we provide a sample complexity lower bound for learning
linear predictors with respect to the squared loss. Our focus is on an agnostic
setting, where no assumptions are made on the data distribution. This contrasts
with standard results in the literature, which either make distributional
assumptions, refer to specific parameter settings, or use other performance
measures.

    

### [[1611.06221] Foundations of Structural Causal Models with Cycles and Latent Variables](http://arxiv.org/abs/1611.06221)


  Structural causal models (SCMs), also known as (nonparametric) structural
equation models (SEMs), are widely used for causal modeling purposes. In
particular, acyclic SCMs, also known as recursive SEMs, form a well-studied
subclass of SCMs that generalize causal Bayesian networks to allow for latent
confounders. In this paper, we investigate SCMs in a more general setting,
allowing for the presence of both latent confounders and cycles. We show that
in the presence of cycles, many of the convenient properties of acyclic SCMs do
not hold in general: they do not always have a solution; they do not always
induce unique observational, interventional and counterfactual distributions; a
marginalization does not always exist, and if it exists the marginal model does
not always respect the latent projection; they do not always satisfy a Markov
property; and their graphs are not always consistent with their causal
semantics. We prove that for SCMs in general each of these properties does hold
under certain solvability conditions. Our work generalizes results for SCMs
with cycles that were only known for certain special cases so far. We introduce
the class of simple SCMs that extends the class of acyclic SCMs to the cyclic
setting, while preserving many of the convenient properties of acyclic SCMs.
With this paper we aim to provide the foundations for a general theory of
statistical causal modeling with SCMs.

    

### [[1708.03731] OpenML Benchmarking Suites](http://arxiv.org/abs/1708.03731)


  Machine learning research depends on objectively interpretable, comparable,
and reproducible algorithm benchmarks. We advocate the use of curated,
comprehensive suites of machine learning tasks to standardize the setup,
execution, and reporting of benchmarks. We enable this through software tools
that help to create and leverage these benchmarking suites. These are
seamlessly integrated into the OpenML platform, and accessible through
interfaces in Python, Java, and R. OpenML benchmarking suites (a) are easy to
use through standardized data formats, APIs, and client libraries; (b) come
with extensive meta-information on the included datasets; and (c) allow
benchmarks to be shared and reused in future studies. We then present a first,
carefully curated and practical benchmarking suite for classification: the
OpenML Curated Classification benchmarking suite 2018 (OpenML-CC18). Finally,
we discuss use cases and applications which demonstrate the usefulness of
OpenML benchmarking suites and the OpenML-CC18 in particular.

    

### [[1901.00117] An Active Learning Framework for Efficient Robust Policy Search](http://arxiv.org/abs/1901.00117)


  Robust Policy Search is the problem of learning policies that do not degrade
in performance when subject to unseen environment model parameters. It is
particularly relevant for transferring policies learned in a simulation
environment to the real world. Several existing approaches involve sampling
large batches of trajectories which reflect the differences in various possible
environments, and then selecting some subset of these to learn robust policies,
such as the ones that result in the worst performance. We propose an active
learning based framework, EffAcTS, to selectively choose model parameters for
this purpose so as to collect only as much data as necessary to select such a
subset. We apply this framework using Linear Bandits, and experimentally
validate the gains in sample efficiency and the performance of our approach on
standard continuous control tasks. We also present a Multi-Task Learning
perspective to the problem of Robust Policy Search, and draw connections from
our proposed framework to existing work on Multi-Task Learning.

    

### [[1910.14594] Deep Learning for 2D and 3D Rotatable Data: An Overview of Methods](http://arxiv.org/abs/1910.14594)


  Convolutional networks are successful due to their equivariance/invariance
under translations. However, rotatable data such as images, volumes, shapes, or
point clouds require processing with equivariance/invariance under rotations in
cases where the rotational orientation of the coordinate system does not affect
the meaning of the data (e.g. object classification). On the other hand,
estimation/processing of rotations is necessary in cases where rotations are
important (e.g. motion estimation). There has been recent progress in methods
and theory in all these regards. Here we provide an overview of existing
methods, both for 2D and 3D rotations (and translations), and identify
commonalities and links between them.

    

### [[1911.06253] Understanding Graph Neural Networks with Asymmetric Geometric Scattering Transforms](http://arxiv.org/abs/1911.06253)


  The scattering transform is a multilayered wavelet-based deep learning
architecture that acts as a model of convolutional neural networks. Recently,
several works have introduced generalizations of the scattering transform for
non-Euclidean settings such as graphs. Our work builds upon these constructions
by introducing windowed and non-windowed geometric scattering transforms for
graphs based upon a very general class of asymmetric wavelets. We show that
these asymmetric graph scattering transforms have many of the same theoretical
guarantees as their symmetric counterparts. As a result, the proposed
construction unifies and extends known theoretical results for many of the
existing graph scattering architectures. In doing so, this work helps bridge
the gap between geometric scattering and other graph neural networks by
introducing a large family of networks with provable stability and invariance
guarantees. These results lay the groundwork for future deep learning
architectures for graph-structured data that have learned filters and also
provably have desirable theoretical properties.

    

### [[1911.07292] Two Efficient Ridge Solutions for the Incremental Broad Learning System on Added Inputs](http://arxiv.org/abs/1911.07292)


  This paper proposes the recursive and square-root BLS algorithms to improve
the original BLS for new added inputs, which utilize the inverse and inverse
Cholesky factor of the Hermitian matrix in the ridge inverse, respectively, to
update the ridge solution. The recursive BLS updates the inverse by the matrix
inversion lemma, while the square-root BLS updates the upper-triangular inverse
Cholesky factor by multiplying it with an upper-triangular intermediate matrix.
When the added p training samples are more than the total k nodes in the
network, i.e., p > k, the inverse of a sum of matrices is applied to take a
smaller matrix inversion or inverse Cholesky factorization. The original BLS
based on the generalized inverse with the ridge regression assumes the ridge
parameter {\lambda}->0 in the ridge inverse. When {\lambda}->0 is not
satisfied, the numerical experiments on the MNIST and NORB datasets show that
both the proposed ridge solutions improve the testing accuracy of the original
BLS, and the improvement becomes more significant as {\lambda} is bigger. On
the other hand, compared to the original BLS, both the proposed BLS algorithms
theoretically require less complexities, and are significantly faster in the
simulations on the MNIST dataset. The speedups in total training time of the
recursive and square-root BLS algorithms over the original BLS are 4.41 and
6.92 respectively when p > k, and are 2.80 and 1.59 respectively when p < k.

    

### [[1912.12528] Applications of Unsupervised Deep Transfer Learning to Intelligent Fault Diagnosis: A Survey and Comparative Study](http://arxiv.org/abs/1912.12528)


  Recent progress on intelligent fault diagnosis (IFD) has greatly depended on
deep representation learning and plenty of labeled data. However, machines
often operate with various working conditions or the target task has different
distributions with the collected data used for training (the domain shift
problem). Besides, the newly collected test data in the target domain are
usually unlabeled, leading to unsupervised deep transfer learning based
(UDTL-based) IFD problem. Although it has achieved huge development, a standard
and open source code framework as well as a comparative study for UDTL-based
IFD are not yet established. In this paper, we construct a new taxonomy and
perform a comprehensive review of UDTL-based IFD according to different tasks.
Comparative analysis of some typical methods and datasets reveals some open and
essential issues in UDTL-based IFD which are rarely studied, including
transferability of features, influence of backbones, negative transfer,
physical priors, etc. To emphasize the importance and reproducibility of
UDTL-based IFD, the whole test framework will be released to the research
community to facilitate future research. In summary, the released framework and
comparative study can serve as an extended interface and basic results to carry
out new studies on UDTL-based IFD. The code framework is available at
\url{this https URL}.

    

### [[2005.06059] A computational model implementing subjectivity with the 'Room Theory'. The case of detecting Emotion from Text](http://arxiv.org/abs/2005.06059)


  This work introduces a new method to consider subjectivity and general
context dependency in text analysis and uses as example the detection of
emotions conveyed in text. The proposed method takes into account subjectivity
using a computational version of the Framework Theory by Marvin Minsky (1974)
leveraging on the Word2Vec approach to text vectorization by Mikolov et al.
(2013), used to generate distributed representation of words based on the
context where they appear. Our approach is based on three components: 1. a
framework/'room' representing the point of view; 2. a benchmark representing
the criteria for the analysis - in this case the emotion classification, from a
study of human emotions by Robert Plutchik (1980); and 3. the document to be
analyzed. By using similarity measure between words, we are able to extract the
relative relevance of the elements in the benchmark - intensities of emotions
in our case study - for the document to be analyzed. Our method provides a
measure that take into account the point of view of the entity reading the
document. This method could be applied to all the cases where evaluating
subjectivity is relevant to understand the relative value or meaning of a text.
Subjectivity can be not limited to human reactions, but it could be used to
provide a text with an interpretation related to a given domain ("room"). To
evaluate our method, we used a test case in the political domain.

    

### [[2005.08644] Intracranial Hemorrhage Detection Using Neural Network Based Methods With Federated Learning](http://arxiv.org/abs/2005.08644)


  Intracranial hemorrhage, bleeding that occurs inside the cranium, is a
serious health problem requiring rapid and often intensive medical treatment.
Such a condition is traditionally diagnosed by highly-trained specialists
analyzing computed tomography (CT) scan of the patient and identifying the
location and type of hemorrhage if one exists. We propose a neural network
approach to find and classify the condition based upon the CT scan. The model
architecture implements a time distributed convolutional network. We observed
accuracy above 92% from such an architecture, provided enough data. We propose
further extensions to our approach involving the deployment of federated
learning. This would be helpful in pooling learned parameters without violating
the inherent privacy of the data involved.

    

### [[2005.08797] Variational quantum Gibbs state preparation with a truncated Taylor series](http://arxiv.org/abs/2005.08797)


  The preparation of quantum Gibbs state is an essential part of quantum
computation and has wide-ranging applications in various areas, including
quantum simulation, quantum optimization, and quantum machine learning. In this
paper, we propose variational hybrid quantum-classical algorithms for quantum
Gibbs state preparation. We first utilize a truncated Taylor series to evaluate
the free energy and choose the truncated free energy as the loss function. Our
protocol then trains the parameterized quantum circuits to learn the desired
quantum Gibbs state. Notably, this algorithm can be implemented on near-term
quantum computers equipped with parameterized quantum circuits. By performing
numerical experiments, we show that shallow parameterized circuits with only
one additional qubit can be trained to prepare the Ising chain and spin chain
Gibbs states with a fidelity higher than 95%. In particular, for the Ising
chain model, we find that a simplified circuit ansatz with only one parameter
and one additional qubit can be trained to realize a 99% fidelity in Gibbs
state preparation at inverse temperatures larger than 2.

    

### [[2006.08679] Feature Space Saturation during Training](http://arxiv.org/abs/2006.08679)


  We propose layer saturation - a simple, online-computable method for
analyzing the information processing in neural networks. First, we show that a
layer's output can be restricted to the eigenspace of its variance matrix
without performance loss. We propose a computationally lightweight method for
approximating the variance matrix during training. From the dimension of its
lossless eigenspace we derive layer saturation - the ratio between the
eigenspace dimension and layer width. We show that saturation seems to indicate
which layers contribute to network performance. We demonstrate how to alter
layer saturation in a neural network by changing network depth, filter sizes
and input resolution. Furthermore, we show that well-chosen input resolution
increases network performance by distributing the inference process more evenly
across the network.

    

### [[2006.14978] Online 3D Bin Packing with Constrained Deep Reinforcement Learning](http://arxiv.org/abs/2006.14978)


  We solve a challenging yet practically useful variant of 3D Bin Packing
Problem (3D-BPP). In our problem, the agent has limited information about the
items to be packed into the bin, and an item must be packed immediately after
its arrival without buffering or readjusting. The item's placement also
subjects to the constraints of collision avoidance and physical stability. We
formulate this online 3D-BPP as a constrained Markov decision process. To solve
the problem, we propose an effective and easy-to-implement constrained deep
reinforcement learning (DRL) method under the actor-critic framework. In
particular, we introduce a feasibility predictor to predict the feasibility
mask for the placement actions and use it to modulate the action probabilities
output by the actor during training. Such supervisions and transformations to
DRL facilitate the agent to learn feasible policies efficiently. Our method can
also be generalized e.g., with the ability to handle lookahead or items with
different orientations. We have conducted extensive evaluation showing that the
learned policy significantly outperforms the state-of-the-art methods. A user
study suggests that our method attains a human-level performance.

    

### [[2009.12263] Flexible Performant GEMM Kernels on GPUs](http://arxiv.org/abs/2009.12263)


  General Matrix Multiplication or GEMM kernels take centre place in high
performance computing and machine learning. Recent NVIDIA GPUs include GEMM
accelerators, such as NVIDIA's Tensor Cores. Their exploitation is hampered by
the two-language problem: it requires either low-level programming which
implies low programmer productivity or using libraries that only offer a
limited set of components. Because rephrasing algorithms in terms of
established components often introduces overhead, the libraries' lack of
flexibility limits the freedom to explore new algorithms. Researchers using
GEMMs can hence not enjoy programming productivity, high performance, and
research flexibility at once.
In this paper we solve this problem. We present three sets of abstractions
and interfaces to program GEMMs within the scientific Julia programming
language. The interfaces and abstractions are co-designed for researchers'
needs and Julia's features to achieve sufficient separation of concerns and
flexibility to easily extend basic GEMMs in many different ways without paying
a performance price. Comparing our GEMMs to state-of-the-art libraries cuBLAS
and CUTLASS, we demonstrate that our performance is in the same ballpark of the
libraries, and in some cases even exceeds it, without having to write a single
line of code in CUDA C++ or assembly, and without facing flexibility
limitations.

    

### [[2009.12780] RENT -- Repeated Elastic Net Technique for Feature Selection](http://arxiv.org/abs/2009.12780)


  Feature selection is an essential step in data science pipelines to reduce
the complexity associated with large datasets. While much research on this
topic focuses on optimizing predictive performance, few studies investigate
stability in the context of the feature selection process. In this study, we
present the Repeated Elastic Net Technique (RENT) for Feature Selection. RENT
uses an ensemble of generalized linear models with elastic net regularization,
each trained on distinct subsets of the training data. The feature selection is
based on three criteria evaluating the weight distributions of features across
all elementary models. This fact leads to the selection of features with high
stability that improve the robustness of the final model. Furthermore, unlike
established feature selectors, RENT provides valuable information for model
interpretation concerning the identification of objects in the data that are
difficult to predict during training. In our experiments, we benchmark RENT
against six established feature selectors on eight multivariate datasets for
binary classification and regression. In the experimental comparison, RENT
shows a well-balanced trade-off between predictive performance and stability.
Finally, we underline the additional interpretational value of RENT with an
exploratory post-hoc analysis of a healthcare dataset.

    

### [[2010.12061] Simple Neighborhood Representative Pre-processing Boosts Outlier Detectors](http://arxiv.org/abs/2010.12061)


  Over the decades, traditional outlier detectors have ignored the group-level
factor when calculating outlier scores for objects in data by evaluating only
the object-level factor, failing to capture the collective outliers. To
mitigate this issue, we present a method called neighborhood representative
(NR), which empowers all the existing outlier detectors to efficiently detect
outliers, including collective outliers, while maintaining their computational
integrity. It achieves this by selecting representative objects, scoring these
objects, then applies the score of the representative objects to its collective
objects. Without altering existing detectors, NR is compatible with existing
detectors, while improving performance on real world datasets with +8% (0.72 to
0.78 AUC) relative to state-of-the-art outlier detectors.

    

### [[2011.07403] A Hybrid Approach for Improved Low Resource Neural Machine Translation using Monolingual Data](http://arxiv.org/abs/2011.07403)


  Many language pairs are low resource, meaning the amount and/or quality of
available parallel data is not sufficient to train a neural machine translation
(NMT) model which can reach an acceptable standard of accuracy. Many works have
explored using the readily available monolingual data in either or both of the
languages to improve the standard of translation models in low, and even high,
resource languages. One of the most successful of such works is the
back-translation that utilizes the translations of the target language
monolingual data to increase the amount of the training data. The quality of
the backward model which is trained on the available parallel data has been
shown to determine the performance of the back-translation approach. Despite
this, only the forward model is improved on the monolingual target data in
standard back-translation. A previous study proposed an iterative
back-translation approach for improving both models over several iterations.
But unlike in the traditional back-translation, it relied on both the target
and source monolingual data. This work, therefore, proposes a novel approach
that enables both the backward and forward models to benefit from the
monolingual target data through a hybrid of self-learning and back-translation
respectively. Experimental results have shown the superiority of the proposed
approach over the traditional back-translation method on English-German low
resource neural machine translation. We also proposed an iterative
self-learning approach that outperforms the iterative back-translation while
also relying only on the monolingual target data and require the training of
less models.

    

### [[2011.08891] Use HiResCAM instead of Grad-CAM for faithful explanations of convolutional neural networks](http://arxiv.org/abs/2011.08891)


  Explanation methods facilitate the development of models that learn
meaningful concepts and avoid exploiting spurious correlations. We illustrate a
previously unrecognized limitation of the popular neural network explanation
method Grad-CAM: as a side effect of the gradient averaging step, Grad-CAM
sometimes highlights locations the model did not actually use. To solve this
problem, we propose HiResCAM, a novel class-specific explanation method that is
guaranteed to highlight only the locations the model used to make each
prediction. We prove that HiResCAM is a generalization of CAM and explore the
relationships between HiResCAM and other gradient-based explanation methods.
Experiments on PASCAL VOC 2012, including crowd-sourced evaluations, illustrate
that while HiResCAM's explanations faithfully reflect the model, Grad-CAM often
expands the attention to create bigger and smoother visualizations. Overall,
this work advances convolutional neural network explanation approaches and may
aid in the development of trustworthy models for sensitive applications.

    

### [[2011.10996] Predictive maintenance on event logs: Application on an ATM fleet](http://arxiv.org/abs/2011.10996)


  Predictive maintenance is used in industrial applications to increase machine
availability and optimize cost related to unplanned maintenance. In most cases,
predictive maintenance applications use output from sensors, recording physical
phenomenons such as temperature or vibration which can be directly linked to
the degradation process of the machine. However, in some applications, outputs
from sensors are not available, and event logs generated by the machine are
used instead. We first study the approaches used in the literature to solve
predictive maintenance problems and present a new public dataset containing the
event logs from 156 machines. After this, we define an evaluation framework for
predictive maintenance systems, which takes into account business constraints,
and conduct experiments to explore suitable solutions, which can serve as
guidelines for future works using this new dataset.

    

### [[2012.01935] Backpropagation-Free Learning Method for Correlated Fuzzy Neural Networks](http://arxiv.org/abs/2012.01935)


  In this paper, a novel stepwise learning approach based on estimating desired
premise parts' outputs by solving a constrained optimization problem is
proposed. This learning approach does not require backpropagating the output
error to learn the premise parts' parameters. Instead, the near best output
values of the rules premise parts are estimated and their parameters are
changed to reduce the error between current premise parts' outputs and the
estimated desired ones. Therefore, the proposed learning method avoids error
backpropagation, which lead to vanishing gradient and consequently getting
stuck in a local optimum. The proposed method does not need any initialization
method. This learning method is utilized to train a new Takagi-Sugeno-Kang
(TSK) Fuzzy Neural Network with correlated fuzzy rules including many
parameters in both premise and consequent parts, avoiding getting stuck in a
local optimum due to vanishing gradient. To learn the proposed network
parameters, first, a constrained optimization problem is introduced and solved
to estimate the desired values of premise parts' output values. Next, the error
between these values and the current ones is utilized to adapt the premise
parts' parameters based on the gradient-descent (GD) approach. Afterward, the
error between the desired and network's outputs is used to learn consequent
parts' parameters by the GD method. The proposed paradigm is successfully
applied to real-world time-series prediction and regression problems. According
to experimental results, its performance outperforms other methods with a more
parsimonious structure.

    

### [[2012.08713] AIST: An Interpretable Attention-based Deep Learning Model for Crime Prediction](http://arxiv.org/abs/2012.08713)


  Accuracy and interpretability are two essential properties for a crime
prediction model. Because of the adverse effects that the crimes can have on
human life, economy and safety, we need a model that can predict future
occurrence of crime as accurately as possible so that early steps can be taken
to avoid the crime. On the other hand, an interpretable model reveals the
reason behind a model's prediction, ensures its transparency and allows us to
plan the crime prevention steps accordingly. The key challenge in developing
the model is to capture the non-linear spatial dependency and temporal patterns
of a specific crime category while keeping the underlying structure of the
model interpretable. In this paper, we develop AIST, an Attention-based
Interpretable Spatio Temporal Network for crime prediction. AIST models the
dynamic spatio-temporal correlations for a crime category based on past crime
occurrences, external features (e.g., traffic flow and point of interest (POI)
information) and recurring trends of crime. Extensive experiments show the
superiority of our model in terms of both accuracy and interpretability using
real datasets.

    

### [[2012.12901] Lattice gauge equivariant convolutional neural networks](http://arxiv.org/abs/2012.12901)


  We propose Lattice gauge equivariant Convolutional Neural Networks (L-CNNs)
for generic machine learning applications on lattice gauge theoretical
problems. At the heart of this network structure is a novel convolutional layer
that preserves gauge equivariance while forming arbitrarily shaped Wilson loops
in successive bilinear layers. Together with topological information, for
example from Polyakov loops, such a network can in principle approximate any
gauge covariant function on the lattice. We demonstrate that L-CNNs can learn
and generalize gauge invariant quantities that traditional convolutional neural
networks are incapable of finding.

    

### [[2101.06639] Removing Undesirable Feature Contributions Using Out-of-Distribution Data](http://arxiv.org/abs/2101.06639)


  Several data augmentation methods deploy unlabeled-in-distribution (UID) data
to bridge the gap between the training and inference of neural networks.
However, these methods have clear limitations in terms of availability of UID
data and dependence of algorithms on pseudo-labels. Herein, we propose a data
augmentation method to improve generalization in both adversarial and standard
learning by using out-of-distribution (OOD) data that are devoid of the
abovementioned issues. We show how to improve generalization theoretically
using OOD data in each learning scenario and complement our theoretical
analysis with experiments on CIFAR-10, CIFAR-100, and a subset of ImageNet. The
results indicate that undesirable features are shared even among image data
that seem to have little correlation from a human point of view. We also
present the advantages of the proposed method through comparison with other
data augmentation methods, which can be used in the absence of UID data.
Furthermore, we demonstrate that the proposed method can further improve the
existing state-of-the-art adversarial training.

    

### [[2102.01570] Symmetric Sparse Boolean Matrix Factorization and Applications](http://arxiv.org/abs/2102.01570)


  In this work, we study a variant of nonnegative matrix factorization where we
wish to find a symmetric factorization of a given input matrix into a sparse,
Boolean matrix. Formally speaking, given $\mathbf{M}\in\mathbb{Z}^{m\times m}$,
we want to find $\mathbf{W}\in\{0,1\}^{m\times r}$ such that $\| \mathbf{M} -
\mathbf{W}\mathbf{W}^\top \|_0$ is minimized among all $\mathbf{W}$ for which
each row is $k$-sparse. This question turns out to be closely related to a
number of questions like recovering a hypergraph from its line graph, as well
as reconstruction attacks for private neural network training.
As this problem is hard in the worst-case, we study a natural average-case
variant that arises in the context of these reconstruction attacks: $\mathbf{M}
= \mathbf{W}\mathbf{W}^{\top}$ for $\mathbf{W}$ a random Boolean matrix with
$k$-sparse rows, and the goal is to recover $\mathbf{W}$ up to column
permutation. Equivalently, this can be thought of as recovering a uniformly
random $k$-uniform hypergraph from its line graph.
Our main result is a polynomial-time algorithm for this problem based on
bootstrapping higher-order information about $\mathbf{W}$ and then decomposing
an appropriate tensor. The key ingredient in our analysis, which may be of
independent interest, is to show that such a matrix $\mathbf{W}$ has full
column rank with high probability as soon as $m = \widetilde{\Omega}(r)$, which
we do using tools from Littlewood-Offord theory and estimates for binary
Krawtchouk polynomials.

    

### [[2102.06527] Mutually exciting point process graphs for modelling dynamic networks](http://arxiv.org/abs/2102.06527)


  A new class of models for dynamic networks is proposed, called mutually
exciting point process graphs (MEG). MEG is a scalable network-wide statistical
model for point processes with dyadic marks, which can be used for anomaly
detection when assessing the significance future events, including previously
unobserved connections. The model combines mutually exciting point processes to
estimate dependencies between events and latent space models to infer
relationships between the nodes. The intensity functions for each network edge
are parameterised exclusively by node-specific parameters, which allows
information to be shared across the network. This construction enables
estimation of intensities even for unobserved edges, which is particularly
important in real world applications, such as computer networks arising in
cyber-security. A recursive form of the log-likelihood is obtained, which is
used to derive fast inferential procedures via modern gradient ascent
algorithms. An EM algorithm is also derived. The model is tested on simulated
graphs and real world datasets, demonstrating excellent performance.

    

### [[2103.01479] Statistical Post-Processing for Gridded Temperature Prediction Using Encoder-Decoder-Based Deep Convolutional Neural Networks](http://arxiv.org/abs/2103.01479)


  The Japan Meteorological Agency operates gridded temperature guidance to
predict two-dimensional snowfall amounts and precipitation types, e.g., rain
and snow, because surface temperature is one of the key elements to predict
them. Operational temperature guidance is based on the Kalman filter, which
uses temperature observation and numerical weather prediction (NWP) outputs
only around observation sites. Correcting a temperature field when NWP models
incorrectly predict a front's location or when observed temperatures are
extremely cold or hot has been challenging. In this study, an
encoder-decoder-based convolutional neural network has been proposed to predict
gridded temperatures at the surface around the Kanto region in Japan.
Verification results showed that the proposed model greatly improves the
operational guidance and can correct NWP model biases, such as a positional
error of fronts and extreme temperatures.

    

### [[2103.02695] Shift Invariance Can Reduce Adversarial Robustness](http://arxiv.org/abs/2103.02695)


  Shift invariance is a critical property of CNNs that improves performance on
classification. However, we show that invariance to circular shifts can also
lead to greater sensitivity to adversarial attacks. We first characterize the
margin between classes when a shift-invariant linear classifier is used. We
show that the margin can only depend on the DC component of the signals. Then,
using results about infinitely wide networks, we show that in some simple
cases, fully connected and shift-invariant neural networks produce linear
decision boundaries. Using this, we prove that shift invariance in neural
networks produces adversarial examples for the simple case of two classes, each
consisting of a single image with a black or white dot on a gray background.
This is more than a curiosity; we show empirically that with real datasets and
realistic architectures, shift invariance reduces adversarial robustness.
Finally, we describe initial experiments using synthetic data to probe the
source of this connection.

    

### [[2103.03570] Second-order step-size tuning of SGD for non-convex optimization](http://arxiv.org/abs/2103.03570)


  In view of a direct and simple improvement of vanilla SGD, this paper
presents a fine-tuning of its step-sizes in the mini-batch case. For doing so,
one estimates curvature, based on a local quadratic model and using only noisy
gradient approximations. One obtains a new stochastic first-order method
(Step-Tuned SGD), enhanced by second-order information, which can be seen as a
stochastic version of the classical Barzilai-Borwein method. Our theoretical
results ensure almost sure convergence to the critical set and we provide
convergence rates. Experiments on deep residual network training illustrate the
favorable properties of our approach. For such networks we observe, during
training, both a sudden drop of the loss and an improvement of test accuracy at
medium stages, yielding better results than SGD, RMSprop, or ADAM.

    

### [[2104.00269] The Compact Support Neural Network](http://arxiv.org/abs/2104.00269)


  Neural networks are popular and useful in many fields, but they have the
problem of giving high confidence responses for examples that are away from the
training data. This makes the neural networks very confident in their
prediction while making gross mistakes, thus limiting their reliability for
safety-critical applications such as autonomous driving, space exploration,
etc. In this paper, we present a neuron generalization that has the standard
dot-product-based neuron and the RBF neuron as two extreme cases of a shape
parameter. Using ReLU as the activation function we obtain a novel neuron that
has compact support, which means its output is zero outside a bounded domain.
We show how to avoid difficulties in training a neural network with such
neurons, by training a standard neural network first, then gradually increasing
the shape parameter to the desired value. We also prove that a neural network
using the proposed neurons has the universal approximation property, which
means it can approximate any continuous and integrable function with an
arbitrary degree of accuracy. Through experiments on standard benchmark
datasets, we show the promise of the proposed approach, in that it can have
good prediction accuracy on in-distribution samples, while being able to
consistently detect and have low confidence on out-of-distribution samples.

    

### [[2104.04644] Fast and Efficient Locomotion via Learned Gait Transitions](http://arxiv.org/abs/2104.04644)


  We focus on the problem of developing energy efficient controllers for
quadrupedal robots. Animals can actively switch gaits at different speeds to
lower their energy consumption. In this paper, we devise a hierarchical
learning framework, in which distinctive locomotion gaits and natural gait
transitions emerge automatically with a simple reward of energy minimization.
We use evolutionary strategies (ES) to train a high-level gait policy that
specifies gait patterns of each foot, while the low-level convex MPC controller
optimizes the motor commands so that the robot can walk at a desired velocity
using that gait pattern. We test our learning framework on a quadruped robot
and demonstrate automatic gait transitions, from walking to trotting and to
fly-trotting, as the robot increases its speed. We show that the learned
hierarchical controller consumes much less energy across a wide range of
locomotion speed than baseline controllers.

    

### [[2104.05018] TedNet: A Pytorch Toolkit for Tensor Decomposition Networks](http://arxiv.org/abs/2104.05018)


  Tensor Decomposition Networks (TDNs) prevail for their inherent compact
architectures. To give more researchers a flexible way to exploit TDNs, we
present a Pytorch toolkit named TedNet. TedNet implements 5 kinds of tensor
decomposition(i.e., CANDECOMP/PARAFAC (CP), Block-Term Tucker (BTT), Tucker-2,
Tensor Train (TT) and Tensor Ring (TR) on traditional deep neural layers, the
convolutional layer and the fully-connected layer. By utilizing the basic
layers, it is simple to construct a variety of TDNs. TedNet is available at
this https URL.

    

### [[2104.07059] Neural population geometry: An approach for understanding biological and artificial neural networks](http://arxiv.org/abs/2104.07059)


  Advances in experimental neuroscience have transformed our ability to explore
the structure and function of neural circuits. At the same time, advances in
machine learning have unleashed the remarkable computational power of
artificial neural networks (ANNs). While these two fields have different tools
and applications, they present a similar challenge: namely, understanding how
information is embedded and processed through high-dimensional representations
to solve complex tasks. One approach to addressing this challenge is to utilize
mathematical and computational tools to analyze the geometry of these
high-dimensional representations, i.e., neural population geometry. We review
examples of geometrical approaches providing insight into the function of
biological and artificial neural networks: representation untangling in
perception, a geometric theory of classification capacity, disentanglement and
abstraction in cognitive systems, topological representations underlying
cognitive maps, dynamic untangling in motor systems, and a dynamical approach
to cognition. Together, these findings illustrate an exciting trend at the
intersection of machine learning, neuroscience, and geometry, in which neural
population geometry provides a useful population-level mechanistic descriptor
underlying task implementation. Importantly, geometric descriptions are
applicable across sensory modalities, brain regions, network architectures and
timescales. Thus, neural population geometry has the potential to unify our
understanding of structure and function in biological and artificial neural
networks, bridging the gap between single neurons, populations and behavior.

    

### [[2104.07232] Iterative Alignment Flows](http://arxiv.org/abs/2104.07232)


  The unsupervised task of aligning two or more distributions in a shared
latent space has many applications including fair representations, batch effect
mitigation, and unsupervised domain adaptation. Existing flow-based approaches
estimate multiple flows independently, which is equivalent to learning multiple
full generative models. Other approaches require adversarial learning, which
can be computationally expensive and challenging to optimize. Thus, we aim to
jointly align multiple distributions while avoiding adversarial learning.
Inspired by efficient alignment algorithms from optimal transport (OT) theory
for univariate distributions, we develop a simple iterative method to build
deep and expressive flows. Our method decouples each iteration into two
subproblems: 1) form a variational approximation of a distribution divergence
and 2) minimize this variational approximation via closed-form invertible
alignment maps based on known OT results. Our empirical results give evidence
that this iterative algorithm achieves competitive distribution alignment at
low computational cost while being able to naturally handle more than two
distributions.

    

### [[2104.08038] Noise-Aware Video Saliency Prediction](http://arxiv.org/abs/2104.08038)


  We tackle the problem of predicting saliency maps for videos of dynamic
scenes. We note that the accuracy of the maps reconstructed from the gaze data
of a fixed number of observers varies with the frame, as it depends on the
content of the scene. This issue is particularly pressing when a limited number
of observers are available. In such cases, directly minimizing the discrepancy
between the predicted and measured saliency maps, as traditional deep-learning
methods do, results in overfitting to the noisy data. We propose a noise-aware
training (NAT) paradigm that quantifies and accounts for the uncertainty
arising from frame-specific gaze data inaccuracy. We show that NAT is
especially advantageous when limited training data is available, with
experiments across different models, loss functions, and datasets. We also
introduce a video game-based saliency dataset, with rich temporal semantics,
and multiple gaze attractors per frame. The dataset and source code are
available at this https URL.

    

### [[2104.14074] Statistical Inference with M-Estimators on Adaptively Collected Data](http://arxiv.org/abs/2104.14074)


  Bandit algorithms are increasingly used in real-world sequential
decision-making problems. Associated with this is an increased desire to be
able to use the resulting datasets to answer scientific questions like: Did one
type of ad lead to more purchases? In which contexts is a mobile health
intervention effective? However, classical statistical approaches fail to
provide valid confidence intervals when used with data collected with bandit
algorithms. Alternative methods have recently been developed for simple models
(e.g., comparison of means). Yet there is a lack of general methods for
conducting statistical inference using more complex models on data collected
with (contextual) bandit algorithms; for example, current methods cannot be
used for valid inference on parameters in a logistic regression model for a
binary reward. In this work, we develop theory justifying the use of
M-estimators -- which includes estimators based on empirical risk minimization
as well as maximum likelihood -- on data collected with adaptive algorithms,
including (contextual) bandit algorithms. Specifically, we show that
M-estimators, modified with particular adaptive weights, can be used to
construct asymptotically valid confidence regions for a variety of inferential
targets.

    

### [[2105.05911] The Power of the Weisfeiler-Leman Algorithm for Machine Learning with Graphs](http://arxiv.org/abs/2105.05911)


  In recent years, algorithms and neural architectures based on the
Weisfeiler-Leman algorithm, a well-known heuristic for the graph isomorphism
problem, emerged as a powerful tool for (supervised) machine learning with
graphs and relational data. Here, we give a comprehensive overview of the
algorithm's use in a machine learning setting. We discuss the theoretical
background, show how to use it for supervised graph- and node classification,
discuss recent extensions, and its connection to neural architectures.
Moreover, we give an overview of current applications and future directions to
stimulate research.

    

### [[2105.07530] Advances in Multi-Variate Analysis Methods for New Physics Searches at the Large Hadron Collider](http://arxiv.org/abs/2105.07530)


  Between the years 2015 and 2019, members of the Horizon 2020-funded
Innovative Training Network named "AMVA4NewPhysics" studied the customization
and application of advanced multivariate analysis methods and statistical
learning tools to high-energy physics problems, as well as developed entirely
new ones. Many of those methods were successfully used to improve the
sensitivity of data analyses performed by the ATLAS and CMS experiments at the
CERN Large Hadron Collider; several others, still in the testing phase, promise
to further improve the precision of measurements of fundamental physics
parameters and the reach of searches for new phenomena. In this paper, the most
relevant new tools, among those studied and developed, are presented along with
the evaluation of their performances.

    

### [[2105.08040] Unsupervised Deep Learning Methods for Biological Image Reconstruction and Enhancement](http://arxiv.org/abs/2105.08040)


  Recently, deep learning approaches have become the main research frontier for
biological image reconstruction and enhancement problems thanks to their high
performance, along with their ultra-fast inference times. However, due to the
difficulty of obtaining matched reference data for supervised learning, there
has been increasing interest in unsupervised learning approaches that do not
need paired reference data. In particular, self-supervised learning and
generative models have been successfully used for various biological imaging
applications. In this paper, we overview these approaches from a coherent
perspective in the context of classical inverse problems, and discuss their
applications to biological imaging, including electron, fluorescence and
deconvolution microscopy, optical diffraction tomography and functional
neuroimaging.

    

### [[2106.02757] Heuristic-Guided Reinforcement Learning](http://arxiv.org/abs/2106.02757)


  We provide a framework for accelerating reinforcement learning (RL)
algorithms by heuristics constructed from domain knowledge or offline data.
Tabula rasa RL algorithms require environment interactions or computation that
scales with the horizon of the sequential decision-making task. Using our
framework, we show how heuristic-guided RL induces a much shorter-horizon
subproblem that provably solves the original task. Our framework can be viewed
as a horizon-based regularization for controlling bias and variance in RL under
a finite interaction budget. On the theoretical side, we characterize
properties of a good heuristic and its impact on RL acceleration. In
particular, we introduce the novel concept of an improvable heuristic, a
heuristic that allows an RL agent to extrapolate beyond its prior knowledge. On
the empirical side, we instantiate our framework to accelerate several
state-of-the-art algorithms in simulated robotic control tasks and procedurally
generated games. Our framework complements the rich literature on warm-starting
RL with expert demonstrations or exploratory datasets, and introduces a
principled method for injecting prior knowledge into RL.

    

### [[2106.02836] Constrained Generalized Additive 2 Model with Consideration of High-Order Interactions](http://arxiv.org/abs/2106.02836)


  In recent years, machine learning and AI have been introduced in many
industrial fields. In fields such as finance, medicine, and autonomous driving,
where the inference results of a model may have serious consequences, high
interpretability as well as prediction accuracy is required. In this study, we
propose CGA2M+, which is based on the Generalized Additive 2 Model (GA2M) and
differs from it in two major ways. The first is the introduction of
monotonicity. Imposing monotonicity on some functions based on an analyst's
knowledge is expected to improve not only interpretability but also
generalization performance. The second is the introduction of a higher-order
term: given that GA2M considers only second-order interactions, we aim to
balance interpretability and prediction accuracy by introducing a higher-order
term that can capture higher-order interactions. In this way, we can improve
prediction performance without compromising interpretability by applying
learning innovation. Numerical experiments showed that the proposed model has
high predictive performance and interpretability. Furthermore, we confirmed
that generalization performance is improved by introducing monotonicity.

    

### [[2106.04384] FL-Market: Trading Private Models in Federated Learning](http://arxiv.org/abs/2106.04384)


  Federated learning (FL) is an emerging machine learning paradigm, in which
data owners can collaboratively train a model without sharing their raw data.
Two fundamental research problems in FL are the incentive mechanism and privacy
protection. The former focuses on how to incentivize data owners to participate
in FL. The latter studies how to protect data owners' privacy while maintaining
high utility of trained models. However, the incentive mechanism and privacy
protection in FL have been studied separately, and no work simultaneously
solves both problems. In this work, we address the two problems simultaneously
with FL-Market, which incentivizes data owners' participation by providing
appropriate payments and privacy protection. FL-Market enables data owners to
obtain compensation according to their privacy loss quantified by local
differential privacy (LDP). Our insight is that by meeting data owners'
personalized privacy preferences and providing appropriate payments, we can (1)
incentivize privacy risk-tolerant data owners to set larger privacy parameters
(i.e., gradients with less noise) and (2) provide preferred privacy protection
for privacy risk-averse data owners. To achieve this, we design a personalized
LDP-based FL framework with a deep learning-empowered auction mechanism to
incentivize trading private models with less noise and an optimal aggregation
mechanism for aggregating local gradients into an accurate global gradient. Our
experiments verify the effectiveness of the proposed framework and mechanisms.

    

### [[2106.05522] A Mathematical Foundation for Robust Machine Learning based on Bias-Variance Trade-off](http://arxiv.org/abs/2106.05522)


  A common assumption in machine learning is that samples are independently and
identically distributed (i.i.d). However, the contributions of different
samples are not identical in training. Some samples are difficult to learn and
some samples are noisy. The unequal contributions of samples has a considerable
effect on training performances. Studies focusing on unequal sample
contributions (e.g., easy, hard, noisy) in learning usually refer to these
contributions as robust machine learning (RML). Weighing and regularization are
two common techniques in RML. Numerous learning algorithms have been proposed
but the strategies for dealing with easy/hard/noisy samples differ or even
contradict with different learning algorithms. For example, some strategies
take the hard samples first, whereas some strategies take easy first.
Conducting a clear comparison for existing RML algorithms in dealing with
different samples is difficult due to lack of a unified theoretical framework
for RML. This study attempts to construct a mathematical foundation for RML
based on the bias-variance trade-off theory. A series of definitions and
properties are presented and proved. Several classical learning algorithms are
also explained and compared. Improvements of existing methods are obtained
based on the comparison. A unified method that combines two classical learning
strategies is proposed.

    

### [[2106.06513] Learning the optimal Tikhonov regularizer for inverse problems](http://arxiv.org/abs/2106.06513)


  In this work, we consider the linear inverse problem $y=Ax+\epsilon$, where
$A\colon X\to Y$ is a known linear operator between the separable Hilbert
spaces $X$ and $Y$, $x$ is a random variable in $X$ and $\epsilon$ is a
zero-mean random process in $Y$. This setting covers several inverse problems
in imaging including denoising, deblurring, and X-ray tomography. Within the
classical framework of regularization, we focus on the case where the
regularization functional is not given a priori but learned from data. Our
first result is a characterization of the optimal generalized Tikhonov
regularizer, with respect to the mean squared error. We find that it is
completely independent of the forward operator $A$ and depends only on the mean
and covariance of $x$. Then, we consider the problem of learning the
regularizer from a finite training set in two different frameworks: one
supervised, based on samples of both $x$ and $y$, and one unsupervised, based
only on samples of $x$. In both cases, we prove generalization bounds, under
some weak assumptions on the distribution of $x$ and $\epsilon$, including the
case of sub-Gaussian variables. Our bounds hold in infinite-dimensional spaces,
thereby showing that finer and finer discretizations do not make this learning
problem harder. The results are validated through numerical simulations.

    

### [[2106.07141] Selection of Source Images Heavily Influences the Effectiveness of Adversarial Attacks](http://arxiv.org/abs/2106.07141)


  Although the adoption rate of deep neural networks (DNNs) has tremendously
increased in recent years, a solution for their vulnerability against
adversarial examples has not yet been found. As a result, substantial research
efforts are dedicated to fix this weakness, with many studies typically using a
subset of source images to generate adversarial examples, treating every image
in this subset as equal. We demonstrate that, in fact, not every source image
is equally suited for this kind of assessment. To do so, we devise a
large-scale model-to-model transferability scenario for which we meticulously
analyze the properties of adversarial examples, generated from every suitable
source image in ImageNet by making use of three of the most frequently deployed
attacks. In this transferability scenario, which involves seven distinct DNN
models, including the recently proposed vision transformers, we reveal that it
is possible to have a difference of up to $12.5\%$ in model-to-model
transferability success, $1.01$ in average $L_2$ perturbation, and $0.03$
($8/225$) in average $L_{\infty}$ perturbation when $1,000$ source images are
sampled randomly among all suitable candidates. We then take one of the first
steps in evaluating the robustness of images used to create adversarial
examples, proposing a number of simple but effective methods to identify
unsuitable source images, thus making it possible to mitigate extreme cases in
experimentation and support high-quality benchmarking.

    

### [[2106.08992] A unifying point of view on expressive power of GNNs](http://arxiv.org/abs/2106.08992)


  Graph Neural Networks (GNNs) are a wide class of connectionist models for
graph processing. They perform an iterative message passing operation on each
node and its neighbors, to solve classification/ clustering tasks -- on some
nodes or on the whole graph -- collecting all such messages, regardless of
their order. Despite the differences among the various models belonging to this
class, most of them adopt the same computation scheme, based on a local
aggregation mechanism and, intuitively, the local computation framework is
mainly responsible for the expressive power of GNNs. In this paper, we prove
that the Weisfeiler--Lehman test induces an equivalence relationship on the
graph nodes that exactly corresponds to the unfolding equivalence, defined on
the original GNN model. Therefore, the results on the expressive power of the
original GNNs can be extended to general GNNs which, under mild conditions, can
be proved capable of approximating, in probability and up to any precision, any
function on graphs that respects the unfolding equivalence.

    

### [[2106.11562] SSUL: Semantic Segmentation with Unknown Label for Exemplar-based Class-Incremental Learning](http://arxiv.org/abs/2106.11562)


  This paper introduces a solid state-of-the-art baseline for a
class-incremental semantic segmentation (CISS) problem. While the recent CISS
algorithms utilize variants of the knowledge distillation (KD) technique to
tackle the problem, they failed to fully address the critical challenges in
CISS causing the catastrophic forgetting; the semantic drift of the background
class and the multi-label prediction issue. To better address these challenges,
we propose a new method, dubbed SSUL-M (Semantic Segmentation with Unknown
Label with Memory), by carefully combining techniques tailored for semantic
segmentation. Specifically, we claim three main contributions. (1) defining
unknown classes within the background class to help to learn future classes
(help plasticity), (2) freezing backbone network and past classifiers with
binary cross-entropy loss and pseudo-labeling to overcome catastrophic
forgetting (help stability), and (3) utilizing tiny exemplar memory for the
first time in CISS to improve both plasticity and stability. The extensively
conducted experiments show the effectiveness of our method, achieving
significantly better performance than the recent state-of-the-art baselines on
the standard benchmark datasets. Furthermore, we justify our contributions with
thorough ablation analyses and discuss different natures of the CISS problem
compared to the traditional class-incremental learning targeting
classification. The official code is available at
this https URL.

    

### [[2106.11719] Test Distribution-Aware Active Learning: A Principled Approach Against Distribution Shift and Outliers](http://arxiv.org/abs/2106.11719)


  Expanding on MacKay (1992), we argue that conventional model-based methods
for active learning - like BALD - have a fundamental shortfall: they fail to
directly account for the test-time distribution of the input variables. This
can lead to pathologies in the acquisition strategy, as what is maximally
informative for model parameters may not be maximally informative for
prediction: for example, when the data in the pool set is more dispersed than
that of the final prediction task, or when the distribution of pool and test
samples differs. To correct this, we revisit an acquisition strategy that is
based on maximizing the expected information gained about possible future
predictions, referring to this as the Expected Predictive Information Gain
(EPIG). As EPIG does not scale well for batch acquisition, we further examine
an alternative strategy, a hybrid between BALD and EPIG, which we call the
Joint Expected Predictive Information Gain (JEPIG). We consider using both for
active learning with Bayesian neural networks on a variety of datasets,
examining the behavior under distribution shift in the pool set.

    

### [[2106.11899] Local policy search with Bayesian optimization](http://arxiv.org/abs/2106.11899)


  Reinforcement learning (RL) aims to find an optimal policy by interaction
with an environment. Consequently, learning complex behavior requires a vast
number of samples, which can be prohibitive in practice. Nevertheless, instead
of systematically reasoning and actively choosing informative samples, policy
gradients for local search are often obtained from random perturbations. These
random samples yield high variance estimates and hence are sub-optimal in terms
of sample complexity. Actively selecting informative samples is at the core of
Bayesian optimization, which constructs a probabilistic surrogate of the
objective from past samples to reason about informative subsequent ones. In
this paper, we propose to join both worlds. We develop an algorithm utilizing a
probabilistic model of the objective function and its gradient. Based on the
model, the algorithm decides where to query a noisy zeroth-order oracle to
improve the gradient estimates. The resulting algorithm is a novel type of
policy search method, which we compare to existing black-box algorithms. The
comparison reveals improved sample complexity and reduced variance in extensive
empirical evaluations on synthetic objectives. Further, we highlight the
benefits of active sampling on popular RL benchmarks.

    

### [[2106.13727] Interval and fuzzy physics-informed neural networks for uncertain fields](http://arxiv.org/abs/2106.13727)


  Temporally and spatially dependent uncertain parameters are regularly
encountered in engineering applications. Commonly these uncertainties are
accounted for using random fields and processes, which require knowledge about
the appearing probability distributions functions that is not readily
available. In these cases non-probabilistic approaches such as interval
analysis and fuzzy set theory are helpful uncertainty measures. Partial
differential equations involving fuzzy and interval fields are traditionally
solved using the finite element method where the input fields are sampled using
some basis function expansion methods. This approach however is problematic, as
it is reliant on knowledge about the spatial correlation fields. In this work
we utilize physics-informed neural networks (PINNs) to solve interval and fuzzy
partial differential equations. The resulting network structures termed
interval physics-informed neural networks (iPINNs) and fuzzy physics-informed
neural networks (fPINNs) show promising results for obtaining bounded solutions
of equations involving spatially and/or temporally uncertain parameter fields.
In contrast to finite element approaches, no correlation length specification
of the input fields as well as no Monte-Carlo simulations are necessary. In
fact, information about the input interval fields is obtained directly as a
byproduct of the presented solution scheme. Furthermore, all major advantages
of PINNs are retained, i.e. meshfree nature of the scheme, and ease of inverse
problem set-up.

    

### [[2106.14077] The Role of Contextual Information in Best Arm Identification](http://arxiv.org/abs/2106.14077)


  We study the best-arm identification problem with fixed confidence when
contextual (covariate) information is available in stochastic bandits. Although
we can use contextual information in each round, we are interested in the
marginalized mean reward over the contextual distribution. Our goal is to
identify the best arm with a minimal number of samplings under a given value of
the error rate. We show the instance-specific sample complexity lower bounds
for the problem. Then, we propose a context-aware version of the
"Track-and-Stop" strategy, wherein the proportion of the arm draws tracks the
set of optimal allocations and prove that the expected number of arm draws
matches the lower bound asymptotically. We demonstrate that contextual
information can be used to improve the efficiency of the identification of the
best marginalized mean reward compared with the results of Garivier & Kaufmann
(2016). We experimentally confirm that context information contributes to
faster best-arm identification.

    

### [[2106.15563] Learning latent causal graphs via mixture oracles](http://arxiv.org/abs/2106.15563)


  We study the problem of reconstructing a causal graphical model from data in
the presence of latent variables. The main problem of interest is recovering
the causal structure over the latent variables while allowing for general,
potentially nonlinear dependence between the variables. In many practical
problems, the dependence between raw observations (e.g. pixels in an image) is
much less relevant than the dependence between certain high-level, latent
features (e.g. concepts or objects), and this is the setting of interest. We
provide conditions under which both the latent representations and the
underlying latent causal model are identifiable by a reduction to a mixture
oracle. These results highlight an intriguing connection between the
well-studied problem of learning the order of a mixture model and the problem
of learning the bipartite structure between observables and unobservables. The
proof is constructive, and leads to several algorithms for explicitly
reconstructing the full graphical model. We discuss efficient algorithms and
provide experiments illustrating the algorithms in practice.

    

### [[2107.00101] Latent Execution for Neural Program Synthesis](http://arxiv.org/abs/2107.00101)


  Program synthesis from input-output (IO) examples has been a long-standing
challenge. While recent works demonstrated limited success on domain-specific
languages (DSL), it remains highly challenging to apply them to real-world
programming languages, such as C. Due to complicated syntax and token
variation, there are three major challenges: (1) unlike many DSLs, programs in
languages like C need to compile first and are not executed via interpreters;
(2) the program search space grows exponentially when the syntax and semantics
of the programming language become more complex; and (3) collecting a
large-scale dataset of real-world programs is non-trivial. As a first step to
address these challenges, we propose LaSynth and show its efficacy in a
restricted-C domain. More specifically, LaSynth learns the latent
representation to approximate the execution of partially generated programs,
even if they are incomplete in syntax (addressing (1)). The learned execution
significantly improves the performance of next token prediction over existing
approaches, facilitating search (addressing (2)). Finally, once trained with
randomly generated ground-truth programs and their IO pairs, LaSynth can
synthesize more concise programs that resemble human-written code. Furthermore,
retraining our model with these synthesized programs yields better performance
with fewer samples for both Karel and C program synthesis, indicating the
promise of leveraging the learned program synthesizer to improve the dataset
quality for input-output program synthesis (addressing (3)). When evaluating on
whether the program execution outputs match the IO pairs, LaSynth achieves
55.2% accuracy on generating simple C code with tens of tokens including loops
and branches, outperforming existing approaches without executors by around
20%.

    

### [[2109.13096] Learning Transport Processes with Machine Intelligence](http://arxiv.org/abs/2109.13096)


  We present a machine learning based approach to address the study of
transport processes, ubiquitous in continuous mechanics, with particular
attention to those phenomena ruled by complex micro-physics, impractical to
theoretical investigation, yet exhibiting emergent behavior describable by a
closed mathematical expression. Our machine learning model, built using simple
components and following a few well established practices, is capable of
learning latent representations of the transport process substantially closer
to the ground truth than expected from the nominal error characterising the
data, leading to sound generalisation properties. This is demonstrated through
an idealized study of the long standing problem of heat flux suppression
relevant to fusion and cosmic plasmas. Our analysis shows that the result
applies beyond those case specific assumptions and that, in particular, the
accuracy of the learned representation is controllable through knowledge of the
data quality (error properties) and a suitable choice of the dataset size.
While the learned representation can be used as a plug-in for numerical
modeling purposes, it can also be leveraged with the above error analysis to
obtain reliable mathematical expressions describing the transport mechanism and
of great theoretical value.

    

### [[2110.05445] Data-driven approaches for predicting spread of infectious diseases through DINNs: Disease Informed Neural Networks](http://arxiv.org/abs/2110.05445)


  In this work, we present an approach called Disease Informed Neural Networks
(DINNs) that can be employed to effectively predict the spread of infectious
diseases. This approach builds on a successful physics informed neural network
approaches that have been applied to a variety of applications that can be
modeled by linear and non-linear ordinary and partial differential equations.
Specifically, we build on the application of PINNs to SIR compartmental models
and expand it a scaffolded family of mathematical models describing various
infectious diseases. We show how the neural networks are capable of learning
how diseases spread, forecasting their progression, and finding their unique
parameters (e.g. death rate). To demonstrate the robustness and efficacy of
DINNs, we apply the approach to eleven highly infectious diseases that have
been modeled in increasing levels of complexity. Our computational experiments
suggest that DINNs is a reliable candidate for effectively learn about the
dynamics of spread and forecast its progression into the future from available
real-world data.

    

### [[2110.12464] Requirement analysis for an artificial intelligence model for the diagnosis of the COVID-19 from chest X-ray data](http://arxiv.org/abs/2110.12464)


  There are multiple papers published about different AI models for the
COVID-19 diagnosis with promising results. Unfortunately according to the
reviews many of the papers do not reach the level of sophistication needed for
a clinically usable model. In this paper I go through multiple review papers,
guidelines, and other relevant material in order to generate more comprehensive
requirements for the future papers proposing a AI based diagnosis of the
COVID-19 from chest X-ray data (CXR). Main findings are that a clinically
usable AI needs to have an extremely good documentation, comprehensive
statistical analysis of the possible biases and performance, and an
explainability module.

    

### [[2111.10726] The Case for Approximate Intermittent Computing](http://arxiv.org/abs/2111.10726)


  We present the concept of approximate intermittent computing and demonstrate
its application. Intermittent computations stem from the erratic energy
patterns caused by energy harvesting: computations unpredictably terminate
whenever energy is insufficient. Existing solutions maintain equivalence to
continuous executions by creating persistent state. The performance penalty is
massive: system throughput reduces while energy consumption increases.
Approximate intermittent computations trade the accuracy of the results for
sparing the entire overhead to maintain equivalence to a continuous execution.
We use approximation to limit the extent of stateful computations to the single
power cycle, enabling the system to shift the energy budget for managing
persistent state towards an immediate approximate result. First, we apply
approximate intermittent computing to human activity recognition. We design an
anytime variation of support vector machines able to improve the accuracy of
the classification as energy is available. We build a hw/sw prototype using
kinetic energy and show a 7x improvement in system throughput compared to state
of the art, while retaining 83% accuracy in a setting where the best attainable
accuracy is 88%. Next, we apply approximate intermittent computing in a sharply
different scenario, that is, embedded image processing, using loop perforation.
Using a different hw/sw prototype we build and diverse energy traces, we show a
5x improvement in system throughput compared to state of the art, while
providing an equivalent output in 84% of the cases.

    

### [[2111.10577] Distributed CONGEST Approximation of Weighted Vertex Covers and Matchings](http://arxiv.org/abs/2111.10577)


  We provide CONGEST model algorithms for approximating minimum weighted vertex
cover and the maximum weighted matching. For bipartite graphs, we show that a
$(1+\varepsilon)$-approximate weighted vertex cover can be computed
deterministically in polylogarithmic time. This generalizes a corresponding
result for the unweighted vertex cover problem shown in [Faour, Kuhn; OPODIS
'20]. Moreover, we show that in general weighted graph families that are closed
under taking subgraphs and in which we can compute an independent set of weight
at least a $\lambda$-fraction of the total weight, one can compute a
$(2-2\lambda +\varepsilon)$-approximate weighted vertex cover in
polylogarithmic time in the CONGEST model. Our result in particular implies
that in graphs of arboricity $a$, one can compute a
$(2-1/a+\varepsilon)$-approximate weighted vertex cover.
For maximum weighted matchings, we show that a $(1-\varepsilon)$-approximate
solution can be computed deterministically in polylogarithmic CONGEST rounds
(for constant $\varepsilon$). We also provide a more efficient randomized
algorithm. Our algorithm generalizes results of [Lotker, Patt-Shamir, Pettie;
SPAA '08] and [Bar-Yehuda, Hillel, Ghaffari, Schwartzman; PODC '17] for the
unweighted case.
Finally, we show that even in the LOCAL model and in bipartite graphs of
degree $\leq 3$, if $\varepsilon<\varepsilon_0$ for some constant
$\varepsilon_0>0$, then computing a $(1+\varepsilon)$-approximation for the
unweighted minimum vertex cover problem requires $\Omega\big(\frac{\log
n}{\varepsilon}\big)$ rounds. This generalizes aresult of [Göös, Suomela;
DISC '12], who showed that computing a $(1+\varepsilon_0)$-approximation in
such graphs requires $\Omega(\log n)$ rounds.

    

### [[2111.10822] New Clocks, Optimal Line Formation and Efficient Replication Population Protocols (Making Population Protocols Alive)](http://arxiv.org/abs/2111.10822)


  We consider the model of population protocols permitting presence of
dynamically changing edges connecting agents. Our main contribution is a new
constant space phase clock allowing to count parallel time $O(n\log n)$ whp in
the adopted model. This clock admits confirmation of slow leader election and
in turn construction of a line (and a ring) comprising every agent in the
optimal parallel time $\Theta(n\log n)$ and constant space. This improves on
the currently best known upper bound $O(n^2).$
We also discuss a variant of the new clock in which utilisation of edges is
replaced by interaction of agents with a unique leader. This variant provides a
universal (for models with and without edges) synchronisation mechanism and is
adopted in some of our efficient line replication protocols.

    

### [[2111.10980] Theoretically and Practically Efficient Parallel Nucleus Decomposition](http://arxiv.org/abs/2111.10980)


  This paper studies the nucleus decomposition problem, which has been shown to
be useful in finding dense substructures in graphs. We present a novel parallel
algorithm that is efficient both in theory and in practice. Our algorithm
achieves a work complexity matching the best sequential algorithm while also
having low depth (parallel running time), which significantly improves upon the
only existing parallel nucleus decomposition algorithm (Sariyuce et al., PVLDB
2018). The key to the theoretical efficiency of our algorithm is a new lemma
that bounds the amount of work done when peeling cliques from the graph,
combined with the use of a theoretically-efficient parallel algorithms for
clique listing and bucketing. We introduce several new practical optimizations,
including a new multi-level hash table structure to store information on
cliques space-efficiently and a technique for traversing this structure
cache-efficiently. On a 30-core machine with two-way hyper-threading on
real-world graphs, we achieve up to a 55x speedup over the state-of-the-art
parallel nucleus decomposition algorithm by Sariyuce et al., and up to a 40x
self-relative parallel speedup. We are able to efficiently compute larger
nucleus decompositions than prior work on several million-scale graphs for the
first time.

    

### [[2111.11052] IAD: Indirect Anomalous VMMs Detection in the Cloud-based Environment](http://arxiv.org/abs/2111.11052)


  Server virtualization in the form of virtual machines (VMs) with the use of a
hypervisor or a Virtual Machine Monitor (VMM) is an essential part of cloud
computing technology to provide infrastructure-as-a-service (IaaS). A fault or
an anomaly in the VMM can propagate to the VMs hosted on it and ultimately
affect the availability and reliability of the applications running on those
VMs. Therefore, identifying and eventually resolving it quickly is highly
important. However, anomalous VMM detection is a challenge in the cloud
environment since the user does not have access to the VMM.
This paper addresses this challenge of anomalous VMM detection in the
cloud-based environment without having any knowledge or data from VMM by
introducing a novel machine learning-based algorithm called IAD: Indirect
Anomalous VMMs Detection. This algorithm solely uses the VM's resources
utilization data hosted on those VMMs for the anomalous VMMs detection. The
developed algorithm's accuracy was tested on four datasets comprising the
synthetic and real and compared against four other popular algorithms, which
can also be used to the described problem. It was found that the proposed IAD
algorithm has an average F1-score of 83.7% averaged across four datasets, and
also outperforms other algorithms by an average F1-score of 11\%.

    

### [[1901.11282] Priority Inheritance with Backtracking for Iterative Multi-agent Path Finding](http://arxiv.org/abs/1901.11282)


  In the Multi-Agent Path Finding (MAPF) problem, a set of agents moving on a
graph must reach their own respective destinations without inter-agent
collisions. In practical MAPF applications such as navigation in automated
warehouses, where occasionally there are hundreds or more agents, MAPF must be
solved iteratively online on a lifelong basis. Such scenarios rule out simple
adaptations of offline compute-intensive optimal approaches; therefore,
scalable sub-optimal algorithms are appealing for such settings. Ideal scalable
algorithms are applicable to iterative scenarios and output plausible solutions
in predictable computation time.
For the aforementioned purpose, in this study, a novel algorithm named
Priority Inheritance with Backtracking (PIBT) is presented to solve MAPF
iteratively. PIBT relies on an adaptive prioritization scheme to focus on the
adjacent movements of multiple agents; hence it can be applied to several
domains. We prove that, regardless of their number, all agents are guaranteed
to reach their destination within a finite time when the environment is a graph
such that all pairs of adjacent nodes belong to a simple cycle (e.g.,
biconnected). Experimental results covering various scenarios, including real
robot demonstration, reveal the benefits of the proposed method. Even with
hundreds of agents, PIBT yields acceptable solutions immediately and can solve
large instances that other de facto MAPF approaches cannot. In addition, PIBT
outperforms an existing approach on an iterative scenario of conveying packages
in an automated warehouse in both runtime and solution quality.

    

### [[2111.10403] Towards Integrative Multi-Modal Personal Health Navigation Systems: Framework and Application](http://arxiv.org/abs/2111.10403)


  It is well understood that an individual's health trajectory is influenced by
choices made in each moment, such as from lifestyle or medical decisions. With
the advent of modern sensing technologies, individuals have more data and
information about themselves than any other time in history. How can we use
this data to make the best decisions to keep the health state optimal? We
propose a generalized Personal Health Navigation (PHN) framework. PHN takes
individuals towards their personal health goals through a system which
perpetually digests data streams, estimates current health status, computes the
best route through intermediate states utilizing personal models, and guides
the best inputs that carry a user towards their goal.
In addition to describing the general framework, we test the PHN system in
two experiments within the field of cardiology. First, we prospectively test a
knowledge-infused cardiovascular PHN system with a pilot clinical trial of 41
users. Second, we build a data-driven personalized model on cardiovascular
exercise response variability on a smartwatch data-set of 33,269 real-world
users. We conclude with critical challenges in health computing for PHN systems
that require deep future investigation.

    

### [[2111.10480] TransMorph: Transformer for unsupervised medical image registration](http://arxiv.org/abs/2111.10480)


  In the last decade, convolutional neural networks (ConvNets) have dominated
the field of medical image analysis. However, it is found that the performances
of ConvNets may still be limited by their inability to model long-range spatial
relations between voxels in an image. Numerous vision Transformers have been
proposed recently to address the shortcomings of ConvNets, demonstrating
state-of-the-art performances in many medical imaging applications.
Transformers may be a strong candidate for image registration because their
self-attention mechanism enables a more precise comprehension of the spatial
correspondence between moving and fixed images. In this paper, we present
TransMorph, a hybrid Transformer-ConvNet model for volumetric medical image
registration. We also introduce three variants of TransMorph, with two
diffeomorphic variants ensuring the topology-preserving deformations and a
Bayesian variant producing a well-calibrated registration uncertainty estimate.
The proposed models are extensively validated against a variety of existing
registration methods and Transformer architectures using volumetric medical
images from two applications: inter-patient brain MRI registration and
phantom-to-CT registration. Qualitative and quantitative results demonstrate
that TransMorph and its variants lead to a substantial performance improvement
over the baseline methods, demonstrating the effectiveness of Transformers for
medical image registration.

    

### [[2111.10481] Zero-Shot Certified Defense against Adversarial Patches with Vision Transformers](http://arxiv.org/abs/2111.10481)


  Adversarial patch attack aims to fool a machine learning model by arbitrarily
modifying pixels within a restricted region of an input image. Such attacks are
a major threat to models deployed in the physical world, as they can be easily
realized by presenting a customized object in the camera view. Defending
against such attacks is challenging due to the arbitrariness of patches, and
existing provable defenses suffer from poor certified accuracy. In this paper,
we propose PatchVeto, a zero-shot certified defense against adversarial patches
based on Vision Transformer (ViT) models. Rather than training a robust model
to resist adversarial patches which may inevitably sacrifice accuracy,
PatchVeto reuses a pretrained ViT model without any additional training, which
can achieve high accuracy on clean inputs while detecting adversarial patched
inputs by simply manipulating the attention map of ViT. Specifically, each
input is tested by voting over multiple inferences with different attention
masks, where at least one inference is guaranteed to exclude the adversarial
patch. The prediction is certifiably robust if all masked inferences reach
consensus, which ensures that any adversarial patch would be detected with no
false negative. Extensive experiments have shown that PatchVeto is able to
achieve high certified accuracy (e.g. 67.1% on ImageNet for 2%-pixel
adversarial patches), significantly outperforming state-of-the-art methods. The
clean accuracy is the same as vanilla ViT models (81.8% on ImageNet) since the
model parameters are directly reused. Meanwhile, our method can flexibly handle
different adversarial patch sizes by simply changing the masking strategy.

    

### [[2111.10484] Inter-Domain Fusion for Enhanced Intrusion Detection in Power Systems: An Evidence Theoretic and Meta-Heuristic Approach](http://arxiv.org/abs/2111.10484)


  False alerts due to misconfigured/ compromised IDS in ICS networks can lead
to severe economic and operational damage. To solve this problem, research has
focused on leveraging deep learning techniques that help reduce false alerts.
However, a shortcoming is that these works often require or implicitly assume
the physical and cyber sensors to be trustworthy. Implicit trust of data is a
major problem with using artificial intelligence or machine learning for CPS
security, because during critical attack detection time they are more at risk,
with greater likelihood and impact, of also being compromised. To address this
shortcoming, the problem is reframed on how to make good decisions given
uncertainty. Then, the decision is detection, and the uncertainty includes
whether the data used for ML-based IDS is compromised. Thus, this work presents
an approach for reducing false alerts in CPS power systems by dealing
uncertainty without the knowledge of prior distribution of alerts.
Specifically, an evidence theoretic based approach leveraging Dempster Shafer
combination rules are proposed for reducing false alerts. A multi-hypothesis
mass function model is designed that leverages probability scores obtained from
various supervised-learning classifiers. Using this model, a
location-cum-domain based fusion framework is proposed and evaluated with
different combination rules, that fuse multiple evidence from inter-domain and
intra-domain sensors. The approach is demonstrated in a cyber-physical power
system testbed with Man-In-The-Middle attack emulation in a large-scale
synthetic electric grid. For evaluating the performance, plausibility, belief,
pignistic, etc. metrics as decision functions are considered. To improve the
performance, a multi-objective based genetic algorithm is proposed for feature
selection considering the decision metrics as the fitness function.

    

### [[2111.10488] Imitation and Supervised Learning of Compliance for Robotic Assembly](http://arxiv.org/abs/2111.10488)


  We present the design of a learning-based compliance controller for assembly
operations for industrial robots. We propose a solution within the general
setting of learning from demonstration (LfD), where a nominal trajectory is
provided through demonstration by an expert teacher. This can be used to learn
a suitable representation of the skill that can be generalized to novel
positions of one of the parts involved in the assembly, for example the hole in
a peg-in-hole (PiH) insertion task. Under the expectation that this novel
position might not be entirely accurately estimated by a vision or other
sensing system, the robot will need to further modify the generated trajectory
in response to force readings measured by means of a force-torque (F/T) sensor
mounted at the wrist of the robot or another suitable location. Under the
assumption of constant velocity of traversing the reference trajectory during
assembly, we propose a novel accommodation force controller that allows the
robot to safely explore different contact configurations. The data collected
using this controller is used to train a Gaussian process model to predict the
misalignment in the position of the peg with respect to the target hole. We
show that the proposed learning-based approach can correct various contact
configurations caused by misalignment between the assembled parts in a PiH
task, achieving high success rate during insertion. We show results using an
industrial manipulator arm, and demonstrate that the proposed method can
perform adaptive insertion using force feedback from the trained machine
learning models.

    

### [[2111.10518] Towards safe, explainable, and regulated autonomous driving](http://arxiv.org/abs/2111.10518)


  There has been growing interest in the development and deployment of
autonomous vehicles on modern road networks over the last few years, encouraged
by the empirical successes of powerful artificial intelligence approaches (AI),
especially in the applications of deep and reinforcement learning. However,
there have been several road accidents with ``autonomous'' cars that prevent
this technology from being publicly acceptable at a wider level. As AI is the
main driving force behind the intelligent navigation systems of such vehicles,
both the stakeholders and transportation jurisdictions require their AI-driven
software architecture to be safe, explainable, and regulatory compliant. We
present a framework that integrates autonomous control, explainable AI
architecture, and regulatory compliance to address this issue and further
provide several conceptual models from this perspective, to help guide future
research directions.

    

### [[2111.10522] Real-World Semantic Grasping Detection](http://arxiv.org/abs/2111.10522)


  Reducing the scope of grasping detection according to the semantic
information of the target is significant to improve the accuracy of the
grasping detection model and expand its application. Researchers have been
trying to combine these capabilities in an end-to-end network to grasp specific
objects in a cluttered scene efficiently. In this paper, we propose an
end-to-end semantic grasping detection model, which can accomplish both
semantic recognition and grasping detection. And we also design a target
feature filtering mechanism, which only maintains the features of a single
object according to the semantic information for grasping detection. This
method effectively reduces the background features that are weakly correlated
to the target object, thus making the features more unique and guaranteeing the
accuracy and efficiency of grasping detection. Experimental results show that
the proposed method can achieve 98.38% accuracy in Cornell grasping dataset
Furthermore, our results on different datasets or evaluation metrics show the
domain adaptability of our method over the state-of-the-art.

    

### [[2111.10524] ACR-Pose: Adversarial Canonical Representation Reconstruction Network for Category Level 6D Object Pose Estimation](http://arxiv.org/abs/2111.10524)


  Recently, category-level 6D object pose estimation has achieved significant
improvements with the development of reconstructing canonical 3D
representations. However, the reconstruction quality of existing methods is
still far from excellent. In this paper, we propose a novel Adversarial
Canonical Representation Reconstruction Network named ACR-Pose. ACR-Pose
consists of a Reconstructor and a Discriminator. The Reconstructor is primarily
composed of two novel sub-modules: Pose-Irrelevant Module (PIM) and Relational
Reconstruction Module (RRM). PIM tends to learn canonical-related features to
make the Reconstructor insensitive to rotation and translation, while RRM
explores essential relational information between different input modalities to
generate high-quality features. Subsequently, a Discriminator is employed to
guide the Reconstructor to generate realistic canonical representations. The
Reconstructor and the Discriminator learn to optimize through adversarial
training. Experimental results on the prevalent NOCS-CAMERA and NOCS-REAL
datasets demonstrate that our method achieves state-of-the-art performance.

    

### [[2111.10545] RDF-to-Text Generation with Reinforcement Learning Based Graph-augmented Structural Neural Encoders](http://arxiv.org/abs/2111.10545)


  Considering a collection of RDF triples, the RDF-to-text generation task aims
to generate a text description. Most previous methods solve this task using a
sequence-to-sequence model or using a graph-based model to encode RDF triples
and to generate a text sequence. Nevertheless, these approaches fail to clearly
model the local and global structural information between and within RDF
triples. Moreover, the previous methods also face the non-negligible problem of
low faithfulness of the generated text, which seriously affects the overall
performance of these models. To solve these problems, we propose a model
combining two new graph-augmented structural neural encoders to jointly learn
both local and global structural information in the input RDF triples. To
further improve text faithfulness, we innovatively introduce a reinforcement
learning (RL) reward based on information extraction (IE). We first extract
triples from the generated text using a pretrained IE model and regard the
correct number of the extracted triples as the additional RL reward.
Experimental results on two benchmark datasets demonstrate that our proposed
model outperforms the state-of-the-art baselines, and the additional
reinforcement learning reward does help to improve the faithfulness of the
generated text.

    

### [[2111.10595] Quality and Computation Time in Optimization Problems](http://arxiv.org/abs/2111.10595)


  Optimization problems are crucial in artificial intelligence. Optimization
algorithms are generally used to adjust the performance of artificial
intelligence models to minimize the error of mapping inputs to outputs. Current
evaluation methods on optimization algorithms generally consider the
performance in terms of quality. However, not all optimization algorithms for
all test cases are evaluated equal from quality, the computation time should be
also considered for optimization tasks. In this paper, we investigate the
quality and computation time of optimization algorithms in optimization
problems, instead of the one-for-all evaluation of quality. We select the
well-known optimization algorithms (Bayesian optimization and evolutionary
algorithms) and evaluate them on the benchmark test functions in terms of
quality and computation time. The results show that BO is suitable to be
applied in the optimization tasks that are needed to obtain desired quality in
the limited function evaluations, and the EAs are suitable to search the
optimal of the tasks that are allowed to find the optimal solution with enough
function evaluations. This paper provides the recommendation to select suitable
optimization algorithms for optimization problems with different numbers of
function evaluations, which contributes to the efficiency that obtains the
desired quality with less computation time for optimization problems.

    

### [[2111.10627] Calculus of Consent via MARL: Legitimating the Collaborative Governance Supplying Public Goods](http://arxiv.org/abs/2111.10627)


  Public policies that supply public goods, especially those involve
collaboration by limiting individual liberty, always give rise to controversies
over governance legitimacy. Multi-Agent Reinforcement Learning (MARL) methods
are appropriate for supporting the legitimacy of the public policies that
supply public goods at the cost of individual interests. Among these policies,
the inter-regional collaborative pandemic control is a prominent example, which
has become much more important for an increasingly inter-connected world facing
a global pandemic like COVID-19. Different patterns of collaborative strategies
have been observed among different systems of regions, yet it lacks an
analytical process to reason for the legitimacy of those strategies. In this
paper, we use the inter-regional collaboration for pandemic control as an
example to demonstrate the necessity of MARL in reasoning, and thereby
legitimizing policies enforcing such inter-regional collaboration. Experimental
results in an exemplary environment show that our MARL approach is able to
demonstrate the effectiveness and necessity of restrictions on individual
liberty for collaborative supply of public goods. Different optimal policies
are learned by our MARL agents under different collaboration levels, which
change in an interpretable pattern of collaboration that helps to balance the
losses suffered by regions of different types, and consequently promotes the
overall welfare. Meanwhile, policies learned with higher collaboration levels
yield higher global rewards, which illustrates the benefit of, and thus
provides a novel justification for the legitimacy of, promoting inter-regional
collaboration. Therefore, our method shows the capability of MARL in
computationally modeling and supporting the theory of calculus of consent,
developed by Nobel Prize winner J. M. Buchanan.

    

### [[2111.10845] A hybrid optimization approach for employee rostering: Use cases at Swissgrid and lessons learned](http://arxiv.org/abs/2111.10845)


  Employee rostering is a process of assigning available employees to open
shifts. Automating it has ubiquitous practical benefits for nearly all
industries, such as reducing manual workload and producing flexible,
high-quality schedules. In this work, we develop a hybrid methodology which
combines Mixed-Integer Linear Programming (MILP) with scatter search, an
evolutionary algorithm, having as use case the optimization of employee
rostering for Swissgrid, where it is currently a largely manual process. The
hybrid methodology guarantees compliance with labor laws, maximizes employees'
preference satisfaction, and distributes workload as uniformly as possible
among them. Above all, it is shown to be a robust and efficient algorithm,
consistently solving realistic problems of varying complexity to
near-optimality an order of magnitude faster than an MILP-alone approach using
a state-of-the-art commercial solver. Several practical extensions and use
cases are presented, which are incorporated into a software tool currently
being in pilot use at Swissgrid.

    

### [[2111.10871] A Software Tool for Evaluating Unmanned Autonomous Systems](http://arxiv.org/abs/2111.10871)


  The North Carolina Agriculture and Technical State University (NC A&T) in
collaboration with Georgia Tech Research Institute (GTRI) has developed
methodologies for creating simulation-based technology tools that are capable
of inferring the perceptions and behavioral states of autonomous systems. These
methodologies have the potential to provide the Test and Evaluation (T&E)
community at the Department of Defense (DoD) with a greater insight into the
internal processes of these systems. The methodologies use only external
observations and do not require complete knowledge of the internal processing
of and/or any modifications to the system under test. This paper presents an
example of one such simulation-based technology tool, named as the Data-Driven
Intelligent Prediction Tool (DIPT). DIPT was developed for testing a
multi-platform Unmanned Aerial Vehicle (UAV) system capable of conducting
collaborative search missions. DIPT's Graphical User Interface (GUI) enables
the testers to view the aircraft's current operating state, predicts its
current target-detection status, and provides reasoning for exhibiting a
particular behavior along with an explanation of assigning a particular task to
it.

    

### [[2111.10896] Surprise Minimization Revision Operators](http://arxiv.org/abs/2111.10896)


  Prominent approaches to belief revision prescribe the adoption of a new
belief that is as close as possible to the prior belief, in a process that,
even in the standard case, can be described as attempting to minimize surprise.
Here we extend the existing model by proposing a measure of surprise, dubbed
relative surprise, in which surprise is computed with respect not just to the
prior belief, but also to the broader context provided by the new information,
using a measure derived from familiar distance notions between truth-value
assignments. We characterize the surprise minimization revision operator thus
defined using a set of intuitive rationality postulates in the AGM mould, along
the way obtaining representation results for other existing revision operators
in the literature, such as the Dalal operator and a recently introduced
distance-based min-max operator.

    

### [[2111.10961] MidNet: An Anchor-and-Angle-Free Detector for Oriented Ship Detection in Aerial Images](http://arxiv.org/abs/2111.10961)


  Ship detection in aerial images remains an active yet challenging task due to
arbitrary object orientation and complex background from a bird's-eye
perspective. Most of the existing methods rely on angular prediction or
predefined anchor boxes, making these methods highly sensitive to unstable
angular regression and excessive hyper-parameter setting. To address these
issues, we replace the angular-based object encoding with an
anchor-and-angle-free paradigm, and propose a novel detector deploying a center
and four midpoints for encoding each oriented object, namely MidNet. MidNet
designs a symmetrical deformable convolution customized for enhancing the
midpoints of ships, then the center and midpoints for an identical ship are
adaptively matched by predicting corresponding centripetal shift and matching
radius. Finally, a concise analytical geometry algorithm is proposed to refine
the centers and midpoints step-wisely for building precise oriented bounding
boxes. On two public ship detection datasets, HRSC2016 and FGSD2021, MidNet
outperforms the state-of-the-art detectors by achieving APs of 90.52% and
86.50%. Additionally, MidNet obtains competitive results in the ship detection
of DOTA.

    

### [[2111.10962] Knowledge Based Multilingual Language Model](http://arxiv.org/abs/2111.10962)


  Knowledge enriched language representation learning has shown promising
performance across various knowledge-intensive NLP tasks. However, existing
knowledge based language models are all trained with monolingual knowledge
graph data, which limits their application to more languages. In this work, we
present a novel framework to pretrain knowledge based multilingual language
models (KMLMs). We first generate a large amount of code-switched synthetic
sentences and reasoning-based multilingual training data using the Wikidata
knowledge graphs. Then based on the intra- and inter-sentence structures of the
generated data, we design pretraining tasks to facilitate knowledge learning,
which allows the language models to not only memorize the factual knowledge but
also learn useful logical patterns. Our pretrained KMLMs demonstrate
significant performance improvements on a wide range of knowledge-intensive
cross-lingual NLP tasks, including named entity recognition, factual knowledge
retrieval, relation classification, and a new task designed by us, namely,
logic reasoning. Our code and pretrained language models will be made publicly
available.

    

### [[2111.10970] Operations for Autonomous Spacecraft](http://arxiv.org/abs/2111.10970)


  Onboard autonomy technologies such as planning and scheduling, identification
of scientific targets, and content-based data summarization, will lead to
exciting new space science missions. However, the challenge of operating
missions with such onboard autonomous capabilities has not been studied to a
level of detail sufficient for consideration in mission concepts. These
autonomy capabilities will require changes to current operations processes,
practices, and tools. We have developed a case study to assess the changes
needed to enable operators and scientists to operate an autonomous spacecraft
by facilitating a common model between the ground personnel and the onboard
algorithms. We assess the new operations tools and workflows necessary to
enable operators and scientists to convey their desired intent to the
spacecraft, and to be able to reconstruct and explain the decisions made
onboard and the state of the spacecraft. Mock-ups of these tools were used in a
user study to understand the effectiveness of the processes and tools in
enabling a shared framework of understanding, and in the ability of the
operators and scientists to effectively achieve mission science objectives.

    

### [[2111.10974] Many Heads but One Brain: an Overview of Fusion Brain Challenge on AI Journey 2021](http://arxiv.org/abs/2111.10974)


  Supporting the current trend in the AI community, we propose the AI Journey
2021 Challenge called Fusion Brain which is targeted to make the universal
architecture process different modalities (namely, images, texts, and code) and
to solve multiple tasks for vision and language. The Fusion Brain Challenge
this https URL combines the following
specific tasks: Code2code Translation, Handwritten Text recognition, Zero-shot
Object Detection, and Visual Question Answering. We have created datasets for
each task to test the participants' submissions on it. Moreover, we have opened
a new handwritten dataset in both Russian and English, which consists of 94,130
pairs of images and texts. The Russian part of the dataset is the largest
Russian handwritten dataset in the world. We also propose the baseline solution
and corresponding task-specific solutions as well as overall metrics.

    

### [[2111.10991] Backdoor Attack through Frequency Domain](http://arxiv.org/abs/2111.10991)


  Backdoor attacks have been shown to be a serious threat against deep learning
systems such as biometric authentication and autonomous driving. An effective
backdoor attack could enforce the model misbehave under certain predefined
conditions, i.e., triggers, but behave normally otherwise. However, the
triggers of existing attacks are directly injected in the pixel space, which
tend to be detectable by existing defenses and visually identifiable at both
training and inference stages. In this paper, we propose a new backdoor attack
FTROJAN through trojaning the frequency domain. The key intuition is that
triggering perturbations in the frequency domain correspond to small pixel-wise
perturbations dispersed across the entire image, breaking the underlying
assumptions of existing defenses and making the poisoning images visually
indistinguishable from clean ones. We evaluate FTROJAN in several datasets and
tasks showing that it achieves a high attack success rate without significantly
degrading the prediction accuracy on benign inputs. Moreover, the poisoning
images are nearly invisible and retain high perceptual quality. We also
evaluate FTROJAN against state-of-the-art defenses as well as several adaptive
defenses that are designed on the frequency domain. The results show that
FTROJAN can robustly elude or significantly degenerate the performance of these
defenses.

    

### [[2111.11011] CDistNet: Perceiving Multi-Domain Character Distance for Robust Text Recognition](http://arxiv.org/abs/2111.11011)


  The attention-based encoder-decoder framework is becoming popular in scene
text recognition, largely due to its superiority in integrating recognition
clues from both visual and semantic domains. However, recent studies show the
two clues might be misaligned in the difficult text (e.g., with rare text
shapes) and introduce constraints such as character position to alleviate the
problem. Despite certain success, a content-free positional embedding hardly
associates with meaningful local image regions stably. In this paper, we
propose a novel module called Multi-Domain Character Distance Perception
(MDCDP) to establish a visual and semantic related position encoding. MDCDP
uses positional embedding to query both visual and semantic features following
the attention mechanism. It naturally encodes the positional clue, which
describes both visual and semantic distances among characters. We develop a
novel architecture named CDistNet that stacks MDCDP several times to guide
precise distance modeling. Thus, the visual-semantic alignment is well built
even various difficulties presented. We apply CDistNet to two augmented
datasets and six public benchmarks. The experiments demonstrate that CDistNet
achieves state-of-the-art recognition accuracy. While the visualization also
shows that CDistNet achieves proper attention localization in both visual and
semantic domains. We will release our code upon acceptance.

    

### [[2111.11026] Learning Explicit User Interest Boundary for Recommendation](http://arxiv.org/abs/2111.11026)


  The core objective of modelling recommender systems from implicit feedback is
to maximize the positive sample score $s_p$ and minimize the negative sample
score $s_n$, which can usually be summarized into two paradigms: the pointwise
and the pairwise. The pointwise approaches fit each sample with its label
individually, which is flexible in weighting and sampling on instance-level but
ignores the inherent ranking property. By qualitatively minimizing the relative
score $s_n - s_p$, the pairwise approaches capture the ranking of samples
naturally but suffer from training efficiency. Additionally, both approaches
are hard to explicitly provide a personalized decision boundary to determine if
users are interested in items unseen. To address those issues, we innovatively
introduce an auxiliary score $b_u$ for each user to represent the User Interest
Boundary(UIB) and individually penalize samples that cross the boundary with
pairwise paradigms, i.e., the positive samples whose score is lower than $b_u$
and the negative samples whose score is higher than $b_u$. In this way, our
approach successfully achieves a hybrid loss of the pointwise and the pairwise
to combine the advantages of both. Analytically, we show that our approach can
provide a personalized decision boundary and significantly improve the training
efficiency without any special sampling strategy. Extensive results show that
our approach achieves significant improvements on not only the classical
pointwise or pairwise models but also state-of-the-art models with complex loss
function and complicated feature encoding.

    

### [[2111.11089] Monocular Road Planar Parallax Estimation](http://arxiv.org/abs/2111.11089)


  Estimating the 3D structure of the drivable surface and surrounding
environment is a crucial task for assisted and autonomous driving. It is
commonly solved either by using expensive 3D sensors such as LiDAR or directly
predicting the depth of points via deep learning. Instead of following existing
methodologies, we propose Road Planar Parallax Attention Network (RPANet), a
new deep neural network for 3D sensing from monocular image sequences based on
planar parallax, which takes full advantage of the commonly seen road plane
geometry in driving scenes. RPANet takes a pair of images aligned by the
homography of the road plane as input and outputs a $\gamma$ map for 3D
reconstruction. Beyond estimating the depth or height, the $\gamma$ map has a
potential to construct a two-dimensional transformation between two consecutive
frames while can be easily derived to depth or height. By warping the
consecutive frames using the road plane as a reference, the 3D structure can be
estimated from the planar parallax and the residual image displacements.
Furthermore, to make the network better perceive the displacements caused by
planar parallax, we introduce a novel cross-attention module. We sample data
from the Waymo Open Dataset and construct data related to planar parallax.
Comprehensive experiments are conducted on the sampled dataset to demonstrate
the 3D reconstruction accuracy of our approach in challenging scenarios.

    

### [[2111.11099] Talk-to-Resolve: Combining scene understanding and spatial dialogue to resolve granular task ambiguity for a collocated robot](http://arxiv.org/abs/2111.11099)


  The utility of collocating robots largely depends on the easy and intuitive
interaction mechanism with the human. If a robot accepts task instruction in
natural language, first, it has to understand the user's intention by decoding
the instruction. However, while executing the task, the robot may face
unforeseeable circumstances due to the variations in the observed scene and
therefore requires further user intervention. In this article, we present a
system called Talk-to-Resolve (TTR) that enables a robot to initiate a coherent
dialogue exchange with the instructor by observing the scene visually to
resolve the impasse. Through dialogue, it either finds a cue to move forward in
the original plan, an acceptable alternative to the original plan, or
affirmation to abort the task altogether. To realize the possible stalemate, we
utilize the dense captions of the observed scene and the given instruction
jointly to compute the robot's next action. We evaluate our system based on a
data set of initial instruction and situational scene pairs. Our system can
identify the stalemate and resolve them with appropriate dialogue exchange with
82% accuracy. Additionally, a user study reveals that the questions from our
systems are more natural (4.02 on average on a scale of 1 to 5) as compared to
a state-of-the-art (3.08 on average).

    

### [[2111.11104] Hierarchy Decoder is All You Need To Text Classification](http://arxiv.org/abs/2111.11104)


  Hierarchical text classification (HTC) to a taxonomy is essential for various
real applications butchallenging since HTC models often need to process a large
volume of data that are severelyimbalanced and have hierarchy dependencies.
Existing local and global approaches use deep learningto improve HTC by
reducing the time complexity and incorporating the hierarchy
dependencies.However, it is difficult to satisfy both conditions in a single
HTC model. This paper proposes ahierarchy decoder (HiDEC) that uses recursive
hierarchy decoding based on an encoder-decoderarchitecture. The key idea of the
HiDEC involves decoding a context matrix into a sub-hierarchysequence using
recursive hierarchy decoding, while staying aware of hierarchical
dependenciesand level information. The HiDEC is a unified model that
incorporates the benefits of existingapproaches, thereby alleviating the
aforementioned difficulties without any trade-off. In addition, itcan be
applied to both single- and multi-label classification with a minor
modification. The superiorityof the proposed model was verified on two
benchmark datasets (WOS-46985 and RCV1) with anexplanation of the reasons for
its success

    

### [[2111.11107] Branching Time Active Inference: the theory and its generality](http://arxiv.org/abs/2111.11107)


  Over the last 10 to 15 years, active inference has helped to explain various
brain mechanisms from habit formation to dopaminergic discharge and even
modelling curiosity. However, the current implementations suffer from an
exponential (space and time) complexity class when computing the prior over all
the possible policies up to the time-horizon. Fountas et al (2020) used Monte
Carlo tree search to address this problem, leading to impressive results in two
different tasks. In this paper, we present an alternative framework that aims
to unify tree search and active inference by casting planning as a structure
learning problem. Two tree search algorithms are then presented. The first
propagates the expected free energy forward in time (i.e., towards the leaves),
while the second propagates it backward (i.e., towards the root). Then, we
demonstrate that forward and backward propagations are related to active
inference and sophisticated inference, respectively, thereby clarifying the
differences between those two planning strategies.

    

### [[2104.14907] Deep Learning Based Steel Pipe Weld Defect Detection](http://arxiv.org/abs/2104.14907)


  Steel pipes are widely used in high-risk and high-pressure scenarios such as
oil, chemical, natural gas, shale gas, etc. If there is some defect in steel
pipes, it will lead to serious adverse consequences. Applying object detection
in the field of deep learning to pipe weld defect detection and identification
can effectively improve inspection efficiency and promote the development of
industrial automation. Most predecessors used traditional computer vision
methods applied to detect defects of steel pipe weld seams. However,
traditional computer vision methods rely on prior knowledge and can only detect
defects with a single feature, so it is difficult to complete the task of
multi-defect classification, while deep learning is end-to-end. In this paper,
the state-of-the-art single-stage object detection algorithm YOLOv5 is proposed
to be applied to the field of steel pipe weld defect detection, and compared
with the two-stage representative object detection algorithm Faster R-CNN. The
experimental results show that applying YOLOv5 to steel pipe weld defect
detection can greatly improve the accuracy, complete the multi-classification
task, and meet the criteria of real-time detection.

    

### [[2106.15202] Inconspicuous Adversarial Patches for Fooling Image Recognition Systems on Mobile Devices](http://arxiv.org/abs/2106.15202)


  Deep learning based image recognition systems have been widely deployed on
mobile devices in today's world. In recent studies, however, deep learning
models are shown vulnerable to adversarial examples. One variant of adversarial
examples, called adversarial patch, draws researchers' attention due to its
strong attack abilities. Though adversarial patches achieve high attack success
rates, they are easily being detected because of the visual inconsistency
between the patches and the original images. Besides, it usually requires a
large amount of data for adversarial patch generation in the literature, which
is computationally expensive and time-consuming. To tackle these challenges, we
propose an approach to generate inconspicuous adversarial patches with one
single image. In our approach, we first decide the patch locations basing on
the perceptual sensitivity of victim models, then produce adversarial patches
in a coarse-to-fine way by utilizing multiple-scale generators and
discriminators. The patches are encouraged to be consistent with the background
images with adversarial training while preserving strong attack abilities. Our
approach shows the strong attack abilities in white-box settings and the
excellent transferability in black-box settings through extensive experiments
on various models with different architectures and training methods. Compared
to other adversarial patches, our adversarial patches hold the most negligible
risks to be detected and can evade human observations, which is supported by
the illustrations of saliency maps and results of user evaluations. Lastly, we
show that our adversarial patches can be applied in the physical world.

    

### [[2111.10703] The Gittins Policy in the M/G/1 Queue](http://arxiv.org/abs/2111.10703)


  The Gittins policy is a highly general scheduling policy that minimizes a
wide variety of mean holding cost metrics in the M/G/1 queue. Perhaps most
famously, Gittins minimizes mean response time in the M/G/1 when jobs' service
times are unknown to the scheduler. Gittins also minimizes weighted versions of
mean response time. For example, the well-known "$c\mu$ rule", which minimizes
class-weighted mean response time in the multiclass M/M/1, is a special case of
Gittins.
However, despite the extensive literature on Gittins in the M/G/1, it
contains no fully general proof of Gittins's optimality. This is because
Gittins was originally developed for the multi-armed bandit problem.
Translating arguments from the multi-armed bandit to the M/G/1 is technically
demanding, so it has only been done rigorously in some special cases. The
extent of Gittins's optimality in the M/G/1 is thus not entirely clear.
In this work we provide the first fully general proof of Gittins's optimality
in the M/G/1. The optimality result we obtain is even more general than was
previously known. For example, we show that Gittins minimizes mean slowdown in
the M/G/1 with unknown or partially known service times, and we show that
Gittins's optimality holds under batch arrivals. Our proof uses a novel
approach that works directly with the M/G/1, avoiding the difficulties of
translating from the multi-armed bandit problem.

    

### [[2111.10411] A Transient Semantics for Typed Racket](http://arxiv.org/abs/2111.10411)


  Mixed-typed languages enable programmers to link typed and untyped components
in various ways. Some offer rich type systems to facilitate the smooth
migration of untyped code to the typed world; others merely provide a
convenient form of type Dynamic together with a conventional structural type
system. Orthogonal to this dimension, Natural systems ensure the integrity of
types with a sophisticated contract system, while Transient systems insert
simple first-order checks at strategic places within typed code. Furthermore,
each method of ensuring type integrity comes with its own blame-assignment
strategy.
Typed Racket has a rich migratory type system and enforces the types with a
Natural semantics. Reticulated Python has a simple structural type system
extended with Dynamic and enforces types with a Transient semantics. While
Typed Racket satisfies the most stringent gradual-type soundness properties at
a significant performance cost, Reticulated Python seems to limit the
performance penalty to a tolerable degree and is nevertheless type sound. This
comparison raises the question of whether Transient checking is applicable to
and beneficial for a rich migratory type system.
This paper reports on the surprising difficulties of adapting the Transient
semantics of Reticulated Python to the rich migratory type system of Typed
Racket. The resulting implementation, Shallow Typed Racket, is faster than the
standard Deep Typed Racket but only when the Transient blame assignment
strategy is disabled. For language designers, this report provides valuable
hints on how to equip an existing compiler to support a Transient semantics.
For theoreticians, the negative experience with Transient blame calls for a
thorough investigation of this strategy.

    

### [[2111.10412] Types for Tables: A Language Design Benchmark](http://arxiv.org/abs/2111.10412)


  Context: Tables are ubiquitous formats for data. Therefore, techniques for
writing correct programs over tables, and debugging incorrect ones, are vital.
Our specific focus in this paper is on rich types that articulate the
properties of tabular operations. We wish to study both their expressive power
and _diagnostic quality_.
Inquiry: There is no "standard library" of table operations. As a result,
every paper (and project) is free to use its own (sub)set of operations. This
makes artifacts very difficult to compare, and it can be hard to tell whether
omitted operations were left out by oversight or because they cannot actually
be expressed. Furthermore, virtually no papers discuss the quality of type
error feedback.
Approach: We combed through several existing languages and libraries to
create a "standard library" of table operations. Each entry is accompanied by a
detailed specification of its "type," expressed independent of (and hence not
constrained by) any type language. We also studied and categorized a corpus of
(student) program edits that resulted in table-related errors. We used this to
generate a suite of erroneous programs. Finally, we adapted the concept of a
datasheet to facilitate comparisons of different implementations.
Knowledge: Our benchmark creates a common ground to frame work in this area.
Language designers who claim to support typed programming over tables have a
clear suite against which to demonstrate their system's expressive power. Our
family of errors also gives them a chance to demonstrate the quality of
feedback. Researchers who improve one aspect -- especially error reporting --
without changing the other can demonstrate their improvement, as can those who
engage in trade-offs between the two. The net result should be much better
science in both expressiveness and diagnostics. We also introduce a datasheet
format for presenting this knowledge in a methodical way.
Grounding: We have generated our benchmark from real languages, libraries,
and programs, as well as personal experience conducting and teaching data
science. We have drawn on experience in engineering and, more recently, in data
science to generate the datasheet.
Importance: Claims about type support for tabular programming are hard to
evaluate. However, tabular programming is ubiquitous, and the expressive power
of type systems keeps growing. Our benchmark and datasheet can help lead to
more orderly science. It also benefits programmers trying to choose a language.

    

### [[2111.10413] Continuation-Passing Style, Defunctionalization, Accumulations, and Associativity](http://arxiv.org/abs/2111.10413)


  Context: Reynolds showed us how to use continuation-passing style and
defunctionalization to transform a recursive interpreter for a language into an
abstract machine for programs in that language. The same techniques explain
other programming tricks, including zippers and accumulating parameters.
Inquiry: Buried within all those applications there is usually a hidden appeal
to the algebraic property of associativity. Approach: Our purpose in this paper
is to entice associativity out of the shadows and into the limelight.
Knowledge: We revisit some well-known applications (factorial, fast reverse,
tree flattening, and a compiler for a simple expression language) to spotlight
their dependence on associativity. Grounding: We replay developments of these
programs through a series of program transformations and data refinements,
justified by equational reasoning. Importance: Understanding the crucial role
played by associativity clarifies when continuation-passing style and
defunctionalization can help and when they cannot, and may prompt other
applications of these techniques.

    

### [[2111.10414] Automated, Targeted Testing of Property-Based Testing Predicates](http://arxiv.org/abs/2111.10414)


  Context: This work is based on property-based testing (PBT). PBT is an
increasingly important form of software testing. Furthermore, it serves as a
concrete gateway into the abstract area of formal methods. Specifically, we
focus on students learning PBT methods.
Inquiry: How well do students do at PBT? Our goal is to assess the quality of
the predicates they write as part of PBT. Prior work introduced the idea of
decomposing the predicate's property into a conjunction of independent
subproperties. Testing the predicate against each subproperty gives a
"semantic" understanding of their performance.
Approach: The notion of independence of subproperties both seems intuitive
and was an important condition in prior work. First, we show that this
condition is overly restrictive and might hide valuable information: it both
undercounts errors and makes it hard to capture misconceptions. Second, we
introduce two forms of automation, one based on PBT tools and the other on
SAT-solving, to enable testing of student predicates. Third, we compare the
output of these automated tools against manually-constructed tests. Fourth, we
also measure the performance of those tools. Finally, we re-assess student
performance reported in prior work.
Knowledge: We show the difficulty caused by the independent subproperty
requirement. We provide insight into how to use automation effectively to
assess PBT predicates. In particular, we discuss the steps we had to take to
beat human performance. We also provide insight into how to make the automation
work efficiently. Finally, we present a much richer account than prior work of
how students did.
Grounding: Our methods are grounded in mathematical logic. We also make use
of well-understood principles of test generation from more formal
specifications. This combination ensures the soundness of our work. We use
standard methods to measure performance.
Importance: As both educators and programmers, we believe PBT is a valuable
tool for students to learn, and its importance will only grow as more
developers appreciate its value. Effective teaching requires a clear
understanding of student knowledge and progress. Our methods enable a rich and
automated analysis of student performance on PBT that yields insight into their
understanding and can capture misconceptions. We therefore expect these results
to be valuable to educators.

    

### [[2111.10589] Freeing Compute Caches from Serialization and Garbage Collection in Managed Big Data Analytics](http://arxiv.org/abs/2111.10589)


  Managed analytics frameworks (e.g., Spark) cache intermediate results in
memory (on-heap) or storage devices (off-heap) to avoid costly recomputations,
especially in graph processing. As datasets grow, on-heap caching requires more
memory for long-lived objects, resulting in high garbage collection (GC)
overhead. On the other hand, off-heap caching moves cached objects on the
storage device, reducing GC overhead, but at the cost of serialization and
deserialization (S/D). In this work, we propose TeraHeap, a novel approach for
providing large analytics caches. TeraHeap uses two heaps within the JVM (1) a
garbage-collected heap for ordinary Spark objects and (2) a large heap
memory-mapped over fast storage devices for cached objects. TeraHeap eliminates
both S/D and GC over cached data without imposing any language restrictions. We
implement TeraHeap in Oracle's Java runtime (OpenJDK-1.8). We use five popular,
memory-intensive graph analytics workloads to understand S/D and GC overheads
and evaluate TeraHeap. TeraHeap improves total execution time compared to
state-of-the-art Apache Spark configurations by up to 72% and 81% for NVMe SSD
and non-volatile memory, respectively. Furthermore, TeraCache requires 8x less
DRAM capacity to provide performance comparable or higher than native Spark.
This paper opens up emerging memory and storage devices for practical use in
scalable analytics caching.

    

### [[2111.10867] Qimaera: Type-safe (Variational) Quantum Programming in Idris](http://arxiv.org/abs/2111.10867)


  Variational Quantum Algorithms are hybrid classical-quantum algorithms where
classical and quantum computation work in tandem to solve computational
problems. These algorithms create interesting challenges for the design of
suitable programming languages. In this paper we introduce Qimaera, which is a
set of libraries for the Idris 2 programming language that enable the
programmer to implement (variational) quantum algorithms where the full power
of the elegant Idris language works in synchrony with quantum programming
primitives that we introduce. The two key ingredients of Idris that make this
possible are (1) dependent types which allow us to implement unitary (i.e.
reversible and controllable) quantum operations; and (2) linearity which allows
us to enforce fine-grained control over the execution of quantum operations
that ensures compliance with the laws of quantum mechanics. We demonstrate that
Qimaera is suitable for variational quantum programming by providing
implementations of the two most prominent variational quantum algorithms --
QAOA and VQE. To the best of our knowledge, this is the first implementation of
these algorithms that has been achieved in a type-safe framework.

    

### [<title>Xgb.shap.data not found - XGBoost</title>](https://discuss.xgboost.ai/t/xgb-shap-data-not-found/2557/3)

### [<title>Xgb.shap.data not found - XGBoost</title>](https://discuss.xgboost.ai/t/xgb-shap-data-not-found/2557/2)

### [<title>What are the tradeoffs associated with the various XGBoost interfaces? - XGBoost</title>](https://discuss.xgboost.ai/t/what-are-the-tradeoffs-associated-with-the-various-xgboost-interfaces/2555/2)

### [[2111.10818] New Binary-Addition Tree Algorithm for the All-Multiterminal Binary-State Network Reliability Problem](http://arxiv.org/abs/2111.10818)


  Various real-life applications, for example, Internet of Things, wireless
sensor networks, smart grids, transportation networks, communication networks,
social networks, and computer grid systems, are always modeled as network
structures. The binary-state network composed of binary-state (e.g.,
functioning or failed) components (arcs and/or nodes) is one of the most
popular network structures. The two-terminal network reliability is a success
probability that the network is still functioning and can be calculated by
verifying the connectivity between two specific nodes, and is an effective and
popular technique for evaluating the performance of all types of networks. To
obtain complete information for a making better decisions, a multi-terminal
network reliability extends the two specific nodes to a specific node subset in
which all nodes are connected. In this study, a new algorithm called the
all-multiterminal BAT is proposed by revising the binary-addition-tree
algorithm (BAT) and the layered-search algorithm (LSA) to calculate all
multi-terminal reliabilities. The efficiency and effectiveness of the proposed
all-multiterminal BAT are analyzed from the perspective of time complexity and
explained via numerical experiments to solve the all-multiterminal network
reliability problems.

    

### [<title>Xgb.shap.data not found - XGBoost</title>](https://discuss.xgboost.ai/t/xgb-shap-data-not-found/2557/6)

### [<title>Xgb.shap.data not found - XGBoost</title>](https://discuss.xgboost.ai/t/xgb-shap-data-not-found/2557/5)

### [<title>Xgb.shap.data not found - XGBoost</title>](https://discuss.xgboost.ai/t/xgb-shap-data-not-found/2557/4)