
## 2021-9-14

### [<title>关于惠州哪里能开具餐饮费电子发票-惠州本地宝 - DockOne.io</title>](http://dockone.io/question/1312666)

### [<title>关于佛山哪里能开具餐饮费电子发票-佛山本地宝 - DockOne.io</title>](http://dockone.io/question/1312665)

### [<title>关于深圳哪里能开具实体店餐饮电子发票-开票服务大厅-深圳本地宝 - DockOne.io</title>](http://dockone.io/question/1312664)

### [<title>关于北京哪里能开具实体店餐饮电子发票-开票服务大厅-北京本地宝 - DockOne.io</title>](http://dockone.io/question/1312663)

### [<title>关于温州哪里能开具住宿费电子版发票-开票服务大厅-百度经验 - DockOne.io</title>](http://dockone.io/question/1312662)

### [<title>关于昆明哪里能开具住宿费电子版发票-开票服务大厅-百度经验 - DockOne.io</title>](http://dockone.io/question/1312661)

### [<title>关于上海哪里能开具实体店餐饮电子发票-开票服务大厅-上海本地宝 - DockOne.io</title>](http://dockone.io/question/1312660)

### [<title>关于东莞哪里能开具餐饮费电子发票-东莞本地宝 - DockOne.io</title>](http://dockone.io/question/1312659)

### [<title>关于青岛哪里能开具餐饮费电子发票-青岛本地宝 - DockOne.io</title>](http://dockone.io/question/1312658)

### [<title>关于哪里能开具实体店餐饮电子发票-开票服务大厅-本地宝 - DockOne.io</title>](http://dockone.io/question/1312657)

### [<title>关于南昌哪里能开具住宿费电子版发票-开票服务大厅-百度经验 - DockOne.io</title>](http://dockone.io/question/1312656)

### [<title>关于南宁哪里能开具餐饮费电子发票-南宁本地宝 - DockOne.io</title>](http://dockone.io/question/1312655)

### [<title>关于呼和浩特哪里能开具实体店餐饮费发票-开票服务大厅-呼和浩特本地宝 - DockOne.io</title>](http://dockone.io/question/1312654)

### [<title>关于厦门哪里能开具餐饮费电子发票-厦门本地宝 - DockOne.io</title>](http://dockone.io/question/1312653)

### [<title>关于福州哪里能开具住宿费电子版发票-开票服务大厅-百度经验 - DockOne.io</title>](http://dockone.io/question/1312652)

### [<title>关于温州哪里能开具实体店餐饮费发票-开票服务大厅-温州本地宝 - DockOne.io</title>](http://dockone.io/question/1312651)

### [<title>关于惠州哪里能开具住宿费电子版发票-开票服务大厅-百度经验 - DockOne.io</title>](http://dockone.io/question/1312650)

### [<title>关于贵阳哪里能开具餐饮费电子发票-贵阳本地宝 - DockOne.io</title>](http://dockone.io/question/1312649)

### [<title>关于昆明哪里能开具实体店餐饮费发票-开票服务大厅-昆明本地宝 - DockOne.io</title>](http://dockone.io/question/1312648)

### [<title>关于南昌哪里能开具实体店餐饮费发票-开票服务大厅-南昌本地宝 - DockOne.io</title>](http://dockone.io/question/1312647)

### [<title>When will GPU pre-built be available for R on Win 64? - XGBoost</title>](https://discuss.xgboost.ai/t/when-will-gpu-pre-built-be-available-for-r-on-win-64/2463/4)

### [<title>When will GPU pre-built be available for R on Win 64? - XGBoost</title>](https://discuss.xgboost.ai/t/when-will-gpu-pre-built-be-available-for-r-on-win-64/2463/3)

### [<title>When will GPU pre-built be available for R on Win 64? - XGBoost</title>](https://discuss.xgboost.ai/t/when-will-gpu-pre-built-be-available-for-r-on-win-64/2463/2)

### [<title>When will GPU pre-built be available for R on Win 64? - XGBoost</title>](https://discuss.xgboost.ai/t/when-will-gpu-pre-built-be-available-for-r-on-win-64/2463/1)

### [[2109.05103] No Size Fits All: Automated Radio Configuration for LPWANs](http://arxiv.org/abs/2109.05103)


  Low power long-range networks like LoRa have become increasingly mainstream
for Internet of Things deployments. Given the versatility of applications that
these protocols enable, they support many data rates and bandwidths. Yet, for a
given network that supports hundreds of devices over multiple miles, the
network operator typically needs to specify the same configuration or among a
small subset of configurations for all the client devices to communicate with
the gateway. This one-size-fits-all approach is highly inefficient in large
networks. We propose an alternative approach -- we allow network devices to
transmit at any data rate they choose. The gateway uses the first few symbols
in the preamble to classify the correct data rate, switches its configuration,
and then decodes the data. Our design leverages the inherent asymmetry in
outdoor IoT deployments where the clients are power-starved and
resource-constrained, but the gateway is not. Our gateway design, Proteus, runs
a neural network architecture and is backward compatible with existing LoRa
protocols. Our experiments reveal that Proteus can identify the correct
configuration with over 97% accuracy in both indoor and outdoor deployments.
Our network architecture leads to a 3.8 to 11 times increase in throughput for
our LoRa testbed.

    

### [[2109.05193] ECCR: Edge-Cloud Collaborative Recovery for Low-Power Wide-Area Networks interference mitigation](http://arxiv.org/abs/2109.05193)


  Recent advances in Low-Power Wide-Area Networks have mitigated interference
by using cloud assistance. Those methods transmit the RSSI samples and
corrupted packets to the cloud to restore the correct message. However, the
effectiveness of those methods is challenged by the high transmission data
amount. This paper presents a novel method for interference mitigation in a
Edge-Cloud collaborative manner, namely ECCR. It does not require transmitting
RSSI sample any more, whose length is eight times of the packet's. We
demonstrate the disjointness of the bit errors of packets at the base stations
via real-word experiments. ECCR leverages this to collaborate with multiple
base stations for error recovery. Each base station detects and reports bit
error locations to the cloud, then both error checking code and interfered
packets from other receivers are utilized to restore correct packets. ECCR
takes the advantages of both the global management ability of the cloud and the
signal to perceive the benefit of each base station, and it is applicable to
deployed LP-WAN systems (e.g. sx1280) without any extra hardware requirement.
Experimental results show that ECCR is able to accurately decode packets when
packets have nearly 51.76% corruption.

    

### [[2109.05411] Cost-Effective Federated Learning in Mobile Edge Networks](http://arxiv.org/abs/2109.05411)


  Federated learning (FL) is a distributed learning paradigm that enables a
large number of mobile devices to collaboratively learn a model under the
coordination of a central server without sharing their raw data. Despite its
practical efficiency and effectiveness, the iterative on-device learning
process (e.g., local computations and global communications with the server)
incurs a considerable cost in terms of learning time and energy consumption,
which depends crucially on the number of selected clients and the number of
local iterations in each training round. In this paper, we analyze how to
design adaptive FL in mobile edge networks that optimally chooses these
essential control variables to minimize the total cost while ensuring
convergence. We establish the analytical relationship between the total cost
and the control variables with the convergence upper bound. To efficiently
solve the cost minimization problem, we develop a low-cost sampling-based
algorithm to learn the convergence related unknown parameters. We derive
important solution properties that effectively identify the design principles
for different optimization metrics. Practically, we evaluate our theoretical
results both in a simulated environment and on a hardware prototype.
Experimental evidence verifies our derived properties and demonstrates that our
proposed solution achieves near-optimal performance for different optimization
metrics for various datasets and heterogeneous system and statistical settings.

    

### [[1903.10685] Protocols for Packet Quantum Network Intercommunication](http://arxiv.org/abs/1903.10685)


  A quantum network, which involves multiple parties pinging each other with
quantum messages, could revolutionize communication, computing and basic
sciences. The future internet will be a global system of various packet
switching quantum and classical networks and we call it \emph{quantum
internet}. To build a quantum internet, unified protocols that support the
distribution of quantum messages within it are necessary. Intuitively one would
extend classical internet protocols to handle quantum messages. However,
classical network mechanisms, especially those related to error control and
reliable connection, implicitly assume that information can be duplicated,
which is not true in the quantum world due to the no-cloning theorem and
monogamy of entanglement. In this paper, we investigate and propose protocols
for packet quantum network intercommunication. To handle the packet loss
problem in transport, we propose a quantum retransmission protocol based on the
recursive use of a quantum secret sharing scheme. Other internet protocols are
also discussed. In particular, the creation of logical process-to-process
connections is accomplished by a quantum version of the three-way handshake
protocol.

    

### [[2004.00365] An Outlook on the Interplay of Machine Learning and Reconfigurable Intelligent Surfaces: An Overview of Opportunities and Limitations](http://arxiv.org/abs/2004.00365)


  Recent advances in programmable metasurfaces, also dubbed as reconfigurable
intelligent surfaces (RISs), are envisioned to offer a paradigm shift from
uncontrollable to fully tunable and customizable wireless propagation
environments, enabling a plethora of new applications and technological trends.
Therefore, in view of this cutting edge technological concept, we first review
the architecture and electromagnetic waves manipulation functionalities of
RISs. We then detail some of the recent advancements that have been made
towards realizing these programmable functionalities in wireless communication
applications. Furthermore, we elaborate on how machine learning (ML) can
address various constraints introduced by the real-time deployment of RISs,
particularly in terms of latency, storage, energy efficiency, and computation.
A review of the state-of-the-art research on the integration of ML with RISs is
presented, highlighting their potentials as well as challenges. Finally, the
paper concludes by offering a look ahead towards unexplored possibilities of ML
mechanisms in the context of RISs.

    

### [[2004.00799] Cost-efficient and Skew-aware Data Scheduling for Incremental Learning in 5G Network](http://arxiv.org/abs/2004.00799)


  To facilitate the emerging applications in 5G networks, mobile network
operators will provide many network functions in terms of control and
prediction. Recently, they have recognized the power of machine learning (ML)
and started to explore its potential to facilitate those network functions.
Nevertheless, the current ML models for network functions are often derived in
an offline manner, which is inefficient due to the excessive overhead for
transmitting a huge volume of dataset to remote ML training clouds and failing
to provide the incremental learning capability for the continuous model
updating. As an alternative solution, we propose Cocktail, an incremental
learning framework within a reference 5G network architecture. To achieve cost
efficiency while increasing trained model accuracy, an efficient online data
scheduling policy is essential. To this end, we formulate an online data
scheduling problem to optimize the framework cost while alleviating the data
skew issue caused by the capacity heterogeneity of training workers from the
long-term perspective. We exploit the stochastic gradient descent to devise an
online asymptotically optimal algorithm, including two optimal policies based
on novel graph constructions for skew-aware data collection and data training.
Small-scale testbed and large-scale simulations validate the superior
performance of our proposed framework.

    

### [[2011.06304] Machine Learning Interpretability Meets TLS Fingerprinting](http://arxiv.org/abs/2011.06304)


  Protecting users' privacy over the Internet is of great importance; however,
it becomes harder and harder to maintain due to the increasing complexity of
network protocols and components. Therefore, investigating and understanding
how data is leaked from the information transmission platforms and protocols
can lead us to a more secure environment.
In this paper, we propose a framework to systematically find the most
vulnerable information fields in a network protocol. To this end, focusing on
the transport layer security (TLS) protocol, we perform different
machine-learning-based fingerprinting attacks on the collected data from more
than 70 domains (websites) to understand how and where this information leakage
occurs in the TLS protocol. Then, by employing the interpretation techniques
developed in the machine learning community and applying our framework, we find
the most vulnerable information fields in the TLS protocol. Our findings
demonstrate that the TLS handshake (which is mainly unencrypted), the TLS
record length appearing in the TLS application data header, and the
initialization vector (IV) field are among the most critical leaker parts in
this protocol, respectively.

    

### [[2105.15096] Small-Scale Spatial-Temporal Correlation and Degrees of Freedom for Reconfigurable Intelligent Surfaces](http://arxiv.org/abs/2105.15096)


  The reconfigurable intelligent surface (RIS) is an emerging promising
candidate technology for future wireless networks, where the element spacing is
usually of sub-wavelength. Only limited knowledge, however, has been gained
about the spatial-temporal correlation behavior among the elements in an RIS.
In this paper, we investigate the spatial-temporal correlation for an
RIS-enabled wireless communication system. Specifically, a joint small-scale
spatial-temporal correlation model is derived under isotropic scattering, which
can be represented by a four-dimensional sinc function. Furthermore, based upon
the spatial-only correlation at a certain time instant, an essential RIS
property -- the spatial degrees of freedom (DoF) -- is revisited, and an
analytical expression is propounded to characterize the spatial DoF for RISs
with realistic hence non-infinitesimal element spacing and finite aperture
sizes. The results are vital to the accurate evaluation of various system
performance metrics.

    

### [[2109.05022] Potential-based Reward Shaping in Sokoban](http://arxiv.org/abs/2109.05022)


  Learning to solve sparse-reward reinforcement learning problems is difficult,
due to the lack of guidance towards the goal. But in some problems, prior
knowledge can be used to augment the learning process. Reward shaping is a way
to incorporate prior knowledge into the original reward function in order to
speed up the learning. While previous work has investigated the use of expert
knowledge to generate potential functions, in this work, we study whether we
can use a search algorithm(A*) to automatically generate a potential function
for reward shaping in Sokoban, a well-known planning task. The results showed
that learning with shaped reward function is faster than learning from scratch.
Our results indicate that distance functions could be a suitable function for
Sokoban. This work demonstrates the possibility of solving multiple instances
with the help of reward shaping. The result can be compressed into a single
policy, which can be seen as the first phrase towards training a general policy
that is able to solve unseen instances.

    

### [[2109.05023] Real-time multimodal image registration with partial intraoperative point-set data](http://arxiv.org/abs/2109.05023)


  We present Free Point Transformer (FPT) - a deep neural network architecture
for non-rigid point-set registration. Consisting of two modules, a global
feature extraction module and a point transformation module, FPT does not
assume explicit constraints based on point vicinity, thereby overcoming a
common requirement of previous learning-based point-set registration methods.
FPT is designed to accept unordered and unstructured point-sets with a variable
number of points and uses a "model-free" approach without heuristic
constraints. Training FPT is flexible and involves minimizing an intuitive
unsupervised loss function, but supervised, semi-supervised, and partially- or
weakly-supervised training are also supported. This flexibility makes FPT
amenable to multimodal image registration problems where the ground-truth
deformations are difficult or impossible to measure. In this paper, we
demonstrate the application of FPT to non-rigid registration of prostate
magnetic resonance (MR) imaging and sparsely-sampled transrectal ultrasound
(TRUS) images. The registration errors were 4.71 mm and 4.81 mm for complete
TRUS imaging and sparsely-sampled TRUS imaging, respectively. The results
indicate superior accuracy to the alternative rigid and non-rigid registration
algorithms tested and substantially lower computation time. The rapid inference
possible with FPT makes it particularly suitable for applications where
real-time registration is beneficial.

    

### [[2109.05024] Optimizing a domestic battery and solar photovoltaic system with deep reinforcement learning](http://arxiv.org/abs/2109.05024)


  A lowering in the cost of batteries and solar PV systems has led to a high
uptake of solar battery home systems. In this work, we use the deep
deterministic policy gradient algorithm to optimise the charging and
discharging behaviour of a battery within such a system. Our approach outputs a
continuous action space when it charges and discharges the battery, and can
function well in a stochastic environment. We show good performance of this
algorithm by lowering the expenditure of a single household on electricity to
almost \$1AUD for large batteries across selected weeks within a year.

    

### [[2109.05052] Entity-Based Knowledge Conflicts in Question Answering](http://arxiv.org/abs/2109.05052)


  Knowledge-dependent tasks typically use two sources of knowledge: parametric,
learned at training time, and contextual, given as a passage at inference time.
To understand how models use these sources together, we formalize the problem
of knowledge conflicts, where the contextual information contradicts the
learned information. Analyzing the behaviour of popular models, we measure
their over-reliance on memorized information (the cause of hallucinations), and
uncover important factors that exacerbate this behaviour. Lastly, we propose a
simple method to mitigate over-reliance on parametric knowledge, which
minimizes hallucination, and improves out-of-distribution generalization by
4%-7%. Our findings demonstrate the importance for practitioners to evaluate
model tendency to hallucinate rather than read, and show that our mitigation
strategy encourages generalization to evolving information (i.e.,
time-dependent queries). To encourage these practices, we have released our
framework for generating knowledge conflicts.

    

### [[2109.05053] Physics-based machine learning for modeling stochastic IP3-dependent calcium dynamics](http://arxiv.org/abs/2109.05053)


  We present a machine learning method for model reduction which incorporates
domain-specific physics through candidate functions. Our method estimates an
effective probability distribution and differential equation model from
stochastic simulations of a reaction network. The close connection between
reduced and fine scale descriptions allows approximations derived from the
master equation to be introduced into the learning problem. This representation
is shown to improve generalization and allows a large reduction in network size
for a classic model of inositol trisphosphate (IP3) dependent calcium
oscillations in non-excitable cells.

    

### [[2109.05070] Instance-Conditioned GAN](http://arxiv.org/abs/2109.05070)


  Generative Adversarial Networks (GANs) can generate near photo realistic
images in narrow domains such as human faces. Yet, modeling complex
distributions of datasets such as ImageNet and COCO-Stuff remains challenging
in unconditional settings. In this paper, we take inspiration from kernel
density estimation techniques and introduce a non-parametric approach to
modeling distributions of complex datasets. We partition the data manifold into
a mixture of overlapping neighborhoods described by a datapoint and its nearest
neighbors, and introduce a model, called instance-conditioned GAN (IC-GAN),
which learns the distribution around each datapoint. Experimental results on
ImageNet and COCO-Stuff show that IC-GAN significantly improves over
unconditional models and unsupervised data partitioning baselines. Moreover, we
show that IC-GAN can effortlessly transfer to datasets not seen during training
by simply changing the conditioning instances, and still generate realistic
images. Finally, we extend IC-GAN to the class-conditional case and show
semantically controllable generation and competitive quantitative results on
ImageNet; while improving over BigGAN on ImageNet-LT. We will opensource our
code and trained models to reproduce the reported results.

    

### [[2109.05074] FBERT: A Neural Transformer for Identifying Offensive Content](http://arxiv.org/abs/2109.05074)


  Transformer-based models such as BERT, XLNET, and XLM-R have achieved
state-of-the-art performance across various NLP tasks including the
identification of offensive language and hate speech, an important problem in
social media. In this paper, we present fBERT, a BERT model retrained on SOLID,
the largest English offensive language identification corpus available with
over $1.4$ million offensive instances. We evaluate fBERT's performance on
identifying offensive content on multiple English datasets and we test several
thresholds for selecting instances from SOLID. The fBERT model will be made
freely available to the community.

    

### [[2109.05075] On the Compression of Neural Networks Using $\ell_0$-Norm Regularization and Weight Pruning](http://arxiv.org/abs/2109.05075)


  Despite the growing availability of high-capacity computational platforms,
implementation complexity still has been a great concern for the real-world
deployment of neural networks. This concern is not exclusively due to the huge
costs of state-of-the-art network architectures, but also due to the recent
push towards edge intelligence and the use of neural networks in embedded
applications. In this context, network compression techniques have been gaining
interest due to their ability for reducing deployment costs while keeping
inference accuracy at satisfactory levels. The present paper is dedicated to
the development of a novel compression scheme for neural networks. To this end,
a new $\ell_0$-norm-based regularization approach is firstly developed, which
is capable of inducing strong sparseness in the network during training. Then,
targeting the smaller weights of the trained network with pruning techniques,
smaller yet highly effective networks can be obtained. The proposed compression
scheme also involves the use of $\ell_2$-norm regularization to avoid
overfitting as well as fine tuning to improve the performance of the pruned
network. Experimental results are presented aiming to show the effectiveness of
the proposed scheme as well as to make comparisons with competing approaches.

    

### [[2109.05077] Data Generation Method for Learning a Low-dimensional Safe Region in Safe Reinforcement Learning](http://arxiv.org/abs/2109.05077)


  Safe reinforcement learning aims to learn a control policy while ensuring
that neither the system nor the environment gets damaged during the learning
process. For implementing safe reinforcement learning on highly nonlinear and
high-dimensional dynamical systems, one possible approach is to find a
low-dimensional safe region via data-driven feature extraction methods, which
provides safety estimates to the learning algorithm. As the reliability of the
learned safety estimates is data-dependent, we investigate in this work how
different training data will affect the safe reinforcement learning approach.
By balancing between the learning performance and the risk of being unsafe, a
data generation method that combines two sampling methods is proposed to
generate representative training data. The performance of the method is
demonstrated with a three-link inverted pendulum example.

    

### [[2109.05087] Global and Local Interpretation of black-box Machine Learning models to determine prognostic factors from early COVID-19 data](http://arxiv.org/abs/2109.05087)


  The COVID-19 corona virus has claimed 4.1 million lives, as of July 24, 2021.
A variety of machine learning models have been applied to related data to
predict important factors such as the severity of the disease, infection rate
and discover important prognostic factors. Often the usefulness of the findings
from the use of these techniques is reduced due to lack of method
interpretability. Some recent progress made on the interpretability of machine
learning models has the potential to unravel more insights while using
conventional machine learning models. In this work, we analyze COVID-19 blood
work data with some of the popular machine learning models; then we employ
state-of-the-art post-hoc local interpretability techniques(e.g.- SHAP, LIME),
and global interpretability techniques(e.g. - symbolic metamodeling) to the
trained black-box models to draw interpretable conclusions. In the gamut of
machine learning algorithms, regressions remain one of the simplest and most
explainable models with clear mathematical formulation. We explore one of the
most recent techniques called symbolic metamodeling to find the mathematical
expression of the machine learning models for COVID-19. We identify Acute
Kidney Injury (AKI), initial Albumin level (ALBI), Aspartate aminotransferase
(ASTI), Total Bilirubin initial(TBILI) and D-Dimer initial (DIMER) as major
prognostic factors of the disease severity. Our contributions are- (i) uncover
the underlying mathematical expression for the black-box models on COVID-19
severity prediction task (ii) we are the first to apply symbolic metamodeling
to this task, and (iii) discover important features and feature interactions.

    

### [[2109.05095] Stochastic Adversarial Koopman Model for Dynamical Systems](http://arxiv.org/abs/2109.05095)


  Dynamical systems are ubiquitous and are often modeled using a non-linear
system of governing equations. Numerical solution procedures for many dynamical
systems have existed for several decades, but can be slow due to
high-dimensional state space of the dynamical system. Thus, deep learning-based
reduced order models (ROMs) are of interest and one such family of algorithms
along these lines are based on the Koopman theory. This paper extends a
recently developed adversarial Koopman model (Balakrishnan \& Upadhyay,
arXiv:2006.05547) to stochastic space, where the Koopman operator applies on
the probability distribution of the latent encoding of an encoder.
Specifically, the latent encoding of the system is modeled as a Gaussian, and
is advanced in time by using an auxiliary neural network that outputs two
Koopman matrices $K_{\mu}$ and $K_{\sigma}$. Adversarial and gradient losses
are used and this is found to lower the prediction errors. A reduced Koopman
formulation is also undertaken where the Koopman matrices are assumed to have a
tridiagonal structure, and this yields predictions comparable to the baseline
model with full Koopman matrices. The efficacy of the stochastic Koopman model
is demonstrated on different test problems in chaos, fluid dynamics,
combustion, and reaction-diffusion models. The proposed model is also applied
in a setting where the Koopman matrices are conditioned on other input
parameters for generalization and this is applied to simulate the state of a
Lithium-ion battery in time. The Koopman models discussed in this study are
very promising for the wide range of problems considered.

    

### [[2109.05097] HypoGen: Hyperbole Generation with Commonsense and Counterfactual Knowledge](http://arxiv.org/abs/2109.05097)


  A hyperbole is an intentional and creative exaggeration not to be taken
literally. Despite its ubiquity in daily life, the computational explorations
of hyperboles are scarce. In this paper, we tackle the under-explored and
challenging task: sentence-level hyperbole generation. We start with a
representative syntactic pattern for intensification and systematically study
the semantic (commonsense and counterfactual) relationships between each
component in such hyperboles. Next, we leverage the COMeT and reverse COMeT
models to do commonsense and counterfactual inference. We then generate
multiple hyperbole candidates based on our findings from the pattern, and train
neural classifiers to rank and select high-quality hyperboles. Automatic and
human evaluations show that our generation method is able to generate
hyperboles creatively with high success rate and intensity scores.

    

### [[2109.05104] Machine learning reveals how personalized climate communication can both succeed and backfire](http://arxiv.org/abs/2109.05104)


  Different advertising messages work for different people. Machine learning
can be an effective way to personalise climate communications. In this paper we
use machine learning to reanalyse findings from a recent study, showing that
online advertisements increased some people's belief in climate change while
resulting in decreased belief in others. In particular, we show that the effect
of the advertisements could change depending on people's age and ethnicity.

    

### [[2109.05109] Toward Communication Efficient Adaptive Gradient Method](http://arxiv.org/abs/2109.05109)


  In recent years, distributed optimization is proven to be an effective
approach to accelerate training of large scale machine learning models such as
deep neural networks. With the increasing computation power of GPUs, the
bottleneck of training speed in distributed training is gradually shifting from
computation to communication. Meanwhile, in the hope of training machine
learning models on mobile devices, a new distributed training paradigm called
``federated learning'' has become popular. The communication time in federated
learning is especially important due to the low bandwidth of mobile devices.
While various approaches to improve the communication efficiency have been
proposed for federated learning, most of them are designed with SGD as the
prototype training algorithm. While adaptive gradient methods have been proven
effective for training neural nets, the study of adaptive gradient methods in
federated learning is scarce. In this paper, we propose an adaptive gradient
method that can guarantee both the convergence and the communication efficiency
for federated learning.

    

### [[2109.05110] An Empirical Comparison of Off-policy Prediction Learning Algorithms in the Four Rooms Environment](http://arxiv.org/abs/2109.05110)


  Many off-policy prediction learning algorithms have been proposed in the past
decade, but it remains unclear which algorithms learn faster than others. We
empirically compare 11 off-policy prediction learning algorithms with linear
function approximation on two small tasks: the Rooms task, and the High
Variance Rooms task. The tasks are designed such that learning fast in them is
challenging. In the Rooms task, the product of importance sampling ratios can
be as large as $2^{14}$ and can sometimes be two. To control the high variance
caused by the product of the importance sampling ratios, step size should be
set small, which in turn slows down learning. The High Variance Rooms task is
more extreme in that the product of the ratios can become as large as
$2^{14}\times 25$. This paper builds upon the empirical study of off-policy
prediction learning algorithms by Ghiassian and Sutton (2021). We consider the
same set of algorithms as theirs and employ the same experimental methodology.
The algorithms considered are: Off-policy TD($\lambda$), five Gradient-TD
algorithms, two Emphatic-TD algorithms, Tree Backup($\lambda$),
Vtrace($\lambda$), and ABTD($\zeta$). We found that the algorithms' performance
is highly affected by the variance induced by the importance sampling ratios.
The data shows that Tree Backup($\lambda$), Vtrace($\lambda$), and
ABTD($\zeta$) are not affected by the high variance as much as other algorithms
but they restrict the effective bootstrapping parameter in a way that is too
limiting for tasks where high variance is not present. We observed that
Emphatic TD($\lambda$) tends to have lower asymptotic error than other
algorithms, but might learn more slowly in some cases. We suggest algorithms
for practitioners based on their problem of interest, and suggest approaches
that can be applied to specific algorithms that might result in substantially
improved algorithms.

    

### [[2109.05125] MURAL: Multimodal, Multitask Retrieval Across Languages](http://arxiv.org/abs/2109.05125)


  Both image-caption pairs and translation pairs provide the means to learn
deep representations of and connections between languages. We use both types of
pairs in MURAL (MUltimodal, MUltitask Representations Across Languages), a dual
encoder that solves two tasks: 1) image-text matching and 2) translation pair
matching. By incorporating billions of translation pairs, MURAL extends ALIGN
(Jia et al. PMLR'21)--a state-of-the-art dual encoder learned from 1.8 billion
noisy image-text pairs. When using the same encoders, MURAL's performance
matches or exceeds ALIGN's cross-modal retrieval performance on well-resourced
languages across several datasets. More importantly, it considerably improves
performance on under-resourced languages, showing that text-text learning can
overcome a paucity of image-caption examples for these languages. On the
Wikipedia Image-Text dataset, for example, MURAL-base improves zero-shot mean
recall by 8.1% on average for eight under-resourced languages and by 6.8% on
average when fine-tuning. We additionally show that MURAL's text
representations cluster not only with respect to genealogical connections but
also based on areal linguistics, such as the Balkan Sprachbund.

    

### [[2109.05131] Near Instance Optimal Model Selection for Pure Exploration Linear Bandits](http://arxiv.org/abs/2109.05131)


  The model selection problem in the pure exploration linear bandit setting is
introduced and studied in both the fixed confidence and fixed budget settings.
The model selection problem considers a nested sequence of hypothesis classes
of increasing complexities. Our goal is to automatically adapt to the
instance-dependent complexity measure of the smallest hypothesis class
containing the true model, rather than suffering from the complexity measure
related to the largest hypothesis class. We provide evidence showing that a
standard doubling trick over dimension fails to achieve the optimal
instance-dependent sample complexity. Our algorithms define a new optimization
problem based on experimental design that leverages the geometry of the action
set to efficiently identify a near-optimal hypothesis class. Our fixed budget
algorithm uses a novel application of a selection-validation trick in bandits.
This provides a new method for the understudied fixed budget setting in linear
bandits (even without the added challenge of model selection). We further
generalize the model selection problem to the misspecified regime, adapting our
algorithms in both fixed confidence and fixed budget settings.

    

### [[2109.05159] Co-Correcting: Noise-tolerant Medical Image Classification via mutual Label Correction](http://arxiv.org/abs/2109.05159)


  With the development of deep learning, medical image classification has been
significantly improved. However, deep learning requires massive data with
labels. While labeling the samples by human experts is expensive and
time-consuming, collecting labels from crowd-sourcing suffers from the noises
which may degenerate the accuracy of classifiers. Therefore, approaches that
can effectively handle label noises are highly desired. Unfortunately, recent
progress on handling label noise in deep learning has gone largely unnoticed by
the medical image. To fill the gap, this paper proposes a noise-tolerant
medical image classification framework named Co-Correcting, which significantly
improves classification accuracy and obtains more accurate labels through
dual-network mutual learning, label probability estimation, and curriculum
label correcting. On two representative medical image datasets and the MNIST
dataset, we test six latest Learning-with-Noisy-Labels methods and conduct
comparative studies. The experiments show that Co-Correcting achieves the best
accuracy and generalization under different noise ratios in various tasks. Our
project can be found at: this https URL.

    

### [[2109.05173] Making Table Understanding Work in Practice](http://arxiv.org/abs/2109.05173)


  Understanding the semantics of tables at scale is crucial for tasks like data
integration, preparation, and search. Table understanding methods aim at
detecting a table's topic, semantic column types, column relations, or
entities. With the rise of deep learning, powerful models have been developed
for these tasks with excellent accuracy on benchmarks. However, we observe that
there exists a gap between the performance of these models on these benchmarks
and their applicability in practice. In this paper, we address the question:
what do we need for these models to work in practice?
We discuss three challenges of deploying table understanding models and
propose a framework to address them. These challenges include 1) difficulty in
customizing models to specific domains, 2) lack of training data for typical
database tables often found in enterprises, and 3) lack of confidence in the
inferences made by models. We present SigmaTyper which implements this
framework for the semantic column type detection task. SigmaTyper encapsulates
a hybrid model trained on GitTables and integrates a lightweight
human-in-the-loop approach to customize the model. Lastly, we highlight avenues
for future research that further close the gap towards making table
understanding effective in practice.

    

### [[2109.05175] Estimation of Local Average Treatment Effect by Data Combination](http://arxiv.org/abs/2109.05175)


  It is important to estimate the local average treatment effect (LATE) when
compliance with a treatment assignment is incomplete. The previously proposed
methods for LATE estimation required all relevant variables to be jointly
observed in a single dataset; however, it is sometimes difficult or even
impossible to collect such data in many real-world problems for technical or
privacy reasons. We consider a novel problem setting in which LATE, as a
function of covariates, is nonparametrically identified from the combination of
separately observed datasets. For estimation, we show that the direct least
squares method, which was originally developed for estimating the average
treatment effect under complete compliance, is applicable to our setting.
However, model selection and hyperparameter tuning for the direct least squares
estimator can be unstable in practice since it is defined as a solution to the
minimax problem. We then propose a weighted least squares estimator that
enables simpler model selection by avoiding the minimax objective formulation.
Unlike the inverse probability weighted (IPW) estimator, the proposed estimator
directly uses the pre-estimated weight without inversion, avoiding the problems
caused by the IPW methods. We demonstrate the effectiveness of our method
through experiments using synthetic and real-world datasets.

    

### [[2109.05180] A Novel Intrinsic Measure of Data Separability](http://arxiv.org/abs/2109.05180)


  In machine learning, the performance of a classifier depends on both the
classifier model and the separability/complexity of datasets. To quantitatively
measure the separability of datasets, we create an intrinsic measure -- the
Distance-based Separability Index (DSI), which is independent of the classifier
model. We consider the situation in which different classes of data are mixed
in the same distribution to be the most difficult for classifiers to separate.
We then formally show that the DSI can indicate whether the distributions of
datasets are identical for any dimensionality. And we verify the DSI to be an
effective separability measure by comparing to several state-of-the-art
separability/complexity measures using synthetic and real datasets. Having
demonstrated the DSI's ability to compare distributions of samples, we also
discuss some of its other promising applications, such as measuring the
performance of generative adversarial networks (GANs) and evaluating the
results of clustering methods.

    

### [[2109.05198] Doubly Adaptive Scaled Algorithm for Machine Learning Using Second-Order Information](http://arxiv.org/abs/2109.05198)


  We present a novel adaptive optimization algorithm for large-scale machine
learning problems. Equipped with a low-cost estimate of local curvature and
Lipschitz smoothness, our method dynamically adapts the search direction and
step-size. The search direction contains gradient information preconditioned by
a well-scaled diagonal preconditioning matrix that captures the local curvature
information. Our methodology does not require the tedious task of learning rate
tuning, as the learning rate is updated automatically without adding an extra
hyperparameter. We provide convergence guarantees on a comprehensive collection
of optimization problems, including convex, strongly convex, and nonconvex
problems, in both deterministic and stochastic regimes. We also conduct an
extensive empirical evaluation on standard machine learning problems,
justifying our algorithm's versatility and demonstrating its strong performance
compared to other start-of-the-art first-order and second-order methods.

    

### [[2109.05201] Conditional Generation of Synthetic Geospatial Images from Pixel-level and Feature-level Inputs](http://arxiv.org/abs/2109.05201)


  Training robust supervised deep learning models for many geospatial
applications of computer vision is difficult due to dearth of class-balanced
and diverse training data. Conversely, obtaining enough training data for many
applications is financially prohibitive or may be infeasible, especially when
the application involves modeling rare or extreme events. Synthetically
generating data (and labels) using a generative model that can sample from a
target distribution and exploit the multi-scale nature of images can be an
inexpensive solution to address scarcity of labeled data. Towards this goal, we
present a deep conditional generative model, called VAE-Info-cGAN, that
combines a Variational Autoencoder (VAE) with a conditional Information
Maximizing Generative Adversarial Network (InfoGAN), for synthesizing
semantically rich images simultaneously conditioned on a pixel-level condition
(PLC) and a macroscopic feature-level condition (FLC). Dimensionally, the PLC
can only vary in the channel dimension from the synthesized image and is meant
to be a task-specific input. The FLC is modeled as an attribute vector in the
latent space of the generated image which controls the contributions of various
characteristic attributes germane to the target distribution. Experiments on a
GPS trajectories dataset show that the proposed model can accurately generate
various forms of spatiotemporal aggregates across different geographic
locations while conditioned only on a raster representation of the road
network. The primary intended application of the VAE-Info-cGAN is synthetic
data (and label) generation for targeted data augmentation for computer
vision-based modeling of problems relevant to geospatial analysis and remote
sensing.

    

### [[2109.05207] AstronomicAL: An interactive dashboard for visualisation, integration and classification of data using Active Learning](http://arxiv.org/abs/2109.05207)


  AstronomicAL is a human-in-the-loop interactive labelling and training
dashboard that allows users to create reliable datasets and robust classifiers
using active learning. This technique prioritises data that offer high
information gain, leading to improved performance using substantially less
data. The system allows users to visualise and integrate data from different
sources and deal with incorrect or missing labels and imbalanced class sizes.
AstronomicAL enables experts to visualise domain-specific plots and key
information relating both to broader context and details of a point of interest
drawn from a variety of data sources, ensuring reliable labels. In addition,
AstronomicAL provides functionality to explore all aspects of the training
process, including custom models and query strategies. This makes the software
a tool for experimenting with both domain-specific classifications and more
general-purpose machine learning strategies. We illustrate using the system
with an astronomical dataset due to the field's immediate need; however,
AstronomicAL has been designed for datasets from any discipline. Finally, by
exporting a simple configuration file, entire layouts, models, and assigned
labels can be shared with the community. This allows for complete transparency
and ensures that the process of reproducing results is effortless

    

### [[2109.05222] Fundamental limits of over-the-air optimization: Are analog schemes optimal?](http://arxiv.org/abs/2109.05222)


  We consider over-the-air convex optimization on a d dimensional space where
coded gradients are sent over an additive Gaussian noise channel with variance
\sigma^2. The codewords satisfy an average power constraint P, resulting in the
signal-to-noise ratio (SNR) of P/\sigma^2. We derive bounds for the convergence
rates for over-the-air optimization. Our first result is a lower bound for the
convergence rate showing that any code must slowdown the convergence rate by a
factor of roughly \sqrt{d/log(1 + SNR)}. Next, we consider a popular class of
schemes called analog coding, where a linear function of the gradient is sent.
We show that a simple scaled transmission analog coding scheme results in a
slowdown in convergence rate by a factor of \sqrt{d(1 + 1/SNR)}. This matches
the previous lower bound up to constant factors for low SNR, making the scaled
transmission scheme optimal at low SNR. However, we show that this slowdown is
necessary for any analog coding scheme. In particular, a slowdown in
convergence by a factor of \sqrt{d} for analog coding remains even when SNR
tends to infinity. Remarkably, we present a simple quantize-and-modulate scheme
that uses Amplitude Shift Keying and almost attains the optimal convergence
rate at all SNRs.

    

### [[2109.05223] 2-in-1 Accelerator: Enabling Random Precision Switch for Winning Both Adversarial Robustness and Efficiency](http://arxiv.org/abs/2109.05223)


  The recent breakthroughs of deep neural networks (DNNs) and the advent of
billions of Internet of Things (IoT) devices have excited an explosive demand
for intelligent IoT devices equipped with domain-specific DNN accelerators.
However, the deployment of DNN accelerator enabled intelligent functionality
into real-world IoT devices still remains particularly challenging. First,
powerful DNNs often come at prohibitive complexities, whereas IoT devices often
suffer from stringent resource constraints. Second, while DNNs are vulnerable
to adversarial attacks especially on IoT devices exposed to complex real-world
environments, many IoT applications require strict security. Existing DNN
accelerators mostly tackle only one of the two aforementioned challenges (i.e.,
efficiency or adversarial robustness) while neglecting or even sacrificing the
other. To this end, we propose a 2-in-1 Accelerator, an integrated
algorithm-accelerator co-design framework aiming at winning both the
adversarial robustness and efficiency of DNN accelerators. Specifically, we
first propose a Random Precision Switch (RPS) algorithm that can effectively
defend DNNs against adversarial attacks by enabling random DNN quantization as
an in-situ model switch. Furthermore, we propose a new precision-scalable
accelerator featuring (1) a new precision-scalable MAC unit architecture which
spatially tiles the temporal MAC units to boost both the achievable efficiency
and flexibility and (2) a systematically optimized dataflow that is searched by
our generic accelerator optimizer. Extensive experiments and ablation studies
validate that our 2-in-1 Accelerator can not only aggressively boost both the
adversarial robustness and efficiency of DNN accelerators under various
attacks, but also naturally support instantaneous robustness-efficiency
trade-offs adapting to varied resources without the necessity of DNN
retraining.

    

### [[2109.05225] Space Meets Time: Local Spacetime Neural Network For Traffic Flow Forecasting](http://arxiv.org/abs/2109.05225)


  Traffic flow forecasting is a crucial task in urban computing. The challenge
arises as traffic flows often exhibit intrinsic and latent spatio-temporal
correlations that cannot be identified by extracting the spatial and temporal
patterns of traffic data separately. We argue that such correlations are
universal and play a pivotal role in traffic flow. We put forward spacetime
interval learning as a paradigm to explicitly capture these correlations
through a unified analysis of both spatial and temporal features. Unlike the
state-of-the-art methods, which are restricted to a particular road network, we
model the universal spatio-temporal correlations that are transferable from
cities to cities. To this end, we propose a new spacetime interval learning
framework that constructs a local-spacetime context of a traffic sensor
comprising the data from its neighbors within close time points. Based on this
idea, we introduce spacetime neural network (STNN), which employs novel
spacetime convolution and attention mechanism to learn the universal
spatio-temporal correlations. The proposed STNN captures local traffic
patterns, which does not depend on a specific network structure. As a result, a
trained STNN model can be applied on any unseen traffic networks. We evaluate
the proposed STNN on two public real-world traffic datasets and a simulated
dataset on dynamic networks. The experiment results show that STNN not only
improves prediction accuracy by 15% over state-of-the-art methods, but is also
effective in handling the case when the traffic network undergoes dynamic
changes as well as the superior generalization capability.

    

### [[2109.05237] Physics-based Deep Learning](http://arxiv.org/abs/2109.05237)


  This digital book contains a practical and comprehensive introduction of
everything related to deep learning in the context of physical simulations. As
much as possible, all topics come with hands-on code examples in the form of
Jupyter notebooks to quickly get started. Beyond standard supervised learning
from data, we'll look at physical loss constraints, more tightly coupled
learning algorithms with differentiable simulations, as well as reinforcement
learning and uncertainty modeling. We live in exciting times: these methods
have a huge potential to fundamentally change what computer simulations can
achieve.

    

### [[2109.05257] Towards a Rigorous Evaluation of Time-series Anomaly Detection](http://arxiv.org/abs/2109.05257)


  In recent years, proposed studies on time-series anomaly detection (TAD)
report high F1 scores on benchmark TAD datasets, giving the impression of clear
improvements. However, most studies apply a peculiar evaluation protocol called
point adjustment (PA) before scoring. In this paper, we theoretically and
experimentally reveal that the PA protocol has a great possibility of
overestimating the detection performance; that is, even a random anomaly score
can easily turn into a state-of-the-art TAD method. Therefore, the comparison
of TAD methods with F1 scores after the PA protocol can lead to misguided
rankings. Furthermore, we question the potential of existing TAD methods by
showing that an untrained model obtains comparable detection performance to the
existing methods even without PA. Based on our findings, we propose a new
baseline and an evaluation protocol. We expect that our study will help a
rigorous evaluation of TAD and lead to further improvement in future
researches.

    

### [[2109.05265] RVMDE: Radar Validated Monocular Depth Estimation for Robotics](http://arxiv.org/abs/2109.05265)


  Stereoscopy exposits a natural perception of distance in a scene, and its
manifestation in 3D world understanding is an intuitive phenomenon. However, an
innate rigid calibration of binocular vision sensors is crucial for accurate
depth estimation. Alternatively, a monocular camera alleviates the limitation
at the expense of accuracy in estimating depth, and the challenge exacerbates
in harsh environmental conditions. Moreover, an optical sensor often fails to
acquire vital signals in harsh environments, and radar is used instead, which
gives coarse but more accurate signals. This work explores the utility of
coarse signals from radar when fused with fine-grained data from a monocular
camera for depth estimation in harsh environmental conditions. A variant of
feature pyramid network (FPN) extensively operates on fine-grained image
features at multiple scales with a fewer number of parameters. FPN feature maps
are fused with sparse radar features extracted with a Convolutional neural
network. The concatenated hierarchical features are used to predict the depth
with ordinal regression. We performed experiments on the nuScenes dataset, and
the proposed architecture stays on top in quantitative evaluations with reduced
parameters and faster inference. The depth estimation results suggest that the
proposed techniques can be used as an alternative to stereo depth estimation in
critical applications in robotics and self-driving cars. The source code will
be available in the following: \url{this https URL}.

    

### [[2109.05267] Utility Fairness for the Differentially Private Federated Learning](http://arxiv.org/abs/2109.05267)


  Federated learning (FL) allows predictive model training on the sensed data
in a wireless Internet of things (IoT) network evading data collection cost in
terms of energy, time, and privacy. In this paper, for a FL setting, we model
the learning gain achieved by an IoT device against its participation cost as
its utility. The local model quality and the associated cost differs from
device to device due to the device-heterogeneity which could be time-varying.
We identify that this results in utility unfairness because the same global
model is shared among the devices. In the vanilla FL setting, the master is
unaware of devices' local model computation and transmission costs, thus it is
unable to address the utility unfairness problem. In addition, a device may
exploit this lack of knowledge at the master to intentionally reduce its
expenditure and thereby boost its utility. We propose to control the quality of
the global model shared with the devices, in each round, based on their
contribution and expenditure. This is achieved by employing differential
privacy to curtail global model divulgence based on the learning contribution.
Furthermore, we devise adaptive computation and transmission policies for each
device to control its expenditure in order to mitigate utility unfairness. Our
results show that the proposed scheme reduces the standard deviation of the
energy cost of devices by 99% in comparison to the benchmark scheme, while the
standard deviation of the training loss of devices varies around 0.103.

    

### [[2109.05276] Benchmarking Processor Performance by Multi-Threaded Machine Learning Algorithms](http://arxiv.org/abs/2109.05276)


  Machine learning algorithms have enabled computers to predict things by
learning from previous data. The data storage and processing power are
increasing rapidly, thus increasing machine learning and Artificial
intelligence applications. Much of the work is done to improve the accuracy of
the models built in the past, with little research done to determine the
computational costs of machine learning acquisitions. In this paper, I will
proceed with this later research work and will make a performance comparison of
multi-threaded machine learning clustering algorithms. I will be working on
Linear Regression, Random Forest, and K-Nearest Neighbors to determine the
performance characteristics of the algorithms as well as the computation costs
to the obtained results. I will be benchmarking system hardware performance by
running these multi-threaded algorithms to train and test the models on a
dataset to note the differences in performance matrices of the algorithms. In
the end, I will state the best performing algorithms concerning the performance
efficiency of these algorithms on my system.

    

### [[2109.05278] Existence conditions for hidden feedback loops in online recommender systems](http://arxiv.org/abs/2109.05278)


  We explore a hidden feedback loops effect in online recommender systems.
Feedback loops result in degradation of online multi-armed bandit (MAB)
recommendations to a small subset and loss of coverage and novelty. We study
how uncertainty and noise in user interests influence the existence of feedback
loops. First, we show that an unbiased additive random noise in user interests
does not prevent a feedback loop. Second, we demonstrate that a non-zero
probability of resetting user interests is sufficient to limit the feedback
loop and estimate the size of the effect. Our experiments confirm the
theoretical findings in a simulated environment for four bandit algorithms.

    

### [[2109.05280] Learning To Describe Player Form in The MLB](http://arxiv.org/abs/2109.05280)


  Major League Baseball (MLB) has a storied history of using statistics to
better understand and discuss the game of baseball, with an entire discipline
of statistics dedicated to the craft, known as sabermetrics. At their core, all
sabermetrics seek to quantify some aspect of the game, often a specific aspect
of a player's skill set - such as a batter's ability to drive in runs (RBI) or
a pitcher's ability to keep batters from reaching base (WHIP). While useful,
such statistics are fundamentally limited by the fact that they are derived
from an account of what happened on the field, not how it happened. As a first
step towards alleviating this shortcoming, we present a novel, contrastive
learning-based framework for describing player form in the MLB. We use form to
refer to the way in which a player has impacted the course of play in their
recent appearances. Concretely, a player's form is described by a
72-dimensional vector. By comparing clusters of players resulting from our form
representations and those resulting from traditional abermetrics, we
demonstrate that our form representations contain information about how players
impact the course of play, not present in traditional, publicly available
statistics. We believe these embeddings could be utilized to predict both
in-game and game-level events, such as the result of an at-bat or the winner of
a game.

    

### [[2109.05281] COSMic: A Coherence-Aware Generation Metric for Image Descriptions](http://arxiv.org/abs/2109.05281)


  Developers of text generation models rely on automated evaluation metrics as
a stand-in for slow and expensive manual evaluations. However, image captioning
metrics have struggled to give accurate learned estimates of the semantic and
pragmatic success of output text. We address this weakness by introducing the
first discourse-aware learned generation metric for evaluating image
descriptions. Our approach is inspired by computational theories of discourse
for capturing information goals using coherence. We present a dataset of
image$\unicode{x2013}$description pairs annotated with coherence relations. We
then train a coherence-aware metric on a subset of the Conceptual Captions
dataset and measure its effectiveness$\unicode{x2014}$its ability to predict
human ratings of output captions$\unicode{x2014}$on a test set composed of
out-of-domain images. We demonstrate a higher Kendall Correlation Coefficient
for our proposed metric with the human judgments for the results of a number of
state-of-the-art coherence-aware caption generation models when compared to
several other metrics including recently proposed learned metrics such as
BLEURT and BERTScore.

    

### [[2109.05294] MLReal: Bridging the gap between training on synthetic data and real data applications in machine learning](http://arxiv.org/abs/2109.05294)


  Among the biggest challenges we face in utilizing neural networks trained on
waveform data (i.e., seismic, electromagnetic, or ultrasound) is its
application to real data. The requirement for accurate labels forces us to
develop solutions using synthetic data, where labels are readily available.
However, synthetic data often do not capture the reality of the field/real
experiment, and we end up with poor performance of the trained neural network
(NN) at the inference stage. We describe a novel approach to enhance supervised
training on synthetic data with real data features (domain adaptation).
Specifically, for tasks in which the absolute values of the vertical axis (time
or depth) of the input data are not crucial, like classification, or can be
corrected afterward, like velocity model building using a well-log, we suggest
a series of linear operations on the input so the training and application data
have similar distributions. This is accomplished by applying two operations on
the input data to the NN model: 1) The crosscorrelation of the input data
(i.e., shot gather, seismic image, etc.) with a fixed reference trace from the
same dataset. 2) The convolution of the resulting data with the mean (or a
random sample) of the autocorrelated data from another domain. In the training
stage, the input data are from the synthetic domain and the auto-correlated
data are from the real domain, and random samples from real data are drawn at
every training epoch. In the inference/application stage, the input data are
from the real subset domain and the mean of the autocorrelated sections are
from the synthetic data subset domain. Example applications on passive seismic
data for microseismic event source location determination and active seismic
data for predicting low frequencies are used to demonstrate the power of this
approach in improving the applicability of trained models to real data.

    

### [[2109.05317] Bayesian Topic Regression for Causal Inference](http://arxiv.org/abs/2109.05317)


  Causal inference using observational text data is becoming increasingly
popular in many research areas. This paper presents the Bayesian Topic
Regression (BTR) model that uses both text and numerical information to model
an outcome variable. It allows estimation of both discrete and continuous
treatment effects. Furthermore, it allows for the inclusion of additional
numerical confounding factors next to text data. To this end, we combine a
supervised Bayesian topic model with a Bayesian regression framework and
perform supervised representation learning for the text features jointly with
the regression parameter training, respecting the Frisch-Waugh-Lovell theorem.
Our paper makes two main contributions. First, we provide a regression
framework that allows causal inference in settings when both text and numerical
confounders are of relevance. We show with synthetic and semi-synthetic
datasets that our joint approach recovers ground truth with lower bias than any
benchmark model, when text and numerical features are correlated. Second,
experiments on two real-world datasets demonstrate that a joint and supervised
learning strategy also yields superior prediction results compared to
strategies that estimate regression weights for text and non-text features
separately, being even competitive with more complex deep neural networks.

    

### [[2109.05319] HyP-ABC: A Novel Automated Hyper-Parameter Tuning Algorithm Using Evolutionary Optimization](http://arxiv.org/abs/2109.05319)


  Machine learning techniques lend themselves as promising decision-making and
analytic tools in a wide range of applications. Different ML algorithms have
various hyper-parameters. In order to tailor an ML model towards a specific
application, a large number of hyper-parameters should be tuned. Tuning the
hyper-parameters directly affects the performance (accuracy and run-time).
However, for large-scale search spaces, efficiently exploring the ample number
of combinations of hyper-parameters is computationally challenging. Existing
automated hyper-parameter tuning techniques suffer from high time complexity.
In this paper, we propose HyP-ABC, an automatic innovative hybrid
hyper-parameter optimization algorithm using the modified artificial bee colony
approach, to measure the classification accuracy of three ML algorithms, namely
random forest, extreme gradient boosting, and support vector machine. Compared
to the state-of-the-art techniques, HyP-ABC is more efficient and has a limited
number of parameters to be tuned, making it worthwhile for real-world
hyper-parameter optimization problems. We further compare our proposed HyP-ABC
algorithm with state-of-the-art techniques. In order to ensure the robustness
of the proposed method, the algorithm takes a wide range of feasible
hyper-parameter values, and is tested using a real-world educational dataset.

    

### [[2109.05351] Remaining Useful Life Estimation of Hard Disk Drives using Bidirectional LSTM Networks](http://arxiv.org/abs/2109.05351)


  Physical and cloud storage services are well-served by functioning and
reliable high-volume storage systems. Recent observations point to hard disk
reliability as one of the most pressing reliability issues in data centers
containing massive volumes of storage devices such as HDDs. In this regard,
early detection of impending failure at the disk level aids in reducing system
downtime and reduces operational loss making proactive health monitoring a
priority for AIOps in such settings. In this work, we introduce methods of
extracting meaningful attributes associated with operational failure and of
pre-processing the highly imbalanced health statistics data for subsequent
prediction tasks using data-driven approaches. We use a Bidirectional LSTM with
a multi-day look back period to learn the temporal progression of health
indicators and baseline them against vanilla LSTM and Random Forest models to
come up with several key metrics that establish the usefulness of and
superiority of our model under some tightly defined operational constraints.
For example, using a 15 day look back period, our approach can predict the
occurrence of disk failure with an accuracy of 96.4% considering test data 60
days before failure. This helps to alert operations maintenance well in-advance
about potential mitigation needs. In addition, our model reports a mean
absolute error of 0.12 for predicting failure up to 60 days in advance, placing
it among the state-of-the-art in recent literature.

    

### [[2109.05352] DeepPyram: Enabling Pyramid View and Deformable Pyramid Reception for Semantic Segmentation in Cataract Surgery Videos](http://arxiv.org/abs/2109.05352)


  Semantic segmentation in cataract surgery has a wide range of applications
contributing to surgical outcome enhancement and clinical risk reduction.
However, the varying issues in segmenting the different relevant instances make
the designation of a unique network quite challenging. This paper proposes a
semantic segmentation network termed as DeepPyram that can achieve superior
performance in segmenting relevant objects in cataract surgery videos with
varying issues. This superiority mainly originates from three modules: (i)
Pyramid View Fusion, which provides a varying-angle global view of the
surrounding region centering at each pixel position in the input convolutional
feature map; (ii) Deformable Pyramid Reception, which enables a wide deformable
receptive field that can adapt to geometric transformations in the object of
interest; and (iii) Pyramid Loss that adaptively supervises multi-scale
semantic feature maps. These modules can effectively boost semantic
segmentation performance, especially in the case of transparency,
deformability, scalability, and blunt edges in objects. The proposed approach
is evaluated using four datasets of cataract surgery for objects with different
contextual features and compared with thirteen state-of-the-art segmentation
networks. The experimental results confirm that DeepPyram outperforms the rival
approaches without imposing additional trainable parameters. Our comprehensive
ablation study further proves the effectiveness of the proposed modules.

    

### [[2109.05360] Adaptive network reliability analysis: Methodology and applications to power grid](http://arxiv.org/abs/2109.05360)


  Flow network models can capture the underlying physics and operational
constraints of many networked systems including the power grid and
transportation and water networks. However, analyzing reliability of systems
using computationally expensive flow-based models faces substantial challenges,
especially for rare events. Existing actively trained meta-models, which
present a new promising direction in reliability analysis, are not applicable
to networks due to the inability of these methods to handle high-dimensional
problems as well as discrete or mixed variable inputs. This study presents the
first adaptive surrogate-based Network Reliability Analysis using Bayesian
Additive Regression Trees (ANR-BART). This approach integrates BART and Monte
Carlo simulation (MCS) via an active learning method that identifies the most
valuable training samples based on the credible intervals derived by BART over
the space of predictor variables as well as the proximity of the points to the
estimated limit state. Benchmark power grids including IEEE 30, 57, 118, and
300-bus systems and their power flow models for cascading failure analysis are
considered to investigate ANR-BART, MCS, subset simulation, and
passively-trained optimal deep neural networks and BART. Results indicate that
ANR-BART is robust and yields accurate estimates of network failure
probability, while significantly reducing the computational cost of reliability
analysis.

    

### [[2109.05364] Structure-preserving Sparse Identification of Nonlinear Dynamics for Data-driven Modeling](http://arxiv.org/abs/2109.05364)


  Discovery of dynamical systems from data forms the foundation for data-driven
modeling and recently, structure-preserving geometric perspectives have been
shown to provide improved forecasting, stability, and physical realizability
guarantees. We present here a unification of the Sparse Identification of
Nonlinear Dynamics (SINDy) formalism with neural ordinary differential
equations. The resulting framework allows learning of both "black-box" dynamics
and learning of structure preserving bracket formalisms for both reversible and
irreversible dynamics. We present a suite of benchmarks demonstrating
effectiveness and structure preservation, including for chaotic systems.

    

### [[2109.05385] On the Initial Behavior Monitoring Issues in Federated Learning](http://arxiv.org/abs/2109.05385)


  In Federated Learning (FL), a group of workers participate to build a global
model under the coordination of one node, the chief. Regarding the
cybersecurity of FL, some attacks aim at injecting the fabricated local model
updates into the system. Some defenses are based on malicious worker detection
and behavioral pattern analysis. In this context, without timely and dynamic
monitoring methods, the chief cannot detect and remove the malicious or
unreliable workers from the system. Our work emphasize the urgency to prepare
the federated learning process for monitoring and eventually behavioral pattern
analysis. We study the information inside the learning process in the early
stages of training, propose a monitoring process and evaluate the monitoring
period required. The aim is to analyse at what time is it appropriate to start
the detection algorithm in order to remove the malicious or unreliable workers
from the system and optimise the defense mechanism deployment. We tested our
strategy on a behavioral pattern analysis defense applied to the FL process of
different benchmark systems for text and image classification. Our results show
that the monitoring process lowers false positives and false negatives and
consequently increases system efficiency by enabling the distributed learning
system to achieve better performance in the early stage of training.

    

### [[2109.05389] Omnipredictors](http://arxiv.org/abs/2109.05389)


  Loss minimization is a dominant paradigm in machine learning, where a
predictor is trained to minimize some loss function that depends on an
uncertain event (e.g., "will it rain tomorrow?''). Different loss functions
imply different learning algorithms and, at times, very different predictors.
While widespread and appealing, a clear drawback of this approach is that the
loss function may not be known at the time of learning, requiring the algorithm
to use a best-guess loss function. We suggest a rigorous new paradigm for loss
minimization in machine learning where the loss function can be ignored at the
time of learning and only be taken into account when deciding an action.
We introduce the notion of an (${\mathcal{L}},\mathcal{C}$)-omnipredictor,
which could be used to optimize any loss in a family ${\mathcal{L}}$. Once the
loss function is set, the outputs of the predictor can be post-processed (a
simple univariate data-independent transformation of individual predictions) to
do well compared with any hypothesis from the class $\mathcal{C}$. The post
processing is essentially what one would perform if the outputs of the
predictor were true probabilities of the uncertain events. In a sense,
omnipredictors extract all the predictive power from the class $\mathcal{C}$,
irrespective of the loss function in $\mathcal{L}$.
We show that such "loss-oblivious'' learning is feasible through a connection
to multicalibration, a notion introduced in the context of algorithmic
fairness. In addition, we show how multicalibration can be viewed as a solution
concept for agnostic boosting, shedding new light on past results. Finally, we
transfer our insights back to the context of algorithmic fairness by providing
omnipredictors for multi-group loss minimization.

    

### [[2109.05391] Gradients and Subgradients of Buffered Failure Probability](http://arxiv.org/abs/2109.05391)


  Gradients and subgradients are central to optimization and sensitivity
analysis of buffered failure probabilities. We furnish a characterization of
subgradients based on subdifferential calculus in the case of finite
probability distributions and, under additional assumptions, also a gradient
expression for general distributions. Several examples illustrate the
application of the results, especially in the context of optimality conditions.

    

### [[2109.05402] Differentially Private Variable Selection via the Knockoff Filter](http://arxiv.org/abs/2109.05402)


  The knockoff filter, recently developed by Barber and Candes, is an effective
procedure to perform variable selection with a controlled false discovery rate
(FDR). We propose a private version of the knockoff filter by incorporating
Gaussian and Laplace mechanisms, and show that variable selection with
controlled FDR can be achieved. Simulations demonstrate that our setting has
reasonable statistical power.

    

### [[2109.05408] On the Fundamental Limits of Matrix Completion: Leveraging Hierarchical Similarity Graphs](http://arxiv.org/abs/2109.05408)


  We study the matrix completion problem that leverages hierarchical similarity
graphs as side information in the context of recommender systems. Under a
hierarchical stochastic block model that well respects practically-relevant
social graphs and a low-rank rating matrix model, we characterize the exact
information-theoretic limit on the number of observed matrix entries (i.e.,
optimal sample complexity) by proving sharp upper and lower bounds on the
sample complexity. In the achievability proof, we demonstrate that probability
of error of the maximum likelihood estimator vanishes for sufficiently large
number of users and items, if all sufficient conditions are satisfied. On the
other hand, the converse (impossibility) proof is based on the genie-aided
maximum likelihood estimator. Under each necessary condition, we present
examples of a genie-aided estimator to prove that the probability of error does
not vanish for sufficiently large number of users and items. One important
consequence of this result is that exploiting the hierarchical structure of
social graphs yields a substantial gain in sample complexity relative to the
one that simply identifies different groups without resorting to the relational
structure across them. More specifically, we analyze the optimal sample
complexity and identify different regimes whose characteristics rely on quality
metrics of side information of the hierarchical similarity graph. Finally, we
present simulation results to corroborate our theoretical findings and show
that the characterized information-theoretic limit can be asymptotically
achieved.

    

### [[2109.05409] Team NeuroPoly: Description of the Pipelines for the MICCAI 2021 MS New Lesions Segmentation Challenge](http://arxiv.org/abs/2109.05409)


  This paper gives a detailed description of the pipelines used for the 2nd
edition of the MICCAI 2021 Challenge on Multiple Sclerosis Lesion Segmentation.
An overview of the data preprocessing steps applied is provided along with a
brief description of the pipelines used, in terms of the architecture and the
hyperparameters. Our code for this work can be found at:
this https URL.

    

### [[2109.05424] Pairwise Supervised Contrastive Learning of Sentence Representations](http://arxiv.org/abs/2109.05424)


  Many recent successes in sentence representation learning have been achieved
by simply fine-tuning on the Natural Language Inference (NLI) datasets with
triplet loss or siamese loss. Nevertheless, they share a common weakness:
sentences in a contradiction pair are not necessarily from different semantic
categories. Therefore, optimizing the semantic entailment and contradiction
reasoning objective alone is inadequate to capture the high-level semantic
structure. The drawback is compounded by the fact that the vanilla siamese or
triplet losses only learn from individual sentence pairs or triplets, which
often suffer from bad local optima. In this paper, we propose PairSupCon, an
instance discrimination based approach aiming to bridge semantic entailment and
contradiction understanding with high-level categorical concept encoding. We
evaluate PairSupCon on various downstream tasks that involve understanding
sentence semantics at different granularities. We outperform the previous
state-of-the-art method with $10\%$--$13\%$ averaged improvement on eight
clustering tasks, and $5\%$--$6\%$ averaged improvement on seven semantic
textual similarity (STS) tasks.

    

### [[2109.05429] EMVLight: A Decentralized Reinforcement Learning Framework for EfficientPassage of Emergency Vehicles](http://arxiv.org/abs/2109.05429)


  Emergency vehicles (EMVs) play a crucial role in responding to time-critical
events such as medical emergencies and fire outbreaks in an urban area. The
less time EMVs spend traveling through the traffic, the more likely it would
help save people's lives and reduce property loss. To reduce the travel time of
EMVs, prior work has used route optimization based on historical traffic-flow
data and traffic signal pre-emption based on the optimal route. However,
traffic signal pre-emption dynamically changes the traffic flow which, in turn,
modifies the optimal route of an EMV. In addition, traffic signal pre-emption
practices usually lead to significant disturbances in traffic flow and
subsequently increase the travel time for non-EMVs. In this paper, we propose
EMVLight, a decentralized reinforcement learning (RL) framework for
simultaneous dynamic routing and traffic signal control. EMVLight extends
Dijkstra's algorithm to efficiently update the optimal route for the EMVs in
real time as it travels through the traffic network. The decentralized RL
agents learn network-level cooperative traffic signal phase strategies that not
only reduce EMV travel time but also reduce the average travel time of non-EMVs
in the network. This benefit has been demonstrated through comprehensive
experiments with synthetic and real-world maps. These experiments show that
EMVLight outperforms benchmark transportation engineering techniques and
existing RL-based signal control methods.

    

### [[2109.05439] Concave Utility Reinforcement Learning with Zero-Constraint Violations](http://arxiv.org/abs/2109.05439)


  We consider the problem of tabular infinite horizon concave utility
reinforcement learning (CURL) with convex constraints. Various learning
applications with constraints, such as robotics, do not allow for policies that
can violate constraints. To this end, we propose a model-based learning
algorithm that achieves zero constraint violations. To obtain this result, we
assume that the concave objective and the convex constraints have a solution
interior to the set of feasible occupation measures. We then solve a tighter
optimization problem to ensure that the constraints are never violated despite
the imprecise model knowledge and model stochasticity. We also propose a novel
Bellman error based analysis for tabular infinite-horizon setups which allows
to analyse stochastic policies. Combining the Bellman error based analysis and
tighter optimization equation, for $T$ interactions with the environment, we
obtain a regret guarantee for objective which grows as $\Tilde{O}(1/\sqrt{T})$,
excluding other factors.

    

### [[2109.05461] Accurate Prediction Using Triangular Type-2 Fuzzy Linear Regression](http://arxiv.org/abs/2109.05461)


  Many works have been done to handle the uncertainties in the data using type
1 fuzzy regression. Few type 2 fuzzy regression works used interval type 2 for
indeterminate modeling using type 1 fuzzy membership. The current survey
proposes a triangular type-2 fuzzy regression (TT2FR) model to ameliorate the
efficiency of the model by handling the uncertainty in the data. The triangular
secondary membership function is used instead of widely used interval type
models. In the proposed model, vagueness in primary and secondary fuzzy sets is
minimized and also, a specified x-plane of observed value is included in the
same {\alpha}- plane of the predicted value. Complex calculations of the type-2
fuzzy (T2F) model are simplified by reducing three dimensional type-2 fuzzy set
(3DT2FS) into two dimensional interval type-2 fuzzy (2DIT2F) models. The
current survey presents a new regression model of T2F by considering the more
general form of T2F membership functions and thus avoids high complexity. The
performance of the developed model is evaluated using the TAIEX and COVID-19
forecasting datasets. Our developed model reached the highest performance as
compared to the other state-of-art techniques. Our developed method is ready to
be tested with more uncertain data and has the potential to use to predict the
weather and stock prediction.

    

### [[2109.05463] The Logic Traps in Evaluating Post-hoc Interpretations](http://arxiv.org/abs/2109.05463)


  Post-hoc interpretation aims to explain a trained model and reveal how the
model arrives at a decision. Though research on post-hoc interpretations has
developed rapidly, one growing pain in this field is the difficulty in
evaluating interpretations. There are some crucial logic traps behind existing
evaluation methods, which are ignored by most works. In this opinion piece, we
summarize four kinds evaluation methods and point out the corresponding logic
traps behind them. We argue that we should be clear about these traps rather
than ignore them and draw conclusions assertively.

    

### [[2109.05466] Graph Attention Network Based Single-Pixel Compressive Direction of Arrival Estimation](http://arxiv.org/abs/2109.05466)


  In this paper, we present a single-pixel compressive direction of arrival
(DoA) estimation technique leveraging a graph attention network (GAT) based
deep-learning framework. The physical layer compression is achieved using a
coded-aperture technique, probing the spectrum of far-field sources incident on
the aperture using a set of spatio-temporally incoherent modes. This
information is then encoded and compressed into the channel of the
coded-aperture. The coded-aperture based receiver exhibits a single-channel,
replacing the conventional multichannel raster scan based solutions for DoA
estimation. The GAT network enables the compressive DoA estimation framework to
learn the DoA information directly from the measurements acquired using the
coded-aperture. This step eliminates the need for an additional reconstruction
step and significantly simplifies the processing layer to obtain the DoA
estimate. We show that the presented GAT integrated single-pixel radar
framework can retrieve high fidelity DoA information even under relatively low
signal-to-noise ratio (SNR) levels.

    

### [[2109.05468] Feature Importance in Gradient Boosting Trees with Cross-Validation Feature Selection](http://arxiv.org/abs/2109.05468)


  Gradient Boosting Machines (GBM) are among the go-to algorithms on tabular
data, which produce state of the art results in many prediction tasks. Despite
its popularity, the GBM framework suffers from a fundamental flaw in its base
learners. Specifically, most implementations utilize decision trees that are
typically biased towards categorical variables with large cardinalities. The
effect of this bias was extensively studied over the years, mostly in terms of
predictive performance. In this work, we extend the scope and study the effect
of biased base learners on GBM feature importance (FI) measures. We show that
although these implementation demonstrate highly competitive predictive
performance, they still, surprisingly, suffer from bias in FI. By utilizing
cross-validated (CV) unbiased base learners, we fix this flaw at a relatively
low computational cost. We demonstrate the suggested framework in a variety of
synthetic and real-world setups, showing a significant improvement in all GBM
FI measures while maintaining relatively the same level of prediction accuracy.

    

### [[2109.05470] DRo: A data-scarce mechanism to revolutionize the performance of Deep Learning based Security Systems](http://arxiv.org/abs/2109.05470)


  Supervised Deep Learning requires plenty of labeled data to converge, and
hence perform optimally for task-specific learning. Therefore, we propose a
novel mechanism named DRo (for Deep Routing) for data-scarce domains like
security. The DRo approach builds upon some of the recent developments in
Deep-Clustering. In particular, it exploits the self-augmented training
mechanism using synthetically generated local perturbations. DRo not only
allays the challenges with sparse-labeled data but also offers many unique
advantages. We also developed a system named DRoID that uses the DRo mechanism
for enhancing the performance of an existing Malware Detection System that uses
(low information features like the) Android implicit Intent(s) as the only
features. We conduct experiments on DRoID using a popular and standardized
Android malware dataset and found that the DRo mechanism could successfully
reduce the false-alarms generated by the downstream classifier by 67.9%, and
also simultaneously boosts its accuracy by 11.3%. This is significant not only
because the gains achieved are unparalleled but also because the features used
were never considered rich enough to train a classifier on; and hence no decent
performance could ever be reported by any malware classification system
till-date using these features in isolation. Owing to the results achieved, the
DRo mechanism claims a dominant position amongst all known systems that aims to
enhance the classification performance of deep learning models with
sparse-labeled data.

    

### [[2109.05472] Compute and Energy Consumption Trends in Deep Learning Inference](http://arxiv.org/abs/2109.05472)


  The progress of some AI paradigms such as deep learning is said to be linked
to an exponential growth in the number of parameters. There are many studies
corroborating these trends, but does this translate into an exponential
increase in energy consumption? In order to answer this question we focus on
inference costs rather than training costs, as the former account for most of
the computing effort, solely because of the multiplicative factors. Also, apart
from algorithmic innovations, we account for more specific and powerful
hardware (leading to higher FLOPS) that is usually accompanied with important
energy efficiency optimisations. We also move the focus from the first
implementation of a breakthrough paper towards the consolidated version of the
techniques one or two year later. Under this distinctive and comprehensive
perspective, we study relevant models in the areas of computer vision and
natural language processing: for a sustained increase in performance we see a
much softer growth in energy consumption than previously anticipated. The only
caveat is, yet again, the multiplicative factor, as future AI increases
penetration and becomes more pervasive.

    

### [[2109.05478] Single-Read Reconstruction for DNA Data Storage Using Transformers](http://arxiv.org/abs/2109.05478)


  As the global need for large-scale data storage is rising exponentially,
existing storage technologies are approaching their theoretical and functional
limits in terms of density and energy consumption, making DNA based storage a
potential solution for the future of data storage. Several studies introduced
DNA based storage systems with high information density (petabytes/gram).
However, DNA synthesis and sequencing technologies yield erroneous outputs.
Algorithmic approaches for correcting these errors depend on reading multiple
copies of each sequence and result in excessive reading costs. The
unprecedented success of Transformers as a deep learning architecture for
language modeling has led to its repurposing for solving a variety of tasks
across various domains. In this work, we propose a novel approach for
single-read reconstruction using an encoder-decoder Transformer architecture
for DNA based data storage. We address the error correction process as a
self-supervised sequence-to-sequence task and use synthetic noise injection to
train the model using only the decoded reads. Our approach exploits the
inherent redundancy of each decoded file to learn its underlying structure. To
demonstrate our proposed approach, we encode text, image and code-script files
to DNA, produce errors with high-fidelity error simulator, and reconstruct the
original files from the noisy reads. Our model achieves lower error rates when
reconstructing the original data from a single read of each DNA strand compared
to state-of-the-art algorithms using 2-3 copies. This is the first
demonstration of using deep learning models for single-read reconstruction in
DNA based storage which allows for the reduction of the overall cost of the
process. We show that this approach is applicable for various domains and can
be generalized to new domains as well.

    

### [[2109.05485] Facial Anatomical Landmark Detection using Regularized Transfer Learning with Application to Fetal Alcohol Syndrome Recognition](http://arxiv.org/abs/2109.05485)


  Fetal alcohol syndrome (FAS) caused by prenatal alcohol exposure can result
in a series of cranio-facial anomalies, and behavioral and neurocognitive
problems. Current diagnosis of FAS is typically done by identifying a set of
facial characteristics, which are often obtained by manual examination.
Anatomical landmark detection, which provides rich geometric information, is
important to detect the presence of FAS associated facial anomalies. This
imaging application is characterized by large variations in data appearance and
limited availability of labeled data. Current deep learning-based heatmap
regression methods designed for facial landmark detection in natural images
assume availability of large datasets and are therefore not wellsuited for this
application. To address this restriction, we develop a new regularized transfer
learning approach that exploits the knowledge of a network learned on large
facial recognition datasets. In contrast to standard transfer learning which
focuses on adjusting the pre-trained weights, the proposed learning approach
regularizes the model behavior. It explicitly reuses the rich visual semantics
of a domain-similar source model on the target task data as an additional
supervisory signal for regularizing landmark detection optimization.
Specifically, we develop four regularization constraints for the proposed
transfer learning, including constraining the feature outputs from
classification and intermediate layers, as well as matching activation
attention maps in both spatial and channel levels. Experimental evaluation on a
collected clinical imaging dataset demonstrate that the proposed approach can
effectively improve model generalizability under limited training samples, and
is advantageous to other approaches in the literature.

    

### [[2109.05490] HyAR: Addressing Discrete-Continuous Action Reinforcement Learning via Hybrid Action Representation](http://arxiv.org/abs/2109.05490)


  Discrete-continuous hybrid action space is a natural setting in many
practical problems, such as robot control and game AI. However, most previous
Reinforcement Learning (RL) works only demonstrate the success in controlling
with either discrete or continuous action space, while seldom take into account
the hybrid action space. One naive way to address hybrid action RL is to
convert the hybrid action space into a unified homogeneous action space by
discretization or continualization, so that conventional RL algorithms can be
applied. However, this ignores the underlying structure of hybrid action space
and also induces the scalability issue and additional approximation
difficulties, thus leading to degenerated results. In this paper, we propose
Hybrid Action Representation (HyAR) to learn a compact and decodable latent
representation space for the original hybrid action space. HyAR constructs the
latent space and embeds the dependence between discrete action and continuous
parameter via an embedding table and conditional Variantional Auto-Encoder
(VAE). To further improve the effectiveness, the action representation is
trained to be semantically smooth through unsupervised environmental dynamics
prediction. Finally, the agent then learns its policy with conventional DRL
algorithms in the learned representation space and interacts with the
environment by decoding the hybrid action embeddings to the original action
space. We evaluate HyAR in a variety of environments with discrete-continuous
action space. The results demonstrate the superiority of HyAR when compared
with previous baselines, especially for high-dimensional action spaces.

    

### [[2109.05491] DynSTGAT: Dynamic Spatial-Temporal Graph Attention Network for Traffic Signal Control](http://arxiv.org/abs/2109.05491)


  Adaptive traffic signal control plays a significant role in the construction
of smart cities. This task is challenging because of many essential factors,
such as cooperation among neighboring intersections and dynamic traffic
scenarios. First, to facilitate cooperation of traffic signals, existing work
adopts graph neural networks to incorporate the temporal and spatial influences
of the surrounding intersections into the target intersection, where
spatial-temporal information is used separately. However, one drawback of these
methods is that the spatial-temporal correlations are not adequately exploited
to obtain a better control scheme. Second, in a dynamic traffic environment,
the historical state of the intersection is also critical for predicting future
signal switching. Previous work mainly solves this problem using the current
intersection's state, neglecting the fact that traffic flow is continuously
changing both spatially and temporally and does not handle the historical
state.
In this paper, we propose a novel neural network framework named DynSTGAT,
which integrates dynamic historical state into a new spatial-temporal graph
attention network to address the above two problems. More specifically, our
DynSTGAT model employs a novel multi-head graph attention mechanism, which aims
to adequately exploit the joint relations of spatial-temporal information.
Then, to efficiently utilize the historical state information of the
intersection, we design a sequence model with the temporal convolutional
network (TCN) to capture the historical information and further merge it with
the spatial information to improve its performance. Extensive experiments
conducted in the multi-intersection scenario on synthetic data and real-world
data confirm that our method can achieve superior performance in travel time
and throughput against the state-of-the-art methods.

    

### [[2109.05493] LEA-Net: Layer-wise External Attention Network for Efficient Color Anomaly Detection](http://arxiv.org/abs/2109.05493)


  The utilization of prior knowledge about anomalies is an essential issue for
anomaly detections. Recently, the visual attention mechanism has become a
promising way to improve the performance of CNNs for some computer vision
tasks. In this paper, we propose a novel model called Layer-wise External
Attention Network (LEA-Net) for efficient image anomaly detection. The core
idea relies on the integration of unsupervised and supervised anomaly detectors
via the visual attention mechanism. Our strategy is as follows: (i) Prior
knowledge about anomalies is represented as the anomaly map generated by
unsupervised learning of normal instances, (ii) The anomaly map is translated
to an attention map by the external network, (iii) The attention map is then
incorporated into intermediate layers of the anomaly detection network.
Notably, this layer-wise external attention can be applied to any CNN model in
an end-to-end training manner. For a pilot study, we validate LEA-Net on color
anomaly detection tasks. Through extensive experiments on PlantVillage, MVTec
AD, and Cloud datasets, we demonstrate that the proposed layer-wise visual
attention mechanism consistently boosts anomaly detection performances of an
existing CNN model, even on imbalanced datasets. Moreover, we show that our
attention mechanism successfully boosts the performance of several CNN models.

    

### [[2109.05507] Check Your Other Door! Establishing Backdoor Attacks in the Frequency Domain](http://arxiv.org/abs/2109.05507)


  Deep Neural Networks (DNNs) have been utilized in various applications
ranging from image classification and facial recognition to medical imagery
analysis and real-time object detection. As our models become more
sophisticated and complex, the computational cost of training such models
becomes a burden for small companies and individuals; for this reason,
outsourcing the training process has been the go-to option for such users.
Unfortunately, outsourcing the training process comes at the cost of
vulnerability to backdoor attacks. These attacks aim at establishing hidden
backdoors in the DNN such that the model performs well on benign samples but
outputs a particular target label when a trigger is applied to the input.
Current backdoor attacks rely on generating triggers in the image/pixel domain;
however, as we show in this paper, it is not the only domain to exploit and one
should always "check the other doors". In this work, we propose a complete
pipeline for generating a dynamic, efficient, and invisible backdoor attack in
the frequency domain. We show the advantages of utilizing the frequency domain
for establishing undetectable and powerful backdoor attacks through extensive
experiments on various datasets and network architectures. The backdoored
models are shown to break various state-of-the-art defences. We also show two
possible defences that succeed against frequency-based backdoor attacks and
possible ways for the attacker to bypass them. We conclude the work with some
remarks regarding a network's learning capacity and the capability of embedding
a backdoor attack in the model.

    

### [[2109.05522] TEASEL: A Transformer-Based Speech-Prefixed Language Model](http://arxiv.org/abs/2109.05522)


  Multimodal language analysis is a burgeoning field of NLP that aims to
simultaneously model a speaker's words, acoustical annotations, and facial
expressions. In this area, lexicon features usually outperform other modalities
because they are pre-trained on large corpora via Transformer-based models.
Despite their strong performance, training a new self-supervised learning (SSL)
Transformer on any modality is not usually attainable due to insufficient data,
which is the case in multimodal language learning. This work proposes a
Transformer-Based Speech-Prefixed Language Model called TEASEL to approach the
mentioned constraints without training a complete Transformer model. TEASEL
model includes speech modality as a dynamic prefix besides the textual modality
compared to a conventional language model. This method exploits a conventional
pre-trained language model as a cross-modal Transformer model. We evaluated
TEASEL for the multimodal sentiment analysis task defined by CMU-MOSI dataset.
Extensive experiments show that our model outperforms unimodal baseline
language models by 4% and outperforms the current multimodal state-of-the-art
(SoTA) model by 1% in F1-score. Additionally, our proposed method is 72%
smaller than the SoTA model.

    

### [[2109.05526] An Unsupervised Deep-Learning Method for Fingerprint Classification: the CCAE Network and the Hybrid Clustering Strategy](http://arxiv.org/abs/2109.05526)


  The fingerprint classification is an important and effective method to
quicken the process and improve the accuracy in the fingerprint matching
process. Conventional supervised methods need a large amount of pre-labeled
data and thus consume immense human resources. In this paper, we propose a new
and efficient unsupervised deep learning method that can extract fingerprint
features and classify fingerprint patterns automatically. In this approach, a
new model named constraint convolutional auto-encoder (CCAE) is used to extract
fingerprint features and a hybrid clustering strategy is applied to obtain the
final clusters. A set of experiments in the NIST-DB4 dataset shows that the
proposed unsupervised method exhibits the efficient performance on fingerprint
classification. For example, the CCAE achieves an accuracy of 97.3% on only
1000 unlabeled fingerprints in the NIST-DB4.

    

### [[2109.05535] Adversarial Representation Learning With Closed-Form Solvers](http://arxiv.org/abs/2109.05535)


  Adversarial representation learning aims to learn data representations for a
target task while removing unwanted sensitive information at the same time.
Existing methods learn model parameters iteratively through stochastic gradient
descent-ascent, which is often unstable and unreliable in practice. To overcome
this challenge, we adopt closed-form solvers for the adversary and target task.
We model them as kernel ridge regressors and analytically determine an
upper-bound on the optimal dimensionality of representation. Our solution,
dubbed OptNet-ARL, reduces to a stable one one-shot optimization problem that
can be solved reliably and efficiently. OptNet-ARL can be easily generalized to
the case of multiple target tasks and sensitive attributes. Numerical
experiments, on both small and large scale datasets, show that, from an
optimization perspective, OptNet-ARL is stable and exhibits three to five times
faster convergence. Performance wise, when the target and sensitive attributes
are dependent, OptNet-ARL learns representations that offer a better trade-off
front between (a) utility and bias for fair classification and (b) utility and
privacy by mitigating leakage of private information than existing solutions.

    

### [[2109.05536] Link Scheduling using Graph Neural Networks](http://arxiv.org/abs/2109.05536)


  Efficient scheduling of transmissions is a key problem in wireless networks.
The main challenge stems from the fact that optimal link scheduling involves
solving a maximum weighted independent set (MWIS) problem, which is known to be
NP-hard. For practical link scheduling schemes, centralized and distributed
greedy heuristics are commonly used to approximate the solution to the MWIS
problem. However, these greedy schemes mostly ignore important topological
information of the wireless network. To overcome this limitation, we propose
fast heuristics based on graph convolutional networks (GCNs) that can be
implemented in centralized and distributed manners. Our centralized MWIS solver
is based on tree search guided by a trainable GCN module and 1-step rollout. In
our distributed MWIS solver, a trainable GCN module learns topology-aware node
embeddings that are combined with the network weights before calling a
distributed greedy solver. Test results on medium-sized wireless networks show
that a GCN-based centralized MWIS solver can reach a near-optimal solution
quickly. Moreover, we demonstrate that a shallow GCN-based distributed MWIS
scheduler can reduce by nearly half the suboptimality gap of the distributed
greedy solver with minimal increase in complexity. The proposed scheduling
solutions also exhibit good generalizability across graph and weight
distributions.

    

### [[2109.05539] BioLCNet: Reward-modulated Locally Connected Spiking Neural Networks](http://arxiv.org/abs/2109.05539)


  Recent studies have shown that convolutional neural networks (CNNs) are not
the only feasible solution for image classification. Furthermore, weight
sharing and backpropagation used in CNNs do not correspond to the mechanisms
present in the primate visual system. To propose a more biologically plausible
solution, we designed a locally connected spiking neural network (SNN) trained
using spike-timing-dependent plasticity (STDP) and its reward-modulated variant
(R-STDP) learning rules. The use of spiking neurons and local connections along
with reinforcement learning (RL) led us to the nomenclature BioLCNet for our
proposed architecture. Our network consists of a rate-coded input layer
followed by a locally connected hidden layer and a decoding output layer. A
spike population-based voting scheme is adopted for decoding in the output
layer. We used the MNIST dataset to obtain image classification accuracy and to
assess the robustness of our rewarding system to varying target responses.

    

### [[2109.05546] Improved Algorithms for Misspecified Linear Markov Decision Processes](http://arxiv.org/abs/2109.05546)


  For the misspecified linear Markov decision process (MLMDP) model of Jin et
al. [2020], we propose an algorithm with three desirable properties. (P1) Its
regret after $K$ episodes scales as $K \max \{ \varepsilon_{\text{mis}},
\varepsilon_{\text{tol}} \}$, where $\varepsilon_{\text{mis}}$ is the degree of
misspecification and $\varepsilon_{\text{tol}}$ is a user-specified error
tolerance. (P2) Its space and per-episode time complexities remain bounded as
$K \rightarrow \infty$. (P3) It does not require $\varepsilon_{\text{mis}}$ as
input. To our knowledge, this is the first algorithm satisfying all three
properties. For concrete choices of $\varepsilon_{\text{tol}}$, we also improve
existing regret bounds (up to log factors) while achieving either (P2) or (P3)
(existing algorithms satisfy neither). At a high level, our algorithm
generalizes (to MLMDPs) and refines the Sup-Lin-UCB algorithm, which Takemura
et al. [2021] recently showed satisfies (P3) in the contextual bandit setting.

    

### [[2109.05547] Towards a variational Jordan-Lee-Preskill quantum algorithm](http://arxiv.org/abs/2109.05547)


  Rapid developments of quantum information technology show promising
opportunities for simulating quantum field theory in near-term quantum devices.
In this work, we formulate the theory of (time-dependent) variational quantum
simulation, explicitly designed for quantum simulation of quantum field theory.
We develop hybrid quantum-classical algorithms for crucial ingredients in
particle scattering experiments, including encoding, state preparation, and
time evolution, with several numerical simulations to demonstrate our
algorithms in the 1+1 dimensional $\lambda \phi^4$ quantum field theory. These
algorithms could be understood as near-term analogs of the Jordan-Lee-Preskill
algorithm, the basic algorithm for simulating quantum field theory using
universal quantum devices. Our contribution also includes a bosonic version of
the Unitary Coupled Cluster ansatz with physical interpretation in quantum
field theory, a discussion about the subspace fidelity, a comparison among
different bases in the 1+1 dimensional $\lambda \phi^4$ theory, and the
"spectral crowding" in the quantum field theory simulation.

    

### [[2109.05549] Federated Ensemble Model-based Reinforcement Learning](http://arxiv.org/abs/2109.05549)


  Federated learning (FL) is a privacy-preserving machine learning paradigm
that enables collaborative training among geographically distributed and
heterogeneous users without gathering their data. Extending FL beyond the
conventional supervised learning paradigm, federated Reinforcement Learning
(RL) was proposed to handle sequential decision-making problems for various
privacy-sensitive applications such as autonomous driving. However, the
existing federated RL algorithms directly combine model-free RL with FL, and
thus generally have high sample complexity and lack theoretical guarantees. To
address the above challenges, we propose a new federated RL algorithm that
incorporates model-based RL and ensemble knowledge distillation into FL.
Specifically, we utilise FL and knowledge distillation to create an ensemble of
dynamics models from clients, and then train the policy by solely using the
ensemble model without interacting with the real environment. Furthermore, we
theoretically prove that the monotonic improvement of the proposed algorithm is
guaranteed. Extensive experimental results demonstrate that our algorithm
obtains significantly higher sample efficiency compared to federated model-free
RL algorithms in the challenging continuous control benchmark environments. The
results also show the impact of non-IID client data and local update steps on
the performance of federated RL, validating the insights obtained from our
theoretical analysis.

    

### [[2109.05554] No True State-of-the-Art? OOD Detection Methods are Inconsistent across Datasets](http://arxiv.org/abs/2109.05554)


  Out-of-distribution detection is an important component of reliable ML
systems. Prior literature has proposed various methods (e.g., MSP (Hendrycks &
Gimpel, 2017), ODIN (Liang et al., 2018), Mahalanobis (Lee et al., 2018)),
claiming they are state-of-the-art by showing they outperform previous methods
on a selected set of in-distribution (ID) and out-of-distribution (OOD)
datasets. In this work, we show that none of these methods are inherently
better at OOD detection than others on a standardized set of 16 (ID, OOD)
pairs. We give possible explanations for these inconsistencies with simple toy
datasets where whether one method outperforms another depends on the structure
of the ID and OOD datasets in question. Finally, we show that a method
outperforming another on a certain (ID, OOD) pair may not do so in a low-data
regime. In the low-data regime, we propose a distance-based method, Pairwise
OOD detection (POD), which is based on Siamese networks and improves over
Mahalanobis by sidestepping the expensive covariance estimation step. Our
results suggest that the OOD detection problem may be too broad, and we should
consider more specific structures for leverage.

    

### [[2109.05558] CoG: a Two-View Co-training Framework for Defending Adversarial Attacks on Graph](http://arxiv.org/abs/2109.05558)


  Graph neural networks exhibit remarkable performance in graph data analysis.
However, the robustness of GNN models remains a challenge. As a result, they
are not reliable enough to be deployed in critical applications. Recent studies
demonstrate that GNNs could be easily fooled with adversarial perturbations,
especially structural perturbations. Such vulnerability is attributed to the
excessive dependence on the structure information to make predictions. To
achieve better robustness, it is desirable to build the prediction of GNNs with
more comprehensive features. Graph data, in most cases, has two views of
information, namely structure information and feature information. In this
paper, we propose CoG, a simple yet effective co-training framework to combine
these two views for the purpose of robustness. CoG trains sub-models from the
feature view and the structure view independently and allows them to distill
knowledge from each other by adding their most confident unlabeled data into
the training set. The orthogonality of these two views diversifies the
sub-models, thus enhancing the robustness of their ensemble. We evaluate our
framework on three popular datasets, and results show that CoG significantly
improves the robustness of graph models against adversarial attacks without
sacrificing their performance on clean data. We also show that CoG still
achieves good robustness when both node features and graph structures are
perturbed.

    

### [[2109.05565] SphereFace Revived: Unifying Hyperspherical Face Recognition](http://arxiv.org/abs/2109.05565)


  This paper addresses the deep face recognition problem under an open-set
protocol, where ideal face features are expected to have smaller maximal
intra-class distance than minimal inter-class distance under a suitably chosen
metric space. To this end, hyperspherical face recognition, as a promising line
of research, has attracted increasing attention and gradually become a major
focus in face recognition research. As one of the earliest works in
hyperspherical face recognition, SphereFace explicitly proposed to learn face
embeddings with large inter-class angular margin. However, SphereFace still
suffers from severe training instability which limits its application in
practice. In order to address this problem, we introduce a unified framework to
understand large angular margin in hyperspherical face recognition. Under this
framework, we extend the study of SphereFace and propose an improved variant
with substantially better training stability -- SphereFace-R. Specifically, we
propose two novel ways to implement the multiplicative margin, and study
SphereFace-R under three different feature normalization schemes (no feature
normalization, hard feature normalization and soft feature normalization). We
also propose an implementation strategy -- "characteristic gradient detachment"
-- to stabilize training. Extensive experiments on SphereFace-R show that it is
consistently better than or competitive with state-of-the-art methods.

    

### [[2109.05574] Machine Learning for Naval Architecture, Ocean and Marine Engineering](http://arxiv.org/abs/2109.05574)


  Machine Learning (ML) based algorithms have found significant impact in many
fields of engineering and sciences, where datasets are available from
experiments and high fidelity numerical simulations. Those datasets are
generally utilized in a machine learning model to extract information about the
underlying physics and derive functional relationships mapping input variables
to target quantities of interest. Commonplace machine learning algorithms
utilized in Scientific Machine Learning (SciML) include neural networks,
regression trees, random forests, support vector machines, etc. The focus of
this article is to review the applications of ML in naval architecture, ocean,
and marine engineering problems; and identify priority directions of research.
We discuss the applications of machine learning algorithms for different
problems such as wave height prediction, calculation of wind loads on ships,
damage detection of offshore platforms, calculation of ship added resistance,
and various other applications in coastal and marine environments. The details
of the data sets including the source of data-sets utilized in the ML model
development are included. The features used as the inputs to the ML models are
presented in detail and finally, the methods employed in optimization of the ML
models were also discussed. Based on this comprehensive analysis we point out
future directions of research that may be fruitful for the application of ML to
the ocean and marine engineering problems.

    

### [[2109.05578] Kernel PCA with the Nyström method](http://arxiv.org/abs/2109.05578)


  Kernel methods are powerful but computationally demanding techniques for
non-linear learning. A popular remedy, the Nyström method has been shown to
be able to scale up kernel methods to very large datasets with little loss in
accuracy. However, kernel PCA with the Nyström method has not been widely
studied. In this paper we derive kernel PCA with the Nyström method and study
its accuracy, providing a finite-sample confidence bound on the difference
between the Nyström and standard empirical reconstruction errors. The
behaviours of the method and bound are illustrated through extensive computer
experiments on real-world data. As an application of the method we present
kernel principal component regression with the Nyström method.

    

### [[2109.05580] A Joint Graph and Image Convolution Network for Automatic Brain Tumor Segmentation](http://arxiv.org/abs/2109.05580)


  We present a joint graph convolution-image convolution neural network as our
submission to the Brain Tumor Segmentation (BraTS) 2021 challenge. We model
each brain as a graph composed of distinct image regions, which is initially
segmented by a graph neural network (GNN). Subsequently, the tumorous volume
identified by the GNN is further refined by a simple (voxel) convolutional
neural network (CNN), which produces the final segmentation. This approach
captures both global brain feature interactions via the graphical
representation and local image details through the use of convolutional
filters. We find that the GNN component by itself can effectively identify and
segment the brain tumors. The addition of the CNN further improves the median
performance of the model by 2 percent across all metrics evaluated. On the
validation set, our joint GNN-CNN model achieves mean Dice scores of 0.89,
0.81, 0.73 and mean Hausdorff distances (95th percentile) of 6.8, 12.6, 28.2mm
on the whole tumor, core tumor, and enhancing tumor, respectively.

    

### [[2109.05581] Data Analytics for Smart cities: Challenges and Promises](http://arxiv.org/abs/2109.05581)


  The explosion of advancements in artificial intelligence, sensor
technologies, and wireless communication activates ubiquitous sensing through
distributed sensors. These sensors are various domains of networks that lead us
to smart systems in healthcare, transportation, environment, and other relevant
branches/networks. Having collaborative interaction among the smart systems
connects end-user devices to each other which enables achieving a new
integrated entity called Smart Cities. The goal of this study is to provide a
comprehensive survey of data analytics in smart cities. In this paper, we aim
to focus on one of the smart cities important branches, namely Smart Mobility,
and its positive ample impact on the smart cities decision-making process.
Intelligent decision-making systems in smart mobility offer many advantages
such as saving energy, relaying city traffic, and more importantly, reducing
air pollution by offering real-time useful information and imperative
knowledge. Making a decision in smart cities in time is challenging due to
various and high dimensional factors and parameters, which are not frequently
collected. In this paper, we first address current challenges in smart cities
and provide an overview of potential solutions to these challenges. Then, we
offer a framework of these solutions, called universal smart cities decision
making, with three main sections of data capturing, data analysis, and decision
making to optimize the smart mobility within smart cities. With this framework,
we elaborate on fundamental concepts of big data, machine learning, and deep
leaning algorithms that have been applied to smart cities and discuss the role
of these algorithms in decision making for smart mobility in smart cities.

    

### [[2109.05583] Automatic Componentwise Boosting: An Interpretable AutoML System](http://arxiv.org/abs/2109.05583)


  In practice, machine learning (ML) workflows require various different steps,
from data preprocessing, missing value imputation, model selection, to model
tuning as well as model evaluation. Many of these steps rely on human ML
experts. AutoML - the field of automating these ML pipelines - tries to help
practitioners to apply ML off-the-shelf without any expert knowledge. Most
modern AutoML systems like auto-sklearn, H20-AutoML or TPOT aim for high
predictive performance, thereby generating ensembles that consist almost
exclusively of black-box models. This, in turn, makes the interpretation for
the layperson more intricate and adds another layer of opacity for users. We
propose an AutoML system that constructs an interpretable additive model that
can be fitted using a highly scalable componentwise boosting algorithm. Our
system provides tools for easy model interpretation such as visualizing partial
effects and pairwise interactions, allows for a straightforward calculation of
feature importance, and gives insights into the required model complexity to
fit the given task. We introduce the general framework and outline its
implementation autocompboost. To demonstrate the frameworks efficacy, we
compare autocompboost to other existing systems based on the OpenML
AutoML-Benchmark. Despite its restriction to an interpretable model space, our
system is competitive in terms of predictive performance on most data sets
while being more user-friendly and transparent.

    

### [[2109.05587] On the Efficiency of Subclass Knowledge Distillation in Classification Tasks](http://arxiv.org/abs/2109.05587)


  This work introduces a novel knowledge distillation framework for
classification tasks where information on existing subclasses is available and
taken into consideration. In classification tasks with a small number of
classes or binary detection (two classes) the amount of information transferred
from the teacher to the student network is restricted, thus limiting the
utility of knowledge distillation. Performance can be improved by leveraging
information about possible subclasses within the available classes in the
classification task. To that end, we propose the so-called Subclass Knowledge
Distillation (SKD) framework, which is the process of transferring the
subclasses' prediction knowledge from a large teacher model into a smaller
student one. Through SKD, additional meaningful information which is not in the
teacher's class logits but exists in subclasses (e.g., similarities inside
classes) will be conveyed to the student and boost its performance.
Mathematically, we measure how many extra information bits the teacher can
provide for the student via SKD framework. The framework developed is evaluated
in clinical application, namely colorectal polyp binary classification. In this
application, clinician-provided annotations are used to define subclasses based
on the annotation label's variability in a curriculum style of learning. A
lightweight, low complexity student trained with the proposed framework
achieves an F1-score of 85.05%, an improvement of 2.14% and 1.49% gain over the
student that trains without and with conventional knowledge distillation,
respectively. These results show that the extra subclasses' knowledge (i.e.,
0.4656 label bits per training sample in our experiment) can provide more
information about the teacher generalization, and therefore SKD can benefit
from using more information to increase the student performance.

    

### [[2109.05594] Detecting Handwritten Mathematical Terms with Sensor Based Data](http://arxiv.org/abs/2109.05594)


  In this work we propose a solution to the UbiComp 2021 Challenge by Stabilo
in which handwritten mathematical terms are supposed to be automatically
classified based on time series sensor data captured on the DigiPen. The input
data set contains data of different writers, with label strings constructed
from a total of 15 different possible characters. The label should first be
split into separate characters to classify them one by one. This issue is
solved by applying a data-dependant and rule-based information extraction
algorithm to the labeled data. Using the resulting data, two classifiers are
constructed. The first is a binary classifier that is able to predict, for
unknown data, if a sample is part of a writing activity, and consists of a Deep
Neural Network feature extractor in concatenation with a Random Forest that is
trained to classify the extracted features at an F1 score of >90%. The second
classifier is a Deep Neural Network that combines convolution layers with
recurrent layers to predict windows with a single label, out of the 15 possible
classes, at an F1 score of >60%. A simulation of the challenge evaluation
procedure reports a Levensthein Distance of 8 and shows that the chosen
approach still lacks in overall accuracy and real-time applicability.

    

### [[2109.05598] Neural network based order parameter for phase transitions and its applications in high-entropy alloys](http://arxiv.org/abs/2109.05598)


  Phase transition is one of the most important phenomena in nature and plays a
central role in materials design. All phase transitions are characterized by
suitable order parameters, including the order-disorder phase transition.
However, finding a representative order parameter for complex systems is
nontrivial, such as for high-entropy alloys. Given variational autoencoder's
(VAE) strength of reducing high dimensional data into few principal components,
here we coin a new concept of "VAE order parameter". We propose that the
Manhattan distance in the VAE latent space can serve as a generic order
parameter for order-disorder phase transitions. The physical properties of the
order parameter are quantitatively interpreted and demonstrated by multiple
refractory high-entropy alloys. Assisted by it, a generally applicable alloy
design concept is proposed by mimicking the nature mixing of elements. Our
physically interpretable "VAE order parameter" lays the foundation for the
understanding of and alloy design by chemical ordering.

    

### [[2109.05604] Direct Random Search for Fine Tuning of Deep Reinforcement Learning Policies](http://arxiv.org/abs/2109.05604)


  Researchers have demonstrated that Deep Reinforcement Learning (DRL) is a
powerful tool for finding policies that perform well on complex robotic
systems. However, these policies are often unpredictable and can induce highly
variable behavior when evaluated with only slightly different initial
conditions. Training considerations constrain DRL algorithm designs in that
most algorithms must use stochastic policies during training. The resulting
policy used during deployment, however, can and frequently is a deterministic
one that uses the Maximum Likelihood Action (MLA) at each step. In this work,
we show that a direct random search is very effective at fine-tuning DRL
policies by directly optimizing them using deterministic rollouts. We
illustrate this across a large collection of reinforcement learning
environments, using a wide variety of policies obtained from different
algorithms. Our results show that this method yields more consistent and higher
performing agents on the environments we tested. Furthermore, we demonstrate
how this method can be used to extend our previous work on shrinking the
dimensionality of the reachable state space of closed-loop systems run under
Deep Neural Network (DNN) policies.

    

### [[2109.05612] FedTriNet: A Pseudo Labeling Method with Three Players for Federated Semi-supervised Learning](http://arxiv.org/abs/2109.05612)


  Federated Learning has shown great potentials for the distributed data
utilization and privacy protection. Most existing federated learning approaches
focus on the supervised setting, which means all the data stored in each client
has labels. However, in real-world applications, the client data are impossible
to be fully labeled. Thus, how to exploit the unlabeled data should be a new
challenge for federated learning. Although a few studies are attempting to
overcome this challenge, they may suffer from information leakage or misleading
information usage problems. To tackle these issues, in this paper, we propose a
novel federated semi-supervised learning method named FedTriNet, which consists
of two learning phases. In the first phase, we pre-train FedTriNet using
labeled data with FedAvg. In the second phase, we aim to make most of the
unlabeled data to help model learning. In particular, we propose to use three
networks and a dynamic quality control mechanism to generate high-quality
pseudo labels for unlabeled data, which are added to the training set. Finally,
FedTriNet uses the new training set to retrain the model. Experimental results
on three publicly available datasets show that the proposed FedTriNet
outperforms state-of-the-art baselines under both IID and Non-IID settings.

    

### [[2109.05613] Critical Learning Periods in Federated Learning](http://arxiv.org/abs/2109.05613)


  Federated learning (FL) is a popular technique to train machine learning (ML)
models with decentralized data. Extensive works have studied the performance of
the global model; however, it is still unclear how the training process affects
the final test accuracy. Exacerbating this problem is the fact that FL
executions differ significantly from traditional ML with heterogeneous data
characteristics across clients, involving more hyperparameters. In this work,
we show that the final test accuracy of FL is dramatically affected by the
early phase of the training process, i.e., FL exhibits critical learning
periods, in which small gradient errors can have irrecoverable impact on the
final test accuracy. To further explain this phenomenon, we generalize the
trace of the Fisher Information Matrix (FIM) to FL and define a new notion
called FedFIM, a quantity reflecting the local curvature of each clients from
the beginning of the training in FL. Our findings suggest that the {\em initial
learning phase} plays a critical role in understanding the FL performance. This
is in contrast to many existing works which generally do not connect the final
accuracy of FL to the early phase training. Finally, seizing critical learning
periods in FL is of independent interest and could be useful for other problems
such as the choices of hyperparameters such as the number of client selected
per round, batch size, and more, so as to improve the performance of FL
training and testing.

    

### [[2109.05629] AdViCE: Aggregated Visual Counterfactual Explanations for Machine Learning Model Validation](http://arxiv.org/abs/2109.05629)


  Rapid improvements in the performance of machine learning models have pushed
them to the forefront of data-driven decision-making. Meanwhile, the increased
integration of these models into various application domains has further
highlighted the need for greater interpretability and transparency. To identify
problems such as bias, overfitting, and incorrect correlations, data scientists
require tools that explain the mechanisms with which these model decisions are
made. In this paper we introduce AdViCE, a visual analytics tool that aims to
guide users in black-box model debugging and validation. The solution rests on
two main visual user interface innovations: (1) an interactive visualization
design that enables the comparison of decisions on user-defined data subsets;
(2) an algorithm and visual design to compute and visualize counterfactual
explanations - explanations that depict model outcomes when data features are
perturbed from their original values. We provide a demonstration of the tool
through a use case that showcases the capabilities and potential limitations of
the proposed approach.

    

### [[2109.05633] Generating Datasets of 3D Garments with Sewing Patterns](http://arxiv.org/abs/2109.05633)


  Garments are ubiquitous in both real and many of the virtual worlds. They are
highly deformable objects, exhibit an immense variety of designs and shapes,
and yet, most garments are created from a set of regularly shaped flat pieces.
Exploration of garment structure presents a peculiar case for an object
structure estimation task and might prove useful for downstream tasks of neural
3D garment modeling and reconstruction by providing strong prior on garment
shapes. To facilitate research in these directions, we propose a method for
generating large synthetic datasets of 3D garment designs and their sewing
patterns. Our method consists of a flexible description structure for
specifying parametric sewing pattern templates and the automatic generation
pipeline to produce garment 3D models with little-to-none manual intervention.
To add realism, the pipeline additionally creates corrupted versions of the
final meshes that imitate artifacts of 3D scanning.
With this pipeline, we created the first large-scale synthetic dataset of 3D
garment models with their sewing patterns. The dataset contains more than 20000
garment design variations produced from 19 different base types. Seven of these
garment types are specifically designed to target evaluation of the
generalization across garment sewing pattern topologies.

    

### [[2109.05635] Mixing between the Cross Entropy and the Expectation Loss Terms](http://arxiv.org/abs/2109.05635)


  The cross entropy loss is widely used due to its effectiveness and solid
theoretical grounding. However, as training progresses, the loss tends to focus
on hard to classify samples, which may prevent the network from obtaining gains
in performance. While most work in the field suggest ways to classify hard
negatives, we suggest to strategically leave hard negatives behind, in order to
focus on misclassified samples with higher probabilities. We show that adding
to the optimization goal the expectation loss, which is a better approximation
of the zero-one loss, helps the network to achieve better accuracy. We,
therefore, propose to shift between the two losses during training, focusing
more on the expectation loss gradually during the later stages of training. Our
experiments show that the new training protocol improves performance across a
diverse set of classification domains, including computer vision, natural
language processing, tabular data, and sequences. Our code and scripts are
available at supplementary.

    

### [[2109.05641] Is Heterophily A Real Nightmare For Graph Neural Networks To Do Node Classification?](http://arxiv.org/abs/2109.05641)


  Graph Neural Networks (GNNs) extend basic Neural Networks (NNs) by using the
graph structures based on the relational inductive bias (homophily assumption).
Though GNNs are believed to outperform NNs in real-world tasks, performance
advantages of GNNs over graph-agnostic NNs seem not generally satisfactory.
Heterophily has been considered as a main cause and numerous works have been
put forward to address it. In this paper, we first show that not all cases of
heterophily are harmful for GNNs with aggregation operation. Then, we propose
new metrics based on a similarity matrix which considers the influence of both
graph structure and input features on GNNs. The metrics demonstrate advantages
over the commonly used homophily metrics by tests on synthetic graphs. From the
metrics and the observations, we find some cases of harmful heterophily can be
addressed by diversification operation. With this fact and knowledge of
filterbanks, we propose the Adaptive Channel Mixing (ACM) framework to
adaptively exploit aggregation, diversification and identity channels in each
GNN layer to address harmful heterophily. We validate the ACM-augmented
baselines with 10 real-world node classification tasks. They consistently
achieve significant performance gain and exceed the state-of-the-art GNNs on
most of the tasks without incurring significant computational burden.

    

### [[1712.10024] Debiased Machine Learning of Set-Identified Linear Models](http://arxiv.org/abs/1712.10024)


  This paper provides estimation and inference methods for an identified set's
boundary (i.e., support function) where the selection among a very large number
of covariates is based on modern regularized tools. I characterize the boundary
using a semiparametric moment equation. Combining Neyman-orthogonality and
sample splitting ideas, I construct a root-N consistent, uniformly
asymptotically Gaussian estimator of the boundary and propose a multiplier
bootstrap procedure to conduct inference. I apply this result to the partially
linear model and the partially linear IV model with an interval-valued outcome.

    

### [[1804.02969] A review of possible effects of cognitive biases on the interpretation of rule-based machine learning models](http://arxiv.org/abs/1804.02969)


  While the interpretability of machine learning models is often equated with
their mere syntactic comprehensibility, we think that interpretability goes
beyond that, and that human interpretability should also be investigated from
the point of view of cognitive science. The goal of this paper is to discuss to
what extent cognitive biases may affect human understanding of interpretable
machine learning models, in particular of logical rules discovered from data.
Twenty cognitive biases are covered, as are possible debiasing techniques that
can be adopted by designers of machine learning algorithms and software. Our
review transfers results obtained in cognitive psychology to the domain of
machine learning, aiming to bridge the current gap between these two areas. It
needs to be followed by empirical studies specifically focused on the machine
learning domain.

    

### [[1806.04823] Regularized Orthogonal Machine Learning for Nonlinear Semiparametric Models](http://arxiv.org/abs/1806.04823)


  This paper proposes a Lasso-type estimator for a high-dimensional sparse
parameter identified by a single index conditional moment restriction (CMR). In
addition to this parameter, the moment function can also depend on a nuisance
function, such as the propensity score or the conditional choice probability,
which we estimate by modern machine learning tools. We first adjust the moment
function so that the gradient of the future loss function is insensitive
(formally, Neyman-orthogonal) with respect to the first-stage regularization
bias, preserving the single index property. We then take the loss function to
be an indefinite integral of the adjusted moment function with respect to the
single index. The proposed Lasso estimator converges at the oracle rate, where
the oracle knows the nuisance function and solves only the parametric problem.
We demonstrate our method by estimating the short-term heterogeneous impact of
Connecticut's Jobs First welfare reform experiment on women's welfare
participation decision.

    

### [[1904.06366] Visualization of Labeled Mixed-featured Datasets](http://arxiv.org/abs/1904.06366)


  We develop methodology for visualization of labeled mixed-featured datasets.
We first investigate datasets with continuous features where our Max-Ratio
Projection (MRP) method utilizes the group information in high dimensions to
provide distinctive lower-dimensional projections that are then displayed using
Radviz3D. Our methodology is extended to datasets with discrete and continuous
features where a Gaussianized distributional transform is used in conjunction
with copula models before applying MRP and visualizing the result using
RadViz3D. A R package $radviz3d$ implementing our complete methodology is
available.

    

### [[1905.03927] Generalized Second Order Value Iteration in Markov Decision Processes](http://arxiv.org/abs/1905.03927)


  Value iteration is a fixed point iteration technique utilized to obtain the
optimal value function and policy in a discounted reward Markov Decision
Process (MDP). Here, a contraction operator is constructed and applied
repeatedly to arrive at the optimal solution. Value iteration is a first order
method and therefore it may take a large number of iterations to converge to
the optimal solution. Successive relaxation is a popular technique that can be
applied to solve a fixed point equation. It has been shown in the literature
that, under a special structure of the MDP, successive over-relaxation
technique computes the optimal value function faster than standard value
iteration. In this work, we propose a second order value iteration procedure
that is obtained by applying the Newton-Raphson method to the successive
relaxation value iteration scheme. We prove the global convergence of our
algorithm to the optimal solution asymptotically and show the second order
convergence. Through experiments, we demonstrate the effectiveness of our
proposed approach.

    

### [[1906.00460] On The Radon-Nikodym Spectral Approach With Optimal Clustering](http://arxiv.org/abs/1906.00460)


  Problems of interpolation, classification, and clustering are considered. In
the tenets of Radon--Nikodym approach $\langle f(\mathbf{x})\psi^2 \rangle /
\langle\psi^2\rangle$, where the $\psi(\mathbf{x})$ is a linear function on
input attributes, all the answers are obtained from a generalized eigenproblem
$|f|\psi^{[i]}\rangle = \lambda^{[i]} |\psi^{[i]}\rangle$. The solution to the
interpolation problem is a regular Radon-Nikodym derivative. The solution to
the classification problem requires prior and posterior probabilities that are
obtained using the Lebesgue quadrature[1] technique. Whereas in a Bayesian
approach new observations change only outcome probabilities, in the
Radon-Nikodym approach not only outcome probabilities but also the probability
space $|\psi^{[i]}\rangle$ change with new observations. This is a remarkable
feature of the approach: both the probabilities and the probability space are
constructed from the data. The Lebesgue quadrature technique can be also
applied to the optimal clustering problem. The problem is solved by
constructing a Gaussian quadrature on the Lebesgue measure. A distinguishing
feature of the Radon-Nikodym approach is the knowledge of the invariant group:
all the answers are invariant relatively any non-degenerated linear transform
of input vector $\mathbf{x}$ components. A software product implementing the
algorithms of interpolation, classification, and optimal clustering is
available from the authors.

    

### [[1908.01241] Robust Max Entrywise Error Bounds for Sparse Tensor Estimation via Similarity Based Collaborative Filtering](http://arxiv.org/abs/1908.01241)


  Consider the task of estimating a 3-order $n \times n \times n$ tensor from
noisy observations of randomly chosen entries in the sparse regime. We
introduce a similarity based collaborative filtering algorithm for sparse
tensor estimation and argue that it achieves sample complexity that nearly
matches the conjectured computationally efficient lower bound on the sample
complexity for the setting of low-rank tensors. Our algorithm uses the matrix
obtained from the flattened tensor to compute similarity, and estimates the
tensor entries using a nearest neighbor estimator. We prove that the algorithm
recovers a low rank tensor with maximum entry-wise error (MEE) and
mean-squared-error (MSE) decaying to $0$ as long as each entry is observed
independently with probability $p = \Omega(n^{-3/2 + \kappa})$ for any
arbitrarily small $\kappa > 0$. % as long as tensor has finite rank $r =
\Theta(1)$. More generally, we establish robustness of the estimator, showing
that when arbitrary noise bounded by $\epsilon \geq 0$ is added to each
observation, the estimation error with respect to MEE and MSE degrades by ${\sf
poly}(\epsilon)$. Consequently, even if the tensor may not have finite rank but
can be approximated within $\epsilon \geq 0$ by a finite rank tensor, then the
estimation error converges to ${\sf poly}(\epsilon)$. Our analysis sheds
insight into the conjectured sample complexity lower bound, showing that it
matches the connectivity threshold of the graph used by our algorithm for
estimating similarity between coordinates.

    

### [[1908.03918] DynaNet: Neural Kalman Dynamical Model for Motion Estimation and Prediction](http://arxiv.org/abs/1908.03918)


  Dynamical models estimate and predict the temporal evolution of physical
systems. State Space Models (SSMs) in particular represent the system dynamics
with many desirable properties, such as being able to model uncertainty in both
the model and measurements, and optimal (in the Bayesian sense) recursive
formulations e.g. the Kalman Filter. However, they require significant domain
knowledge to derive the parametric form and considerable hand-tuning to
correctly set all the parameters. Data driven techniques e.g. Recurrent Neural
Networks have emerged as compelling alternatives to SSMs with wide success
across a number of challenging tasks, in part due to their ability to extract
relevant features from rich inputs. They however lack interpretability and
robustness to unseen conditions. In this work, we present DynaNet, a hybrid
deep learning and time-varying state-space model which can be trained
end-to-end. Our neural Kalman dynamical model allows us to exploit the relative
merits of each approach. We demonstrate state-of-the-art estimation and
prediction on a number of physically challenging tasks, including visual
odometry, sensor fusion for visual-inertial navigation and pendulum control. In
addition we show how DynaNet can indicate failures through investigation of
properties such as the rate of innovation (Kalman Gain).

    

### [[1909.07815] Particle reconstruction of volumetric particle image velocimetry with strategy of machine learning](http://arxiv.org/abs/1909.07815)


  Three-dimensional particle reconstruction with limited two-dimensional
projections is an under-determined inverse problem that the exact solution is
often difficult to be obtained. In general, approximate solutions can be
obtained by iterative optimization methods. In the current work, a practical
particle reconstruction method based on a convolutional neural network (CNN)
with geometry-informed features is proposed. The proposed technique can refine
the particle reconstruction from a very coarse initial guess of particle
distribution generated by any traditional algebraic reconstruction technique
(ART) based methods. Compared with available ART-based algorithms, the novel
technique makes significant improvements in terms of reconstruction quality,
{robustness to noises}, and at least an order of magnitude faster in the
offline stage.

    

### [[1910.05862] Constrained Non-Affine Alignment of Embeddings](http://arxiv.org/abs/1910.05862)


  Embeddings are one of the fundamental building blocks for data analysis
tasks. Embeddings are already essential tools for large language models and
image analysis, and their use is being extended to many other research domains.
The generation of these distributed representations is often a data- and
computation-expensive process; yet the holistic analysis and adjustment of them
after they have been created is still a developing area. In this paper, we
first propose a very general quantitatively measure for the presence of
features in the embedding data based on if it can be learned. We then devise a
method to remove or alleviate undesired features in the embedding while
retaining the essential structure of the data. We use a Domain Adversarial
Network (DAN) to generate a non-affine transformation, but we add constraints
to ensure the essential structure of the embedding is preserved. Our empirical
results demonstrate that the proposed algorithm significantly outperforms the
state-of-art unsupervised algorithm on several data sets, including novel
applications from the industry.

    

### [[1911.02728] Auto-encoding brain networks with applications to analyzing large-scale brain imaging datasets](http://arxiv.org/abs/1911.02728)


  There has been huge interest in studying human brain connectomes inferred
from different imaging modalities and exploring their relationship with human
traits, such as cognition. Brain connectomes are usually represented as
networks, with nodes corresponding to different regions of interest (ROIs) and
edges to connection strengths between ROIs. Due to the high-dimensionality and
non-Euclidean nature of networks, it is challenging to depict their population
distribution and relate them to human traits. Current approaches focus on
summarizing the network using either pre-specified topological features or
principal components analysis (PCA). In this paper, building on recent advances
in deep learning, we develop a nonlinear latent factor model to characterize
the population distribution of brain graphs and infer the relationships between
brain structural connectomes and human traits. We refer to our method as Graph
AuTo-Encoding (GATE). We applied GATE to two large-scale brain imaging
datasets, the Adolescent Brain Cognitive Development (ABCD) study and the Human
Connectome Project (HCP) for adults, to understand the structural brain
connectome and its relationship with cognition. Numerical results demonstrate
huge advantages of GATE over competitors in terms of prediction accuracy,
statistical inference and computing efficiency. We found that structural
connectomes have a stronger association with a wide range of human cognitive
traits than was apparent using previous approaches.

    

### [[2003.02320] Knowledge Graphs](http://arxiv.org/abs/2003.02320)


  In this paper we provide a comprehensive introduction to knowledge graphs,
which have recently garnered significant attention from both industry and
academia in scenarios that require exploiting diverse, dynamic, large-scale
collections of data. After some opening remarks, we motivate and contrast
various graph-based data models and query languages that are used for knowledge
graphs. We discuss the roles of schema, identity, and context in knowledge
graphs. We explain how knowledge can be represented and extracted using a
combination of deductive and inductive techniques. We summarise methods for the
creation, enrichment, quality assessment, refinement, and publication of
knowledge graphs. We provide an overview of prominent open knowledge graphs and
enterprise knowledge graphs, their applications, and how they use the
aforementioned techniques. We conclude with high-level future research
directions for knowledge graphs.

    

### [[2004.14545] Explainable Deep Learning: A Field Guide for the Uninitiated](http://arxiv.org/abs/2004.14545)


  Deep neural networks (DNNs) have become a proven and indispensable machine
learning tool. As a black-box model, it remains difficult to diagnose what
aspects of the model's input drive the decisions of a DNN. In countless
real-world domains, from legislation and law enforcement to healthcare, such
diagnosis is essential to ensure that DNN decisions are driven by aspects
appropriate in the context of its use. The development of methods and studies
enabling the explanation of a DNN's decisions has thus blossomed into an
active, broad area of research. A practitioner wanting to study explainable
deep learning may be intimidated by the plethora of orthogonal directions the
field has taken. This complexity is further exacerbated by competing
definitions of what it means ``to explain'' the actions of a DNN and to
evaluate an approach's ``ability to explain''. This article offers a field
guide to explore the space of explainable deep learning aimed at those
uninitiated in the field. The field guide: i) Introduces three simple
dimensions defining the space of foundational methods that contribute to
explainable deep learning, ii) discusses the evaluations for model
explanations, iii) places explainability in the context of other related deep
learning research areas, and iv) finally elaborates on user-oriented
explanation designing and potential future directions on explainable deep
learning. We hope the guide is used as an easy-to-digest starting point for
those just embarking on research in this field.

    

### [[2005.05276] CupNet -- Pruning a network for geometric data](http://arxiv.org/abs/2005.05276)


  Using data from a simulated cup drawing process, we demonstrate how the
inherent geometrical structure of cup meshes can be used to effectively prune
an artificial neural network in a straightforward way.

    

### [[2005.05889] Upper Bounds on the Generalization Error of Private Algorithms for Discrete Data](http://arxiv.org/abs/2005.05889)


  In this work, we study the generalization capability of algorithms from an
information-theoretic perspective. It has been shown that the expected
generalization error of an algorithm is bounded from above by a function of the
relative entropy between the conditional probability distribution of the
algorithm's output hypothesis, given the dataset with which it was trained, and
its marginal probability distribution. We build upon this fact and introduce a
mathematical formulation to obtain upper bounds on this relative entropy.
Assuming that the data is discrete, we then develop a strategy using this
formulation, based on the method of types and typicality, to find explicit
upper bounds on the generalization error of stable algorithms, i.e., algorithms
that produce similar output hypotheses given similar input datasets. In
particular, we show the bounds obtained with this strategy for the case of
$\epsilon$-DP and $\mu$-GDP algorithms.

    

### [[2006.04836] A Modified AUC for Training Convolutional Neural Networks: Taking Confidence into Account](http://arxiv.org/abs/2006.04836)


  Receiver operating characteristic (ROC) curve is an informative tool in
binary classification and Area Under ROC Curve (AUC) is a popular metric for
reporting performance of binary classifiers. In this paper, first we present a
comprehensive review of ROC curve and AUC metric. Next, we propose a modified
version of AUC that takes confidence of the model into account and at the same
time, incorporates AUC into Binary Cross Entropy (BCE) loss used for training a
Convolutional neural Network for classification tasks. We demonstrate this on
three datasets: MNIST, prostate MRI, and brain MRI. Furthermore, we have
published GenuineAI, a new python library, which provides the functions for
conventional AUC and the proposed modified AUC along with metrics including
sensitivity, specificity, recall, precision, and F1 for each point of the ROC
curve.

    

### [[2006.07107] Understanding and Resolving Performance Degradation in Graph Convolutional Networks](http://arxiv.org/abs/2006.07107)


  A Graph Convolutional Network (GCN) stacks several layers and in each layer
performs a PROPagation operation (PROP) and a TRANsformation operation (TRAN)
for learning node representations over graph-structured data. Though powerful,
GCNs tend to suffer performance drop when the model gets deep. Previous works
focus on PROPs to study and mitigate this issue, but the role of TRANs is
barely investigated. In this work, we study performance degradation of GCNs by
experimentally examining how stacking only TRANs or PROPs works. We find that
TRANs contribute significantly, or even more than PROPs, to declining
performance, and moreover that they tend to amplify node-wise feature variance
in GCNs, causing variance inflammation that we identify as a key factor for
causing performance drop. Motivated by such observations, we propose a
variance-controlling technique termed Node Normalization (NodeNorm), which
scales each node's features using its own standard deviation. Experimental
results validate the effectiveness of NodeNorm on addressing performance
degradation of GCNs. Specifically, it enables deep GCNs to outperform shallow
ones in cases where deep models are needed, and to achieve comparable results
with shallow ones on 6 benchmark datasets. NodeNorm is a generic plug-in and
can well generalize to other GNN architectures. Code is publicly available at
this https URL.

    

### [[2006.08767] Systematic Generalisation through Task Temporal Logic and Deep Reinforcement Learning](http://arxiv.org/abs/2006.08767)


  This work introduces a neuro-symbolic agent that combines deep reinforcement
learning (DRL) with temporal logic (TL) to achieve systematic zero-shot, i.e.,
never-seen-before, generalisation of formally specified instructions. In
particular, we present a neuro-symbolic framework where a symbolic module
transforms TL specifications into a form that helps the training of a DRL agent
targeting generalisation, while a neural module learns systematically to solve
the given tasks. We study the emergence of systematic learning in different
settings and find that the architecture of the convolutional layers is key when
generalising to new instructions. We also provide evidence that systematic
learning can emerge with abstract operators such as negation when learning from
a few training examples, which previous research have struggled with.

    

### [[2007.00049] OSCaR: Orthogonal Subspace Correction and Rectification of Biases in Word Embeddings](http://arxiv.org/abs/2007.00049)


  Language representations are known to carry stereotypical biases and, as a
result, lead to biased predictions in downstream tasks. While existing methods
are effective at mitigating biases by linear projection, such methods are too
aggressive: they not only remove bias, but also erase valuable information from
word embeddings. We develop new measures for evaluating specific information
retention that demonstrate the tradeoff between bias removal and information
retention. To address this challenge, we propose OSCaR (Orthogonal Subspace
Correction and Rectification), a bias-mitigating method that focuses on
disentangling biased associations between concepts instead of removing concepts
wholesale. Our experiments on gender biases show that OSCaR is a well-balanced
approach that ensures that semantic information is retained in the embeddings
and bias is also effectively mitigated.

    

### [[2007.00736] Tensor Estimation with Nearly Linear Samples Given Weak Side Information](http://arxiv.org/abs/2007.00736)


  Tensor completion exhibits an interesting computational-statistical gap in
terms of the number of samples needed to perform tensor estimation. While there
are only $\Theta(tn)$ degrees of freedom in a $t$-order tensor with $n^t$
entries, the best known polynomial time algorithm requires $O(n^{t/2})$ samples
in order to guarantee consistent estimation. In this paper, we show that weak
side information is sufficient to reduce the sample complexity to $O(n)$. The
side information consists of a weight vector for each of the modes which is not
orthogonal to any of the latent factors along that mode; this is significantly
weaker than assuming noisy knowledge of the subspaces. We provide an algorithm
that utilizes this side information to produce a consistent estimator with
$O(n^{1+\kappa})$ samples for any small constant $\kappa > 0$.

    

### [[2007.08844] Distribution Aligning Refinery of Pseudo-label for Imbalanced Semi-supervised Learning](http://arxiv.org/abs/2007.08844)


  While semi-supervised learning (SSL) has proven to be a promising way for
leveraging unlabeled data when labeled data is scarce, the existing SSL
algorithms typically assume that training class distributions are balanced.
However, these SSL algorithms trained under imbalanced class distributions can
severely suffer when generalizing to a balanced testing criterion, since they
utilize biased pseudo-labels of unlabeled data toward majority classes. To
alleviate this issue, we formulate a convex optimization problem to softly
refine the pseudo-labels generated from the biased model, and develop a simple
algorithm, named Distribution Aligning Refinery of Pseudo-label (DARP) that
solves it provably and efficiently. Under various class-imbalanced
semi-supervised scenarios, we demonstrate the effectiveness of DARP and its
compatibility with state-of-the-art SSL schemes.

    

### [[2007.12826] The Interpolation Phase Transition in Neural Networks: Memorization and Generalization under Lazy Training](http://arxiv.org/abs/2007.12826)


  Modern neural networks are often operated in a strongly overparametrized
regime: they comprise so many parameters that they can interpolate the training
set, even if actual labels are replaced by purely random ones. Despite this,
they achieve good prediction error on unseen data: interpolating the training
set does not lead to a large generalization error. Further, overparametrization
appears to be beneficial in that it simplifies the optimization landscape. Here
we study these phenomena in the context of two-layers neural networks in the
neural tangent (NT) regime. We consider a simple data model, with isotropic
covariates vectors in $d$ dimensions, and $N$ hidden neurons. We assume that
both the sample size $n$ and the dimension $d$ are large, and they are
polynomially related. Our first main result is a characterization of the
eigenstructure of the empirical NT kernel in the overparametrized regime $Nd\gg
n$. This characterization implies as a corollary that the minimum eigenvalue of
the empirical NT kernel is bounded away from zero as soon as $Nd\gg n$, and
therefore the network can exactly interpolate arbitrary labels in the same
regime.
Our second main result is a characterization of the generalization error of
NT ridge regression including, as a special case, min-$\ell_2$ norm
interpolation. We prove that, as soon as $Nd\gg n$, the test error is well
approximated by the one of kernel ridge regression with respect to the
infinite-width kernel. The latter is in turn well approximated by the error of
polynomial ridge regression, whereby the regularization parameter is increased
by a `self-induced' term related to the high-degree components of the
activation function. The polynomial degree depends on the sample size and the
dimension (in particular on $\log n/\log d$).

    

### [[2008.01846] Stabilizing Deep Tomographic Reconstruction](http://arxiv.org/abs/2008.01846)


  Tomographic image reconstruction with deep learning is an emerging field, but
a recent landmark study reveals that several deep reconstruction networks are
unstable for computed tomography (CT) and magnetic resonance imaging (MRI).
Specifically, three kinds of instabilities were reported: (1) strong image
artefacts from tiny perturbations, (2) small features missing in a deeply
reconstructed image, and (3) decreased imaging performance with increased input
data. On the other hand, compressed sensing (CS) inspired reconstruction
methods do not suffer from these instabilities because of their built-in kernel
awareness. For deep reconstruction to realize its full potential and become a
mainstream approach for tomographic imaging, it is thus critically important to
meet this challenge by stabilizing deep reconstruction networks. Here we
propose an Analytic Compressed Iterative Deep (ACID) framework to address this
challenge. ACID synergizes a deep reconstruction network trained on big data,
kernel awareness from CS-inspired processing, and iterative refinement to
minimize the data residual relative to real measurement. Our study demonstrates
that the deep reconstruction using ACID is accurate and stable, and sheds light
on the converging mechanism of the ACID iteration under a Bounded Relative
Error Norm (BREN) condition. In particular, the study shows that ACID-based
reconstruction is resilient against adversarial attacks, superior to classic
sparsity-regularized reconstruction alone, and eliminates the three kinds of
instabilities. We anticipate that this integrative data-driven approach will
help promote development and translation of deep tomographic image
reconstruction networks into clinical applications.

    

### [[2008.02714] Multi-source Heterogeneous Domain Adaptation with Conditional Weighting Adversarial Network](http://arxiv.org/abs/2008.02714)


  Heterogeneous domain adaptation (HDA) tackles the learning of cross-domain
samples with both different probability distributions and feature
representations. Most of the existing HDA studies focus on the single-source
scenario. In reality, however, it is not uncommon to obtain samples from
multiple heterogeneous domains. In this article, we study the multisource HDA
problem and propose a conditional weighting adversarial network (CWAN) to
address it. The proposed CWAN adversarially learns a feature transformer, a
label classifier, and a domain discriminator. To quantify the importance of
different source domains, CWAN introduces a sophisticated conditional weighting
scheme to calculate the weights of the source domains according to the
conditional distribution divergence between the source and target domains.
Different from existing weighting schemes, the proposed conditional weighting
scheme not only weights the source domains but also implicitly aligns the
conditional distributions during the optimization process. Experimental results
clearly demonstrate that the proposed CWAN performs much better than several
state-of-the-art methods on four real-world datasets.

    

### [[2008.03959] Lenient Regret for Multi-Armed Bandits](http://arxiv.org/abs/2008.03959)


  We consider the Multi-Armed Bandit (MAB) problem, where an agent sequentially
chooses actions and observes rewards for the actions it took. While the
majority of algorithms try to minimize the regret, i.e., the cumulative
difference between the reward of the best action and the agent's action, this
criterion might lead to undesirable results. For example, in large problems, or
when the interaction with the environment is brief, finding an optimal arm is
infeasible, and regret-minimizing algorithms tend to over-explore. To overcome
this issue, algorithms for such settings should instead focus on playing
near-optimal arms. To this end, we suggest a new, more lenient, regret
criterion that ignores suboptimality gaps smaller than some $\epsilon$. We then
present a variant of the Thompson Sampling (TS) algorithm, called
$\epsilon$-TS, and prove its asymptotic optimality in terms of the lenient
regret. Importantly, we show that when the mean of the optimal arm is high
enough, the lenient regret of $\epsilon$-TS is bounded by a constant. Finally,
we show that $\epsilon$-TS can be applied to improve the performance when the
agent knows a lower bound of the suboptimality gaps.

    

### [[2008.05089] Quaternion Graph Neural Networks](http://arxiv.org/abs/2008.05089)


  Recently, graph neural networks (GNNs) have become an important and active
research direction in deep learning. It is worth noting that most of the
existing GNN-based methods learn graph representations within the Euclidean
vector space. Beyond the Euclidean space, learning representation and
embeddings in hyper-complex space have also shown to be a promising and
effective approach. To this end, we propose Quaternion Graph Neural Networks
(QGNN) and Gated Quaternion Graph Neural Networks (GQGNN) to learn graph
representations within the Quaternion space. As demonstrated, the Quaternion
space, a hyper-complex vector space, provides highly meaningful computations
and analogical calculus through Hamilton product compared to the Euclidean and
complex vector spaces. Our QGNN obtains state-of-the-art results on a range of
benchmark datasets for graph classification and node classification. Besides,
regarding knowledge graphs, our QGNN-based knowledge graph embedding method
gets state-of-the-art results on three new and challenging benchmark datasets
for knowledge graph completion. Furthermore, regarding text graphs, our
GQGNN-based text classification method works better than state-of-the-art
methods on benchmark datasets for inductive text classification. Our code is
available at: \url{this https URL}.

    

### [[2009.07110] MO-PaDGAN: Reparameterizing Engineering Designs for Augmented Multi-objective Optimization](http://arxiv.org/abs/2009.07110)


  Multi-objective optimization is key to solving many Engineering Design
problems, where design parameters are optimized for several performance
indicators. However, optimization results are highly dependent on how the
designs are parameterized. Researchers have shown that deep generative models
can learn compact design representations, providing a new way of parameterizing
designs to achieve faster convergence and improved optimization performance.
Despite their success in capturing complex distributions, existing generative
models face three challenges when used for design problems: 1) generated
designs have limited design space coverage, 2) the generator ignores design
performance, and 3)~the new parameterization is unable to represent designs
beyond training data. To address these challenges, we propose MO-PaDGAN, which
adds a Determinantal Point Processes based loss function to the generative
adversarial network to simultaneously model diversity and (multi-variate)
performance. MO-PaDGAN can thus improve the performances and coverage of
generated designs, and even generate designs with performances exceeding those
from training data. When using MO-PaDGAN as a new parameterization in
multi-objective optimization, we can discover much better Pareto fronts even
though the training data do not cover those Pareto fronts. In a real-world
multi-objective airfoil design example, we demonstrate that MO-PaDGAN achieves,
on average, an over 180\% improvement in the hypervolume indicator when
compared to the vanilla GAN or other state-of-the-art parameterization methods.

    

### [[2009.09780] Impact of lung segmentation on the diagnosis and explanation of COVID-19 in chest X-ray images](http://arxiv.org/abs/2009.09780)


  COVID-19 frequently provokes pneumonia, which can be diagnosed using imaging
exams. Chest X-ray (CXR) is often useful because it is cheap, fast, widespread,
and uses less radiation. Here, we demonstrate the impact of lung segmentation
in COVID-19 identification using CXR images and evaluate which contents of the
image influenced the most. Semantic segmentation was performed using a U-Net
CNN architecture, and the classification using three CNN architectures (VGG,
ResNet, and Inception). Explainable Artificial Intelligence techniques were
employed to estimate the impact of segmentation. A three-classes database was
composed: lung opacity (pneumonia), COVID-19, and normal. We assessed the
impact of creating a CXR image database from different sources, and the
COVID-19 generalization from one source to another. The segmentation achieved a
Jaccard distance of 0.034 and a Dice coefficient of 0.982. The classification
using segmented images achieved an F1-Score of 0.88 for the multi-class setup,
and 0.83 for COVID-19 identification. In the cross-dataset scenario, we
obtained an F1-Score of 0.74 and an area under the ROC curve of 0.9 for
COVID-19 identification using segmented images. Experiments support the
conclusion that even after segmentation, there is a strong bias introduced by
underlying factors from different sources.

    

### [[2009.12724] Beneficial Perturbations Network for Defending Adversarial Examples](http://arxiv.org/abs/2009.12724)


  Deep neural networks can be fooled by adversarial attacks: adding carefully
computed small adversarial perturbations to clean inputs can cause
misclassification on state-of-the-art machine learning models. The reason is
that neural networks fail to accommodate the distribution drift of the input
data caused by adversarial perturbations. Here, we present a new solution -
Beneficial Perturbation Network (BPN) - to defend against adversarial attacks
by fixing the distribution drift. During training, BPN generates and leverages
beneficial perturbations (somewhat opposite to well-known adversarial
perturbations) by adding new, out-of-network biasing units. Biasing units
influence the parameter space of the network, to preempt and neutralize future
adversarial perturbations on input data samples. To achieve this, BPN creates
reverse adversarial attacks during training, with very little cost, by
recycling the training gradients already computed. Reverse attacks are captured
by the biasing units, and the biases can in turn effectively defend against
future adversarial examples. Reverse attacks are a shortcut, i.e., they affect
the network's parameters without requiring instantiation of adversarial
examples that could assist training. We provide comprehensive empirical
evidence showing that 1) BPN is robust to adversarial examples and is much more
running memory and computationally efficient compared to classical adversarial
training. 2) BPN can defend against adversarial examples with negligible
additional computation and parameter costs compared to training only on clean
examples; 3) BPN hurts the accuracy on clean examples much less than classic
adversarial training; 4) BPN can improve the generalization of the network 5)
BPN trained only with Fast Gradient Sign Attack can generalize to defend PGD
attacks.

    

### [[2009.14061] GraphITE: Estimating Individual Effects of Graph-structured Treatments](http://arxiv.org/abs/2009.14061)


  Outcome estimation of treatments for target individuals is an important
foundation for decision making based on causal relations. Most existing outcome
estimation methods deal with binary or multiple-choice treatments; however, in
some applications, the number of treatments can be significantly large, while
the treatments themselves have rich information. In this study, we considered
one important instance of such cases: the outcome estimation problem of
graph-structured treatments such as drugs. Owing to the large number of
possible treatments, the counterfactual nature of observational data that
appears in conventional treatment effect estimation becomes more of a concern
for this problem. Our proposed method, GraphITE (pronounced "graphite") learns
the representations of graph-structured treatments using graph neural networks
while mitigating observation biases using Hilbert-Schmidt Independence
Criterion regularization, which increases the independence of the
representations of the targets and treatments. Experiments on two real-world
datasets show that GraphITE outperforms baselines, especially in cases with a
large number of treatments.

    

### [[2010.03084] Program Enhanced Fact Verification with Verbalization and Graph Attention Network](http://arxiv.org/abs/2010.03084)


  Performing fact verification based on structured data is important for many
real-life applications and is a challenging research problem, particularly when
it involves both symbolic operations and informal inference based on language
understanding. In this paper, we present a Program-enhanced Verbalization and
Graph Attention Network (ProgVGAT) to integrate programs and execution into
textual inference models. Specifically, a verbalization with program execution
model is proposed to accumulate evidences that are embedded in operations over
the tables. Built on that, we construct the graph attention verification
networks, which are designed to fuse different sources of evidences from
verbalized program execution, program structures, and the original statements
and tables, to make the final verification decision. To support the above
framework, we propose a program selection module optimized with a new training
strategy based on margin loss, to produce more accurate programs, which is
shown to be effective in enhancing the final verification results. Experimental
results show that the proposed framework achieves the new state-of-the-art
performance, a 74.4% accuracy, on the benchmark dataset TABFACT.

    

### [[2010.09662] Attention Augmented ConvLSTM for Environment Prediction](http://arxiv.org/abs/2010.09662)


  Safe and proactive planning in robotic systems generally requires accurate
predictions of the environment. Prior work on environment prediction applied
video frame prediction techniques to bird's-eye view environment
representations, such as occupancy grids. ConvLSTM-based frameworks used
previously often result in significant blurring and vanishing of moving
objects, thus hindering their applicability for use in safety-critical
applications. In this work, we propose two extensions to the ConvLSTM to
address these issues. We present the Temporal Attention Augmented ConvLSTM
(TAAConvLSTM) and Self-Attention Augmented ConvLSTM (SAAConvLSTM) frameworks
for spatiotemporal occupancy prediction, and demonstrate improved performance
over baseline architectures on the real-world KITTI and Waymo datasets.

    

### [[2010.11708] Context-aware surrogate modeling for balancing approximation and sampling costs in multi-fidelity importance sampling and Bayesian inverse problems](http://arxiv.org/abs/2010.11708)


  Multi-fidelity methods leverage low-cost surrogate models to speed up
computations and make occasional recourse to expensive high-fidelity models to
establish accuracy guarantees. Because surrogate and high-fidelity models are
used together, poor predictions by surrogate models can be compensated with
frequent recourse to high-fidelity models. Thus, there is a trade-off between
investing computational resources to improve the accuracy of surrogate models
versus simply making more frequent recourse to expensive high-fidelity models;
however, this trade-off is ignored by traditional modeling methods that
construct surrogate models that are meant to replace high-fidelity models
rather than being used together with high-fidelity models. This work considers
multi-fidelity importance sampling and theoretically and computationally trades
off increasing the fidelity of surrogate models for constructing more accurate
biasing densities and the numbers of samples that are required from the
high-fidelity models to compensate poor biasing densities. Numerical examples
demonstrate that such context-aware surrogate models for multi-fidelity
importance sampling have lower fidelity than what typically is set as tolerance
in traditional model reduction, leading to runtime speedups of up to one order
of magnitude in the presented examples.

    

### [[2011.03176] On the Ergodicity, Bias and Asymptotic Normality of Randomized Midpoint Sampling Method](http://arxiv.org/abs/2011.03176)


  The randomized midpoint method, proposed by [SL19], has emerged as an optimal
discretization procedure for simulating the continuous time Langevin
diffusions. Focusing on the case of strong-convex and smooth potentials, in
this paper, we analyze several probabilistic properties of the randomized
midpoint discretization method for both overdamped and underdamped Langevin
diffusions. We first characterize the stationary distribution of the discrete
chain obtained with constant step-size discretization and show that it is
biased away from the target distribution. Notably, the step-size needs to go to
zero to obtain asymptotic unbiasedness. Next, we establish the asymptotic
normality for numerical integration using the randomized midpoint method and
highlight the relative advantages and disadvantages over other discretizations.
Our results collectively provide several insights into the behavior of the
randomized midpoint discretization method, including obtaining confidence
intervals for numerical integrations.

    

### [[2011.07491] Anomaly Detection in Video via Self-Supervised and Multi-Task Learning](http://arxiv.org/abs/2011.07491)


  Anomaly detection in video is a challenging computer vision problem. Due to
the lack of anomalous events at training time, anomaly detection requires the
design of learning methods without full supervision. In this paper, we approach
anomalous event detection in video through self-supervised and multi-task
learning at the object level. We first utilize a pre-trained detector to detect
objects. Then, we train a 3D convolutional neural network to produce
discriminative anomaly-specific information by jointly learning multiple proxy
tasks: three self-supervised and one based on knowledge distillation. The
self-supervised tasks are: (i) discrimination of forward/backward moving
objects (arrow of time), (ii) discrimination of objects in
consecutive/intermittent frames (motion irregularity) and (iii) reconstruction
of object-specific appearance information. The knowledge distillation task
takes into account both classification and detection information, generating
large prediction discrepancies between teacher and student models when
anomalies occur. To the best of our knowledge, we are the first to approach
anomalous event detection in video as a multi-task learning problem,
integrating multiple self-supervised and knowledge distillation proxy tasks in
a single architecture. Our lightweight architecture outperforms the
state-of-the-art methods on three benchmarks: Avenue, ShanghaiTech and UCSD
Ped2. Additionally, we perform an ablation study demonstrating the importance
of integrating self-supervised learning and normality-specific distillation in
a multi-task learning setting.

    

### [[2011.08472] Data-Driven Reachability Analysis Using Matrix Zonotopes](http://arxiv.org/abs/2011.08472)


  In this paper, we propose a data-driven reachability analysis approach for
unknown system dynamics. Reachability analysis is an essential tool for
guaranteeing safety properties. However, most current reachability analysis
heavily relies on the existence of a suitable system model, which is often not
directly available in practice. We instead propose a data-driven reachability
analysis approach from noisy data. More specifically, we first provide an
algorithm for over-approximating the reachable set of a linear time-invariant
system using matrix zonotopes. Then we introduce an extension for Lipschitz
nonlinear systems. We provide theoretical guarantees in both cases. Numerical
examples show the potential and applicability of the introduced methods.

    

### [[2011.11715] Multi-task Language Modeling for Improving Speech Recognition of Rare Words](http://arxiv.org/abs/2011.11715)


  End-to-end automatic speech recognition (ASR) systems are increasingly
popular due to their relative architectural simplicity and competitive
performance. However, even though the average accuracy of these systems may be
high, the performance on rare content words often lags behind hybrid ASR
systems. To address this problem, second-pass rescoring is often applied
leveraging upon language modeling. In this paper, we propose a second-pass
system with multi-task learning, utilizing semantic targets (such as intent and
slot prediction) to improve speech recognition performance. We show that our
rescoring model trained with these additional tasks outperforms the baseline
rescoring model, trained with only the language modeling task, by 1.4% on a
general test and by 2.6% on a rare word test set in terms of word-error-rate
relative (WERR). Our best ASR system with multi-task LM shows 4.6% WERR
deduction compared with RNN Transducer only ASR baseline for rare words
recognition.

    

### [[2012.00560] Quick and Robust Feature Selection: the Strength of Energy-efficient Sparse Training for Autoencoders](http://arxiv.org/abs/2012.00560)


  Major complications arise from the recent increase in the amount of
high-dimensional data, including high computational costs and memory
requirements. Feature selection, which identifies the most relevant and
informative attributes of a dataset, has been introduced as a solution to this
problem. Most of the existing feature selection methods are computationally
inefficient; inefficient algorithms lead to high energy consumption, which is
not desirable for devices with limited computational and energy resources. In
this paper, a novel and flexible method for unsupervised feature selection is
proposed. This method, named QuickSelection, introduces the strength of the
neuron in sparse neural networks as a criterion to measure the feature
importance. This criterion, blended with sparsely connected denoising
autoencoders trained with the sparse evolutionary training procedure, derives
the importance of all input features simultaneously. We implement
QuickSelection in a purely sparse manner as opposed to the typical approach of
using a binary mask over connections to simulate sparsity. It results in a
considerable speed increase and memory reduction. When tested on several
benchmark datasets, including five low-dimensional and three high-dimensional
datasets, the proposed method is able to achieve the best trade-off of
classification and clustering accuracy, running time, and maximum memory usage,
among widely used approaches for feature selection. Besides, our proposed
method requires the least amount of energy among the state-of-the-art
autoencoder-based feature selection methods.

    

### [[2012.11747] RealFormer: Transformer Likes Residual Attention](http://arxiv.org/abs/2012.11747)


  Transformer is the backbone of modern NLP models. In this paper, we propose
RealFormer, a simple and generic technique to create Residual Attention Layer
Transformer networks that significantly outperform the canonical Transformer
and its variants (BERT, ETC, etc.) on a wide spectrum of tasks including Masked
Language Modeling, GLUE, SQuAD, Neural Machine Translation, WikiHop, HotpotQA,
Natural Questions, and OpenKP. We also observe empirically that RealFormer
stabilizes training and leads to models with sparser attention. Source code and
pre-trained checkpoints for RealFormer can be found at
this https URL.

    

### [[2012.13567] Towards Real-World BCI: CCSPNet, A Compact Subject-Independent Motor Imagery Framework](http://arxiv.org/abs/2012.13567)


  A conventional subject-dependent (SD) brain-computer interface (BCI) requires
a complete data-gathering, training, and calibration phase for each user before
it can be used. In recent years, a number of subject-independent (SI) BCIs have
been developed. However, there are many problems preventing them from being
used in real-world BCI applications. A weaker performance compared to the
subject-dependent (SD) approach, and a relatively large model requiring high
computational power are the most important ones. Therefore, a potential
real-world BCI would greatly benefit from a compact low-power
subject-independent BCI framework, ready to be used immediately after the user
puts it on. To move towards this goal, we propose a novel subject-independent
BCI framework named CCSPNet (Convolutional Common Spatial Pattern Network)
trained on the motor imagery (MI) paradigm of a large-scale
electroencephalography (EEG) signals database consisting of 21600 trials for 54
subjects performing two-class hand-movement MI tasks. The proposed framework
applies a wavelet kernel convolutional neural network (WKCNN) and a temporal
convolutional neural network (TCNN) in order to represent and extract the
diverse spectral features of EEG signals. The outputs of the convolutional
layers go through a common spatial pattern (CSP) algorithm for spatial feature
extraction. The number of CSP features is reduced by a dense neural network,
and the final class label is determined by a linear discriminative analysis
(LDA) classifier. The CCSPNet framework evaluation results show that it is
possible to have a low-power compact BCI that achieves both SD and SI
performance comparable to complex and computationally expensive models.

    

### [[2012.14336] Spatiotemporal Pattern Mining for Nowcasting Extreme Earthquakes in Southern California](http://arxiv.org/abs/2012.14336)


  Geoscience and seismology have utilized the most advanced technologies and
equipment to monitor seismic events globally from the past few decades. With
the enormous amount of data, modern GPU-powered deep learning presents a
promising approach to analyze data and discover patterns. In recent years,
there are plenty of successful deep learning models for picking seismic waves.
However, forecasting extreme earthquakes, which can cause disasters, is still
an underdeveloped topic in history. Relevant research in spatiotemporal
dynamics mining and forecasting has revealed some successful predictions, a
crucial topic in many scientific research fields. Most studies of them have
many successful applications of using deep neural networks. In Geology and
Earth science studies, earthquake prediction is one of the world's most
challenging problems, about which cutting-edge deep learning technologies may
help discover some valuable patterns. In this project, we propose a deep
learning modeling approach, namely \tseqpre, to mine spatiotemporal patterns
from data to nowcast extreme earthquakes by discovering visual dynamics in
regional coarse-grained spatial grids over time. In this modeling approach, we
use synthetic deep learning neural networks with domain knowledge in geoscience
and seismology to exploit earthquake patterns for prediction using
convolutional long short-term memory neural networks. Our experiments show a
strong correlation between location prediction and magnitude prediction for
earthquakes in Southern California. Ablation studies and visualization validate
the effectiveness of the proposed modeling method.

    

### [[2012.14541] YASO: A Targeted Sentiment Analysis Evaluation Dataset for Open-Domain Reviews](http://arxiv.org/abs/2012.14541)


  Current TSA evaluation in a cross-domain setup is restricted to the small set
of review domains available in existing datasets. Such an evaluation is
limited, and may not reflect true performance on sites like Amazon or Yelp that
host diverse reviews from many domains. To address this gap, we present YASO -
a new TSA evaluation dataset of open-domain user reviews. YASO contains 2,215
English sentences from dozens of review domains, annotated with target terms
and their sentiment. Our analysis verifies the reliability of these
annotations, and explores the characteristics of the collected data. Benchmark
results using five contemporary TSA systems show there is ample room for
improvement on this challenging new dataset. YASO is available at
this https URL.

    

### [[2101.02337] Learning Temporal Dynamics from Cycles in Narrated Video](http://arxiv.org/abs/2101.02337)


  Learning to model how the world changes as time elapses has proven a
challenging problem for the computer vision community. We propose a
self-supervised solution to this problem using temporal cycle consistency
jointly in vision and language, training on narrated video. Our model learns
modality-agnostic functions to predict forward and backward in time, which must
undo each other when composed. This constraint leads to the discovery of
high-level transitions between moments in time, since such transitions are
easily inverted and shared across modalities. We justify the design of our
model with an ablation study on different configurations of the cycle
consistency problem. We then show qualitatively and quantitatively that our
approach yields a meaningful, high-level model of the future and past. We apply
the learned dynamics model without further training to various tasks, such as
predicting future action and temporally ordering sets of images. Project page:
this https URL


### [[2101.11703] Unified Framework for Feature Extraction based on Contrastive Learning](http://arxiv.org/abs/2101.11703)


  Feature extraction is an efficient approach for alleviating the issue of
dimensionality in high-dimensional data. As a popular self-supervised learning
method, contrastive learning has recently garnered considerable attention. In
this study, we proposed a unified framework based on a new perspective of
contrastive learning (CL) that is suitable for both unsupervised and supervised
feature extraction. The proposed framework first constructed two CL graph for
uniquely defining the positive and negative pairs. Subsequently, the projection
matrix was determined by minimizing the contrastive loss function. In addition,
the proposed framework considered both similar and dissimilar samples to unify
unsupervised and supervised feature extraction. Moreover, we propose the three
specific methods: unsupervised contrastive learning method, supervised
contrastive learning method 1 ,and supervised contrastive learning method 2.
Finally, the numerical experiments on five real datasets demonstrated the
superior performance of the proposed framework in comparison to the existing
methods.

    

### [[2102.01103] Machine-Learned Phase Diagrams of Generalized Kitaev Honeycomb Magnets](http://arxiv.org/abs/2102.01103)


  We use a recently developed interpretable and unsupervised machine-learning
method, the tensorial kernel support vector machine (TK-SVM), to investigate
the low-temperature classical phase diagram of a generalized
Heisenberg-Kitaev-$\Gamma$ ($J$-$K$-$\Gamma$) model on a honeycomb lattice.
Aside from reproducing phases reported by previous quantum and classical
studies, our machine finds a hitherto missed nested zigzag-stripy order and
establishes the robustness of a recently identified modulated $S_3 \times Z_3$
phase, which emerges through the competition between the Kitaev and $\Gamma$
spin liquids, against Heisenberg interactions. The results imply that, in the
restricted parameter space spanned by the three primary exchange interactions
-- $J$, $K$, and $\Gamma$, the representative Kitaev material $\alpha$-${\rm
RuCl}_3$ lies close to the boundaries of several phases, including a simple
ferromagnet, the unconventional $S_3 \times Z_3$ and nested zigzag-stripy
magnets. A zigzag order is stabilized by a finite $\Gamma^{\prime}$ and/or
$J_3$ term, whereas the four magnetic orders may compete in particular if
$\Gamma^{\prime}$ is anti-ferromagnetic.

    

### [[2102.03888] Black-Box Optimization via Generative Adversarial Nets](http://arxiv.org/abs/2102.03888)


  Black-box optimization (BBO) algorithms are concerned with finding the best
solutions for problems with missing analytical details. Most classical methods
for such problems are based on strong and fixed a priori assumptions, such as
Gaussianity. However, the complex real-world problems, especially when the
global optimum is desired, could be very far from the a priori assumptions
because of their diversities, bringing some unexpected obstacles to these
methods. In this paper, we present a generative adversarial nets-based
optimizer (OPT-GAN) to adapt to diverse black-box problems via estimating the
distribution of optima. The method learns the extensive distribution of the
optimal region dominated by selective and randomly moving candidates, balancing
the exploration and exploitation. Experiments conducted on Black-box
Optimization Benchmarking (BBOB) problems and several other benchmarks with
diversified distributions exhibit that, the OPT-GAN outperforms many
traditional and neural net-based BBO algorithms.

    

### [[2102.04279] Constrained Ensemble Langevin Monte Carlo](http://arxiv.org/abs/2102.04279)


  The classical Langevin Monte Carlo method looks for samples from a target
distribution by descending the samples along the gradient of the target
distribution. The method enjoys a fast convergence rate. However, the numerical
cost is sometimes high because each iteration requires the computation of a
gradient. One approach to eliminate the gradient computation is to employ the
concept of "ensemble". A large number of particles are evolved together so the
neighboring particles provide gradient information to each other. In this
article, we discuss two algorithms that integrate the ensemble feature into LMC
and the associated properties.
In particular, we find that if one directly surrogates the gradient using the
ensemble approximation, the algorithm, termed Ensemble Langevin Monte Carlo, is
unstable due to a high variance term. If the gradients are replaced by the
ensemble approximations only in a constrained manner, to protect from the
unstable points, the algorithm, termed Constrained Ensemble Langevin Monte
Carlo, resembles the classical LMC up to an ensemble error but removes most of
the gradient computation.

    

### [[2102.10472] Learning Neural Network Subspaces](http://arxiv.org/abs/2102.10472)


  Recent observations have advanced our understanding of the neural network
optimization landscape, revealing the existence of (1) paths of high accuracy
containing diverse solutions and (2) wider minima offering improved
performance. Previous methods observing diverse paths require multiple training
runs. In contrast we aim to leverage both property (1) and (2) with a single
method and in a single training run. With a similar computational cost as
training one model, we learn lines, curves, and simplexes of high-accuracy
neural networks. These neural network subspaces contain diverse solutions that
can be ensembled, approaching the ensemble performance of independently trained
networks without the training cost. Moreover, using the subspace midpoint
boosts accuracy, calibration, and robustness to label noise, outperforming
Stochastic Weight Averaging.

    

### [[2102.11972] Do Transformer Modifications Transfer Across Implementations and Applications?](http://arxiv.org/abs/2102.11972)


  The research community has proposed copious modifications to the Transformer
architecture since it was introduced over three years ago, relatively few of
which have seen widespread adoption. In this paper, we comprehensively evaluate
many of these modifications in a shared experimental setting that covers most
of the common uses of the Transformer in natural language processing.
Surprisingly, we find that most modifications do not meaningfully improve
performance. Furthermore, most of the Transformer variants we found beneficial
were either developed in the same codebase that we used or are relatively minor
changes. We conjecture that performance improvements may strongly depend on
implementation details and correspondingly make some recommendations for
improving the generality of experimental results.

    

### [[2102.13177] Efficient and Interpretable Robot Manipulation with Graph Neural Networks](http://arxiv.org/abs/2102.13177)


  Manipulation tasks like loading a dishwasher can be seen as a sequence of
spatial constraints and relationships between different objects. For example, a
plate can be placed in a tray only if the tray is open. We aim to discover such
task-specific rules from demonstrations. We pose manipulation as a
classification problem over a graph, whose nodes represent task relevant
entities like objects and goals, transform the environment scene into a graph
and learn a graph neural network (GNN) policy using imitation learning. In our
experiments, a single learned GNN policy, trained using 20 expert
demonstrations, can solve multiple blockstacking and rearrangement tasks in
both simulation and on hardware, without any task description. The policy
successfully generalizes over the number of objects in the environment, their
positions, and goal configurations (trained on single stacks, generalizes to
pyramids and multiple stacks). We also apply our approach to a complex
simulated dishwasher environment, where a robot learns to load a dishwasher
from only 5 high-level human demonstrations. These experiments show that
imitation learning on a graphical state and policy is a simple, yet powerful
tool for solving complex long-horizon manipulation problems, without requiring
detailed task descriptions. Videos can be found at:
this https URL.

    

### [[2103.02735] Fairness of Exposure in Stochastic Bandits](http://arxiv.org/abs/2103.02735)


  Contextual bandit algorithms have become widely used for recommendation in
online systems (e.g. marketplaces, music streaming, news), where they now wield
substantial influence on which items get exposed to the users. This raises
questions of fairness to the items -- and to the sellers, artists, and writers
that benefit from this exposure. We argue that the conventional bandit
formulation can lead to an undesirable and unfair winner-takes-all allocation
of exposure. To remedy this problem, we propose a new bandit objective that
guarantees merit-based fairness of exposure to the items while optimizing
utility to the users. We formulate fairness regret and reward regret in this
setting, and present algorithms for both stochastic multi-armed bandits and
stochastic linear bandits. We prove that the algorithms achieve sub-linear
fairness regret and reward regret. Beyond the theoretical analysis, we also
provide empirical evidence that these algorithms can fairly allocate exposure
to different arms effectively.

    

### [[2103.04505] Split Computing and Early Exiting for Deep Learning Applications: Survey and Research Challenges](http://arxiv.org/abs/2103.04505)


  Mobile devices such as smartphones and autonomous vehicles increasingly rely
on deep neural networks (DNNs) to execute complex inference tasks such as image
classification and speech recognition, among others. However, continuously
executing the entire DNN on the mobile device can quickly deplete its battery.
Although task offloading to edge servers may decrease the mobile device's
computational burden, erratic patterns in channel quality, network and edge
server load can lead to a significant delay in task execution.
Recently,approaches based on split computing (SC) have been proposed, where the
DNN is split into a head and a tail model, executed respectively on the mobile
device and on the edge server. Ultimately, this may reduce bandwidth usage as
well as energy consumption. Another approach, called early exiting (EE), trains
models to present multiple "exits" earlier in the architecture, each providing
increasingly higher target accuracy. Therefore, the trade-off between accuracy
and delay can be tuned according to the current conditions or application
demands. In this paper, we provide a comprehensive survey of the state of the
art in SC and EE strategies, by presenting a comparison of the most relevant
approaches. We conclude the paper by providing a set of compelling research
challenges.

    

### [[2103.04886] Lipschitz Normalization for Self-Attention Layers with Application to Graph Neural Networks](http://arxiv.org/abs/2103.04886)


  Attention based neural networks are state of the art in a large range of
applications. However, their performance tends to degrade when the number of
layers increases. In this work, we show that enforcing Lipschitz continuity by
normalizing the attention scores can significantly improve the performance of
deep attention models. First, we show that, for deep graph attention networks
(GAT), gradient explosion appears during training, leading to poor performance
of gradient-based training algorithms. To address this issue, we derive a
theoretical analysis of the Lipschitz continuity of attention modules and
introduce LipschitzNorm, a simple and parameter-free normalization for
self-attention mechanisms that enforces the model to be Lipschitz continuous.
We then apply LipschitzNorm to GAT and Graph Transformers and show that their
performance is substantially improved in the deep setting (10 to 30 layers).
More specifically, we show that a deep GAT model with LipschitzNorm achieves
state of the art results for node label prediction tasks that exhibit
long-range dependencies, while showing consistent improvements over their
unnormalized counterparts in benchmark node classification tasks.

    

### [[2103.06544] Causal Learner: A Toolbox for Causal Structure and Markov Blanket Learning](http://arxiv.org/abs/2103.06544)


  Causal Learner is a toolbox for learning causal structure and Markov blanket
(MB) from data. It integrates functions for generating simulated Bayesian
network data, a set of state-of-the-art global causal structure learning
algorithms, a set of state-of-the-art local causal structure learning
algorithms, a set of state-of-the-art MB learning algorithms, and functions for
evaluating algorithms. The data generation part of Causal Learner is written in
R, and the rest of Causal Learner is written in MATLAB. Causal Learner aims to
provide researchers and practitioners with an open-source platform for causal
learning from data and for the development and evaluation of new causal
learning algorithms. The Causal Learner project is available at
this http URL.

    

### [[2103.06615] Controlled Gaussian Process Dynamical Models with Application to Robotic Cloth Manipulation](http://arxiv.org/abs/2103.06615)


  Over the last years, robotic cloth manipulation has gained relevance within
the research community. While significant advances have been made in robotic
manipulation of rigid objects, the manipulation of non-rigid objects such as
cloth garments is still a challenging problem. The uncertainty on how cloth
behaves often requires the use of model-based approaches. However, cloth models
have a very high dimensionality. Therefore, it is difficult to find a middle
point between providing a manipulator with a dynamics model of cloth and
working with a state space of tractable dimensionality. For this reason, most
cloth manipulation approaches in literature perform static or quasi-static
manipulation. In this paper, we propose a variation of Gaussian Process
Dynamical Models (GPDMs) to model cloth dynamics in a low-dimensional manifold.
GPDMs project a high-dimensional state space into a smaller dimension latent
space which is capable of keeping the dynamic properties. Using such approach,
we add control variables to the original formulation. In this way, it is
possible to take into account the robot commands exerted on the cloth dynamics.
We call this new version Controlled Gaussian Process Dynamical Model (CGPDM).
Moreover, we propose an alternative parametric structure for the model, that is
richer than the one employed in previous GPDM realizations. The modeling
capacity of our proposal has been tested in both a simulated and a real
scenario, where CGPDM proved to be capable of generalizing over a wide range of
movements and correctly predicting the cloth motions obtained by previously
unseen sequences of control actions.

    

### [[2103.11161] MonteFloor: Extending MCTS for Reconstructing Accurate Large-Scale Floor Plans](http://arxiv.org/abs/2103.11161)


  We propose a novel method for reconstructing floor plans from noisy 3D point
clouds. Our main contribution is a principled approach that relies on the Monte
Carlo Tree Search (MCTS) algorithm to maximize a suitable objective function
efficiently despite the complexity of the problem. Like previous work, we first
project the input point cloud to a top view to create a density map and extract
room proposals from it. Our method selects and optimizes the polygonal shapes
of these room proposals jointly to fit the density map and outputs an accurate
vectorized floor map even for large complex scenes. To do this, we adapted
MCTS, an algorithm originally designed to learn to play games, to select the
room proposals by maximizing an objective function combining the fitness with
the density map as predicted by a deep network and regularizing terms on the
room shapes. We also introduce a refinement step to MCTS that adjusts the shape
of the room proposals. For this step, we propose a novel differentiable method
for rendering the polygonal shapes of these proposals. We evaluate our method
on the recent and challenging Structured3D and Floor-SP datasets and show a
significant improvement over the state-of-the-art, without imposing any hard
constraints nor assumptions on the floor plan configurations.

    

### [[2103.11879] Real-time End-to-End Federated Learning: An Automotive Case Study](http://arxiv.org/abs/2103.11879)


  With the development and the increasing interests in ML/DL fields, companies
are eager to apply Machine Learning/Deep Learning approaches to increase
service quality and customer experience. Federated Learning was implemented as
an effective model training method for distributing and accelerating
time-consuming model training while protecting user data privacy. However,
common Federated Learning approaches, on the other hand, use a synchronous
protocol to conduct model aggregation, which is inflexible and unable to adapt
to rapidly changing environments and heterogeneous hardware settings in
real-world scenarios. In this paper, we present an approach to real-time
end-to-end Federated Learning combined with a novel asynchronous model
aggregation protocol. Our method is validated in an industrial use case in the
automotive domain, focusing on steering wheel angle prediction for autonomous
driving. Our findings show that asynchronous Federated Learning can
significantly improve the prediction performance of local edge models while
maintaining the same level of accuracy as centralized machine learning.
Furthermore, by using a sliding training window, the approach can minimize
communication overhead, accelerate model training speed and consume real-time
streaming data, proving high efficiency when deploying ML/DL components to
heterogeneous real-world embedded systems.

    

### [[2103.12487] Improved Analysis of the Tsallis-INF Algorithm in Stochastically Constrained Adversarial Bandits and Stochastic Bandits with Adversarial Corruptions](http://arxiv.org/abs/2103.12487)


  We derive improved regret bounds for the Tsallis-INF algorithm of Zimmert and
Seldin (2021). We show that in adversarial regimes with a $(\Delta,C,T)$
self-bounding constraint the algorithm achieves
$\mathcal{O}\left(\left(\sum_{i\neq i^*}
\frac{1}{\Delta_i}\right)\log_+\left(\frac{(K-1)T}{\left(\sum_{i\neq i^*}
\frac{1}{\Delta_i}\right)^2}\right)+\sqrt{C\left(\sum_{i\neq
i^*}\frac{1}{\Delta_i}\right)\log_+\left(\frac{(K-1)T}{C\sum_{i\neq
i^*}\frac{1}{\Delta_i}}\right)}\right)$ regret bound, where $T$ is the time
horizon, $K$ is the number of arms, $\Delta_i$ are the suboptimality gaps,
$i^*$ is the best arm, $C$ is the corruption magnitude, and $\log_+(x) =
\max\left(1,\log x\right)$. The regime includes stochastic bandits,
stochastically constrained adversarial bandits, and stochastic bandits with
adversarial corruptions as special cases. Additionally, we provide a general
analysis, which allows to achieve the same kind of improvement for
generalizations of Tsallis-INF to other settings beyond multiarmed bandits.

    

### [[2104.02600] Noise Estimation for Generative Diffusion Models](http://arxiv.org/abs/2104.02600)


  Generative diffusion models have emerged as leading models in speech and
image generation. However, in order to perform well with a small number of
denoising steps, a costly tuning of the set of noise parameters is needed. In
this work, we present a simple and versatile learning scheme that can
step-by-step adjust those noise parameters, for any given number of steps,
while the previous work needs to retune for each number separately.
Furthermore, without modifying the weights of the diffusion model, we are able
to significantly improve the synthesis results, for a small number of steps.
Our approach comes at a negligible computation cost.

    

### [[2104.03416] Pushing the Limits of Non-Autoregressive Speech Recognition](http://arxiv.org/abs/2104.03416)


  We combine recent advancements in end-to-end speech recognition to
non-autoregressive automatic speech recognition. We push the limits of
non-autoregressive state-of-the-art results for multiple datasets: LibriSpeech,
Fisher+Switchboard and Wall Street Journal. Key to our recipe, we leverage CTC
on giant Conformer neural network architectures with SpecAugment and wav2vec2
pre-training. We achieve 1.8%/3.6% WER on LibriSpeech test/test-other sets,
5.1%/9.8% WER on Switchboard, and 3.4% on the Wall Street Journal, all without
a language model.

    

### [[2104.06901] Enhancing Interpretable Clauses Semantically using Pretrained Word Representation](http://arxiv.org/abs/2104.06901)


  Tsetlin Machine (TM) is an interpretable pattern recognition algorithm based
on propositional logic, which has demonstrated competitive performance in many
Natural Language Processing (NLP) tasks, including sentiment analysis, text
classification, and Word Sense Disambiguation. To obtain human-level
interpretability, legacy TM employs Boolean input features such as bag-of-words
(BOW). However, the BOW representation makes it difficult to use any
pre-trained information, for instance, word2vec and GloVe word representations.
This restriction has constrained the performance of TM compared to deep neural
networks (DNNs) in NLP. To reduce the performance gap, in this paper, we
propose a novel way of using pre-trained word representations for TM. The
approach significantly enhances the performance and interpretability of TM. We
achieve this by extracting semantically related words from pre-trained word
representations as input features to the TM. Our experiments show that the
accuracy of the proposed approach is significantly higher than the previous
BOW-based TM, reaching the level of DNN-based models.

    

### [[2104.07155] Disentangling Representations of Text by Masking Transformers](http://arxiv.org/abs/2104.07155)


  Representations from large pretrained models such as BERT encode a range of
features into monolithic vectors, affording strong predictive accuracy across a
multitude of downstream tasks. In this paper we explore whether it is possible
to learn disentangled representations by identifying existing subnetworks
within pretrained models that encode distinct, complementary aspect
representations. Concretely, we learn binary masks over transformer weights or
hidden units to uncover subsets of features that correlate with a specific
factor of variation; this eliminates the need to train a disentangled model
from scratch for a particular task. We evaluate this method with respect to its
ability to disentangle representations of sentiment from genre in movie
reviews, "toxicity" from dialect in Tweets, and syntax from semantics.
By combining masking with magnitude pruning we find that we can identify
sparse subnetworks within BERT that strongly encode particular aspects (e.g.,
toxicity) while only weakly encoding others (e.g., race). Moreover, despite
only learning masks, we find that disentanglement-via-masking performs as well
as -- and often better than -- previously proposed methods based on variational
autoencoders and adversarial training.

    

### [[2104.07467] Cross-Domain Label-Adaptive Stance Detection](http://arxiv.org/abs/2104.07467)


  Stance detection concerns the classification of a writer's viewpoint towards
a target. There are different task variants, e.g., stance of a tweet vs. a full
article, or stance with respect to a claim vs. an (implicit) topic. Moreover,
task definitions vary, which includes the label inventory, the data collection,
and the annotation protocol. All these aspects hinder cross-domain studies, as
they require changes to standard domain adaptation approaches. In this paper,
we perform an in-depth analysis of 16 stance detection datasets, and we explore
the possibility for cross-domain learning from them. Moreover, we propose an
end-to-end unsupervised framework for out-of-domain prediction of unseen,
user-defined labels. In particular, we combine domain adaptation techniques
such as mixture of experts and domain-adversarial training with label
embeddings, and we demonstrate sizable performance gains over strong baselines,
both (i) in-domain, i.e., for seen targets, and (ii) out-of-domain, i.e., for
unseen targets. Finally, we perform an exhaustive analysis of the cross-domain
results, and we highlight the important factors influencing the model
performance.

    

### [[2104.07814] Detecting Polarized Topics Using Partisanship-aware Contextualized Topic Embeddings](http://arxiv.org/abs/2104.07814)


  Growing polarization of the news media has been blamed for fanning
disagreement, controversy and even violence. Early identification of polarized
topics is thus an urgent matter that can help mitigate conflict. However,
accurate measurement of topic-wise polarization is still an open research
challenge. To address this gap, we propose Partisanship-aware Contextualized
Topic Embeddings (PaCTE), a method to automatically detect polarized topics
from partisan news sources. Specifically, utilizing a language model that has
been finetuned on recognizing partisanship of the news articles, we represent
the ideology of a news corpus on a topic by corpus-contextualized topic
embedding and measure the polarization using cosine distance. We apply our
method to a dataset of news articles about the COVID-19 pandemic. Extensive
experiments on different news sources and topics demonstrate the efficacy of
our method to capture topical polarization, as indicated by its effectiveness
of retrieving the most polarized topics.

    

### [[2104.08028] Back to the Basics: A Quantitative Analysis of Statistical and Graph-Based Term Weighting Schemes for Keyword Extraction](http://arxiv.org/abs/2104.08028)


  Term weighting schemes are widely used in Natural Language Processing and
Information Retrieval. In particular, term weighting is the basis for keyword
extraction. However, there are relatively few evaluation studies that shed
light about the strengths and shortcomings of each weighting scheme. In fact,
in most cases researchers and practitioners resort to the well-known tf-idf as
default, despite the existence of other suitable alternatives, including
graph-based models. In this paper, we perform an exhaustive and large-scale
empirical comparison of both statistical and graph-based term weighting methods
in the context of keyword extraction. Our analysis reveals some interesting
findings such as the advantages of the less-known lexical specificity with
respect to tf-idf, or the qualitative differences between statistical and
graph-based methods. Finally, based on our findings we discuss and devise some
suggestions for practitioners. Source code to reproduce our experimental
results, including a keyword extraction library, are available in the following
repository: this https URL


### [[2104.10459] Jacobian Regularization for Mitigating Universal Adversarial Perturbations](http://arxiv.org/abs/2104.10459)


  Universal Adversarial Perturbations (UAPs) are input perturbations that can
fool a neural network on large sets of data. They are a class of attacks that
represents a significant threat as they facilitate realistic, practical, and
low-cost attacks on neural networks. In this work, we derive upper bounds for
the effectiveness of UAPs based on norms of data-dependent Jacobians. We
empirically verify that Jacobian regularization greatly increases model
robustness to UAPs by up to four times whilst maintaining clean performance.
Our theoretical analysis also allows us to formulate a metric for the strength
of shared adversarial perturbations between pairs of inputs. We apply this
metric to benchmark datasets and show that it is highly correlated with the
actual observed robustness. This suggests that realistic and practical
universal attacks can be reliably mitigated without sacrificing clean accuracy,
which shows promise for the robustness of machine learning systems.

    

### [[2105.06508] Internet of Things (IoT) Based Video Analytics: a use case of Smart Doorbell](http://arxiv.org/abs/2105.06508)


  The vision of the internet of things (IoT) is a reality now. IoT devices are
getting cheaper, smaller. They are becoming more and more computationally and
energy-efficient. The global market of IoT-based video analytics has seen
significant growth in recent years and it is expected to be a growing market
segment. For any IoT-based video analytics application, few key points
required, such as cost-effectiveness, widespread use, flexible design, accurate
scene detection, reusability of the framework. Video-based smart doorbell
system is one such application domain for video analytics where many commercial
offerings are available in the consumer market. However, such existing
offerings are costly, monolithic, and proprietary. Also, there will be a
trade-off between accuracy and portability. To address the foreseen problems,
I'm proposing a distributed framework for video analytics with a use case of a
smart doorbell system. The proposed framework uses AWS cloud services as a base
platform and to meet the price affordability constraint, the system was
implemented on affordable Raspberry Pi. The smart doorbell will be able to
recognize the known/unknown person with at most accuracy. The smart doorbell
system is also having additional detection functionalities such as harmful
weapon detection, noteworthy vehicle detection, animal/pet detection. An iOS
application is specifically developed for this implementation which can receive
the notification from the smart doorbell in real-time. Finally, the paper also
mentions the classical approaches for video analytics, their feasibility in
implementing with this use-case, and comparative analysis in terms of accuracy
and time required to detect an object in the frame is carried out. Results
conclude that AWS cloud-based approach is worthy for this smart doorbell use
case.

    

### [[2105.06631] Ordering-Based Causal Discovery with Reinforcement Learning](http://arxiv.org/abs/2105.06631)


  It is a long-standing question to discover causal relations among a set of
variables in many empirical sciences. Recently, Reinforcement Learning (RL) has
achieved promising results in causal discovery from observational data.
However, searching the space of directed graphs and enforcing acyclicity by
implicit penalties tend to be inefficient and restrict the existing RL-based
method to small scale problems. In this work, we propose a novel RL-based
approach for causal discovery, by incorporating RL into the ordering-based
paradigm. Specifically, we formulate the ordering search problem as a
multi-step Markov decision process, implement the ordering generating process
with an encoder-decoder architecture, and finally use RL to optimize the
proposed model based on the reward mechanisms designed for~each ordering. A
generated ordering would then be processed using variable selection to obtain
the final causal graph. We analyze the consistency and computational complexity
of the proposed method, and empirically show that a pretrained model can be
exploited to accelerate training. Experimental results on both synthetic and
real data sets shows that the proposed method achieves a much improved
performance over existing RL-based method.

    

### [[2105.07231] Bilevel Programs Meet Deep Learning: A Unifying View on Inference Learning Methods](http://arxiv.org/abs/2105.07231)


  In this work we unify a number of inference learning methods, that are
proposed in the literature as alternative training algorithms to the ones based
on regular error back-propagation. These inference learning methods were
developed with very diverse motivations, mainly aiming to enhance the
biological plausibility of deep neural networks and to improve the intrinsic
parallelism of training methods. We show that these superficially very
different methods can all be obtained by successively applying a particular
reformulation of bilevel optimization programs. As a by-product it becomes also
evident that all considered inference learning methods include back-propagation
as a special case, and therefore at least approximate error back-propagation in
typical settings. Finally, we propose Fenchel back-propagation, that replaces
the propagation of infinitesimal corrections performed in standard
back-propagation with finite targets as the learning signal. Fenchel
back-propagation can therefore be seen as an instance of learning via explicit
target propagation.

    

### [[2105.07407] Protein sequence-to-structure learning: Is this the end(-to-end revolution)?](http://arxiv.org/abs/2105.07407)


  The potential of deep learning has been recognized in the protein structure
prediction community for some time, and became indisputable after CASP13. In
CASP14, deep learning has boosted the field to unanticipated levels reaching
near-experimental accuracy. This success comes from advances transferred from
other machine learning areas, as well as methods specifically designed to deal
with protein sequences and structures, and their abstractions. Novel emerging
approaches include (i) geometric learning, i.e. learning on representations
such as graphs, 3D Voronoi tessellations, and point clouds; (ii) pre-trained
protein language models leveraging attention; (iii) equivariant architectures
preserving the symmetry of 3D space; (iv) use of large meta-genome databases;
(v) combinations of protein representations; (vi) and finally truly end-to-end
architectures, i.e. differentiable models starting from a sequence and
returning a 3D structure. Here, we provide an overview and our opinion of the
novel deep learning approaches developed in the last two years and widely used
in CASP14.

    

### [[2105.12221] Geometry of the Loss Landscape in Overparameterized Neural Networks: Symmetries and Invariances](http://arxiv.org/abs/2105.12221)


  We study how permutation symmetries in overparameterized multi-layer neural
networks generate `symmetry-induced' critical points. Assuming a network with $
L $ layers of minimal widths $ r_1^*, \ldots, r_{L-1}^* $ reaches a zero-loss
minimum at $ r_1^*! \cdots r_{L-1}^*! $ isolated points that are permutations
of one another, we show that adding one extra neuron to each layer is
sufficient to connect all these previously discrete minima into a single
manifold. For a two-layer overparameterized network of width $ r^*+ h =: m $ we
explicitly describe the manifold of global minima: it consists of $ T(r^*, m) $
affine subspaces of dimension at least $ h $ that are connected to one another.
For a network of width $m$, we identify the number $G(r,m)$ of affine subspaces
containing only symmetry-induced critical points that are related to the
critical points of a smaller network of width $r<r^*$. Via a combinatorial
analysis, we derive closed-form formulas for $ T $ and $ G $ and show that the
number of symmetry-induced critical subspaces dominates the number of affine
subspaces forming the global minima manifold in the mildly overparameterized
regime (small $ h $) and vice versa in the vastly overparameterized regime ($h
\gg r^*$). Our results provide new insights into the minimization of the
non-convex loss function of overparameterized neural networks.

    

### [[2106.00225] Locally Valid and Discriminative Predictive Intervals for Deep Learning Models](http://arxiv.org/abs/2106.00225)


  Crucial for building trust in deep learning models for critical real-world
applications is efficient and theoretically sound uncertainty quantification, a
task that continues to be challenging. Useful uncertainty information is
expected to have two key properties: It should be valid (guaranteeing coverage)
and discriminative (more uncertain when the expected risk is high). Moreover,
when combined with deep learning (DL) methods, it should be scalable and affect
the DL model performance minimally. Most existing Bayesian methods lack
frequentist coverage guarantees and usually affect model performance. The few
available frequentist methods are rarely discriminative and/or violate coverage
guarantees due to unrealistic assumptions. Moreover, many methods are expensive
or require substantial modifications to the base neural network. Building upon
recent advances in conformal prediction [13, 31] and leveraging the classical
idea of kernel regression, we propose Locally Valid and Discriminative
predictive intervals (LVD), a simple, efficient, and lightweight method to
construct discriminative predictive intervals (PIs) for almost any DL model.
With no assumptions on the data distribution, such PIs also offer finite-sample
local coverage guarantees (contrasted to the simpler marginal coverage). We
empirically verify, using diverse datasets, that besides being the only locally
valid method, LVD also exceeds or matches the performance (including coverage
rate and prediction accuracy) of existing uncertainty quantification methods,
while offering additional benefits in scalability and flexibility.

    

### [[2106.02206] Stochastic Iterative Graph Matching](http://arxiv.org/abs/2106.02206)


  Recent works leveraging Graph Neural Networks to approach graph matching
tasks have shown promising results. Recent progress in learning discrete
distributions poses new opportunities for learning graph matching models. In
this work, we propose a new model, Stochastic Iterative Graph MAtching (SIGMA),
to address the graph matching problem. Our model defines a distribution of
matchings for a graph pair so the model can explore a wide range of possible
matchings. We further introduce a novel multi-step matching procedure, which
learns how to refine a graph pair's matching results incrementally. The model
also includes dummy nodes so that the model does not have to find matchings for
nodes without correspondence. We fit this model to data via scalable stochastic
optimization. We conduct extensive experiments across synthetic graph datasets
as well as biochemistry and computer vision applications. Across all tasks, our
results show that SIGMA can produce significantly improved graph matching
results compared to state-of-the-art models. Ablation studies verify that each
of our components (stochastic training, iterative matching, and dummy nodes)
offers noticeable improvement.

    

### [[2106.03776] CDN-MEDAL: Two-stage Density and Difference Approximation Framework for Motion Analysis](http://arxiv.org/abs/2106.03776)


  Background modeling is a promising research area in video analysis with a
variety of video surveillance applications. Recent years have witnessed the
proliferation of deep neural networks via effective learning-based approaches
in motion analysis. However, these techniques only provide a limited
description of the observed scenes' insufficient properties where a
single-valued mapping is learned to approximate the temporal conditional
averages of the target background. On the other hand, statistical learning in
imagery domains has become one of the most prevalent approaches with high
adaptation to dynamic context transformation, notably Gaussian Mixture Models,
combined with a foreground extraction step. In this work, we propose a novel,
two-stage method of change detection with two convolutional neural networks.
The first architecture is grounded on the unsupervised Gaussian mixtures
statistical learning to describe the scenes' salient features. The second one
implements a light-weight pipeline of foreground detection. Our two-stage
framework contains approximately 3.5K parameters in total but still maintains
rapid convergence to intricate motion patterns. Our experiments on publicly
available datasets show that our proposed networks are not only capable of
generalizing regions of moving objects in unseen cases with promising results
but also are competitive in performance efficiency and effectiveness regarding
foreground segmentation.

    

### [[2106.05727] Cooperative Multi-Agent Fairness and Equivariant Policies](http://arxiv.org/abs/2106.05727)


  We study fairness through the lens of cooperative multi-agent learning. Our
work is motivated by empirical evidence that naive maximization of team reward
yields unfair outcomes for individual team members. To address fairness in
multi-agent contexts, we introduce team fairness, a group-based fairness
measure for multi-agent learning. We then prove that it is possible to enforce
team fairness during policy optimization by transforming the team's joint
policy into an equivariant map. We refer to our multi-agent learning strategy
as Fairness through Equivariance (Fair-E) and demonstrate its effectiveness
empirically. We then introduce Fairness through Equivariance Regularization
(Fair-ER) as a soft-constraint version of Fair-E and show that it reaches
higher levels of utility than Fair-E and fairer outcomes than non-equivariant
policies. Finally, we present novel findings regarding the fairness-utility
trade-off in multi-agent settings; showing that the magnitude of the trade-off
is dependent on agent skill level.

    

### [[2106.06749] A decreasing scaling transition scheme from Adam to SGD](http://arxiv.org/abs/2106.06749)


  Adaptive gradient algorithm (AdaGrad) and its variants, such as RMSProp,
Adam, AMSGrad, etc, have been widely used in deep learning. Although these
algorithms are faster in the early phase of training, their generalization
performance is often not as good as stochastic gradient descent (SGD). Hence, a
trade-off method of transforming Adam to SGD after a certain iteration to gain
the merits of both algorithms is theoretically and practically significant. To
that end, we propose a decreasing scaling transition scheme to achieve a smooth
and stable transition from Adam to SGD, which is called DSTAdam. The
convergence of the proposed DSTAdam is also proved in an online convex setting.
Finally, the effectiveness of the DSTAdam is verified on the CIFAR-10/100
datasets. Our implementation is available at:
this https URL.

    

### [[2106.06755] Tight FPT Approximation for Socially Fair Clustering](http://arxiv.org/abs/2106.06755)


  In this work, we study the socially fair $k$-median/$k$-means problem. We are
given a set of points $P$ in a metric space $\mathcal{X}$ with a distance
function $d(.,.)$. There are $\ell$ groups: $P_1,\dotsc,P_{\ell} \subseteq P$.
We are also given a set $F$ of feasible centers in $\mathcal{X}$. The goal in
the socially fair $k$-median problem is to find a set $C \subseteq F$ of $k$
centers that minimizes the maximum average cost over all the groups. That is,
find $C$ that minimizes the objective function $\Phi(C,P) \equiv \max_{j}
\Big\{ \sum_{x \in P_j} d(C,x)/|P_j| \Big\}$, where $d(C,x)$ is the distance of
$x$ to the closest center in $C$. The socially fair $k$-means problem is
defined similarly by using squared distances, i.e., $d^{2}(.,.)$ instead of
$d(.,.)$. The current best approximation guarantee for both the problems is
$O\left( \frac{\log \ell}{\log \log \ell} \right)$ due to Makarychev and
Vakilian [COLT 2021]. In this work, we study the fixed parameter tractability
of the problems with respect to parameter $k$. We design $(3+\varepsilon)$ and
$(9 + \varepsilon)$ approximation algorithms for the socially fair $k$-median
and $k$-means problems, respectively, in FPT (fixed parameter tractable) time
$f(k,\varepsilon) \cdot n^{O(1)}$, where $f(k,\varepsilon) =
(k/\varepsilon)^{{O}(k)}$ and $n = |P \cup F|$. Furthermore, we show that if
Gap-ETH holds, then better approximation guarantees are not possible in FPT
time.

    

### [[2106.07464] Meta-Interpretive Learning as Metarule Specialisation](http://arxiv.org/abs/2106.07464)


  In Meta-Interpretive Learning (MIL) the metarules, second-order datalog
clauses acting as inductive bias, are manually defined by the user. In this
work we show that second-order metarules for MIL can be learned by MIL. We
define a generality ordering of metarules by $\theta$-subsumption and show that
user-defined sort metarules are derivable by specialisation of the most-general
matrix metarules in a language class; and that these matrix metarules are in
turn derivable by specialisation of third-order punch metarules with variables
that range over the set of second-order literals and for which only an upper
bound on their number of literals need be user-defined. We show that the
cardinality of a metarule language is polynomial in the number of literals in
punch metarules. We re-frame MIL as metarule specialisation by resolution. We
modify the MIL metarule specialisation operator to return new metarules rather
than first-order clauses and prove the correctness of the new operator. We
implement the new operator as TOIL, a sub-system of the MIL system Louise. Our
experiments show that as user-defined sort metarules are progressively replaced
by sort metarules learned by TOIL, Louise's predictive accuracy is maintained
at the cost of a small increase in training times. We conclude that
automatically derived metarules can replace user-defined metarules.

    

### [[2106.10466] TS2Vec: Towards Universal Representation of Time Series](http://arxiv.org/abs/2106.10466)


  This paper presents TS2Vec, a universal framework for learning
representations of time series in an arbitrary semantic level. Unlike existing
methods, TS2Vec performs contrastive learning in a hierarchical way over
augmented context views, which enables a robust contextual representation for
each timestamp. Furthermore, to obtain the representation of an arbitrary
sub-sequence in the time series, we can apply a simple aggregation over the
representations of corresponding timestamps. We conduct extensive experiments
on time series classification tasks to evaluate the quality of time series
representations. As a result, TS2Vec achieves significant improvement over
existing SOTAs of unsupervised time series representation on 125 UCR datasets
and 29 UEA datasets. The learned timestamp-level representations also achieve
superior results in time series forecasting and anomaly detection tasks. A
linear regression trained on top of the learned representations outperforms
previous SOTAs of time series forecasting. Furthermore, we present a simple way
to apply the learned representations for unsupervised anomaly detection, which
establishes SOTA results in the literature. The source code is publicly
available at this https URL.

    

### [[2106.13039] Low-Latency Federated Learning over Wireless Channels with Differential Privacy](http://arxiv.org/abs/2106.13039)


  In federated learning (FL), model training is distributed over clients and
local models are aggregated by a central server. The performance of uploaded
models in such situations can vary widely due to imbalanced data distributions,
potential demands on privacy protections, and quality of transmissions. In this
paper, we aim to minimize FL training delay over wireless channels, constrained
by overall training performance as well as each client's differential privacy
(DP) requirement. We solve this problem in the framework of multi-agent
multi-armed bandit (MAMAB) to deal with the situation where there are multiple
clients confornting different unknown transmission environments, e.g., channel
fading and interferences. Specifically, we first transform the long-term
constraints on both training performance and each client's DP into a virtual
queue based on the Lyapunov drift technique. Then, we convert the MAMAB to a
max-min bipartite matching problem at each communication round, by estimating
rewards with the upper confidence bound (UCB) approach. More importantly, we
propose two efficient solutions to this matching problem, i.e., modified
Hungarian algorithm and greedy matching with a better alternative (GMBA), in
which the first one can achieve the optimal solution with a high complexity
while the second one approaches a better trade-off by enabling a verified
low-complexity with little performance loss. In addition, we develop an upper
bound on the expected regret of this MAMAB based FL framework, which shows a
linear growth over the logarithm of communication rounds, justifying its
theoretical feasibility. Extensive experimental results are conducted to
validate the effectiveness of our proposed algorithms, and the impacts of
various parameters on the FL performance over wireless edge networks are also
discussed.

    

### [[2106.14707] Realtime Robust Malicious Traffic Detection via Frequency Domain Analysis](http://arxiv.org/abs/2106.14707)


  Machine learning (ML) based malicious traffic detection is an emerging
security paradigm, particularly for zero-day attack detection, which is
complementary to existing rule based detection. However, the existing ML based
detection has low detection accuracy and low throughput incurred by inefficient
traffic features extraction. Thus, they cannot detect attacks in realtime
especially in high throughput networks. Particularly, these detection systems
similar to the existing rule based detection can be easily evaded by
sophisticated attacks. To this end, we propose Whisper, a realtime ML based
malicious traffic detection system that achieves both high accuracy and high
throughput by utilizing frequency domain features. It utilizes sequential
features represented by the frequency domain features to achieve bounded
information loss, which ensures high detection accuracy, and meanwhile
constrains the scale of features to achieve high detection throughput.
Particularly, attackers cannot easily interfere with the frequency domain
features and thus Whisper is robust against various evasion attacks. Our
experiments with 42 types of attacks demonstrate that, compared with the
state-of-theart systems, Whisper can accurately detect various sophisticated
and stealthy attacks, achieving at most 18.36% improvement, while achieving two
orders of magnitude throughput. Even under various evasion attacks, Whisper is
still able to maintain around 90% detection accuracy.

    

### [[2107.00051] Local-Global Knowledge Distillation in Heterogeneous Federated Learning with Non-IID Data](http://arxiv.org/abs/2107.00051)


  Federated learning enables multiple clients to collaboratively learn a global
model by periodically aggregating the clients' models without transferring the
local data. However, due to the heterogeneity of the system and data, many
approaches suffer from the "client-drift" issue that could significantly slow
down the convergence of the global model training. As clients perform local
updates on heterogeneous data through heterogeneous systems, their local models
drift apart. To tackle this issue, one intuitive idea is to guide the local
model training by the global teachers, i.e., past global models, where each
client learns the global knowledge from past global models via adaptive
knowledge distillation techniques. Coming from these insights, we propose a
novel approach for heterogeneous federated learning, namely FedGKD, which fuses
the knowledge from historical global models for local training to alleviate the
"client-drift" issue. In this paper, we evaluate FedGKD with extensive
experiments on various CV/NLP datasets (i.e., CIFAR-10/100, Tiny-ImageNet, AG
News, SST5) and different heterogeneous settings. The proposed method is
guaranteed to converge under common assumptions, and achieves superior
empirical accuracy in fewer communication runs than five state-of-the-art
methods.

    

### [[2109.05371] F1: A Fast and Programmable Accelerator for Fully Homomorphic Encryption (Extended Version)](http://arxiv.org/abs/2109.05371)


  Fully Homomorphic Encryption (FHE) allows computing on encrypted data,
enabling secure offloading of computation to untrusted serves. Though it
provides ideal security, FHE is expensive when executed in software, 4 to 5
orders of magnitude slower than computing on unencrypted data. These overheads
are a major barrier to FHE's widespread adoption. We present F1, the first FHE
accelerator that is programmable, i.e., capable of executing full FHE programs.
F1 builds on an in-depth architectural analysis of the characteristics of FHE
computations that reveals acceleration opportunities. F1 is a wide-vector
processor with novel functional units deeply specialized to FHE primitives,
such as modular arithmetic, number-theoretic transforms, and structured
permutations. This organization provides so much compute throughput that data
movement becomes the bottleneck. Thus, F1 is primarily designed to minimize
data movement. The F1 hardware provides an explicitly managed memory hierarchy
and mechanisms to decouple data movement from execution. A novel compiler
leverages these mechanisms to maximize reuse and schedule off-chip and on-chip
data movement. We evaluate F1 using cycle-accurate simulations and RTL
synthesis. F1 is the first system to accelerate complete FHE programs and
outperforms state-of-the-art software implementations by gmean 5400x and by up
to 17000x. These speedups counter most of FHE's overheads and enable new
applications, like real-time private deep learning in the cloud.

    

### [[2109.05430] Ohm-GPU: Integrating New Optical Network and Heterogeneous Memory into GPU Multi-Processors](http://arxiv.org/abs/2109.05430)


  Traditional graphics processing units (GPUs) suffer from the low memory
capacity and demand for high memory bandwidth. To address these challenges, we
propose Ohm-GPU, a new optical network based heterogeneous memory design for
GPUs. Specifically, Ohm-GPU can expand the memory capacity by combing a set of
high-density 3D XPoint and DRAM modules as heterogeneous memory. To prevent
memory channels from throttling throughput of GPU memory system, Ohm-GPU
replaces the electrical lanes in the traditional memory channel with a
high-performance optical network. However, the hybrid memory can introduce
frequent data migrations between DRAM and 3D XPoint, which can unfortunately
occupy the memory channel and increase the optical network traffic. To prevent
the intensive data migrations from blocking normal memory services, Ohm-GPU
revises the existing memory controller and designs a new optical network
infrastructure, which enables the memory channel to serve the data migrations
and memory requests, in parallel. Our evaluation results reveal that Ohm-GPU
can improve the performance by 181% and 27%, compared to a DRAM-based GPU
memory system and the baseline optical network based heterogeneous memory
system, respectively.

    

### [[2005.01593] Electromigration-Aware Architecture for Modern Microprocessors](http://arxiv.org/abs/2005.01593)


  Reliability is a fundamental requirement in any microprocessor to guarantee
correct execution over its lifetime. The design rules related to reliability
depend on the process technology being used and the expected operating
conditions of the device. To meet reliability requirements, advanced process
technologies (28 nm and below) impose highly challenging design rules. Such
design-for-reliability rules have become a major burden on the flow of VLSI
implementation because of the severe physical constraints they impose.
This paper focuses on electromigration (EM), which is one of the major
critical factors affecting semiconductor reliability. EM is the aging process
of on-die wires and vias and is induced by excessive current flow that can
damage wires and may also significantly impact the integrated-circuit clock
frequency. EM exerts a comprehensive global effect on devices because it
impacts wires that may reside inside the standard or custom logical cells,
between logical cells, inside memory elements, and within wires that
interconnect functional blocks.
The design-implementation flow (synthesis and place-and-route) currently
detects violations of EM-reliability rules and attempts to solve them. In
contrast, this paper proposes a new approach to enhance these flows by using
EM-aware architecture. Our results show that the proposed solution can relax EM
design efforts in microprocessors and more than double microprocessor lifetime.
This work demonstrates this proposed approach for modern microprocessors,
although the principals and ideas can be adapted to other cases as well.

    

### [[2109.05072] GPU Algorithms for Efficient Exascale Discretizations](http://arxiv.org/abs/2109.05072)


  In this paper we describe the research and development activities in the
Center for Efficient Exascale Discretization within the US Exascale Computing
Project, targeting state-of-the-art high-order finite-element algorithms for
high-order applications on GPU-accelerated platforms. We discuss the GPU
developments in several components of the CEED software stack, including the
libCEED, MAGMA, MFEM, libParanumal, and Nek projects. We report performance and
capability improvements in several CEED-enabled applications on both NVIDIA and
AMD GPU systems.

    

### [[2109.05151] Accelerated Distributed Laplacian Solvers via Shortcuts](http://arxiv.org/abs/2109.05151)


  In this work we refine the analysis of the distributed Laplacian solver
recently established by Forster, Goranci, Liu, Peng, Sun, and Ye (FOCS '21),
via the Ghaffari-Haeupler framework (SODA '16) of low-congestion shortcuts.
Specifically, if $\epsilon > 0$ represents the error of the solver, we derive
two main results. First, for any $n$-node graph $G$ with hop-diameter $D$ and
treewidth bounded by $k$, we show the existence of a Laplacian solver with
round complexity $O(n^{o(1)}kD \log(1/\epsilon))$ in the CONGEST model. For
graphs with bounded treewidth this circumvents the notorious $\Omega(\sqrt{n})$
lower bound for "global" problems in general graphs. Moreover, following a
recent line of work in distributed algorithms, we consider a hybrid
communication model which enhances CONGEST with very limited global power in
the form of the recently introduced node-capacitated clique. In this model, we
show the existence of a Laplacian solver with round complexity $O(n^{o(1)}
\log(1/\epsilon))$. The unifying thread of these results is an application of
accelerated distributed algorithms for a congested variant of the standard
part-wise aggregation problem that we introduce. This primitive constitutes the
primary building block for simulating "local" operations on low-congestion
minors, and we believe that this framework could be more generally applicable.

    

### [[2109.05176] Implementing Parallel Quick Sort Algorithm on OTIS Hyper Hexa-Cell (OHHC) Interconnection Network](http://arxiv.org/abs/2109.05176)


  This work explores the characteristics of implementing parallel Quick Sort
algorithm over the OTIS Hyper Hexa-Cell interconnection network OHHC. OHHC
interconnection architecture offers efficient processor connectivity by
utilizing both electronic and optical based connections. The work presented
includes analytical evaluation of the algorithm as well as simulated evaluation
over a multi-threading environment. Different experiments were performed using
different OHHC dimensions, different integer array types and different array
sizes. The evaluation and simulation demonstrated encouraging results that
proposes the OHHC connectivity networks as a promising architecture.

    

### [[2109.05329] MODC: Resilience for disaggregated memory architectures using task-based programming](http://arxiv.org/abs/2109.05329)


  Disaggregated memory architectures provide benefits to applications beyond
traditional scale out environments, such as independent scaling of compute and
memory resources. They also provide an independent failure model, where
computations or the compute nodes they run on may fail independently of the
disaggregated memory; thus, data that's resident in the disaggregated memory is
unaffected by the compute failure. Blind application of traditional techniques
for resilience (e.g., checkpoints or data replication) does not take advantage
of these architectures. To demonstrate the potential benefit of these
architectures for resilience, we develop Memory-Oriented Distributed Computing
(MODC), a framework for programming disaggregated architectures that borrows
and adapts ideas from task-based programming models, concurrent programming
techniques, and lock-free data structures. This framework includes a task-based
application programming model and a runtime system that provides scheduling,
coordination, and fault tolerance mechanisms. We present highlights of our MODC
prototype and experimental results demonstrating that MODC-style resilience
outperforms a checkpoint-based approach in the face of failures.

    

### [[2109.05366] A readahead prefetcher for GPU file system layer](http://arxiv.org/abs/2109.05366)


  GPUs are broadly used in I/O-intensive big data applications. Prior works
demonstrate the benefits of using GPU-side file system layer, GPUfs, to improve
the GPU performance and programmability in such workloads. However, GPUfs fails
to provide high performance for a common I/O pattern where a GPU is used to
process a whole data set sequentially. In this work, we propose a number of
system-level optimizations to improve the performance of GPUfs for such
workloads. We perform an in-depth analysis of the interplay between the GPU I/O
access pattern, CPU-GPU PCIe transfers and SSD storage, and identify the main
bottlenecks. We propose a new GPU I/O readahead prefetcher and a GPU page cache
replacement mechanism to resolve them. The GPU I/O readahead prefetcher
achieves more than $2\times$ (geometric mean) higher bandwidth in a series of
microbenchmarks compared to the original GPUfs. Furthermore, we evaluate the
system on 14 applications derived from the RODINIA, PARBOIL and POLYBENCH
benchmark suites. Our prefetching mechanism improves their execution time by up
to 50% and their I/O bandwidth by 82% compared to the traditional CPU-only data
transfer techniques.

    

### [[2109.05410] Accelerating GPU-Based Out-of-Core Stencil Computation with On-the-Fly Compression](http://arxiv.org/abs/2109.05410)


  Stencil computation is an important class of scientific applications that can
be efficiently executed by graphics processing units (GPUs). Out-of-core
approach helps run large scale stencil codes that process data with sizes
larger than the limited capacity of GPU memory. However, the performance of the
GPU-based out-of-core stencil computation is always limited by the data
transfer between the CPU and GPU. Many optimizations have been explored to
reduce such data transfer, but the study on the use of on-the-fly compression
techniques is far from sufficient. In this study, we propose a method that
accelerates the GPU-based out-of-core stencil computation with on-the-fly
compression. We introduce a novel data compression approach that solves the
data dependency between two contiguous decomposed data blocks. We also modify a
widely used GPU-based compression library to support pipelining that overlaps
CPU/GPU data transfer with GPU computation. Experimental results show that the
proposed method achieved a speedup of 1.2x compared the method without
compression. Moreover, although the precision loss involved by compression
increased with the number of time steps, the precision loss was trivial up to
4,320 time steps, demonstrating the usefulness of the proposed method.

    

### [[2109.05412] Hybrid Workload Scheduling on HPC Systems](http://arxiv.org/abs/2109.05412)


  Traditionally, on-demand, rigid, and malleable applications have been
scheduled and executed on separate systems. The ever-growing workload demands
and rapidly developing HPC infrastructure trigger the interest of converging
these applications on a single HPC system. Although allocating the hybrid
workloads within one system could potentially improve system efficiency, it is
difficult to balance the tradeoff between the responsiveness of on-demand
requests, the incentive for malleable jobs, and the performance of rigid
applications. In this study, we present several scheduling mechanisms to
address the issues involved in co-scheduling on-demand, rigid, and malleable
jobs on a single HPC system. We extensively evaluate and compare their
performance under various configurations and workloads. Our experimental
results show that our proposed mechanisms are capable of serving on-demand
workloads with minimal delay, offering incentives for declaring malleability,
and improving system performance.

    

### [[2109.05451] H2Opus: A distributed-memory multi-GPU software package for non-local operators](http://arxiv.org/abs/2109.05451)


  Hierarchical $\mathcal{H}^2$-matrices are asymptotically optimal
representations for the discretizations of non-local operators such as those
arising in integral equations or from kernel functions. Their $O(N)$ complexity
in both memory and operator application makes them particularly suited for
large-scale problems. As a result, there is a need for software that provides
support for distributed operations on these matrices to allow large-scale
problems to be represented. In this paper, we present high-performance,
distributed-memory GPU-accelerated algorithms and implementations for
matrix-vector multiplication and matrix recompression of hierarchical matrices
in the $\mathcal{H}^2$ format.
The algorithms are a new module of H2Opus, a performance-oriented package
that supports a broad variety of $\mathcal{H}^2$-matrix operations on CPUs and
GPUs. Performance in the distributed GPU setting is achieved by marshaling the
tree data of the hierarchical matrix representation to allow batched kernels to
be executed on the individual GPUs. MPI is used for inter-process
communication. We optimize the communication data volume and hide much of the
communication cost with local compute phases of the algorithms. Results show
near-ideal scalability up to 1024 NVIDIA V100 GPUs on Summit, with performance
exceeding 2.3 Tflop/s/GPU for the matrix-vector multiplication, and 670
Gflops/s/GPU for matrix compression, which involves batched QR and SVD
operations.
We illustrate the flexibility and efficiency of the library by solving a 2D
variable diffusivity integral fractional diffusion problem with an algebraic
multigrid-preconditioned Krylov solver and demonstrate scalability up to 16M
degrees of freedom problems on 64 GPUs.

    

### [[2109.05636] IFogSim2: An Extended iFogSim Simulator for Mobility, Clustering, and Microservice Management in Edge and Fog Computing Environments](http://arxiv.org/abs/2109.05636)


  Internet of Things (IoT) has already proven to be the building block for
next-generation Cyber-Physical Systems (CPSs). The considerable amount of data
generated by the IoT devices needs latency-sensitive processing, which is not
feasible by deploying the respective applications in remote Cloud datacentres.
Edge/Fog computing, a promising extension of Cloud at the IoT-proximate
network, can meet such requirements for smart CPSs. However, the structural and
operational differences of Edge/Fog infrastructure resist employing Cloud-based
service regulations directly to these environments. As a result, many research
works have been recently conducted, focusing on efficient application and
resource management in Edge/Fog computing environments. Scalable Edge/Fog
infrastructure is a must to validate these policies, which is also challenging
to accommodate in the real-world due to high cost and implementation time.
Considering simulation as a key to this constraint, various software has been
developed that can imitate the physical behaviour of Edge/Fog computing
environments. Nevertheless, the existing simulators often fail to support
advanced service management features because of their monolithic architecture,
lack of actual dataset, and limited scope for a periodic update. To overcome
these issues, we have developed multiple simulation models for service
migration, dynamic distributed cluster formation, and microservice
orchestration for Edge/Fog computing in this work and integrated with the
existing iFogSim simulation toolkit for launching it as iFogSim2. The
performance of iFogSim2 and its built-in policies are evaluated using three use
case scenarios and compared with the contemporary simulators and benchmark
policies under different settings. Results indicate that the proposed solution
outperform others in service management time, network usage, ram consumption,
and simulation time.

    

### [[2004.04613] QuickSilver: A Modeling and Parameterized Verification Framework for Systems with Distributed Agreement (Extended Version)](http://arxiv.org/abs/2004.04613)


  The last decade has sparked several valiant efforts in deductive verification
of distributed agreement protocols such as consensus and leader election.
Oddly, there have been far fewer verification efforts that go beyond the core
protocols and target applications that are built on top of agreement protocols.
This is unfortunate, as agreement-based distributed services such as data
stores, locks, and ledgers are ubiquitous and potentially permit modular,
scalable verification approaches that mimic their modular design.
We address this need for verification of distributed agreement-based systems
through our novel modeling and verification framework, QuickSilver, that is not
only modular, but also fully automated. The key enabling feature of QuickSilver
is our encoding of abstractions of verified agreement protocols that
facilitates modular, decidable, and scalable automated verification. We
demonstrate the potential of QuickSilver by modeling and efficiently verifying
a series of tricky case studies, adapted from real-world applications, such as
a data store, a lock service, a surveillance system, a pathfinding algorithm
for mobile robots, and more.

    

### [[2011.05277] Qualities, challenges and future of genetic algorithms: a literature review](http://arxiv.org/abs/2011.05277)


  Genetic algorithms, computer programs that simulate natural evolution, are
increasingly applied across many disciplines. They have been used to solve
various optimisation problems from neural network architecture search to
strategic games, and to model phenomena of adaptation and learning. Expertise
on the qualities and drawbacks of this technique is largely scattered across
the literature or former, motivating an compilation of this knowledge at the
light of the most recent developments of the field. In this review, we present
genetic algorithms, their qualities, limitations and challenges, as well as
some future development perspectives. Genetic algorithms are capable of
exploring large and complex spaces of possible solutions, to quickly locate
promising elements, and provide an adequate modelling tool to describe
evolutionary systems, from games to economies. They however suffer from high
computation costs, difficult parameter configuration, and crucial
representation of the solutions. Recent developments such as GPU, parallel and
quantum computing, conception of powerful parameter control methods, and novel
approaches in representation strategies, may be keys to overcome those
limitations. This compiling review aims at informing practitioners and
newcomers in the field alike in their genetic algorithm research, and at
outlining promising avenues for future research. It highlights the potential
for interdisciplinary research associating genetic algorithms to pulse original
discoveries in social sciences, open ended evolution, artificial life and AI.

    

### [[2105.05408] A survey of size counting in population protocols](http://arxiv.org/abs/2105.05408)


  The population protocol model describes a network of $n$ anonymous agents who
cannot control with whom they interact. The agents collectively solve some
computational problem through random pairwise interactions, each agent updating
its own state in response to seeing the state of the other agent. They are
equivalent to the model of chemical reaction networks, describing abstract
chemical reactions such as $A+B \rightarrow C+D$, when the latter is subject to
the restriction that all reactions have two reactants and two products, and all
rate constants are 1. The counting problem is that of designing a protocol so
that $n$ agents, all starting in the same state, eventually converge to states
where each agent encodes in its state an exact or approximate description of
population size $n$. In this survey paper, we describe recent algorithmic
advances on the counting problem.

    

### [[2109.05073] Simultaneous Perception-Action Design via Invariant Finite Belief Sets](http://arxiv.org/abs/2109.05073)


  Although perception is an increasingly dominant portion of the overall
computational cost for autonomous systems, only a fraction of the information
perceived is likely to be relevant to the current task. To alleviate these
perception costs, we develop a novel simultaneous perception-action design
framework wherein an agent senses only the task-relevant information. This
formulation differs from that of a partially observable Markov decision
process, since the agent is free to synthesize not only its policy for action
selection but also its belief-dependent observation function. The method
enables the agent to balance its perception costs with those incurred by
operating in its environment. To obtain a computationally tractable solution,
we approximate the value function using a novel method of invariant finite
belief sets, wherein the agent acts exclusively on a finite subset of the
continuous belief space. We solve the approximate problem through value
iteration in which a linear program is solved individually for each belief
state in the set, in each iteration. Finally, we prove that the value
functions, under an assumption on their structure, converge to their continuous
state-space values as the sample density increases.

    

### [[2109.05083] Preliminary Wildfire Detection Using State-of-the-art PTZ (Pan, Tilt, Zoom) Camera Technology and Convolutional Neural Networks](http://arxiv.org/abs/2109.05083)


  Wildfires are uncontrolled fires in the environment that can be caused by
humans or nature. In 2020 alone, wildfires in California have burned 4.2
million acres, damaged 10,500 buildings or structures, and killed more than 31
people, exacerbated by climate change and a rise in average global
temperatures. This also means there has been an increase in the costs of
extinguishing these treacherous wildfires. The objective of the research is to
detect forest fires in their earlier stages to prevent them from spreading,
prevent them from causing damage to a variety of things, and most importantly,
reduce or eliminate the chances of someone dying from a wildfire. A fire
detection system should be efficient and accurate with respect to extinguishing
wildfires in their earlier stages to prevent the spread of them along with
their consequences. Computer Vision is potentially a more reliable, fast, and
widespread method we need. The current research in the field of preliminary
fire detection has several problems related to unrepresentative data being used
to train models and their existing varied amounts of label imbalance in the
classes of their dataset. We propose a more representative and evenly
distributed data through better settings, lighting, atmospheres, etc., and
class distribution in the entire dataset. After thoroughly examining the
results of this research, it can be inferred that they supported the datasets
strengths by being a viable resource when tested in the real world on
unfamiliar data. This is evident since as the model trains on the dataset, it
is able to generalize on it, hence confirming this is a viable Machine Learning
setting that has practical impact.

    

### [[2109.05187] TopicRefine: Joint Topic Prediction and Dialogue Response Generation for Multi-turn End-to-End Dialogue System](http://arxiv.org/abs/2109.05187)


  A multi-turn dialogue always follows a specific topic thread, and topic shift
at the discourse level occurs naturally as the conversation progresses,
necessitating the model's ability to capture different topics and generate
topic-aware responses. Previous research has either predicted the topic first
and then generated the relevant response, or simply applied the attention
mechanism to all topics, ignoring the joint distribution of the topic
prediction and response generation models and resulting in uncontrollable and
unrelated responses. In this paper, we propose a joint framework with a topic
refinement mechanism to learn these two tasks simultaneously. Specifically, we
design a three-pass iteration mechanism to generate coarse response first, then
predict corresponding topics, and finally generate refined response conditioned
on predicted topics. Moreover, we utilize GPT2DoubleHeads and BERT for the
topic prediction task respectively, aiming to investigate the effects of joint
learning and the understanding ability of GPT model. Experimental results
demonstrate that our proposed framework achieves new state-of-the-art
performance at response generation task and the great potential understanding
capability of GPT model.

    

### [[2109.05190] Eliciting Knowledge from Language Models for Event Extraction](http://arxiv.org/abs/2109.05190)


  Eliciting knowledge contained in language models via prompt-based learning
has shown great potential in many natural language processing tasks, such as
text classification and generation. Whereas, the applications for more complex
tasks such as event extraction are less studied, since the design of prompt is
not straightforward due to the complicated types and arguments. In this paper,
we explore to elicit the knowledge from pre-trained language models for event
trigger detection and argument extraction. Specifically, we present various
joint trigger/argument prompt methods, which can elicit more complementary
knowledge by modeling the interactions between different triggers or arguments.
The experimental results on the benchmark dataset, namely ACE2005, show the
great advantages of our proposed approach. In particular, our approach is
superior to the recent advanced methods in the few-shot scenario where only a
few samples are used for training.

    

### [[2109.05205] Contrastive Quantization with Code Memory for Unsupervised Image Retrieval](http://arxiv.org/abs/2109.05205)


  The high efficiency in computation and storage makes hashing (including
binary hashing and quantization) a common strategy in large-scale retrieval
systems. To alleviate the reliance on expensive annotations, unsupervised deep
hashing becomes an important research problem. This paper provides a novel
solution to unsupervised deep quantization, namely Contrastive Quantization
with Code Memory (MeCoQ). Different from existing reconstruction-based
strategies, we learn unsupervised binary descriptors by contrastive learning,
which can better capture discriminative visual semantics. Besides, we uncover
that codeword diversity regularization is critical to prevent contrastive
learning-based quantization from model degeneration. Moreover, we introduce a
novel quantization code memory module that boosts contrastive learning with
lower feature drift than conventional feature memories. Extensive experiments
on benchmark datasets show that MeCoQ outperforms state-of-the-art methods.

    

### [[2109.05206] Pyramid Hybrid Pooling Quantization for Efficient Fine-Grained Image Retrieval](http://arxiv.org/abs/2109.05206)


  Deep hashing approaches, including deep quantization and deep binary hashing,
have become a common solution to large-scale image retrieval due to high
computation and storage efficiency. Most existing hashing methods can not
produce satisfactory results for fine-grained retrieval, because they usually
adopt the outputs of the last CNN layer to generate binary codes, which is less
effective to capture subtle but discriminative visual details. To improve
fine-grained image hashing, we propose Pyramid Hybrid Pooling Quantization
(PHPQ). Specifically, we propose a Pyramid Hybrid Pooling (PHP) module to
capture and preserve fine-grained semantic information from multi-level
features. Besides, we propose a learnable quantization module with a partial
attention mechanism, which helps to optimize the most relevant codewords and
improves the quantization. Comprehensive experiments demonstrate that PHPQ
outperforms state-of-the-art methods.

    

### [[2109.05217] Empirical Analysis of Training Strategies of Transformer-based Japanese Chit-chat Systems](http://arxiv.org/abs/2109.05217)


  In recent years, several high-performance conversational systems have been
proposed based on the Transformer encoder-decoder model. Although previous
studies analyzed the effects of the model parameters and the decoding method on
subjective dialogue evaluations with overall metrics, they did not analyze how
the differences of fine-tuning datasets affect on user's detailed impression.
In addition, the Transformer-based approach has only been verified for English,
not for such languages with large inter-language distances as Japanese. In this
study, we develop large-scale Transformer-based Japanese dialogue models and
Japanese chit-chat datasets to examine the effectiveness of the
Transformer-based approach for building chit-chat dialogue systems. We
evaluated and analyzed the impressions of human dialogues in different
fine-tuning datasets, model parameters, and the use of additional information.

    

### [[2109.05233] AdaK-NER: An Adaptive Top-K Approach for Named Entity Recognition with Incomplete Annotations](http://arxiv.org/abs/2109.05233)


  State-of-the-art Named Entity Recognition(NER) models rely heavily on large
amountsof fully annotated training data. However, ac-cessible data are often
incompletely annotatedsince the annotators usually lack comprehen-sive
knowledge in the target domain. Normallythe unannotated tokens are regarded as
non-entities by default, while we underline thatthese tokens could either be
non-entities orpart of any entity. Here, we study NER mod-eling with incomplete
annotated data whereonly a fraction of the named entities are la-beled, and the
unlabeled tokens are equiva-lently multi-labeled by every possible label.Taking
multi-labeled tokens into account, thenumerous possible paths can distract the
train-ing model from the gold path (ground truthlabel sequence), and thus
hinders the learn-ing ability. In this paper, we propose AdaK-NER, named the
adaptive top-Kapproach, tohelp the model focus on a smaller feasible re-gion
where the gold path is more likely to belocated. We demonstrate the superiority
ofour approach through extensive experimentson both English and Chinese
datasets, aver-agely improving 2% in F-score on the CoNLL-2003 and over 10% on
two Chinese datasetscompared with the prior state-of-the-art works.

    

### [[2109.05234] Prior Omission of Dissimilar Source Domain(s) for Cost-Effective Few-Shot Learning](http://arxiv.org/abs/2109.05234)


  Few-shot slot tagging is an emerging research topic in the field of Natural
Language Understanding (NLU). With sufficient annotated data from source
domains, the key challenge is how to train and adapt the model to another
target domain which only has few labels. Conventional few-shot approaches use
all the data from the source domains without considering inter-domain relations
and implicitly assume each sample in the domain contributes equally. However,
our experiments show that the data distribution bias among different domains
will significantly affect the adaption performance. Moreover, transferring
knowledge from dissimilar domains will even introduce some extra noises so that
affect the performance of models. To tackle this problem, we propose an
effective similarity-based method to select data from the source domains. In
addition, we propose a Shared-Private Network (SP-Net) for the few-shot slot
tagging task. The words from the same class would have some shared features. We
extract those shared features from the limited annotated data on the target
domain and merge them together as the label embedding to help us predict other
unlabelled data on the target domain. The experiment shows that our method
outperforms the state-of-the-art approaches with fewer source data. The result
also proves that some training data from dissimilar sources are redundant and
even negative for the adaption.

    

### [[2109.05238] Universal Simultaneous Machine Translation with Mixture-of-Experts Wait-k Policy](http://arxiv.org/abs/2109.05238)


  Simultaneous machine translation (SiMT) generates translation before reading
the entire source sentence and hence it has to trade off between translation
quality and latency. To fulfill the requirements of different translation
quality and latency in practical applications, the previous methods usually
need to train multiple SiMT models for different latency levels, resulting in
large computational costs. In this paper, we propose a universal SiMT model
with Mixture-of-Experts Wait-k Policy to achieve the best translation quality
under arbitrary latency with only one trained model. Specifically, our method
employs multi-head attention to accomplish the mixture of experts where each
head is treated as a wait-k expert with its own waiting words number, and given
a test latency and source inputs, the weights of the experts are accordingly
adjusted to produce the best translation. Experiments on three datasets show
that our method outperforms all the strong baselines under different latency,
including the state-of-the-art adaptive policy.

    

### [[2109.05244] Modeling Concentrated Cross-Attention for Neural Machine Translation with Gaussian Mixture Model](http://arxiv.org/abs/2109.05244)


  Cross-attention is an important component of neural machine translation
(NMT), which is always realized by dot-product attention in previous methods.
However, dot-product attention only considers the pair-wise correlation between
words, resulting in dispersion when dealing with long sentences and neglect of
source neighboring relationships. Inspired by linguistics, the above issues are
caused by ignoring a type of cross-attention, called concentrated attention,
which focuses on several central words and then spreads around them. In this
work, we apply Gaussian Mixture Model (GMM) to model the concentrated attention
in cross-attention. Experiments and analyses we conducted on three datasets
show that the proposed method outperforms the baseline and has significant
improvement on alignment quality, N-gram accuracy, and long sentence
translation.

    

### [[2109.05300] On syntactically similar logic programs and sequential decompositions](http://arxiv.org/abs/2109.05300)


  Rule-based reasoning is an essential part of human intelligence prominently
formalized in artificial intelligence research via logic programs. Describing
complex objects as the composition of elementary ones is a common strategy in
computer science and science in general. The author has recently introduced the
sequential composition of logic programs in the context of logic-based
analogical reasoning and learning in logic programming. Motivated by these
applications, in this paper we construct a qualitative and algebraic notion of
syntactic logic program similarity from sequential decompositions of programs.
We then show how similarity can be used to answer queries across different
domains via a one-step reduction. In a broader sense, this paper is a further
step towards an algebra of logic programs first envisioned by Richard O. Keefe
in 1985 with applications to analogical reasoning.

    

### [[2109.05327] An Objective Metric for Explainable AI: How and Why to Estimate the Degree of Explainability](http://arxiv.org/abs/2109.05327)


  Numerous government initiatives (e.g. the EU with GDPR) are coming to the
conclusion that the increasing complexity of modern software systems must be
contrasted with some Rights to Explanation and metrics for the Impact
Assessment of these tools, that allow humans to understand and oversee the
output of Automated Decision Making systems. Explainable AI was born as a
pathway to allow humans to explore and understand the inner working of complex
systems. But establishing what is an explanation and objectively evaluating
explainability, are not trivial tasks. With this paper, we present a new
model-agnostic metric to measure the Degree of eXplainability of correct
information in an objective way, exploiting a specific model from Ordinary
Language Philosophy called the Achinstein's Theory of Explanations. In order to
understand whether this metric is actually behaving as explainability is
expected to, we designed a few experiments and a user-study on two realistic
AI-based systems for healthcare and finance, involving famous AI technology
including Artificial Neural Networks and TreeSHAP. The results we obtained are
very encouraging, suggesting that our proposed metric for measuring the Degree
of eXplainability is robust on several scenarios and it can be eventually
exploited for a lawful Impact Assessment of an Automated Decision Making
system.

    

### [[2109.05346] BGT-Net: Bidirectional GRU Transformer Network for Scene Graph Generation](http://arxiv.org/abs/2109.05346)


  Scene graphs are nodes and edges consisting of objects and object-object
relationships, respectively. Scene graph generation (SGG) aims to identify the
objects and their relationships. We propose a bidirectional GRU (BiGRU)
transformer network (BGT-Net) for the scene graph generation for images. This
model implements novel object-object communication to enhance the object
information using a BiGRU layer. Thus, the information of all objects in the
image is available for the other objects, which can be leveraged later in the
object prediction step. This object information is used in a transformer
encoder to predict the object class as well as to create object-specific edge
information via the use of another transformer encoder. To handle the dataset
bias induced by the long-tailed relationship distribution, softening with a
log-softmax function and adding a bias adaptation term to regulate the bias for
every relation prediction individually showed to be an effective approach. We
conducted an elaborate study on experiments and ablations using open-source
datasets, i.e., Visual Genome, Open-Images, and Visual Relationship Detection
datasets, demonstrating the effectiveness of the proposed model over state of
the art.

    

### [[2109.05353] Border-SegGCN: Improving Semantic Segmentation by Refining the Border Outline using Graph Convolutional Network](http://arxiv.org/abs/2109.05353)


  We present Border-SegGCN, a novel architecture to improve semantic
segmentation by refining the border outline using graph convolutional networks
(GCN). The semantic segmentation network such as Unet or DeepLabV3+ is used as
a base network to have pre-segmented output. This output is converted into a
graphical structure and fed into the GCN to improve the border pixel prediction
of the pre-segmented output. We explored and studied the factors such as border
thickness, number of edges for a node, and the number of features to be fed
into the GCN by performing experiments. We demonstrate the effectiveness of the
Border-SegGCN on the CamVid and Carla dataset, achieving a test set performance
of 81.96% without any post-processing on CamVid dataset. It is higher than the
reported state of the art mIoU achieved on CamVid dataset by 0.404%

    

### [[2109.05406] Guiding Topic Flows in the Generative Chatbot by Enhancing the ConceptNet with the Conversation Corpora](http://arxiv.org/abs/2109.05406)


  Human conversations consist of reasonable and natural topic flows, which are
observed as the shifts of the mentioned concepts across utterances. Previous
chatbots that incorporate the external commonsense knowledge graph prove that
modeling the concept shifts can effectively alleviate the dull and
uninformative response dilemma. However, there still exists a gap between the
concept relations in the natural conversation and those in the external
commonsense knowledge graph, which is an issue to solve. Specifically, the
concept relations in the external commonsense knowledge graph are not
intuitively built from the conversational scenario but the world knowledge,
which makes them insufficient for the chatbot construction. To bridge the above
gap, we propose the method to supply more concept relations extracted from the
conversational corpora and reconstruct an enhanced concept graph for the
chatbot construction. In addition, we present a novel, powerful, and fast graph
encoding architecture named the Edge-Transformer to replace the traditional GNN
architecture. Experimental results on the Reddit conversation dataset indicate
our proposed method significantly outperforms strong baseline systems and
achieves new SOTA results. Further analysis individually proves the
effectiveness of the enhanced concept graph and the Edge-Transformer
architecture.

    

### [[2109.05413] Learning Selective Communication for Multi-Agent Path Finding](http://arxiv.org/abs/2109.05413)


  Learning communication via deep reinforcement learning (RL) or imitation
learning (IL) has recently been shown to be an effective way to solve
Multi-Agent Path Finding (MAPF). However, existing communication based MAPF
solvers focus on broadcast communication, where an agent broadcasts its message
to all other or predefined agents. It is not only impractical but also leads to
redundant information that could even impair the multi-agent cooperation. A
succinct communication scheme should learn which information is relevant and
influential to each agent's decision making process. To address this problem,
we consider a request-reply scenario and propose Decision Causal Communication
(DCC), a simple yet efficient model to enable agents to select neighbors to
conduct communication during both training and execution. Specifically, a
neighbor is determined as relevant and influential only when the presence of
this neighbor causes the decision adjustment on the central agent. This
judgment is learned only based on agent's local observation and thus suitable
for decentralized execution to handle large scale problems. Empirical
evaluation in obstacle-rich environment indicates the high success rate with
low communication overhead of our method.

    

### [[2109.05426] Zero-Shot Text-to-Speech for Text-Based Insertion in Audio Narration](http://arxiv.org/abs/2109.05426)


  Given a piece of speech and its transcript text, text-based speech editing
aims to generate speech that can be seamlessly inserted into the given speech
by editing the transcript. Existing methods adopt a two-stage approach:
synthesize the input text using a generic text-to-speech (TTS) engine and then
transform the voice to the desired voice using voice conversion (VC). A major
problem of this framework is that VC is a challenging problem which usually
needs a moderate amount of parallel training data to work satisfactorily. In
this paper, we propose a one-stage context-aware framework to generate natural
and coherent target speech without any training data of the target speaker. In
particular, we manage to perform accurate zero-shot duration prediction for the
inserted text. The predicted duration is used to regulate both text embedding
and speech embedding. Then, based on the aligned cross-modality input, we
directly generate the mel-spectrogram of the edited speech with a
transformer-based decoder. Subjective listening tests show that despite the
lack of training data for the speaker, our method has achieved satisfactory
results. It outperforms a recent zero-shot TTS engine by a large margin.

    

### [[2109.05427] Not All Negatives are Equal: Label-Aware Contrastive Loss for Fine-grained Text Classification](http://arxiv.org/abs/2109.05427)


  Fine-grained classification involves dealing with datasets with larger number
of classes with subtle differences between them. Guiding the model to focus on
differentiating dimensions between these commonly confusable classes is key to
improving performance on fine-grained tasks. In this work, we analyse the
contrastive fine-tuning of pre-trained language models on two fine-grained text
classification tasks, emotion classification and sentiment analysis. We
adaptively embed class relationships into a contrastive objective function to
help differently weigh the positives and negatives, and in particular,
weighting closely confusable negatives more than less similar negative
examples. We find that Label-aware Contrastive Loss outperforms previous
contrastive methods, in the presence of larger number and/or more confusable
classes, and helps models to produce output distributions that are more
differentiated.

    

### [[2109.05460] End-to-End Conversational Search for Online Shopping with Utterance Transfer](http://arxiv.org/abs/2109.05460)


  Successful conversational search systems can present natural, adaptive and
interactive shopping experience for online shopping customers. However,
building such systems from scratch faces real word challenges from both
imperfect product schema/knowledge and lack of training dialog this http URL this
work we first propose ConvSearch, an end-to-end conversational search system
that deeply combines the dialog system with search. It leverages the text
profile to retrieve products, which is more robust against imperfect product
schema/knowledge compared with using product attributes alone. We then address
the lack of data challenges by proposing an utterance transfer approach that
generates dialogue utterances by using existing dialog from other domains, and
leveraging the search behavior data from e-commerce retailer. With utterance
transfer, we introduce a new conversational search dataset for online shopping.
Experiments show that our utterance transfer method can significantly improve
the availability of training dialogue data without crowd-sourcing, and the
conversational search system significantly outperformed the best tested
baseline.

    

### [[2109.05486] A Socially Aware Reinforcement Learning Agent for The Single Track Road Problem](http://arxiv.org/abs/2109.05486)


  We present the single track road problem. In this problem two agents face
each-other at opposite positions of a road that can only have one agent pass at
a time. We focus on the scenario in which one agent is human, while the other
is an autonomous agent. We run experiments with human subjects in a simple grid
domain, which simulates the single track road problem. We show that when data
is limited, building an accurate human model is very challenging, and that a
reinforcement learning agent, which is based on this data, does not perform
well in practice. However, we show that an agent that tries to maximize a
linear combination of the human's utility and its own utility, achieves a high
score, and significantly outperforms other baselines, including an agent that
tries to maximize only its own utility.

    

### [[2109.05488] ArtiBoost: Boosting Articulated 3D Hand-Object Pose Estimation via Online Exploration and Synthesis](http://arxiv.org/abs/2109.05488)


  Estimating the articulated 3D hand-object pose from a single RGB image is a
highly ambiguous and challenging problem requiring large-scale datasets that
contain diverse hand poses, object poses, and camera viewpoints. Most
real-world datasets lack this diversity. In contrast, synthetic datasets can
easily ensure vast diversity, but learning from them is inefficient and suffers
from heavy training consumption. To address the above issues, we propose
ArtiBoost, a lightweight online data enrichment method that boosts articulated
hand-object pose estimation from the data perspective. ArtiBoost is employed
along with a real-world source dataset. During training, ArtiBoost
alternatively performs data exploration and synthesis. ArtiBoost can cover
various hand-object poses and camera viewpoints based on a Compositional
hand-object Configuration and Viewpoint space (CCV-space) and can adaptively
enrich the current hard-discernable samples by a mining strategy. We apply
ArtiBoost on a simple learning baseline network and demonstrate the performance
boost on several hand-object benchmarks. As an illustrative example, with
ArtiBoost, even a simple baseline network can outperform the previous
start-of-the-art based on Transformer on the HO3D dataset. Our code is
available at this https URL.

    

### [[2109.05489] Illuminating Diverse Neural Cellular Automata for Level Generation](http://arxiv.org/abs/2109.05489)


  We present a method of generating a collection of neural cellular automata
(NCA) to design video game levels. While NCAs have so far only been trained via
supervised learning, we present a quality diversity (QD) approach to generating
a collection of NCA level generators. By framing the problem as a QD problem,
our approach can train diverse level generators, whose output levels vary based
on aesthetic or functional criteria. To efficiently generate NCAs, we train
generators via Covariance Matrix Adaptation MAP-Elites (CMA-ME), a quality
diversity algorithm which specializes in continuous search spaces. We apply our
new method to generate level generators for several 2D tile-based games: a maze
game, Sokoban, and Zelda. Our results show that CMA-ME can generate small NCAs
that are diverse yet capable, often satisfying complex solvability criteria for
deterministic agents. We compare against a Compositional Pattern-Producing
Network (CPPN) baseline trained to produce diverse collections of generators
and show that the NCA representation yields a better exploration of
level-space.

    

### [[2109.05542] Unsupervised Domain Adaptive Learning via Synthetic Data for Person Re-identification](http://arxiv.org/abs/2109.05542)


  Person re-identification (re-ID) has gained more and more attention due to
its widespread applications in intelligent video surveillance. Unfortunately,
the mainstream deep learning methods still need a large quantity of labeled
data to train models, and annotating data is an expensive work in real-world
scenarios. In addition, due to domain gaps between different datasets, the
performance is dramatically decreased when re-ID models pre-trained on
label-rich datasets (source domain) are directly applied to other unlabeled
datasets (target domain). In this paper, we attempt to remedy these problems
from two aspects, namely data and methodology. Firstly, we develop a data
collector to automatically generate synthetic re-ID samples in a computer game,
and construct a data labeler to simultaneously annotate them, which free humans
from heavy data collections and annotations. Based on them, we build two
synthetic person re-ID datasets with different scales, "GSPR" and "mini-GSPR"
datasets. Secondly, we propose a synthesis-based multi-domain collaborative
refinement (SMCR) network, which contains a synthetic pretraining module and
two collaborative-refinement modules to implement sufficient learning for the
valuable knowledge from multiple domains. Extensive experiments show that our
proposed framework obtains significant performance improvements over the
state-of-the-art methods on multiple unsupervised domain adaptation tasks of
person re-ID.

    

### [[2109.05622] Nimber-Preserving Reductions and Homomorphic Sprague-Grundy Game Encodings](http://arxiv.org/abs/2109.05622)


  The concept of nimbers--a.k.a. Grundy-values or nim-values--is fundamental to
combinatorial game theory. Nimbers provide a complete characterization of
strategic interactions among impartial games in their disjunctive sums as well
as the winnability. In this paper, we initiate a study of nimber-preserving
reductions among impartial games. These reductions enhance the
winnability-preserving reductions in traditional computational
characterizations of combinatorial games. We prove that Generalized Geography
is complete for the natural class, $\cal{I}^P$ , of polynomially-short
impartial rulesets under nimber-preserving reductions, a property we refer to
as Sprague-Grundy-complete. In contrast, we also show that not every
PSPACE-complete ruleset in $\cal{I}^P$ is Sprague-Grundy-complete for
$\cal{I}^P$ .
By considering every impartial game as an encoding of its nimber, our
technical result establishes the following striking cryptography-inspired
homomorphic theorem: Despite the PSPACE-completeness of nimber computation for
$\cal{I}^P$ , there exists a polynomial-time algorithm to construct, for any
pair of games $G_1$, $G_2$ of $\cal{I}^P$ , a prime game (i.e. a game that
cannot be written as a sum) $H$ of $\cal{I}^P$ , satisfying: nimber($H$) =
nimber($G_1$) $\oplus$ nimber($G_2$).

    

### [[2009.09571] Semi-supervised Semantic Segmentation of Prostate and Organs-at-Risk on 3D Pelvic CT Images](http://arxiv.org/abs/2009.09571)


  Automated segmentation can assist radiotherapy treatment planning by saving
manual contouring efforts and reducing intra-observer and inter-observer
variations. The recent development of deep learning approaches has revoluted
medical data processing, including semantic segmentation, by dramatically
improving performance. However, training effective deep learning models usually
require a large amount of high-quality labeled data, which are often costly to
collect. We developed a novel semi-supervised adversarial deep learning
approach for 3D pelvic CT image semantic segmentation. Unlike supervised deep
learning methods, the new approach can utilize both annotated and un-annotated
data for training. It generates un-annotated synthetic data by a data
augmentation scheme using generative adversarial networks (GANs). We applied
the new approach to segmenting multiple organs in male pelvic CT images, where
CT images without annotations and GAN-synthesized un-annotated images were used
in semi-supervised learning. Experimental results, evaluated by three metrics
(Dice similarity coefficient, average Hausdorff distance, and average surface
Hausdorff distance), showed that the new method achieved either comparable
performance with substantially fewer annotated images or better performance
with the same amount of annotated data, outperforming the existing
state-of-the-art methods.

    

### [[2010.01470] User Fairness, Item Fairness, and Diversity for Rankings in Two-Sided Markets](http://arxiv.org/abs/2010.01470)


  Ranking items by their probability of relevance has long been the goal of
conventional ranking systems. While this maximizes traditional criteria of
ranking performance, there is a growing understanding that it is an
oversimplification in online platforms that serve not only a diverse user
population, but also the producers of the items. In particular, ranking
algorithms are expected to be fair in how they serve all groups of users -- not
just the majority group -- and they also need to be fair in how they divide
exposure among the items. These fairness considerations can partially be met by
adding diversity to the rankings, as done in several recent works. However, we
show in this paper that user fairness, item fairness and diversity are
fundamentally different concepts. In particular, we find that algorithms that
consider only one of the three desiderata can fail to satisfy and even harm the
other two. To overcome this shortcoming, we present the first ranking algorithm
that explicitly enforces all three desiderata. The algorithm optimizes user and
item fairness as a convex optimization problem which can be solved optimally.
From its solution, a ranking policy can be derived via a novel Birkhoff-von
Neumann decomposition algorithm that optimizes diversity. Beyond the
theoretical analysis, we investigate empirically on a new benchmark dataset how
effectively the proposed ranking algorithm can control user fairness, item
fairness and diversity, as well as the trade-offs between them.

    

### [[2102.08643] Temporal Memory Attention for Video Semantic Segmentation](http://arxiv.org/abs/2102.08643)


  Video semantic segmentation requires to utilize the complex temporal
relations between frames of the video sequence. Previous works usually exploit
accurate optical flow to leverage the temporal relations, which suffer much
from heavy computational cost. In this paper, we propose a Temporal Memory
Attention Network (TMANet) to adaptively integrate the long-range temporal
relations over the video sequence based on the self-attention mechanism without
exhaustive optical flow prediction. Specially, we construct a memory using
several past frames to store the temporal information of the current frame. We
then propose a temporal memory attention module to capture the relation between
the current frame and the memory to enhance the representation of the current
frame. Our method achieves new state-of-the-art performances on two challenging
video semantic segmentation datasets, particularly 80.3% mIoU on Cityscapes and
76.5% mIoU on CamVid with ResNet-50.

    

### [[2103.03793] Learning Collision-free and Torque-limited Robot Trajectories based on Alternative Safe Behaviors](http://arxiv.org/abs/2103.03793)


  This paper presents an approach to learn online generation of collision-free
and torque-limited robot trajectories. In order to generate future motions, a
neural network is periodically invoked. Based on the current kinematic state of
the robot and the network prediction, a trajectory for the current time
interval can be calculated. The main idea of our paper is to execute the
predicted motion only if a collision-free and torque-limited way to continue
the trajectory is known. In practice, the motion predicted for the current time
interval is extended by a braking trajectory and simulated using a physics
engine. If the simulated trajectory complies with all safety constraints, the
predicted motion is carried out. Otherwise, the braking trajectory calculated
in the previous time interval serves as an alternative safe behavior. Given a
task-specific reward function, the neural network is trained using
reinforcement learning. The design of the action space used for reinforcement
learning ensures that all predicted trajectories comply with kinematic joint
limits. For our evaluation, simulated industrial robots and humanoid robots are
trained to reach as many randomly placed target points as possible. We show
that our method reliably prevents collisions with static obstacles and
collisions between the robot arms, while generating motions that respect both
torque limits and kinematic joint limits. Experiments with a real robot
demonstrate that safe trajectories can be generated in real-time.

    

### [[2103.10453] Massively parallel hybrid search for the partial Latin square extension problem](http://arxiv.org/abs/2103.10453)


  The partial Latin square extension problem is to fill as many as possible
empty cells of a partially filled Latin square. This problem is a useful model
for a wide range of relevant applications in diverse domains. This paper
presents the first massively parallel hybrid search algorithm for this
computationally challenging problem based on a transformation of the problem to
partial graph coloring. The algorithm features the following original elements.
Based on a very large population (with more than $10^4$ individuals) and modern
graphical processing units, the algorithm performs many local searches in
parallel to ensure an intensified exploitation of the search space. It employs
a dedicated crossover with a specific parent matching strategy to create a
large number of diversified and information-preserving offspring at each
generation. Extensive experiments on 1800 benchmark instances show a high
competitiveness of the algorithm compared with the current best performing
methods. Competitive results are also reported on the related Latin square
completion problem. Analyses are performed to shed lights on the understanding
of the main algorithmic components. The code of the algorithm will be made
publicly available.

    

### [[2104.06239] Reducing Discontinuous to Continuous Parsing with Pointer Network Reordering](http://arxiv.org/abs/2104.06239)


  Discontinuous constituent parsers have always lagged behind continuous
approaches in terms of accuracy and speed, as the presence of constituents with
discontinuous yield introduces extra complexity to the task. However, a
discontinuous tree can be converted into a continuous variant by reordering
tokens. Based on that, we propose to reduce discontinuous parsing to a
continuous problem, which can then be directly solved by any off-the-shelf
continuous parser. To that end, we develop a Pointer Network capable of
accurately generating the continuous token arrangement for a given input
sentence and define a bijective function to recover the original order.
Experiments on the main benchmarks with two continuous parsers prove that our
approach is on par in accuracy with purely discontinuous state-of-the-art
algorithms, but considerably faster.

    

### [[2104.08154] Counter-Interference Adapter for Multilingual Machine Translation](http://arxiv.org/abs/2104.08154)


  Developing a unified multilingual model has long been a pursuit for machine
translation. However, existing approaches suffer from performance degradation
-- a single multilingual model is inferior to separately trained bilingual ones
on rich-resource languages. We conjecture that such a phenomenon is due to
interference caused by joint training with multiple languages. To accommodate
the issue, we propose CIAT, an adapted Transformer model with a small parameter
overhead for multilingual machine translation. We evaluate CIAT on multiple
benchmark datasets, including IWSLT, OPUS-100, and WMT. Experiments show that
CIAT consistently outperforms strong multilingual baselines on 64 of total 66
language directions, 42 of which see above 0.5 BLEU improvement. Our code is
available at \url{this https URL}~.

    

### [[2104.08727] GooAQ: Open Question Answering with Diverse Answer Types](http://arxiv.org/abs/2104.08727)


  While day-to-day questions come with a variety of answer types, the current
question-answering (QA) literature has failed to adequately address the answer
diversity of questions. To this end, we present GooAQ, a large-scale dataset
with a variety of answer types. This dataset contains over 5 million questions
and 3 million answers collected from Google. GooAQ questions are collected
semi-automatically from the Google search engine using its autocomplete
feature. This results in naturalistic questions of practical interest that are
nonetheless short and expressed using simple language. GooAQ answers are mined
from Google's responses to our collected questions, specifically from the
answer boxes in the search results. This yields a rich space of answer types,
containing both textual answers (short and long) as well as more structured
ones such as collections. We benchmarkT5 models on GooAQ and observe that: (a)
in line with recent work, LM's strong performance on GooAQ's short-answer
questions heavily benefit from annotated data; however, (b) their quality in
generating coherent and accurate responses for questions requiring long
responses (such as 'how' and 'why' questions) is less reliant on observing
annotated data and mainly supported by their pre-training. We release GooAQ to
facilitate further research on improving QA with diverse response types.

    

### [[2104.08731] Can NLI Models Verify QA Systems' Predictions?](http://arxiv.org/abs/2104.08731)


  To build robust question answering systems, we need the ability to verify
whether answers to questions are truly correct, not just "good enough" in the
context of imperfect QA datasets. We explore the use of natural language
inference (NLI) as a way to achieve this goal, as NLI inherently requires the
premise (document context) to contain all necessary information to support the
hypothesis (proposed answer to the question). We leverage large pre-trained
models and recent prior datasets to construct powerful question converter and
decontextualization modules, which can reformulate QA instances as
premise-hypothesis pairs with very high reliability. Then, by combining
standard NLI datasets with NLI examples automatically derived from QA training
data, we can train NLI models to judge the correctness of QA models' proposed
answers. We show that our NLI approach can generally improve the confidence
estimation of a QA model across different domains, evaluated in a selective QA
setting. Careful manual analysis over the predictions of our NLI model shows
that it can further identify cases where the QA model produces the right answer
for the wrong reason, or where the answer cannot be verified as addressing all
aspects of the question.

    

### [[2105.02418] A Unified Transferable Model for ML-Enhanced DBMS](http://arxiv.org/abs/2105.02418)


  Recently, the database management system (DBMS) community has witnessed the
power of machine learning (ML) solutions for DBMS tasks. Despite their
promising performance, these existing solutions can hardly be considered
satisfactory. First, these ML-based methods in DBMS are not effective enough
because they are optimized on each specific task, and cannot explore or
understand the intrinsic connections between tasks. Second, the training
process has serious limitations that hinder their practicality, because they
need to retrain the entire model from scratch for a new DB. Moreover, for each
retraining, they require an excessive amount of training data, which is very
expensive to acquire and unavailable for a new DB. We propose to explore the
transferabilities of the ML methods both across tasks and across DBs to tackle
these fundamental drawbacks.
In this paper, we propose a unified model MTMLF that uses a multi-task
training procedure to capture the transferable knowledge across tasks and a
pre-train fine-tune procedure to distill the transferable meta knowledge across
DBs. We believe this paradigm is more suitable for cloud DB service, and has
the potential to revolutionize the way how ML is used in DBMS. Furthermore, to
demonstrate the predicting power and viability of MTMLF, we provide a concrete
and very promising case study on query optimization tasks. Last but not least,
we discuss several concrete research opportunities along this line of work.

    

### [[2109.02444] Top-N Recommendation with Counterfactual User Preference Simulation](http://arxiv.org/abs/2109.02444)


  Top-N recommendation, which aims to learn user ranking-based preference, has
long been a fundamental problem in a wide range of applications. Traditional
models usually motivate themselves by designing complex or tailored
architectures based on different assumptions. However, the training data of
recommender system can be extremely sparse and imbalanced, which poses great
challenges for boosting the recommendation performance. To alleviate this
problem, in this paper, we propose to reformulate the recommendation task
within the causal inference framework, which enables us to counterfactually
simulate user ranking-based preferences to handle the data scarce problem. The
core of our model lies in the counterfactual question: "what would be the
user's decision if the recommended items had been different?". To answer this
question, we firstly formulate the recommendation process with a series of
structural equation models (SEMs), whose parameters are optimized based on the
observed data. Then, we actively indicate many recommendation lists (called
intervention in the causal inference terminology) which are not recorded in the
dataset, and simulate user feedback according to the learned SEMs for
generating new training samples. Instead of randomly intervening on the
recommendation list, we design a learning-based method to discover more
informative training samples. Considering that the learned SEMs can be not
perfect, we, at last, theoretically analyze the relation between the number of
generated samples and the model prediction error, based on which a heuristic
method is designed to control the negative effect brought by the prediction
error. Extensive experiments are conducted based on both synthetic and
real-world datasets to demonstrate the effectiveness of our framework.

    

### [[2109.05343] Sharp Waiting-Time Bounds for Multiserver Jobs](http://arxiv.org/abs/2109.05343)


  Multiserver jobs, which are jobs that occupy multiple servers simultaneously
during service, are prevalent in today's computing clusters. But little is
known about the delay performance of systems with multiserver jobs. We consider
queueing models for multiserver jobs in a scaling regime where the total number
of servers in the system becomes large and meanwhile both the system load and
the number of servers that a job needs scale with the total number of servers.
Prior work has derived upper bounds on the queueing probability in this scaling
regime. However, without proper lower bounds, the existing results cannot be
used to differentiate between policies. In this paper, we study the delay
performance by establishing sharp bounds on the mean waiting time of
multiserver jobs, where the waiting time of a job is the time spent in queueing
rather than in service. We first consider the commonly used
First-Come-First-Serve (FCFS) policy and characterize the exact order of its
mean waiting time. We then prove a lower bound on the mean waiting time of all
policies, and demonstrate that there is an order gap between this lower bound
and the mean waiting time under FCFS. We finally complement the lower bound
with an achievability result: we show that under a priority policy that we call
P-Priority, the mean waiting time achieves the order of the lower bound. This
achievability result implies the tightness of the lower bound, the asymptotic
optimality of P-Priority, and the strict suboptimality of FCFS.

    

### [[2109.05049] Solver-based Gradual Type Migration](http://arxiv.org/abs/2109.05049)


  Gradually typed languages allow programmers to mix statically and dynamically
typed code, enabling them to incrementally reap the benefits of static typing
as they add type annotations to their code. However, this type migration
process is typically a manual effort with limited tool support. This paper
examines the problem of \emph{automated type migration}: given a dynamic
program, infer additional or improved type annotations.
Existing type migration algorithms prioritize different goals, such as
maximizing type precision, maintaining compatibility with unmigrated code, and
preserving the semantics of the original program. We argue that the type
migration problem involves fundamental compromises: optimizing for a single
goal often comes at the expense of others. Ideally, a type migration tool would
flexibly accommodate a range of user priorities.
We present TypeWhich, a new approach to automated type migration for the
gradually-typed lambda calculus with some extensions. Unlike prior work, which
relies on custom solvers, TypeWhich produces constraints for an off-the-shelf
MaxSMT solver. This allows us to easily express objectives, such as minimizing
the number of necessary syntactic coercions, and constraining the type of the
migration to be compatible with unmigrated code.
We present the first comprehensive evaluation of GTLC type migration
algorithms, and compare TypeWhich to four other tools from the literature. Our
evaluation uses prior benchmarks, and a new set of ``challenge problems.''
Moreover, we design a new evaluation methodology that highlights the subtleties
of gradual type migration. In addition, we apply TypeWhich to a suite of
benchmarks for Grift, a programming language based on the GTLC. TypeWhich is
able to reconstruct all human-written annotations on all but one program.

    

### [[2109.05093] PICARD: Parsing Incrementally for Constrained Auto-Regressive Decoding from Language Models](http://arxiv.org/abs/2109.05093)


  Large pre-trained language models for textual data have an unconstrained
output space; at each decoding step, they can produce any of 10,000s of
sub-word tokens. When fine-tuned to target constrained formal languages like
SQL, these models often generate invalid code, rendering it unusable. We
propose PICARD (code and trained models available at
this https URL), a method for constraining auto-regressive
decoders of language models through incremental parsing. PICARD helps to find
valid output sequences by rejecting inadmissible tokens at each decoding step.
On the challenging Spider and CoSQL text-to-SQL translation tasks, we show that
PICARD transforms fine-tuned T5 models with passable performance into
state-of-the-art solutions.

    

### [[2109.05631] Verifying Concurrent Multicopy Search Structures](http://arxiv.org/abs/2109.05631)


  Multicopy search structures such as log-structured merge (LSM) trees are
optimized for high insert/update/delete (collectively known as upsert)
performance. In such data structures, an upsert on key $k$, which adds $(k,v)$
where $v$ can be a value or a tombstone, is added to the root node even if $k$
is already present in other nodes. Thus there may be multiple copies of $k$ in
the search structure. A search on $k$ aims to return the value associated with
the most recent upsert. We present a general framework for verifying
linearizability of concurrent multicopy search structures that abstracts from
the underlying representation of the data structure in memory, enabling
proof-reuse across diverse implementations. Based on our framework, we propose
template algorithms for a) LSM structures forming arbitrary directed acyclic
graphs and b) differential file structures, and formally verify these templates
in the concurrent separation logic Iris. We also instantiate the LSM template
to obtain the first verified concurrent in-memory LSM tree implementation.

    

### [[2106.14586] A Dictionary-Passing Translation of Featherweight Go](http://arxiv.org/abs/2106.14586)


  The Go programming language is an increasingly popular language but some of
its features lack a formal investigation. This article explains Go's resolution
mechanism for overloaded methods and its support for structural subtyping by
means of translation from Featherweight Go to a simple target language. The
translation employs a form of dictionary passing known from type classes in
Haskell and preserves the dynamic behavior of Featherweight Go programs.

    