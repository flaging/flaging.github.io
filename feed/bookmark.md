
## 2021-9-30

### [[2109.14063] Stochastic geometric modelling and simulation of cellular systems for coverage probability characterization](http://arxiv.org/abs/2109.14063)


  Stochastic geometry (SG) has been successfully used as a modelling tool for
cellular networks to characterize the coverage probability in both the downlink
(DL) and uplink (UL) systems, under the assumption that the base stations (BS)
are deployed as a Poisson point process. In the present article, we extend this
use and provide further results for interference limited and Rayleigh fading
networks, culminating in a multifaceted contribution. First, we compactly model
the two systems at once, allowing parallels to be drawn and contrast to be
created. Also, for DL we manage to obtain two closed form expressions for two
special cases. Moreover, for UL, notorious for being difficult, we develop a
clever approximation that overcomes the difficulty, yielding excellent results.
Additionally, we present two efficient Monte Carlo simulation algorithms,
designed primarily to validate the models, but can be of great use for SG
modelling of communications systems in general. Finally, we prove two theorems
at odds with popular belief in cellular communications research. Specifically,
we prove that under the SG model, the coverage probability in both DL and UL is
independent of BS density. Based on this revelation, a plethora of results in
the literature have to be re-examined to rid them of a parameter that has been
proven superfluous.

    

### [[2109.14276] Sequential Deep Learning Architectures for Anomaly Detection in Virtual Network Function Chains](http://arxiv.org/abs/2109.14276)


  Software-defined networking (SDN) and network function virtualization (NFV)
have enabled the efficient provision of network service. However, they also
raised new tasks to monitor and ensure the status of virtualized service, and
anomaly detection is one of such tasks. There have been many data-driven
approaches to implement anomaly detection system (ADS) for virtual network
functions in service function chains (SFCs). In this paper, we aim to develop
more advanced deep learning models for ADS. Previous approaches used learning
algorithms such as random forest (RF), gradient boosting machine (GBM), or deep
neural networks (DNNs). However, these models have not utilized sequential
dependencies in the data. Furthermore, they are limited as they can only apply
to the SFC setting from which they were trained. Therefore, we propose several
sequential deep learning models to learn time-series patterns and sequential
patterns of the virtual network functions (VNFs) in the chain with variable
lengths. As a result, the suggested models improve detection performance and
apply to SFCs with varying numbers of VNFs.

    

### [[2109.14527] Human-centric Data Dissemination in the IoP: Large-scale Modeling and Evaluation](http://arxiv.org/abs/2109.14527)


  Data management using Device-to-Device (D2D) communications and opportunistic
networks (ONs) is one of the main focuses of human-centric pervasive Internet
services. In the recently proposed "Internet of People" paradigm, accessing
relevant data dynamically generated in the environment nearby is one of the key
services. Moreover, personal mobile devices become proxies of their human users
while exchanging data in the cyber world and, thus, largely use ONs and D2D
communications for exchanging data directly. Recently, researchers have
successfully demonstrated the viability of embedding human cognitive schemes in
data dissemination algorithms for ONs. In this paper, we consider one such
scheme based on the recognition heuristic, a human decision-making scheme used
to efficiently assess the relevance of data. While initial evidence about its
effectiveness is available, the evaluation of its behaviour in large-scale
settings is still unsatisfactory. To overcome these limitations, we have
developed a novel hybrid modelling methodology, which combines an analytical
model of data dissemination within small-scale communities of mobile users,
with detailed simulations of interactions between different communities. This
methodology allows us to evaluate the algorithm in large-scale city- and
country-wide scenarios. Results confirm the effectiveness of cognitive data
dissemination schemes, even when content popularity is very heterogenous.

    

### [[2109.14535] Analyse or Transmit: Utilising Correlation at the Edge with Deep Reinforcement Learning](http://arxiv.org/abs/2109.14535)


  Millions of sensors, cameras, meters, and other edge devices are deployed in
networks to collect and analyse data. In many cases, such devices are powered
only by Energy Harvesting(EH) and have limited energy available to analyse
acquired data. When edge infrastructure is available, a device has a choice: to
perform analysis locally or offload the task to other resource-rich devices
such as cloudlet servers. However, such a choice carries a price in terms of
consumed energy and accuracy. On the one hand, transmitting raw data can result
in a higher energy cost in comparison to the required energy to process data
locally. On the other hand, performing data analytics on servers can improve
the task's accuracy. Additionally, due to the correlation between information
sent by multiple devices, accuracy might not be affected if some edge devices
decide to neither process nor send data and preserve energy instead. For such a
scenario, we propose a Deep Reinforcement Learning (DRL) based solution capable
of learning and adapting the policy to the time-varying energy arrival due to
EH patterns. We leverage two datasets, one to model energy an EH device can
collect and the other to model the correlation between cameras. Furthermore, we
compare the proposed solution performance to three baseline policies. Our
results show that we can increase accuracy by 15% in comparison to conventional
approaches while preventing outages.

    

### [[2109.14546] An Energy Efficient Health Monitoring Approach with Wireless Body Area Networks](http://arxiv.org/abs/2109.14546)


  Wireless Body Area Networks (WBANs) comprise a network of sensors
subcutaneously implanted or placed near the body surface and facilitate
continuous monitoring of health parameters of a patient. Research endeavours
involving WBAN are directed towards effective transmission of detected
parameters to a Local Processing Unit (LPU, usually a mobile device) and
analysis of the parameters at the LPU or a back-end cloud. An important concern
in WBAN is the lightweight nature of WBAN nodes and the need to conserve their
energy. This is especially true for subcutaneously implanted nodes that cannot
be recharged or regularly replaced. Work in energy conservation is mostly aimed
at optimising the routing of signals to minimise energy expended. In this
paper, a simple yet innovative approach to energy conservation and detection of
alarming health status is proposed. Energy conservation is ensured through a
two-tier approach wherein the first tier eliminates `uninteresting' health
parameter readings at the site of a sensing node and prevents these from being
transmitted across the WBAN to the LPU. A reading is categorised as
uninteresting if it deviates very slightly from its immediately preceding
reading and does not provide new insight on the patient's well being. In
addition to this, readings that are faulty and emanate from possible sensor
malfunctions are also eliminated. These eliminations are done at the site of
the sensor using algorithms that are light enough to effectively function in
the extremely resource-constrained environments of the sensor nodes. We notice,
through experiments, that this eliminates and thus reduces around 90% of the
readings that need to be transmitted to the LPU leading to significant energy
savings. Furthermore, the proper functioning of these algorithms in such
constrained environments is confirmed and validated over a hardware simulation
set up. The second tier of assessment includes a proposed anomaly detection
model at the LPU that is capable of identifying anomalies from streaming health
parameter readings and indicates an adverse medical condition. In addition to
being able to handle streaming data, the model works within the
resource-constrained environments of an LPU and eliminates the need of
transmitting the data to a back-end cloud, ensuring further energy savings. The
anomaly detection capability of the model is validated using data available
from the critical care units of hospitals and is shown to be superior to other
anomaly detection techniques.

    

### [[2109.14581] Towards 6G Non-Terrestrial Networks](http://arxiv.org/abs/2109.14581)


  Sixth-Generation (6G) technologies will revolutionize the wireless ecosystem
by enabling the delivery of futuristic services through terrestrial and
non-terrestrial transmissions. In this context, the Non-Terrestrial Network
(NTN) is growing in importance owing to its capability to deliver services
anywhere and anytime and also provide coverage in areas that are unreachable by
any conventional Terrestrial Network (TN). The exploitation of the same radio
technology could greatly facilitate the integration of NTNs and TNs into a
unified wireless system. Since New Radio (NR) is the de facto standard to
deliver manifold heterogeneous services in terrestrial wireless systems, 3GPP
is investigating new solutions to extend NR to NTNs. In this paper, the
constraints that NTN features place on NR procedures are investigated by going
thoroughly into 3GPP specifications; strengths and weaknesses of the NR
technology in enabling typical 6G services on NR-enabled NTNs are identified;
finally, open issues and insights are provided as guidelines to steer future
research towards 6G NTNs.

    

### [[2109.14593] Federated Learning over Next-Generation Ethernet Passive Optical Networks](http://arxiv.org/abs/2109.14593)


  Federated Learning (FL) is a distributed machine learning (ML) type of
processing that preserves the privacy of user data, sharing only the parameters
of ML models with a common server. The processing of FL requires specific
latency and bandwidth demands that need to be fulfilled by the operation of the
communication network. This paper introduces a Dynamic Wavelength and Bandwidth
Allocation algorithm for Quality of Service (QoS) provisioning for FL traffic
over 50 Gb/s Ethernet Passive Optical Networks. The proposed algorithm
prioritizes FL traffic and reduces the delay of FL and delay-critical
applications supported on the same infrastructure.

    

### [[2010.06880] Transportation Internet: Concepts, Models, and Architectures](http://arxiv.org/abs/2010.06880)


  Disruptive changes in vehicles and transportation have been triggered by
automated, connected, electrified and shared mobility. Autonomous vehicles,
like Internet data packets, are transported from one address to another through
the road network. The Internet has become a general network transmission
paradigm, and the Energy Internet is a successful application of this paradigm
to the field of energy. By introducing the Internet paradigm to the field of
transportation, this paper is the first to propose the Transportation Internet.
Based on the concept of the Transportation Internet, fundamental models, such
as the switching, routing, and hierarchical models, are established to form
basic theories; new architectures, such as transportation routers and software
defined transportation, are proposed to make transportation interconnected and
open; system verifications, such as prototype and simulation, are also carried
out to prove feasibility and advancement. The Transportation Internet, which is
of far-reaching significance in science and industry, has brought systematic
breakthroughs in theory, architecture, and technology, explored innovative
research directions, and provided an Internet-like solution for the new
generation of transportation.

    

### [[2011.09747] Energy Aware Deep Reinforcement Learning Scheduling for Sensors Correlated in Time and Space](http://arxiv.org/abs/2011.09747)


  Millions of battery-powered sensors deployed for monitoring purposes in a
multitude of scenarios, e.g., agriculture, smart cities, industry, etc.,
require energy-efficient solutions to prolong their lifetime. When these
sensors observe a phenomenon distributed in space and evolving in time, it is
expected that collected observations will be correlated in time and space. In
this paper, we propose a Deep Reinforcement Learning (DRL) based scheduling
mechanism capable of taking advantage of correlated information. We design our
solution using the Deep Deterministic Policy Gradient (DDPG) algorithm. The
proposed mechanism is capable of determining the frequency with which sensors
should transmit their updates, to ensure accurate collection of observations,
while simultaneously considering the energy available. To evaluate our
scheduling mechanism, we use multiple datasets containing environmental
observations obtained in multiple real deployments. The real observations
enable us to model the environment with which the mechanism interacts as
realistically as possible. We show that our solution can significantly extend
the sensors' lifetime. We compare our mechanism to an idealized, all-knowing
scheduler to demonstrate that its performance is near-optimal. Additionally, we
highlight the unique feature of our design, energy-awareness, by displaying the
impact of sensors' energy levels on the frequency of updates.

    

### [[2109.13921] Click-through Rate Prediction with Auto-Quantized Contrastive Learning](http://arxiv.org/abs/2109.13921)


  Click-through rate (CTR) prediction becomes indispensable in ubiquitous web
recommendation applications. Nevertheless, the current methods are struggling
under the cold-start scenarios where the user interactions are extremely
sparse. We consider this problem as an automatic identification about whether
the user behaviors are rich enough to capture the interests for prediction, and
propose an Auto-Quantized Contrastive Learning (AQCL) loss to regularize the
model. Different from previous methods, AQCL explores both the
instance-instance and the instance-cluster similarity to robustify the latent
representation, and automatically reduces the information loss to the active
users due to the quantization. The proposed framework is agnostic to different
model architectures and can be trained in an end-to-end fashion. Extensive
results show that it consistently improves the current state-of-the-art CTR
models.

    

### [[2109.13925] Fine-tuning Vision Transformers for the Prediction of State Variables in Ising Models](http://arxiv.org/abs/2109.13925)


  Transformers are state-of-the-art deep learning models that are composed of
stacked attention and point-wise, fully connected layers designed for handling
sequential data. Transformers are not only ubiquitous throughout Natural
Language Processing (NLP), but, recently, they have inspired a new wave of
Computer Vision (CV) applications research. In this work, a Vision Transformer
(ViT) is applied to predict the state variables of 2-dimensional Ising model
simulations. Our experiments show that ViT outperform state-of-the-art
Convolutional Neural Networks (CNN) when using a small number of microstate
images from the Ising model corresponding to various boundary conditions and
temperatures. This work opens the possibility of applying ViT to other
simulations, and raises interesting research directions on how attention maps
can learn about the underlying physics governing different phenomena.

    

### [[2109.13963] Smart at what cost? Characterising Mobile Deep Neural Networks in the wild](http://arxiv.org/abs/2109.13963)


  With smartphones' omnipresence in people's pockets, Machine Learning (ML) on
mobile is gaining traction as devices become more powerful. With applications
ranging from visual filters to voice assistants, intelligence on mobile comes
in many forms and facets. However, Deep Neural Network (DNN) inference remains
a compute intensive workload, with devices struggling to support intelligence
at the cost of responsiveness.On the one hand, there is significant research on
reducing model runtime requirements and supporting deployment on embedded
devices. On the other hand, the strive to maximise the accuracy of a task is
supported by deeper and wider neural networks, making mobile deployment of
state-of-the-art DNNs a moving target.
In this paper, we perform the first holistic study of DNN usage in the wild
in an attempt to track deployed models and match how these run on widely
deployed devices. To this end, we analyse over 16k of the most popular apps in
the Google Play Store to characterise their DNN usage and performance across
devices of different capabilities, both across tiers and generations.
Simultaneously, we measure the models' energy footprint, as a core cost
dimension of any mobile deployment. To streamline the process, we have
developed gaugeNN, a tool that automates the deployment, measurement and
analysis of DNNs on devices, with support for different frameworks and
platforms. Results from our experience study paint the landscape of deep
learning deployments on smartphones and indicate their popularity across app
developers. Furthermore, our study shows the gap between bespoke techniques and
real-world deployments and the need for optimised deployment of deep learning
models in a highly dynamic and heterogeneous ecosystem.

    

### [[2109.13964] An Accelerated Stochastic Gradient for Canonical Polyadic Decomposition](http://arxiv.org/abs/2109.13964)


  We consider the problem of structured canonical polyadic decomposition. If
the size of the problem is very big, then stochastic gradient approaches are
viable alternatives to classical methods, such as Alternating Optimization and
All-At-Once optimization. We extend a recent stochastic gradient approach by
employing an acceleration step (Nesterov momentum) in each iteration. We
compare our approach with state-of-the-art alternatives, using both synthetic
and real-world data, and find it to be very competitive.

    

### [[2109.13977] Risk averse non-stationary multi-armed bandits](http://arxiv.org/abs/2109.13977)


  This paper tackles the risk averse multi-armed bandits problem when incurred
losses are non-stationary. The conditional value-at-risk (CVaR) is used as the
objective function. Two estimation methods are proposed for this objective
function in the presence of non-stationary losses, one relying on a weighted
empirical distribution of losses and another on the dual representation of the
CVaR. Such estimates can then be embedded into classic arm selection methods
such as epsilon-greedy policies. Simulation experiments assess the performance
of the arm selection algorithms based on the two novel estimation approaches,
and such policies are shown to outperform naive benchmarks not taking
non-stationarity into account.

    

### [[2109.13983] Guidelines for the Computational Testing of Machine Learning approaches to Vehicle Routing Problems](http://arxiv.org/abs/2109.13983)


  Despite the extensive research efforts and the remarkable results obtained on
Vehicle Routing Problems (VRP) by using algorithms proposed by the Machine
Learning community that are partially or entirely based on data-driven
analysis, most of these approaches are still seldom employed by the Operations
Research (OR) community. Among the possible causes, we believe, the different
approach to the computational evaluation of the proposed methods may play a
major role. With the current work, we want to highlight a number of challenges
(and possible ways to handle them) arising during the computational studies of
heuristic approaches to VRPs that, if appropriately addressed, may produce a
computational study having the characteristics of those presented in OR papers,
thus hopefully promoting the collaboration between the two communities.

    

### [[2109.13986] Symbolic Brittleness in Sequence Models: on Systematic Generalization in Symbolic Mathematics](http://arxiv.org/abs/2109.13986)


  Neural sequence models trained with maximum likelihood estimation have led to
breakthroughs in many tasks, where success is defined by the gap between
training and test performance. However, their ability to achieve stronger forms
of generalization remains unclear. We consider the problem of symbolic
mathematical integration, as it requires generalizing systematically beyond the
test set. We develop a methodology for evaluating generalization that takes
advantage of the problem domain's structure and access to a verifier. Despite
promising in-distribution performance of sequence-to-sequence models in this
domain, we demonstrate challenges in achieving robustness, compositionality,
and out-of-distribution generalization, through both carefully constructed
manual test suites and a genetic algorithm that automatically finds large
collections of failures in a controllable manner. Our investigation highlights
the difficulty of generalizing well with the predominant modeling and learning
approach, and the importance of evaluating beyond the test set, across
different aspects of generalization.

    

### [[2109.13995] IGLU: Efficient GCN Training via Lazy Updates](http://arxiv.org/abs/2109.13995)


  Graph Convolution Networks (GCN) are used in numerous settings involving a
large underlying graph as well as several layers. Standard SGD-based training
scales poorly here since each descent step ends up updating node embeddings for
a large portion of the graph. Recent methods attempt to remedy this by
sub-sampling the graph which does reduce the compute load, but at the cost of
biased gradients which may offer suboptimal performance. In this work we
introduce a new method IGLU that caches forward-pass embeddings for all nodes
at various GCN layers. This enables IGLU to perform lazy updates that do not
require updating a large number of node embeddings during descent which offers
much faster convergence but does not significantly bias the gradients. Under
standard assumptions such as objective smoothness, IGLU provably converges to a
first-order saddle point. We validate IGLU extensively on a variety of
benchmarks, where it offers up to 1.2% better accuracy despite requiring up to
88% less wall-clock time.

    

### [[2109.14002] slimTrain -- A Stochastic Approximation Method for Training Separable Deep Neural Networks](http://arxiv.org/abs/2109.14002)


  Deep neural networks (DNNs) have shown their success as high-dimensional
function approximators in many applications; however, training DNNs can be
challenging in general. DNN training is commonly phrased as a stochastic
optimization problem whose challenges include non-convexity, non-smoothness,
insufficient regularization, and complicated data distributions. Hence, the
performance of DNNs on a given task depends crucially on tuning
hyperparameters, especially learning rates and regularization parameters. In
the absence of theoretical guidelines or prior experience on similar tasks,
this requires solving many training problems, which can be time-consuming and
demanding on computational resources. This can limit the applicability of DNNs
to problems with non-standard, complex, and scarce datasets, e.g., those
arising in many scientific applications. To remedy the challenges of DNN
training, we propose slimTrain, a stochastic optimization method for training
DNNs with reduced sensitivity to the choice hyperparameters and fast initial
convergence. The central idea of slimTrain is to exploit the separability
inherent in many DNN architectures; that is, we separate the DNN into a
nonlinear feature extractor followed by a linear model. This separability
allows us to leverage recent advances made for solving large-scale, linear,
ill-posed inverse problems. Crucially, for the linear weights, slimTrain does
not require a learning rate and automatically adapts the regularization
parameter. Since our method operates on mini-batches, its computational
overhead per iteration is modest. In our numerical experiments, slimTrain
outperforms existing DNN training methods with the recommended hyperparameter
settings and reduces the sensitivity of DNN training to the remaining
hyperparameters.

    

### [[2109.14025] Deep Unrolled Recovery in Sparse Biological Imaging](http://arxiv.org/abs/2109.14025)


  Deep algorithm unrolling has emerged as a powerful model-based approach to
develop deep architectures that combine the interpretability of iterative
algorithms with the performance gains of supervised deep learning, especially
in cases of sparse optimization. This framework is well-suited to applications
in biological imaging, where physics-based models exist to describe the
measurement process and the information to be recovered is often highly
structured. Here, we review the method of deep unrolling, and show how it
improves source localization in several biological imaging settings.

    

### [[2109.14026] Learning Perceptual Locomotion on Uneven Terrains using Sparse Visual Observations](http://arxiv.org/abs/2109.14026)


  Legged robots have achieved remarkable performance in blind walking using
either model-based control or data-driven deep reinforcement learning. To
proactively navigate and traverse various terrains, active use of visual
perception becomes indispensable, and this work aims to exploit the use of
sparse visual observations to achieve perceptual locomotion over a range of
commonly seen bumps, ramps, and stairs in human-centred environments. We first
formulate the selection of minimal visual input that can represent the uneven
surfaces of interest, and propose a learning framework that integrates such
exteroceptive and proprioceptive data. We specifically select state
observations and design a training curriculum to learn feedback control
policies more effectively over a range of different terrains. Using an
extensive benchmark, we validate the learned policy in tasks that require
omnidirectional walking over flat ground and forward locomotion over terrains
with obstacles, showing a high success rate of traversal. Particularly, the
robot performs autonomous perceptual locomotion with minimal visual perception
using depth measurements, which are easily available from a Lidar or RGB-D
sensor, and successfully demonstrates robust ascent and descent over high
stairs of 20 cm step height, i.e., 50% of its leg length.

    

### [[2109.14035] Formalizing the Generalization-Forgetting Trade-off in Continual Learning](http://arxiv.org/abs/2109.14035)


  We formulate the continual learning (CL) problem via dynamic programming and
model the trade-off between catastrophic forgetting and generalization as a
two-player sequential game. In this approach, player 1 maximizes the cost due
to lack of generalization whereas player 2 minimizes the cost due to
catastrophic forgetting. We show theoretically that a balance point between the
two players exists for each task and that this point is stable (once the
balance is achieved, the two players stay at the balance point). Next, we
introduce balanced continual learning (BCL), which is designed to attain
balance between generalization and forgetting and empirically demonstrate that
BCL is comparable to or better than the state of the art.

    

### [[2109.14041] Local Repair of Neural Networks Using Optimization](http://arxiv.org/abs/2109.14041)


  In this paper, we propose a framework to repair a pre-trained feed-forward
neural network (NN) to satisfy a set of properties. We formulate the properties
as a set of predicates that impose constraints on the output of NN over the
target input domain. We define the NN repair problem as a Mixed Integer
Quadratic Program (MIQP) to adjust the weights of a single layer subject to the
given predicates while minimizing the original loss function over the original
training domain. We demonstrate the application of our framework in bounding an
affine transformation, correcting an erroneous NN in classification, and
bounding the inputs of a NN controller.

    

### [[2109.14046] Federated Learning Algorithms for Generalized Mixed-effects Model (GLMM) on Horizontally Partitioned Data from Distributed Sources](http://arxiv.org/abs/2109.14046)


  Objectives: This paper develops two algorithms to achieve federated
generalized linear mixed effect models (GLMM), and compares the developed
model's outcomes with each other, as well as that from the standard R package
(`lme4').
Methods: The log-likelihood function of GLMM is approximated by two numerical
methods (Laplace approximation and Gaussian Hermite approximation), which
supports federated decomposition of GLMM to bring computation to data.
Results: Our developed method can handle GLMM to accommodate hierarchical
data with multiple non-independent levels of observations in a federated
setting. The experiment results demonstrate comparable (Laplace) and superior
(Gaussian-Hermite) performances with simulated and real-world data.
Conclusion: We developed and compared federated GLMMs with different
approximations, which can support researchers in analyzing biomedical data to
accommodate mixed effects and address non-independence due to hierarchical
structures (i.e., institutes, region, country, etc.).

    

### [[2109.14059] Generating Summaries for Scientific Paper Review](http://arxiv.org/abs/2109.14059)


  The review process is essential to ensure the quality of publications.
Recently, the increase of submissions for top venues in machine learning and
NLP has caused a problem of excessive burden on reviewers and has often caused
concerns regarding how this may not only overload reviewers, but also may
affect the quality of the reviews. An automatic system for assisting with the
reviewing process could be a solution for ameliorating the problem. In this
paper, we explore automatic review summary generation for scientific papers. We
posit that neural language models have the potential to be valuable candidates
for this task. In order to test this hypothesis, we release a new dataset of
scientific papers and their reviews, collected from papers published in the
NeurIPS conference from 2013 to 2020. We evaluate state of the art neural
summarization models, present initial results on the feasibility of automatic
review summary generation, and propose directions for the future.

    

### [[2109.14061] The impact of non-target events in synthetic soundscapes for sound event detection](http://arxiv.org/abs/2109.14061)


  Detection and Classification Acoustic Scene and Events Challenge 2021 Task 4
uses a heterogeneous dataset that includes both recorded and synthetic
soundscapes. Until recently only target sound events were considered when
synthesizing the soundscapes. However, recorded soundscapes often contain a
substantial amount of non-target events that may affect the performance. In
this paper, we focus on the impact of these non-target events in the synthetic
soundscapes. Firstly, we investigate to what extent using non-target events
alternatively during the training or validation phase (or none of them) helps
the system to correctly detect target events. Secondly, we analyze to what
extend adjusting the signal-to-noise ratio between target and non-target events
at training improves the sound event detection performance. The results show
that using both target and non-target events for only one of the phases
(validation or training) helps the system to properly detect sound events,
outperforming the baseline (which uses non-target events in both phases). The
paper also reports the results of a preliminary study on evaluating the system
on clips that contain only non-target events. This opens questions for future
work on non-target subset and acoustic similarity between target and non-target
events which might confuse the system.

    

### [[2109.14076] RAFT: A Real-World Few-Shot Text Classification Benchmark](http://arxiv.org/abs/2109.14076)


  Large pre-trained language models have shown promise for few-shot learning,
completing text-based tasks given only a few task-specific examples. Will
models soon solve classification tasks that have so far been reserved for human
research assistants? Existing benchmarks are not designed to measure progress
in applied settings, and so don't directly answer this question. The RAFT
benchmark (Real-world Annotated Few-shot Tasks) focuses on naturally occurring
tasks and uses an evaluation setup that mirrors deployment. Baseline
evaluations on RAFT reveal areas current techniques struggle with: reasoning
over long texts and tasks with many classes. Human baselines show that some
classification tasks are difficult for non-expert humans, reflecting that
real-world value sometimes depends on domain expertise. Yet even non-expert
human baseline F1 scores exceed GPT-3 by an average of 0.11. The RAFT datasets
and leaderboard will track which model improvements translate into real-world
benefits at this https URL .

    

### [[2109.14082] Sample-Efficient Safety Assurances using Conformal Prediction](http://arxiv.org/abs/2109.14082)


  When deploying machine learning models in high-stakes robotics applications,
the ability to detect unsafe situations is crucial. Early warning systems can
provide alerts when an unsafe situation is imminent (in the absence of
corrective action). To reliably improve safety, these warning systems should
have a provable false negative rate; i.e. of the situations that are unsafe,
fewer than $\epsilon$ will occur without an alert. In this work, we present a
framework that combines a statistical inference technique known as conformal
prediction with a simulator of robot/environment dynamics, in order to tune
warning systems to provably achieve an $\epsilon$ false negative rate using as
few as $1/\epsilon$ data points. We apply our framework to a driver warning
system and a robotic grasping application, and empirically demonstrate
guaranteed false negative rate and low false detection (positive) rate using
very little data.

    

### [[2109.14090] Reversible Gromov-Monge Sampler for Simulation-Based Inference](http://arxiv.org/abs/2109.14090)


  This paper introduces a new simulation-based inference procedure to model and
sample from multi-dimensional probability distributions given access to i.i.d.
samples, circumventing usual approaches of explicitly modeling the density
function or designing Markov chain Monte Carlo. Motivated by the seminal work
of Mémoli (2011) and Sturm (2012) on distance and isomorphism between metric
measure spaces, we propose a new notion called the Reversible Gromov-Monge
(RGM) distance and study how RGM can be used to design new transform samplers
in order to perform simulation-based inference. Our RGM sampler can also
estimate optimal alignments between two heterogenous metric measure spaces
$(\mathcal{X}, \mu, c_{\mathcal{X}})$ and $(\mathcal{Y}, \nu, c_{\mathcal{Y}})$
from empirical data sets, with estimated maps that approximately push forward
one measure $\mu$ to the other $\nu$, and vice versa. Analytic properties of
RGM distance are derived; statistical rate of convergence, representation, and
optimization questions regarding the induced sampler are studied. Synthetic and
real-world examples showcasing the effectiveness of the RGM sampler are also
demonstrated.

    

### [[2109.14097] How Much Data Analytics is Enough? The ROI of Machine Learning Classification and its Application to Requirements Dependency Classification](http://arxiv.org/abs/2109.14097)


  Machine Learning (ML) can substantially improve the efficiency and
effectiveness of organizations and is widely used for different purposes within
Software Engineering. However, the selection and implementation of ML
techniques rely almost exclusively on accuracy criteria. Thus, for
organizations wishing to realize the benefits of ML investments, this narrow
approach ignores crucial considerations around the anticipated costs of the ML
activities across the ML lifecycle, while failing to account for the benefits
that are likely to accrue from the proposed activity. We present findings for
an approach that addresses this gap by enhancing the accuracy criterion with
return on investment (ROI) considerations. Specifically, we analyze the
performance of the two state-of-the-art ML techniques: Random Forest and
Bidirectional Encoder Representations from Transformers (BERT), based on
accuracy and ROI for two publicly available data sets. Specifically, we compare
decision-making on requirements dependency extraction (i) exclusively based on
accuracy and (ii) extended to include ROI analysis. As a result, we propose
recommendations for selecting ML classification techniques based on the degree
of training data used. Our findings indicate that considering ROI as additional
criteria can drastically influence ML selection when compared to decisions
based on accuracy as the sole criterion

    

### [[2109.14099] An Explainable-AI approach for Diagnosis of COVID-19 using MALDI-ToF Mass Spectrometry](http://arxiv.org/abs/2109.14099)


  The novel severe acute respiratory syndrome coronavirus type-2 (SARS-CoV-2)
caused a global pandemic that has taken more than 4.5 million lives and
severely affected the global economy. To curb the spread of the virus, an
accurate, cost-effective, and quick testing for large populations is
exceedingly important in order to identify, isolate, and treat infected people.
Current testing methods commonly use PCR (Polymerase Chain Reaction) based
equipment that have limitations on throughput, cost-effectiveness, and
simplicity of procedure which creates a compelling need for developing
additional coronavirus disease-2019 (COVID-19) testing mechanisms, that are
highly sensitive, rapid, trustworthy, and convenient to use by the public. We
propose a COVID-19 testing method using artificial intelligence (AI) techniques
on MALDI-ToF (matrix-assisted laser desorption/ionization time-of-flight) data
extracted from 152 human gargle samples (60 COVID-19 positive tests and 92
COVID-19 negative tests). Our AI-based approach leverages explainable-AI (X-AI)
methods to explain the decision rules behind the predictive algorithm both on a
local (per-sample) and global (all-samples) basis to make the AI model more
trustworthy. Finally, we evaluated our proposed method using a 70%-30%
train-test-split strategy and achieved a training accuracy of 86.79% and a
testing accuracy of 91.30%.

    

### [[2109.14117] Neural Network Ensembles: Theory, Training, and the Importance of Explicit Diversity](http://arxiv.org/abs/2109.14117)


  Ensemble learning is a process by which multiple base learners are
strategically generated and combined into one composite learner. There are two
features that are essential to an ensemble's performance, the individual
accuracies of the component learners and the overall diversity in the ensemble.
The right balance of learner accuracy and ensemble diversity can improve the
performance of machine learning tasks on benchmark and real-world data sets,
and recent theoretical and practical work has demonstrated the subtle trade-off
between accuracy and diversity in an ensemble. In this paper, we extend the
extant literature by providing a deeper theoretical understanding for assessing
and improving the optimality of any given ensemble, including random forests
and deep neural network ensembles. We also propose a training algorithm for
neural network ensembles and demonstrate that our approach provides improved
performance when compared to both state-of-the-art individual learners and
ensembles of state-of-the-art learners trained using standard loss functions.
Our key insight is that it is better to explicitly encourage diversity in an
ensemble, rather than merely allowing diversity to occur by happenstance, and
that rigorous theoretical bounds on the trade-off between diversity and learner
accuracy allow one to know when an optimal arrangement has been achieved.

    

### [[2109.14119] Stochastic Training is Not Necessary for Generalization](http://arxiv.org/abs/2109.14119)


  It is widely believed that the implicit regularization of stochastic gradient
descent (SGD) is fundamental to the impressive generalization behavior we
observe in neural networks. In this work, we demonstrate that non-stochastic
full-batch training can achieve strong performance on CIFAR-10 that is on-par
with SGD, using modern architectures in settings with and without data
augmentation. To this end, we utilize modified hyperparameters and show that
the implicit regularization of SGD can be completely replaced with explicit
regularization. This strongly suggests that theories that rely heavily on
properties of stochastic sampling to explain generalization are incomplete, as
strong generalization behavior is still observed in the absence of stochastic
sampling. Fundamentally, deep learning can succeed without stochasticity. Our
observations further indicate that the perceived difficulty of full-batch
training is largely the result of its optimization properties and the
disproportionate time and effort spent by the ML community tuning optimizers
and hyperparameters for small-batch training.

    

### [[2109.14120] Meta Learning on a Sequence of Imbalanced Domains with Difficulty Awareness](http://arxiv.org/abs/2109.14120)


  Recognizing new objects by learning from a few labeled examples in an
evolving environment is crucial to obtain excellent generalization ability for
real-world machine learning systems. A typical setting across current meta
learning algorithms assumes a stationary task distribution during meta
training. In this paper, we explore a more practical and challenging setting
where task distribution changes over time with domain shift. Particularly, we
consider realistic scenarios where task distribution is highly imbalanced with
domain labels unavailable in nature. We propose a kernel-based method for
domain change detection and a difficulty-aware memory management mechanism that
jointly considers the imbalanced domain size and domain importance to learn
across domains continuously. Furthermore, we introduce an efficient adaptive
task sampling method during meta training, which significantly reduces task
gradient variance with theoretical guarantees. Finally, we propose a
challenging benchmark with imbalanced domain sequences and varied domain
difficulty. We have performed extensive evaluations on the proposed benchmark,
demonstrating the effectiveness of our method. We made our code publicly
available.

    

### [[2109.14124] Vitruvion: A Generative Model of Parametric CAD Sketches](http://arxiv.org/abs/2109.14124)


  Parametric computer-aided design (CAD) tools are the predominant way that
engineers specify physical structures, from bicycle pedals to airplanes to
printed circuit boards. The key characteristic of parametric CAD is that design
intent is encoded not only via geometric primitives, but also by parameterized
constraints between the elements. This relational specification can be viewed
as the construction of a constraint program, allowing edits to coherently
propagate to other parts of the design. Machine learning offers the intriguing
possibility of accelerating the design process via generative modeling of these
structures, enabling new tools such as autocompletion, constraint inference,
and conditional synthesis. In this work, we present such an approach to
generative modeling of parametric CAD sketches, which constitute the basic
computational building blocks of modern mechanical design. Our model, trained
on real-world designs from the SketchGraphs dataset, autoregressively
synthesizes sketches as sequences of primitives, with initial coordinates, and
constraints that reference back to the sampled primitives. As samples from the
model match the constraint graph representation used in standard CAD software,
they may be directly imported, solved, and edited according to downstream
design tasks. In addition, we condition the model on various contexts,
including partial sketches (primers) and images of hand-drawn sketches.
Evaluation of the proposed approach demonstrates its ability to synthesize
realistic CAD sketches and its potential to aid the mechanical design workflow.

    

### [[2109.14133] Fine-Grained Zero-Shot Learning with DNA as Side Information](http://arxiv.org/abs/2109.14133)


  Fine-grained zero-shot learning task requires some form of side-information
to transfer discriminative information from seen to unseen classes. As manually
annotated visual attributes are extremely costly and often impractical to
obtain for a large number of classes, in this study we use DNA as side
information for the first time for fine-grained zero-shot classification of
species. Mitochondrial DNA plays an important role as a genetic marker in
evolutionary biology and has been used to achieve near-perfect accuracy in the
species classification of living organisms. We implement a simple hierarchical
Bayesian model that uses DNA information to establish the hierarchy in the
image space and employs local priors to define surrogate classes for unseen
ones. On the benchmark CUB dataset, we show that DNA can be equally promising
yet in general a more accessible alternative than word vectors as a side
information. This is especially important as obtaining robust word
representations for fine-grained species names is not a practicable goal when
information about these species in free-form text is limited. On a newly
compiled fine-grained insect dataset that uses DNA information from over a
thousand species, we show that the Bayesian approach outperforms
state-of-the-art by a wide margin.

    

### [[2109.14142] On the Provable Generalization of Recurrent Neural Networks](http://arxiv.org/abs/2109.14142)


  Recurrent Neural Network (RNN) is a fundamental structure in deep learning.
Recently, some works study the training process of over-parameterized neural
networks, and show that over-parameterized networks can learn functions in some
notable concept classes with a provable generalization error bound. In this
paper, we analyze the training and generalization for RNNs with random
initialization, and provide the following improvements over recent works:
1) For a RNN with input sequence $x=(X_1,X_2,...,X_L)$, previous works study
to learn functions that are summation of $f(\beta^T_lX_l)$ and require
normalized conditions that $||X_l||\leq\epsilon$ with some very small
$\epsilon$ depending on the complexity of $f$. In this paper, using detailed
analysis about the neural tangent kernel matrix, we prove a generalization
error bound to learn such functions without normalized conditions and show that
some notable concept classes are learnable with the numbers of iterations and
samples scaling almost-polynomially in the input length $L$.
2) Moreover, we prove a novel result to learn N-variables functions of input
sequence with the form $f(\beta^T[X_{l_1},...,X_{l_N}])$, which do not belong
to the ``additive'' concept class, i,e., the summation of function $f(X_l)$.
And we show that when either $N$ or $l_0=\max(l_1,..,l_N)-\min(l_1,..,l_N)$ is
small, $f(\beta^T[X_{l_1},...,X_{l_N}])$ will be learnable with the number
iterations and samples scaling almost-polynomially in the input length $L$.

    

### [[2109.14144] Improving Dialogue State Tracking by Joint Slot Modeling](http://arxiv.org/abs/2109.14144)


  Dialogue state tracking models play an important role in a task-oriented
dialogue system. However, most of them model the slot types conditionally
independently given the input. We discover that it may cause the model to be
confused by slot types that share the same data type. To mitigate this issue,
we propose TripPy-MRF and TripPy-LSTM that models the slots jointly. Our
results show that they are able to alleviate the confusion mentioned above, and
they push the state-of-the-art on dataset MultiWoZ 2.1 from 58.7 to 61.3. Our
implementation is available at this https URL.

    

### [[2109.14147] Temporal Clustering with External Memory Network for Disease Progression Modeling](http://arxiv.org/abs/2109.14147)


  Disease progression modeling (DPM) involves using mathematical frameworks to
quantitatively measure the severity of how certain disease progresses. DPM is
useful in many ways such as predicting health state, categorizing disease
stages, and assessing patients disease trajectory etc. Recently, with wider
availability of electronic health records (EHR) and the broad application of
data-driven machine learning method, DPM has attracted much attention yet
remains two major challenges: (i) Due to the existence of irregularity,
heterogeneity and long-term dependency in EHRs, most existing DPM methods might
not be able to provide comprehensive patient representations. (ii) Lots of
records in EHRs might be irrelevant to the target disease. Most existing models
learn to automatically focus on the relevant information instead of explicitly
capture the target-relevant events, which might make the learned model
suboptimal. To address these two issues, we propose Temporal Clustering with
External Memory Network (TC-EMNet) for DPM that groups patients with similar
trajectories to form disease clusters/stages. TC-EMNet uses a variational
autoencoder (VAE) to capture internal complexity from the input data and
utilizes an external memory work to capture long term distance information,
both of which are helpful for producing comprehensive patient states. Last but
not least, k-means algorithm is adopted to cluster the extracted comprehensive
patient states to capture disease progression. Experiments on two real-world
datasets show that our model demonstrates competitive clustering performance
against state-of-the-art methods and is able to identify clinically meaningful
clusters. The visualization of the extracted patient states shows that the
proposed model can generate better patient states than the baselines.

    

### [[2109.14158] Second-Order Neural ODE Optimizer](http://arxiv.org/abs/2109.14158)


  We propose a novel second-order optimization framework for training the
emerging deep continuous-time models, specifically the Neural Ordinary
Differential Equations (Neural ODEs). Since their training already involves
expensive gradient computation by solving a backward ODE, deriving efficient
second-order methods becomes highly nontrivial. Nevertheless, inspired by the
recent Optimal Control (OC) interpretation of training deep networks, we show
that a specific continuous-time OC methodology, called Differential
Programming, can be adopted to derive backward ODEs for higher-order
derivatives at the same O(1) memory cost. We further explore a low-rank
representation of the second-order derivatives and show that it leads to
efficient preconditioned updates with the aid of Kronecker-based factorization.
The resulting method converges much faster than first-order baselines in
wall-clock time, and the improvement remains consistent across various
applications, e.g. image classification, generative flow, and time-series
prediction. Our framework also enables direct architecture optimization, such
as the integration time of Neural ODEs, with second-order feedback policies,
strengthening the OC perspective as a principled tool of analyzing optimization
in deep learning.

    

### [[2109.14159] Adaptive Multi-layer Contrastive Graph Neural Networks](http://arxiv.org/abs/2109.14159)


  We present Adaptive Multi-layer Contrastive Graph Neural Networks (AMC-GNN),
a self-supervised learning framework for Graph Neural Network, which learns
feature representations of sample data without data labels. AMC-GNN generates
two graph views by data augmentation and compares different layers' output
embeddings of Graph Neural Network encoders to obtain feature representations,
which could be used for downstream tasks. AMC-GNN could learn the importance
weights of embeddings in different layers adaptively through the attention
mechanism, and an auxiliary encoder is introduced to train graph contrastive
encoders better. The accuracy is improved by maximizing the representation's
consistency of positive pairs in the early layers and the final embedding
space. Our experiments show that the results can be consistently improved by
using the AMC-GNN framework, across four established graph benchmarks: Cora,
Citeseer, Pubmed, DBLP citation network datasets, as well as four newly
proposed datasets: Co-author-CS, Co-author-Physics, Amazon-Computers,
Amazon-Photo.

    

### [[2109.14162] Can multi-label classification networks know what they don't know?](http://arxiv.org/abs/2109.14162)


  Estimating out-of-distribution (OOD) uncertainty is a central challenge for
safely deploying machine learning models in the open-world environment.
Improved methods for OOD detection in multi-class classification have emerged,
while OOD detection methods for multi-label classification remain underexplored
and use rudimentary techniques. We propose JointEnergy, a simple and effective
method, which estimates the OOD indicator scores by aggregating energy scores
from multiple labels. We show that JointEnergy can be mathematically
interpreted from a joint likelihood perspective. Our results show consistent
improvement over previous methods that are based on the maximum-valued scores,
which fail to capture joint information from multiple labels. We demonstrate
the effectiveness of our method on three common multi-label classification
benchmarks, including MS-COCO, PASCAL-VOC, and NUS-WIDE. We show that
JointEnergy can reduce the FPR95 by up to 10.05% compared to the previous best
baseline, establishing state-of-the-art performance.

    

### [[2109.14171] Non-stationary Gaussian process discriminant analysis with variable selection for high-dimensional functional data](http://arxiv.org/abs/2109.14171)


  High-dimensional classification and feature selection tasks are ubiquitous
with the recent advancement in data acquisition technology. In several
application areas such as biology, genomics and proteomics, the data are often
functional in their nature and exhibit a degree of roughness and
non-stationarity. These structures pose additional challenges to commonly used
methods that rely mainly on a two-stage approach performing variable selection
and classification separately. We propose in this work a novel Gaussian process
discriminant analysis (GPDA) that combines these steps in a unified framework.
Our model is a two-layer non-stationary Gaussian process coupled with an Ising
prior to identify differentially-distributed locations. Scalable inference is
achieved via developing a variational scheme that exploits advances in the use
of sparse inverse covariance matrices. We demonstrate the performance of our
methodology on simulated datasets and two proteomics datasets: breast cancer
and SARS-CoV-2. Our approach distinguishes itself by offering explainability as
well as uncertainty quantification in addition to low computational cost, which
are crucial to increase trust and social acceptance of data-driven tools.

    

### [[2109.14176] Linear Asymptotic Convergence of Anderson Acceleration: Fixed-Point Analysis](http://arxiv.org/abs/2109.14176)


  We study the asymptotic convergence of AA($m$), i.e., Anderson acceleration
with window size $m$ for accelerating fixed-point methods $x_{k+1}=q(x_{k})$,
$x_k \in R^n$. Convergence acceleration by AA($m$) has been widely observed but
is not well understood. We consider the case where the fixed-point iteration
function $q(x)$ is differentiable and the convergence of the fixed-point method
itself is root-linear. We identify numerically several conspicuous properties
of AA($m$) convergence: First, AA($m$) sequences $\{x_k\}$ converge
root-linearly but the root-linear convergence factor depends strongly on the
initial condition. Second, the AA($m$) acceleration coefficients $\beta^{(k)}$
do not converge but oscillate as $\{x_k\}$ converges to $x^*$. To shed light on
these observations, we write the AA($m$) iteration as an augmented fixed-point
iteration $z_{k+1} =\Psi(z_k)$, $z_k \in R^{n(m+1)}$ and analyze the continuity
and differentiability properties of $\Psi(z)$ and $\beta(z)$. We find that the
vector of acceleration coefficients $\beta(z)$ is not continuous at the fixed
point $z^*$. However, we show that, despite the discontinuity of $\beta(z)$,
the iteration function $\Psi(z)$ is Lipschitz continuous and directionally
differentiable at $z^*$ for AA(1), and we generalize this to AA($m$) with $m>1$
for most cases. Furthermore, we find that $\Psi(z)$ is not differentiable at
$z^*$. We then discuss how these theoretical findings relate to the observed
convergence behaviour of AA($m$). The discontinuity of $\beta(z)$ at $z^*$
allows $\beta^{(k)}$ to oscillate as $\{x_k\}$ converges to $x^*$, and the
non-differentiability of $\Psi(z)$ allows AA($m$) sequences to converge with
root-linear convergence factors that strongly depend on the initial condition.
Additional numerical results illustrate our findings.

    

### [[2109.14180] Efficient Reinforced Feature Selection via Early Stopping Traverse Strategy](http://arxiv.org/abs/2109.14180)


  In this paper, we propose a single-agent Monte Carlo based reinforced feature
selection (MCRFS) method, as well as two efficiency improvement strategies,
i.e., early stopping (ES) strategy and reward-level interactive (RI) strategy.
Feature selection is one of the most important technologies in data
prepossessing, aiming to find the optimal feature subset for a given downstream
machine learning task. Enormous research has been done to improve its
effectiveness and efficiency. Recently, the multi-agent reinforced feature
selection (MARFS) has achieved great success in improving the performance of
feature selection. However, MARFS suffers from the heavy burden of
computational cost, which greatly limits its application in real-world
scenarios. In this paper, we propose an efficient reinforcement feature
selection method, which uses one agent to traverse the whole feature set, and
decides to select or not select each feature one by one. Specifically, we first
develop one behavior policy and use it to traverse the feature set and generate
training data. And then, we evaluate the target policy based on the training
data and improve the target policy by Bellman equation. Besides, we conduct the
importance sampling in an incremental way, and propose an early stopping
strategy to improve the training efficiency by the removal of skew data. In the
early stopping strategy, the behavior policy stops traversing with a
probability inversely proportional to the importance sampling weight. In
addition, we propose a reward-level interactive strategy to improve the
training efficiency via reward-level external advice. Finally, we design
extensive experiments on real-world data to demonstrate the superiority of the
proposed method.

    

### [[2109.14181] Anderson Acceleration as a Krylov Method with Application to Asymptotic Convergence Analysis](http://arxiv.org/abs/2109.14181)


  Anderson acceleration is widely used for accelerating the convergence of
fixed-point methods $x_{k+1}=q(x_{k})$, $x_k \in \mathbb{R}^n$. We consider the
case of linear fixed-point methods $x_{k+1}=M x_{k}+b$ and obtain polynomial
residual update formulas for AA($m$), i.e., Anderson acceleration with window
size $m$. We find that the standard AA($m$) method with initial iterates $x_k$,
$k=0, \ldots, m$ defined recursively using AA($k$), is a Krylov space method.
This immediately implies that $k$ iterations of AA($m$) cannot produce a
smaller residual than $k$ iterations of GMRES without restart (but without
implying anything about the relative convergence speed of (windowed) AA($m$)
versus restarted GMRES($m$)). We introduce the notion of multi-Krylov method
and show that AA($m$) with general initial iterates $\{x_0, \ldots, x_m\}$ is a
multi-Krylov method. We find that the AA($m$) residual polynomials observe a
periodic memory effect where increasing powers of the error iteration matrix
$M$ act on the initial residual as the iteration number increases. We derive
several further results based on these polynomial residual update formulas,
including orthogonality relations, a lower bound on the AA(1) acceleration
coefficient $\beta_k$, and explicit nonlinear recursions for the AA(1)
residuals and residual polynomials that do not include the acceleration
coefficient $\beta_k$. We apply these results to study the influence of the
initial guess on the asymptotic convergence factor of AA(1).

    

### [[2109.14188] Information-Bottleneck-Based Behavior Representation Learning for Multi-agent Reinforcement learning](http://arxiv.org/abs/2109.14188)


  In multi-agent deep reinforcement learning, extracting sufficient and compact
information of other agents is critical to attain efficient convergence and
scalability of an algorithm. In canonical frameworks, distilling of such
information is often done in an implicit and uninterpretable manner, or
explicitly with cost functions not able to reflect the relationship between
information compression and utility in representation. In this paper, we
present Information-Bottleneck-based Other agents' behavior Representation
learning for Multi-agent reinforcement learning (IBORM) to explicitly seek
low-dimensional mapping encoder through which a compact and informative
representation relevant to other agents' behaviors is established. IBORM
leverages the information bottleneck principle to compress observation
information, while retaining sufficient information relevant to other agents'
behaviors used for cooperation decision. Empirical results have demonstrated
that IBORM delivers the fastest convergence rate and the best performance of
the learned policies, as compared with implicit behavior representation
learning and explicit behavior representation learning without explicitly
considering information compression and utility.

    

### [[2109.14198] Breaking the curse of dimensionality with Isolation Kernel](http://arxiv.org/abs/2109.14198)


  The curse of dimensionality has been studied in different aspects. However,
breaking the curse has been elusive. We show for the first time that it is
possible to break the curse using the recently introduced Isolation Kernel. We
show that only Isolation Kernel performs consistently well in indexed search,
spectral & density peaks clustering, SVM classification and t-SNE visualization
in both low and high dimensions, compared with distance, Gaussian and linear
kernels. This is also supported by our theoretical analyses that Isolation
Kernel is the only kernel that has the provable ability to break the curse,
compared with existing metric-based Lipschitz continuous kernels.

    

### [[2109.14200] Can phones, syllables, and words emerge as side-products of cross-situational audiovisual learning? -- A computational investigation](http://arxiv.org/abs/2109.14200)


  Decades of research has studied how language learning infants learn to
discriminate speech sounds, segment words, and associate words with their
meanings. While gradual development of such capabilities is unquestionable, the
exact nature of these skills and the underlying mental representations yet
remains unclear. In parallel, computational studies have shown that basic
comprehension of speech can be achieved by statistical learning between speech
and concurrent referentially ambiguous visual input. These models can operate
without prior linguistic knowledge such as representations of linguistic units,
and without learning mechanisms specifically targeted at such units. This has
raised the question of to what extent knowledge of linguistic units, such as
phone(me)s, syllables, and words, could actually emerge as latent
representations supporting the translation between speech and representations
in other modalities, and without the units being proximal learning targets for
the learner. In this study, we formulate this idea as the so-called latent
language hypothesis (LLH), connecting linguistic representation learning to
general predictive processing within and across sensory modalities. We review
the extent that the audiovisual aspect of LLH is supported by the existing
computational studies. We then explore LLH further in extensive learning
simulations with different neural network models for audiovisual
cross-situational learning, and comparing learning from both synthetic and real
speech data. We investigate whether the latent representations learned by the
networks reflect phonetic, syllabic, or lexical structure of input speech by
utilizing an array of complementary evaluation metrics related to linguistic
selectivity and temporal characteristics of the representations. As a result,
we find that representations associated...

    

### [[2109.14205] On Brightness Agnostic Adversarial Examples Against Face Recognition Systems](http://arxiv.org/abs/2109.14205)


  This paper introduces a novel adversarial example generation method against
face recognition systems (FRSs). An adversarial example (AX) is an image with
deliberately crafted noise to cause incorrect predictions by a target system.
The AXs generated from our method remain robust under real-world brightness
changes. Our method performs non-linear brightness transformations while
leveraging the concept of curriculum learning during the attack generation
procedure. We demonstrate that our method outperforms conventional techniques
from comprehensive experimental investigations in the digital and physical
world. Furthermore, this method enables practical risk assessment of FRSs
against brightness agnostic AXs.

    

### [[2109.14206] Exact Statistical Inference for the Wasserstein Distance by Selective Inference](http://arxiv.org/abs/2109.14206)


  In this paper, we study statistical inference for the Wasserstein distance,
which has attracted much attention and has been applied to various machine
learning tasks. Several studies have been proposed in the literature, but
almost all of them are based on asymptotic approximation and do not have
finite-sample validity. In this study, we propose an exact (non-asymptotic)
inference method for the Wasserstein distance inspired by the concept of
conditional Selective Inference (SI). To our knowledge, this is the first
method that can provide a valid confidence interval (CI) for the Wasserstein
distance with finite-sample coverage guarantee, which can be applied not only
to one-dimensional problems but also to multi-dimensional problems. We evaluate
the performance of the proposed method on both synthetic and real-world
datasets.

    

### [[2109.14213] On the One-sided Convergence of Adam-type Algorithms in Non-convex Non-concave Min-max Optimization](http://arxiv.org/abs/2109.14213)


  Adam-type methods, the extension of adaptive gradient methods, have shown
great performance in the training of both supervised and unsupervised machine
learning models. In particular, Adam-type optimizers have been widely used
empirically as the default tool for training generative adversarial networks
(GANs). On the theory side, however, despite the existence of theoretical
results showing the efficiency of Adam-type methods in minimization problems,
the reason of their wonderful performance still remains absent in GAN's
training. In existing works, the fast convergence has long been considered as
one of the most important reasons and multiple works have been proposed to give
a theoretical guarantee of the convergence to a critical point of min-max
optimization algorithms under certain assumptions. In this paper, we firstly
argue empirically that in GAN's training, Adam does not converge to a critical
point even upon successful training: Only the generator is converging while the
discriminator's gradient norm remains high throughout the training. We name
this one-sided convergence. Then we bridge the gap between experiments and
theory by showing that Adam-type algorithms provably converge to a one-sided
first order stationary points in min-max optimization problems under the
one-sided MVI condition. We also empirically verify that such one-sided MVI
condition is satisfied for standard GANs after trained over standard data sets.
To the best of our knowledge, this is the very first result which provides an
empirical observation and a strict theoretical guarantee on the one-sided
convergence of Adam-type algorithms in min-max optimization.

    

### [[2109.14216] Flow Based Models For Manifold Data](http://arxiv.org/abs/2109.14216)


  Flow-based generative models typically define a latent space with
dimensionality identical to the observational space. In many problems, however,
the data does not populate the full ambient data-space that they natively
reside in, rather inhabiting a lower-dimensional manifold. In such scenarios,
flow-based models are unable to represent data structures exactly as their
density will always have support off the data manifold, potentially resulting
in degradation of model performance. In addition, the requirement for equal
latent and data space dimensionality can unnecessarily increase complexity for
contemporary flow models. Towards addressing these problems, we propose to
learn a manifold prior that affords benefits to both sample generation and
representation quality. An auxiliary benefit of our approach is the ability to
identify the intrinsic dimension of the data distribution.

    

### [[2109.14218] Equivariant Neural Network for Factor Graphs](http://arxiv.org/abs/2109.14218)


  Several indices used in a factor graph data structure can be permuted without
changing the underlying probability distribution. An algorithm that performs
inference on a factor graph should ideally be equivariant or invariant to
permutations of global indices of nodes, variable orderings within a factor,
and variable assignment orderings. However, existing neural network-based
inference procedures fail to take advantage of this inductive bias. In this
paper, we precisely characterize these isomorphic properties of factor graphs
and propose two inference models: Factor-Equivariant Neural Belief Propagation
(FE-NBP) and Factor-Equivariant Graph Neural Networks (FE-GNN). FE-NBP is a
neural network that generalizes BP and respects each of the above properties of
factor graphs while FE-GNN is an expressive GNN model that relaxes an
isomorphic property in favor of greater expressivity. Empirically, we
demonstrate on both real-world and synthetic datasets, for both marginal
inference and MAP inference, that FE-NBP and FE-GNN together cover a range of
sample complexity regimes: FE-NBP achieves state-of-the-art performance on
small datasets while FE-GNN achieves state-of-the-art performance on large
datasets.

    

### [[2109.14235] Error rate control for classification rules in multiclass mixture models](http://arxiv.org/abs/2109.14235)


  In the context of finite mixture models one considers the problem of
classifying as many observations as possible in the classes of interest while
controlling the classification error rate in these same classes. Similar to
what is done in the framework of statistical test theory, different type I and
type II-like classification error rates can be defined, along with their
associated optimal rules, where optimality is defined as minimizing type II
error rate while controlling type I error rate at some nominal level. It is
first shown that finding an optimal classification rule boils down to searching
an optimal region in the observation space where to apply the classical Maximum
A Posteriori (MAP) rule. Depending on the misclassification rate to be
controlled, the shape of the optimal region is provided, along with a heuristic
to compute the optimal classification rule in practice. In particular, a
multiclass FDR-like optimal rule is defined and compared to the thresholded MAP
rules that is used in most applications. It is shown on both simulated and real
datasets that the FDR-like optimal rule may be significantly less conservative
than the thresholded MAP rule.

    

### [[2109.14236] LightSecAgg: Rethinking Secure Aggregation in Federated Learning](http://arxiv.org/abs/2109.14236)


  Secure model aggregation is a key component of federated learning (FL) that
aims at protecting the privacy of each user's individual model, while allowing
their global aggregation. It can be applied to any aggregation-based
approaches, including algorithms for training a global model, as well as
personalized FL frameworks. Model aggregation needs to also be resilient to
likely user dropouts in FL system, making its design substantially more
complex. State-of-the-art secure aggregation protocols essentially rely on
secret sharing of the random-seeds that are used for mask generations at the
users, in order to enable the reconstruction and cancellation of those
belonging to dropped users. The complexity of such approaches, however, grows
substantially with the number of dropped users. We propose a new approach,
named LightSecAgg, to overcome this bottleneck by turning the focus from
"random-seed reconstruction of the dropped users" to "one-shot aggregate-mask
reconstruction of the active users". More specifically, in LightSecAgg each
user protects its local model by generating a single random mask. This mask is
then encoded and shared to other users, in such a way that the aggregate-mask
of any sufficiently large set of active users can be reconstructed directly at
the server via encoded masks. We show that LightSecAgg achieves the same
privacy and dropout-resiliency guarantees as the state-of-the-art protocols,
while significantly reducing the overhead for resiliency to dropped users.
Furthermore, our system optimization helps to hide the runtime cost of offline
processing by parallelizing it with model training. We evaluate LightSecAgg via
extensive experiments for training diverse models on various datasets in a
realistic FL system, and demonstrate that LightSecAgg significantly reduces the
total training time, achieving a performance gain of up to $12.7\times$ over
baselines.

    

### [[2109.14247] Training Feedback Spiking Neural Networks by Implicit Differentiation on the Equilibrium State](http://arxiv.org/abs/2109.14247)


  Spiking neural networks (SNNs) are brain-inspired models that enable
energy-efficient implementation on neuromorphic hardware. However, the
supervised training of SNNs remains a hard problem due to the discontinuity of
the spiking neuron model. Most existing methods imitate the backpropagation
framework and feedforward architectures for artificial neural networks, and use
surrogate derivatives or compute gradients with respect to the spiking time to
deal with the problem. These approaches either accumulate approximation errors
or only propagate information limitedly through existing spikes, and usually
require information propagation along time steps with large memory costs and
biological implausibility. In this work, we consider feedback spiking neural
networks, which are more brain-like, and propose a novel training method that
does not rely on the exact reverse of the forward computation. First, we show
that the average firing rates of SNNs with feedback connections would gradually
evolve to an equilibrium state along time, which follows a fixed-point
equation. Then by viewing the forward computation of feedback SNNs as a
black-box solver for this equation, and leveraging the implicit differentiation
on the equation, we can compute the gradient for parameters without considering
the exact forward procedure. In this way, the forward and backward procedures
are decoupled and therefore the problem of non-differentiable spiking functions
is avoided. We also briefly discuss the biological plausibility of implicit
differentiation, which only requires computing another equilibrium. Extensive
experiments on MNIST, Fashion-MNIST, N-MNIST, CIFAR-10, and CIFAR-100
demonstrate the superior performance of our method for feedback models with
fewer neurons and parameters in a small number of time steps. Our code is
avaiable at this https URL.

    

### [[2109.14248] EBSD Grain Knowledge Graph Representation Learning for Material Structure-Property Prediction](http://arxiv.org/abs/2109.14248)


  The microstructure is an essential part of materials, storing the genes of
materials and having a decisive influence on materials' physical and chemical
properties. The material genetic engineering program aims to establish the
relationship between material composition/process, organization, and
performance to realize the reverse design of materials, thereby accelerating
the research and development of new materials. However, tissue analysis methods
of materials science, such as metallographic analysis, XRD analysis, and EBSD
analysis, cannot directly establish a complete quantitative relationship
between tissue structure and performance. Therefore, this paper proposes a
novel data-knowledge-driven organization representation and performance
prediction method to obtain a quantitative structure-performance relationship.
First, a knowledge graph based on EBSD is constructed to describe the
material's mesoscopic microstructure. Then a graph representation learning
network based on graph attention is constructed, and the EBSD organizational
knowledge graph is input into the network to obtain graph-level feature
embedding. Finally, the graph-level feature embedding is input to a graph
feature mapping network to obtain the material's mechanical properties. The
experimental results show that our method is superior to traditional machine
learning and machine vision methods.

    

### [[2109.14251] Road Network Guided Fine-Grained Urban Traffic Flow Inference](http://arxiv.org/abs/2109.14251)


  Accurate inference of fine-grained traffic flow from coarse-grained one is an
emerging yet crucial problem, which can help greatly reduce the number of
traffic monitoring sensors for cost savings. In this work, we notice that
traffic flow has a high correlation with road network, which was either
completely ignored or simply treated as an external factor in previous works.
To facilitate this problem, we propose a novel Road-Aware Traffic Flow
Magnifier (RATFM) that explicitly exploits the prior knowledge of road networks
to fully learn the road-aware spatial distribution of fine-grained traffic
flow. Specifically, a multi-directional 1D convolutional layer is first
introduced to extract the semantic feature of the road network. Subsequently,
we incorporate the road network feature and coarse-grained flow feature to
regularize the short-range spatial distribution modeling of road-relative
traffic flow. Furthermore, we take the road network feature as a query to
capture the long-range spatial distribution of traffic flow with a transformer
architecture. Benefiting from the road-aware inference mechanism, our method
can generate high-quality fine-grained traffic flow maps. Extensive experiments
on three real-world datasets show that the proposed RATFM outperforms
state-of-the-art models under various scenarios.

    

### [[2109.14259] Hierarchical Character Tagger for Short Text Spelling Error Correction](http://arxiv.org/abs/2109.14259)


  State-of-the-art approaches to spelling error correction problem include
Transformer-based Seq2Seq models, which require large training sets and suffer
from slow inference time; and sequence labeling models based on Transformer
encoders like BERT, which involve token-level label space and therefore a large
pre-defined vocabulary dictionary. In this paper we present a Hierarchical
Character Tagger model, or HCTagger, for short text spelling error correction.
We use a pre-trained language model at the character level as a text encoder,
and then predict character-level edits to transform the original text into its
error-free form with a much smaller label space. For decoding, we propose a
hierarchical multi-task approach to alleviate the issue of long-tail label
distribution without introducing extra model parameters. Experiments on two
public misspelling correction datasets demonstrate that HCTagger is an accurate
and much faster approach than many existing models.

    

### [[2109.14268] Formulation and validation of a car-following model based on deep reinforcement learning](http://arxiv.org/abs/2109.14268)


  We propose and validate a novel car following model based on deep
reinforcement learning. Our model is trained to maximize externally given
reward functions for the free and car-following regimes rather than reproducing
existing follower trajectories. The parameters of these reward functions such
as desired speed, time gap, or accelerations resemble that of traditional
models such as the Intelligent Driver Model (IDM) and allow for explicitly
implementing different driving styles. Moreover, they partially lift the
black-box nature of conventional neural network models. The model is trained on
leading speed profiles governed by a truncated Ornstein-Uhlenbeck process
reflecting a realistic leader's kinematics.
This allows for arbitrary driving situations and an infinite supply of
training data. For various parameterizations of the reward functions, and for a
wide variety of artificial and real leader data, the model turned out to be
unconditionally string stable, comfortable, and crash-free. String stability
has been tested with a platoon of five followers following an artificial and a
real leading trajectory. A cross-comparison with the IDM calibrated to the
goodness-of-fit of the relative gaps showed a higher reward compared to the
traditional model and a better goodness-of-fit.

    

### [[2109.14271] (Machine) Learning to Improve the Empirical Performance of Discrete Algorithms](http://arxiv.org/abs/2109.14271)


  This paper discusses a data-driven, empirically-based framework to make
algorithmic decisions or recommendations without expert knowledge. We improve
the performance of two algorithmic case studies: the selection of a pivot rule
for the Simplex method and the selection of an all-pair shortest paths
algorithm. We train machine learning methods to select the optimal algorithm
for given data without human expert opinion. We use two types of techniques,
neural networks and boosted decision trees. We concluded, based on our
experiments, that:
1) Our selection framework recommends various pivot rules that improve
overall total performance over just using a fixed default pivot rule.
Over many years experts identified steepest-edge pivot rule as a favorite
pivot rule. Our data analysis corroborates that the number of iterations by
steepest-edge is no more than 4 percent more than the optimal selection which
corroborates human expert knowledge, but this time the knowledge was obtained
using machine learning. Here our recommendation system is best when using
gradient boosted trees.
2) For the all-pairs shortest path problem, the models trained made a large
improvement and our selection is on average .07 percent away from the optimal
choice. The conclusions do not seem to be affected by the machine learning
method we used.
We tried to make a parallel analysis of both algorithmic problems, but it is
clear that there are intrinsic differences. For example, in the all-pairs
shortest path problem the graph density is a reasonable predictor, but there is
no analogous single parameter for decisions in the Simplex method.

    

### [[2109.14274] Designing Counterfactual Generators using Deep Model Inversion](http://arxiv.org/abs/2109.14274)


  Explanation techniques that synthesize small, interpretable changes to a
given image while producing desired changes in the model prediction have become
popular for introspecting black-box models. Commonly referred to as
counterfactuals, the synthesized explanations are required to contain
discernible changes (for easy interpretability) while also being realistic
(consistency to the data manifold). In this paper, we focus on the case where
we have access only to the trained deep classifier and not the actual training
data. While the problem of inverting deep models to synthesize images from the
training distribution has been explored, our goal is to develop a deep
inversion approach to generate counterfactual explanations for a given query
image. Despite their effectiveness in conditional image synthesis, we show that
existing deep inversion methods are insufficient for producing meaningful
counterfactuals. We propose DISC (Deep Inversion for Synthesizing
Counterfactuals) that improves upon deep inversion by utilizing (a) stronger
image priors, (b) incorporating a novel manifold consistency objective and (c)
adopting a progressive optimization strategy. We find that, in addition to
producing visually meaningful explanations, the counterfactuals from DISC are
effective at learning classifier decision boundaries and are robust to unknown
test-time corruptions.

    

### [[2109.14275] Simulation-based Bayesian inference for multi-fingered robotic grasping](http://arxiv.org/abs/2109.14275)


  Multi-fingered robotic grasping is an undeniable stepping stone to universal
picking and dexterous manipulation. Yet, multi-fingered grippers remain
challenging to control because of their rich nonsmooth contact dynamics or
because of sensor noise. In this work, we aim to plan hand configurations by
performing Bayesian posterior inference through the full stochastic forward
simulation of the robot in its environment, hence robustly accounting for many
of the uncertainties in the system. While previous methods either relied on
simplified surrogates of the likelihood function or attempted to learn to
directly predict maximum likelihood estimates, we bring a novel
simulation-based approach for full Bayesian inference based on a deep neural
network surrogate of the likelihood-to-evidence ratio. Hand configurations are
found by directly optimizing through the resulting amortized and differentiable
expression for the posterior. The geometry of the configuration space is
accounted for by proposing a Riemannian manifold optimization procedure through
the neural posterior. Simulation and physical benchmarks demonstrate the high
success rate of the procedure.

    

### [[2109.14282] A gradient-based variable selection for binary classification in reproducing kernel Hilbert space](http://arxiv.org/abs/2109.14282)


  Variable selection is essential in high-dimensional data analysis. Although
various variable selection methods have been developed, most rely on the linear
model assumption. This article proposes a nonparametric variable selection
method for the large-margin classifier defined by reproducing the kernel
Hilbert space (RKHS). we propose a gradient-based representation of the
large-margin classifier and then regularize the gradient functions by the
group-lasso penalty to obtain sparse gradients that naturally lead to the
variable selection. The groupwise-majorization-decent algorithm (GMD, Yang and
Zou, 2015) is proposed to efficiently solve the proposed problem with a large
number of parameters. We employ the strong sequential rule (Tibshirani et al.,
2012) to facilitate the tuning procedure. The selection consistency of the
proposed method is established by obtaining the risk bound of the estimated
classifier and its gradient. Finally, we demonstrate the promising performance
of the proposed method through simulations and real data illustration.

    

### [[2109.14285] Be Confident! Towards Trustworthy Graph Neural Networks via Confidence Calibration](http://arxiv.org/abs/2109.14285)


  Despite Graph Neural Networks (GNNs) have achieved remarkable accuracy,
whether the results are trustworthy is still unexplored. Previous studies
suggest that many modern neural networks are over-confident on the predictions,
however, surprisingly, we discover that GNNs are primarily in the opposite
direction, i.e., GNNs are under-confident. Therefore, the confidence
calibration for GNNs is highly desired. In this paper, we propose a novel
trustworthy GNN model by designing a topology-aware post-hoc calibration
function. Specifically, we first verify that the confidence distribution in a
graph has homophily property, and this finding inspires us to design a
calibration GNN model (CaGCN) to learn the calibration function. CaGCN is able
to obtain a unique transformation from logits of GNNs to the calibrated
confidence for each node, meanwhile, such transformation is able to preserve
the order between classes, satisfying the accuracy-preserving property.
Moreover, we apply the calibration GNN to self-training framework, showing that
more trustworthy pseudo labels can be obtained with the calibrated confidence
and further improve the performance. Extensive experiments demonstrate the
effectiveness of our proposed model in terms of both calibration and accuracy.

    

### [[2109.14288] Self-Supervised Learning for 3D Medical Image Analysis using 3D SimCLR and Monte Carlo Dropout](http://arxiv.org/abs/2109.14288)


  Self-supervised learning methods can be used to learn meaningful
representations from unlabeled data that can be transferred to supervised
downstream tasks to reduce the need for labeled data. In this paper, we propose
a 3D self-supervised method that is based on the contrastive (SimCLR) method.
Additionally, we show that employing Bayesian neural networks (with Monte-Carlo
Dropout) during the inference phase can further enhance the results on the
downstream tasks. We showcase our models on two medical imaging segmentation
tasks: i) Brain Tumor Segmentation from 3D MRI, ii) Pancreas Tumor Segmentation
from 3D CT. Our experimental results demonstrate the benefits of our proposed
methods in both downstream data-efficiency and performance.

    

### [[2109.14306] Three-Stream 3D/1D CNN for Fine-Grained Action Classification and Segmentation in Table Tennis](http://arxiv.org/abs/2109.14306)


  This paper proposes a fusion method of modalities extracted from video
through a three-stream network with spatio-temporal and temporal convolutions
for fine-grained action classification in sport. It is applied to TTStroke-21
dataset which consists of untrimmed videos of table tennis games. The goal is
to detect and classify table tennis strokes in the videos, the first step of a
bigger scheme aiming at giving feedback to the players for improving their
performance. The three modalities are raw RGB data, the computed optical flow
and the estimated pose of the player. The network consists of three branches
with attention blocks. Features are fused at the latest stage of the network
using bilinear layers. Compared to previous approaches, the use of three
modalities allows faster convergence and better performances on both tasks:
classification of strokes with known temporal boundaries and joint segmentation
and classification. The pose is also further investigated in order to offer
richer feedback to the athletes.

    

### [[2109.14309] Online Aggregation of Probability Forecasts with Confidence](http://arxiv.org/abs/2109.14309)


  The paper presents numerical experiments and some theoretical developments in
prediction with expert advice (PEA). One experiment deals with predicting
electricity consumption depending on temperature and uses real data. As the
pattern of dependence can change with season and time of the day, the domain
naturally admits PEA formulation with experts having different ``areas of
expertise''. We consider the case where several competing methods produce
online predictions in the form of probability distribution functions. The
dissimilarity between a probability forecast and an outcome is measured by a
loss function (scoring rule). A popular example of scoring rule for continuous
outcomes is Continuous Ranked Probability Score (CRPS). In this paper the
problem of combining probabilistic forecasts is considered in the PEA
framework. We show that CRPS is a mixable loss function and then the
time-independent upper bound for the regret of the Vovk aggregating algorithm
using CRPS as a loss function can be obtained. Also, we incorporate a
``smooth'' version of the method of specialized experts in this scheme which
allows us to combine the probabilistic predictions of the specialized experts
with overlapping domains of their competence.

    

### [[2109.14311] Learning Dynamics Models for Model Predictive Agents](http://arxiv.org/abs/2109.14311)


  Model-Based Reinforcement Learning involves learning a \textit{dynamics
model} from data, and then using this model to optimise behaviour, most often
with an online \textit{planner}. Much of the recent research along these lines
presents a particular set of design choices, involving problem definition,
model learning and planning. Given the multiple contributions, it is difficult
to evaluate the effects of each. This paper sets out to disambiguate the role
of different design choices for learning dynamics models, by comparing their
performance to planning with a ground-truth model -- the simulator. First, we
collect a rich dataset from the training sequence of a model-free agent on 5
domains of the DeepMind Control Suite. Second, we train feed-forward dynamics
models in a supervised fashion, and evaluate planner performance while varying
and analysing different model design choices, including ensembling,
stochasticity, multi-step training and timestep size. Besides the quantitative
analysis, we describe a set of qualitative findings, rules of thumb, and future
research directions for planning with learned dynamics models. Videos of the
results are available at this https URL.

    

### [[2109.14320] Google Neural Network Models for Edge Devices: Analyzing and Mitigating Machine Learning Inference Bottlenecks](http://arxiv.org/abs/2109.14320)


  Emerging edge computing platforms often contain machine learning (ML)
accelerators that can accelerate inference for a wide range of neural network
(NN) models. These models are designed to fit within the limited area and
energy constraints of the edge computing platforms, each targeting various
applications (e.g., face detection, speech recognition, translation, image
captioning, video analytics). To understand how edge ML accelerators perform,
we characterize the performance of a commercial Google Edge TPU, using 24
Google edge NN models (which span a wide range of NN model types) and analyzing
each NN layer within each model. We find that the Edge TPU suffers from three
major shortcomings: (1) it operates significantly below peak computational
throughput, (2) it operates significantly below its theoretical energy
efficiency, and (3) its memory system is a large energy and performance
bottleneck. Our characterization reveals that the one-size-fits-all, monolithic
design of the Edge TPU ignores the high degree of heterogeneity both across
different NN models and across different NN layers within the same NN model,
leading to the shortcomings we observe.
We propose a new acceleration framework called Mensa. Mensa incorporates
multiple heterogeneous edge ML accelerators (including both on-chip and
near-data accelerators), each of which caters to the characteristics of a
particular subset of NN models and layers. During NN inference, for each NN
layer, Mensa decides which accelerator to schedule the layer on, taking into
account both the optimality of each accelerator for the layer and
layer-to-layer communication costs. Averaged across all 24 Google edge NN
models, Mensa improves energy efficiency and throughput by 3.0x and 3.1x over
the Edge TPU, and by 2.4x and 4.3x over Eyeriss~v2, a state-of-the-art
accelerator.

    

### [[2109.14326] Large-scale Crash Localization using Multi-Task Learning](http://arxiv.org/abs/2109.14326)


  Crash localization, an important step in debugging crashes, is challenging
when dealing with an extremely large number of diverse applications and
platforms and underlying root causes. Large-scale error reporting systems,
e.g., Windows Error Reporting (WER), commonly rely on manually developed rules
and heuristics to localize blamed frames causing the crashes. As new
applications and features are routinely introduced and existing applications
are run under new environments, developing new rules and maintaining existing
ones become extremely challenging. We propose a data-driven solution to address
the problem. We start with the first large-scale empirical study of 362K
crashes and their blamed methods reported to WER by tens of thousands of
applications running in the field. The analysis provides valuable insights on
where and how the crashes happen and what methods to blame for the crashes.
These insights enable us to develop DeepAnalyze, a novel multi-task sequence
labeling approach for identifying blamed frames in stack traces. We evaluate
our model with over a million real-world crashes from four popular Microsoft
applications and show that DeepAnalyze, trained with crashes from one set of
applications, not only accurately localizes crashes of the same applications,
but also bootstraps crash localization for other applications with zero to very
little additional training data.

    

### [[2109.14333] Distribution Knowledge Embedding for Graph Pooling](http://arxiv.org/abs/2109.14333)


  Graph-level representation learning is the pivotal step for downstream tasks
that operate on the whole graph. The most common approach to this problem
heretofore is graph pooling, where node features are typically averaged or
summed to obtain the graph representations. However, pooling operations like
averaging or summing inevitably cause massive information missing, which may
severely downgrade the final performance. In this paper, we argue what is
crucial to graph-level downstream tasks includes not only the topological
structure but also the distribution from which nodes are sampled. Therefore,
powered by existing Graph Neural Networks (GNN), we propose a new plug-and-play
pooling module, termed as Distribution Knowledge Embedding (DKEPool), where
graphs are rephrased as distributions on top of GNNs and the pooling goal is to
summarize the entire distribution information instead of retaining a certain
feature vector by simple predefined pooling operations. A DKEPool network de
facto disassembles representation learning into two stages, structure learning
and distribution learning. Structure learning follows a recursive neighborhood
aggregation scheme to update node features where structure information is
obtained. Distribution learning, on the other hand, omits node interconnections
and focuses more on the distribution depicted by all the nodes. Extensive
experiments demonstrate that the proposed DKEPool significantly and
consistently outperforms the state-of-the-art methods.

    

### [[2109.14337] Deep Reinforcement Q-Learning for Intelligent Traffic Signal Control with Partial Detection](http://arxiv.org/abs/2109.14337)


  Intelligent traffic signal controllers, applying DQN algorithms to traffic
light policy optimization, efficiently reduce traffic congestion by adjusting
traffic signals to real-time traffic. Most propositions in the literature
however consider that all vehicles at the intersection are detected, an
unrealistic scenario. Recently, new wireless communication technologies have
enabled cost-efficient detection of connected vehicles by infrastructures. With
only a small fraction of the total fleet currently equipped, methods able to
perform under low detection rates are desirable. In this paper, we propose a
deep reinforcement Q-learning model to optimize traffic signal control at an
isolated intersection, in a partially observable environment with connected
vehicles. First, we present the novel DQN model within the RL framework. We
introduce a new state representation for partially observable environments and
a new reward function for traffic signal control, and provide a network
architecture and tuned hyper-parameters. Second, we evaluate the performances
of the model in numerical simulations on multiple scenarios, in two steps. At
first in full detection against existing actuated controllers, then in partial
detection with loss estimates for proportions of connected vehicles. Finally,
from the obtained results, we define thresholds for detection rates with
acceptable and optimal performance levels.

    

### [[2109.14375] Dynamic Regret Analysis for Online Meta-Learning](http://arxiv.org/abs/2109.14375)


  The online meta-learning framework has arisen as a powerful tool for the
continual lifelong learning setting. The goal for an agent is to quickly learn
new tasks by drawing on prior experience, while it faces with tasks one after
another. This formulation involves two levels: outer level which learns
meta-learners and inner level which learns task-specific models, with only a
small amount of data from the current task. While existing methods provide
static regret analysis for the online meta-learning framework, we establish
performance in terms of dynamic regret which handles changing environments from
a global prospective. We also build off of a generalized version of the
adaptive gradient methods that covers both ADAM and ADAGRAD to learn
meta-learners in the outer level. We carry out our analyses in a stochastic
setting, and in expectation prove a logarithmic local dynamic regret which
depends explicitly on the total number of iterations T and parameters of the
learner. Apart from, we also indicate high probability bounds on the
convergence rates of proposed algorithm with appropriate selection of
parameters, which have not been argued before.

    

### [[2109.14376] Fairness-Driven Private Collaborative Machine Learning](http://arxiv.org/abs/2109.14376)


  The performance of machine learning algorithms can be considerably improved
when trained over larger datasets. In many domains, such as medicine and
finance, larger datasets can be obtained if several parties, each having access
to limited amounts of data, collaborate and share their data. However, such
data sharing introduces significant privacy challenges. While multiple recent
studies have investigated methods for private collaborative machine learning,
the fairness of such collaborative algorithms was overlooked. In this work we
suggest a feasible privacy-preserving pre-process mechanism for enhancing
fairness of collaborative machine learning algorithms. Our experimentation with
the proposed method shows that it is able to enhance fairness considerably with
only a minor compromise in accuracy.

    

### [[2109.14412] Apple Tasting Revisited: Bayesian Approaches to Partially Monitored Online Binary Classification](http://arxiv.org/abs/2109.14412)


  We consider a variant of online binary classification where a learner
sequentially assigns labels ($0$ or $1$) to items with unknown true class. If,
but only if, the learner chooses label $1$ they immediately observe the true
label of the item. The learner faces a trade-off between short-term
classification accuracy and long-term information gain. This problem has
previously been studied under the name of the `apple tasting' problem. We
revisit this problem as a partial monitoring problem with side information, and
focus on the case where item features are linked to true classes via a logistic
regression model. Our principal contribution is a study of the performance of
Thompson Sampling (TS) for this problem. Using recently developed
information-theoretic tools, we show that TS achieves a Bayesian regret bound
of an improved order to previous approaches. Further, we experimentally verify
that efficient approximations to TS and Information Directed Sampling via
Pólya-Gamma augmentation have superior empirical performance to existing
methods.

    

### [[2109.14419] On the Estimation Bias in Double Q-Learning](http://arxiv.org/abs/2109.14419)


  Double Q-learning is a classical method for reducing overestimation bias,
which is caused by taking maximum estimated values in the Bellman operation.
Its variants in the deep Q-learning paradigm have shown great promise in
producing reliable value prediction and improving learning performance.
However, as shown by prior work, double Q-learning is not fully unbiased and
suffers from underestimation bias. In this paper, we show that such
underestimation bias may lead to multiple non-optimal fixed points under an
approximated Bellman operator. To address the concerns of converging to
non-optimal stationary solutions, we propose a simple but effective approach as
a partial fix for the underestimation bias in double Q-learning. This approach
leverages an approximate dynamic programming to bound the target value. We
extensively evaluate our proposed method in the Atari benchmark tasks and
demonstrate its significant improvement over baseline algorithms.

    

### [[2109.14420] FastCorrect 2: Fast Error Correction on Multiple Candidates for Automatic Speech Recognition](http://arxiv.org/abs/2109.14420)


  Error correction is widely used in automatic speech recognition (ASR) to
post-process the generated sentence, and can further reduce the word error rate
(WER). Although multiple candidates are generated by an ASR system through beam
search, current error correction approaches can only correct one sentence at a
time, failing to leverage the voting effect from multiple candidates to better
detect and correct error tokens. In this work, we propose FastCorrect 2, an
error correction model that takes multiple ASR candidates as input for better
correction accuracy. FastCorrect 2 adopts non-autoregressive generation for
fast inference, which consists of an encoder that processes multiple source
sentences and a decoder that generates the target sentence in parallel from the
adjusted source sentence, where the adjustment is based on the predicted
duration of each source token. However, there are some issues when handling
multiple source sentences. First, it is non-trivial to leverage the voting
effect from multiple source sentences since they usually vary in length. Thus,
we propose a novel alignment algorithm to maximize the degree of token
alignment among multiple sentences in terms of token and pronunciation
similarity. Second, the decoder can only take one adjusted source sentence as
input, while there are multiple source sentences. Thus, we develop a candidate
predictor to detect the most suitable candidate for the decoder. Experiments on
our inhouse dataset and AISHELL-1 show that FastCorrect 2 can further reduce
the WER over the previous correction model with single candidate by 3.2% and
2.6%, demonstrating the effectiveness of leveraging multiple candidates in ASR
error correction. FastCorrect 2 achieves better performance than the cascaded
re-scoring and correction pipeline and can serve as a unified post-processing
module for ASR.

    

### [[2109.14422] Multi-class Probabilistic Bounds for Self-learning](http://arxiv.org/abs/2109.14422)


  Self-learning is a classical approach for learning with both labeled and
unlabeled observations which consists in giving pseudo-labels to unlabeled
training instances with a confidence score over a predetermined threshold. At
the same time, the pseudo-labeling technique is prone to error and runs the
risk of adding noisy labels into unlabeled training data. In this paper, we
present a probabilistic framework for analyzing self-learning in the
multi-class classification scenario with partially labeled data. First, we
derive a transductive bound over the risk of the multi-class majority vote
classifier. Based on this result, we propose to automatically choose the
threshold for pseudo-labeling that minimizes the transductive bound. Then, we
introduce a mislabeling error model to analyze the error of the majority vote
classifier in the case of the pseudo-labeled data. We derive a probabilistic
C-bound over the majority vote error when an imperfect label is given.
Empirical results on different data sets show the effectiveness of our
framework compared to several state-of-the-art semi-supervised approaches.

    

### [[2109.14423] Digital Twins based Day-ahead Integrated Energy System Scheduling under Load and Renewable Energy Uncertainties](http://arxiv.org/abs/2109.14423)


  By constructing digital twins (DT) of an integrated energy system (IES), one
can benefit from DT's predictive capabilities to improve coordinations among
various energy converters, hence enhancing energy efficiency, cost savings and
carbon emission reduction. This paper is motivated by the fact that practical
IESs suffer from multiple uncertainty sources, and complicated surrounding
environment. To address this problem, a novel DT-based day-ahead scheduling
method is proposed. The physical IES is modelled as a multi-vector energy
system in its virtual space that interacts with the physical IES to manipulate
its operations. A deep neural network is trained to make statistical
cost-saving scheduling by learning from both historical forecasting errors and
day-ahead forecasts. Case studies of IESs show that the proposed DT-based
method is able to reduce the operating cost of IES by 63.5%, comparing to the
existing forecast-based scheduling methods. It is also found that both electric
vehicles and thermal energy storages play proactive roles in the proposed
method, highlighting their importance in future energy system integration and
decarbonisation.

    

### [[2109.14429] Minimal Expected Regret in Linear Quadratic Control](http://arxiv.org/abs/2109.14429)


  We consider the problem of online learning in Linear Quadratic Control
systems whose state transition and state-action transition matrices $A$ and $B$
may be initially unknown. We devise an online learning algorithm and provide
guarantees on its expected regret. This regret at time $T$ is upper bounded (i)
by $\widetilde{O}((d_u+d_x)\sqrt{d_xT})$ when $A$ and $B$ are unknown, (ii) by
$\widetilde{O}(d_x^2\log(T))$ if only $A$ is unknown, and (iii) by
$\widetilde{O}(d_x(d_u+d_x)\log(T))$ if only $B$ is unknown and under some mild
non-degeneracy condition ($d_x$ and $d_u$ denote the dimensions of the state
and of the control input, respectively). These regret scalings are minimal in
$T$, $d_x$ and $d_u$ as they match existing lower bounds in scenario (i) when
$d_x\le d_u$ [SF20], and in scenario (ii) [lai1986]. We conjecture that our
upper bounds are also optimal in scenario (iii) (there is no known lower bound
in this setting).
Existing online algorithms proceed in epochs of (typically exponentially)
growing durations. The control policy is fixed within each epoch, which
considerably simplifies the analysis of the estimation error on $A$ and $B$ and
hence of the regret. Our algorithm departs from this design choice: it is a
simple variant of certainty-equivalence regulators, where the estimates of $A$
and $B$ and the resulting control policy can be updated as frequently as we
wish, possibly at every step. Quantifying the impact of such a
constantly-varying control policy on the performance of these estimates and on
the regret constitutes one of the technical challenges tackled in this paper.

    

### [[2109.14430] PyHard: a novel tool for generating hardness embeddings to support data-centric analysis](http://arxiv.org/abs/2109.14430)


  For building successful Machine Learning (ML) systems, it is imperative to
have high quality data and well tuned learning models. But how can one assess
the quality of a given dataset? And how can the strengths and weaknesses of a
model on a dataset be revealed? Our new tool PyHard employs a methodology known
as Instance Space Analysis (ISA) to produce a hardness embedding of a dataset
relating the predictive performance of multiple ML models to estimated instance
hardness meta-features. This space is built so that observations are
distributed linearly regarding how hard they are to classify. The user can
visually interact with this embedding in multiple ways and obtain useful
insights about data and algorithmic performance along the individual
observations of the dataset. We show in a COVID prognosis dataset how this
analysis supported the identification of pockets of hard observations that
challenge ML models and are therefore worth closer inspection, and the
delineation of regions of strengths and weaknesses of ML models.

    

### [[2109.14433] Multi-loss ensemble deep learning for chest X-ray classification](http://arxiv.org/abs/2109.14433)


  Class imbalance is common in medical image classification tasks, where the
number of abnormal samples is fewer than the number of normal samples. The
difficulty of imbalanced classification is compounded by other issues such as
the size and distribution of the dataset. Reliable training of deep neural
networks continues to be a major challenge in such class-imbalanced conditions.
The loss function used to train the deep neural networks highly impact the
performance of both balanced and imbalanced tasks. Currently, the cross-entropy
loss remains the de-facto loss function for balanced and imbalanced
classification tasks. This loss, however, asserts equal learning to all
classes, leading to the classification of most samples as the majority normal
class. To provide a critical analysis of different loss functions and identify
those suitable for class-imbalanced classification, we benchmark various
state-of-the-art loss functions and propose novel loss functions to train a DL
model and analyze its performance in a multiclass classification setting that
classifies pediatric chest X-rays as showing normal lungs, bacterial pneumonia,
or viral pneumonia manifestations. We also construct prediction-level and
model-level ensembles of the models that are trained with various loss
functions to improve classification performance. We performed localization
studies to interpret model behavior to ensure that the individual models and
their ensembles precisely learned the regions of interest showing disease
manifestations to classify the chest X-rays to their respective categories.

    

### [[2109.14449] One Loss for All: Deep Hashing with a Single Cosine Similarity based Learning Objective](http://arxiv.org/abs/2109.14449)


  A deep hashing model typically has two main learning objectives: to make the
learned binary hash codes discriminative and to minimize a quantization error.
With further constraints such as bit balance and code orthogonality, it is not
uncommon for existing models to employ a large number (>4) of losses. This
leads to difficulties in model training and subsequently impedes their
effectiveness. In this work, we propose a novel deep hashing model with only a
single learning objective. Specifically, we show that maximizing the cosine
similarity between the continuous codes and their corresponding binary
orthogonal codes can ensure both hash code discriminativeness and quantization
error minimization. Further, with this learning objective, code balancing can
be achieved by simply using a Batch Normalization (BN) layer and multi-label
classification is also straightforward with label smoothing. The result is an
one-loss deep hashing model that removes all the hassles of tuning the weights
of various losses. Importantly, extensive experiments show that our model is
highly effective, outperforming the state-of-the-art multi-loss hashing models
on three large-scale instance retrieval benchmarks, often by significant
margins. Code is available at this https URL


### [[2109.14456] Overview of the Arabic Sentiment Analysis 2021 Competition at KAUST](http://arxiv.org/abs/2109.14456)


  This paper provides an overview of the Arabic Sentiment Analysis Challenge
organized by King Abdullah University of Science and Technology (KAUST). The
task in this challenge is to develop machine learning models to classify a
given tweet into one of the three categories Positive, Negative, or Neutral.
From our recently released ASAD dataset, we provide the competitors with 55K
tweets for training, and 20K tweets for validation, based on which the
performance of participating teams are ranked on a leaderboard,
this https URL. The competition
received in total 1247 submissions from 74 teams (99 team members). The final
winners are determined by another private set of 20K tweets that have the same
distribution as the training and validation set. In this paper, we present the
main findings in the competition and summarize the methods and tools used by
the top ranked teams. The full dataset of 100K labeled tweets is also released
for public usage, at
this https URL.

    

### [[2109.14492] Variational Inference for Continuous-Time Switching Dynamical Systems](http://arxiv.org/abs/2109.14492)


  Switching dynamical systems provide a powerful, interpretable modeling
framework for inference in time-series data in, e.g., the natural sciences or
engineering applications. Since many areas, such as biology or discrete-event
systems, are naturally described in continuous time, we present a model based
on an Markov jump process modulating a subordinated diffusion process. We
provide the exact evolution equations for the prior and posterior marginal
densities, the direct solutions of which are however computationally
intractable. Therefore, we develop a new continuous-time variational inference
algorithm, combining a Gaussian process approximation on the diffusion level
with posterior inference for Markov jump processes. By minimizing the path-wise
Kullback-Leibler divergence we obtain (i) Bayesian latent state estimates for
arbitrary points on the real axis and (ii) point estimates of unknown system
parameters, utilizing variational expectation maximization. We extensively
evaluate our algorithm under the model assumption and for real-world examples.

    

### [[2109.14501] Towards a theory of out-of-distribution learning](http://arxiv.org/abs/2109.14501)


  What is learning? 20 century formalizations of learning theory -- which
precipitated revolutions in artificial intelligence -- focus primarily on
\textit{in-distribution} learning, that is, learning under the assumption that
the training data are sampled from the same distribution as the evaluation
distribution. This assumption renders these theories inadequate for
characterizing 21$^{st}$ century real world data problems, which are typically
characterized by evaluation distributions that differ from the training data
distributions (referred to as out-of-distribution learning). We therefore make
a small change to existing formal definitions of learnability by relaxing that
assumption. We then introduce \textbf{learning efficiency} (LE) to quantify the
amount a learner is able to leverage data for a given problem, regardless of
whether it is an in- or out-of-distribution problem. We then define and prove
the relationship between generalized notions of learnability, and show how this
framework is sufficiently general to characterize transfer, multitask, meta,
continual, and lifelong learning. We hope this unification helps bridge the gap
between empirical practice and theoretical guidance in real world problems.
Finally, because biological learning continues to outperform machine learning
algorithms on certain OOD challenges, we discuss the limitations of this
framework vis-á-vis its ability to formalize biological learning, suggesting
multiple avenues for future research.

    

### [[2109.14502] Untangling Braids with Multi-agent Q-Learning](http://arxiv.org/abs/2109.14502)


  We use reinforcement learning to tackle the problem of untangling braids. We
experiment with braids with 2 and 3 strands. Two competing players learn to
tangle and untangle a braid. We interface the braid untangling problem with the
OpenAI Gym environment, a widely used way of connecting agents to reinforcement
learning problems. The results provide evidence that the more we train the
system, the better the untangling player gets at untangling braids. At the same
time, our tangling player produces good examples of tangled braids.

    

### [[2109.14509] PAC-Bayes Information Bottleneck](http://arxiv.org/abs/2109.14509)


  Information bottleneck (IB) depicts a trade-off between the accuracy and
conciseness of encoded representations. IB has succeeded in explaining the
objective and behavior of neural networks (NNs) as well as learning better
representations. However, there are still critics of the universality of IB,
e.g., phase transition usually fades away, representation compression is not
causally related to generalization, and IB is trivial in deterministic cases.
In this work, we build a new IB based on the trade-off between the accuracy and
complexity of learned weights of NNs. We argue that this new IB represents a
more solid connection to the objective of NNs since the information stored in
weights bounds their PAC-Bayes generalization capability, hence we name it as
PAC-Bayes IB (PIB). On PIB, we can identify the phase transition phenomenon in
general cases and solidify the causality between compression and
generalization. We then derive a tractable solution of PIB and design a
stochastic inference algorithm by Markov chain Monte Carlo sampling. We
empirically verify our claims through extensive experiments. We also
substantiate the superiority of the proposed algorithm on training better NNs.

    

### [[2109.14516] On Assessing the Usefulness of Proxy Domains for Developing and Evaluating Embodied Agents](http://arxiv.org/abs/2109.14516)


  In many situations it is either impossible or impractical to develop and
evaluate agents entirely on the target domain on which they will be deployed.
This is particularly true in robotics, where doing experiments on hardware is
much more arduous than in simulation. This has become arguably more so in the
case of learning-based agents. To this end, considerable recent effort has been
devoted to developing increasingly realistic and higher fidelity simulators.
However, we lack any principled way to evaluate how good a ``proxy domain'' is,
specifically in terms of how useful it is in helping us achieve our end
objective of building an agent that performs well in the target domain. In this
work, we investigate methods to address this need. We begin by clearly
separating two uses of proxy domains that are often conflated: 1) their ability
to be a faithful predictor of agent performance and 2) their ability to be a
useful tool for learning. In this paper, we attempt to clarify the role of
proxy domains and establish new proxy usefulness (PU) metrics to compare the
usefulness of different proxy domains. We propose the relative predictive PU to
assess the predictive ability of a proxy domain and the learning PU to quantify
the usefulness of a proxy as a tool to generate learning data. Furthermore, we
argue that the value of a proxy is conditioned on the task that it is being
used to help solve. We demonstrate how these new metrics can be used to
optimize parameters of the proxy domain for which obtaining ground truth via
system identification is not trivial.

    

### [[2109.14523] Online Robust Reinforcement Learning with Model Uncertainty](http://arxiv.org/abs/2109.14523)


  Robust reinforcement learning (RL) is to find a policy that optimizes the
worst-case performance over an uncertainty set of MDPs. In this paper, we focus
on model-free robust RL, where the uncertainty set is defined to be centering
at a misspecified MDP that generates a single sample trajectory sequentially
and is assumed to be unknown. We develop a sample-based approach to estimate
the unknown uncertainty set and design a robust Q-learning algorithm (tabular
case) and robust TDC algorithm (function approximation setting), which can be
implemented in an online and incremental fashion. For the robust Q-learning
algorithm, we prove that it converges to the optimal robust Q function, and for
the robust TDC algorithm, we prove that it converges asymptotically to some
stationary points. Unlike the results in [Roy et al., 2017], our algorithms do
not need any additional conditions on the discount factor to guarantee the
convergence. We further characterize the finite-time error bounds of the two
algorithms and show that both the robust Q-learning and robust TDC algorithms
converge as fast as their vanilla counterparts(within a constant factor). Our
numerical experiments further demonstrate the robustness of our algorithms. Our
approach can be readily extended to robustify many other algorithms, e.g., TD,
SARSA, and other GTD algorithms.

    

### [[2109.14528] Sublinear Time and Space Algorithms for Correlation Clustering via Sparse-Dense Decompositions](http://arxiv.org/abs/2109.14528)


  We present a new approach for solving (minimum disagreement) correlation
clustering that results in sublinear algorithms with highly efficient time and
space complexity for this problem. In particular, we obtain the following
algorithms for $n$-vertex $(+/-)$-labeled graphs $G$:
-- A sublinear-time algorithm that with high probability returns a constant
approximation clustering of $G$ in $O(n\log^2{n})$ time assuming access to the
adjacency list of the $(+)$-labeled edges of $G$ (this is almost quadratically
faster than even reading the input once). Previously, no sublinear-time
algorithm was known for this problem with any multiplicative approximation
guarantee.
-- A semi-streaming algorithm that with high probability returns a constant
approximation clustering of $G$ in $O(n\log{n})$ space and a single pass over
the edges of the graph $G$ (this memory is almost quadratically smaller than
input size). Previously, no single-pass algorithm with $o(n^2)$ space was known
for this problem with any approximation guarantee.
The main ingredient of our approach is a novel connection to sparse-dense
graph decompositions that are used extensively in the graph coloring
literature. To our knowledge, this connection is the first application of these
decompositions beyond graph coloring, and in particular for the correlation
clustering problem, and can be of independent interest.

    

### [[2109.14530] Deep Spatio-Temporal Wind Power Forecasting](http://arxiv.org/abs/2109.14530)


  Wind power forecasting has drawn increasing attention among researchers as
the consumption of renewable energy grows. In this paper, we develop a deep
learning approach based on encoder-decoder structure. Our model forecasts wind
power generated by a wind turbine using its spatial location relative to other
turbines and historical wind speed data. In this way, we effectively integrate
spatial dependency and temporal trends to make turbine-specific predictions.
The advantages of our method over existing work can be summarized as 1) it
directly predicts wind power based on historical wind speed, without the need
for prediction of wind speed first, and then using a transformation; 2) it can
effectively capture long-term dependency 3) our model is more scalable and
efficient compared with other deep learning based methods. We demonstrate the
efficacy of our model on the benchmarks real-world datasets.

    

### [[2109.14536] PINNup: Robust neural network wavefield solutions using frequency upscaling and neuron splitting](http://arxiv.org/abs/2109.14536)


  Solving for the frequency-domain scattered wavefield via physics-informed
neural network (PINN) has great potential in seismic modeling and inversion.
However, when dealing with high-frequency wavefields, its accuracy and training
cost limits its applications. Thus, we propose a novel implementation of PINN
using frequency upscaling and neuron splitting, which allows the neural network
model to grow in size as we increase the frequency while leveraging the
information from the pre-trained model for lower-frequency wavefields,
resulting in fast convergence to high-accuracy solutions. Numerical results
show that, compared to the commonly used PINN with random initialization, the
proposed PINN exhibits notable superiority in terms of convergence and accuracy
and can achieve neuron based high-frequency wavefield solutions with a
two-hidden-layer model.

    

### [[2109.14545] A Comprehensive Survey and Performance Analysis of Activation Functions in Deep Learning](http://arxiv.org/abs/2109.14545)


  Neural networks have shown tremendous growth in recent years to solve
numerous problems. Various types of neural networks have been introduced to
deal with different types of problems. However, the main goal of any neural
network is to transform the non-linearly separable input data into more
linearly separable abstract features using a hierarchy of layers. These layers
are combinations of linear and nonlinear functions. The most popular and common
non-linearity layers are activation functions (AFs), such as Logistic Sigmoid,
Tanh, ReLU, ELU, Swish and Mish. In this paper, a comprehensive overview and
survey is presented for AFs in neural networks for deep learning. Different
classes of AFs such as Logistic Sigmoid and Tanh based, ReLU based, ELU based,
and Learning based are covered. Several characteristics of AFs such as output
range, monotonicity, and smoothness are also pointed out. A performance
comparison is also performed among 18 state-of-the-art AFs with different
networks on different types of data. The insights of AFs are presented to
benefit the researchers for doing further research and practitioners to select
among different choices. The code used for experimental comparison is released
at: \url{this https URL}.

    

### [[2109.14549] Vision-Guided Quadrupedal Locomotion in the Wild with Multi-Modal Delay Randomization](http://arxiv.org/abs/2109.14549)


  Developing robust vision-guided controllers for quadrupedal robots in complex
environments, with various obstacles, dynamical surroundings and uneven
terrains, is very challenging. While Reinforcement Learning (RL) provides a
promising paradigm for agile locomotion skills with vision inputs in
simulation, it is still very challenging to deploy the RL policy in the real
world. Our key insight is that aside from the discrepancy in the domain gap, in
visual appearance between the simulation and the real world, the latency from
the control pipeline is also a major cause of difficulty. In this paper, we
propose Multi-Modal Delay Randomization (MMDR) to address this issue when
training RL agents. Specifically, we simulate the latency of real hardware by
using past observations, sampled with randomized periods, for both
proprioception and vision. We train the RL policy for end-to-end control in a
physical simulator without any predefined controller or reference motion, and
directly deploy it on the real A1 quadruped robot running in the wild. We
evaluate our method in different outdoor environments with complex terrains and
obstacles. We demonstrate the robot can smoothly maneuver at a high speed,
avoid the obstacles, and show significant improvement over the baselines. Our
project page with videos is at this https URL.

    

### [[2109.14563] Robust Temporal Ensembling for Learning with Noisy Labels](http://arxiv.org/abs/2109.14563)


  Successful training of deep neural networks with noisy labels is an essential
capability as most real-world datasets contain some amount of mislabeled data.
Left unmitigated, label noise can sharply degrade typical supervised learning
approaches. In this paper, we present robust temporal ensembling (RTE), which
combines robust loss with semi-supervised regularization methods to achieve
noise-robust learning. We demonstrate that RTE achieves state-of-the-art
performance across the CIFAR-10, CIFAR-100, ImageNet, WebVision, and Food-101N
datasets, while forgoing the recent trend of label filtering and/or fixing.
Finally, we show that RTE also retains competitive corruption robustness to
unforeseen input noise using CIFAR-10-C, obtaining a mean corruption error
(mCE) of 13.50% even in the presence of an 80% noise ratio, versus 26.9% mCE
with standard methods on clean data.

    

### [[2109.14567] Implicit Generative Copulas](http://arxiv.org/abs/2109.14567)


  Copulas are a powerful tool for modeling multivariate distributions as they
allow to separately estimate the univariate marginal distributions and the
joint dependency structure. However, known parametric copulas offer limited
flexibility especially in high dimensions, while commonly used non-parametric
methods suffer from the curse of dimensionality. A popular remedy is to
construct a tree-based hierarchy of conditional bivariate copulas. In this
paper, we propose a flexible, yet conceptually simple alternative based on
implicit generative neural networks. The key challenge is to ensure marginal
uniformity of the estimated copula distribution. We achieve this by learning a
multivariate latent distribution with unspecified marginals but the desired
dependency structure. By applying the probability integral transform, we can
then obtain samples from the high-dimensional copula distribution without
relying on parametric assumptions or the need to find a suitable tree
structure. Experiments on synthetic and real data from finance, physics, and
image generation demonstrate the performance of this approach.

    

### [[2109.14569] Partitioning Cloud-based Microservices (via Deep Learning)](http://arxiv.org/abs/2109.14569)


  Cloud-based software has many advantages. When services are divided into many
independent components, they are easier to update. Also, during peak demand, it
is easier to scale cloud services (just hire more CPUs). Hence, many
organizations are partitioning their monolithic enterprise applications into
cloud-based microservices.
Recently there has been much work using machine learning to simplify this
partitioning task. Despite much research, no single partitioning method can be
recommended as generally useful. More specifically, those prior solutions are
"brittle''; i.e. if they work well for one kind of goal in one dataset, then
they can be sub-optimal if applied to many datasets and multiple goals.
In order to find a generally useful partitioning method, we propose DEEPLY.
This new algorithm extends the CO-GCN deep learning partition generator with
(a) a novel loss function and (b) some hyper-parameter optimization. As shown
by our experiments, DEEPLY generally outperforms prior work (including CO-GCN,
and others) across multiple datasets and goals. To the best of our knowledge,
this is the first report in SE of such stable hyper-parameter optimization.
To aid reuse of this work, DEEPLY is available on-line at
this https URL.

    

### [[2109.14591] Combining Human Predictions with Model Probabilities via Confusion Matrices and Calibration](http://arxiv.org/abs/2109.14591)


  An increasingly common use case for machine learning models is augmenting the
abilities of human decision makers. For classification tasks where neither the
human or model are perfectly accurate, a key step in obtaining high performance
is combining their individual predictions in a manner that leverages their
relative strengths. In this work, we develop a set of algorithms that combine
the probabilistic output of a model with the class-level output of a human. We
show theoretically that the accuracy of our combination model is driven not
only by the individual human and model accuracies, but also by the model's
confidence. Empirical results on image classification with CIFAR-10 and a
subset of ImageNet demonstrate that such human-model combinations consistently
have higher accuracies than the model or human alone, and that the parameters
of the combination method can be estimated effectively with as few as ten
labeled datapoints.

    

### [[2109.14595] Generalization Bounds For Meta-Learning: An Information-Theoretic Analysis](http://arxiv.org/abs/2109.14595)


  We derive a novel information-theoretic analysis of the generalization
property of meta-learning algorithms. Concretely, our analysis proposes a
generic understanding of both the conventional learning-to-learn framework and
the modern model-agnostic meta-learning (MAML) algorithms. Moreover, we provide
a data-dependent generalization bound for a stochastic variant of MAML, which
is non-vacuous for deep few-shot learning. As compared to previous bounds that
depend on the square norm of gradients, empirical validations on both simulated
data and a well-known few-shot benchmark show that our bound is orders of
magnitude tighter in most situations.

    

### [[1902.01981] CodedReduce: A Fast and Robust Framework for Gradient Aggregation in Distributed Learning](http://arxiv.org/abs/1902.01981)


  We focus on the commonly used synchronous Gradient Descent paradigm for
large-scale distributed learning, for which there has been a growing interest
to develop efficient and robust gradient aggregation strategies that overcome
two key system bottlenecks: communication bandwidth and stragglers' delays. In
particular, Ring-AllReduce (RAR) design has been proposed to avoid bandwidth
bottleneck at any particular node by allowing each worker to only communicate
with its neighbors that are arranged in a logical ring. On the other hand,
Gradient Coding (GC) has been recently proposed to mitigate stragglers in a
master-worker topology by allowing carefully designed redundant allocation of
the data set to the workers. We propose a joint communication topology design
and data set allocation strategy, named CodedReduce (CR), that combines the
best of both RAR and GC. That is, it parallelizes the communications over a
tree topology leading to efficient bandwidth utilization, and carefully designs
a redundant data set allocation and coding strategy at the nodes to make the
proposed gradient aggregation scheme robust to stragglers. In particular, we
quantify the communication parallelization gain and resiliency of the proposed
CR scheme, and prove its optimality when the communication topology is a
regular tree. Moreover, we characterize the expected run-time of CR and show
order-wise speedups compared to the benchmark schemes. Finally, we empirically
evaluate the performance of our proposed CR design over Amazon EC2 and
demonstrate that it achieves speedups of up to 27.2x and 7.0x, respectively
over the benchmarks GC and RAR.

    

### [[1906.10228] A Theoretical Connection Between Statistical Physics and Reinforcement Learning](http://arxiv.org/abs/1906.10228)


  Sequential decision making in the presence of uncertainty and stochastic
dynamics gives rise to distributions over state/action trajectories in
reinforcement learning (RL) and optimal control problems. This observation has
led to a variety of connections between RL and inference in probabilistic
graphical models (PGMs). Here we explore a different dimension to this
relationship, examining reinforcement learning using the tools and abstractions
of statistical physics. The central object in the statistical physics
abstraction is the idea of a partition function $\mathcal{Z}$, and here we
construct a partition function from the ensemble of possible trajectories that
an agent might take in a Markov decision process. Although value functions and
$Q$-functions can be derived from this partition function and interpreted via
average energies, the $\mathcal{Z}$-function provides an object with its own
Bellman equation that can form the basis of alternative dynamic programming
approaches. Moreover, when the MDP dynamics are deterministic, the Bellman
equation for $\mathcal{Z}$ is linear, allowing direct solutions that are
unavailable for the nonlinear equations associated with traditional value
functions. The policies learned via these $\mathcal{Z}$-based Bellman updates
are tightly linked to Boltzmann-like policy parameterizations. In addition to
sampling actions proportionally to the exponential of the expected cumulative
reward as Boltzmann policies would, these policies take entropy into account
favoring states from which many outcomes are possible.

    

### [[1910.13141] Decomposable-Net: Scalable Low-Rank Compression for Neural Networks](http://arxiv.org/abs/1910.13141)


  Compressing DNNs is important for the real-world applications operating on
resource-constrained devices. However, we typically observe drastic performance
deterioration when changing model size after training is completed. Therefore,
retraining is required to resume the performance of the compressed models
suitable for different devices. In this paper, we propose Decomposable-Net (the
network decomposable in any size), which allows flexible changes to model size
without retraining. We decompose weight matrices in the DNNs via singular value
decomposition and adjust ranks according to the target model size. Unlike the
existing low-rank compression methods that specialize the model to a fixed
size, we propose a novel backpropagation scheme that jointly minimizes losses
for both of full- and low-rank networks. This enables not only to maintain the
performance of a full-rank network {\it without retraining} but also to improve
low-rank networks in multiple sizes. Additionally, we introduce a simple
criterion for rank selection that effectively suppresses approximation error.
In experiments on the ImageNet classification task, Decomposable-Net yields
superior accuracy in a wide range of model sizes. In particular,
Decomposable-Net achieves the top-1 accuracy of $73.2\%$ with $0.27\times$MACs
with ResNet-50, compared to Tucker decomposition ($67.4\% / 0.30\times$),
Trained Rank Pruning ($70.6\% / 0.28\times$), and universally slimmable
networks ($71.4\% / 0.26\times$).

    

### [[2003.01497] A Permutation-Equivariant Neural Network Architecture For Auction Design](http://arxiv.org/abs/2003.01497)


  Designing an incentive compatible auction that maximizes expected revenue is
a central problem in Auction Design. Theoretical approaches to the problem have
hit some limits in the past decades and analytical solutions are known for only
a few simple settings. Computational approaches to the problem through the use
of LPs have their own set of limitations. Building on the success of deep
learning, a new approach was recently proposed by Duetting et al. (2019) in
which the auction is modeled by a feed-forward neural network and the design
problem is framed as a learning problem. The neural architectures used in that
work are general purpose and do not take advantage of any of the symmetries the
problem could present, such as permutation equivariance. In this work, we
consider auction design problems that have permutation-equivariant symmetry and
construct a neural architecture that is capable of perfectly recovering the
permutation-equivariant optimal mechanism, which we show is not possible with
the previous architecture. We demonstrate that permutation-equivariant
architectures are not only capable of recovering previous results, they also
have better generalization properties.

    

### [[2004.02932] Beyond Background-Aware Correlation Filters: Adaptive Context Modeling by Hand-Crafted and Deep RGB Features for Visual Tracking](http://arxiv.org/abs/2004.02932)


  In recent years, the background-aware correlation filters have achie-ved a
lot of research interest in the visual target tracking. However, these methods
cannot suitably model the target appearance due to the exploitation of
hand-crafted features. On the other hand, the recent deep learning-based visual
tracking methods have provided a competitive performance along with extensive
computations. In this paper, an adaptive background-aware correlation
filter-based tracker is proposed that effectively models the target appearance
by using either the histogram of oriented gradients (HOG) or convolutional
neural network (CNN) feature maps. The proposed method exploits the fast 2D
non-maximum suppression (NMS) algorithm and the semantic information comparison
to detect challenging situations. When the HOG-based response map is not
reliable, or the context region has a low semantic similarity with prior
regions, the proposed method constructs the CNN context model to improve the
target region estimation. Furthermore, the rejection option allows the proposed
method to update the CNN context model only on valid regions. Comprehensive
experimental results demonstrate that the proposed adaptive method clearly
outperforms the accuracy and robustness of visual target tracking compared to
the state-of-the-art methods on the OTB-50, OTB-100, TC-128, UAV-123, and
VOT-2015 datasets.

    

### [[2006.00587] Towards Understanding Cooperative Multi-Agent Q-Learning with Value Factorization](http://arxiv.org/abs/2006.00587)


  Value factorization is a popular and promising approach to scaling up
multi-agent reinforcement learning in cooperative settings. However, the
theoretical understanding of such methods is limited. In this paper, we
formalize a multi-agent fitted Q-iteration framework for analyzing factorized
multi-agent Q-learning. Based on this framework, we investigate linear value
factorization and reveal that multi-agent Q-learning with this simple
decomposition implicitly realizes a powerful counterfactual credit assignment,
but may not converge in some settings. Through further analysis, we find that
on-policy training or richer joint value function classes can improve its local
or global convergence properties, respectively. Finally, to support and extend
our theoretical implications to practical realization, we conduct an empirical
analysis of state-of-the-art deep multi-agent Q-learning algorithms on didactic
examples and a broad set of StarCraft II unit micromanagement tasks.

    

### [[2006.05684] Auction learning as a two-player game](http://arxiv.org/abs/2006.05684)


  Designing an incentive compatible auction that maximizes expected revenue is
a central problem in Auction Design. While theoretical approaches to the
problem have hit some limits, a recent research direction initiated by Duetting
et al. (2019) consists in building neural network architectures to find optimal
auctions. We propose two conceptual deviations from their approach which result
in enhanced performance. First, we use recent results in theoretical auction
design (Rubinstein and Weinberg, 2018) to introduce a time-independent
Lagrangian. This not only circumvents the need for an expensive hyper-parameter
search (as in prior work), but also provides a principled metric to compare the
performance of two auctions (absent from prior work). Second, the optimization
procedure in previous work uses an inner maximization loop to compute optimal
misreports. We amortize this process through the introduction of an additional
neural network. We demonstrate the effectiveness of our approach by learning
competitive or strictly improved auctions compared to prior work. Both results
together further imply a novel formulation of Auction Design as a two-player
game with stationary utility functions.

    

### [[2006.05805] Distribution Regression for Sequential Data](http://arxiv.org/abs/2006.05805)


  Distribution regression refers to the supervised learning problem where
labels are only available for groups of inputs instead of individual inputs. In
this paper, we develop a rigorous mathematical framework for distribution
regression where inputs are complex data streams. Leveraging properties of the
expected signature and a recent signature kernel trick for sequential data from
stochastic analysis, we introduce two new learning techniques, one
feature-based and the other kernel-based. Each is suited to a different data
regime in terms of the number of data streams and the dimensionality of the
individual streams. We provide theoretical results on the universality of both
approaches and demonstrate empirically their robustness to irregularly sampled
multivariate time-series, achieving state-of-the-art performance on both
synthetic and real-world examples from thermodynamics, mathematical finance and
agricultural science.

    

### [[2006.07691] Synthetic Interventions](http://arxiv.org/abs/2006.07691)


  Consider a setting where there are $N$ heterogeneous units (e.g.,
individuals, sub-populations) and $D$ interventions (e.g., socio-economic
policies). Our goal is to learn the potential outcome associated with every
intervention on every unit (i.e., $N \times D$ causal parameters). Towards
this, we present a causal framework, synthetic interventions (SI), to infer
these $N \times D$ causal parameters while only observing each of the $N$ units
under at most two interventions, independent of $D$. This can be significant as
the number of interventions, i.e, level of personalization, grows. Importantly,
our estimator also allows for latent confounders that determine how
interventions are assigned. Theoretically, under a novel tensor factor model
across units, measurements, and interventions, we formally establish an
identification result for each of these $N \times D$ causal parameters and
establish finite-sample consistency and asymptotic normality of our estimator.
The estimator is furnished with a data-driven test to verify its suitability.
Empirically, we validate our framework through both experimental and
observational case studies; namely, a large-scale A/B test performed on an
e-commerce platform, and an evaluation of mobility restriction on morbidity
outcomes due to COVID-19. We believe this has important implications for
program evaluation and the design of data-efficient RCTs with heterogeneous
units and multiple interventions.

    

### [[2006.14444] Clustering with Tangles: Algorithmic Framework and Theoretical Guarantees](http://arxiv.org/abs/2006.14444)


  Originally, tangles had been invented as an abstract tool in mathematical
graph theory to prove the famous graph minor theorem. In this paper we showcase
the practical potential of tangles in machine learning applications. Given a
collection of weak cuts (bipartitions) of any dataset, tangles provide a
mechanism to aggregate these cuts such that they point in the direction of a
dense structure. As a result, a cluster is softly characterized by a set of
consistent pointers. This highly flexible approach can solve soft clustering
problems in a variety of setups, ranging from questionnaires over community
detection in graphs to clustering points in metric spaces. The output of our
proposed framework is of a hierarchical nature and induces the notion of a soft
dendrogram, which can be helpful to explore the cluster structure of a dataset.
The computational complexity of aggregating the cuts is only linear in the
number of data points and places the bottleneck on generating the cuts, for
which simple and fast algorithms form a sufficient basis. The contribution of
this paper is to translate the abstract mathematical literature into algorithms
that can be used in machine learning. We demonstrate the power of tangles in
many different use cases and provide theoretical guarantees for their
performance.

    

### [[2008.03006] Polynomial-time algorithms for Multimarginal Optimal Transport problems with structure](http://arxiv.org/abs/2008.03006)


  Multimarginal Optimal Transport (MOT) has attracted significant interest due
to applications in machine learning, statistics, and the sciences. However, in
most applications, the success of MOT is severely limited by a lack of
efficient algorithms. Indeed, MOT in general requires exponential time in the
number of marginals k and their support sizes n. This paper develops a general
theory about what "structure" makes MOT solvable in poly(n,k) time.
We develop a unified algorithmic framework for solving MOT in poly(n,k) time
by characterizing the "structure" that different algorithms require in terms of
simple variants of the dual feasibility oracle. This framework has several
benefits. First, it enables us to show that the Sinkhorn algorithm, which is
currently the most popular MOT algorithm, requires strictly more structure than
other algorithms do to solve MOT in poly(n,k) time. Second, our framework makes
it much simpler to develop poly(n,k) time algorithms for a given MOT problem.
In particular, it is necessary and sufficient to (approximately) solve the dual
feasibility oracle -- which is much more amenable to standard algorithmic
techniques.
We illustrate this ease-of-use by developing poly(n,k) time algorithms for
three general classes of MOT cost structures: (1) graphical structure; (2)
set-optimization structure; and (3) low-rank plus sparse structure. For
structure (1), we recover the known result that Sinkhorn has poly(n,k) runtime;
moreover, we provide the first poly(n,k) time algorithms for computing
solutions that are exact and sparse. For structures (2)-(3), we give the first
poly(n,k) time algorithms, even for approximate computation. Together, these
three structures encompass many -- if not most -- current applications of MOT.

    

### [[2008.10066] Learning Off-Policy with Online Planning](http://arxiv.org/abs/2008.10066)


  Reinforcement learning (RL) in low-data and risk-sensitive domains requires
performant and flexible deployment policies that can readily incorporate
constraints during deployment. One such class of policies are the
semi-parametric H-step lookahead policies, which select actions using
trajectory optimization over a dynamics model for a fixed horizon with a
terminal value function. In this work, we investigate a novel instantiation of
H-step lookahead with a learned model and a terminal value function learned by
a model-free off-policy algorithm, named Learning Off-Policy with Online
Planning (LOOP). We provide a theoretical analysis of this method, suggesting a
tradeoff between model errors and value function errors and empirically
demonstrate this tradeoff to be beneficial in deep reinforcement learning.
Furthermore, we identify the "Actor Divergence" issue in this framework and
propose Actor Regularized Control (ARC), a modified trajectory optimization
procedure. We evaluate our method on a set of robotic tasks for Offline and
Online RL and demonstrate improved performance. We also show the flexibility of
LOOP to incorporate safety constraints during deployment with a set of
navigation environments. We demonstrate that LOOP is a desirable framework for
robotics applications based on its strong performance in various important RL
settings. Project video and details can be found at
$\href{this https URL}{\text{this http URL}}$.

    

### [[2010.01652] FORK: A Forward-Looking Actor For Model-Free Reinforcement Learning](http://arxiv.org/abs/2010.01652)


  In this paper, we propose a new type of Actor, named forward-looking Actor or
FORK for short, for Actor-Critic algorithms. FORK can be easily integrated into
a model-free Actor-Critic algorithm. Our experiments on six Box2D and MuJoCo
environments with continuous state and action spaces demonstrate significant
performance improvement FORK can bring to the state-of-the-art algorithms. A
variation of FORK can further solve Bipedal-WalkerHardcore in as few as four
hours using a single GPU.

    

### [[2011.00440] Learning When to Switch: Composing Controllers to Traverse a Sequence of Terrain Artifacts](http://arxiv.org/abs/2011.00440)


  Legged robots often use separate control policiesthat are highly engineered
for traversing difficult terrain suchas stairs, gaps, and steps, where
switching between policies isonly possible when the robot is in a region that
is commonto adjacent controllers. Deep Reinforcement Learning (DRL)is a
promising alternative to hand-crafted control design,though typically requires
the full set of test conditions to beknown before training. DRL policies can
result in complex(often unrealistic) behaviours that have few or no
overlappingregions between adjacent policies, making it difficult to
switchbehaviours. In this work we develop multiple DRL policieswith Curriculum
Learning (CL), each that can traverse asingle respective terrain condition,
while ensuring an overlapbetween policies. We then train a network for each
destinationpolicy that estimates the likelihood of successfully switchingfrom
any other policy. We evaluate our switching methodon a previously unseen
combination of terrain artifacts andshow that it performs better than heuristic
methods. Whileour method is trained on individual terrain types, it
performscomparably to a Deep Q Network trained on the full set ofterrain
conditions. This approach allows the development ofseparate policies in
constrained conditions with embedded priorknowledge about each behaviour, that
is scalable to any numberof behaviours, and prepares DRL methods for
applications inthe real world

    

### [[2011.03904] Interpretable Locally Adaptive Nearest Neighbors](http://arxiv.org/abs/2011.03904)


  When training automated systems, it has been shown to be beneficial to adapt
the representation of data by learning a problem-specific metric. This metric
is global. We extend this idea and, for the widely used family of k nearest
neighbors algorithms, develop a method that allows learning locally adaptive
metrics. These local metrics not only improve performance but are naturally
interpretable. To demonstrate important aspects of how our approach works, we
conduct a number of experiments on synthetic data sets, and we show its
usefulness on real-world benchmark data sets.

    

### [[2011.05961] Real-Time Decentralized knowledge Transfer at the Edge](http://arxiv.org/abs/2011.05961)


  The proliferation of edge networks creates islands of learning agents working
on local streams of data. Transferring knowledge between these agents in
real-time without exposing private data allows for collaboration to decrease
learning time and increase model confidence. Incorporating knowledge from data
that a local model did not see creates an ability to debias a local model or
add to classification abilities on data never before seen. Transferring
knowledge in a selective decentralized approach enables models to retain their
local insights, allowing for local flavors of a machine learning model. This
approach suits the decentralized architecture of edge networks, as a local edge
node will serve a community of learning agents that will likely encounter
similar data. We propose a method based on knowledge distillation for pairwise
knowledge transfer pipelines from models trained on non-i.i.d. data and compare
it to other popular knowledge transfer methods. Additionally, we test different
scenarios of knowledge transfer network construction and show the practicality
of our approach. Our experiments show knowledge transfer using our model
outperforms standard methods in a real-time transfer scenario.

    

### [[2011.06146] Learning Models for Actionable Recourse](http://arxiv.org/abs/2011.06146)


  As machine learning models are increasingly deployed in high-stakes domains
such as legal and financial decision-making, there has been growing interest in
post-hoc methods for generating counterfactual explanations. Such explanations
provide individuals adversely impacted by predicted outcomes (e.g., an
applicant denied a loan) with recourse -- i.e., a description of how they can
change their features to obtain a positive outcome. We propose a novel
algorithm that leverages adversarial training and PAC confidence sets to learn
models that theoretically guarantee recourse to affected individuals with high
probability. We demonstrate the efficacy of our approach with extensive
experiments on real data.

    

### [[2011.11048] GNNLens: A Visual Analytics Approach for Prediction Error Diagnosis of Graph Neural Networks](http://arxiv.org/abs/2011.11048)


  Graph Neural Networks (GNNs) aim to extend deep learning techniques to graph
data and have achieved significant progress in graph analysis tasks (e.g., node
classification) in recent years. However, similar to other deep neural networks
like Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs),
GNNs behave like a black box with their details hidden from model developers
and users. It is therefore difficult to diagnose possible errors of GNNs.
Despite many visual analytics studies being done on CNNs and RNNs, little
research has addressed the challenges for GNNs. This paper fills the research
gap with an interactive visual analysis tool, GNNLens, to assist model
developers and users in understanding and analyzing GNNs. Specifically,
Parallel Sets View and Projection View enable users to quickly identify and
validate error patterns in the set of wrong predictions; Graph View and Feature
Matrix View offer a detailed analysis of individual nodes to assist users in
forming hypotheses about the error patterns. Since GNNs jointly model the graph
structure and the node features, we reveal the relative influences of the two
types of information by comparing the predictions of three models: GNN,
Multi-Layer Perceptron (MLP), and GNN Without Using Features (GNNWUF). Two case
studies and interviews with domain experts demonstrate the effectiveness of
GNNLens in facilitating the understanding of GNN models and their errors.

    

### [[2012.03115] Bayesian optimization assisted unsupervised learning for efficient intra-tumor partitioning in MRI and survival prediction for glioblastoma patients](http://arxiv.org/abs/2012.03115)


  Glioblastoma is profoundly heterogeneous in microstructure and vasculature,
which may lead to tumor regional diversity and distinct treatment response.
Although successful in tumor sub-region segmentation and survival prediction,
radiomics based on machine learning algorithms, is challenged by its
robustness, due to the vague intermediate process and track changes. Also, the
weak interpretability of the model poses challenges to clinical application.
Here we proposed a machine learning framework to semi-automatically fine-tune
the clustering algorithms and quantitatively identify stable sub-regions for
reliable clinical survival prediction. Hyper-parameters are automatically
determined by the global minimum of the trained Gaussian Process (GP) surrogate
model through Bayesian optimization(BO) to alleviate the difficulty of tuning
parameters for clinical researchers. To enhance the interpretability of the
survival prediction model, we incorporated the prior knowledge of intra-tumoral
heterogeneity, by segmenting tumor sub-regions and extracting sub-regional
features. The results demonstrated that the global minimum of the trained GP
surrogate can be used as sub-optimal hyper-parameter solutions for efficient.
The sub-regions segmented based on physiological MRI can be applied to predict
patient survival, which could enhance the clinical interpretability for the
machine learning model.

    

### [[2012.09394] Metrical Task Systems with Online Machine Learned Advice](http://arxiv.org/abs/2012.09394)


  Machine learning algorithms are designed to make accurate predictions of the
future based on existing data, while online algorithms seek to bound some
performance measure (typically the competitive ratio) without knowledge of the
future. Lykouris and Vassilvitskii demonstrated that augmenting online
algorithms with a machine learned predictor can provably decrease the
competitive ratio under as long as the predictor is suitably accurate.
In this work we apply this idea to the Online Metrical Task System problem,
which was put forth by Borodin, Linial, and Saks as a general model for dynamic
systems processing tasks in an online fashion. We focus on the specific class
of uniform task systems on $n$ tasks, for which the best deterministic
algorithm is $O(n)$ competitive and the best randomized algorithm is $O(\log
n)$ competitive.
By giving an online algorithms access to a machine learned oracle with
absolute predictive error bounded above by $\eta_0$, we construct a
$\Theta(\min(\sqrt{\eta_0}, \log n))$ competitive algorithm for the uniform
case of the metrical task systems problem. We also give a $\Theta(\log \eta_0)$
lower bound on the competitive ratio of any randomized algorithm.

    

### [[2012.13551] Intuitiveness in Active Teaching](http://arxiv.org/abs/2012.13551)


  While Machine learning gives rise to astonishing results in automated
systems, it is usually at the cost of large data requirements. This makes many
successful algorithms from machine learning unsuitable for human-machine
interaction, where the machine must learn from a small number of training
samples that can be provided by a user within a reasonable time frame.
Fortunately, the user can tailor the training data they create to be as useful
as possible, severely limiting its necessary size -- as long as they know about
the machine's requirements and limitations. Of course, acquiring this knowledge
can in turn be cumbersome and costly. This raises the question of how easy
machine learning algorithms are to interact with. In this work, we address this
issue by analyzing the intuitiveness of certain algorithms when they are
actively taught by users. After developing a theoretical framework of
intuitiveness as a property of algorithms, we introduce an active teaching
paradigm involving a prototypical two-dimensional spatial learning task as a
method to judge the efficacy of human-machine interactions. Finally, we present
and discuss the results of a large-scale user study into the performance and
teaching strategies of 800 users interacting with two prominent machine
learning algorithms in our system, providing first evidence for the role of
intuition as an important factor impacting human-machine interaction.

    

### [[2102.03291] baller2vec: A Multi-Entity Transformer For Multi-Agent Spatiotemporal Modeling](http://arxiv.org/abs/2102.03291)


  Multi-agent spatiotemporal modeling is a challenging task from both an
algorithmic design and computational complexity perspective. Recent work has
explored the efficacy of traditional deep sequential models in this domain, but
these architectures are slow and cumbersome to train, particularly as model
size increases. Further, prior attempts to model interactions between agents
across time have limitations, such as imposing an order on the agents, or
making assumptions about their relationships. In this paper, we introduce
baller2vec, a multi-entity generalization of the standard Transformer that can,
with minimal assumptions, simultaneously and efficiently integrate information
across entities and time. We test the effectiveness of baller2vec for
multi-agent spatiotemporal modeling by training it to perform two different
basketball-related tasks: (1) simultaneously modeling the trajectories of all
players on the court and (2) modeling the trajectory of the ball. Not only does
baller2vec learn to perform these tasks well (outperforming a graph recurrent
neural network with a similar number of parameters by a wide margin), it also
appears to "understand" the game of basketball, encoding idiosyncratic
qualities of players in its embeddings, and performing basketball-relevant
functions with its attention heads.

    

### [[2102.04062] An Update on a Progressively Expanded Database for Automated Lung Sound Analysis](http://arxiv.org/abs/2102.04062)


  Purpose: We previously established an open-access lung sound database,
HF_Lung_V1, and developed deep learning models for inhalation, exhalation,
continuous adventitious sound (CAS), and discontinuous adventitious sound (DAS)
detection. The amount of data used for training contributes to model accuracy.
Herein, we collected larger quantities of data to further improve model
performance. Moreover, the issues of noisy labels and sound overlapping were
explored. Methods: HF_Lung_V1 was expanded to HF_Lung_V2 with a 1.45x increase
in the number of audio files. Convolutional neural network-bidirectional gated
recurrent unit network models were trained separately using the HF_Lung_V1
(V1_Train) and HF_Lung_V2 (V2_Train) training sets and then tested using the
HF_Lung_V1 (V1_Test) and HF_Lung_V2 (V2_Test) test sets, respectively. Segment
and event detection performance was evaluated using the F1 scores. Label
quality was assessed. Moreover, the overlap ratios between inhalation,
exhalation, CAS, and DAS labels were computed. Results: The model trained using
V2_Train exhibited improved F1 scores in inhalation, exhalation, and CAS
detection on both V1_Test and V2_Test but not in DAS detection. Poor CAS
detection was attributed to the quality of CAS labels. DAS detection was
strongly influenced by the overlapping of DAS labels with inhalation and
exhalation labels. Conclusion: Collecting greater quantities of lung sound data
is vital for developing more accurate lung sound analysis models. To build real
ground-truth labels, the labels must be reworked; this process is ongoing.
Furthermore, a method for addressing the sound overlapping problem in DAS
detection must be formulated.

    

### [[2102.11628] FINE Samples for Learning with Noisy Labels](http://arxiv.org/abs/2102.11628)


  Modern deep neural networks (DNNs) become frail when the datasets contain
noisy (incorrect) class labels. Robust techniques in the presence of noisy
labels can be categorized into two folds: developing noise-robust functions or
using noise-cleansing methods by detecting the noisy data. Recently,
noise-cleansing methods have been considered as the most competitive
noisy-label learning algorithms. Despite their success, their noisy label
detectors are often based on heuristics more than a theory, requiring a robust
classifier to predict the noisy data with loss values. In this paper, we
propose a novel detector for filtering label noise. Unlike most existing
methods, we focus on each data's latent representation dynamics and measure the
alignment between the latent distribution and each representation using the
eigendecomposition of the data covariance matrix. Our framework, coined as
filtering noisy instances via their eigenvectors (FINE), provides a robust
detector with derivative-free simple methods having theoretical guarantees.
Under our framework, we propose three applications of the FINE:
sample-selection approach, semi-supervised learning approach, and collaboration
with noise-robust loss functions. Experimental results show that the proposed
methods consistently outperform corresponding baselines for all three
applications on various benchmark datasets.

    

### [[2103.00328] Terahertz-Band Joint Ultra-Massive MIMO Radar-Communications: Model-Based and Model-Free Hybrid Beamforming](http://arxiv.org/abs/2103.00328)


  Wireless communications and sensing at terahertz (THz) band are increasingly
investigated as promising short-range technologies because of the availability
of high operational bandwidth at THz. In order to address the extremely high
attenuation at THz, ultra-massive multiple-input multiple-output (MIMO) antenna
systems have been proposed for THz communications to compensate propagation
losses. However, the cost and power associated with fully digital beamformers
of these huge antenna arrays are prohibitive. In this paper, we develop
wideband hybrid beamformers based on both model-based and model-free techniques
for a new group-of-subarrays (GoSA) ultra-massive MIMO structure in low-THz
band. Further, driven by the recent developments to save the spectrum, we
propose beamformers for a joint ultra-massive MIMO radar-communications system,
wherein the base station serves multi-antenna user equipment (RX), and tracks
radar targets by generating multiple beams toward both RX and the targets. We
formulate the GoSA beamformer design as an optimization problem to provide a
trade-off between the unconstrained communications beamformers and the desired
radar beamformers. To mitigate the beam split effect at THz band arising from
frequency-independent analog beamformers, we propose a phase correction
technique to align the beams of multiple subcarriers toward a single physical
direction. To further decrease the ultra-massive MIMO computational complexity
and enhance robustness, we also implement deep learning solutions to the
proposed model-based hybrid beamformers. Numerical experiments demonstrate that
both techniques outperform the conventional approaches in terms of spectral
efficiency and radar beampatterns, as well as exhibiting less hardware cost and
computation time.

    

### [[2103.00686] Accelerating Recommendation System Training by Leveraging Popular Choices](http://arxiv.org/abs/2103.00686)


  Recommender models are commonly used to suggest relevant items to a user for
e-commerce and online advertisement-based applications. These models use
massive embedding tables to store numerical representation of items' and users'
categorical variables (memory intensive) and employ neural networks (compute
intensive) to generate final recommendations. Training these large-scale
recommendation models is evolving to require increasing data and compute
resources. The highly parallel neural networks portion of these models can
benefit from GPU acceleration however, large embedding tables often cannot fit
in the limited-capacity GPU device memory. Hence, this paper deep dives into
the semantics of training data and obtains insights about the feature access,
transfer, and usage patterns of these models. We observe that, due to the
popularity of certain inputs, the accesses to the embeddings are highly skewed
with a few embedding entries being accessed up to 10000x more. This paper
leverages this asymmetrical access pattern to offer a framework, called FAE,
and proposes a hot-embedding aware data layout for training recommender models.
This layout utilizes the scarce GPU memory for storing the highly accessed
embeddings, thus reduces the data transfers from CPU to GPU. At the same time,
FAE engages the GPU to accelerate the executions of these hot embedding
entries. Experiments on production-scale recommendation models with real
datasets show that FAE reduces the overall training time by 2.3x and 1.52x in
comparison to XDL CPU-only and XDL CPU-GPU execution while maintaining baseline
accuracy

    

### [[2103.10251] Optimal Targeting in Fundraising: A Causal Machine-Learning Approach](http://arxiv.org/abs/2103.10251)


  Ineffective fundraising lowers the resources charities can use to provide
goods. We combine a field experiment and a causal machine-learning approach to
increase a charity's fundraising effectiveness. The approach optimally targets
a fundraising instrument to individuals whose expected donations exceed
solicitation costs. Our results demonstrate that machine-learning-based optimal
targeting allows the charity to substantially increase donations net of
fundraising costs relative to uniform benchmarks in which either everybody or
no one receives the gift. To that end, it (a) should direct its fundraising
efforts to a subset of past donors and (b) never address individuals who were
previously asked but never donated. Further, we show that the benefits of
machine-learning-based optimal targeting even materialize when the charity only
exploits publicly available geospatial information or applies the estimated
optimal targeting rule to later fundraising campaigns conducted in similar
samples. We conclude that charities not engaging in optimal targeting waste
significant resources.

    

### [[2103.10504] UNETR: Transformers for 3D Medical Image Segmentation](http://arxiv.org/abs/2103.10504)


  Fully Convolutional Neural Networks (FCNNs) with contracting and expanding
paths have shown prominence for the majority of medical image segmentation
applications since the past decade. In FCNNs, the encoder plays an integral
role by learning both global and local features and contextual representations
which can be utilized for semantic output prediction by the decoder. Despite
their success, the locality of convolutional layers in FCNNs, limits the
capability of learning long-range spatial dependencies. Inspired by the recent
success of transformers for Natural Language Processing (NLP) in long-range
sequence learning, we reformulate the task of volumetric (3D) medical image
segmentation as a sequence-to-sequence prediction problem. We introduce a novel
architecture, dubbed as UNEt TRansformers (UNETR), that utilizes a transformer
as the encoder to learn sequence representations of the input volume and
effectively capture the global multi-scale information, while also following
the successful "U-shaped" network design for the encoder and decoder. The
transformer encoder is directly connected to a decoder via skip connections at
different resolutions to compute the final semantic segmentation output. We
have validated the performance of our method on the Multi Atlas Labeling Beyond
The Cranial Vault (BTCV) dataset for multi-organ segmentation and the Medical
Segmentation Decathlon (MSD) dataset for brain tumor and spleen segmentation
tasks. Our benchmarks demonstrate new state-of-the-art performance on the BTCV
leaderboard. Code: this https URL


### [[2103.13521] Conditions and Assumptions for Constraint-based Causal Structure Learning](http://arxiv.org/abs/2103.13521)


  This paper formalizes constraint-based structure learning of the "true"
causal graph from observed data when unobserved variables are also existent. We
provide conditions for a "natural" family of constraint-based
structure-learning algorithms that output graphs that are Markov equivalent to
the causal graph. Under the faithfulness assumption, this natural family
contains all exact structure-learning algorithms. More importantly, we provide
clear and testable assumptions, as an alternative to faithfulness, under which
any natural structure-learning algorithm outputs Markov equivalent graphs to
the causal graph. We provide these definitions and results for the general
class of models under the assumption that the distribution is Markovian to the
true causal graph, and we specialize the definitions and results for structural
causal models.

    

### [[2104.02317] A hybrid ensemble method with negative correlation learning for regression](http://arxiv.org/abs/2104.02317)


  Hybrid ensemble, an essential branch of ensembles, has flourished in numerous
machine learning problems, especially regression. Several studies have
confirmed the importance of diversity; however, previous ensembles only
consider diversity in the sub-model training stage, with limited improvement
compared to single models. In contrast, this study focuses on the sub-model
combination stage of the ensemble. It solves a non-convex optimization problem
using an interior-point filtering linear-search algorithm to select and weight
sub-models from a heterogeneous model pool automatically. This optimization
problem innovatively incorporates negative correlation learning as a penalty
term. Thus, a diverse model subset can be selected. Experimental results show
that the approach outperforms single model and overcomes the instability of the
models and parameters. Compared to bagging and stacking without model
diversity, our method stands out more and confirms the importance of diversity
in the ensemble. Additionally, the performance of our proposed method is better
than that of simple and weighted averages, and the variance of the weights is
lower and more stable than that of a linear model. Finally, the prediction
accuracy can be further improved by fine-tuning the weights using the error
inverse weights. In conclusion, the value of this study lies in its ease of use
and effectiveness, allowing the hybrid ensemble to embrace both diversity and
accuracy.

    

### [[2104.02836] Non-Asymptotic Analysis for Two Time-scale TDC with General Smooth Function Approximation](http://arxiv.org/abs/2104.02836)


  Temporal-difference learning with gradient correction (TDC) is a two
time-scale algorithm for policy evaluation in reinforcement learning. This
algorithm was initially proposed with linear function approximation, and was
later extended to the one with general smooth function approximation. The
asymptotic convergence for the on-policy setting with general smooth function
approximation was established in [bhatnagar2009convergent], however, the
finite-sample analysis remains unsolved due to challenges in the non-linear and
two-time-scale update structure, non-convex objective function and the
time-varying projection onto a tangent plane. In this paper, we develop novel
techniques to explicitly characterize the finite-sample error bound for the
general off-policy setting with i.i.d.\ or Markovian samples, and show that it
converges as fast as $\mathcal O(1/\sqrt T)$ (up to a factor of $\mathcal
O(\log T)$). Our approach can be applied to a wide range of value-based
reinforcement learning algorithms with general smooth function approximation.

    

### [[2104.02929] Minimax Kernel Machine Learning for a Class of Doubly Robust Functionals with Application to Proximal Causal Inference](http://arxiv.org/abs/2104.02929)


  A moment function is called doubly robust if it is comprised of two nuisance
functions and has the desired property that the estimator based on it is a
consistent estimator of the target parameter even if one of the nuisance
functions is misspecified. A common approach for obtaining such a moment
function is based on using the influence function (IF) of the parameter of
interest. Robins et al. (2008) introduced a large class of doubly robust IFs.
However, that class does not include the IF of functionals for which the
nuisance functions are solutions to integral equations. Such functionals are
particularly important in the field of causal inference, specifically in the
recently proposed proximal inference framework of (Miao et al., 2018; Tchetgen
Tchetgen et al., 2020), which allows for estimating the average causal effect
when unobserved confounders are present in the system. Motivated by the
proximal inference framework, in this paper, we first extend the class of
Robins et al. to include doubly robust IFs in which the nuisance functions are
solutions to integral equations. Then we demonstrate that the double robustness
property of these IFs can be leveraged to construct estimating equations for
the nuisance functions, which enables us to solve the integral equations
without resorting to parametric models. The main idea is to choose each
nuisance function such that it minimizes the dependency of the expected value
of the moment function to the other nuisance function. We frame this idea as a
minimax optimization problem and use RKHSes as the function spaces. We provide
convergence rates for the nuisance functions and conditions required for
asymptotic linearity of the estimator of the functional of interest. The
experiment results demonstrate that our proposed methodology leads to robust
and high-performance estimators for average causal effect in the proximal
inference framework.

    

### [[2104.11980] baller2vec++: A Look-Ahead Multi-Entity Transformer For Modeling Coordinated Agents](http://arxiv.org/abs/2104.11980)


  In many multi-agent spatiotemporal systems, agents operate under the
influence of shared, unobserved variables (e.g., the play a team is executing
in a game of basketball). As a result, the trajectories of the agents are often
statistically dependent at any given time step; however, almost universally,
multi-agent models implicitly assume the agents' trajectories are statistically
independent at each time step. In this paper, we introduce baller2vec++, a
multi-entity Transformer that can effectively model coordinated agents.
Specifically, baller2vec++ applies a specially designed self-attention mask to
a mixture of location and "look-ahead" trajectory sequences to learn the
distributions of statistically dependent agent trajectories. We show that,
unlike baller2vec (baller2vec++'s predecessor), baller2vec++ can learn to
emulate the behavior of perfectly coordinated agents in a simulated toy
dataset. Additionally, when modeling the trajectories of professional
basketball players, baller2vec++ outperforms baller2vec by a wide margin.

    

### [[2105.02522] Graphical modelling in continuous-time: consistency guarantees and algorithms using Neural ODEs](http://arxiv.org/abs/2105.02522)


  The discovery of structure from time series data is a key problem in fields
of study working with complex systems. Most identifiability results and
learning algorithms assume the underlying dynamics to be discrete in time.
Comparatively few, in contrast, explicitly define dependencies in infinitesimal
intervals of time, independently of the scale of observation and of the
regularity of sampling. In this paper, we consider score-based graph learning
for the study of dynamical systems. We prove that for vector fields
parameterized in a large class of neural networks, least squares optimization
with adaptive regularization schemes consistently recovers directed graphs of
local independencies in systems of stochastic differential equations. Using
this insight, we propose a score-based learning algorithm based on penalized
Neural Ordinary Differential Equations (modelling the mean process) that we
show to be applicable to the general setting of irregularly-sampled
multivariate time series and to outperform the state of the art across a range
of dynamical systems.

    

### [[2105.04211] SigGPDE: Scaling Sparse Gaussian Processes on Sequential Data](http://arxiv.org/abs/2105.04211)


  Making predictions and quantifying their uncertainty when the input data is
sequential is a fundamental learning challenge, recently attracting increasing
attention. We develop SigGPDE, a new scalable sparse variational inference
framework for Gaussian Processes (GPs) on sequential data. Our contribution is
twofold. First, we construct inducing variables underpinning the sparse
approximation so that the resulting evidence lower bound (ELBO) does not
require any matrix inversion. Second, we show that the gradients of the GP
signature kernel are solutions of a hyperbolic partial differential equation
(PDE). This theoretical insight allows us to build an efficient
back-propagation algorithm to optimize the ELBO. We showcase the significant
computational gains of SigGPDE compared to existing methods, while achieving
state-of-the-art performance for classification tasks on large datasets of up
to 1 million multivariate time series.

    

### [[2105.07342] Self-supervised Learning on Graphs: Contrastive, Generative,or Predictive](http://arxiv.org/abs/2105.07342)


  Deep learning on graphs has recently achieved remarkable success on a variety
of tasks, while such success relies heavily on the massive and carefully
labeled data. However, precise annotations are generally very expensive and
time-consuming. To address this problem, self-supervised learning (SSL) is
emerging as a new paradigm for extracting informative knowledge through
well-designed pretext tasks without relying on manual labels. In this survey,
we extend the concept of SSL, which first emerged in the fields of computer
vision and natural language processing, to present a timely and comprehensive
review of existing SSL techniques for graph data. Specifically, we divide
existing graph SSL methods into three categories: contrastive, generative, and
predictive. More importantly, unlike other surveys that only provide a
high-level description of published research, we present an additional
mathematical summary of existing works in a unified framework. Furthermore, to
facilitate methodological development and empirical comparisons, we also
summarize the commonly used datasets, evaluation metrics, downstream tasks,
open-source implementations, and experimental study of various algorithms.
Finally, we discuss the technical challenges and potential future directions
for improving graph self-supervised learning. Latest advances in graph SSL are
summarized in a GitHub repository
this https URL.

    

### [[2105.08120] Itsy Bitsy SpiderNet: Fully Connected Residual Network for Fraud Detection](http://arxiv.org/abs/2105.08120)


  With the development of high technology, the scope of fraud is increasing,
resulting in annual losses of billions of dollars worldwide. The preventive
protection measures become obsolete and vulnerable over time, so effective
detective tools are needed. In this paper, we propose a convolutional neural
network architecture SpiderNet designed to solve fraud detection problems. We
noticed that the principles of pooling and convolutional layers in neural
networks are very similar to the way antifraud analysts work when conducting
investigations. Moreover, the skip-connections used in neural networks make the
usage of features of various power in antifraud models possible. Our
experiments have shown that SpiderNet provides better quality compared to
Random Forest and adapted for antifraud modeling problems 1D-CNN, 1D-DenseNet,
F-DenseNet neural networks. We also propose new approaches for fraud feature
engineering called B-tests and W-tests, which generalize the concepts of
Benford's Law for fraud anomalies detection. Our results showed that B-tests
and W-tests give a significant increase to the quality of our anti-fraud
models. The SpiderNet code is available at
this https URL


### [[2106.02097] A Consciousness-Inspired Planning Agent for Model-Based Reinforcement Learning](http://arxiv.org/abs/2106.02097)


  We present an end-to-end, model-based deep reinforcement learning agent which
dynamically attends to relevant parts of its state, in order to plan and to
generalize better out-of-distribution. The agent uses a bottleneck mechanism
over a set-based representation to force the number of entities to which the
agent attends at each planning step to be small. In experiments, we investigate
the bottleneck mechanism with several sets of customized environments featuring
different challenges. We consistently observe that the design allows the
planning agents to generalize their learned task-solving abilities in
compatible unseen environments by attending to the relevant objects, leading to
better out-of-distribution performance.

    

### [[2106.02104] Semi-Empirical Objective Functions for Neural MCMC Proposal Optimization](http://arxiv.org/abs/2106.02104)


  Current objective functions used for training neural MCMC proposal
distributions implicitly rely on architectural restrictions to yield sensible
optimization results, which hampers the development of highly expressive neural
MCMC proposal architectures. In this work, we introduce and demonstrate a
semi-empirical procedure for determining approximate objective functions
suitable for optimizing arbitrarily parameterized proposal distributions in
MCMC methods. Our proposed Ab Initio objective functions consist of the
weighted combination of functions following constraints on their global optima
and transformation invariances that we argue should be upheld by general
measures of MCMC efficiency for use in proposal optimization. Our experimental
results demonstrate that Ab Initio objective functions maintain favorable
performance and preferable optimization behavior compared to existing objective
functions for neural MCMC optimization. We find that Ab Initio objective
functions are sufficiently robust to enable the confident optimization of
neural proposal distributions parameterized by deep generative networks
extending beyond the regimes of traditional MCMC schemes

    

### [[2106.02126] A Closer Look at the Worst-case Behavior of Multi-armed Bandit Algorithms](http://arxiv.org/abs/2106.02126)


  One of the key drivers of complexity in the classical (stochastic)
multi-armed bandit (MAB) problem is the difference between mean rewards in the
top two arms, also known as the instance gap. The celebrated Upper Confidence
Bound (UCB) policy is among the simplest optimism-based MAB algorithms that
naturally adapts to this gap: for a horizon of play n, it achieves optimal
O(log n) regret in instances with "large" gaps, and a near-optimal O(\sqrt{n
log n}) minimax regret when the gap can be arbitrarily "small." This paper
provides new results on the arm-sampling behavior of UCB, leading to several
important insights. Among these, it is shown that arm-sampling rates under UCB
are asymptotically deterministic, regardless of the problem complexity. This
discovery facilitates new sharp asymptotics and a novel alternative proof for
the O(\sqrt{n log n}) minimax regret of UCB. Furthermore, the paper also
provides the first complete process-level characterization of the MAB problem
under UCB in the conventional diffusion scaling. Among other things, the
"small" gap worst-case lens adopted in this paper also reveals profound
distinctions between the behavior of UCB and Thompson Sampling, such as an
"incomplete learning" phenomenon characteristic of the latter.

    

### [[2106.03790] Multi-armed Bandit Requiring Monotone Arm Sequences](http://arxiv.org/abs/2106.03790)


  In many online learning or multi-armed bandit problems, the taken actions or
pulled arms are ordinal and required to be monotone over time. Examples include
dynamic pricing, in which the firms use markup pricing policies to please early
adopters and deter strategic waiting, and clinical trials, in which the dose
allocation usually follows the dose escalation principle to prevent dose
limiting toxicities. We consider the continuum-armed bandit problem when the
arm sequence is required to be monotone. We show that when the unknown
objective function is Lipschitz continuous, the regret is $O(T)$. When in
addition the objective function is unimodal or quasiconcave, the regret is
$\tilde O(T^{3/4})$ under the proposed algorithm, which is also shown to be the
optimal rate. This deviates from the optimal rate $\tilde O(T^{2/3})$ in the
continuous-armed bandit literature and demonstrates the cost to the learning
efficiency brought by the monotonicity requirement.

    

### [[2106.07540] Evaluating Various Tokenizers for Arabic Text Classification](http://arxiv.org/abs/2106.07540)


  The first step in any NLP pipeline is to split the text into individual
tokens. The most obvious and straightforward approach is to use words as
tokens. However, given a large text corpus, representing all the words is not
efficient in terms of vocabulary size. In the literature, many tokenization
algorithms have emerged to tackle this problem by creating subwords which in
turn limits the vocabulary size in a given text corpus. Most tokenization
techniques are language-agnostic i.e they don't incorporate the linguistic
features of a given language. Not to mention the difficulty of evaluating such
techniques in practice. In this paper, we introduce three new tokenization
algorithms for Arabic and compare them to three other baselines using
unsupervised evaluations. In addition to that, we compare all the six
algorithms by evaluating them on three supervised classification tasks which
are sentiment analysis, news classification and poetry classification using six
publicly available datasets. Our experiments show that none of the tokenization
technique is the best choice overall and that the performance of a given
tokenization algorithm depends on the size of the dataset, type of the task,
and the amount of morphology that exists in the dataset. However, some
tokenization techniques are better overall as compared to others on various
text classification tasks.

    

### [[2109.09367] Network Clustering by Embedding of Attribute-augmented Graphs](http://arxiv.org/abs/2109.09367)


  In this paper we propose a new approach to detect clusters in undirected
graphs with attributed vertices. The aim is to group vertices which are similar
not only in terms of structural connectivity but also in terms of attribute
values. We incorporate structural and attribute similarities between the
vertices in an augmented graph by creating additional vertices and edges as
proposed in [5, 27]. The augmented graph is embedded in a Euclidean space
associated to its Laplacian and apply a modified K-means algorithm to identify
clusters. The modified K-means uses a vector distance measure where to each
original vertex is assigned a vector-valued set of coordinates depending on
both structural connectivity and attribute similarities. To define the
coordinate vectors we employ an adaptive AMG (Algebraic MultiGrid) method to
identify the coordinate directions in the embedding Euclidean space extending
our previous result for graphs without attributes. We demonstrate the
effectiveness of our proposed clustering method on both synthetic and
real-world attributed graphs.

    

### [[2109.12894] Training Spiking Neural Networks Using Lessons From Deep Learning](http://arxiv.org/abs/2109.12894)


  The brain is the perfect place to look for inspiration to develop more
efficient neural networks. The inner workings of our synapses and neurons
provide a glimpse at what the future of deep learning might look like. This
paper shows how to apply the lessons learnt from several decades of research in
deep learning, gradient descent, backpropagation and neuroscience to
biologically plausible spiking neural neural networks. This paper explores the
delicate interplay between encoding data as spikes and the learning process;
the challenges and solutions of applying gradient-based learning to spiking
neural networks; the subtle link between temporal backpropagation and spike
timing dependent plasticity, and how deep learning might move towards
biologically plausible online learning. Some ideas are well accepted and
commonly used amongst the neuromorphic engineering community, while others are
presented or justified for the first time here.

    

### [[2109.12909] Compressive Visual Representations](http://arxiv.org/abs/2109.12909)


  Learning effective visual representations that generalize well without human
supervision is a fundamental problem in order to apply Machine Learning to a
wide variety of tasks. Recently, two families of self-supervised methods,
contrastive learning and latent bootstrapping, exemplified by SimCLR and BYOL
respectively, have made significant progress. In this work, we hypothesize that
adding explicit information compression to these algorithms yields better and
more robust representations. We verify this by developing SimCLR and BYOL
formulations compatible with the Conditional Entropy Bottleneck (CEB)
objective, allowing us to both measure and control the amount of compression in
the learned representation, and observe their impact on downstream tasks.
Furthermore, we explore the relationship between Lipschitz continuity and
compression, showing a tractable lower bound on the Lipschitz constant of the
encoders we learn. As Lipschitz continuity is closely related to robustness,
this provides a new explanation for why compressed models are more robust. Our
experiments confirm that adding compression to SimCLR and BYOL significantly
improves linear evaluation accuracies and model robustness across a wide range
of domain shifts. In particular, the compressed version of BYOL achieves 76.0%
Top-1 linear evaluation accuracy on ImageNet with ResNet-50, and 78.8% with
ResNet-50 2x.

    

### [[2109.14210] Spatially Coupled PLDPC-Hadamard Convolutional Codes](http://arxiv.org/abs/2109.14210)


  In this paper, we propose a new type of ultimate-Shannon-limit-approaching
codes called spatially coupled protograph-based low-density parity-check
Hadamard convolutional codes (SC-PLDPCH-CCs), which are constructed by
spatially coupling PLDPC-Hadamard block codes. We also develop an efficient
decoding algorithm that combines pipeline decoding and layered scheduling for
the decoding of SCPLDPCH- CCs. To estimate the decoding thresholds of
SC-PLDPCH-CCs, we first propose a layered protograph extrinsic information
transfer (PEXIT) algorithm to evaluate the thresholds of spatially coupled
PLDPC-Hadamard terminated codes (SC-PLDPCH-TDCs) with a moderate coupling
length. With the use of the proposed layered PEXIT method, we develop a genetic
algorithm to look for good SC-PLDPCH-TDCs in a systematic way. Subsequently, we
extend the coupling length of these SC-PLDPCH-TDCs with good thresholds to form
good SC-PLDPCH-CCs. Based on the same set of split protomatrices, we regard the
threshold of SC-PLDPCH-TDC as the proxy of SC-PLDPCH-CC when the SC-PLDPCH-TDC
with long coupling length has almost the same code rate as the SC-PLDPCH-CC.
Results show that our optimized SC-PLDPCH-CCs can achieve comparable thresholds
to the block code counterparts. Simulations also illustrate the superiority of
the SC-PLDPCH-CCs over the block code counterparts in terms of error
performance. Moreover, for the rate-0.00295 SC-PLDPCH-CC, a BER of 1e-7 is
achieved at Eb/N0 = -1.45 dB, which is only 0.14 dB from the ultimate Shannon
limit.

    

### [[2109.14349] Relational Memory: Native In-Memory Accesses on Rows and Columns](http://arxiv.org/abs/2109.14349)


  Analytical database systems are typically designed to use a column-first data
layout that maps better to analytical queries of access patterns. This choice
is premised on the assumption that storing data in a row-first format leads to
accessing unwanted fields; moreover, transforming rows to columns at runtime is
expensive. On the other hand, new data items are constantly ingested in
row-first form and transformed in the background to columns to facilitate
future analytical queries. How will this design change if we can always access
only the desired set of columns? In this paper, to address this question, we
present a radically new approach to data transformation from rows to columns.
We build upon recent advancements in commercial embedded platforms with
tightly-coupled re-programmable logic to design native in-memory access on rows
and columns. We propose a new database management system (DBMS) architecture
that is the first hardware/software co-design. It relies on an FPGA-based
accelerator to transparently transform base data to any group of columns with
minimal overhead at runtime. This design allows the DBMS to access any group of
columns as if it already exists in memory. Our method, termed relational
memory, currently implements projection, and offers the groundwork for
implementing selection, group by, aggregation, and supporting joins in
hardware, thus, vastly simplifying the software logic and accelerating the
query execution. We present a detailed analysis of relational memory using both
synthetic benchmarks and realistic workloads. Our relational memory
implementation can convert on the fly rows to arbitrary groups of columns
without any latency penalty. Essentially, relational memory can load in cache
the desired columns from a row-oriented base data layout as fast as reading
from column-oriented base data layout by outsourcing data transformation to the
hardware.

    

### [[2109.14520] Improving DRAM Performance, Security, and Reliability by Understanding and Exploiting DRAM Timing Parameter Margins](http://arxiv.org/abs/2109.14520)


  This dissertation rigorously characterizes many modern commodity DRAM devices
and shows that by exploiting DRAM access timing margins within
manufacturer-recommended DRAM timing specifications, we can significantly
improve system performance, reduce power consumption, and improve device
reliability and security. First, we characterize DRAM timing parameter margins
and find that certain regions of DRAM can be accessed faster than other regions
due to DRAM cell process manufacturing variation. We exploit this by enabling
variable access times depending on the DRAM cells being accessed, which not
only improves overall system performance, but also decreases power consumption.
Second, we find that we can uniquely identify DRAM devices by the locations of
failures that result when we access DRAM with timing parameters reduced below
specification values. Because we induce these failures with DRAM accesses, we
can generate these unique identifiers significantly more quickly than prior
work. Third, we propose a random number generator that is based on our
observation that timing failures in certain DRAM cells are randomly induced and
can thus be repeatedly polled to very quickly generate true random values.
Finally, we characterize the RowHammer security vulnerability on a wide range
of modern DRAM chips while violating the DRAM refresh requirement in order to
directly characterize the underlying DRAM technology without the interference
of refresh commands. We demonstrate with our characterization of real chips,
that existing RowHammer mitigation mechanisms either are not scalable or suffer
from prohibitively large performance overheads in projected future devices and
it is critical to research more effective solutions to RowHammer. Overall, our
studies build a new understanding of modern DRAM devices to improve computing
system performance, reliability and security all at the same time.

    

### [[2109.13993] ParaLiNGAM: Parallel Causal Structure Learning for Linear non-Gaussian Acyclic Models](http://arxiv.org/abs/2109.13993)


  One of the key objectives in many fields in machine learning is to discover
causal relationships among a set of variables from observational data. In
linear non-Gaussian acyclic models (LiNGAM), it can be shown that the true
underlying causal structure can be identified uniquely from merely
observational data. DirectLiNGAM algorithm is a well-known solution to learn
the true causal structure in high dimensional setting. DirectLiNGAM algorithm
executes in a sequence of iterations and it performs a set of comparisons
between pairs of variables in each iteration. Unfortunately, the runtime of
this algorithm grows significantly as the number of variables increases. In
this paper, we propose a parallel algorithm, called ParaLiNGAM, to learn casual
structures based on DirectLiNGAM algorithm. We propose a threshold mechanism
that can reduce the number of comparisons remarkably compared with the
sequential solution. Moreover, in order to further reduce runtime, we employ a
messaging mechanism between workers and derive some mathematical formulations
to simplify the execution of comparisons. We also present an implementation of
ParaLiNGAM on GPU, considering hardware constraints. Experimental results on
synthetic and real data show that the implementation of proposed algorithm on
GPU can outperform DirectLiNGAM by a factor up to 4600 X.

    

### [[2109.14072] A Look at Communication-Intensive Performance in Julia](http://arxiv.org/abs/2109.14072)


  The Julia programming language continues to gain popularity both for its
potential for programmer productivity and for its impressive performance on
scientific code. It thus holds potential for large-scale HPC, but we have not
yet seen this potential fully realized. While Julia certainly has the machinery
to run at scale, and while others have done so for embarrassingly parallel
workloads, we have yet to see an analysis of Julia's performance on
communication-intensive codes that are so common in the HPC domain. In this
paper we investigate Julia's performance in this light, first with a suite of
microbenchmarks within and without the node, and then using the first Julia
port of a standard, HPC benchmarking code, high-performance conjugate gradient
(HPCG). We show that if programmers properly balance the computation to
communication ratio, Julia can actually outperform C/MPI in a cluster computing
environment.

    

### [[2109.14111] Modeling and Control of Google bittide Synchronization](http://arxiv.org/abs/2109.14111)


  Distributed system applications rely on a fine-grain common sense of time.
Existing systems maintain the common sense of time by keeping each independent
machine as close as possible to wall-clock time through a combination of
software protocols like NTP and GPS signals and/or precision references like
atomic clocks. This approach is expensive and has tolerance limitations that
require protocols to deal with asynchrony and its performance consequences.
Moreover, at data-center scale it is impractical to distribute a physical clock
as is done on a chip or printed circuit board. In this paper we introduce a
distributed system design that removes the need for physical clock distribution
or mechanisms for maintaining close alignment to wall-clock time, and instead
provides applications with a perfectly synchronized logical clock. We discuss
the abstract frame model (AFM), a mathematical model that underpins the system
synchronization. The model is based on the rate of communication between nodes
in a topology without requiring a global clock. We show that there are families
of controllers that satisfy the properties required for existence and
uniqueness of solutions to the AFM, and give examples.

    

### [[2109.14125] Applications and Implications of a General Framework for Self-Stabilizing Overlay Networks](http://arxiv.org/abs/2109.14125)


  From data centers to IoT devices to Internet-based applications, overlay
networks have become an important part of modern computing. Many of these
overlay networks operate in fragile environments where processes are
susceptible to faults which may perturb a node's state and the network
topology. Self-stabilizing overlay networks have been proposed as one way to
manage these faults, promising to build or restore a particular topology from
any initial configuration or after the occurrence of any transient fault. To
date there have been several self-stabilizing protocols designed for overlay
networks. These protocols, however, are either focused on a single specific
topology, or provide very inefficient solutions for a general set of overlay
networks.
In this paper, we analyze an existing algorithm and show it can be used as a
general framework for building many other self-stabilizing overlay networks.
Our analysis for time and space complexity depends upon several properties of
the target topology itself, providing insight into how topology selection
impacts the complexity of convergence. We then demonstrate the application of
this framework by analyzing the complexity for several existing topologies.
Next, using insights gained from our analysis, we present a new topology
designed to provide efficient performance during convergence with the general
framework. Our process demonstrates how the implications of our analysis help
isolate the factors of interest to allow a network designer to select an
appropriate topology for the problem requirements.

    

### [[2109.14126] Network Scaffolding for Efficient Stabilization of the Chord Overlay Network](http://arxiv.org/abs/2109.14126)


  Overlay networks, where nodes communicate with neighbors over logical links
consisting of zero or more physical links, have become an important part of
modern networking. From data centers to IoT devices, overlay networks are used
to organize a diverse set of processes for efficient operations like searching
and routing. Many of these overlay networks operate in fragile environments
where faults that perturb the logical network topology are commonplace.
Self-stabilizing overlay networks offer one approach for managing these faults,
promising to build or restore a particular topology from any weakly-connected
initial configuration.
Designing efficient self-stabilizing algorithms for many topologies, however,
is not an easy task. For non-trivial topologies that have desirable properties
like low diameter and robust routing in the face of node or link failures,
self-stabilizing algorithms to date have had at least linear running time or
space requirements. In this work, we address this issue by presenting an
algorithm for building a Chord network that has polylogarithmic time and space
complexity. Furthermore, we discuss how the technique we use for building this
Chord network can be generalized into a ``design pattern'' for other desirable
overlay network topologies.

    

### [[2109.14130] When Blockchain Meets Smart Grids: A Comprehensive Survey](http://arxiv.org/abs/2109.14130)


  Recent years have witnessed an increasing interest in the blockchain
technology, and many blockchain-based applications have been developed to take
advantage of its decentralization, transparency, fault tolerance, and strong
security. In the field of smart grids, a plethora of proposals have emerged to
utilize blockchain for augmenting intelligent energy management, energy
trading, security and privacy protection, microgrid management, and energy
vehicles. Compared with traditional centralized approaches, blockchain-based
solutions are able to exploit the advantages of blockchain to realize better
functionality in smart grids. However, the blockchain technology itself has its
disadvantages in low processing throughput and weak privacy protection.
Therefore, it is of paramount importance to study how to integrate blockchain
with smart grids in a more effective way so that the advantages of blockchain
can be maximized and its disadvantages can be avoided.
This article surveys the state-of-the-art solutions aiming to integrate the
emergent blockchain technology with smart grids. The goal of this survey is to
discuss the necessity of applying blockchain in different components of smart
grids, identify the challenges encountered by current solutions, and highlight
the frameworks and techniques used to integrate blockchain with smart grids. We
also present thorough comparison studies among blockchain-based solutions for
smart grids from different perspectives, with the aim to provide insights on
integrating blockchain with smart grids for different smart grid management
tasks. Finally, we list the current projects and initiatives demonstrating the
current effort from the practice side. Additionally, we draw attention to open
problems that have not yet been tackled by existing solutions, and point out
possible future research directions.

    

### [[2109.14189] Byz-GentleRain: An Efficient Byzantine-tolerant Causal Consistency Protocol](http://arxiv.org/abs/2109.14189)


  Causal consistency is a widely used weak consistency model that allows high
availability despite network partitions. There are plenty of research
prototypes and industrial deployments of causally consistent distributed
systems. However, as far as we know, none of them consider Byzantine faults,
except Byz-RCM proposed by Tseng et al. Byz-RCM achieves causal consistency in
the client-server model with $3f + 1$ servers where up to $f$ servers may
suffer Byzantine faults, but assumes that clients are non-Byzantine. In this
work, we present Byz-Gentlerain, the first causal consistency protocol which
tolerates up to $f$ Byzantine servers among $3f + 1$ servers in each partition
and any number of Byzantine clients. Byz-GentleRain is inspired by the
stabilization mechanism of GentleRain for causal consistency. To prevent causal
violations due to Byzantine faults, Byz-GentleRain relies on PBFT to reach
agreement on a sequence of global stable times and updates among servers, and
only updates with timestamps less than or equal to such common global stable
times are visible to clients. We prove that Byz-GentleRain achieves Byz-CC, the
causal consistency variant in the presence of Byzantine faults. We evaluate
Byz-GentleRain on Aliyun. The preliminary results show that Byz-GentleRain is
efficient on typical workloads.

    

### [[2109.14604] Fast B4B: Fast BFT for Blockchains](http://arxiv.org/abs/2109.14604)


  Low latency is one of the desired properties for partially synchronous
Byzantine consensus protocols. Previous protocols have achieved consensus with
just two communication steps either by reducing the bound on the number of
faults the protocol can tolerate ($f \leq \frac{n-1}{5}$) or use of trusted
hardware like Trusted Execution Environment or TEEs. In this paper, we propose
a protocol called Fast B4B, in which the protocol achieves consensus in just
two communication steps. Fast B4B can tolerate maximum number of faults a
partial BFT consensus can tolerate ($f \leq \frac{n-1}{3}$). Furthermore, Fast
B4B does not require the use of any trusted hardware. The trade-off for this
achievement is that at most $f$ times some nodes may revert their blocks. We
show that this reversion of a block will not compromise the safety of the
protocol at all, yet it may incur a small amount of additional latency during
view change.

    

### [[1912.05848] EPIC: An Energy-Efficient, High-Performance GPGPU Computing Research Infrastructure](http://arxiv.org/abs/1912.05848)


  The pursuit of many research questions requires massive computational
resources. State-of-the-art research in physical processes using simulations,
the training of neural networks for deep learning, or the analysis of big data
are all dependent on the availability of sufficient and performant
computational resources. For such research, access to a high-performance
computing infrastructure is indispensable. Many scientific workloads from such
research domains are inherently parallel and can benefit from the data-parallel
architecture of general purpose graphics processing units (GPGPUs). However,
GPGPU resources are scarce at Norway's national infrastructure. EPIC is a GPGPU
enabled computing research infrastructure at NTNU. It enables NTNU's
researchers to perform experiments that otherwise would be impossible, as
time-to-solution would simply take too long.

    

### [[2012.13806] Time-Fluid Field-Based Coordination through Programmable Distributed Schedulers](http://arxiv.org/abs/2012.13806)


  Emerging application scenarios, such as cyber-physical systems (CPSs), the
Internet of Things (IoT), and edge computing, call for coordination approaches
addressing openness, self-adaptation, heterogeneity, and deployment
agnosticism. Field-based coordination is one such approach, promoting the idea
of programming system coordination declaratively from a global perspective, in
terms of functional manipulation and evolution in "space and time" of
distributed data structures called fields. More specifically regarding time, in
field-based coordination (as in many other distributed approaches to
coordination) it is assumed that local activities in each device are regulated
by a fair and unsynchronised fixed clock working at the platform level. In this
work, we challenge this assumption, and propose an alternative approach where
scheduling is programmed in a natural way (along with usual field-based
coordination) in terms of causality fields, each enacting a programmable
distributed notion of a computation "cause" (why and when a field computation
has to be locally computed) and how it should change across time and space.
Starting from low-level platform triggers, such causality fields can be
organised into multiple layers, up to high-level, collectively-computed time
abstractions, to be used at the application level. This reinterpretation of
time in terms of articulated causality relations allows us to express what we
call "time-fluid" coordination, where scheduling can be finely tuned so as to
select the triggers to react to, generally allowing to adaptively balance
performance (system reactivity) and cost (resource usage) of computations. We
formalise the proposed scheduling framework for field-based coordination in the
context of the field calculus, discuss an implementation in the aggregate
computing framework, and finally evaluate the approach via simulation on
several case studies.

    

### [[2109.13978] Identifying Reasoning Flaws in Planning-Based RL Using Tree Explanations](http://arxiv.org/abs/2109.13978)


  Enabling humans to identify potential flaws in an agent's decision making is
an important Explainable AI application. We consider identifying such flaws in
a planning-based deep reinforcement learning (RL) agent for a complex real-time
strategy game. In particular, the agent makes decisions via tree search using a
learned model and evaluation function over interpretable states and actions.
This gives the potential for humans to identify flaws at the level of reasoning
steps in the tree, even if the entire reasoning process is too complex to
understand. However, it is unclear whether humans will be able to identify such
flaws due to the size and complexity of trees. We describe a user interface and
case study, where a small group of AI experts and developers attempt to
identify reasoning flaws due to inaccurate agent learning. Overall, the
interface allowed the group to identify a number of significant flaws of
varying types, demonstrating the promise of this approach.

    

### [[2109.14053] AutoPhaseNN: Unsupervised Physics-aware Deep Learning of 3D Nanoscale Coherent Imaging](http://arxiv.org/abs/2109.14053)


  The problem of phase retrieval, or the algorithmic recovery of lost phase
information from measured intensity alone, underlies various imaging methods
from astronomy to nanoscale imaging. Traditional methods of phase retrieval are
iterative in nature, and are therefore computationally expensive and time
consuming. More recently, deep learning (DL) models have been developed to
either provide learned priors to iterative phase retrieval or in some cases
completely replace phase retrieval with networks that learn to recover the lost
phase information from measured intensity alone. However, such models require
vast amounts of labeled data, which can only be obtained through simulation or
performing computationally prohibitive phase retrieval on hundreds of or even
thousands of experimental datasets. Using a 3D nanoscale X-ray imaging modality
(Bragg Coherent Diffraction Imaging or BCDI) as a representative technique, we
demonstrate AutoPhaseNN, a DL-based approach which learns to solve the phase
problem without labeled data. By incorporating the physics of the imaging
technique into the DL model during training, AutoPhaseNN learns to invert 3D
BCDI data from reciprocal space to real space in a single shot without ever
being shown real space images. Once trained, AutoPhaseNN is about one hundred
times faster than traditional iterative phase retrieval methods while providing
comparable image quality.

    

### [[2109.14112] An epistemic approach to model uncertainty in data-graphs](http://arxiv.org/abs/2109.14112)


  Graph databases are becoming widely successful as data models that allow to
effectively represent and process complex relationships among various types of
data. As with any other type of data repository, graph databases may suffer
from errors and discrepancies with respect to the real-world data they intend
to represent. In this work we explore the notion of probabilistic unclean graph
databases, previously proposed for relational databases, in order to capture
the idea that the observed (unclean) graph database is actually the noisy
version of a clean one that correctly models the world but that we know
partially. As the factors that may be involved in the observation can be many,
e.g, all different types of clerical errors or unintended transformations of
the data, we assume a probabilistic model that describes the distribution over
all possible ways in which the clean (uncertain) database could have been
polluted. Based on this model we define two computational problems: data
cleaning and probabilistic query answering and study for both of them their
corresponding complexity when considering that the transformation of the
database can be caused by either removing (subset) or adding (superset) nodes
and edges.

    

### [[2109.14128] Grouptron: Dynamic Multi-Scale Graph Convolutional Networks for Group-Aware Dense Crowd Trajectory Forecasting](http://arxiv.org/abs/2109.14128)


  Accurate, long-term forecasting of human pedestrian trajectories in highly
dynamic and interactive scenes is a long-standing challenge. Recent advances in
using data-driven approaches have achieved significant improvements in terms of
prediction accuracy. However, the lack of group-aware analysis has limited the
performance of forecasting models. This is especially apparent in highly
populated scenes, where pedestrians are moving in groups and the interactions
between groups are extremely complex and dynamic. In this paper, we present
Grouptron, a multi-scale dynamic forecasting framework that leverages
pedestrian group detection and utilizes individual-level, group-level, and
scene-level information for better understanding and representation of the
scenes. Our approach employs spatio-temporal clustering algorithms to identify
pedestrian groups, creates spatio-temporal graphs at the individual, group, and
scene levels. It then uses graph neural networks to encode dynamics at
different scales and incorporates encoding across different scales for
trajectory prediction. We carried out extensive comparisons and ablation
experiments to demonstrate the effectiveness of our approach. Our method
achieves 9.3% decrease in final displacement error (FDE) compared with
state-of-the-art methods on ETH/UCY benchmark datasets, and 16.1% decrease in
FDE in more crowded scenes where extensive human group interactions are more
frequently present.

    

### [[2109.14155] Customs Fraud Detection in the Presence of Concept Drift](http://arxiv.org/abs/2109.14155)


  Capturing the changing trade pattern is critical in customs fraud detection.
As new goods are imported and novel frauds arise, a drift-aware fraud detection
system is needed to detect both known frauds and unknown frauds within a
limited budget. The current paper proposes ADAPT, an adaptive selection method
that controls the balance between exploitation and exploration strategies used
for customs fraud detection. ADAPT makes use of the model performance trends
and the amount of concept drift to determine the best exploration ratio at
every time. Experiments on data from four countries over several years show
that each country requires a different amount of exploration for maintaining
its fraud detection system. We find the system with ADAPT can gradually adapt
to the dataset and find the appropriate amount of exploration ratio with high
performance.

    

### [[2109.14196] WEDGE: Web-Image Assisted Domain Generalization for Semantic Segmentation](http://arxiv.org/abs/2109.14196)


  Domain generalization for semantic segmentation is highly demanded in real
applications, where a trained model is expected to work well in previously
unseen domains. One challenge lies in the lack of data which could cover the
diverse distributions of the possible unseen domains for training. In this
paper, we propose a WEb-image assisted Domain GEneralization (WEDGE) scheme,
which is the first to exploit the diversity of web-crawled images for
generalizable semantic segmentation. To explore and exploit the real-world data
distributions, we collect a web-crawled dataset which presents large diversity
in terms of weather conditions, sites, lighting, camera styles, etc. We also
present a method which injects the style representation of the web-crawled data
into the source domain on-the-fly during training, which enables the network to
experience images of diverse styles with reliable labels for effective
training. Moreover, we use the web-crawled dataset with predicted pseudo labels
for training to further enhance the capability of the network. Extensive
experiments demonstrate that our method clearly outperforms existing domain
generalization techniques.

    

### [[2109.14233] A Next Basket Recommendation Reality Check](http://arxiv.org/abs/2109.14233)


  The goal of a next basket recommendation (NBR) system is to recommend items
for the next basket for a user, based on the sequence of their prior baskets.
Recently, a number of methods with complex modules have been proposed that
claim state-of-the-art performance. They rarely look into the predicted basket
and just provide intuitive reasons for the observed improvements, e.g., better
representation, capturing intentions or relations, etc. We provide a novel
angle on the evaluation of next basket recommendation methods, centered on the
distinction between repetition and exploration: the next basket is typically
composed of previously consumed items (i.e., repeat items) and new items (i.e,
explore items). We propose a set of metrics that measure the repeat/explore
ratio and performance of NBR models. Using these new metrics, we analyze
state-of-the-art NBR models. The results of our analysis help to clarify the
extent of the actual progress achieved by existing NBR methods as well as the
underlying reasons for the improvements. Overall, our work sheds light on the
evaluation problem of NBR and provides useful insights into the model design
for this task.

    

### [[2109.14325] Improving Safety in Deep Reinforcement Learning using Unsupervised Action Planning](http://arxiv.org/abs/2109.14325)


  One of the key challenges to deep reinforcement learning (deep RL) is to
ensure safety at both training and testing phases. In this work, we propose a
novel technique of unsupervised action planning to improve the safety of
on-policy reinforcement learning algorithms, such as trust region policy
optimization (TRPO) or proximal policy optimization (PPO). We design our
safety-aware reinforcement learning by storing all the history of "recovery"
actions that rescue the agent from dangerous situations into a separate
"safety" buffer and finding the best recovery action when the agent encounters
similar states. Because this functionality requires the algorithm to query
similar states, we implement the proposed safety mechanism using an
unsupervised learning algorithm, k-means clustering. We evaluate the proposed
algorithm on six robotic control tasks that cover navigation and manipulation.
Our results show that the proposed safety RL algorithm can achieve higher
rewards compared with multiple baselines in both discrete and continuous
control problems. The supplemental video can be found at:
this https URL.

    

### [[2109.14334] Secure Multi-Party Computation based Privacy Preserving Data Analysis in Healthcare IoT Systems](http://arxiv.org/abs/2109.14334)


  Recently, many innovations have been experienced in healthcare by rapidly
growing Internet-of-Things (IoT) technology that provides significant
developments and facilities in the health sector and improves daily human life.
The IoT bridges people, information technology and speed up shopping. For these
reasons, IoT technology has started to be used on a large scale. Thanks to the
use of IoT technology in health services, chronic disease monitoring, health
monitoring, rapid intervention, early diagnosis and treatment, etc. facilitates
the delivery of health services. However, the data transferred to the digital
environment pose a threat of privacy leakage. Unauthorized persons have used
them, and there have been malicious attacks on the health and privacy of
individuals. In this study, it is aimed to propose a model to handle the
privacy problems based on federated learning. Besides, we apply secure multi
party computation. Our proposed model presents an extensive privacy and data
analysis and achieve high performance.

    

### [[2109.14381] From Organisational Structure to Organisational Behaviour Formalisation](http://arxiv.org/abs/2109.14381)


  To understand how an organisational structure relates to organisational
behaviour is an interesting fundamental challenge in the area of organisation
modelling. Specifications of organisational structure usually have a
diagrammatic form that abstracts from more detailed dynamics. Dynamic
properties of agent systems, on the other hand, are often specified in the form
of a set of logical formulae in some temporal language. This paper addresses
the question how these two perspectives can be combined in one framework. It is
shown how for different aggregation levels and other elements within an
organisation structure, sets of dynamic properties can be specified.
Organisational structure provides a structure of (interlevel) relationships
between these multiple sets of dynamic properties. Thus organisational
structure is reflected in the formalisation of the dynamics of organisational
behaviour. To illustrate the effectiveness of the approach a formal foundation
is presented for the integrated specification of both structure and behaviour
of an AGR organisation model.

    

### [[2109.14406] Neural Knitworks: Patched Neural Implicit Representation Networks](http://arxiv.org/abs/2109.14406)


  Coordinate-based Multilayer Perceptron (MLP) networks, despite being capable
of learning neural implicit representations, are not performant for internal
image synthesis applications. Convolutional Neural Networks (CNNs) are
typically used instead for a variety of internal generative tasks, at the cost
of a larger model. We propose Neural Knitwork, an architecture for neural
implicit representation learning of natural images that achieves image
synthesis by optimizing the distribution of image patches in an adversarial
manner and by enforcing consistency between the patch predictions. To the best
of our knowledge, this is the first implementation of a coordinate-based MLP
tailored for synthesis tasks such as image inpainting, super-resolution, and
denoising. We demonstrate the utility of the proposed technique by training on
these three tasks. The results show that modeling natural images using patches,
rather than pixels, produces results of higher fidelity. The resulting model
requires 80% fewer parameters than alternative CNN-based solutions while
achieving comparable performance and training time.

    

### [[2109.14493] Automatic discovery and description of human planning strategies](http://arxiv.org/abs/2109.14493)


  Scientific discovery concerns finding patterns in data and creating
insightful hypotheses that explain these patterns. Traditionally, this process
required human ingenuity, but with the galloping advances in artificial
intelligence (AI) it becomes feasible to automate some parts of scientific
discovery. In this work we leverage AI for strategy discovery for understanding
human planning. In the state-of-the-art methods data about the process of human
planning is often used to group similar behaviors together and formulate verbal
descriptions of the strategies which might underlie those groups. Here, we
automate these two steps. Our algorithm, called Human-Interpret, uses imitation
learning to describe process-tracing data collected in psychological
experiments with the Mouselab-MDP paradigm in terms of a procedural formula.
Then, it translates that formula to natural language using a pre-defined
predicate dictionary. We test our method on a benchmark data set that
researchers have previously scrutinized manually. We find that the descriptions
of human planning strategies obtained automatically are about as understandable
as human-generated descriptions. They also cover a substantial proportion of
all types of human planning strategies that had been discovered manually. Our
method saves scientists' time and effort as all the reasoning about human
planning is done automatically. This might make it feasible to more rapidly
scale up the search for yet undiscovered cognitive strategies to many new
decision environments, populations, tasks, and domains. Given these results, we
believe that the presented work may accelerate scientific discovery in
psychology, and due to its generality, extend to problems from other fields.

    

### [[2106.01177] Learning to Time-Decode in Spiking Neural Networks Through the Information Bottleneck](http://arxiv.org/abs/2106.01177)


  One of the key challenges in training Spiking Neural Networks (SNNs) is that
target outputs typically come in the form of natural signals, such as labels
for classification or images for generative models, and need to be encoded into
spikes. This is done by handcrafting target spiking signals, which in turn
implicitly fixes the mechanisms used to decode spikes into natural signals,
e.g., rate decoding. The arbitrary choice of target signals and decoding rule
generally impairs the capacity of the SNN to encode and process information in
the timing of spikes. To address this problem, this work introduces a hybrid
variational autoencoder architecture, consisting of an encoding SNN and a
decoding Artificial Neural Network (ANN). The role of the decoding ANN is to
learn how to best convert the spiking signals output by the SNN into the target
natural signal. A novel end-to-end learning rule is introduced that optimizes a
directed information bottleneck training criterion via surrogate gradients. We
demonstrate the applicability of the technique in an experimental settings on
various tasks, including real-life datasets.

    

### [[2109.14156] Labor-right Protecting Dispatch of Meal Delivery Platforms](http://arxiv.org/abs/2109.14156)


  The boom in the meal delivery industry brings growing concern about the labor
rights of riders. Current dispatch policies of meal-delivery platforms focus
mainly on satisfying consumers or minimizing the number of riders for cost
savings. There are few discussions on improving the working conditions of
riders by algorithm design. The lack of concerns on labor rights in mechanism
and dispatch design has resulted in a very large time waste for riders and
their risky driving. In this research, we propose a queuing-model-based
framework to discuss optimal dispatch policy with the goal of labor rights
protection. We apply our framework to develop an algorithm minimizing the
waiting time of food delivery riders with guaranteed user experience. Our
framework also allows us to manifest the value of restaurants' data about their
offline-order numbers on improving the benefits of riders.

    

### [[2109.14266] n-Qubit Operations on Sphere and Queueing Scaling Limits for Programmable Quantum Computer](http://arxiv.org/abs/2109.14266)


  We study n-qubit operation rules on (n+1)-sphere with the target to help
developing a (photon or other technique) based programmable quantum computer.
In the meanwhile, we derive the scaling limits (called reflecting Gaussian
random fields on a (n+1)-sphere) for n-qubit quantum computer based queueing
systems under two different heavy traffic regimes. The queueing systems are
with multiple classes of users and batch quantum random walks over the
$(n+1)$-sphere as arrival inputs. In the first regime, the qubit number $n$ is
fixed and the scaling is in terms of both time and space. Under this regime,
performance modeling during deriving the scaling limit in terms of balancing
the arrival and service rates under first-in first-out and work conserving
service policy is conducted. In the second regime, besides the time and space
scaling parameters, the qubit number $n$ itself is also considered as a varying
scaling parameter with the additional aim to find a suitable number of qubits
for the design of a quantum computer. This regime is in contrast to the
well-known Halfin-Whitt regime.

    

### [[2009.07935] Towards an Objective Metric for the Performance of Exact Triangle Count](http://arxiv.org/abs/2009.07935)


  The performance of graph algorithms is often measured in terms of the number
of traversed edges per second (TEPS). However, this performance metric is
inadequate for a graph operation such as exact triangle counting. In triangle
counting, execution times on graphs with a similar number of edges can be
distinctly different as demonstrated by results from the past Graph Challenge
entries. We discuss the need for an objective performance metric for graph
operations and the desired characteristics of such a metric such that it more
accurately captures the interactions between the amount of work performed and
the capabilities of the hardware on which the code is executed. Using exact
triangle counting as an example, we derive a metric that captures how certain
techniques employed in many implementations improve performance. We demonstrate
that our proposed metric can be used to evaluate and compare multiple
approaches for triangle counting, using a SIMD approach as a case study against
a scalar baseline.

    

### [[2109.14534] A verified algebraic representation of Cairo program execution](http://arxiv.org/abs/2109.14534)


  Cryptographic interactive proof systems provide an efficient and scalable
means of verifying the results of computation on blockchain. A prover
constructs a proof, off-chain, that the execution of a program on a given input
terminates with a certain result. The prover then publishes a certificate that
can be verified efficiently and reliably modulo commonly accepted cryptographic
assumptions. The method relies on an algebraic encoding of execution traces of
programs. Here we report on a verification of the correctness of such an
encoding of the Cairo model of computation with respect to the STARK
interactive proof system, using the Lean 3 proof assistant.

    