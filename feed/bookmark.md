
## 2021-10-20

### [[2110.09563] WONDER: Workload Optimized Network Defined Edge Routing](http://arxiv.org/abs/2110.09563)


  The 5G standards enable cellular network capabilities that significantly
improve key network characteristics such as latency, capacity, throughput and
reliability, compared to the previous generations of wireless networks. It is,
however, clear that in order to achieve these improvements in real network
implementations, the supporting physical and logical infrastructure needs to be
designed appropriately. The key components of this infrastructure are Radio
Access Network, Edge Data Centers, Packet/Optical Interconnection Fabric and
Edge Computing. This paper concentrates on the Edge Data Centers,
Interconnection and Edge Computing capabilities that target ability to deliver
high-performing services on the 5G network by means of intelligent network
slicing, traffic routing and coordinated compute workload distribution. We
propose new methods of ensuring optimal traffic routing and edge compute
workload placement under mobility conditions, subject to application
requirements and constraints within a set of interconnected Edge Data Centers,
utilizing Segment Routing/IPv6 and software defined control mechanisms.

    

### [[2110.09607] Hierarchical Mobility Label Based Network: System Model and Performance Analysis](http://arxiv.org/abs/2110.09607)


  Hierarchical Mobility Label Based Network (HMLBN) is a new approach to the
network layer mobility management problem that relies on MPLS-aware control
plane and MPLS-based forwarding plane to provide IP mobility support for IPv4
and IPv6 mobile hosts and routers while being able to ensure optimal traffic
delivery between the communicating devices. The hierarchical system is capable
of both macro- and micro-mobility support without the use of Mobile IP and its
derivatives thus eliminating the user and network facing performance penalties
associated with triangular routing and bi-directional tunneling. This paper
presents a system model and provides performance analysis for H-MLBN and
compares its performance with the Mobile IP based schemes. The results indicate
significant performance improvements in the forwarding plane traffic delivery
as well as the control plane network update costs.

    

### [[2110.09609] Mobility Label Based Network: Hierarchical Mobility Management and Packet Forwarding Architecture](http://arxiv.org/abs/2110.09609)


  Scalability of the network layer mobility management solution is one of the
most important requirements for the mobility control plane. Mobility Label
Based Network (MLBN) is a new approach to the network layer mobility management
problem that relies solely on MPLS to provide both macro- and micro-mobility
for IPv4 and IPv6 mobile hosts and routers. This new approach does not rely on
the existing IP mobility management protocols such as Mobile IP and is based on
the combination of Multi- Protocol BGP (MP-BGP) and MPLS. In the context of the
MLBN the scalable control plane should be capable of efficient Mobility Label
distribution while allowing the MPLS based forwarding plane to deliver mobile
traffic in an optimal manner. This paper presents a hierarchical mobility
management system capable of both macro- and micromobility support without the
use of Mobile IP and its derivatives and allows scalable Mobility Label
distribution and MPLS label stack based packet forwarding in support of optimal
traffic delivery between the communicating mobile users.

    

### [[2110.09648] Data Flow Dissemination in a Network](http://arxiv.org/abs/2110.09648)


  We consider the following network model motivated, in particular, by
blockchains and peer-to-peer live streaming. Data packet flows originate at the
network nodes and need to be disseminated to all other nodes. Packets are
relayed through the network via links of limited capacity. A packet leaves the
network when it is disseminated to all nodes. The network is stable when it is
positive recurrent; and when it is, the age of the oldest packet, referred to
as Age-of-Information (AoI) is stochastically bounded. Under the Random-Useful
(RU) discipline a node $u$ communicates on link $(u,v)$ a randomly chosen
available packet not present at $v$. RU discipline is known to have the maximum
stability region for a single flow; we show that this extends to arbitrary
number of flows. Our main results concern the Oldest-Useful (OU) discipline,
under which a node $u$ communicates on link $(u,v)$ the oldest available packet
not present at $v$. OU discipline is a natural candidate for reducing the AoI.
We show that, surprisingly, OU \emph{does not} provide the maximum stability
region. As the main result of this paper, we prove that OU \emph{does} have the
maximum stability region in the important special case of a complete graph
network with equal capacities on all links and equal flow rates originating in
all nodes. Simulation results show that, in the latter special case, OU
out-performs RU in terms of AoI.

    

### [[2110.09840] Stability analysis of two-class retrial systems with constant retrial rates and general service times](http://arxiv.org/abs/2110.09840)


  We establish stability criterion for a two-class retrial system with Poisson
inputs, general class-dependent service times and class-dependent constant
retrial rates. We also characterise an interesting phenomenon of partial
stability when one orbit is tight but the other orbit goes to infinity in
probability. All theoretical results are illustrated by numerical experiments.

    

### [[2110.09934] Elevating the future of mobility: UAV-enabled Intelligent Transportation Systems](http://arxiv.org/abs/2110.09934)


  Intelligent Transportation Systems (ITS) improve traffic efficiency, traffic
management, driver's comfort, and safety. They consist of a broad range of
components, including vehicles, sensors, Base Stations, Road Side Units, and
road infrastructure (i.e., traffic signals). ITS of the near future will need
to support multi-modal transportation schemes (including ground and aerial
vehicles, so-called Urban Air Mobility). ITS will have to be integrated with
Unmanned Aerial Systems Traffic Management (UTM) and rely on 3 Dimensional (3D)
connectivity provided by Integrated Aerial-Terrestrial 6G networks to achieve
this support. In other words, various types of Unmanned Aerial Vehicles (UAVs)
will become integral parts of future ITS due to their mobility, autonomous
operation, and communication/processing capabilities. This article presents our
view on the future integration of ITS and UTM systems, enabling wireless
technologies and open research questions. We also present how UAVs can be used
to enhance the performance of the currently available ITS.

    

### [[2110.09977] An Ultra-Reliable Low-Latency Non-Binary Polar Coded SCMA Scheme](http://arxiv.org/abs/2110.09977)


  The joint transmission scheme of polar codes and sparse code multiple access
(SCMA) has been regarded as a promising technology for future wireless
communication systems. However, most of the existing polar-coded SCMA (PC-SCMA)
systems suffer from high latency caused by the feedback iteration and list
decoding. In addition, the error performance of PC-SCMA systems is
unsatisfactory for ultra-reliable transmission. Inspired by the compelling
benefits of non-binary polar codes, in this paper, we design a non-binary
polar-coded SCMA (NB-PC-SCMA) system with a free order matching strategy to
address the issues of delay and reliability. Specifically, we first formulate a
joint factor graph for NB-PC-SCMA and propose a non-binary successive
cancellation list (NB-SCL) and damping based joint iterative detection and
decoding (NSD-JIDD) multiuser receiver to improve the BER and latency
performance. Then, a lazy-search based NB-SCL (L-NB-SCL) decoding is proposed
to reduce the computational complexity by modifying the path search pattern of
the list decoder. After that, we optimize the update of user nodes for SCMA
detection to improve the convergence error and finally propose the optimized
NSD-JIDD (OSD-JIDD) algorithm, which can avoid redundant operations by
exploiting L-NB-SCL decoding. Simulation results show that the proposed
NB-PC-SCMA system achieves better bit error rate (BER) performance and
considerable latency gain when compared to its counterparts. In particular, the
proposed OSD-JIDD can achieve similar BER performance of NSD-JIDD with less
complexity.

    

### [[2110.09995] Design of AoI-Aware 5G Uplink Scheduler UsingReinforcement Learning](http://arxiv.org/abs/2110.09995)


  Age of Information (AoI) reflects the time that is elapsed from the
generation of a packet by a 5G user equipment(UE) to the reception of the
packet by a controller. A design of an AoI-aware radio resource scheduler for
UEs via reinforcement learning is proposed in this paper. In this paper, we
consider a remote control environment in which a number of UEs are transmitting
time-sensitive measurements to a remote controller. We consider the AoI
minimization problem and formulate the problem as a trade-off between
minimizing the sum of the expected AoI of all UEs and maximizing the throughput
of the network. Inspired by the success of machine learning in solving large
networking problems at low complexity, we develop a reinforcement
learning-based method to solve the formulated problem. We used the
state-of-the-art proximal policy optimization algorithm to solve this problem.
Our simulation results showthat the proposed algorithm outperforms the
considered baselines in terms of minimizing the expected AoI while maintaining
the network throughput.

    

### [[2110.09365] Optical Front/Mid-haul with Open Access-Edge Server Deployment Framework for Sliced O-RAN](http://arxiv.org/abs/2110.09365)


  The fifth-generation of mobile radio technologies is expected to be agile,
flexible, and scalable while provisioning ultra-reliable and low-latency
communication (uRLLC), enhanced mobile broadband (eMBB), and massive machine
type communication (mMTC) applications. These are implemented by adopting
cloudification, network function virtualization, and network slicing techniques
in open-radio access network (O-RAN) architecture where remote radio heads
(RRHs) are connected to dis-aggregated virtual base-band units (BBUs), i.e.,
radio unit (RU), distributed unit (DU), and centralized unit (CU) over
front/mid-haul interfaces. However, cost-efficient solutions are required for
designing front/mid-haul interfaces and time-wavelength division multiplexed
(TWDM) passive optical network (PON) appears as a potential candidate.
Therefore, in this paper, we propose a framework for the optimal placement of
RUs based on long-term network statistics and connecting them to open
access-edge servers for hosting the corresponding DUs and CUs over
front/mid-haul interfaces while satisfying the diverse QoS requirements of
uRLLC, eMBB, and mMTC slices. In turn, we formulate a two-stage integer
programming problem and time-efficient heuristics for users to RU association
and flexible deployment of the corresponding DUs and CUs. We evaluate the O-RAN
deployment cost and latency requirements with our TWDM-PON-based framework
against urban, rural, and industrial areas and show its efficiency over the
optical transport network (OTN)-based framework.

    

### [[2110.09516] Kernel Minimum Divergence Portfolios](http://arxiv.org/abs/2110.09516)


  Portfolio optimization is a key challenge in finance with the aim of creating
portfolios matching the investors' preference. The target distribution approach
relying on the Kullback-Leibler or the $f$-divergence represents one of the
most effective forms of achieving this goal. In this paper, we propose to use
kernel and optimal transport (KOT) based divergences to tackle the task, which
relax the assumptions and the optimization constraints of the previous
approaches. In case of the kernel-based maximum mean discrepancy (MMD) we (i)
prove the analytic computability of the underlying mean embedding for various
target distribution-kernel pairs, (ii) show that such analytic knowledge can
lead to faster convergence of MMD estimators, and (iii) extend the results to
the unbounded exponential kernel with minimax lower bounds. Numerical
experiments demonstrate the improved performance of our KOT estimators both on
synthetic and real-world examples.

    

### [[2110.09524] Understanding GNN Computational Graph: A Coordinated Computation, IO, and Memory Perspective](http://arxiv.org/abs/2110.09524)


  Graph Neural Networks (GNNs) have been widely used in various domains, and
GNNs with sophisticated computational graph lead to higher latency and larger
memory consumption. Optimizing the GNN computational graph suffers from: (1)
Redundant neural operator computation. The same data are propagated through the
graph structure to perform the same neural operation multiple times in GNNs,
leading to redundant computation which accounts for 92.4% of total operators.
(2) Inconsistent thread mapping. Efficient thread mapping schemes for
vertex-centric and edge-centric operators are different. This inconsistency
prohibits operator fusion to reduce memory IO. (3) Excessive intermediate data.
For GNN training which is usually performed concurrently with inference,
intermediate data must be stored for the backward pass, consuming 91.9% of the
total memory requirement. To tackle these challenges, we propose following
designs to optimize the GNN computational graph from a novel coordinated
computation, IO, and memory perspective: (1) Propagation-postponed operator
reorganization. We reorganize operators to perform neural operations before the
propagation, thus the redundant computation is eliminated. (2) Unified thread
mapping for fusion. We propose a unified thread mapping scheme for both vertex-
and edge-centric operators to enable fusion and reduce IO. (3) Intermediate
data recomputation. Intermediate data are recomputed during the backward pass
to reduce the total memory consumption. Extensive experimental results on three
typical GNN models show that, we achieve up to 2.75x end-to-end speedup, 6.89x
less memory IO, and 7.73x less memory consumption over state-of-the-art
frameworks.

    

### [[2110.09525] Eigenbehaviour as an Indicator of Cognitive Abilities](http://arxiv.org/abs/2110.09525)


  With growing usage of machine learning algorithms and big data in health
applications, digital biomarkers have become an important key feature to ensure
the success of those applications. In this paper, we focus on one important
use-case, the long-term continuous monitoring of the cognitive ability of older
adults. The cognitive ability is a factor both for long-term monitoring of
people living alone as well as an outcome in clinical studies. In this work, we
propose a new digital biomarker for cognitive abilities based on location
eigenbehaviour obtained from contactless ambient sensors. Indoor location
information obtained from passive infrared sensors is used to build a location
matrix covering several weeks of measurement. Based on the eigenvectors of this
matrix, the reconstruction error is calculated for various numbers of used
eigenvectors. The reconstruction error is used to predict cognitive ability
scores collected at baseline, using linear regression. Additionally,
classification of normal versus pathological cognition level is performed using
a support-vector-machine. Prediction performance is strong for high levels of
cognitive ability, but grows weaker for low levels of cognitive ability.
Classification into normal versus pathological cognitive ability level reaches
high accuracy with a AUC = 0.94. Due to the unobtrusive method of measurement
based on contactless ambient sensors, this digital biomarker of cognitive
ability is easily obtainable. The usage of the reconstruction error is a strong
digital biomarker for the binary classification and, to a lesser extent, for
more detailed prediction of interindividual differences in cognition.

    

### [[2110.09541] Wideband and Entropy-Aware Deep Soft Bit Quantization](http://arxiv.org/abs/2110.09541)


  Deep learning has been recently applied to physical layer processing in
digital communication systems in order to improve end-to-end performance. In
this work, we introduce a novel deep learning solution for soft bit
quantization across wideband channels. Our method is trained end-to-end with
quantization- and entropy-aware augmentations to the loss function and is used
at inference in conjunction with source coding to achieve near-optimal
compression gains over wideband channels. To efficiently train our method, we
prove and verify that a fixed feature space quantization scheme is sufficient
for efficient learning. When tested on channel distributions never seen during
training, the proposed method achieves a compression gain of up to $10 \%$ in
the high SNR regime versus previous state-of-the-art methods. To encourage
reproducible research, our implementation is publicly available at
this https URL.

    

### [[2110.09548] Path Regularization: A Convexity and Sparsity Inducing Regularization for Parallel ReLU Networks](http://arxiv.org/abs/2110.09548)


  Despite several attempts, the fundamental mechanisms behind the success of
deep neural networks still remain elusive. To this end, we introduce a novel
analytic framework to unveil hidden convexity in training deep neural networks.
We consider a parallel architecture with multiple ReLU sub-networks, which
includes many standard deep architectures and ResNets as its special cases. We
then show that the training problem with path regularization can be cast as a
single convex optimization problem in a high-dimensional space. We further
prove that the equivalent convex program is regularized via a group sparsity
inducing norm. Thus, a path regularized parallel architecture with ReLU
sub-networks can be viewed as a parsimonious feature selection method in
high-dimensions. More importantly, we show that the computational complexity
required to globally optimize the equivalent convex problem is polynomial-time
with respect to the number of data samples and feature dimension. Therefore, we
prove exact polynomial-time trainability for path regularized deep ReLU
networks with global optimality guarantees. We also provide several numerical
experiments corroborating our theory.

    

### [[2110.09564] BGaitR-Net: Occluded Gait Sequence reconstructionwith temporally constrained model for gait recognition](http://arxiv.org/abs/2110.09564)


  Recent advancements in computational resources and Deep Learning
methodologies has significantly benefited development of intelligent
vision-based surveillance applications. Gait recognition in the presence of
occlusion is one of the challenging research topics in this area, and the
solutions proposed by researchers to date lack in robustness and also dependent
of several unrealistic constraints, which limits their practical applicability.
We improve the state-of-the-art by developing novel deep learning-based
algorithms to identify the occluded frames in an input sequence and next
reconstruct these occluded frames by exploiting the spatio-temporal information
present in the gait sequence. The multi-stage pipeline adopted in this work
consists of key pose mapping, occlusion detection and reconstruction, and
finally gait recognition. While the key pose mapping and occlusion detection
phases are done %using Constrained KMeans Clustering and via a graph sorting
algorithm, reconstruction of occluded frames is done by fusing the key
pose-specific information derived in the previous step along with the
spatio-temporal information contained in a gait sequence using a Bi-Directional
Long Short Time Memory. This occlusion reconstruction model has been trained
using synthetically occluded CASIA-B and OU-ISIR data, and the trained model is
termed as Bidirectional Gait Reconstruction Network BGait-R-Net. Our LSTM-based
model reconstructs occlusion and generates frames that are temporally
consistent with the periodic pattern of a gait cycle, while simultaneously
preserving the body structure.

    

### [[2110.09578] Permutation Invariance of Deep Neural Networks with ReLUs](http://arxiv.org/abs/2110.09578)


  Consider a deep neural network (DNN) that is being used to suggest the
direction in which an aircraft must turn to avoid a possible collision with an
intruder aircraft. Informally, such a network is well-behaved if it asks the
own ship to turn right (left) when an intruder approaches from the left
(right). Consider another network that takes four inputs -- the cards dealt to
the players in a game of contract bridge -- and decides which team can bid
game. Loosely speaking, if you exchange the hands of partners (north and south,
or east and west), the decision would not change. However, it will change if,
say, you exchange north's hand with east. This permutation invariance property,
for certain permutations at input and output layers, is central to the
correctness and robustness of these networks.
This paper proposes a sound, abstraction-based technique to establish
permutation invariance in DNNs with ReLU as the activation function. The
technique computes an over-approximation of the reachable states, and an
under-approximation of the safe states, and propagates this information across
the layers, both forward and backward. The novelty of our approach lies in a
useful tie-class analysis, that we introduce for forward propagation, and a
scalable 2-polytope under-approximation method that escapes the exponential
blow-up in the number of regions during backward propagation.
An experimental comparison shows the efficiency of our algorithm over that of
verifying permutation invariance as a two-safety property (using FFNN
verification over two copies of the network).

    

### [[2110.09585] A-Optimal Active Learning](http://arxiv.org/abs/2110.09585)


  In this work we discuss the problem of active learning. We present an
approach that is based on A-optimal experimental design of ill-posed problems
and show how one can optimally label a data set by partially probing it, and
use it to train a deep network. We present two approaches that make different
assumptions on the data set. The first is based on a Bayesian interpretation of
the semi-supervised learning problem with the graph Laplacian that is used for
the prior distribution and the second is based on a frequentist approach, that
updates the estimation of the bias term based on the recovery of the labels. We
demonstrate that this approach can be highly efficient for estimating labels
and training a deep network.

    

### [[2110.09598] Adversarial Domain Adaptation with Paired Examples for Acoustic Scene Classification on Different Recording Devices](http://arxiv.org/abs/2110.09598)


  In classification tasks, the classification accuracy diminishes when the data
is gathered in different domains. To address this problem, in this paper, we
investigate several adversarial models for domain adaptation (DA) and their
effect on the acoustic scene classification task. The studied models include
several types of generative adversarial networks (GAN), with different loss
functions, and the so-called cycle GAN which consists of two interconnected GAN
models. The experiments are performed on the DCASE20 challenge task 1A dataset,
in which we can leverage the paired examples of data recorded using different
devices, i.e., the source and target domain recordings. The results of
performed experiments indicate that the best performing domain adaptation can
be obtained using the cycle GAN, which achieves as much as 66% relative
improvement in accuracy for the target domain device, while only 6\% relative
decrease in accuracy on the source domain. In addition, by utilizing the paired
data examples, we are able to improve the overall accuracy over the model
trained using larger unpaired data set, while decreasing the computational cost
of the model training.

    

### [[2110.09599] Label-Descriptive Patterns and their Application to Characterizing Classification Errors](http://arxiv.org/abs/2110.09599)


  State-of-the-art deep learning methods achieve human-like performance on many
tasks, but make errors nevertheless. Characterizing these errors in easily
interpretable terms gives insight into whether a model is prone to making
systematic errors, but also gives a way to act and improve the model. In this
paper we propose a method that allows us to do so for arbitrary classifiers by
mining a small set of patterns that together succinctly describe the input data
that is partitioned according to correctness of prediction. We show this is an
instance of the more general label description problem, which we formulate in
terms of the Minimum Description Length principle. To discover good pattern
sets we propose the efficient and hyperparameter-free Premise algorithm, which
through an extensive set of experiments we show on both synthetic and
real-world data performs very well in practice; unlike existing solutions it
ably recovers ground truth patterns, even on highly imbalanced data over many
unique items, or where patterns are only weakly associated to labels. Through
two real-world case studies we confirm that Premise gives clear and actionable
insight into the systematic errors made by modern NLP classifiers.

    

### [[2110.09605] Neural Synthesis of Footsteps Sound Effects with Generative Adversarial Networks](http://arxiv.org/abs/2110.09605)


  Footsteps are among the most ubiquitous sound effects in multimedia
applications. There is substantial research into understanding the acoustic
features and developing synthesis models for footstep sound effects. In this
paper, we present a first attempt at adopting neural synthesis for this task.
We implemented two GAN-based architectures and compared the results with real
recordings as well as six traditional sound synthesis methods. Our
architectures reached realism scores as high as recorded samples, showing
encouraging results for the task at hand.

    

### [[2110.09606] Efficient Analysis of COVID-19 Clinical Data using Machine Learning Models](http://arxiv.org/abs/2110.09606)


  Because of the rapid spread of COVID-19 to almost every part of the globe,
huge volumes of data and case studies have been made available, providing
researchers with a unique opportunity to find trends and make discoveries like
never before, by leveraging such big data. This data is of many different
varieties, and can be of different levels of veracity e.g., precise, imprecise,
uncertain, and missing, making it challenging to extract important information
from such data. Yet, efficient analyses of this continuously growing and
evolving COVID-19 data is crucial to inform -- often in real-time -- the
relevant measures needed for controlling, mitigating, and ultimately avoiding
viral spread. Applying machine learning based algorithms to this big data is a
natural approach to take to this aim, since they can quickly scale to such
data, and extract the relevant information in the presence of variety and
different levels of veracity. This is important for COVID-19, and for potential
future pandemics in general.
In this paper, we design a straightforward encoding of clinical data (on
categorical attributes) into a fixed-length feature vector representation, and
then propose a model that first performs efficient feature selection from such
representation. We apply this approach on two clinical datasets of the COVID-19
patients and then apply different machine learning algorithms downstream for
classification purposes. We show that with the efficient feature selection
algorithm, we can achieve a prediction accuracy of more than 90\% in most
cases. We also computed the importance of different attributes in the dataset
using information gain. This can help the policy makers to focus on only
certain attributes for the purposes of studying this disease rather than
focusing on multiple random factors that may not be very informative to patient
outcomes.

    

### [[2110.09610] A Survey on Machine Learning Techniques for Source Code Analysis](http://arxiv.org/abs/2110.09610)


  Context: The advancements in machine learning techniques have encouraged
researchers to apply these techniques to a myriad of software engineering tasks
that use source code analysis such as testing and vulnerabilities detection. A
large number of studies poses challenges to the community to understand the
current landscape. Objective: We aim to summarize the current knowledge in the
area of applied machine learning for source code analysis. Method: We
investigate studies belonging to twelve categories of software engineering
tasks and corresponding machine learning techniques, tools, and datasets that
have been applied to solve them. To do so, we carried out an extensive
literature search and identified 364 primary studies published between 2002 and
2021. We summarize our observations and findings with the help of the
identified studies. Results: Our findings suggest that the usage of machine
learning techniques for source code analysis tasks is consistently increasing.
We synthesize commonly used steps and the overall workflow for each task, and
summarize the employed machine learning techniques. Additionally, we collate a
comprehensive list of available datasets and tools useable in this context.
Finally, we summarize the perceived challenges in this area that include
availability of standard datasets, reproducibility and replicability, and
hardware resources.

    

### [[2110.09618] Interpolating between sampling and variational inference with infinite stochastic mixtures](http://arxiv.org/abs/2110.09618)


  Sampling and Variational Inference (VI) are two large families of methods for
approximate inference with complementary strengths. Sampling methods excel at
approximating arbitrary probability distributions, but can be inefficient. VI
methods are efficient, but can fail when probability distributions are complex.
Here, we develop a framework for constructing intermediate algorithms that
balance the strengths of both sampling and VI. Both approximate a probability
distribution using a mixture of simple component distributions: in sampling,
each component is a delta-function and is chosen stochastically, while in
standard VI a single component is chosen to minimize divergence. We show that
sampling and VI emerge as special cases of an optimization problem over a
mixing distribution, and intermediate approximations arise by varying a single
parameter. We then derive closed-form sampling dynamics over variational
parameters that stochastically build a mixture. Finally, we discuss how to
select the optimal compromise between sampling and VI given a computational
budget. This work is a first step towards a highly flexible yet simple family
of inference methods that combines the complementary strengths of sampling and
VI.

    

### [[2110.09619] Further Generalizations of the Jaccard Index](http://arxiv.org/abs/2110.09619)


  Quantifying the similarity between two sets constitutes a particularly
interesting and useful operation in several theoretical and applied problems
involving set theory. Aimed at quantifying the similarity between two sets, the
Jaccard index has been extensively used in the most diverse types of problems,
also motivating respective generalizations. The present work addressew further
generalizations of this index, including its modification into a coincidence
index capable of accounting also for the level of interiority of the sets, an
extension for sets in continuous vector spaces, the consideration of weights
associated to the involved set elements, the generalization to densities and
generic scalar fields, as well as a means to quantify the joint interdependence
between random variables. The also interesting possibility to take into account
more than two sets was also addressed, including the description of an index
capable of quantifying the level of chaining between three sets. Several of the
described and suggested generalizations have been illustrated with respect to
numeric case examples. It is also posited that these indices can play an
important role while analyzing and integrating datasets in modeling approaches
and pattern recognition activities.

    

### [[2110.09620] Sufficient Dimension Reduction for High-Dimensional Regression and Low-Dimensional Embedding: Tutorial and Survey](http://arxiv.org/abs/2110.09620)


  This is a tutorial and survey paper on various methods for Sufficient
Dimension Reduction (SDR). We cover these methods with both statistical
high-dimensional regression perspective and machine learning approach for
dimensionality reduction. We start with introducing inverse regression methods
including Sliced Inverse Regression (SIR), Sliced Average Variance Estimation
(SAVE), contour regression, directional regression, Principal Fitted Components
(PFC), Likelihood Acquired Direction (LAD), and graphical regression. Then, we
introduce forward regression methods including Principal Hessian Directions
(pHd), Minimum Average Variance Estimation (MAVE), Conditional Variance
Estimation (CVE), and deep SDR methods. Finally, we explain Kernel Dimension
Reduction (KDR) both for supervised and unsupervised learning. We also show
that supervised KDR and supervised PCA are equivalent.

    

### [[2110.09622] Robust Representation and Efficient Feature Selection Allows for Effective Clustering of SARS-CoV-2 Variants](http://arxiv.org/abs/2110.09622)


  The widespread availability of large amounts of genomic data on the
SARS-CoV-2 virus, as a result of the COVID-19 pandemic, has created an
opportunity for researchers to analyze the disease at a level of detail unlike
any virus before it. One one had, this will help biologists, policy makers and
other authorities to make timely and appropriate decisions to control the
spread of the coronavirus. On the other hand, such studies will help to more
effectively deal with any possible future pandemic. Since the SARS-CoV-2 virus
contains different variants, each of them having different mutations,
performing any analysis on such data becomes a difficult task. It is well known
that much of the variation in the SARS-CoV-2 genome happens disproportionately
in the spike region of the genome sequence -- the relatively short region which
codes for the spike protein(s). Hence, in this paper, we propose an approach to
cluster spike protein sequences in order to study the behavior of different
known variants that are increasing at very high rate throughout the world. We
use a k-mers based approach to first generate a fixed-length feature vector
representation for the spike sequences. We then show that with the appropriate
feature selection, we can efficiently and effectively cluster the spike
sequences based on the different variants. Using a publicly available set of
SARS-CoV-2 spike sequences, we perform clustering of these sequences using both
hard and soft clustering methods and show that with our feature selection
methods, we can achieve higher F1 scores for the clusters.

    

### [[2110.09625] Personalized Speech Enhancement: New Models and Comprehensive Evaluation](http://arxiv.org/abs/2110.09625)


  Personalized speech enhancement (PSE) models utilize additional cues, such as
speaker embeddings like d-vectors, to remove background noise and interfering
speech in real-time and thus improve the speech quality of online video
conferencing systems for various acoustic scenarios. In this work, we propose
two neural networks for PSE that achieve superior performance to the previously
proposed VoiceFilter. In addition, we create test sets that capture a variety
of scenarios that users can encounter during video conferencing. Furthermore,
we propose a new metric to measure the target speaker over-suppression (TSOS)
problem, which was not sufficiently investigated before despite its critical
importance in deployment. Besides, we propose multi-task training with a speech
recognition back-end. Our results show that the proposed models can yield
better speech recognition accuracy, speech intelligibility, and perceptual
quality than the baseline models, and the multi-task training can alleviate the
TSOS issue in addition to improving the speech recognition accuracy.

    

### [[2110.09626] A cautionary tale on fitting decision trees to data from additive models: generalization lower bounds](http://arxiv.org/abs/2110.09626)


  Decision trees are important both as interpretable models amenable to
high-stakes decision-making, and as building blocks of ensemble methods such as
random forests and gradient boosting. Their statistical properties, however,
are not well understood. The most cited prior works have focused on deriving
pointwise consistency guarantees for CART in a classical nonparametric
regression setting. We take a different approach, and advocate studying the
generalization performance of decision trees with respect to different
generative regression models. This allows us to elicit their inductive bias,
that is, the assumptions the algorithms make (or do not make) to generalize to
new data, thereby guiding practitioners on when and how to apply these methods.
In this paper, we focus on sparse additive generative models, which have both
low statistical complexity and some nonparametric flexibility. We prove a sharp
squared error generalization lower bound for a large class of decision tree
algorithms fitted to sparse additive models with $C^1$ component functions.
This bound is surprisingly much worse than the minimax rate for estimating such
sparse additive models. The inefficiency is due not to greediness, but to the
loss in power for detecting global structure when we average responses solely
over each leaf, an observation that suggests opportunities to improve
tree-based algorithms, for example, by hierarchical shrinkage. To prove these
bounds, we develop new technical machinery, establishing a novel connection
between decision tree estimation and rate-distortion theory, a sub-field of
information theory.

    

### [[2110.09646] Monotonic Simultaneous Translation with Chunk-wise Reordering and Refinement](http://arxiv.org/abs/2110.09646)


  Recent work in simultaneous machine translation is often trained with
conventional full sentence translation corpora, leading to either excessive
latency or necessity to anticipate as-yet-unarrived words, when dealing with a
language pair whose word orders significantly differ. This is unlike human
simultaneous interpreters who produce largely monotonic translations at the
expense of the grammaticality of a sentence being translated. In this paper, we
thus propose an algorithm to reorder and refine the target side of a full
sentence translation corpus, so that the words/phrases between the source and
target sentences are aligned largely monotonically, using word alignment and
non-autoregressive neural machine translation. We then train a widely used
wait-k simultaneous translation model on this reordered-and-refined corpus. The
proposed approach improves BLEU scores and resulting translations exhibit
enhanced monotonicity with source sentences.

    

### [[2110.09647] Relational Neural Markov Random Fields](http://arxiv.org/abs/2110.09647)


  Statistical Relational Learning (SRL) models have attracted significant
attention due to their ability to model complex data while handling
uncertainty. However, most of these models have been limited to discrete
domains due to their limited potential functions. We introduce Relational
Neural Markov Random Fields (RN-MRFs) which allow for handling of complex
relational hybrid domains. The key advantage of our model is that it makes
minimal data distributional assumptions and can seamlessly allow for human
knowledge through potentials or relational rules. We propose a maximum
pseudolikelihood estimation-based learning algorithm with importance sampling
for training the neural potential parameters. Our empirical evaluations across
diverse domains such as image processing and relational object mapping, clearly
demonstrate its effectiveness against non-neural counterparts.

    

### [[2110.09658] System Norm Regularization Methods for Koopman Operator Approximation](http://arxiv.org/abs/2110.09658)


  Approximating the Koopman operator from data is numerically challenging when
many lifting functions are considered. Even low-dimensional systems can yield
unstable or ill-conditioned results in a high-dimensional lifted space. In this
paper, Extended DMD and DMD with control, two popular methods for approximating
the Koopman operator, are reformulated as convex optimization problems with
linear matrix inequality constraints. Both hard asymptotic stability
constraints and system norm regularizers are considered as methods to improve
the numerical conditioning of the approximate Koopman operator. In particular,
the $\mathcal{H}_\infty$ norm is used as a regularizer to penalize the
input-output gain of the linear system defined by the Koopman operator.
Weighting functions are then applied to penalize the system gain at particular
frequencies.

    

### [[2110.09660] BEV-SGD: Best Effort Voting SGD for Analog Aggregation Based Federated Learning against Byzantine Attackers](http://arxiv.org/abs/2110.09660)


  As a promising distributed learning technology, analog aggregation based
federated learning over the air (FLOA) provides high communication efficiency
and privacy provisioning in edge computing paradigm. When all edge devices
(workers) simultaneously upload their local updates to the parameter server
(PS) through the commonly shared time-frequency resources, the PS can only
obtain the averaged update rather than the individual local ones. As a result,
such a concurrent transmission and aggregation scheme reduces the latency and
costs of communication but makes FLOA vulnerable to Byzantine attacks which
then degrade FLOA performance. For the design of Byzantine-resilient FLOA, this
paper starts from analyzing the channel inversion (CI) power control mechanism
that is widely used in existing FLOA literature. Our theoretical analysis
indicates that although CI can achieve good learning performance in the
non-attacking scenarios, it fails to work well with limited defensive
capability to Byzantine attacks. Then, we propose a novel defending scheme
called best effort voting (BEV) power control policy integrated with stochastic
gradient descent (SGD). Our BEV-SGD improves the robustness of FLOA to
Byzantine attacks, by allowing all the workers to send their local updates at
their maximum transmit power. Under the strongest-attacking circumstance, we
derive the expected convergence rates of FLOA with CI and BEV power control
policies, respectively. The rate comparison reveals that our BEV-SGD
outperforms its counterpart with CI in terms of better convergence behavior,
which is verified by experimental simulations.

    

### [[2110.09670] Private measurement of nonlinear correlations between data hosted across multiple parties](http://arxiv.org/abs/2110.09670)


  We introduce a differentially private method to measure nonlinear
correlations between sensitive data hosted across two entities. We provide
utility guarantees of our private estimator. Ours is the first such private
estimator of nonlinear correlations, to the best of our knowledge within a
multi-party setup. The important measure of nonlinear correlation we consider
is distance correlation. This work has direct applications to private feature
screening, private independence testing, private k-sample tests, private
multi-party causal inference and private data synthesis in addition to
exploratory data analysis. Code access: A link to publicly access the code is
provided in the supplementary file.

    

### [[2110.09672] A survey on active noise control techniques -- Part II: Nonlinear systems](http://arxiv.org/abs/2110.09672)


  Part I of this paper reviewed the development of the linear active noise
control (ANC) technique in the past decade. However, ANC systems might have to
deal with some nonlinear components and the performance of linear ANC
techniques may degrade in this scenario. To overcome this limitation, nonlinear
ANC (NLANC) algorithms were developed. In Part II, we review the development of
NLANC algorithms during the last decade. The contributions of heuristic ANC
algorithms are outlined. Moreover, we emphasize recent advances of NLANC
algorithms, such as spline ANC algorithms, kernel adaptive filters, and
nonlinear distributed ANC algorithms. Then, we present recent applications of
ANC technique including linear and nonlinear perspectives. Future research
challenges regarding ANC techniques are also discussed.

    

### [[2110.09674] Adaptive Distillation: Aggregating Knowledge from Multiple Paths for Efficient Distillation](http://arxiv.org/abs/2110.09674)


  Knowledge Distillation is becoming one of the primary trends among neural
network compression algorithms to improve the generalization performance of a
smaller student model with guidance from a larger teacher model. This momentous
rise in applications of knowledge distillation is accompanied by the
introduction of numerous algorithms for distilling the knowledge such as soft
targets and hint layers. Despite this advancement in different techniques for
distilling the knowledge, the aggregation of different paths for distillation
has not been studied comprehensively. This is of particular significance, not
only because different paths have different importance, but also due to the
fact that some paths might have negative effects on the generalization
performance of the student model. Hence, we need to adaptively adjust the
importance of each path to maximize the impact of distillation on the student
model. In this paper, we explore different approaches for aggregating these
different paths and introduce our proposed adaptive approach based on multitask
learning methods. We empirically demonstrate the effectiveness of the proposed
approach over other baselines on the applications of knowledge distillation in
classification, semantic segmentation, and object detection tasks.

    

### [[2110.09677] Accelerated Graph Learning from Smooth Signals](http://arxiv.org/abs/2110.09677)


  We consider network topology identification subject to a signal smoothness
prior on the nodal observations. A fast dual-based proximal gradient algorithm
is developed to efficiently tackle a strongly convex, smoothness-regularized
network inverse problem known to yield high-quality graph solutions. Unlike
existing solvers, the novel iterations come with global convergence rate
guarantees and do not require additional step-size tuning. Reproducible
simulated tests demonstrate the effectiveness of the proposed method in
accurately recovering random and real-world graphs, markedly faster than
state-of-the-art alternatives and without incurring an extra computational
burden.

    

### [[2110.09680] Multilevel Stochastic Optimization for Imputation in Massive Medical Data Records](http://arxiv.org/abs/2110.09680)


  Exploration and analysis of massive datasets has recently generated
increasing interest in the research and development communities. It has long
been a recognized problem that many datasets contain significant levels of
missing numerical data. We introduce a mathematically principled stochastic
optimization imputation method based on the theory of Kriging. This is shown to
be a powerful method for imputation. However, its computational effort and
potential numerical instabilities produce costly and/or unreliable predictions,
potentially limiting its use on large scale datasets. In this paper, we apply a
recently developed multi-level stochastic optimization approach to the problem
of imputation in massive medical records. The approach is based on
computational applied mathematics techniques and is highly accurate. In
particular, for the Best Linear Unbiased Predictor (BLUP) this multi-level
formulation is exact, and is also significantly faster and more numerically
stable. This permits practical application of Kriging methods to data
imputation problems for massive datasets. We test this approach on data from
the National Inpatient Sample (NIS) data records, Healthcare Cost and
Utilization Project (HCUP), Agency for Healthcare Research and Quality.
Numerical results show the multi-level method significantly outperforms current
approaches and is numerically robust. In particular, it has superior accuracy
as compared with methods recommended in the recent report from HCUP on the
important problem of missing data, which could lead to sub-optimal and poorly
based funding policy decisions. In comparative benchmark tests it is shown that
the multilevel stochastic method is significantly superior to recommended
methods in the report, including Predictive Mean Matching (PMM) and Predicted
Posterior Distribution (PPD), with up to 75% reductions in error.

    

### [[2110.09681] Permutation invariant graph-to-sequence model for template-free retrosynthesis and reaction prediction](http://arxiv.org/abs/2110.09681)


  Synthesis planning and reaction outcome prediction are two fundamental
problems in computer-aided organic chemistry for which a variety of data-driven
approaches have emerged. Natural language approaches that model each problem as
a SMILES-to-SMILES translation lead to a simple end-to-end formulation, reduce
the need for data preprocessing, and enable the use of well-optimized machine
translation model architectures. However, SMILES representations are not an
efficient representation for capturing information about molecular structures,
as evidenced by the success of SMILES augmentation to boost empirical
performance. Here, we describe a novel Graph2SMILES model that combines the
power of Transformer models for text generation with the permutation invariance
of molecular graph encoders that mitigates the need for input data
augmentation. As an end-to-end architecture, Graph2SMILES can be used as a
drop-in replacement for the Transformer in any task involving
molecule(s)-to-molecule(s) transformations. In our encoder, an
attention-augmented directed message passing neural network (D-MPNN) captures
local chemical environments, and the global attention encoder allows for
long-range and intermolecular interactions, enhanced by graph-aware positional
embedding. Graph2SMILES improves the top-1 accuracy of the Transformer
baselines by $1.7\%$ and $1.9\%$ for reaction outcome prediction on USPTO_480k
and USPTO_STEREO datasets respectively, and by $9.8\%$ for one-step
retrosynthesis on the USPTO_50k dataset.

    

### [[2110.09687] Data Driven Prediction of Battery Cycle Life Before Capacity Degradation](http://arxiv.org/abs/2110.09687)


  Ubiquitous use of lithium-ion batteries across multiple industries presents
an opportunity to explore cost saving initiatives as the price to performance
ratio continually decreases in a competitive environment. Manufacturers using
lithium-ion batteries ranging in applications from mobile phones to electric
vehicles need to know how long batteries will last for a given service life. To
understand this, expensive testing is required.
This paper utilizes the data and methods implemented by Kristen A. Severson,
et al, to explore the methodologies that the research team used and presents
another method to compare predicted results vs. actual test data for battery
capacity fade. The fundamental effort is to find out if machine learning
techniques may be trained to use early life cycle data in order to accurately
predict battery capacity over the battery life cycle. Results show comparison
of methods between Gaussian Process Regression (GPR) and Elastic Net Regression
(ENR) and highlight key data features used from the extensive dataset found in
the work of Severson, et al.

    

### [[2110.09695] Tackling Dynamics in Federated Incremental Learning with Variational Embedding Rehearsal](http://arxiv.org/abs/2110.09695)


  Federated Learning is a fast growing area of ML where the training datasets
are extremely distributed, all while dynamically changing over time. Models
need to be trained on clients' devices without any guarantees for either
homogeneity or stationarity of the local private data. The need for continual
training has also risen, due to the ever-increasing production of in-task data.
However, pursuing both directions at the same time is challenging, since client
data privacy is a major constraint, especially for rehearsal methods. Herein,
we propose a novel algorithm to address the incremental learning process in an
FL scenario, based on realistic client enrollment scenarios where clients can
drop in or out dynamically. We first propose using deep Variational Embeddings
that secure the privacy of the client data. Second, we propose a server-side
training method that enables a model to rehearse the previously learnt
knowledge. Finally, we investigate the performance of federated incremental
learning in dynamic client enrollment scenarios. The proposed method shows
parity with offline training on domain-incremental learning, addressing
challenges in both the dynamic enrollment of clients and the domain shifting of
client data.

    

### [[2110.09697] abess: A Fast Best Subset Selection Library in Python and R](http://arxiv.org/abs/2110.09697)


  We introduce a new library named abess that implements a unified framework of
best-subset selection for solving diverse machine learning problems, e.g.,
linear regression, classification, and principal component analysis.
Particularly, the abess certifiably gets the optimal solution within polynomial
times under the linear model. Our efficient implementation allows abess to
attain the solution of best-subset selection problems as fast as or even 100x
faster than existing competing variable (model) selection toolboxes.
Furthermore, it supports common variants like best group subset selection and
$\ell_2$ regularized best-subset selection. The core of the library is
programmed in C++. For ease of use, a Python library is designed for
conveniently integrating with scikit-learn, and it can be installed from the
Python library Index. In addition, a user-friendly R library is available at
the Comprehensive R Archive Network. The source code is available at:
this https URL.

    

### [[2110.09712] Balancing Value Underestimation and Overestimationwith Realistic Actor-Critic](http://arxiv.org/abs/2110.09712)


  Model-free deep reinforcement learning (RL) has been successfully applied to
challenging continuous control domains. However, poor sample efficiency
prevents these methods from being widely used in real-world domains. This paper
introduces a novel model-free algorithm, Realistic Actor-Critic(RAC), which can
be incorporated with any off-policy RL algorithms to improve sample efficiency.
RAC employs Universal Value Function Approximators (UVFA) to simultaneously
learn a policy family with the same neural network, each with different
trade-offs between underestimation and overestimation. To learn such policies,
we introduce uncertainty punished Q-learning, which uses uncertainty from the
ensembling of multiple critics to build various confidence-bounds of
Q-function. We evaluate RAC on the MuJoCo benchmark, achieving 10x sample
efficiency and 25% performance improvement on the most challenging Humanoid
environment compared to SAC.

    

### [[2110.09714] Black-box Adversarial Attacks on Commercial Speech Platforms with Minimal Information](http://arxiv.org/abs/2110.09714)


  Adversarial attacks against commercial black-box speech platforms, including
cloud speech APIs and voice control devices, have received little attention
until recent years. The current "black-box" attacks all heavily rely on the
knowledge of prediction/confidence scores to craft effective adversarial
examples, which can be intuitively defended by service providers without
returning these messages. In this paper, we propose two novel adversarial
attacks in more practical and rigorous scenarios. For commercial cloud speech
APIs, we propose Occam, a decision-only black-box adversarial attack, where
only final decisions are available to the adversary. In Occam, we formulate the
decision-only AE generation as a discontinuous large-scale global optimization
problem, and solve it by adaptively decomposing this complicated problem into a
set of sub-problems and cooperatively optimizing each one. Our Occam is a
one-size-fits-all approach, which achieves 100% success rates of attacks with
an average SNR of 14.23dB, on a wide range of popular speech and speaker
recognition APIs, including Google, Alibaba, Microsoft, Tencent, iFlytek, and
Jingdong, outperforming the state-of-the-art black-box attacks. For commercial
voice control devices, we propose NI-Occam, the first non-interactive physical
adversarial attack, where the adversary does not need to query the oracle and
has no access to its internal information and training data. We combine
adversarial attacks with model inversion attacks, and thus generate the
physically-effective audio AEs with high transferability without any
interaction with target devices. Our experimental results show that NI-Occam
can successfully fool Apple Siri, Microsoft Cortana, Google Assistant, iFlytek
and Amazon Echo with an average SRoA of 52% and SNR of 9.65dB, shedding light
on non-interactive physical attacks against voice control devices.

    

### [[2110.09722] Batched Lipschitz Bandits](http://arxiv.org/abs/2110.09722)


  In this paper, we study the batched Lipschitz bandit problem, where the
expected reward is Lipschitz and the reward observations are collected in
batches. We introduce a novel landscape-aware algorithm, called Batched
Lipschitz Narrowing (BLiN), that naturally fits into the batched feedback
setting. In particular, we show that for a $T$-step problem with Lipschitz
reward of zooming dimension $d_z$, our algorithm achieves theoretically optimal
regret rate of $ \widetilde{\mathcal{O}} \left( T^{\frac{d_z + 1}{d_z + 2}}
\right) $ using only $ \mathcal{O} \left( \frac{\log T}{d_z} \right) $ batches.
For the lower bound, we show that in an environment with $B$-batches, for any
policy $\pi$, there exists a problem instance such that the expected regret is
lower bounded by $ \widetilde{\Omega}
\left(R_z(T)^\frac{1}{1-\left(\frac{1}{d+2}\right)^B}\right) $, where $R_z (T)$
is the regret lower bound for vanilla Lipschitz bandits that depends on the
zooming dimension $d_z$, and $d$ is the dimension of the arm space.

    

### [[2110.09726] CGNN: Traffic Classification with Graph Neural Network](http://arxiv.org/abs/2110.09726)


  Traffic classification associates packet streams with known application
labels, which is vital for network security and network management. With the
rise of NAT, port dynamics, and encrypted traffic, it is increasingly
challenging to obtain unified traffic features for accurate classification.
Many state-of-the-art traffic classifiers automatically extract features from
the packet stream based on deep learning models such as convolution networks.
Unfortunately, the compositional and causal relationships between packets are
not well extracted in these deep learning models, which affects both prediction
accuracy and generalization on different traffic types.
In this paper, we present a chained graph model on the packet stream to keep
the chained compositional sequence. Next, we propose CGNN, a graph neural
network based traffic classification method, which builds a graph classifier
over automatically extracted features over the chained graph.
Extensive evaluation over real-world traffic data sets, including normal,
encrypted and malicious labels, show that, CGNN improves the prediction
accuracy by 23\% to 29\% for application classification, by 2\% to 37\% for
malicious traffic classification, and reaches the same accuracy level for
encrypted traffic classification. CGNN is quite robust in terms of the recall
and precision metrics. We have extensively evaluated the parameter sensitivity
of CGNN, which yields optimized parameters that are quite effective for traffic
classification.

    

### [[2110.09738] Faster Rates for the Frank-Wolfe Algorithm Using Jacobi Polynomials](http://arxiv.org/abs/2110.09738)


  The Frank Wolfe algorithm (FW) is a popular projection-free alternative for
solving large-scale constrained optimization problems. However, the FW
algorithm suffers from a sublinear convergence rate when minimizing a smooth
convex function over a compact convex set. Thus, exploring techniques that
yield a faster convergence rate becomes crucial. A classic approach to obtain
faster rates is to combine previous iterates to obtain the next iterate. In
this work, we extend this approach to the FW setting and show that the optimal
way to combine the past iterates is using a set of orthogonal Jacobi
polynomials. We also a polynomial-based acceleration technique, referred to as
Jacobi polynomial accelerated FW, which combines the current iterate with the
past iterate using combing weights related to the Jacobi recursion. By
carefully choosing parameters of the Jacobi polynomials, we obtain a faster
sublinear convergence rate. We provide numerical experiments on real datasets
to demonstrate the efficacy of the proposed algorithm.

    

### [[2110.09741] Trajectory Prediction with Linguistic Representations](http://arxiv.org/abs/2110.09741)


  Language allows humans to build mental models that interpret what is
happening around them resulting in more accurate long-term predictions. We
present a novel trajectory prediction model that uses linguistic intermediate
representations to forecast trajectories, and is trained using trajectory
samples with partially annotated captions. The model learns the meaning of each
of the words without direct per-word supervision. At inference time, it
generates a linguistic description of trajectories which captures maneuvers and
interactions over an extended time interval. This generated description is used
to refine predictions of the trajectories of multiple agents. We train and
validate our model on the Argoverse dataset, and demonstrate improved accuracy
results in trajectory prediction. In addition, our model is more interpretable:
it presents part of its reasoning in plain language as captions, which can aid
model development and can aid in building confidence in the model before
deploying it.

    

### [[2110.09744] Spectral Variability Augmented Sparse Unmixing of Hyperspectral Images](http://arxiv.org/abs/2110.09744)


  Spectral unmixing (SU) expresses the mixed pixels existed in hyperspectral
images as the product of endmember and abundance, which has been widely used in
hyperspectral imagery analysis. However, the influence of light, acquisition
conditions and the inherent properties of materials, results in that the
identified endmembers can vary spectrally within a given image (construed as
spectral variability). To address this issue, recent methods usually use a
priori obtained spectral library to represent multiple characteristic spectra
of the same object, but few of them extracted the spectral variability
explicitly. In this paper, a spectral variability augmented sparse unmixing
model (SVASU) is proposed, in which the spectral variability is extracted for
the first time. The variable spectra are divided into two parts of intrinsic
spectrum and spectral variability for spectral reconstruction, and modeled
synchronously in the SU model adding the regular terms restricting the sparsity
of abundance and the generalization of the variability coefficient. It is noted
that the spectral variability library and the intrinsic spectral library are
all constructed from the In-situ observed image. Experimental results over both
synthetic and real-world data sets demonstrate that the augmented decomposition
by spectral variability significantly improves the unmixing performance than
the decomposition only by spectral library, as well as compared to
state-of-the-art algorithms.

    

### [[2110.09759] A Regularization Method to Improve Adversarial Robustness of Neural Networks for ECG Signal Classification](http://arxiv.org/abs/2110.09759)


  Electrocardiogram (ECG) is the most widely used diagnostic tool to monitor
the condition of the human heart. By using deep neural networks (DNNs),
interpretation of ECG signals can be fully automated for the identification of
potential abnormalities in a patient's heart in a fraction of a second. Studies
have shown that given a sufficiently large amount of training data, DNN
accuracy for ECG classification could reach human-expert cardiologist level.
However, despite of the excellent performance in classification accuracy, DNNs
are highly vulnerable to adversarial noises that are subtle changes in the
input of a DNN and may lead to a wrong class-label prediction. It is
challenging and essential to improve robustness of DNNs against adversarial
noises, which are a threat to life-critical applications. In this work, we
proposed a regularization method to improve DNN robustness from the perspective
of noise-to-signal ratio (NSR) for the application of ECG signal
classification. We evaluated our method on PhysioNet MIT-BIH dataset and
CPSC2018 ECG dataset, and the results show that our method can substantially
enhance DNN robustness against adversarial noises generated from adversarial
attacks, with a minimal change in accuracy on clean data.

    

### [[2110.09767] Pre and Post Counting for Scalable Statistical-Relational Model Discovery](http://arxiv.org/abs/2110.09767)


  Statistical-Relational Model Discovery aims to find statistically relevant
patterns in relational data. For example, a relational dependency pattern may
stipulate that a user's gender is associated with the gender of their friends.
As with propositional (non-relational) graphical models, the major scalability
bottleneck for model discovery is computing instantiation counts: the number of
times a relational pattern is instantiated in a database. Previous work on
propositional learning utilized pre-counting or post-counting to solve this
task. This paper takes a detailed look at the memory and speed trade-offs
between pre-counting and post-counting strategies for relational learning. A
pre-counting approach computes and caches instantiation counts for a large set
of relational patterns before model search. A post-counting approach computes
an instantiation count dynamically on-demand for each candidate pattern
generated during the model search. We describe a novel hybrid approach,
tailored to relational data, that achieves a sweet spot with pre-counting for
patterns involving positive relationships (e.g. pairs of users who are friends)
and post-counting for patterns involving negative relationships (e.g. pairs of
users who are not friends). Our hybrid approach scales model discovery to
millions of data facts.

    

### [[2110.09770] AEFE: Automatic Embedded Feature Engineering for Categorical Features](http://arxiv.org/abs/2110.09770)


  The challenge of solving data mining problems in e-commerce applications such
as recommendation system (RS) and click-through rate (CTR) prediction is how to
make inferences by constructing combinatorial features from a large number of
categorical features while preserving the interpretability of the method. In
this paper, we propose Automatic Embedded Feature Engineering(AEFE), an
automatic feature engineering framework for representing categorical features,
which consists of various components including custom paradigm feature
construction and multiple feature selection. By selecting the potential field
pairs intelligently and generating a series of interpretable combinatorial
features, our framework can provide a set of unseen generated features for
enhancing model performance and then assist data analysts in discovering the
feature importance for particular data mining tasks. Furthermore, AEFE is
distributed implemented by task-parallelism, data sampling, and searching
schema based on Matrix Factorization field combination, to optimize the
performance and enhance the efficiency and scalability of the framework.
Experiments conducted on some typical e-commerce datasets indicate that our
method outperforms the classical machine learning models and state-of-the-art
deep learning models.

    

### [[2110.09771] On Reward-Free RL with Kernel and Neural Function Approximations: Single-Agent MDP and Markov Game](http://arxiv.org/abs/2110.09771)


  To achieve sample efficiency in reinforcement learning (RL), it necessitates
efficiently exploring the underlying environment. Under the offline setting,
addressing the exploration challenge lies in collecting an offline dataset with
sufficient coverage. Motivated by such a challenge, we study the reward-free RL
problem, where an agent aims to thoroughly explore the environment without any
pre-specified reward function. Then, given any extrinsic reward, the agent
computes the policy via a planning algorithm with offline data collected in the
exploration phase. Moreover, we tackle this problem under the context of
function approximation, leveraging powerful function approximators.
Specifically, we propose to explore via an optimistic variant of the
value-iteration algorithm incorporating kernel and neural function
approximations, where we adopt the associated exploration bonus as the
exploration reward. Moreover, we design exploration and planning algorithms for
both single-agent MDPs and zero-sum Markov games and prove that our methods can
achieve $\widetilde{\mathcal{O}}(1 /\varepsilon^2)$ sample complexity for
generating a $\varepsilon$-suboptimal policy or $\varepsilon$-approximate Nash
equilibrium when given an arbitrary extrinsic reward. To the best of our
knowledge, we establish the first provably efficient reward-free RL algorithm
with kernel and neural function approximators.

    

### [[2110.09778] Explaining Deep Tractable Probabilistic Models: The sum-product network case](http://arxiv.org/abs/2110.09778)


  We consider the problem of explaining a tractable deep probabilistic model,
the Sum-Product Networks (SPNs).To this effect, we define the notion of a
context-specific independence tree and present an iterative algorithm that
converts an SPN to a CSI-tree. The resulting CSI-tree is both interpretable and
explainable to the domain expert. To further compress the tree, we approximate
the CSIs by fitting a supervised classifier. Our extensive empirical
evaluations on synthetic, standard, and real-world clinical data sets
demonstrate that the resulting models exhibit superior explainability without
loss in performance.

    

### [[2110.09795] Geo-DefakeHop: High-Performance Geographic Fake Image Detection](http://arxiv.org/abs/2110.09795)


  A robust fake satellite image detection method, called Geo-DefakeHop, is
proposed in this work. Geo-DefakeHop is developed based on the parallel
subspace learning (PSL) methodology. PSL maps the input image space into
several feature subspaces using multiple filter banks. By exploring response
differences of different channels between real and fake images for a filter
bank, Geo-DefakeHop learns the most discriminant channels and uses their soft
decision scores as features. Then, Geo-DefakeHop selects a few discriminant
features from each filter bank and ensemble them to make a final binary
decision. Geo-DefakeHop offers a light-weight high-performance solution to fake
satellite images detection. Its model size is analyzed, which ranges from 0.8
to 62K parameters. Furthermore, it is shown by experimental results that it
achieves an F1-score higher than 95\% under various common image manipulations
such as resizing, compression and noise corruption.

    

### [[2110.09796] Offline Reinforcement Learning with Value-based Episodic Memory](http://arxiv.org/abs/2110.09796)


  Offline reinforcement learning (RL) shows promise of applying RL to
real-world problems by effectively utilizing previously collected data. Most
existing offline RL algorithms use regularization or constraints to suppress
extrapolation error for actions outside the dataset. In this paper, we adopt a
different framework, which learns the V-function instead of the Q-function to
naturally keep the learning procedure within the support of an offline dataset.
To enable effective generalization while maintaining proper conservatism in
offline learning, we propose Expectile V-Learning (EVL), which smoothly
interpolates between the optimal value learning and behavior cloning. Further,
we introduce implicit planning along offline trajectories to enhance learned
V-values and accelerate convergence. Together, we present a new offline method
called Value-based Episodic Memory (VEM). We provide theoretical analysis for
the convergence properties of our proposed VEM method, and empirical results in
the D4RL benchmark show that our method achieves superior performance in most
tasks, particularly in sparse-reward tasks.

    

### [[2110.09803] Latent reweighting, an almost free improvement for GANs](http://arxiv.org/abs/2110.09803)


  Standard formulations of GANs, where a continuous function deforms a
connected latent space, have been shown to be misspecified when fitting
different classes of images. In particular, the generator will necessarily
sample some low-quality images in between the classes. Rather than modifying
the architecture, a line of works aims at improving the sampling quality from
pre-trained generators at the expense of increased computational cost. Building
on this, we introduce an additional network to predict latent importance
weights and two associated sampling methods to avoid the poorest samples. This
idea has several advantages: 1) it provides a way to inject disconnectedness
into any GAN architecture, 2) since the rejection happens in the latent space,
it avoids going through both the generator and the discriminator, saving
computation time, 3) this importance weights formulation provides a principled
way to reduce the Wasserstein's distance to the target distribution. We
demonstrate the effectiveness of our method on several datasets, both synthetic
and high-dimensional.

    

### [[2110.09807] Learning to Learn Graph Topologies](http://arxiv.org/abs/2110.09807)


  Learning a graph topology to reveal the underlying relationship between data
entities plays an important role in various machine learning and data analysis
tasks. Under the assumption that structured data vary smoothly over a graph,
the problem can be formulated as a regularised convex optimisation over a
positive semidefinite cone and solved by iterative algorithms. Classic methods
require an explicit convex function to reflect generic topological priors, e.g.
the $\ell_1$ penalty for enforcing sparsity, which limits the flexibility and
expressiveness in learning rich topological structures. We propose to learn a
mapping from node data to the graph structure based on the idea of learning to
optimise (L2O). Specifically, our model first unrolls an iterative primal-dual
splitting algorithm into a neural network. The key structural proximal
projection is replaced with a variational autoencoder that refines the
estimated graph with enhanced topological properties. The model is trained in
an end-to-end fashion with pairs of node data and graph samples. Experiments on
both synthetic and real-world data demonstrate that our model is more efficient
than classic iterative algorithms in learning a graph with specific topological
properties.

    

### [[2110.09813] Multi-Objective Loss Balancing for Physics-Informed Deep Learning](http://arxiv.org/abs/2110.09813)


  Physics Informed Neural Networks (PINN) are algorithms from deep learning
leveraging physical laws by including partial differential equations (PDE)
together with a respective set of boundary and initial conditions (BC / IC) as
penalty terms into their loss function. As the PDE, BC and IC loss function
parts can significantly differ in magnitudes, due to their underlying physical
units or stochasticity of initialisation, training of PINNs may suffer from
severe convergence and efficiency problems, causing PINNs to stay beyond
desirable approximation quality. In this work, we observe the significant role
of correctly weighting the combination of multiple competitive loss functions
for training PINNs effectively. To that end, we implement and evaluate
different methods aiming at balancing the contributions of multiple terms of
the PINNs loss function and their gradients. After review of three existing
loss scaling approaches (Learning Rate Annealing, GradNorm as well as
SoftAdapt), we propose a novel self-adaptive loss balancing of PINNs called
ReLoBRaLo (Relative Loss Balancing with Random Lookback). Finally, the
performance of ReLoBRaLo is compared and verified against these approaches by
solving both forward as well as inverse problems on three benchmark PDEs for
PINNs: Burgers' equation, Kirchhoff's plate bending equation and Helmholtz's
equation. Our simulation studies show that ReLoBRaLo training is much faster
and achieves higher accuracy than training PINNs with other balancing methods
and hence is very effective and increases sustainability of PINNs algorithms.
The adaptability of ReLoBRaLo illustrates robustness across different PDE
problem settings. The proposed method can also be employed to the wider class
of penalised optimisation problems, including PDE-constrained and Sobolev
training apart from the studied PINNs examples.

    

### [[2110.09817] State-based Episodic Memory for Multi-Agent Reinforcement Learning](http://arxiv.org/abs/2110.09817)


  Multi-agent reinforcement learning (MARL) algorithms have made promising
progress in recent years by leveraging the centralized training and
decentralized execution (CTDE) paradigm. However, existing MARL algorithms
still suffer from the sample inefficiency problem. In this paper, we propose a
simple yet effective approach, called state-based episodic memory (SEM), to
improve sample efficiency in MARL. SEM adopts episodic memory (EM) to supervise
the centralized training procedure of CTDE in MARL. To the best of our
knowledge, SEM is the first work to introduce EM into MARL. We can
theoretically prove that, when using for MARL, SEM has lower space complexity
and time complexity than state and action based EM (SAEM), which is originally
proposed for single-agent reinforcement learning. Experimental results on
StarCraft multi-agent challenge (SMAC) show that introducing episodic memory
into MARL can improve sample efficiency and SEM can reduce storage cost and
time cost compared with SAEM.

    

### [[2110.09823] Extensive Deep Temporal Point Process](http://arxiv.org/abs/2110.09823)


  Temporal point process as the stochastic process on continuous domain of time
is usually used to model the asynchronous event sequence featuring with
occurence timestamps. With the rise of deep learning, due to the strong
expressivity of deep neural networks, they are emerging as a promising choice
for capturing the patterns in asynchronous sequences, in the setting of
temporal point process. In this paper, we first review recent research emphasis
and difficulties in modeling asynchronous event sequences with deep temporal
point process, which can be concluded into four fields: encoding of history
sequence, formulation of conditional intensity function, relational discovery
of events and learning approaches for optimization. We introduce most of
recently proposed models by dismantling them as the four parts, and conduct
experiments by remodularizing the first three parts with the same learning
strategy for a fair empirical evaluation. Besides, we extend the history
encoders and conditional intensity function family, and propose a Granger
causality discovery framework for exploiting the relations among multi-types of
events. Discrete graph structure learning in the framework of Variational
Inference is employed to reveal latent structures of Granger causality graph,
and further experiments shows the proposed framework with learned latent graph
can both capture the relations and achieve an improved fitting and predicting
performance.

    

### [[2110.09839] Measuring Hidden Bias within Face Recognition via Racial Phenotypes](http://arxiv.org/abs/2110.09839)


  Recent work reports disparate performance for intersectional racial groups
across face recognition tasks: face verification and identification. However,
the definition of those racial groups has a significant impact on the
underlying findings of such racial bias analysis. Previous studies define these
groups based on either demographic information (e.g. African, Asian etc.) or
skin tone (e.g. lighter or darker skins). The use of such sensitive or broad
group definitions has disadvantages for bias investigation and subsequent
counter-bias solutions design. By contrast, this study introduces an
alternative racial bias analysis methodology via facial phenotype attributes
for face recognition. We use the set of observable characteristics of an
individual face where a race-related facial phenotype is hence specific to the
human face and correlated to the racial profile of the subject. We propose
categorical test cases to investigate the individual influence of those
attributes on bias within face recognition tasks. We compare our
phenotype-based grouping methodology with previous grouping strategies and show
that phenotype-based groupings uncover hidden bias without reliance upon any
potentially protected attributes or ill-defined grouping strategies.
Furthermore, we contribute corresponding phenotype attribute category labels
for two face recognition tasks: RFW for face verification and VGGFace2 (test
set) for face identification.

    

### [[2110.09843] AequeVox: Automated Fairness Testing of Speech Recognition Systems](http://arxiv.org/abs/2110.09843)


  Automatic Speech Recognition (ASR) systems have become ubiquitous. They can
be found in a variety of form factors and are increasingly important in our
daily lives. As such, ensuring that these systems are equitable to different
subgroups of the population is crucial. In this paper, we introduce, AequeVox,
an automated testing framework for evaluating the fairness of ASR systems.
AequeVox simulates different environments to assess the effectiveness of ASR
systems for different populations. In addition, we investigate whether the
chosen simulations are comprehensible to humans. We further propose a fault
localization technique capable of identifying words that are not robust to
these varying environments. Both components of AequeVox are able to operate in
the absence of ground truth data.
We evaluated AequeVox on speech from four different datasets using three
different commercial ASRs. Our experiments reveal that non-native English,
female and Nigerian English speakers generate 109%, 528.5% and 156.9% more
errors, on average than native English, male and UK Midlands speakers,
respectively. Our user study also reveals that 82.9% of the simulations
(employed through speech transformations) had a comprehensibility rating above
seven (out of ten), with the lowest rating being 6.78. This further validates
the fairness violations discovered by AequeVox. Finally, we show that the
non-robust words, as predicted by the fault localization technique embodied in
AequeVox, show 223.8% more errors than the predicted robust words across all
ASRs.

    

### [[2110.09864] Learning Pareto-Efficient Decisions with Confidence](http://arxiv.org/abs/2110.09864)


  The paper considers the problem of multi-objective decision support when
outcomes are uncertain. We extend the concept of Pareto-efficient decisions to
take into account the uncertainty of decision outcomes across varying contexts.
This enables quantifying trade-offs between decisions in terms of tail outcomes
that are relevant in safety-critical applications. We propose a method for
learning efficient decisions with statistical confidence, building on results
from the conformal prediction literature. The method adapts to weak or
nonexistent context covariate overlap and its statistical guarantees are
evaluated using both synthetic and real data.

    

### [[2110.09868] Designing A Clinically Applicable Deep Recurrent Model to Identify Neuropsychiatric Symptoms in People Living with Dementia Using In-Home Monitoring Data](http://arxiv.org/abs/2110.09868)


  Agitation is one of the neuropsychiatric symptoms with high prevalence in
dementia which can negatively impact the Activities of Daily Living (ADL) and
the independence of individuals. Detecting agitation episodes can assist in
providing People Living with Dementia (PLWD) with early and timely
interventions. Analysing agitation episodes will also help identify modifiable
factors such as ambient temperature and sleep as possible components causing
agitation in an individual. This preliminary study presents a supervised
learning model to analyse the risk of agitation in PLWD using in-home
monitoring data. The in-home monitoring data includes motion sensors,
physiological measurements, and the use of kitchen appliances from 46 homes of
PLWD between April 2019-June 2021. We apply a recurrent deep learning model to
identify agitation episodes validated and recorded by a clinical monitoring
team. We present the experiments to assess the efficacy of the proposed model.
The proposed model achieves an average of 79.78% recall, 27.66% precision and
37.64% F1 scores when employing the optimal parameters, suggesting a good
ability to recognise agitation events. We also discuss using machine learning
models for analysing the behavioural patterns using continuous monitoring data
and explore clinical applicability and the choices between sensitivity and
specificity in-home monitoring applications.

    

### [[2110.09869] User-Centric Federated Learning](http://arxiv.org/abs/2110.09869)


  Data heterogeneity across participating devices poses one of the main
challenges in federated learning as it has been shown to greatly hamper its
convergence time and generalization capabilities. In this work, we address this
limitation by enabling personalization using multiple user-centric aggregation
rules at the parameter server. Our approach potentially produces a personalized
model for each user at the cost of some extra downlink communication overhead.
To strike a trade-off between personalization and communication efficiency, we
propose a broadcast protocol that limits the number of personalized streams
while retaining the essential advantages of our learning scheme. Through
simulation results, our approach is shown to enjoy higher personalization
capabilities, faster convergence, and better communication efficiency compared
to other competing baseline solutions.

    

### [[2110.09877] Two-stage Voice Application Recommender System for Unhandled Utterances in Intelligent Personal Assistant](http://arxiv.org/abs/2110.09877)


  Intelligent personal assistants (IPA) enable voice applications that
facilitate people's daily tasks. However, due to the complexity and ambiguity
of voice requests, some requests may not be handled properly by the standard
natural language understanding (NLU) component. In such cases, a simple reply
like "Sorry, I don't know" hurts the user's experience and limits the
functionality of IPA. In this paper, we propose a two-stage
shortlister-reranker recommender system to match third-party voice applications
(skills) to unhandled utterances. In this approach, a skill shortlister is
proposed to retrieve candidate skills from the skill catalog by calculating
both lexical and semantic similarity between skills and user requests. We also
illustrate how to build a new system by using observed data collected from a
baseline rule-based system, and how the exposure biases can generate
discrepancy between offline and human metrics. Lastly, we present two
relabeling methods that can handle the incomplete ground truth, and mitigate
exposure bias. We demonstrate the effectiveness of our proposed system through
extensive offline experiments. Furthermore, we present online A/B testing
results that show a significant boost on user experience satisfaction.

    

### [[2110.09887] Time Series Analysis via Network Science: Concepts and Algorithms](http://arxiv.org/abs/2110.09887)


  There is nowadays a constant flux of data being generated and collected in
all types of real world systems. These data sets are often indexed by time,
space or both requiring appropriate approaches to analyze the data. In
univariate settings, time series analysis is a mature and solid field. However,
in multivariate contexts, time series analysis still presents many limitations.
In order to address these issues, the last decade has brought approaches based
on network science. These methods involve transforming an initial time series
data set into one or more networks, which can be analyzed in depth to provide
insight into the original time series. This review provides a comprehensive
overview of existing mapping methods for transforming time series into networks
for a wide audience of researchers and practitioners in machine learning, data
mining and time series. Our main contribution is a structured review of
existing methodologies, identifying their main characteristics and their
differences. We describe the main conceptual approaches, provide authoritative
references and give insight into their advantages and limitations in a unified
notation and language. We first describe the case of univariate time series,
which can be mapped to single layer networks, and we divide the current
mappings based on the underlying concept: visibility, transition and proximity.
We then proceed with multivariate time series discussing both single layer and
multiple layer approaches. Although still very recent, this research area has
much potential and with this survey we intend to pave the way for future
research on the topic.

    

### [[2110.09888] Novel Features for Time Series Analysis: A Complex Networks Approach](http://arxiv.org/abs/2110.09888)


  Time series data are ubiquitous in several domains as climate, economics and
health care. Mining features from these time series is a crucial task with a
multidisciplinary impact. Usually, these features are obtained from structural
characteristics of time series, such as trend, seasonality and autocorrelation,
sometimes requiring data transformations and parametric models. A recent
conceptual approach relies on time series mapping to complex networks, where
the network science methodologies can help characterize time series. In this
paper, we consider two mapping concepts, visibility and transition probability
and propose network topological measures as a new set of time series features.
To evaluate the usefulness of the proposed features, we address the problem of
time series clustering. More specifically, we propose a clustering method that
consists in mapping the time series into visibility graphs and quantile graphs,
calculating global topological metrics of the resulting networks, and using
data mining techniques to form clusters. We apply this method to a data sets of
synthetic and empirical time series. The results indicate that network-based
features capture the information encoded in each of the time series models,
resulting in high accuracy in a clustering task. Our results are promising and
show that network analysis can be used to characterize different types of time
series and that different mapping methods capture different characteristics of
the time series.

    

### [[2110.09890] Multi-Modal Pre-Training for Automated Speech Recognition](http://arxiv.org/abs/2110.09890)


  Traditionally, research in automated speech recognition has focused on
local-first encoding of audio representations to predict the spoken phonemes in
an utterance. Unfortunately, approaches relying on such hyper-local information
tend to be vulnerable to both local-level corruption (such as audio-frame
drops, or loud noises) and global-level noise (such as environmental noise, or
background noise) that has not been seen during training. In this work, we
introduce a novel approach which leverages a self-supervised learning technique
based on masked language modeling to compute a global, multi-modal encoding of
the environment in which the utterance occurs. We then use a new deep-fusion
framework to integrate this global context into a traditional ASR method, and
demonstrate that the resulting method can outperform baseline methods by up to
7% on Librispeech; gains on internal datasets range from 6% (on larger models)
to 45% (on smaller models).

    

### [[2110.09899] POLE: Polarized Embedding for Signed Networks](http://arxiv.org/abs/2110.09899)


  From the 2016 U.S. presidential election to the 2021 Capitol riots to the
spread of misinformation related to COVID-19, many have blamed social media for
today's deeply divided society. Recent advances in machine learning for signed
networks hold the promise to guide small interventions with the goal of
reducing polarization in social media. However, existing models are especially
ineffective in predicting conflicts (or negative links) among users. This is
due to a strong correlation between link signs and the network structure, where
negative links between polarized communities are too sparse to be predicted
even by state-of-the-art approaches. To address this problem, we first design a
partition-agnostic polarization measure for signed graphs based on the signed
random-walk and show that many real-world graphs are highly polarized. Then, we
propose POLE (POLarized Embedding for signed networks), a signed embedding
method for polarized graphs that captures both topological and signed
similarities jointly via signed autocovariance. Through extensive experiments,
we show that POLE significantly outperforms state-of-the-art methods in signed
link prediction, particularly for negative links with gains of up to one order
of magnitude.

    

### [[2110.09902] Understanding Convolutional Neural Networks from Theoretical Perspective via Volterra Convolution](http://arxiv.org/abs/2110.09902)


  This study proposes a general and unified perspective of convolutional neural
networks by exploring the relationship between (deep) convolutional neural
networks and finite Volterra convolutions. It provides a novel approach to
explain and study the overall characteristics of neural networks without being
disturbed by the complex network architectures. Concretely, we examine the
basic structures of finite term Volterra convolutions and convolutional neural
networks. Our results show that convolutional neural network is an
approximation of the finite term Volterra convolution, whose order increases
exponentially with the number of layers and kernel size increases exponentially
with the strides. With this perspective, the specialized perturbations are
directly obtained from the approximated kernels rather than iterative generated
adversarial examples. Extensive experiments on synthetic and real-world data
sets show the correctness and effectiveness of our results.

    

### [[2110.09904] Learning Robotic Manipulation Skills Using an Adaptive Force-Impedance Action Space](http://arxiv.org/abs/2110.09904)


  Intelligent agents must be able to think fast and slow to perform elaborate
manipulation tasks. Reinforcement Learning (RL) has led to many promising
results on a range of challenging decision-making tasks. However, in real-world
robotics, these methods still struggle, as they require large amounts of
expensive interactions and have slow feedback loops. On the other hand, fast
human-like adaptive control methods can optimize complex robotic interactions,
yet fail to integrate multimodal feedback needed for unstructured tasks. In
this work, we propose to factor the learning problem in a hierarchical learning
and adaption architecture to get the best of both worlds. The framework
consists of two components, a slow reinforcement learning policy optimizing the
task strategy given multimodal observations, and a fast, real-time adaptive
control policy continuously optimizing the motion, stability, and effort of the
manipulator. We combine these components through a bio-inspired action space
that we call AFORCE. We demonstrate the new action space on a contact-rich
manipulation task on real hardware and evaluate its performance on three
simulated manipulation tasks. Our experiments show that AFORCE drastically
improves sample efficiency while reducing energy consumption and improving
safety.

    

### [[2110.09910] FedHe: Heterogeneous Models and Communication-Efficient Federated Learning](http://arxiv.org/abs/2110.09910)


  Federated learning (FL) is able to manage edge devices to cooperatively train
a model while maintaining the training data local and private. One common
assumption in FL is that all edge devices share the same machine learning model
in training, for example, identical neural network architecture. However, the
computation and store capability of different devices may not be the same.
Moreover, reducing communication overheads can improve the training efficiency
though it is still a challenging problem in FL. In this paper, we propose a
novel FL method, called FedHe, inspired by knowledge distillation, which can
train heterogeneous models and support asynchronous training processes with
significantly reduced communication overheads. Our analysis and experimental
results demonstrate that the performance of our proposed method is better than
the state-of-the-art algorithms in terms of communication overheads and model
accuracy.

    

### [[2110.09915] Entity Relation Extraction as Dependency Parsing in Visually Rich Documents](http://arxiv.org/abs/2110.09915)


  Previous works on key information extraction from visually rich documents
(VRDs) mainly focus on labeling the text within each bounding box (i.e.,
semantic entity), while the relations in-between are largely unexplored. In
this paper, we adapt the popular dependency parsing model, the biaffine parser,
to this entity relation extraction task. Being different from the original
dependency parsing model which recognizes dependency relations between words,
we identify relations between groups of words with layout information instead.
We have compared different representations of the semantic entity, different
VRD encoders, and different relation decoders. The results demonstrate that our
proposed model achieves 65.96% F1 score on the FUNSD dataset. As for the
real-world application, our model has been applied to the in-house customs
data, achieving reliable performance in the production setting.

    

### [[2110.09916] Identification of high order closure terms from fully kinetic simulations using machine learning](http://arxiv.org/abs/2110.09916)


  Simulations of large-scale plasma systems are typically based on fluid
approximations. However, these methods do not capture the small-scale physical
processes available to fully kinetic models. Traditionally, empirical closure
terms are used to express high order moments of the Boltzmann equation, e.g.
the pressure tensor and heat flux. In this paper, we propose different closure
terms extracted using machine learning techniques as an alternative. We show in
this work how two different machine learning models, a multi-layer perceptron
and a gradient boosting regressor, can synthesize higher-order moments
extracted from a fully kinetic simulation. The accuracy of the models and their
ability to generalize are evaluated and compared to a baseline model. When
trained from more extreme simulations, the models showed better extrapolation
in comparison to traditional simulations, indicating the importance of
outliers. We learn that both models can capture heat flux and pressure tensor
very well, with the gradient boosting regressor being the most stable of the
two models in terms of the accuracy. The performance of the tested models in
the regression task opens the way for new experiments in multi-scale modelling.

    

### [[2110.09919] ToFFi -- Toolbox for Frequency-based Fingerprinting of Brain Signals](http://arxiv.org/abs/2110.09919)


  Spectral fingerprints (SFs) are unique power spectra signatures of human
brain regions of interest (ROIs, Keitel & Gross, 2016). SFs allow for accurate
ROI identification and can serve as biomarkers of differences exhibited by
non-neurotypical groups. At present, there are no open-source, versatile tools
to calculate spectral fingerprints. We have filled this gap by creating a
modular, highly-configurable MATLAB Toolbox for Frequency-based Fingerprinting
(ToFFi). It can transform MEG/EEG signals into unique spectral representations
using ROIs provided by anatomical (AAL, Desikan-Killiany), functional
(Schaefer), or other custom volumetric brain parcellations. Toolbox design
supports reproducibility and parallel computations.

    

### [[2110.09927] Conditional De-Identification of 3D Magnetic Resonance Images](http://arxiv.org/abs/2110.09927)


  Privacy protection of medical image data is challenging. Even if metadata is
removed, brain scans are vulnerable to attacks that match renderings of the
face to facial image databases. Solutions have been developed to de-identify
diagnostic scans by obfuscating or removing parts of the face. However, these
solutions either fail to reliably hide the patient's identity or are so
aggressive that they impair further analyses. We propose a new class of
de-identification techniques that, instead of removing facial features,
remodels them. Our solution relies on a conditional multi-scale GAN
architecture. It takes a patient's MRI scan as input and generates a 3D volume
conditioned on the patient's brain, which is preserved exactly, but where the
face has been de-identified through remodeling. We demonstrate that our
approach preserves privacy far better than existing techniques, without
compromising downstream medical analyses. Analyses were run on the OASIS-3 and
ADNI corpora.

    

### [[2110.09928] CycleFlow: Purify Information Factors by Cycle Loss](http://arxiv.org/abs/2110.09928)


  SpeechFlow is a powerful factorization model based on information bottleneck
(IB), and its effectiveness has been reported by several studies. A potential
problem of SpeechFlow, however, is that if the IB channels are not well
designed, the resultant factors cannot be well disentangled. In this study, we
propose a CycleFlow model that combines random factor substitution and cycle
loss to solve this problem. Experiments on voice conversion tasks demonstrate
that this simple technique can effectively reduce mutual information among
individual factors, and produce clearly better conversion than the IB-based
SpeechFlow. CycleFlow can also be used as a powerful tool for speech editing.
We demonstrate this usage by an emotion perception experiment.

    

### [[2110.09929] Minimal Multi-Layer Modifications of Deep Neural Networks](http://arxiv.org/abs/2110.09929)


  Deep neural networks (DNNs) have become increasingly popular in recent years.
However, despite their many successes, DNNs may also err and produce incorrect
and potentially fatal outputs in safety-critical settings, such as autonomous
driving, medical diagnosis, and airborne collision avoidance systems. Much work
has been put into detecting such erroneous behavior in DNNs, e.g., via testing
or verification, but removing these errors after their detection has received
lesser attention. We present here a new tool, called \textsc{3M-DNN}, for
\emph{repairing} a given DNN, which is known to err on some set of inputs. The
novel repair procedure implemented in \textsc{3M-DNN} computes a modification
to the network's weights that corrects its behavior, and attempts to minimize
this change via a sequence of calls to a backend, black-box DNN verification
engine. To the best of our knowledge, our method is the first one that allows
repairing the network by simultaneously modifying multiple layers. This is
achieved by splitting the network into sub-networks, and applying a
single-layer repairing technique to each component. We evaluated
\textsc{3M-DNN} tool on an extensive set of benchmarks, obtaining promising
results. Data Availability Statement: An artifact will be submitted to the AEC
under EasyChair ID 60.

    

### [[2110.09930] Speech Representation Learning Through Self-supervised Pretraining And Multi-task Finetuning](http://arxiv.org/abs/2110.09930)


  Speech representation learning plays a vital role in speech processing. Among
them, self-supervised learning (SSL) has become an important research
direction. It has been shown that an SSL pretraining model can achieve
excellent performance in various downstream tasks of speech processing. On the
other hand, supervised multi-task learning (MTL) is another representation
learning paradigm, which has been proven effective in computer vision (CV) and
natural language processing (NLP). However, there is no systematic research on
the general representation learning model trained by supervised MTL in speech
processing. In this paper, we show that MTL finetuning can further improve SSL
pretraining. We analyze the generalizability of supervised MTL finetuning to
examine if the speech representation learned by MTL finetuning can generalize
to unseen new tasks.

    

### [[2110.09935] Random Feature Approximation for Online Nonlinear Graph Topology Identification](http://arxiv.org/abs/2110.09935)


  Online topology estimation of graph-connected time series is challenging,
especially since the causal dependencies in many real-world networks are
nonlinear. In this paper, we propose a kernel-based algorithm for graph
topology estimation. The algorithm uses a Fourier-based Random feature
approximation to tackle the curse of dimensionality associated with the kernel
representations. Exploiting the fact that the real-world networks often exhibit
sparse topologies, we propose a group lasso based optimization framework, which
is solve using an iterative composite objective mirror descent method, yielding
an online algorithm with fixed computational complexity per iteration. The
experiments conducted on real and synthetic data show that the proposed method
outperforms its competitors.

    

### [[2110.09940] Learning Representations that Support Robust Transfer of Predictors](http://arxiv.org/abs/2110.09940)


  Ensuring generalization to unseen environments remains a challenge. Domain
shift can lead to substantially degraded performance unless shifts are
well-exercised within the available training environments. We introduce a
simple robust estimation criterion -- transfer risk -- that is specifically
geared towards optimizing transfer to new environments. Effectively, the
criterion amounts to finding a representation that minimizes the risk of
applying any optimal predictor trained on one environment to another. The
transfer risk essentially decomposes into two terms, a direct transfer term and
a weighted gradient-matching term arising from the optimality of
per-environment predictors. Although inspired by IRM, we show that transfer
risk serves as a better out-of-distribution generalization criterion, both
theoretically and empirically. We further demonstrate the impact of optimizing
such transfer risk on two controlled settings, each representing a different
pattern of environment shift, as well as on two real-world datasets.
Experimentally, the approach outperforms baselines across various
out-of-distribution generalization tasks. Code is available at
\url{this https URL}.

    

### [[2110.09943] BAMLD: Bayesian Active Meta-Learning by Disagreement](http://arxiv.org/abs/2110.09943)


  Data-efficient learning algorithms are essential in many practical
applications for which data collection and labeling is expensive or infeasible,
e.g., for autonomous cars. To address this problem, meta-learning infers an
inductive bias from a set of meta-training tasks in order to learn new, but
related, task using a small number of samples. Most studies assume the
meta-learner to have access to labeled data sets from a large number of tasks.
In practice, one may have available only unlabeled data sets from the tasks,
requiring a costly labeling procedure to be carried out before use in standard
meta-learning schemes. To decrease the number of labeling requests for
meta-training tasks, this paper introduces an information-theoretic active task
selection mechanism which quantifies the epistemic uncertainty via
disagreements among the predictions obtained under different inductive biases.
We detail an instantiation for nonparametric methods based on Gaussian Process
Regression, and report its empirical performance results that compare
favourably against existing heuristic acquisition mechanisms.

    

### [[2110.09947] Using Program Synthesis and Inductive Logic Programming to solve Bongard Problems](http://arxiv.org/abs/2110.09947)


  The ability to recognise and make analogies is often used as a measure or
test of human intelligence. The ability to solve Bongard problems is an example
of such a test. It has also been postulated that the ability to rapidly
construct novel abstractions is critical to being able to solve analogical
problems. Given an image, the ability to construct a program that would
generate that image is one form of abstraction, as exemplified in the
Dreamcoder project. In this paper, we present a preliminary examination of
whether programs constructed by Dreamcoder can be used for analogical reasoning
to solve certain Bongard problems. We use Dreamcoder to discover programs that
generate the images in a Bongard problem and represent each of these as a
sequence of state transitions. We decorate the states using positional
information in an automated manner and then encode the resulting sequence into
logical facts in Prolog. We use inductive logic programming (ILP), to learn an
(interpretable) theory for the abstract concept involved in an instance of a
Bongard problem. Experiments on synthetically created Bongard problems for
concepts such as 'above/below' and 'clockwise/counterclockwise' demonstrate
that our end-to-end system can solve such problems. We study the importance and
completeness of each component of our approach, highlighting its current
limitations and pointing to directions for improvement in our formulation as
well as in elements of any Dreamcoder-like program synthesis system used for
such an approach.

    

### [[2110.09948] Analysis of False Data Injection Impact on AI based Solar Photovoltaic Power Generation Forecasting](http://arxiv.org/abs/2110.09948)


  The use of solar photovoltaics (PV) energy provides additional resources to
the electric power grid. The downside of this integration is that the solar
power supply is unreliable and highly dependent on the weather condition. The
predictability and stability of forecasting are critical for the full
utilization of solar power. This study reviews and evaluates various machine
learning-based models for solar PV power generation forecasting using a public
dataset. Furthermore, The root mean squared error (RMSE), mean squared error
(MSE), and mean average error (MAE) metrics are used to evaluate the results.
Linear Regression, Gaussian Process Regression, K-Nearest Neighbor, Decision
Trees, Gradient Boosting Regression Trees, Multi-layer Perceptron, and Support
Vector Regression algorithms are assessed. Their responses against false data
injection attacks are also investigated. The Multi-layer Perceptron Regression
method shows robust prediction on both regular and noise injected datasets over
other methods.

    

### [[2110.09962] PR-CIM: a Variation-Aware Binary-Neural-Network Framework for Process-Resilient Computation-in-memory](http://arxiv.org/abs/2110.09962)


  Binary neural networks (BNNs) that use 1-bit weights and activations have
garnered interest as extreme quantization provides low power dissipation. By
implementing BNNs as computing-in-memory (CIM), which computes multiplication
and accumulations on memory arrays in an analog fashion, namely analog CIM, we
can further improve the energy efficiency to process neural networks. However,
analog CIMs suffer from the potential problem that process variation degrades
the accuracy of BNNs. Our Monte-Carlo simulations show that in an SRAM-based
analog CIM of VGG-9, the classification accuracy of CIFAR-10 is degraded even
below 20% under process variations of 65nm CMOS. To overcome this problem, we
present a variation-aware BNN framework. The proposed framework is developed
for SRAM-based BNN CIMs since SRAM is most widely used as on-chip memory,
however easily extensible to BNN CIMs based on other memories. Our extensive
experimental results show that under process variation of 65nm CMOS, our
framework significantly improves the CIFAR-10 accuracies of SRAM-based BNN
CIMs, from 10% and 10.1% to 87.76% and 77.74% for VGG-9 and RESNET-18
respectively.

    

### [[2110.09966] SleepPriorCL: Contrastive Representation Learning with Prior Knowledge-based Positive Mining and Adaptive Temperature for Sleep Staging](http://arxiv.org/abs/2110.09966)


  The objective of this paper is to learn semantic representations for sleep
stage classification from raw physiological time series. Although supervised
methods have gained remarkable performance, they are limited in clinical
situations due to the requirement of fully labeled data. Self-supervised
learning (SSL) based on contrasting semantically similar (positive) and
dissimilar (negative) pairs of samples have achieved promising success.
However, existing SSL methods suffer the problem that many semantically similar
positives are still uncovered and even treated as negatives. In this paper, we
propose a novel SSL approach named SleepPriorCL to alleviate the above problem.
Advances of our approach over existing SSL methods are two-fold: 1) by
incorporating prior domain knowledge into the training regime of SSL, more
semantically similar positives are discovered without accessing ground-truth
labels; 2) via investigating the influence of the temperature in contrastive
loss, an adaptive temperature mechanism for each sample according to prior
domain knowledge is further proposed, leading to better performance. Extensive
experiments demonstrate that our method achieves state-of-the-art performance
and consistently outperforms baselines.

    

### [[2110.09974] TsmoBN: Interventional Generalization for Unseen Clients in Federated Learning](http://arxiv.org/abs/2110.09974)


  Generalizing federated learning (FL) models to unseen clients with non-iid
data is a crucial topic, yet unsolved so far. In this work, we propose to
tackle this problem from a novel causal perspective. Specifically, we form a
training structural causal model (SCM) to explain the challenges of model
generalization in a distributed learning paradigm. Based on this, we present a
simple yet effective method using test-specific and momentum tracked batch
normalization (TsmoBN) to generalize FL models to testing clients. We give a
causal analysis by formulating another testing SCM and demonstrate that the key
factor in TsmoBN is the test-specific statistics (i.e., mean and variance) of
features. Such statistics can be seen as a surrogate variable for causal
intervention. In addition, by considering generalization bounds in FL, we show
that our TsmoBN method can reduce divergence between training and testing
feature distributions, which achieves a lower generalization gap than standard
model testing. Our extensive experimental evaluations demonstrate significant
improvements for unseen client generalization on three datasets with various
types of feature distributions and numbers of clients. It is worth noting that
our proposed approach can be flexibly applied to different state-of-the-art
federated learning algorithms and is orthogonal to existing domain
generalization methods.

    

### [[2110.09983] ECG-ATK-GAN: Robustness against Adversarial Attacks on ECG using Conditional Generative Adversarial Networks](http://arxiv.org/abs/2110.09983)


  Recently deep learning has reached human-level performance in classifying
arrhythmia from Electrocardiogram (ECG). However, deep neural networks (DNN)
are vulnerable to adversarial attacks, which can misclassify ECG signals by
decreasing the model's precision. Adversarial attacks are crafted perturbations
injected in data that manifest the conventional DNN models to misclassify the
correct class. Thus, safety concerns arise as it becomes challenging to
establish the system's reliability, given that clinical applications require
high levels of trust. To mitigate this problem and make DNN models more robust
in clinical and real-life settings, we introduce a novel Conditional Generative
Adversarial Network (GAN), robust against adversarial attacked ECG signals and
retaining high accuracy. Furthermore, we compared it with other state-of-art
models to detect cardiac abnormalities from indistinguishable adversarial
attacked ECGs. The experiment confirms, our model is more robust against
adversarial attacks compared to other architectures.

    

### [[2110.09997] Hybrid-Layers Neural Network Architectures for Modeling the Self-Interference in Full-Duplex Systems](http://arxiv.org/abs/2110.09997)


  Full-duplex (FD) systems have been introduced to provide high data rates for
beyond fifth-generation wireless networks through simultaneous transmission of
information over the same frequency resources. However, the operation of FD
systems is practically limited by the self-interference (SI), and efficient SI
cancelers are sought to make the FD systems realizable. Typically,
polynomial-based cancelers are employed to mitigate the SI; nevertheless, they
suffer from high complexity. This article proposes two novel hybrid-layers
neural network (NN) architectures to cancel the SI with low complexity. The
first architecture is referred to as hybrid-convolutional recurrent NN (HCRNN),
whereas the second is termed as hybrid-convolutional recurrent dense NN
(HCRDNN). In contrast to the state-of-the-art NNs that employ dense or
recurrent layers for SI modeling, the proposed NNs exploit, in a novel manner,
a combination of different hidden layers (e.g., convolutional, recurrent,
and/or dense) in order to model the SI with lower computational complexity than
the polynomial and the state-of-the-art NN-based cancelers. The key idea behind
using hybrid layers is to build an NN model, which makes use of the
characteristics of the different layers employed in its architecture. More
specifically, in the HCRNN, a convolutional layer is employed to extract the
input data features using a reduced network scale. Moreover, a recurrent layer
is then applied to assist in learning the temporal behavior of the input signal
from the localized feature map of the convolutional layer. In the HCRDNN, an
additional dense layer is exploited to add another degree of freedom for
adapting the NN settings in order to achieve the best compromise between the
cancellation performance and computational complexity. Complexity analysis and
numerical simulations are provided to prove the superiority of the proposed
architectures.

    

### [[2110.10005] Data-driven and Automatic Surface Texture Analysis Using Persistent Homology](http://arxiv.org/abs/2110.10005)


  Surface roughness plays an important role in analyzing engineering surfaces.
It quantifies the surface topography and can be used to determine whether the
resulting surface finish is acceptable or not. Nevertheless, while several
existing tools and standards are available for computing surface roughness,
these methods rely heavily on user input thus slowing down the analysis and
increasing manufacturing costs. Therefore, fast and automatic determination of
the roughness level is essential to avoid costs resulting from surfaces with
unacceptable finish, and user-intensive analysis. In this study, we propose a
Topological Data Analysis (TDA) based approach to classify the roughness level
of synthetic surfaces using both their areal images and profiles. We utilize
persistent homology from TDA to generate persistence diagrams that encapsulate
information on the shape of the surface. We then obtain feature matrices for
each surface or profile using Carlsson coordinates, persistence images, and
template functions. We compare our results to two widely used methods in the
literature: Fast Fourier Transform (FFT) and Gaussian filtering. The results
show that our approach yields mean accuracies as high as 97%. We also show
that, in contrast to existing surface analysis tools, our TDA-based approach is
fully automatable and provides adaptive feature extraction.

    

### [[2110.10009] EEGminer: Discovering Interpretable Features of Brain Activity with Learnable Filters](http://arxiv.org/abs/2110.10009)


  Patterns of brain activity are associated with different brain processes and
can be used to identify different brain states and make behavioral predictions.
However, the relevant features are not readily apparent and accessible. To mine
informative latent representations from multichannel EEG recordings, we propose
a novel differentiable EEG decoding pipeline consisting of learnable filters
and a pre-determined feature extraction module. Specifically, we introduce
filters parameterized by generalized Gaussian functions that offer a smooth
derivative for stable end-to-end model training and allow for learning
interpretable features. For the feature module, we use signal magnitude and
functional connectivity. We demonstrate the utility of our model towards
emotion recognition from EEG signals on the SEED dataset, as well as on a new
EEG dataset of unprecedented size (i.e., 763 subjects), where we identify
consistent trends of music perception and related individual differences. The
discovered features align with previous neuroscience studies and offer new
insights, such as marked differences in the functional connectivity profile
between left and right temporal areas during music listening. This agrees with
the respective specialisation of the temporal lobes regarding music perception
proposed in the literature.

    

### [[2110.10015] DEEPAG: Answering Questions in Portuguese about the Brazilian Environment](http://arxiv.org/abs/2110.10015)


  The challenge of climate change and biome conservation is one of the most
pressing issues of our time - particularly in Brazil, where key environmental
reserves are located. Given the availability of large textual databases on
ecological themes, it is natural to resort to question answering (QA) systems
to increase social awareness and understanding about these topics. In this
work, we introduce multiple QA systems that combine in novel ways the BM25
algorithm, a sparse retrieval technique, with PTT5, a pre-trained
state-of-the-art language model. Our QA systems focus on the Portuguese
language, thus offering resources not found elsewhere in the literature. As
training data, we collected questions from open-domain datasets, as well as
content from the Portuguese Wikipedia and news from the press. We thus
contribute with innovative architectures and novel applications, attaining an
F1-score of 36.2 with our best model.

    

### [[2110.10017] Neural Network Compatible Off-Policy Natural Actor-Critic Algorithm](http://arxiv.org/abs/2110.10017)


  Learning optimal behavior from existing data is one of the most important
problems in Reinforcement Learning (RL). This is known as "off-policy control"
in RL where an agent's objective is to compute an optimal policy based on the
data obtained from the given policy (known as the behavior policy). As the
optimal policy can be very different from the behavior policy, learning optimal
behavior is very hard in the "off-policy" setting compared to the "on-policy"
setting where new data from the policy updates will be utilized in learning.
This work proposes an off-policy natural actor-critic algorithm that utilizes
state-action distribution correction for handling the off-policy behavior and
the natural policy gradient for sample efficiency. The existing natural
gradient-based actor-critic algorithms with convergence guarantees require
fixed features for approximating both policy and value functions. This often
leads to sub-optimal learning in many RL applications. On the other hand, our
proposed algorithm utilizes compatible features that enable one to use
arbitrary neural networks to approximate the policy and the value function and
guarantee convergence to a locally optimal policy. We illustrate the benefit of
the proposed off-policy natural gradient algorithm by comparing it with the
vanilla gradient actor-critic algorithm on benchmark RL tasks.

    

### [[2110.10018] Dynamic pricing and assortment under a contextual MNL demand](http://arxiv.org/abs/2110.10018)


  We consider dynamic multi-product pricing and assortment problems under an
unknown demand over T periods, where in each period, the seller decides on the
price for each product or the assortment of products to offer to a customer who
chooses according to an unknown Multinomial Logit Model (MNL). Such problems
arise in many applications, including online retail and advertising. We propose
a randomized dynamic pricing policy based on a variant of the Online Newton
Step algorithm (ONS) that achieves a $O(d\sqrt{T}\log(T))$ regret guarantee
under an adversarial arrival model. We also present a new optimistic algorithm
for the adversarial MNL contextual bandits problem, which achieves a better
dependency than the state-of-the-art algorithms in a problem-dependent constant
$\kappa$ (potentially exponentially small). Our regret upper bounds scale as
$\tilde{O}(d\sqrt{\kappa T}+ \log(T)/\kappa)$, which gives a significantly
stronger bound than the existing $\tilde{O}(d\sqrt{T}/\kappa)$ guarantees.

    

### [[2110.10026] Private Language Model Adaptation for Speech Recognition](http://arxiv.org/abs/2110.10026)


  Speech model adaptation is crucial to handle the discrepancy between
server-side proxy training data and actual data received on users' local
devices. With the use of federated learning (FL), we introduce an efficient
approach on continuously adapting neural network language models (NNLMs) on
private devices with applications on automatic speech recognition (ASR). To
address the potential speech transcription errors in the on-device training
corpus, we perform empirical studies on comparing various strategies of
leveraging token-level confidence scores to improve the NNLM quality in the FL
settings. Experiments show that compared with no model adaptation, the proposed
method achieves relative 2.6% and 10.8% word error rate (WER) reductions on two
speech evaluation datasets, respectively. We also provide analysis in
evaluating privacy guarantees of our presented procedure.

    

### [[2110.10027] Clinical Trial Information Extraction with BERT](http://arxiv.org/abs/2110.10027)


  Natural language processing (NLP) of clinical trial documents can be useful
in new trial design. Here we identify entity types relevant to clinical trial
design and propose a framework called CT-BERT for information extraction from
clinical trial text. We trained named entity recognition (NER) models to
extract eligibility criteria entities by fine-tuning a set of pre-trained BERT
models. We then compared the performance of CT-BERT with recent baseline
methods including attention-based BiLSTM and Criteria2Query. The results
demonstrate the superiority of CT-BERT in clinical trial NLP.

    

### [[2110.10030] Accelerating Framework of Transformer by Hardware Design and Model Compression Co-Optimization](http://arxiv.org/abs/2110.10030)


  State-of-the-art Transformer-based models, with gigantic parameters, are
difficult to be accommodated on resource constrained embedded devices.
Moreover, with the development of technology, more and more embedded devices
are available to run a Transformer model. For a Transformer model with
different constraints (tight or loose), it can be deployed onto devices with
different computing power. However, in previous work, designers did not choose
the best device among multiple devices. Instead, they just used an existing
device to deploy model, which was not necessarily the best fit and may lead to
underutilization of resources. To address the deployment challenge of
Transformer and the problem to select the best device, we propose an algorithm
& hardware closed-loop acceleration framework. Given a dataset, a model,
latency constraint LC and accuracy constraint AC, our framework can provide a
best device satisfying both constraints. In order to generate a compressed
model with high sparsity ratio, we propose a novel pruning technique,
hierarchical pruning (HP). We optimize the sparse matrix storage format for HP
matrix to further reduce memory usage for FPGA implementation. We design a
accelerator that takes advantage of HP to solve the problem of concurrent
random access. Experiments on Transformer and TinyBert model show that our
framework can find different devices for various LC and AC, covering from
low-end devices to high-end devices. Our HP can achieve higher sparsity ratio
and is more flexible than other sparsity pattern. Our framework can achieve
37x, 1.9x, 1.7x speedup compared to CPU, GPU and FPGA, respectively.

    

### [[2110.10031] Online Continual Learning on Class Incremental Blurry Task Configuration with Anytime Inference](http://arxiv.org/abs/2110.10031)


  Despite rapid advances in continual learning, a large body of research is
devoted to improving performance in the existing setups. While a handful of
work do propose new continual learning setups, they still lack practicality in
certain aspects. For better practicality, we first propose a novel continual
learning setup that is online, task-free, class-incremental, of blurry task
boundaries and subject to inference queries at any moment. We additionally
propose a new metric to better measure the performance of the continual
learning methods subject to inference queries at any moment. To address the
challenging setup and evaluation protocol, we propose an effective method that
employs a new memory management scheme and novel learning techniques. Our
empirical validation demonstrates that the proposed method outperforms prior
arts by large margins.

    

### [[2110.10038] Coalitional Bayesian Autoencoders -- Towards explainable unsupervised deep learning](http://arxiv.org/abs/2110.10038)


  This paper aims to improve the explainability of Autoencoder's (AE)
predictions by proposing two explanation methods based on the mean and
epistemic uncertainty of log-likelihood estimate, which naturally arise from
the probabilistic formulation of the AE called Bayesian Autoencoders (BAE). To
quantitatively evaluate the performance of explanation methods, we test them in
sensor network applications, and propose three metrics based on covariate shift
of sensors : (1) G-mean of Spearman drift coefficients, (2) G-mean of
sensitivity-specificity of explanation ranking and (3) sensor explanation
quality index (SEQI) which combines the two aforementioned metrics.
Surprisingly, we find that explanations of BAE's predictions suffer from high
correlation resulting in misleading explanations. To alleviate this, a
"Coalitional BAE" is proposed, which is inspired by agent-based system theory.
Our comprehensive experiments on publicly available condition monitoring
datasets demonstrate the improved quality of explanations using the Coalitional
BAE.

    

### [[2110.10049] Boosting Graph Embedding on a Single GPU](http://arxiv.org/abs/2110.10049)


  Graphs are ubiquitous, and they can model unique characteristics and complex
relations of real-life systems. Although using machine learning (ML) on graphs
is promising, their raw representation is not suitable for ML algorithms. Graph
embedding represents each node of a graph as a d-dimensional vector which is
more suitable for ML tasks. However, the embedding process is expensive, and
CPU-based tools do not scale to real-world graphs. In this work, we present
GOSH, a GPU-based tool for embedding large-scale graphs with minimum hardware
constraints. GOSH employs a novel graph coarsening algorithm to enhance the
impact of updates and minimize the work for embedding. It also incorporates a
decomposition schema that enables any arbitrarily large graph to be embedded
with a single GPU. As a result, GOSH sets a new state-of-the-art in link
prediction both in accuracy and speed, and delivers high-quality embeddings for
node classification at a fraction of the time compared to the state-of-the-art.
For instance, it can embed a graph with over 65 million vertices and 1.8
billion edges in less than 30 minutes on a single GPU.

    

### [[2110.10054] Generating Symbolic Reasoning Problems with Transformer GANs](http://arxiv.org/abs/2110.10054)


  Constructing training data for symbolic reasoning domains is challenging:
Existing instances are typically hand-crafted and too few to be trained on
directly and synthetically generated instances are often hard to evaluate in
terms of their meaningfulness. We study the capabilities of GANs and
Wasserstein GANs equipped with Transformer encoders to generate sensible and
challenging training data for symbolic reasoning domains. We conduct
experiments on two problem domains where Transformers have been successfully
applied recently: symbolic mathematics and temporal specifications in
verification. Even without autoregression, our GAN models produce syntactically
correct instances. We show that the generated data can be used as a substitute
for real training data when training a classifier, and, especially, that
training data can be generated from a real dataset that is too small to be
trained on directly. Using a GAN setting also allows us to alter the target
distribution: We show that by adding a classifier uncertainty part to the
generator objective, we obtain a dataset that is even harder to solve for a
classifier than our original dataset.

    

### [[2110.10059] On Clustering Categories of Categorical Predictors in Generalized Linear Models](http://arxiv.org/abs/2110.10059)


  We propose a method to reduce the complexity of Generalized Linear Models in
the presence of categorical predictors. The traditional one-hot encoding, where
each category is represented by a dummy variable, can be wasteful, difficult to
interpret, and prone to overfitting, especially when dealing with
high-cardinality categorical predictors. This paper addresses these challenges
by finding a reduced representation of the categorical predictors by clustering
their categories. This is done through a numerical method which aims to
preserve (or even, improve) accuracy, while reducing the number of coefficients
to be estimated for the categorical predictors. Thanks to its design, we are
able to derive a proximity measure between categories of a categorical
predictor that can be easily visualized. We illustrate the performance of our
approach in real-world classification and count-data datasets where we see that
clustering the categorical predictors reduces complexity substantially without
harming accuracy.

    

### [[2110.10067] CORA: Benchmarks, Baselines, and Metrics as a Platform for Continual Reinforcement Learning Agents](http://arxiv.org/abs/2110.10067)


  Progress in continual reinforcement learning has been limited due to several
barriers to entry: missing code, high compute requirements, and a lack of
suitable benchmarks. In this work, we present CORA, a platform for Continual
Reinforcement Learning Agents that provides benchmarks, baselines, and metrics
in a single code package. The benchmarks we provide are designed to evaluate
different aspects of the continual RL challenge, such as catastrophic
forgetting, plasticity, ability to generalize, and sample-efficient learning.
Three of the benchmarks utilize video game environments (Atari, Procgen,
NetHack). The fourth benchmark, CHORES, consists of four different task
sequences in a visually realistic home simulator, drawn from a diverse set of
task and scene parameters. To compare continual RL methods on these benchmarks,
we prepare three metrics in CORA: continual evaluation, forgetting, and
zero-shot forward transfer. Finally, CORA includes a set of performant,
open-source baselines of existing algorithms for researchers to use and expand
on. We release CORA and hope that the continual RL community can benefit from
our contributions, to accelerate the development of new continual RL
algorithms.

    

### [[2110.10075] Improving the Accuracy-Memory Trade-Off of Random Forests Via Leaf-Refinement](http://arxiv.org/abs/2110.10075)


  Random Forests (RF) are among the state-of-the-art in many machine learning
applications. With the ongoing integration of ML models into everyday life, the
deployment and continuous application of models becomes more and more an
important issue. Hence, small models which offer good predictive performance
but use small amounts of memory are required. Ensemble pruning is a standard
technique to remove unnecessary classifiers from an ensemble to reduce the
overall resource consumption and sometimes even improve the performance of the
original ensemble. In this paper, we revisit ensemble pruning in the context of
`modernly' trained Random Forests where trees are very large. We show that the
improvement effects of pruning diminishes for ensembles of large trees but that
pruning has an overall better accuracy-memory trade-off than RF. However,
pruning does not offer fine-grained control over this trade-off because it
removes entire trees from the ensemble. To further improve the accuracy-memory
trade-off we present a simple, yet surprisingly effective algorithm that
refines the predictions in the leaf nodes in the forest via stochastic gradient
descent. We evaluate our method against 7 state-of-the-art pruning methods and
show that our method outperforms the other methods on 11 of 16 datasets with a
statistically significant better accuracy-memory trade-off compared to most
methods. We conclude our experimental evaluation with a case study showing that
our method can be applied in a real-world setting.

    

### [[2110.10077] Deep Learning to Estimate Permeability using Geophysical Data](http://arxiv.org/abs/2110.10077)


  Time-lapse electrical resistivity tomography (ERT) is a popular geophysical
method to estimate three-dimensional (3D) permeability fields from electrical
potential difference measurements. Traditional inversion and data assimilation
methods are used to ingest this ERT data into hydrogeophysical models to
estimate permeability. Due to ill-posedness and the curse of dimensionality,
existing inversion strategies provide poor estimates and low resolution of the
3D permeability field. Recent advances in deep learning provide us with
powerful algorithms to overcome this challenge. This paper presents a deep
learning (DL) framework to estimate the 3D subsurface permeability from
time-lapse ERT data. To test the feasibility of the proposed framework, we
train DL-enabled inverse models on simulation data. Subsurface process models
based on hydrogeophysics are used to generate this synthetic data for deep
learning analyses. Results show that proposed weak supervised learning can
capture salient spatial features in the 3D permeability field. Quantitatively,
the average mean squared error (in terms of the natural log) on the strongly
labeled training, validation, and test datasets is less than 0.5. The R2-score
(global metric) is greater than 0.75, and the percent error in each cell (local
metric) is less than 10%. Finally, an added benefit in terms of computational
cost is that the proposed DL-based inverse model is at least O(104) times
faster than running a forward model. Note that traditional inversion may
require multiple forward model simulations (e.g., in the order of 10 to 1000),
which are very expensive. This computational savings (O(105) - O(107)) makes
the proposed DL-based inverse model attractive for subsurface imaging and
real-time ERT monitoring applications due to fast and yet reasonably accurate
estimations of the permeability field.

    

### [[2110.10080] Surrogate and inverse modeling for two-phase flow in porous media via theory-guided convolutional neural network](http://arxiv.org/abs/2110.10080)


  The theory-guided convolutional neural network (TgCNN) framework, which can
incorporate discretized governing equation residuals into the training of
convolutional neural networks (CNNs), is extended to two-phase porous media
flow problems in this work. The two principal variables of the considered
problem, pressure and saturation, are approximated simultaneously with two
CNNs, respectively. Pressure and saturation are coupled with each other in the
governing equations, and thus the two networks are also mutually conditioned in
the training process by the discretized governing equations, which also
increases the difficulty of model training. The coupled and discretized
equations can provide valuable information in the training process. With the
assistance of theory-guidance, the TgCNN surrogates can achieve better accuracy
than ordinary CNN surrogates in two-phase flow problems. Moreover, a piecewise
training strategy is proposed for the scenario with varying well controls, in
which the TgCNN surrogates are constructed for different segments on the time
dimension and stacked together to predict solutions for the whole time-span.
For scenarios with larger variance of the formation property field, the TgCNN
surrogates can also achieve satisfactory performance. The constructed TgCNN
surrogates are further used for inversion of permeability fields by combining
them with the iterative ensemble smoother (IES) algorithm, and sufficient
inversion accuracy is obtained with improved efficiency.

    

### [[2110.10081] Stateful Offline Contextual Policy Evaluation and Learning](http://arxiv.org/abs/2110.10081)


  We study off-policy evaluation and learning from sequential data in a
structured class of Markov decision processes that arise from repeated
interactions with an exogenous sequence of arrivals with contexts, which
generate unknown individual-level responses to agent actions. This model can be
thought of as an offline generalization of contextual bandits with resource
constraints. We formalize the relevant causal structure of problems such as
dynamic personalized pricing and other operations management problems in the
presence of potentially high-dimensional user types. The key insight is that an
individual-level response is often not causally affected by the state variable
and can therefore easily be generalized across timesteps and states. When this
is true, we study implications for (doubly robust) off-policy evaluation and
learning by instead leveraging single time-step evaluation, estimating the
expectation over a single arrival via data from a population, for fitted-value
iteration in a marginal MDP. We study sample complexity and analyze error
amplification that leads to the persistence, rather than attenuation, of
confounding error over time. In simulations of dynamic and capacitated pricing,
we show improved out-of-sample policy performance in this class of relevant
problems.

    

### [[2110.10082] Nonparametric Sparse Tensor Factorization with Hierarchical Gamma Processes](http://arxiv.org/abs/2110.10082)


  We propose a nonparametric factorization approach for sparsely observed
tensors. The sparsity does not mean zero-valued entries are massive or
dominated. Rather, it implies the observed entries are very few, and even fewer
with the growth of the tensor; this is ubiquitous in practice. Compared with
the existent works, our model not only leverages the structural information
underlying the observed entry indices, but also provides extra interpretability
and flexibility -- it can simultaneously estimate a set of location factors
about the intrinsic properties of the tensor nodes, and another set of
sociability factors reflecting their extrovert activity in interacting with
others; users are free to choose a trade-off between the two types of factors.
Specifically, we use hierarchical Gamma processes and Poisson random measures
to construct a tensor-valued process, which can freely sample the two types of
factors to generate tensors and always guarantees an asymptotic sparsity. We
then normalize the tensor process to obtain hierarchical Dirichlet processes to
sample each observed entry index, and use a Gaussian process to sample the
entry value as a nonlinear function of the factors, so as to capture both the
sparse structure properties and complex node relationships. For efficient
inference, we use Dirichlet process properties over finite sample partitions,
density transformations, and random features to develop a stochastic
variational estimation algorithm. We demonstrate the advantage of our method in
several benchmark datasets.

    

### [[2110.10083] Contrastive Active Inference](http://arxiv.org/abs/2110.10083)


  Active inference is a unifying theory for perception and action resting upon
the idea that the brain maintains an internal model of the world by minimizing
free energy. From a behavioral perspective, active inference agents can be seen
as self-evidencing beings that act to fulfill their optimistic predictions,
namely preferred outcomes or goals. In contrast, reinforcement learning
requires human-designed rewards to accomplish any desired outcome. Although
active inference could provide a more natural self-supervised objective for
control, its applicability has been limited because of the shortcomings in
scaling the approach to complex environments. In this work, we propose a
contrastive objective for active inference that strongly reduces the
computational burden in learning the agent's generative model and planning
future actions. Our method performs notably better than likelihood-based active
inference in image-based tasks, while also being computationally cheaper and
easier to train. We compare to reinforcement learning agents that have access
to human-designed reward functions, showing that our approach closely matches
their performance. Finally, we also show that contrastive methods perform
significantly better in the case of distractors in the environment and that our
method is able to generalize goals to variations in the background.

    

### [[2110.10090] Inductive Biases and Variable Creation in Self-Attention Mechanisms](http://arxiv.org/abs/2110.10090)


  Self-attention, an architectural motif designed to model long-range
interactions in sequential data, has driven numerous recent breakthroughs in
natural language processing and beyond. This work provides a theoretical
analysis of the inductive biases of self-attention modules, where our focus is
to rigorously establish which functions and long-range dependencies
self-attention blocks prefer to represent. Our main result shows that
bounded-norm Transformer layers create sparse variables: they can represent
sparse functions of the input sequence, with sample complexity scaling only
logarithmically with the context length. Furthermore, we propose new
experimental protocols to support this analysis and to guide the practice of
training Transformers, built around the large body of work on provably learning
sparse Boolean functions.

    

### [[2110.10103] Continual self-training with bootstrapped remixing for speech enhancement](http://arxiv.org/abs/2110.10103)


  We propose RemixIT, a simple and novel self-supervised training method for
speech enhancement. The proposed method is based on a continuously
self-training scheme that overcomes limitations from previous studies including
assumptions for the in-domain noise distribution and having access to clean
target signals. Specifically, a separation teacher model is pre-trained on an
out-of-domain dataset and is used to infer estimated target signals for a batch
of in-domain mixtures. Next, we bootstrap the mixing process by generating
artificial mixtures using permuted estimated clean and noise signals. Finally,
the student model is trained using the permuted estimated sources as targets
while we periodically update teacher's weights using the latest student model.
Our experiments show that RemixIT outperforms several previous state-of-the-art
self-supervised methods under multiple speech enhancement tasks. Additionally,
RemixIT provides a seamless alternative for semi-supervised and unsupervised
domain adaptation for speech enhancement tasks, while being general enough to
be applied to any separation task and paired with any separation model.

    

### [[2110.10108] TESSERACT: Gradient Flip Score to Secure Federated Learning Against Model Poisoning Attacks](http://arxiv.org/abs/2110.10108)


  Federated learning---multi-party, distributed learning in a decentralized
environment---is vulnerable to model poisoning attacks, even more so than
centralized learning approaches. This is because malicious clients can collude
and send in carefully tailored model updates to make the global model
inaccurate. This motivated the development of Byzantine-resilient federated
learning algorithms, such as Krum, Bulyan, FABA, and FoolsGold. However, a
recently developed untargeted model poisoning attack showed that all prior
defenses can be bypassed. The attack uses the intuition that simply by changing
the sign of the gradient updates that the optimizer is computing, for a set of
malicious clients, a model can be diverted from the optima to increase the test
error rate. In this work, we develop TESSERACT---a defense against this
directed deviation attack, a state-of-the-art model poisoning attack. TESSERACT
is based on a simple intuition that in a federated learning setting, certain
patterns of gradient flips are indicative of an attack. This intuition is
remarkably stable across different learning algorithms, models, and datasets.
TESSERACT assigns reputation scores to the participating clients based on their
behavior during the training phase and then takes a weighted contribution of
the clients. We show that TESSERACT provides robustness against even a
white-box version of the attack.

    

### [[2110.10116] On the Global Convergence of Momentum-based Policy Gradient](http://arxiv.org/abs/2110.10116)


  Policy gradient (PG) methods are popular and efficient for large-scale
reinforcement learning due to their relative stability and incremental nature.
In recent years, the empirical success of PG methods has led to the development
of a theoretical foundation for these methods. In this work, we generalize this
line of research by studying the global convergence of stochastic PG methods
with momentum terms, which have been demonstrated to be efficient recipes for
improving PG methods. We study both the soft-max and the Fisher-non-degenerate
policy parametrizations, and show that adding a momentum improves the global
optimality sample complexity of vanilla PG methods by
$\tilde{\mathcal{O}}(\epsilon^{-1.5})$ and
$\tilde{\mathcal{O}}(\epsilon^{-1})$, respectively, where $\epsilon>0$ is the
target tolerance. Our work is the first one that obtains global convergence
results for the momentum-based PG methods. For the generic
Fisher-non-degenerate policy parametrizations, our result is the first
single-loop and finite-batch PG algorithm achieving $\tilde{O}(\epsilon^{-3})$
global optimality sample complexity. Finally, as a by-product, our methods also
provide general framework for analyzing the global convergence rates of
stochastic PG methods, which can be easily applied and extended to different PG
estimators.

    

### [[2110.10117] Beyond Exact Gradients: Convergence of Stochastic Soft-Max Policy Gradient Methods with Entropy Regularization](http://arxiv.org/abs/2110.10117)


  Entropy regularization is an efficient technique for encouraging exploration
and preventing a premature convergence of (vanilla) policy gradient methods in
reinforcement learning (RL). However, the theoretical understanding of entropy
regularized RL algorithms has been limited. In this paper, we revisit the
classical entropy regularized policy gradient methods with the soft-max policy
parametrization, whose convergence has so far only been established assuming
access to exact gradient oracles. To go beyond this scenario, we propose the
first set of (nearly) unbiased stochastic policy gradient estimators with
trajectory-level entropy regularization, with one being an unbiased visitation
measure-based estimator and the other one being a nearly unbiased yet more
practical trajectory-based estimator. We prove that although the estimators
themselves are unbounded in general due to the additional logarithmic policy
rewards introduced by the entropy term, the variances are uniformly bounded.
This enables the development of the first set of convergence results for
stochastic entropy regularized policy gradient methods to both stationary
points and globally optimal policies. We also develop some improved sample
complexity results under a good initialization.

    

### [[2110.10128] Robust Event Classification Using Imperfect Real-world PMU Data](http://arxiv.org/abs/2110.10128)


  This paper studies robust event classification using imperfect real-world
phasor measurement unit (PMU) data. By analyzing the real-world PMU data, we
find it is challenging to directly use this dataset for event classifiers due
to the low data quality observed in PMU measurements and event logs. To address
these challenges, we develop a novel machine learning framework for training
robust event classifiers, which consists of three main steps: data
preprocessing, fine-grained event data extraction, and feature engineering.
Specifically, the data preprocessing step addresses the data quality issues of
PMU measurements (e.g., bad data and missing data); in the fine-grained event
data extraction step, a model-free event detection method is developed to
accurately localize the events from the inaccurate event timestamps in the
event logs; and the feature engineering step constructs the event features
based on the patterns of different event types, in order to improve the
performance and the interpretability of the event classifiers. Based on the
proposed framework, we develop a workflow for event classification using the
real-world PMU data streaming into the system in real-time. Using the proposed
framework, robust event classifiers can be efficiently trained based on many
off-the-shelf lightweight machine learning models. Numerical experiments using
the real-world dataset from the Western Interconnection of the U.S power
transmission grid show that the event classifiers trained under the proposed
framework can achieve high classification accuracy while being robust against
low-quality data.

    

### [[2110.10132] FriendlyCore: Practical Differentially Private Aggregation](http://arxiv.org/abs/2110.10132)


  Differentially private algorithms for common metric aggregation tasks, such
as clustering or averaging, often have limited practicality due to their
complexity or a large number of data points that is required for accurate
results. We propose a simple and practical tool $\mathsf{FriendlyCore}$ that
takes a set of points ${\cal D}$ from an unrestricted (pseudo) metric space as
input. When ${\cal D}$ has effective diameter $r$, $\mathsf{FriendlyCore}$
returns a "stable" subset ${\cal D}_G\subseteq {\cal D}$ that includes all
points, except possibly few outliers, and is {\em certified} to have diameter
$r$. $\mathsf{FriendlyCore}$ can be used to preprocess the input before
privately aggregating it, potentially simplifying the aggregation or boosting
its accuracy. Surprisingly, $\mathsf{FriendlyCore}$ is light-weight with no
dependence on the dimension. We empirically demonstrate its advantages in
boosting the accuracy of mean estimation, outperforming tailored methods.

    

### [[2110.10133] Locally Differentially Private Reinforcement Learning for Linear Mixture Markov Decision Processes](http://arxiv.org/abs/2110.10133)


  Reinforcement learning (RL) algorithms can be used to provide personalized
services, which rely on users' private and sensitive data. To protect the
users' privacy, privacy-preserving RL algorithms are in demand. In this paper,
we study RL with linear function approximation and local differential privacy
(LDP) guarantees. We propose a novel $(\varepsilon, \delta)$-LDP algorithm for
learning a class of Markov decision processes (MDPs) dubbed linear mixture
MDPs, and obtains an $\tilde{\mathcal{O}}(
d^{5/4}H^{7/4}T^{3/4}\left(\log(1/\delta)\right)^{1/4}\sqrt{1/\varepsilon})$
regret, where $d$ is the dimension of feature mapping, $H$ is the length of the
planning horizon, and $T$ is the number of interactions with the environment.
We also prove a lower bound
$\Omega(dH\sqrt{T}/\left(e^{\varepsilon}(e^{\varepsilon}-1)\right))$ for
learning linear mixture MDPs under $\varepsilon$-LDP constraint. Experiments on
synthetic datasets verify the effectiveness of our algorithm. To the best of
our knowledge, this is the first provable privacy-preserving RL algorithm with
linear function approximation.

    

### [[2110.10136] Activation Landscapes as a Topological Summary of Neural Network Performance](http://arxiv.org/abs/2110.10136)


  We use topological data analysis (TDA) to study how data transforms as it
passes through successive layers of a deep neural network (DNN). We compute the
persistent homology of the activation data for each layer of the network and
summarize this information using persistence landscapes. The resulting feature
map provides both an informative visual- ization of the network and a kernel
for statistical analysis and machine learning. We observe that the topological
complexity often increases with training and that the topological complexity
does not decrease with each layer.

    

### [[2110.10149] Continuous Control with Action Quantization from Demonstrations](http://arxiv.org/abs/2110.10149)


  In Reinforcement Learning (RL), discrete actions, as opposed to continuous
actions, result in less complex exploration problems and the immediate
computation of the maximum of the action-value function which is central to
dynamic programming-based methods. In this paper, we propose a novel method:
Action Quantization from Demonstrations (AQuaDem) to learn a discretization of
continuous action spaces by leveraging the priors of demonstrations. This
dramatically reduces the exploration problem, since the actions faced by the
agent not only are in a finite number but also are plausible in light of the
demonstrator's behavior. By discretizing the action space we can apply any
discrete action deep RL algorithm to the continuous control problem. We
evaluate the proposed method on three different setups: RL with demonstrations,
RL with play data --demonstrations of a human playing in an environment but not
solving any specific task-- and Imitation Learning. For all three setups, we
only consider human data, which is more challenging than synthetic data. We
found that AQuaDem consistently outperforms state-of-the-art continuous control
methods, both in terms of performance and sample efficiency. We provide
visualizations and videos in the paper's website:
this https URL.

    

### [[1809.10756] An Introduction to Probabilistic Programming](http://arxiv.org/abs/1809.10756)


  This book is a graduate-level introduction to probabilistic programming. It
not only provides a thorough background for anyone wishing to use a
probabilistic programming system, but also introduces the techniques needed to
design and build these systems. It is aimed at people who have an
undergraduate-level understanding of either or, ideally, both probabilistic
machine learning and programming languages.
We start with a discussion of model-based reasoning and explain why
conditioning is a foundational computation central to the fields of
probabilistic machine learning and artificial intelligence. We then introduce a
first-order probabilistic programming language (PPL) whose programs correspond
to graphical models with a known, finite, set of random variables. In the
context of this PPL we introduce fundamental inference algorithms and describe
how they can be implemented.
We then turn to higher-order probabilistic programming languages. Programs in
such languages can define models with dynamic computation graphs, which may not
instantiate the same set of random variables in each execution. Inference
requires methods that generate samples by repeatedly evaluating the program.
Foundational algorithms for this kind of language are discussed in the context
of an interface between program executions and an inference controller.
Finally we consider the intersection of probabilistic and differentiable
programming. We begin with a discussion of automatic differentiation, and how
it can be used to implement efficient inference methods based on Hamiltonian
Monte Carlo. We then discuss gradient-based maximum likelihood estimation in
programs that are parameterized using neural networks, how to amortize
inference using by learning neural approximations to the program posterior, and
how language features impact the design of deep probabilistic programming
systems.

    

### [[2006.08161] Optimal Transport for Conditional Domain Matching and Label Shift](http://arxiv.org/abs/2006.08161)


  We address the problem of unsupervised domain adaptation under the setting of
generalized target shift (joint class-conditional and label shifts). For this
framework, we theoretically show that, for good generalization, it is necessary
to learn a latent representation in which both marginals and class-conditional
distributions are aligned across domains. For this sake, we propose a learning
problem that minimizes importance weighted loss in the source domain and a
Wasserstein distance between weighted marginals. For a proper weighting, we
provide an estimator of target label proportion by blending mixture estimation
and optimal matching by optimal transport. This estimation comes with
theoretical guarantees of correctness under mild assumptions. Our experimental
results show that our method performs better on average than competitors across
a range domain adaptation problems including \emph{digits},\emph{VisDA} and
\emph{Office}. Code for this paper is available at
\url{this https URL}.

    

### [[2006.13431] Multiscale Simulations of Complex Systems by Learning their Effective Dynamics](http://arxiv.org/abs/2006.13431)


  Predictive simulations of complex systems are essential for applications
ranging from weather forecasting to drug design. The veracity of these
predictions hinges on their capacity to capture the effective system dynamics.
Massively parallel simulations predict the system dynamics by resolving all
spatiotemporal scales, often at a cost that prevents experimentation while
their findings may not allow for generalisation. On the other hand reduced
order models are fast but limited by the frequently adopted linearization of
the system dynamics and/or the utilization of heuristic closures. Here we
present a novel systematic framework that bridges large scale simulations and
reduced order models to Learn the Effective Dynamics (LED) of diverse complex
systems. The framework forms algorithmic alloys between non-linear machine
learning algorithms and the Equation-Free approach for modeling complex
systems. LED deploys autoencoders to formulate a mapping between fine and
coarse-grained representations and evolves the latent space dynamics using
recurrent neural networks. The algorithm is validated on benchmark problems and
we find that it outperforms state of the art reduced order models in terms of
predictability and large scale simulations in terms of cost. LED is applicable
to systems ranging from chemistry to fluid mechanics and reduces the
computational effort by up to two orders of magnitude while maintaining the
prediction accuracy of the full system dynamics. We argue that LED provides a
novel potent modality for the accurate prediction of complex systems.

    

### [[2006.16433] Fast OSCAR and OWL Regression via Safe Screening Rules](http://arxiv.org/abs/2006.16433)


  Ordered Weighted $L_{1}$ (OWL) regularized regression is a new regression
analysis for high-dimensional sparse learning. Proximal gradient methods are
used as standard approaches to solve OWL regression. However, it is still a
burning issue to solve OWL regression due to considerable computational cost
and memory usage when the feature or sample size is large. In this paper, we
propose the first safe screening rule for OWL regression by exploring the order
of the primal solution with the unknown order structure via an iterative
strategy, which overcomes the difficulties of tackling the non-separable
regularizer. It effectively avoids the updates of the parameters whose
coefficients must be zero during the learning process. More importantly, the
proposed screening rule can be easily applied to standard and stochastic
proximal gradient methods. Moreover, we prove that the algorithms with our
screening rule are guaranteed to have identical results with the original
algorithms. Experimental results on a variety of datasets show that our
screening rule leads to a significant computational gain without any loss of
accuracy, compared to existing competitive algorithms.

    

### [[2008.11901] Multi-View Fusion of Sensor Data for Improved Perception and Prediction in Autonomous Driving](http://arxiv.org/abs/2008.11901)


  We present an end-to-end method for object detection and trajectory
prediction utilizing multi-view representations of LiDAR returns and camera
images. In this work, we recognize the strengths and weaknesses of different
view representations, and we propose an efficient and generic fusing method
that aggregates benefits from all views. Our model builds on a state-of-the-art
Bird's-Eye View (BEV) network that fuses voxelized features from a sequence of
historical LiDAR data as well as rasterized high-definition map to perform
detection and prediction tasks. We extend this model with additional LiDAR
Range-View (RV) features that use the raw LiDAR information in its native,
non-quantized representation. The RV feature map is projected into BEV and
fused with the BEV features computed from LiDAR and high-definition map. The
fused features are then further processed to output the final detections and
trajectories, within a single end-to-end trainable network. In addition, the RV
fusion of LiDAR and camera is performed in a straightforward and
computationally efficient manner using this framework. The proposed multi-view
fusion approach improves the state-of-the-art on proprietary large-scale
real-world data collected by a fleet of self-driving vehicles, as well as on
the public nuScenes data set with minimal increases on the computational cost.

    

### [[2009.00470] Data Anomaly Detection for Structural Health Monitoring of Bridges using Shapelet Transform](http://arxiv.org/abs/2009.00470)


  With the wider availability of sensor technology, a number of Structural
Health Monitoring (SHM) systems are deployed to monitor civil infrastructure.
The continuous monitoring provides valuable information about the structure
that can help in providing a decision support system for retrofits and other
structural modifications. However, when the sensors are exposed to harsh
environmental conditions, the data measured by the SHM systems tend to be
affected by multiple anomalies caused by faulty or broken sensors. Given a
deluge of high-dimensional data collected continuously over time, research into
using machine learning methods to detect anomalies are a topic of great
interest to the SHM community. This paper contributes to this effort by
proposing the use of a relatively new time series representation named Shapelet
Transform in combination with a Random Forest classifier to autonomously
identify anomalies in SHM data. The shapelet transform is a unique time series
representation that is solely based on the shape of the time series data. In
consideration of the individual characteristics unique to every anomaly, the
application of this transform yields a new shape-based feature representation
that can be combined with any standard machine learning algorithm to detect
anomalous data with no manual intervention. For the present study, the anomaly
detection framework consists of three steps: identifying unique shapes from
anomalous data, using these shapes to transform the SHM data into a local-shape
space and training machine learning algorithm on this transformed data to
identify anomalies. The efficacy of this method is demonstrated by the
identification of anomalies in acceleration data from a SHM system installed on
a long-span bridge in China. The results show that multiple data anomalies in
SHM data can be automatically detected with high accuracy using the proposed
method.

    

### [[2010.02709] An Infinite-Feature Extension for Bayesian ReLU Nets That Fixes Their Asymptotic Overconfidence](http://arxiv.org/abs/2010.02709)


  A Bayesian treatment can mitigate overconfidence in ReLU nets around the
training data. But far away from them, ReLU Bayesian neural networks (BNNs) can
still underestimate uncertainty and thus be asymptotically overconfident. This
issue arises since the output variance of a BNN with finitely many features is
quadratic in the distance from the data region. Meanwhile, Bayesian linear
models with ReLU features converge, in the infinite-width limit, to a
particular Gaussian process (GP) with a variance that grows cubically so that
no asymptotic overconfidence can occur. While this may seem of mostly
theoretical interest, in this work, we show that it can be used in practice to
the benefit of BNNs. We extend finite ReLU BNNs with infinite ReLU features via
the GP and show that the resulting model is asymptotically maximally uncertain
far away from the data while the BNNs' predictive power is unaffected near the
data. Although the resulting model approximates a full GP posterior, thanks to
its structure, it can be applied \emph{post-hoc} to any pre-trained ReLU BNN at
a low cost.

    

### [[2011.01921] Optimizing Molecules using Efficient Queries from Property Evaluations](http://arxiv.org/abs/2011.01921)


  Machine learning based methods have shown potential for optimizing existing
molecules with more desirable properties, a critical step towards accelerating
new chemical discovery. Here we propose QMO, a generic query-based molecule
optimization framework that exploits latent embeddings from a molecule
autoencoder. QMO improves the desired properties of an input molecule based on
efficient queries, guided by a set of molecular property predictions and
evaluation metrics. We show that QMO outperforms existing methods in the
benchmark tasks of optimizing small organic molecules for drug-likeness and
solubility under similarity constraints. We also demonstrate significant
property improvement using QMO on two new and challenging tasks that are also
important in real-world discovery problems: (i) optimizing existing potential
SARS-CoV-2 Main Protease inhibitors toward higher binding affinity; and (ii)
improving known antimicrobial peptides towards lower toxicity. Results from QMO
show high consistency with external validations, suggesting effective means to
facilitate material optimization problems with design constraints.

    

### [[2011.08001] Deep-LIBRA: Artificial intelligence method for robust quantification of breast density with independent validation in breast cancer risk assessment](http://arxiv.org/abs/2011.08001)


  Breast density is an important risk factor for breast cancer that also
affects the specificity and sensitivity of screening mammography. Current
federal legislation mandates reporting of breast density for all women
undergoing breast screening. Clinically, breast density is assessed visually
using the American College of Radiology Breast Imaging Reporting And Data
System (BI-RADS) scale. Here, we introduce an artificial intelligence (AI)
method to estimate breast percentage density (PD) from digital mammograms. Our
method leverages deep learning (DL) using two convolutional neural network
architectures to accurately segment the breast area. A machine-learning
algorithm combining superpixel generation, texture feature analysis, and
support vector machine is then applied to differentiate dense from non-dense
tissue regions, from which PD is estimated. Our method has been trained and
validated on a multi-ethnic, multi-institutional dataset of 15,661 images
(4,437 women), and then tested on an independent dataset of 6,368 digital
mammograms (1,702 women; cases=414) for both PD estimation and discrimination
of breast cancer. On the independent dataset, PD estimates from Deep-LIBRA and
an expert reader were strongly correlated (Spearman correlation coefficient =
0.90). Moreover, Deep-LIBRA yielded a higher breast cancer discrimination
performance (area under the ROC curve, AUC = 0.611 [95% confidence interval
(CI): 0.583, 0.639]) compared to four other widely-used research and commercial
PD assessment methods (AUCs = 0.528 to 0.588). Our results suggest a strong
agreement of PD estimates between Deep-LIBRA and gold-standard assessment by an
expert reader, as well as improved performance in breast cancer risk assessment
over state-of-the-art open-source and commercial methods.

    

### [[2011.14078] Unsupervised Constrained Community Detection via Self-Expressive Graph Neural Network](http://arxiv.org/abs/2011.14078)


  Graph neural networks (GNNs) are able to achieve promising performance on
multiple graph downstream tasks such as node classification and link
prediction. Comparatively lesser work has been done to design GNNs which can
operate directly for community detection on graphs. Traditionally, GNNs are
trained on a semi-supervised or self-supervised loss function and then
clustering algorithms are applied to detect communities. However, such
decoupled approaches are inherently sub-optimal. Designing an unsupervised loss
function to train a GNN and extract communities in an integrated manner is a
fundamental challenge. To tackle this problem, we combine the principle of
self-expressiveness with the framework of self-supervised graph neural network
for unsupervised community detection for the first time in literature. Our
solution is trained in an end-to-end fashion and achieves state-of-the-art
community detection performance on multiple publicly available datasets.

    

### [[2012.11532] Dual-CyCon Net: A Cycle Consistent Dual-Domain Convolutional Neural Network Framework for Detection of Partial Discharge](http://arxiv.org/abs/2012.11532)


  In the last decade, researchers have been investigating the severity of
insulation breakdown caused by partial discharge (PD) in overhead transmission
lines with covered conductors or electrical equipment such as generators and
motors used in various industries. Developing an effective partial discharge
detection system can lead to significant savings on maintenance and prevent
power disruptions. Traditional methods rely on hand-crafted features and domain
expertise to identify partial discharge patterns in the electrical current.
Many data-driven deep learning-based methods have been proposed in recent years
to remove these ad hoc feature extraction. However, most of these methods
either operate in the time-domain or frequency-domain. Many research approaches
have been developed to generate phase-resolved partial discharge (PRPD)
patterns from raw PD sensor data. These PRPD diagrams suggest a correlation
between partial discharge activities occurring in an alternating electrical
waveform's positive and negative half-cycles. However, this correlation
criterion between half-cycles has been remained unexplored in deep
learning-based methods. This work proposes a novel feature-fusion-based
Dual-CyCon Net that can utilize all time, frequency, and phase domain features
for joint learning in one cohesive framework. Our proposed cycle-consistency
loss exploits any relation between an alternating electrical signal's positive
and negative half-cycles to calibrate the model's sensitivity. This loss
explores cycle-invariant PD-specific features, enabling the model to learn more
robust, noise-invariant features for PD detection. A case study of our proposed
framework on a public real-world noisy measurement from high-frequency voltage
sensors to detect damaged power lines has achieved a state-of-the-art MCC score
of 0.8455.

    

### [[2102.03613] Linear Matrix Inequality Approaches to Koopman Operator Approximation](http://arxiv.org/abs/2102.03613)


  The regression problem associated with finding a matrix approximation of the
Koopman operator from data is considered. The regression problem is formulated
as a convex optimization problem subject to linear matrix inequality (LMI)
constraints. Doing so allows for additional LMI constraints to be incorporated
into the regression problem. In particular, asymptotic stability constraints,
regularization using matrix norms, and even regularization using system norms
can be easily incorporated into the regression problem.

    

### [[2102.04170] Learning Task-Oriented Communication for Edge Inference: An Information Bottleneck Approach](http://arxiv.org/abs/2102.04170)


  This paper investigates task-oriented communication for edge inference, where
a low-end edge device transmits the extracted feature vector of a local data
sample to a powerful edge server for processing. It is critical to encode the
data into an informative and compact representation for low-latency inference
given the limited bandwidth. We propose a learning-based communication scheme
that jointly optimizes feature extraction, source coding, and channel coding in
a task-oriented manner, i.e., targeting the downstream inference task rather
than data reconstruction. Specifically, we leverage an information bottleneck
(IB) framework to formalize a rate-distortion tradeoff between the
informativeness of the encoded feature and the inference performance. As the IB
optimization is computationally prohibitive for the high-dimensional data, we
adopt a variational approximation, namely the variational information
bottleneck (VIB), to build a tractable upper bound. To reduce the communication
overhead, we leverage a sparsity-inducing distribution as the variational prior
for the VIB framework to sparsify the encoded feature vector. Furthermore,
considering dynamic channel conditions in practical communication systems, we
propose a variable-length feature encoding scheme based on dynamic neural
networks to adaptively adjust the activated dimensions of the encoded feature
to different channel conditions. Extensive experiments evidence that the
proposed task-oriented communication system achieves a better rate-distortion
tradeoff than baseline methods and significantly reduces the feature
transmission latency in dynamic channel conditions.

    

### [[2102.04523] Multi-Objective Learning to Predict Pareto Fronts Using Hypervolume Maximization](http://arxiv.org/abs/2102.04523)


  Real-world problems are often multi-objective with decision-makers unable to
specify a priori which trade-off between the conflicting objectives is
preferable. Intuitively, building machine learning solutions in such cases
would entail providing multiple predictions that span and uniformly cover the
Pareto front of all optimal trade-off solutions. We propose a novel approach
for multi-objective training of neural networks to approximate the Pareto front
during inference. In our approach, the neural networks are trained
multi-objectively using a dynamic loss function, wherein each network's losses
(corresponding to multiple objectives) are weighted by their hypervolume
maximizing gradients. We discuss and illustrate why training processes to
approximate Pareto fronts need to optimize on fronts of individual training
samples instead of on only the front of average losses. Experiments on three
multi-objective problems show that our approach returns outputs that are
well-spread across different trade-offs on the approximated Pareto front
without requiring the trade-off vectors to be specified a priori. Further,
results of comparisons with the state-of-the-art approaches highlight the added
value of our proposed approach, especially in asymmetric Pareto fronts.

    

### [[2102.04776] Generative Models as Distributions of Functions](http://arxiv.org/abs/2102.04776)


  Generative models are typically trained on grid-like data such as images. As
a result, the size of these models usually scales directly with the underlying
grid resolution. In this paper, we abandon discretized grids and instead
parameterize individual data points by continuous functions. We then build
generative models by learning distributions over such functions. By treating
data points as functions, we can abstract away from the specific type of data
we train on and construct models that are agnostic to discretization. To train
our model, we use an adversarial approach with a discriminator that acts on
continuous signals. Through experiments on a wide variety of data modalities
including images, 3D shapes and climate data, we demonstrate that our model can
learn rich distributions of functions independently of data type and
resolution.

    

### [[2102.09750] Symplectic Adjoint Method for Exact Gradient of Neural ODE with Minimal Memory](http://arxiv.org/abs/2102.09750)


  A neural network model of a differential equation, namely neural ODE, has
enabled the learning of continuous-time dynamical systems and probabilistic
distributions with high accuracy. The neural ODE uses the same network
repeatedly during a numerical integration. The memory consumption of the
backpropagation algorithm is proportional to the number of uses times the
network size. This is true even if a checkpointing scheme divides the
computation graph into sub-graphs. Otherwise, the adjoint method obtains a
gradient by a numerical integration backward in time. Although this method
consumes memory only for a single network use, it requires high computational
cost to suppress numerical errors. This study proposes the symplectic adjoint
method, which is an adjoint method solved by a symplectic integrator. The
symplectic adjoint method obtains the exact gradient (up to rounding error)
with memory proportional to the number of uses plus the network size. The
experimental results demonstrate that the symplectic adjoint method consumes
much less memory than the naive backpropagation algorithm and checkpointing
schemes, performs faster than the adjoint method, and is more robust to
rounding errors.

    

### [[2102.13566] Sparse approximation in learning via neural ODEs](http://arxiv.org/abs/2102.13566)


  We consider the neural ODE and optimal control perspective of supervised
learning with $L^1(0,T;\mathbb{R}^{d_u})$ control penalties, where rather than
only minimizing a final cost for the state, we integrate this cost over the
entire time horizon. Under natural homogeneity assumptions on the nonlinear
dynamics, we prove that any optimal control (for this cost) is sparse, in the
sense that it vanishes beyond some positive stopping time. We also provide a
polynomial stability estimate for the running cost of the state with respect to
the time horizon. This can be seen as a \emph{turnpike property} result, for
nonsmooth functionals and dynamics, and without any smallness assumptions on
the data, both of which are new in the literature. In practical terms, the
temporal sparsity and stability results could then be used to discard
unnecessary layers in the corresponding residual neural network (ResNet),
without removing relevant information.

    

### [[2103.00370] Axiomatic Explanations for Visual Search, Retrieval, and Similarity Learning](http://arxiv.org/abs/2103.00370)


  Visual search, recommendation, and contrastive similarity learning power a
wide breadth of technologies that impact billions of users across the world.
The best-performing approaches are often complex and difficult to interpret,
and there are several competing techniques one can use to explain a search
engine's behavior. We show that the theory of fair credit assignment provides a
unique axiomatic solution that generalizes several existing recommendation- and
metric-explainability techniques in the literature. Using this formalism, we
are able to determine in what regimes existing approaches fall short of
fairness and provide variations that are fair in more situations and handle
counterfactual information. More specifically, we show existing approaches
implicitly approximate second-order Shapley-Taylor indices and use this
perspective to extend CAM, GradCAM, LIME, SHAP, SBSM, and other methods to
search engines. These extensions can extract pairwise correspondences between
images from trained black-box models. We also introduce a fast kernel-based
method for estimating Shapley-Taylor indices that require orders of magnitude
fewer function evaluations to converge. Finally, we evaluate these methods and
show that these game-theoretic measures yield more consistent explanations for
image similarity architectures.

    

### [[2103.13389] Generating Novel Scene Compositions from Single Images and Videos](http://arxiv.org/abs/2103.13389)


  Given a large dataset for training, GANs can achieve remarkable performance
for the image synthesis task. However, training GANs in extremely low data
regimes remains a challenge, as overfitting often occurs, leading to
memorization or training divergence. In this work, we introduce SIV-GAN, an
unconditional generative model that can generate new scene compositions from a
single training image or a single video clip. We propose a two-branch
discriminator architecture, with content and layout branches designed to judge
internal content and scene layout realism separately from each other. This
discriminator design enables synthesis of visually plausible, novel
compositions of a scene, with varying content and layout, while preserving the
context of the original sample. Compared to previous single-image GANs, our
model generates more diverse, higher quality images, while not being restricted
to a single image setting. We show that SIV-GAN successfully deals with a new
challenging task of learning from a single video, for which prior GAN models
fail to achieve synthesis of both high quality and diversity.

    

### [[2104.05702] Image-Level or Object-Level? A Tale of Two Resampling Strategies for Long-Tailed Detection](http://arxiv.org/abs/2104.05702)


  Training on datasets with long-tailed distributions has been challenging for
major recognition tasks such as classification and detection. To deal with this
challenge, image resampling is typically introduced as a simple but effective
approach. However, we observe that long-tailed detection differs from
classification since multiple classes may be present in one image. As a result,
image resampling alone is not enough to yield a sufficiently balanced
distribution at the object level. We address object-level resampling by
introducing an object-centric memory replay strategy based on dynamic, episodic
memory banks. Our proposed strategy has two benefits: 1) convenient
object-level resampling without significant extra computation, and 2) implicit
feature-level augmentation from model updates. We show that image-level and
object-level resamplings are both important, and thus unify them with a joint
resampling strategy (RIO). Our method outperforms state-of-the-art long-tailed
detection and segmentation methods on LVIS v0.5 across various backbones. Code
is available at this https URL.

    

### [[2104.06703] Deep Permutation Equivariant Structure from Motion](http://arxiv.org/abs/2104.06703)


  Existing deep methods produce highly accurate 3D reconstructions in stereo
and multiview stereo settings, i.e., when cameras are both internally and
externally calibrated. Nevertheless, the challenge of simultaneous recovery of
camera poses and 3D scene structure in multiview settings with deep networks is
still outstanding. Inspired by projective factorization for Structure from
Motion (SFM) and by deep matrix completion techniques, we propose a neural
network architecture that, given a set of point tracks in multiple images of a
static scene, recovers both the camera parameters and a (sparse) scene
structure by minimizing an unsupervised reprojection loss. Our network
architecture is designed to respect the structure of the problem: the sought
output is equivariant to permutations of both cameras and scene points.
Notably, our method does not require initialization of camera parameters or 3D
point locations. We test our architecture in two setups: (1) single scene
reconstruction and (2) learning from multiple scenes. Our experiments,
conducted on a variety of datasets in both internally calibrated and
uncalibrated settings, indicate that our method accurately recovers pose and
structure, on par with classical state of the art methods. Additionally, we
show that a pre-trained network can be used to reconstruct novel scenes using
inexpensive fine-tuning with no loss of accuracy.

    

### [[2104.09946] A cappella: Audio-visual Singing Voice Separation](http://arxiv.org/abs/2104.09946)


  The task of isolating a target singing voice in music videos has useful
applications. In this work, we explore the single-channel singing voice
separation problem from a multimodal perspective, by jointly learning from
audio and visual modalities. To do so, we present Acappella, a dataset spanning
around 46 hours of a cappella solo singing videos sourced from YouTube. We also
propose an audio-visual convolutional network based on graphs which achieves
state-of-the-art singing voice separation results on our dataset and compare it
against its audio-only counterpart, U-Net, and a state-of-the-art audio-visual
speech separation model. We evaluate the models in the following challenging
setups: i) presence of overlapping voices in the audio mixtures, ii) the target
voice set to lower volume levels in the mix, and iii) combination of i) and
ii). The third one being the most challenging evaluation setup. We demonstrate
that our model outperforms the baseline models in the singing voice separation
task in the most challenging evaluation setup. The code, the pre-trained
models, and the dataset are publicly available at
this https URL at this https URL


### [[2104.09987] Differentiable Model Compression via Pseudo Quantization Noise](http://arxiv.org/abs/2104.09987)


  We propose to add independent pseudo quantization noise to model parameters
during training to approximate the effect of a quantization operator. This
method, DiffQ, is differentiable both with respect to the unquantized
parameters, and the number of bits used. Given a single hyper-parameter
expressing the desired balance between the quantized model size and accuracy,
DiffQ can optimize the number of bits used per individual weight or groups of
weights, in a single training. We experimentally verify that our method
outperforms state-of-the-art quantization techniques on several benchmarks and
architectures for image classification, language modeling, and audio source
separation. For instance, on the Wikitext-103 language modeling benchmark,
DiffQ compresses a 16 layers transformer model by a factor of 8, equivalent to
4 bits precision, with a loss of 0.3$\%$ in model accuracy. Code is available
at: this https URL


### [[2104.10873] Mosaic Flows: A Transferable Deep Learning Framework for Solving PDEs on Unseen Domains](http://arxiv.org/abs/2104.10873)


  Physics-informed neural networks (PINNs) are increasingly employed to
replace/augment traditional numerical methods in solving partial differential
equations (PDEs). While state-of-the-art PINNs have many attractive features,
they approximate a specific realization of a PDE system and hence are
problem-specific. That is, the model needs to be re-trained each time the
boundary conditions (BCs) and domain shape/size change. This limitation
prohibits the application of PINNs to realistic or large-scale engineering
problems especially since the costs and efforts associated with their training
are considerable. We introduce a transferable framework for solving boundary
value problems (BVPs) via deep neural networks which can be trained once and
used forever for various unseen domains and BCs. We first introduce genomic
flow network(GFNet), a neural network that can infer the solution of a BVP
across arbitrary BCson a small square domain called genome. Then, we proposed
mosaic flow(MF) predictor, a novel iterative algorithm that assembles the
GFNet's inferences for BVPs on large domains with unseen sizes/shapes and BCs
while preserving the spatial regularity of the solution. We demonstrate that
our framework can estimate the solution of Laplace and Navier-Stokes equations
in domains of unseen shapes and BCs that are, respectively, 1200 and 12 times
larger than the training domains. Since our framework eliminates the need to
re-train models for unseen domains and BCs, it demonstrates up to 3
orders-of-magnitude speedups compared to the state-of-the-art.

    

### [[2104.11061] Chasing Collective Variables using Autoencoders and biased trajectories](http://arxiv.org/abs/2104.11061)


  Free energy biasing methods have proven to be powerful tools to accelerate
the simulation of important conformational changes of molecules by modifying
the sampling measure. However, most of these methods rely on the prior
knowledge of low-dimensional slow degrees of freedom, i.e. Collective Variables
(CV). Alternatively, such CVs can be identified using machine learning (ML) and
dimensionality reduction algorithms. In this context, approaches where the CVs
are learned in an iterative way using adaptive biasing have been proposed: at
each iteration, the learned CV is used to perform free energy adaptive biasing
to generate new data and learn a new CV. In this paper, we introduce a new
iterative method involving CV learning with autoencoders: Free Energy Biasing
and Iterative Learning with AutoEncoders (FEBILAE). Our method includes a
reweighting scheme to ensure that the learning model optimizes the same loss at
each iteration, and achieves CV convergence. Using the alanine dipeptide system
and the solvated chignolin mini-protein system as examples, we present results
of our algorithm using the extended adaptive biasing force as the free energy
adaptive biasing method.

    

### [[2105.01593] Regret Bounds for Stochastic Shortest Path Problems with Linear Function Approximation](http://arxiv.org/abs/2105.01593)


  We propose an algorithm that uses linear function approximation (LFA) for
stochastic shortest path (SSP). Under minimal assumptions, it obtains sublinear
regret, is computationally efficient, and uses stationary policies. To our
knowledge, this is the first such algorithm in the LFA literature (for SSP or
other formulations). Our algorithm is a special case of a more general one,
which achieves regret square root in the number of episodes given access to a
certain computation oracle.

    

### [[2105.01648] On Lottery Tickets and Minimal Task Representations in Deep Reinforcement Learning](http://arxiv.org/abs/2105.01648)


  The lottery ticket hypothesis questions the role of overparameterization in
supervised deep learning. But how is the performance of winning lottery tickets
affected by the distributional shift inherent to reinforcement learning
problems? In this work, we address this question by comparing sparse agents who
have to address the non-stationarity of the exploration-exploitation problem
with supervised agents trained to imitate an expert. We show that feed-forward
networks trained with behavioural cloning compared to reinforcement learning
can be pruned to higher levels of sparsity without performance degradation.
This suggests that in order to solve the RL-specific distributional shift
agents require more degrees of freedom. Using a set of carefully designed
baseline conditions, we find that the majority of the lottery ticket effect in
both learning paradigms can be attributed to the identified mask rather than
the weight initialization. The input layer mask selectively prunes entire input
dimensions that turn out to be irrelevant for the task at hand. At a moderate
level of sparsity the mask identified by iterative magnitude pruning yields
minimal task-relevant representations, i.e., an interpretable inductive bias.
Finally, we propose a simple initialization rescaling which promotes the robust
identification of sparse task representations in low-dimensional control tasks.

    

### [[2105.08990] Improved Exploring Starts by Kernel Density Estimation-Based State-Space Coverage Acceleration in Reinforcement Learning](http://arxiv.org/abs/2105.08990)


  Reinforcement learning (RL) is currently a popular research topic in control
engineering and has the potential to make its way to industrial and commercial
applications. Corresponding RL controllers are trained in direct interaction
with the controlled system, rendering them data-driven and performance-oriented
solutions. The best practice of exploring starts (ES) is used by default to
support the learning process via randomly picked initial states. However, this
method might deliver strongly biased results if the system's dynamic and
constraints lead to unfavorable sample distributions in the state space (e.g.,
condensed sample accumulation in certain state-space areas). To overcome this
issue, a kernel density estimation-based state-space coverage acceleration
(DESSCA) is proposed, which improves the ES concept by prioritizing
infrequently visited states for a more balanced coverage of the state space
during training. Compared to neighbouring methods in the field of count-based
exploration, DESSCA can also be applied to continuous state spaces without the
need for artificial discretization of the states. Moreover, the algorithm
allows to define arbitrary reference state distributions such that the state
coverage can be shaped w.r.t. the application needs. Considered test scenarios
are mountain car, cartpole and electric motor control environments. Using DQN
and DDPG as exemplary RL algorithms, it can be shown that DESSCA is a simple
yet effective algorithmic extension to the established ES approach that enables
an increase in learning stability as well as the final control performance.

    

### [[2105.10594] Privacy Amplification Via Bernoulli Sampling](http://arxiv.org/abs/2105.10594)


  Balancing privacy and accuracy is a major challenge in designing
differentially private machine learning algorithms. One way to improve this
tradeoff for free is to leverage the noise in common data operations that
already use randomness. Such operations include noisy SGD and data subsampling.
The additional noise in these operations may amplify the privacy guarantee of
the overall algorithm, a phenomenon known as privacy amplification. In this
paper, we analyze the privacy amplification of sampling from a multidimensional
Bernoulli distribution family given the parameter from a private algorithm.
This setup has applications to Bayesian inference and to data compression. We
provide an algorithm to compute the amplification factor, and we establish
upper and lower bounds on this factor.

    

### [[2105.11724] SHAFF: Fast and consistent SHApley eFfect estimates via random Forests](http://arxiv.org/abs/2105.11724)


  Interpretability of learning algorithms is crucial for applications involving
critical decisions, and variable importance is one of the main interpretation
tools. Shapley effects are now widely used to interpret both tree ensembles and
neural networks, as they can efficiently handle dependence and interactions in
the data, as opposed to most other variable importance measures. However,
estimating Shapley effects is a challenging task, because of the computational
complexity and the conditional expectation estimates. Accordingly, existing
Shapley algorithms have flaws: a costly running time, or a bias when input
variables are dependent. Therefore, we introduce SHAFF, SHApley eFfects via
random Forests, a fast and accurate Shapley effect estimate, even when input
variables are dependent. We show SHAFF efficiency through both a theoretical
analysis of its consistency, and the practical performance improvements over
competitors with extensive experiments. An implementation of SHAFF in C++ and R
is available online.

    

### [[2105.13493] Efficient and Accurate Gradients for Neural SDEs](http://arxiv.org/abs/2105.13493)


  Neural SDEs combine many of the best qualities of both RNNs and SDEs: memory
efficient training, high-capacity function approximation, and strong priors on
model space. This makes them a natural choice for modelling many types of
temporal dynamics. Training a Neural SDE (either as a VAE or as a GAN) requires
backpropagating through an SDE solve. This may be done by solving a
backwards-in-time SDE whose solution is the desired parameter gradients.
However, this has previously suffered from severe speed and accuracy issues,
due to high computational cost and numerical truncation errors. Here, we
overcome these issues through several technical innovations. First, we
introduce the \textit{reversible Heun method}. This is a new SDE solver that is
\textit{algebraically reversible}: eliminating numerical gradient errors, and
the first such solver of which we are aware. Moreover it requires half as many
function evaluations as comparable solvers, giving up to a $1.98\times$
speedup. Second, we introduce the \textit{Brownian Interval}: a new, fast,
memory efficient, and exact way of sampling \textit{and reconstructing}
Brownian motion. With this we obtain up to a $10.6\times$ speed improvement
over previous techniques, which in contrast are both approximate and relatively
slow. Third, when specifically training Neural SDEs as GANs (Kidger et al.
2021), we demonstrate how SDE-GANs may be trained through careful weight
clipping and choice of activation function. This reduces computational cost
(giving up to a $1.87\times$ speedup) and removes the numerical truncation
errors associated with gradient penalty. Altogether, we outperform the
state-of-the-art by substantial margins, with respect to training speed, and
with respect to classification, prediction, and MMD test metrics. We have
contributed implementations of all of our techniques to the torchsde library to
help facilitate their adoption.

    

### [[2106.02770] Accelerating Stochastic Simulation with Interactive Neural Processes](http://arxiv.org/abs/2106.02770)


  Stochastic simulations such as large-scale, spatiotemporal, age-structured
epidemic models are computationally expensive at fine-grained resolution. We
propose Interactive Neural Process (INP), a Bayesian active learning framework
to proactively learn a deep learning surrogate model and accelerate simulation.
Our framework is based on the novel integration of neural process, deep
sequence model and active learning. In particular, we develop a novel
spatiotemporal neural process model to mimic the simulator dynamics. Our model
automatically infers the latent process which describes the intrinsic
uncertainty of the simulator. This also gives rise to a new acquisition
function based on the latent information gain. We design Bayesian active
learning algorithms to iteratively query the simulator, gather more data, and
continuously improve the model. We perform theoretical analysis and demonstrate
that our approach reduces sample complexity compared with random sampling in
high dimension. Empirically, we demonstrate our framework can faithfully
imitate the behavior of a complex infectious disease simulator with a small
number of examples, enabling rapid simulation and scenario exploration.

    

### [[2106.09305] Time Series is a Special Sequence: Forecasting with Sample Convolution and Interaction](http://arxiv.org/abs/2106.09305)


  Time series is a special type of sequence data, a set of observations
collected at even time intervals and ordered chronologically. Existing deep
learning techniques use generic sequence models (e.g., recurrent neural
network, Transformer model, or temporal convolutional network) for time series
analysis, which ignore some of its unique properties. In particular, three
components characterize time series: trend, seasonality, and irregular
components, and the former two components enable us to perform forecasting with
reasonable accuracy. Other types of sequence data do not have such
characteristics. Motivated by the above, in this paper, we propose a novel
neural network architecture that conducts sample convolution and interaction
for temporal modeling and apply it for the time series forecasting problem,
namely \textbf{SCINet}. Compared to conventional dilated causal convolution
architectures, the proposed downsample-convolve-interact architecture enables
multi-resolution analysis besides expanding the receptive field of the
convolution operation, which facilitates extracting temporal relation features
with enhanced predictability. Experimental results show that SCINet achieves
significant prediction accuracy improvement over existing solutions across
various real-world time series forecasting datasets.

    

### [[2106.10314] Differentiable Particle Filtering without Modifying the Forward Pass](http://arxiv.org/abs/2106.10314)


  Particle filters are not compatible with automatic differentiation due to the
presence of discrete resampling steps. While known estimators for the score
function, based on Fisher's identity, can be computed using particle filters,
up to this point they required manual implementation. In this paper we show
that such estimators can be computed using automatic differentiation, after
introducing a simple correction to the particle weights. This correction
utilizes the stop-gradient operator and does not modify the particle filter
operation on the forward pass, while also being cheap and easy to compute.
Surprisingly, with the same correction automatic differentiation also produces
good estimators for gradients of expectations under the posterior. We can
therefore regard our method as a general recipe for making particle filters
differentiable. We additionally show that it produces desired estimators for
second-order derivatives and how to extend it to further reduce variance at the
expense of additional computation.

    

### [[2106.12535] Learning Stochastic Majority Votes by Minimizing a PAC-Bayes Generalization Bound](http://arxiv.org/abs/2106.12535)


  We investigate a stochastic counterpart of majority votes over finite
ensembles of classifiers, and study its generalization properties. While our
approach holds for arbitrary distributions, we instantiate it with Dirichlet
distributions: this allows for a closed-form and differentiable expression for
the expected risk, which then turns the generalization bound into a tractable
training objective. The resulting stochastic majority vote learning algorithm
achieves state-of-the-art accuracy and benefits from (non-vacuous) tight
generalization bounds, in a series of numerical experiments when compared to
competing algorithms which also minimize PAC-Bayes objectives -- both with
uninformed (data-independent) and informed (data-dependent) priors.

    

### [[2107.05612] A Persistent Spatial Semantic Representation for High-level Natural Language Instruction Execution](http://arxiv.org/abs/2107.05612)


  Natural language provides an accessible and expressive interface to specify
long-term tasks for robotic agents. However, non-experts are likely to specify
such tasks with high-level instructions, which abstract over specific robot
actions through several layers of abstraction. We propose that key to bridging
this gap between language and robot actions over long execution horizons are
persistent representations. We propose a persistent spatial semantic
representation method, and show how it enables building an agent that performs
hierarchical reasoning to effectively execute long-term tasks. We evaluate our
approach on the ALFRED benchmark and achieve state-of-the-art results, despite
completely avoiding the commonly used step-by-step instructions.

    

### [[2109.05948] A deep learning guided memetic framework for graph coloring problems](http://arxiv.org/abs/2109.05948)


  Given an undirected graph $G=(V,E)$ with a set of vertices $V$ and a set of
edges $E$, a graph coloring problem involves finding a partition of the
vertices into different independent sets. In this paper we present a new
framework that combines a deep neural network with the best tools of classical
metaheuristics for graph coloring. The proposed method is evaluated on two
popular graph coloring problems (vertex coloring and weighted coloring).
Computational experiments on well-known benchmark graphs show that the proposed
approach is able to obtain highly competitive results for both problems. A
study of the contribution of deep learning in the method highlights that it is
possible to learn relevant patterns useful to obtain better solutions to graph
coloring problems.

    

### [[2109.09304] Deformed semicircle law and concentration of nonlinear random matrices for ultra-wide neural networks](http://arxiv.org/abs/2109.09304)


  In this paper, we study the two-layer fully connected neural network given by
$f(X)=\frac{1}{\sqrt{d_1}}\boldsymbol{a}^\top\sigma\left(WX\right)$, where
$X\in\mathbb{R}^{d_0\times n}$ is a deterministic data matrix,
$W\in\mathbb{R}^{d_1\times d_0}$ and $\boldsymbol{a}\in\mathbb{R}^{d_1}$ are
random Gaussian weights, and $\sigma$ is a nonlinear activation function. We
obtain the limiting spectral distributions of two kernel matrices related to
$f(X)$: the empirical conjugate kernel (CK) and neural tangent kernel (NTK),
beyond the linear-width regime ($d_1\asymp n$). Under the ultra-width regime
$d_1/n\to\infty$, with proper assumptions on $X$ and $\sigma$, a deformed
semicircle law appears. Such limiting law is first proved for general centered
sample covariance matrices with correlation and then specified for our neural
network model. We also prove non-asymptotic concentrations of empirical CK and
NTK around their limiting kernel in the spectral norm, and lower bounds on
their smallest eigenvalues. As an application, we verify the random feature
regression achieves the same asymptotic performance as its limiting kernel
regression in ultra-width limit. The limiting training and test errors for
random feature regression are calculated by corresponding kernel regression. We
also provide a nonlinear Hanson-Wright inequality suitable for neural networks
with random weights and Lipschitz activation functions.

    

### [[2110.08710] NeuralArTS: Structuring Neural Architecture Search with Type Theory](http://arxiv.org/abs/2110.08710)


  Neural Architecture Search (NAS) algorithms automate the task of finding
optimal deep learning architectures given an initial search space of possible
operations. Developing these search spaces is usually a manual affair with
pre-optimized search spaces being more efficient, rather than searching from
scratch. In this paper we present a new framework called Neural Architecture
Type System (NeuralArTS) that categorizes the infinite set of network
operations in a structured type system. We further demonstrate how NeuralArTS
can be applied to convolutional layers and propose several future directions.

    

### [[2110.08820] On-board Fault Diagnosis of a Laboratory Mini SR-30 Gas Turbine Engine](http://arxiv.org/abs/2110.08820)


  Inspired by recent progress in machine learning, a data-driven fault
diagnosis and isolation (FDI) scheme is explicitly developed for failure in the
fuel supply system and sensor measurements of the laboratory gas turbine
system. A passive approach of fault diagnosis is implemented where a model is
trained using machine learning classifiers to detect a given set of fault
scenarios in real-time on which it is trained. Towards the end, a comparative
study is presented for well-known classification techniques, namely Support
vector classifier, linear discriminant analysis, K-neighbor, and decision
trees. Several simulation studies were carried out to demonstrate and
illustrate the proposed fault diagnosis scheme's advantages, capabilities, and
performance.

    

### [[2110.08956] Improving Robustness of Reinforcement Learning for Power System Control with Adversarial Training](http://arxiv.org/abs/2110.08956)


  Due to the proliferation of renewable energy and its intrinsic intermittency
and stochasticity, current power systems face severe operational challenges.
Data-driven decision-making algorithms from reinforcement learning (RL) offer a
solution towards efficiently operating a clean energy system. Although RL
algorithms achieve promising performance compared to model-based control
models, there has been limited investigation of RL robustness in
safety-critical physical systems. In this work, we first show that several
competition-winning, state-of-the-art RL agents proposed for power system
control are vulnerable to adversarial attacks. Specifically, we use an
adversary Markov Decision Process to learn an attack policy, and demonstrate
the potency of our attack by successfully attacking multiple winning agents
from the Learning To Run a Power Network (L2RPN) challenge, under both
white-box and black-box attack settings. We then propose to use adversarial
training to increase the robustness of RL agent against attacks and avoid
infeasible operational decisions. To the best of our knowledge, our work is the
first to highlight the fragility of grid control RL algorithms, and contribute
an effective defense scheme towards improving their robustness and security.

    

### [[2110.08991] Dimensionality Reduction for Wasserstein Barycenter](http://arxiv.org/abs/2110.08991)


  The Wasserstein barycenter is a geometric construct which captures the notion
of centrality among probability distributions, and which has found many
applications in machine learning. However, most algorithms for finding even an
approximate barycenter suffer an exponential dependence on the dimension $d$ of
the underlying space of the distributions. In order to cope with this "curse of
dimensionality," we study dimensionality reduction techniques for the
Wasserstein barycenter problem. When the barycenter is restricted to support of
size $n$, we show that randomized dimensionality reduction can be used to map
the problem to a space of dimension $O(\log n)$ independent of both $d$ and
$k$, and that \emph{any} solution found in the reduced dimension will have its
cost preserved up to arbitrary small error in the original space. We provide
matching upper and lower bounds on the size of the reduced dimension, showing
that our methods are optimal up to constant factors. We also provide a coreset
construction for the Wasserstein barycenter problem that significantly
decreases the number of input distributions. The coresets can be used in
conjunction with random projections and thus further improve computation time.
Lastly, our experimental results validate the speedup provided by
dimensionality reduction while maintaining solution quality.

    

### [[2110.09304] Prediction of Occurrence of Extreme Events using Machine Learning](http://arxiv.org/abs/2110.09304)


  Machine learning models play a vital role in the prediction task in several
fields of study. In this work, we utilize the ability of machine learning
algorithms for the prediction of occurrence of extreme events in a nonlinear
mechanical system. Extreme events are rare events which occur ubiquitously in
nature. We consider four machine learning models, namely Logistic Regression,
Support Vector Machine, Random Forest and Multi-Layer Perceptron in our
prediction task. We train these four machine learning models using training set
data and compute the performance of each model using the test set data. We show
that Multi-Layer Perceptron model performs better among the four models in the
prediction of extreme events in the considered system. The persistent behaviour
of the considered machine learning models are cross-checked with randomly
shuffled training set and test set data.

    

### [[2110.09436] Early Diagnostic Prediction of Covid-19 using Gradient-Boosting Machine Model](http://arxiv.org/abs/2110.09436)


  With the huge spike in the COVID-19 cases across the globe and reverse
transcriptase-polymerase chain reaction (RT-PCR) test remains a key component
for rapid and accurate detection of severe acute respiratory syndrome
coronavirus 2 (SARS-CoV-2). In recent months there has been an acute shortage
of medical supplies in developing countries, especially a lack of RT-PCR
testing resulting in delayed patient care and high infection rates. We present
a gradient-boosting machine model that predicts the diagnostics result of
SARS-CoV- 2 in an RT-PCR test by utilizing eight binary features. We used the
publicly available nationwide dataset released by the Israeli Ministry of
Health.

    

### [[2110.09643] In-memory Multi-valued Associative Processor](http://arxiv.org/abs/2110.09643)


  In-memory associative processor architectures are offered as a great
candidate to overcome memory-wall bottleneck and to enable vector/parallel
arithmetic operations. In this paper, we extend the functionality of the
associative processor to multi-valued arithmetic. To allow for in-memory
compute implementation of arithmetic or logic functions, we propose a
structured methodology enabling the automatic generation of the corresponding
look-up tables (LUTs). We propose two approaches to build the LUTs: a first
approach that formalizes the intuition behind LUT pass ordering and a more
optimized approach that reduces the number of required write cycles. To
demonstrate these methodologies, we present a novel ternary associative
processor (TAP) architecture that is employed to implement efficient ternary
vector in-place addition. A SPICE-MATLAB co-simulator is implemented to test
the functionality of the TAP and to evaluate the performance of the proposed AP
ternary in-place adder implementations in terms of energy, delay, and area.
Results show that compared to the binary AP adder, the ternary AP adder results
in a 12.25\% and 6.2\% reduction in energy and area, respectively. The ternary
AP also demonstrates a 52.64\% reduction in energy and a delay that is up to
9.5x smaller when compared to a state-of-art ternary carry-lookahead adder.

    

### [[2110.09849] Holistic Hardware Security Assessment Framework: A Microarchitectural Perspective](http://arxiv.org/abs/2110.09849)


  Our goal is to enable holistic hardware security evaluation from the
microarchitectural point of view. To achieve this, we propose a framework that
categorizes threat models based on the microarchitectural components being
targeted, and provides a generic security metric that can be used to assess the
vulnerability of components, as well as the system as a whole.

    

### [[2110.09913] Prepartition: Load Balancing Approach for Virtual Machine Reservations in a Cloud Data Center](http://arxiv.org/abs/2110.09913)


  Load balancing is vital for the efficient and long-term operation of cloud
data centers. With virtualization, post (reactive) migration of virtual
machines after allocation is the traditional way for load balancing and
consolidation. However, reactive migration is not easy to obtain predefined
load balance objectives and may interrupt services and bring instability.
Therefore, we provide a new approach, called Prepartition, for load balancing.
It partitions a VM request into a few sub-requests sequentially with start
time, end time and capacity demands, and treats each sub-request as a regular
VM request. In this way, it can proactively set a bound for each VM request on
each physical machine and makes the scheduler get ready before VM migration to
obtain the predefined load balancing goal, which supports the resource
allocation in a fine-grained manner. Simulations with real-world trace and
synthetic data show that Prepartition for offline (PrepartitionOff) scheduling
has 10%-20% better performance than the existing load balancing algorithms
under several metrics, including average utilization, imbalance degree,
makespan and Capacity_makespan. We also extend Prepartition to online load
balancing. Evaluation results show that our proposed approach also outperforms
existing online algorithms.

    

### [[2110.09918] Using RDMA for Efficient Index Replication in LSM Key-Value Stores](http://arxiv.org/abs/2110.09918)


  Log-Structured Merge tree (LSM tree) Key-Value (KV) stores have become a
foundational layer in the storage stacks of datacenter and cloud services.
Current approaches for achieving reliability and availability avoid replication
at the KV store level and instead perform these operations at higher layers,
e.g., the DB layer that runs on top of the KV store. The main reason is that
past designs for replicated KV stores favor reducing network traffic and
increasing I/O size. Therefore, they perform costly compactions to reorganize
data in both the primary and backup nodes, which hurts overall system
performance.
In this paper, we design and implement Talos, an efficient rack-scale
LSM-based KV store that aims to significantly reduce the I/O amplification and
CPU overhead in backup nodes and make replication in the KV store practical. We
rely on two observations: (a) the increased use of RDMA in the datacenter,
which reduces CPU overhead for communication, and (b) the use of KV separation
that is becoming prevalent in modern KV stores. We use a primary-backup
replication scheme that performs compactions only on the primary nodes and
sends the pre-built index to the backup nodes of the region, avoiding all
compactions in backups. Our approach includes an efficient mechanism to deal
with pointer translation across nodes in the region index. Our results show
that Talos reduces in the backup nodes, I/O amplification by up to $3\times$,
CPU overhead by up to $1.6\times$, and memory size needed for the write path by
up to $2\times$, without increasing network bandwidth excessively, and by up to
$1.3\times$. Overall, we show that our approach has benefits even when small KV
pairs dominate in a workload (80%-90%). Finally, it enables KV stores to
operate with larger growth factors (from 10 to 16) to reduce space
amplification without sacrificing precious CPU cycles.

    

### [[2110.09987] Energy-based Accounting Model for Heterogeneous Supercomputers](http://arxiv.org/abs/2110.09987)


  In this paper we present a new accounting model for heterogeneous
supercomputers. An increasing number of supercomputing centres adopt
heterogeneous architectures consisting of CPUs and hardware accelerators for
their systems. Accounting models using the core hour as unit of measure are
redefined to provide an appropriate charging rate based on the computing
performance of different processing elements, as well as their energy
efficiency and purchase price. In this paper we provide an overview of existing
models and define a new model that, while retaining the core hour as a
fundamental concept, takes into account the interplay among resources such as
CPUs and RAM, and that bases the GPU charging rate on energy consumption. We
believe that this model, designed for Pawsey Supercomputing Research Centre's
next supercomputer Setonix, has a lot of advantages compared to other models,
introducing carbon footprint as a primary driver in determining the allocation
of computational workflow on heterogeneous resources.

    

### [[2110.09993] A Unified and Refined Convergence Analysis for Non-Convex Decentralized Learning](http://arxiv.org/abs/2110.09993)


  We study the consensus decentralized optimization problem where the objective
function is the average of $n$ agents private non-convex cost functions;
moreover, the agents can only communicate to their neighbors on a given network
topology. The stochastic online setting is considered in this paper where each
agent can only access a noisy estimate of its gradient. Many decentralized
methods can solve such problems including EXTRA, Exact-Diffusion/D$^2$, and
gradient-tracking. Unlike the famed $\small \text{DSGD}$ algorithm, these
methods have been shown to be robust to the heterogeneity of the local cost
functions. However, the established convergence rates for these methods
indicate that their sensitivity to the network topology is worse than $\small
\text{DSGD}$. Such theoretical results imply that these methods can perform
much worse than $\small \text{DSGD}$ over sparse networks, which, however,
contradicts empirical experiments where $\small \text{DSGD}$ is observed to be
more sensitive to the network topology.
In this work, we study a general stochastic unified decentralized algorithm
($\small\textbf{SUDA}$) that includes the above methods as special cases. We
establish the convergence of $\small\textbf{SUDA}$ under both non-convex and
the Polyak-Lojasiewicz condition settings. Our results provide improved network
topology dependent bounds for these methods (such as Exact-Diffusion/D$^2$ and
gradient-tracking) compared with existing literature. Moreover, our result
shows that these method are less sensitive to the network topology compared to
$\small \text{DSGD}$, which agrees with numerical experiments.

    

### [[2011.10896] HALO 1.0: A Hardware-agnostic Accelerator Orchestration Framework for Enabling Hardware-agnostic Programming with True Performance Portability for Heterogeneous HPC](http://arxiv.org/abs/2011.10896)


  This paper presents HALO 1.0, an open-ended extensible multi-agent software
framework that implements a set of proposed hardware-agnostic accelerator
orchestration (HALO) principles. HALO implements a novel compute-centric
message passing interface (C^2MPI) specification for enabling the
performance-portable execution of a hardware-agnostic host application across
heterogeneous accelerators. The experiment results of evaluating eight widely
used HPC subroutines based on Intel Xeon E5-2620 CPUs, Intel Arria 10 GX FPGAs,
and NVIDIA GeForce RTX 2080 Ti GPUs show that HALO 1.0 allows for a unified
control flow for host programs to run across all the computing devices with a
consistently top performance portability score, which is up to five orders of
magnitude higher than the OpenCL-based solution.

    

### [[2104.07293] Sized Types with Usages for Parallel Complexity of Pi-Calculus Processes](http://arxiv.org/abs/2104.07293)


  We address the problem of analysing the complexity of concurrent programs
written in Pi-calculus. We are interested in parallel complexity, or span,
understood as the execution time in a model with maximal parallelism. A type
system for parallel complexity has been recently proposed by Baillot and
Ghyselen but it is too imprecise for non-linear channels and cannot analyse
some concurrent processes. Aiming for a more precise analysis, we design a type
system which builds on the concepts of sized types and usages. The new variant
of usages we define accounts for the various ways a channel is employed and
relies on time annotations to track under which conditions processes can
synchronize. We prove that a type derivation for a process provides an upper
bound on its parallel complexity.

    

### [[2104.15099] Achieving Causality with Physical Clocks](http://arxiv.org/abs/2104.15099)


  Physical clocks provide more precision than applications can use. For
example, a 64 bit NTP clock allows a precision of 233 picoseconds. In this
paper, we focus on whether the least significant bits that are not useful to
the applications could be used to track (one way) causality among events. We
present PWC (Physical clock With Causality) that uses the extraneous bits in
the physical clock. We show that PWC is very robust to errors in clock skew and
transient errors. We show that PWC can be used as both a physical and logical
clock for a typical distributed application even if just 6-9 extraneous bits
(corresponding to precision of 15-120 nanoseconds) are available. Another
important characteristic of PWC is that the standard integer < operation can be
used to compare timestamps to deduce (one-way) causality among events. Thus,
PWC is significantly more versatile than previous approaches for using the
physical clock to provide causality information.

    

### [[2110.09624] Ideal Partition of Resources for Metareasoning](http://arxiv.org/abs/2110.09624)


  We can achieve significant gains in the value of computation by metareasoning
about the nature or extent of base-level problem solving before executing a
solution. However, resources that are irrevocably committed to metareasoning
are not available for executing a solution. Thus, it is important to determine
the portion of resources we wish to apply to metareasoning and control versus
to the execution of a solution plan. Recent research on rational agency has
highlighted the importance of limiting the consumption of resources by
metareasoning machinery. We shall introduce the metareasoning-partition
problem--the problem of ideally apportioning costly reasoning resources to
planning a solution versus applying resource to executing a solution to a
problem. We exercise prototypical metareasoning-partition models to probe the
relationships between time allocated to metareasoning and to execution for
different problem classes. Finally, we examine the value of metareasoning in
the context of our functional analyses.

    

### [[2110.09665] Ensemble ALBERT on SQuAD 2.0](http://arxiv.org/abs/2110.09665)


  Machine question answering is an essential yet challenging task in natural
language processing. Recently, Pre-trained Contextual Embeddings (PCE) models
like Bidirectional Encoder Representations from Transformers (BERT) and A Lite
BERT (ALBERT) have attracted lots of attention due to their great performance
in a wide range of NLP tasks. In our Paper, we utilized the fine-tuned ALBERT
models and implemented combinations of additional layers (e.g. attention layer,
RNN layer) on top of them to improve model performance on Stanford Question
Answering Dataset (SQuAD 2.0). We implemented four different models with
different layers on top of ALBERT-base model, and two other models based on
ALBERT-xlarge and ALBERT-xxlarge. We compared their performance to our baseline
model ALBERT-base-v2 + ALBERT-SQuAD-out with details. Our best-performing
individual model is ALBERT-xxlarge + ALBERT-SQuAD-out, which achieved an F1
score of 88.435 on the dev set. Furthermore, we have implemented three
different ensemble algorithms to boost overall performance. By passing in
several best-performing models' results into our weighted voting ensemble
algorithm, our final result ranks first on the Stanford CS224N Test PCE SQuAD
Leaderboard with F1 = 90.123.

    

### [[2110.09775] Aesthetic Photo Collage with Deep Reinforcement Learning](http://arxiv.org/abs/2110.09775)


  Photo collage aims to automatically arrange multiple photos on a given canvas
with high aesthetic quality. Existing methods are based mainly on handcrafted
feature optimization, which cannot adequately capture high-level human
aesthetic senses. Deep learning provides a promising way, but owing to the
complexity of collage and lack of training data, a solution has yet to be
found. In this paper, we propose a novel pipeline for automatic generation of
aspect ratio specified collage and the reinforcement learning technique is
introduced in collage for the first time. Inspired by manual collages, we model
the collage generation as sequential decision process to adjust spatial
positions, orientation angles, placement order and the global layout. To
instruct the agent to improve both the overall layout and local details, the
reward function is specially designed for collage, considering subjective and
objective factors. To overcome the lack of training data, we pretrain our deep
aesthetic network on a large scale image aesthetic dataset (CPC) for general
aesthetic feature extraction and propose an attention fusion module for
structural collage feature representation. We test our model against competing
methods on two movie datasets and our results outperform others in aesthetic
quality evaluation. Further user study is also conducted to demonstrate the
effectiveness.

    

### [[2110.09777] Towards Toxic and Narcotic Medication Detection with Rotated Object Detector](http://arxiv.org/abs/2110.09777)


  Recent years have witnessed the advancement of deep learning vision
technologies and applications in the medical industry. Intelligent devices for
special medication management are in great need of, which requires more precise
detection algorithms to identify the specifications and locations. In this
work, YOLO (You only look once) based object detectors are tailored for toxic
and narcotic medications detection tasks. Specifically, a more flexible
annotation with rotated degree ranging from $0^\circ$ to $90^\circ$ and a
mask-mapping-based non-maximum suppression method are proposed to achieve a
feasible and efficient medication detector aiming at arbitrarily oriented
bounding boxes. Extensive experiments demonstrate that the rotated YOLO
detectors are more suitable for identifying densely arranged drugs. The best
shot mean average precision of the proposed network reaches 0.811 while the
inference time is less than 300ms.

    

### [[2110.09784] SSAST: Self-Supervised Audio Spectrogram Transformer](http://arxiv.org/abs/2110.09784)


  Recently, neural networks based purely on self-attention, such as the Vision
Transformer (ViT), have been shown to outperform deep learning models
constructed with convolutional neural networks (CNNs) on various vision tasks,
thus extending the success of Transformers, which were originally developed for
language processing, to the vision domain. A recent study showed that a similar
methodology can also be applied to the audio domain. Specifically, the Audio
Spectrogram Transformer (AST) achieves state-of-the-art results on various
audio classification benchmarks. However, pure Transformer models tend to
require more training data compared to CNNs, and the success of the AST relies
on supervised pretraining that requires a large amount of labeled data and a
complex training pipeline, thus limiting the practical usage of AST.
This paper focuses on audio and speech classification, and aims to alleviate
the data requirement issues with the AST by leveraging self-supervised learning
using unlabeled data. Specifically, we propose to pretrain the AST model with
joint discriminative and generative masked spectrogram patch modeling (MSPM)
using unlabeled audio from AudioSet and Librispeech. We evaluate our pretrained
models on both audio and speech classification tasks including audio event
classification, keyword spotting, emotion recognition, and speaker
identification. The proposed self-supervised framework significantly boosts AST
performance on all tasks, with an average improvement of 60.9%, leading to
similar or even better results than a supervised pretrained AST. To the best of
our knowledge, it is the first patch-based self-supervised learning framework
in the audio and speech domain, and also the first self-supervised learning
framework for AST.

    

### [[2110.09829] Towards Social Situation Awareness in Support Agents](http://arxiv.org/abs/2110.09829)


  Artificial agents that support people in their daily activities (e.g.,
virtual coaches and personal assistants) are increasingly prevalent. Since many
daily activities are social in nature, support agents should understand a
user's social situation to offer comprehensive support. However, there are no
systematic approaches for developing support agents that are social situation
aware. We identify key requirements for a support agent to be social situation
aware and propose steps to realize those requirements. These steps are
presented through a conceptual architecture that centers around two key ideas:
(1) conceptualizing social situation awareness as an instantiation of `general'
situation awareness, and (2) using situation taxonomies as the key element of
such instantiation. This enables support agents to represent a user's social
situation, comprehend its meaning, and assess its impact on the user's
behavior. We discuss empirical results supporting that the proposed approach
can be effective and illustrate how the architecture can be used in support
agents through a use case.

    

### [[2110.09978] What is Learned in Knowledge Graph Embeddings?](http://arxiv.org/abs/2110.09978)


  A knowledge graph (KG) is a data structure which represents entities and
relations as the vertices and edges of a directed graph with edge types. KGs
are an important primitive in modern machine learning and artificial
intelligence. Embedding-based models, such as the seminal TransE [Bordes et
al., 2013] and the recent PairRE [Chao et al., 2020] are among the most popular
and successful approaches for representing KGs and inferring missing edges
(link completion). Their relative success is often credited in the literature
to their ability to learn logical rules between the relations.
In this work, we investigate whether learning rules between relations is
indeed what drives the performance of embedding-based methods. We define motif
learning and two alternative mechanisms, network learning (based only on the
connectivity of the KG, ignoring the relation types), and unstructured
statistical learning (ignoring the connectivity of the graph). Using
experiments on synthetic KGs, we show that KG models can learn motifs and how
this ability is degraded by non-motif (noise) edges. We propose tests to
distinguish the contributions of the three mechanisms to performance, and apply
them to popular KG benchmarks. We also discuss an issue with the standard
performance testing protocol and suggest an improvement.
To appear in the proceedings of Complex Networks 2021.

    

### [[2110.09991] Towards Optimal Correlational Object Search](http://arxiv.org/abs/2110.09991)


  In realistic applications of object search, robots will need to locate target
objects in complex environments while coping with unreliable sensors,
especially for small or hard-to-detect objects. In such settings, correlational
information can be valuable for planning efficiently: when looking for a fork,
the robot could start by locating the easier-to-detect refrigerator, since
forks would probably be found nearby. Previous approaches to object search with
correlational information typically resort to ad-hoc or greedy search
strategies. In this paper, we propose the Correlational Object Search POMDP
(COS-POMDP), which can be solved to produce search strategies that use
correlational information. COS-POMDPs contain a correlation-based observation
model that allows us to avoid the exponential blow-up of maintaining a joint
belief about all objects, while preserving the optimal solution to this naive,
exponential POMDP formulation. We propose a hierarchical planning algorithm to
scale up COS-POMDP for practical domains. We conduct experiments using
AI2-THOR, a realistic simulator of household environments, as well as YOLOv5, a
widely-used object detector. Our results show that, particularly for
hard-to-detect objects, such as scrub brush and remote control, our method
offers the most robust performance compared to baselines that ignore
correlations as well as a greedy, next-best view approach.

    

### [[2110.09994] DPFM: Deep Partial Functional Maps](http://arxiv.org/abs/2110.09994)


  We consider the problem of computing dense correspondences between non-rigid
shapes with potentially significant partiality. Existing formulations tackle
this problem through heavy manifold optimization in the spectral domain, given
hand-crafted shape descriptors. In this paper, we propose the first learning
method aimed directly at partial non-rigid shape correspondence. Our approach
uses the functional map framework, can be trained in a supervised or
unsupervised manner, and learns descriptors directly from the data, thus both
improving robustness and accuracy in challenging cases. Furthermore, unlike
existing techniques, our method is also applicable to partial-to-partial
non-rigid matching, in which the common regions on both shapes are unknown a
priori. We demonstrate that the resulting method is data-efficient, and
achieves state-of-the-art results on several benchmark datasets. Our code and
data can be found online: this https URL


### [[2110.09998] Watch out for the risky actors: Assessing risk in dynamic environments for safe driving](http://arxiv.org/abs/2110.09998)


  Driving in a dynamic environment that consists of other actors is inherently
a risky task as each actor influences the driving decision and may
significantly limit the number of choices in terms of navigation and safety
plan. The risk encountered by the Ego actor depends on the driving scenario and
the uncertainty associated with predicting the future trajectories of the other
actors in the driving scenario. However, not all objects pose a similar risk.
Depending on the object's type, trajectory, position, and the associated
uncertainty with these quantities; some objects pose a much higher risk than
others. The higher the risk associated with an actor, the more attention must
be directed towards that actor in terms of resources and safety planning. In
this paper, we propose a novel risk metric to calculate the importance of each
actor in the world and demonstrate its usefulness through a case study.

    

### [[2110.10007] Gradient-Based Mixed Planning with Discrete and Continuous Actions](http://arxiv.org/abs/2110.10007)


  Dealing with planning problems with both discrete logical relations and
continuous numeric changes in real-world dynamic environments is challenging.
Existing numeric planning systems for the problem often discretize numeric
variables or impose convex quadratic constraints on numeric variables, which
harms the performance when solving the problem. In this paper, we propose a
novel algorithm framework to solve the numeric planning problems mixed with
discrete and continuous actions based on gradient descent. We cast the numeric
planning with discrete and continuous actions as an optimization problem by
integrating a heuristic function based on discrete effects. Specifically, we
propose a gradient-based framework to simultaneously optimize continuous
parameters and actions of candidate plans. The framework is combined with a
heuristic module to estimate the best plan candidate to transit initial state
to the goal based on relaxation. We repeatedly update numeric parameters and
compute candidate plan until it converges to a valid plan to the planning
problem. In the empirical study, we exhibit that our algorithm framework is
both effective and efficient, especially when solving non-convex planning
problems.

    

### [[2110.10024] Risks of AI Foundation Models in Education](http://arxiv.org/abs/2110.10024)


  If the authors of a recent Stanford report (Bommasani et al., 2021) on the
opportunities and risks of "foundation models" are to be believed, these models
represent a paradigm shift for AI and for the domains in which they will
supposedly be used, including education. Although the name is new (and
contested (Field, 2021)), the term describes existing types of algorithmic
models that are "trained on broad data at scale" and "fine-tuned" (i.e.,
adapted) for particular downstream tasks, and is intended to encompass large
language models such as BERT or GPT-3 and computer vision models such as CLIP.
Such technologies have the potential for harm broadly speaking (e.g., Bender et
al., 2021), but their use in the educational domain is particularly fraught,
despite the potential benefits for learners claimed by the authors. In section
3.3 of the Stanford report, Malik et al. argue that achieving the goal of
providing education for all learners requires more efficient computational
approaches that can rapidly scale across educational domains and across
educational contexts, for which they argue foundation models are uniquely
well-suited. However, evidence suggests that not only are foundation models not
likely to achieve the stated benefits for learners, but their use may also
introduce new risks for harm.

    

### [[2110.10144] FaxPlainAC: A Fact-Checking Tool Based on EXPLAINable Models with HumAn Correction in the Loop](http://arxiv.org/abs/2110.10144)


  Fact-checking on the Web has become the main mechanism through which we
detect the credibility of the news or information. Existing fact-checkers
verify the authenticity of the information (support or refute the claim) based
on secondary sources of information. However, existing approaches do not
consider the problem of model updates due to constantly increasing training
data due to user feedback. It is therefore important to conduct user studies to
correct models' inference biases and improve the model in a life-long learning
manner in the future according to the user feedback. In this paper, we present
FaxPlainAC, a tool that gathers user feedback on the output of explainable
fact-checking models. FaxPlainAC outputs both the model decision, i.e., whether
the input fact is true or not, along with the supporting/refuting evidence
considered by the model. Additionally, FaxPlainAC allows for accepting user
feedback both on the prediction and explanation. Developed in Python,
FaxPlainAC is designed as a modular and easily deployable tool. It can be
integrated with other downstream tasks and allowing for fact-checking human
annotation gathering and life-long learning.

    

### [[1801.04819] Robots as Powerful Allies for the Study of Embodied Cognition from the Bottom Up](http://arxiv.org/abs/1801.04819)


  A large body of compelling evidence has been accumulated demonstrating that
embodiment - the agent's physical setup, including its shape, materials,
sensors and actuators - is constitutive for any form of cognition and as a
consequence, models of cognition need to be embodied. In contrast to methods
from empirical sciences to study cognition, robots can be freely manipulated
and virtually all key variables of their embodiment and control programs can be
systematically varied. As such, they provide an extremely powerful tool of
investigation. We present a robotic bottom-up or developmental approach,
focusing on three stages: (a) low-level behaviors like walking and reflexes,
(b) learning regularities in sensorimotor spaces, and (c) human-like cognition.
We also show that robotic based research is not only a productive path to
deepening our understanding of cognition, but that robots can strongly benefit
from human-like cognition in order to become more autonomous, robust,
resilient, and safe.

    

### [[2008.02742] Compositional Networks Enable Systematic Generalization for Grounded Language Understanding](http://arxiv.org/abs/2008.02742)


  Humans are remarkably flexible when understanding new sentences that include
combinations of concepts they have never encountered before. Recent work has
shown that while deep networks can mimic some human language abilities when
presented with novel sentences, systematic variation uncovers the limitations
in the language-understanding abilities of networks. We demonstrate that these
limitations can be overcome by addressing the generalization challenges in the
gSCAN dataset, which explicitly measures how well an agent is able to interpret
novel linguistic commands grounded in vision, e.g., novel pairings of
adjectives and nouns. The key principle we employ is compositionality: that the
compositional structure of networks should reflect the compositional structure
of the problem domain they address, while allowing other parameters to be
learned end-to-end. We build a general-purpose mechanism that enables agents to
generalize their language understanding to compositional domains. Crucially,
our network has the same state-of-the-art performance as prior work while
generalizing its knowledge when prior work does not. Our network also provides
a level of interpretability that enables users to inspect what each part of
networks learns. Robust grounded language understanding without dramatic
failures and without corner cases is critical to building safe and fair robots;
we demonstrate the significant role that compositionality can play in achieving
that goal.

    

### [[2008.12146] Avoiding Negative Side Effects due to Incomplete Knowledge of AI Systems](http://arxiv.org/abs/2008.12146)


  Autonomous agents acting in the real-world often operate based on models that
ignore certain aspects of the environment. The incompleteness of any given
model -- handcrafted or machine acquired -- is inevitable due to practical
limitations of any modeling technique for complex real-world settings. Due to
the limited fidelity of its model, an agent's actions may have unexpected,
undesirable consequences during execution. Learning to recognize and avoid such
negative side effects of an agent's actions is critical to improve the safety
and reliability of autonomous systems. Mitigating negative side effects is an
emerging research topic that is attracting increased attention due to the rapid
growth in the deployment of AI systems and their broad societal impacts. This
article provides a comprehensive overview of different forms of negative side
effects and the recent research efforts to address them. We identify key
characteristics of negative side effects, highlight the challenges in avoiding
negative side effects, and discuss recently developed approaches, contrasting
their benefits and limitations. The article concludes with a discussion of open
questions and suggestions for future research directions.

    

### [[2110.09197] On the Completeness and Complexity of the Lifted Dynamic Junction Tree Algorithm](http://arxiv.org/abs/2110.09197)


  Lifted inference allows to perform inference in polynomial time w.r.t. domain
sizes. For a lifted algorithm, completeness investigates model classes for
which the algorithm is guaranteed to compute a lifted solution. We contribute,
to the best of our knowledge, the first completeness and complexity analysis
for a temporal lifted algorithm, the so-called lifted dynamic junction tree
algorithm (LDJT). To treat time as a first class citizen, LDJT introduces some
constraints. Given these constraints, we analyse the classes of liftable
models. Further, we show that LDJT has many advantages from a complexity point
of view compared to a propositional temporal inference algorithm w.r.t. domain
sizes. Therefore, LDJT advances the number of models for which inference tasks
can be solved in reasonable time not only from a practically point of view, but
also from a theoretical point of view.

    

### [[2110.09526] Infinite Servers Queue Systems Busy Period Time Length Distribution and Parameters Study through Computational Simulation](http://arxiv.org/abs/2110.09526)


  A FORTRAN program to simulate the operation of infinite servers queues is
presented in this work. Poisson arrivals processes are considered but not only.
For many parameters of interest in queuing systems study or application, either
there are not theoretical results or, existing, they are mathematically
intractable what makes their utility doubtful. In this case a possible issue is
to use simulation methods in order to get more useful results. Indeed, using
simulation, some experiences may be performed and the respective results used
to conjecture about certain queue systems interesting quantities. In this paper
this procedure is followed to learn something more about quantities of interest
for those infinite servers queue systems, in particular about busy period
parameters and probability distributions.

    

### [[2105.06024] Type-Based Termination for Futures](http://arxiv.org/abs/2105.06024)


  In sequential functional languages, sized types enable termination checking
of programs with complex patterns of recursion in the presence of mixed
inductive-coinductive types. In this paper, we adapt sized types and their
metatheory to the concurrent setting. We extend the semi-axiomatic sequent
calculus, a subsuming paradigm for futures-based functional concurrency, and
its underlying operational semantics with recursion and arithmetic refinements.
The latter enables a new and highly general sized type scheme we call sized
type refinements. As a widely applicable technical device, we type recursive
programs with infinitely deep typing derivations that unfold all recursive
calls. Then, we observe that certain such derivations can be made infinitely
wide but finitely deep. The resulting trees serve as the induction target of
our strong normalization result, which we develop via a novel logical relations
argument.

    

### [<title>XDBoost in openembedded? - XGBoost</title>](https://discuss.xgboost.ai/t/xdboost-in-openembedded/2506/1)

### [<title>How to debug slow training - RFC - XGBoost</title>](https://discuss.xgboost.ai/t/how-to-debug-slow-training/2504/1)