
## 2021-11-11

### [<title>XGBoost4j - sparse vector prediction - XGBoost</title>](https://discuss.xgboost.ai/t/xgboost4j-sparse-vector-prediction/2534/1)

### [[2111.05459] Intrinsic PUF Instance on Non-Volatile NAND Flash Memory](http://arxiv.org/abs/2111.05459)


  Embedded systems or micro controller based modules have become increasingly
prevalent in our daily lives. However, the security of embedded devices as well
as the authenticity of hardware has become an increasing concern within the
growing Internet of Things (IoT) space. In this paper we setup an experiment
environment where SLC flash program disturbance is observed. We discovered that
intra-page disturbance is easier to be produced than inter-page disturbance. We
also observed that adjacent pages are paired in (2n, 2n+1) manner, and
disturbance only occurs within a pair. Lastly, we found that as page number
increases from 0 to 63, it becomes more difficult to observe the first bit flip
within a page, and thus more difficult to achieve the disturbance stable state.

    

### [[2111.05460] Cross-Layered Distributed Data-driven Framework For Enhanced Smart Grid Cyber-Physical Security](http://arxiv.org/abs/2111.05460)


  Smart Grid (SG) research and development has drawn much attention from
academia, industry and government due to the great impact it will have on
society, economics and the environment. Securing the SG is a considerably
significant challenge due the increased dependency on communication networks to
assist in physical process control, exposing them to various cyber-threats. In
addition to attacks that change measurement values using False Data Injection
(FDI) techniques, attacks on the communication network may disrupt the power
system's real-time operation by intercepting messages, or by flooding the
communication channels with unnecessary data. Addressing these attacks requires
a cross-layer approach. In this paper a cross-layered strategy is presented,
called Cross-Layer Ensemble CorrDet with Adaptive Statistics(CECD-AS), which
integrates the detection of faulty SG measurement data as well as inconsistent
network inter-arrival times and transmission delays for more reliable and
accurate anomaly detection and attack interpretation. Numerical results show
that CECD-AS can detect multiple False Data Injections, Denial of Service (DoS)
and Man In The Middle (MITM) attacks with a high F1-score compared to current
approaches that only use SG measurement data for detection such as the
traditional physics-based State Estimation, Ensemble CorrDet with Adaptive
Statistics strategy and other machine learning classification-based detection
schemes.

    

### [[2111.05475] OPlaceRAN -- a Placement Orchestrator for Virtualized Next-Generation of Radio Access Network](http://arxiv.org/abs/2111.05475)


  The fifth-generation mobile evolution enables transformations on
Next-Generation Radio Access Networks (NG-RAN). The RAN protocol stack is split
into eight disaggregated options combined in three network units, i.e.,
Central, Distributed, and Radio. Besides that, further advances allow the RAN
functions to be virtualized on top of general-purpose hardware, using the
concept of virtualized RAN (vRAN). The Combination of NG-RAN and vRAN results
in vNG-RAN, which enables the management of the disaggregated units and
protocols as a set of radio functions. However, the orchestration-based
placement of these radio functions is a challenging issue since the best
decision can be determined by multiple constraints involving RAN
disaggregation, crosshaul networks requirements, availability of computational
resources, etc. This article proposes OPlaceRAN, a vNG-RAN deployment
orchestrator framed within the NFV reference architecture and aligned with the
Open RAN initiative. OPlaceRAN supports the dynamic placement of radio
functions focusing on vNG-RAN planning and is designed to be agnostic to the
placement optimization model. To validate OPlaceRAN, we developed a prototype
based on up-to-date cloud-native tools to deploy RAN using containerized
virtualization using the OpenAirInterface emulator and considering two distinct
functional splits (options 2 and 6). The evaluation is tested as
proofs-of-concept in a real computing infrastructure using two different
placement solutions. Our results reveal that OPlaceRAN is an effective
cloud-native solution for containerized network functions placement and
agnostic to the optimization model. Additionally, OPlaceRAN is up-to-date with
the most advanced vNG-RAN design and development approaches, contributing to
the evolution of the fifth-generation of mobile networks.

    

### [[2111.05567] VeSoNet: Traffic-Aware Content Caching for Vehicular Social Networks based on Path Planning and Deep Reinforcement Learning](http://arxiv.org/abs/2111.05567)


  Vehicular social networking is an emerging application of the promising
Internet of Vehicles (IoV) which aims to achieve the seamless integration of
vehicular networks and social networks. However, the unique characteristics of
vehicular networks such as high mobility and frequent communication
interruptions make content delivery to end-users under strict delay constrains
an extremely challenging task. In this paper, we propose a social-aware
vehicular edge computing architecture that solves the content delivery problem
by using some of the vehicles in the network as edge servers that can store and
stream popular content to close-by end-users. The proposed architecture
includes three components. First, we propose a social-aware graph pruning
search algorithm that computes and assigns the vehicles to the shortest path
with the most relevant vehicular content providers. Secondly, we use a
traffic-aware content recommendation scheme to recommend relevant content
according to their social context. This scheme uses graph embeddings in which
the vehicles are represented by a set of low-dimension vectors (vehicle2vec) to
store information about previously consumed content. Finally, we propose a Deep
Reinforcement Learning (DRL) method to optimize the content provider vehicles
distribution across the network. The results obtained from a realistic traffic
simulation show the effectiveness and robustness of the proposed system when
compared to the state-of-the-art baselines.

    

### [[2111.05804] Secure and Reliable Transfer Learning Framework for 6G-enabled Internet of Vehicles](http://arxiv.org/abs/2111.05804)


  In the coming 6G era, Internet of Vehicles (IoV) has been evolving towards
6G-enabled IoV with super-high data rate, seamless networking coverage, and
ubiquitous intelligence by Artificial Intelligence (AI). Transfer Learning (TL)
has great potential to empower promising 6G-enabled IoV, such as smart driving
assistance, with its outstanding features including enhancing quality and
quantity of training data, speeding up learning processes and reducing
computing demands. Although TL had been widely adopted in wireless applications
(e.g., spectrum management and caching), its reliability and security in
6G-enabled IoV were still not well investigated. For instance, malicious
vehicles in source domains may transfer and share untrustworthy models (i.e.,
knowledge) about connection availability to target domains, thus adversely
affecting the performance of learning processes. Therefore, it is important to
select and also incentivize trustworthy vehicles to participate in TL. In this
article, we first introduce the integration of TL and 6G-enbaled IoV and
provide TL applications for 6G-enabled IoV. We then design a secure and
reliable transfer learning framework by using reputation to evaluate the
reliability of pre-trained models and utilizing the consortium blockchain to
achieve secure and efficient decentralized reputation management. Moreover, a
deep learning-based auction scheme for the TL model market is designed to
motivate high-reputation vehicles to participate in model sharing. Finally, the
simulation results demonstrate that the proposed framework is secure and
reliable with well-design incentives for TL in 6G-enabled IoV.

    

### [[2111.05811] Internet of Things (IoT) Connectivity in 6G: An Interplay of Time, Space, Intelligence, and Value](http://arxiv.org/abs/2111.05811)


  Internet of Things (IoT) connectivity has a prominent presence in the 5G
wireless communication systems. As these systems are being deployed, there is a
surge of research efforts and visions towards 6G wireless systems. In order to
position the evolution of IoT within the 6G systems, this paper first takes a
critical view on the way IoT connectivity is supported within 5G. Following
that, the wireless IoT evolution is discussed through multiple dimensions:
time, space, intelligence, and value. We also conjecture that the focus will
broaden from IoT devices and their connections towards the emergence of complex
IoT environments, seen as building blocks of the overall IoT ecosystem.

    

### [[2102.12358] Measuring HTTP/3: Adoption and Performance](http://arxiv.org/abs/2102.12358)


  The third version of the Hypertext Transfer Protocol (HTTP) is currently in
its final standardization phase by the IETF. Besides better security and
increased flexibility, it promises benefits in terms of performance. HTTP/3
adopts a more efficient header compression schema and replaces TCP with QUIC, a
transport protocol carried over UDP, originally proposed by Google and
currently under standardization too. Although HTTP/3 early implementations
already exist and some websites announce its support, it has been subject to
few studies. In this work, we provide a first measurement study on HTTP/3. We
testify how, during 2020, it has been adopted by some of the leading Internet
companies such as Google, Facebook and Cloudflare. We run a large-scale
measurement campaign toward thousands of websites adopting HTTP/3, aiming at
understanding to what extent it achieves better performance than HTTP/2. We
find that adopting websites often host most web page objects on third-party
servers, which support only HTTP/2 or even HTTP/1.1. Our experiments show that
HTTP/3 provides sizable benefits only in scenarios with high latency or very
poor bandwidth. Despite the adoption of QUIC, we do not find benefits in case
of high packet loss, but we observe large diversity across website providers'
infrastructures.

    

### [[2111.05329] Self-Supervised Audio-Visual Representation Learning with Relaxed Cross-Modal Temporal Synchronicity](http://arxiv.org/abs/2111.05329)


  We present CrissCross, a self-supervised framework for learning audio-visual
representations. A novel notion is introduced in our framework whereby in
addition to learning the intra-modal and standard 'synchronous' cross-modal
relations, CrissCross also learns 'asynchronous' cross-modal relationships. We
show that by relaxing the temporal synchronicity between the audio and visual
modalities, the network learns strong time-invariant representations. Our
experiments show that strong augmentations for both audio and visual modalities
with relaxation of cross-modal temporal synchronicity optimize performance. To
pretrain our proposed framework, we use 3 different datasets with varying
sizes, Kinetics-Sound, Kinetics-400, and AudioSet. The learned representations
are evaluated on a number of downstream tasks namely action recognition, sound
classification, and retrieval. CrissCross shows state-of-the-art performances
on action recognition (UCF101 and HMDB51) and sound classification (ESC50). The
codes and pretrained models will be made publicly available.

    

### [[2111.05333] Classifying Human Activities with Inertial Sensors: A Machine Learning Approach](http://arxiv.org/abs/2111.05333)


  Human Activity Recognition (HAR) is an ongoing research topic. It has
applications in medical support, sports, fitness, social networking,
human-computer interfaces, senior care, entertainment, surveillance, and the
list goes on. Traditionally, computer vision methods were employed for HAR,
which has numerous problems such as secrecy or privacy, the influence of
environmental factors, less mobility, higher running costs, occlusion, and so
on. A new trend in the use of sensors, especially inertial sensors, has lately
emerged. There are several advantages of employing sensor data as an
alternative to traditional computer vision algorithms. Many of the limitations
of computer vision algorithms have been documented in the literature, including
research on Deep Neural Network (DNN) and Machine Learning (ML) approaches for
activity categorization utilizing sensor data. We examined and analyzed
different Machine Learning and Deep Learning approaches for Human Activity
Recognition using inertial sensor data of smartphones. In order to identify
which approach is best suited for this application.

    

### [[2111.05366] Graph Matching via Optimal Transport](http://arxiv.org/abs/2111.05366)


  The graph matching problem seeks to find an alignment between the nodes of
two graphs that minimizes the number of adjacency disagreements. Solving the
graph matching is increasingly important due to it's applications in operations
research, computer vision, neuroscience, and more. However, current
state-of-the-art algorithms are inefficient in matching very large graphs,
though they produce good accuracy. The main computational bottleneck of these
algorithms is the linear assignment problem, which must be solved at each
iteration. In this paper, we leverage the recent advances in the field of
optimal transport to replace the accepted use of linear assignment algorithms.
We present GOAT, a modification to the state-of-the-art graph matching
approximation algorithm "FAQ" (Vogelstein, 2015), replacing its linear sum
assignment step with the "Lightspeed Optimal Transport" method of Cuturi
(2013). The modification provides improvements to both speed and empirical
matching accuracy. The effectiveness of the approach is demonstrated in
matching graphs in simulated and real data examples.

    

### [[2111.05384] DataWords: Getting Contrarian with Text, Structured Data and Explanations](http://arxiv.org/abs/2111.05384)


  Our goal is to build classification models using a combination of free-text
and structured data. To do this, we represent structured data by text
sentences, DataWords, so that similar data items are mapped into the same
sentence. This permits modeling a mixture of text and structured data by using
only text-modeling algorithms. Several examples illustrate that it is possible
to improve text classification performance by first running extraction tools
(named entity recognition), then converting the output to DataWords, and adding
the DataWords to the original text -- before model building and classification.
This approach also allows us to produce explanations for inferences in terms of
both free text and structured data.

    

### [[2111.05385] Identifying the Risks of Chronic Diseases Using BMI Trajectories](http://arxiv.org/abs/2111.05385)


  Obesity is a major health problem, increasing the risk of various major
chronic diseases, such as diabetes, cancer, and stroke. While the role of
obesity identified by cross-sectional BMI recordings has been heavily studied,
the role of BMI trajectories is much less explored. In this study, we use a
machine learning approach to subtype individuals' risk of developing 18 major
chronic diseases by using their BMI trajectories extracted from a large and
geographically diverse EHR dataset capturing the health status of around two
million individuals for a period of six years. We define nine new interpretable
and evidence-based variables based on the BMI trajectories to cluster the
patients into subgroups using the k-means clustering method. We thoroughly
review each clusters' characteristics in terms of demographic, socioeconomic,
and physiological measurement variables to specify the distinct properties of
the patients in the clusters. In our experiments, direct relationship of
obesity with diabetes, hypertension, Alzheimer's, and dementia have been
re-established and distinct clusters with specific characteristics for several
of the chronic diseases have been found to be conforming or complementary to
the existing body of knowledge.

    

### [[2111.05392] Gaussian Process Meta Few-shot Classifier Learning via Linear Discriminant Laplace Approximation](http://arxiv.org/abs/2111.05392)


  The meta learning few-shot classification is an emerging problem in machine
learning that received enormous attention recently, where the goal is to learn
a model that can quickly adapt to a new task with only a few labeled data. We
consider the Bayesian Gaussian process (GP) approach, in which we meta-learn
the GP prior, and the adaptation to a new task is carried out by the GP
predictive model from the posterior inference. We adopt the Laplace posterior
approximation, but to circumvent the iterative gradient steps for finding the
MAP solution, we introduce a novel linear discriminant analysis (LDA) plugin as
a surrogate for the MAP solution. In essence, the MAP solution is approximated
by the LDA estimate, but to take the GP prior into account, we adopt the
prior-norm adjustment to estimate LDA's shared variance parameters, which
ensures that the adjusted estimate is consistent with the GP prior. This
enables closed-form differentiable GP posteriors and predictive distributions,
thus allowing fast meta training. We demonstrate considerable improvement over
the previous approaches.

    

### [[2111.05393] Object-Centric Representation Learning with Generative Spatial-Temporal Factorization](http://arxiv.org/abs/2111.05393)


  Learning object-centric scene representations is essential for attaining
structural understanding and abstraction of complex scenes. Yet, as current
approaches for unsupervised object-centric representation learning are built
upon either a stationary observer assumption or a static scene assumption, they
often: i) suffer single-view spatial ambiguities, or ii) infer incorrectly or
inaccurately object representations from dynamic scenes. To address this, we
propose Dynamics-aware Multi-Object Network (DyMON), a method that broadens the
scope of multi-view object-centric representation learning to dynamic scenes.
We train DyMON on multi-view-dynamic-scene data and show that DyMON learns --
without supervision -- to factorize the entangled effects of observer motions
and scene object dynamics from a sequence of observations, and constructs scene
object spatial representations suitable for rendering at arbitrary times
(querying across time) and from arbitrary viewpoints (querying across space).
We also show that the factorized scene representations (w.r.t. objects) support
querying about a single object by space and time independently.

    

### [[2111.05408] Robust deep learning-based semantic organ segmentation in hyperspectral images](http://arxiv.org/abs/2111.05408)


  Semantic image segmentation is an important prerequisite for
context-awareness and autonomous robotics in surgery. The state of the art has
focused on conventional RGB video data acquired during minimally invasive
surgery, but full-scene semantic segmentation based on spectral imaging data
and obtained during open surgery has received almost no attention to date. To
address this gap in the literature, we are investigating the following research
questions based on hyperspectral imaging (HSI) data of pigs acquired in an open
surgery setting: (1) What is an adequate representation of HSI data for neural
network-based fully automated organ segmentation, especially with respect to
the spatial granularity of the data (pixels vs. superpixels vs. patches vs.
full images)? (2) Is there a benefit of using HSI data compared to other
modalities, namely RGB data and processed HSI data (e.g. tissue parameters like
oxygenation), when performing semantic organ segmentation? According to a
comprehensive validation study based on 506 HSI images from 20 pigs, annotated
with a total of 19 classes, deep learning-based segmentation performance
increases - consistently across modalities - with the spatial context of the
input data. Unprocessed HSI data offers an advantage over RGB data or processed
data from the camera provider, with the advantage increasing with decreasing
size of the input to the neural network. Maximum performance (HSI applied to
whole images) yielded a mean dice similarity coefficient (DSC) of 0.89
(standard deviation (SD) 0.04), which is in the range of the inter-rater
variability (DSC of 0.89 (SD 0.07)). We conclude that HSI could become a
powerful image modality for fully-automatic surgical scene understanding with
many advantages over traditional imaging, including the ability to recover
additional functional tissue information.

    

### [[2111.05410] Convolutional Neural Network Dynamics: A Graph Perspective](http://arxiv.org/abs/2111.05410)


  The success of neural networks (NNs) in a wide range of applications has led
to increased interest in understanding the underlying learning dynamics of
these models. In this paper, we go beyond mere descriptions of the learning
dynamics by taking a graph perspective and investigating the relationship
between the graph structure of NNs and their performance. Specifically, we
propose (1) representing the neural network learning process as a time-evolving
graph (i.e., a series of static graph snapshots over epochs), (2) capturing the
structural changes of the NN during the training phase in a simple temporal
summary, and (3) leveraging the structural summary to predict the accuracy of
the underlying NN in a classification or regression task. For the dynamic graph
representation of NNs, we explore structural representations for
fully-connected and convolutional layers, which are key components of powerful
NN models. Our analysis shows that a simple summary of graph statistics, such
as weighted degree and eigenvector centrality, over just a few epochs can be
used to accurately predict the performance of NNs. For example, a weighted
degree-based summary of the time-evolving graph that is constructed based on 5
training epochs of the LeNet architecture achieves classification accuracy of
over 93%. Our findings are consistent for different NN architectures, including
LeNet, VGG, AlexNet and ResNet.

    

### [[2111.05412] MNet-Sim: A Multi-layered Semantic Similarity Network to Evaluate Sentence Similarity](http://arxiv.org/abs/2111.05412)


  Similarity is a comparative-subjective measure that varies with the domain
within which it is considered. In several NLP applications such as document
classification, pattern recognition, chatbot question-answering, sentiment
analysis, etc., identifying an accurate similarity score for sentence pairs has
become a crucial area of research. In the existing models that assess
similarity, the limitation of effectively computing this similarity based on
contextual comparisons, the localization due to the centering theory, and the
lack of non-semantic textual comparisons have proven to be drawbacks. Hence,
this paper presents a multi-layered semantic similarity network model built
upon multiple similarity measures that render an overall sentence similarity
score based on the principles of Network Science, neighboring weighted
relational edges, and a proposed extended node similarity computation formula.
The proposed multi-layered network model was evaluated and tested against
established state-of-the-art models and is shown to have demonstrated better
performance scores in assessing sentence similarity.

    

### [[2111.05423] Efficient Data Compression for 3D Sparse TPC via Bicephalous Convolutional Autoencoder](http://arxiv.org/abs/2111.05423)


  Real-time data collection and analysis in large experimental facilities
present a great challenge across multiple domains, including high energy
physics, nuclear physics, and cosmology. To address this, machine learning
(ML)-based methods for real-time data compression have drawn significant
attention. However, unlike natural image data, such as CIFAR and ImageNet that
are relatively small-sized and continuous, scientific data often come in as
three-dimensional data volumes at high rates with high sparsity (many zeros)
and non-Gaussian value distribution. This makes direct application of popular
ML compression methods, as well as conventional data compression methods,
suboptimal. To address these obstacles, this work introduces a dual-head
autoencoder to resolve sparsity and regression simultaneously, called
\textit{Bicephalous Convolutional AutoEncoder} (BCAE). This method shows
advantages both in compression fidelity and ratio compared to traditional data
compression methods, such as MGARD, SZ, and ZFP. To achieve similar fidelity,
the best performer among the traditional methods can reach only half the
compression ratio of BCAE. Moreover, a thorough ablation study of the BCAE
method shows that a dedicated segmentation decoder improves the reconstruction.

    

### [[2111.05426] DistIR: An Intermediate Representation and Simulator for Efficient Neural Network Distribution](http://arxiv.org/abs/2111.05426)


  The rapidly growing size of deep neural network (DNN) models and datasets has
given rise to a variety of distribution strategies such as data, tensor-model,
pipeline parallelism, and hybrid combinations thereof. Each of these strategies
offers its own trade-offs and exhibits optimal performance across different
models and hardware topologies. Selecting the best set of strategies for a
given setup is challenging because the search space grows combinatorially, and
debugging and testing on clusters is expensive. In this work we propose DistIR,
an expressive intermediate representation for distributed DNN computation that
is tailored for efficient analyses, such as simulation. This enables
automatically identifying the top-performing strategies without having to
execute on physical hardware. Unlike prior work, DistIR can naturally express
many distribution strategies including pipeline parallelism with arbitrary
schedules. Our evaluation on MLP training and GPT-2 inference models
demonstrates how DistIR and its simulator enable fast grid searches over
complex distribution spaces spanning up to 1000+ configurations, reducing
optimization time by an order of magnitude for certain regimes.

    

### [[2111.05428] Constrained Instance and Class Reweighting for Robust Learning under Label Noise](http://arxiv.org/abs/2111.05428)


  Deep neural networks have shown impressive performance in supervised
learning, enabled by their ability to fit well to the provided training data.
However, their performance is largely dependent on the quality of the training
data and often degrades in the presence of noise. We propose a principled
approach for tackling label noise with the aim of assigning importance weights
to individual instances and class labels. Our method works by formulating a
class of constrained optimization problems that yield simple closed form
updates for these importance weights. The proposed optimization problems are
solved per mini-batch which obviates the need of storing and updating the
weights over the full dataset. Our optimization framework also provides a
theoretical perspective on existing label smoothing heuristics for addressing
label noise (such as label bootstrapping). We evaluate our method on several
benchmark datasets and observe considerable performance gains in the presence
of label noise.

    

### [[2111.05431] Multi-Task Prediction of Clinical Outcomes in the Intensive Care Unit using Flexible Multimodal Transformers](http://arxiv.org/abs/2111.05431)


  Recent deep learning research based on Transformer model architectures has
demonstrated state-of-the-art performance across a variety of domains and
tasks, mostly within the computer vision and natural language processing
domains. While some recent studies have implemented Transformers for clinical
tasks using electronic health records data, they are limited in scope,
flexibility, and comprehensiveness. In this study, we propose a flexible
Transformer-based EHR embedding pipeline and predictive model framework that
introduces several novel modifications of existing workflows that capitalize on
data attributes unique to the healthcare domain. We showcase the feasibility of
our flexible design in a case study in the intensive care unit, where our
models accurately predict seven clinical outcomes pertaining to readmission and
patient mortality over multiple future time horizons.

    

### [[2111.05440] Dealing with the Unknown: Pessimistic Offline Reinforcement Learning](http://arxiv.org/abs/2111.05440)


  Reinforcement Learning (RL) has been shown effective in domains where the
agent can learn policies by actively interacting with its operating
environment. However, if we change the RL scheme to offline setting where the
agent can only update its policy via static datasets, one of the major issues
in offline reinforcement learning emerges, i.e. distributional shift. We
propose a Pessimistic Offline Reinforcement Learning (PessORL) algorithm to
actively lead the agent back to the area where it is familiar by manipulating
the value function. We focus on problems caused by out-of-distribution (OOD)
states, and deliberately penalize high values at states that are absent in the
training dataset, so that the learned pessimistic value function lower bounds
the true value anywhere within the state space. We evaluate the PessORL
algorithm on various benchmark tasks, where we show that our method gains
better performance by explicitly handling OOD states, when compared to those
methods merely considering OOD actions.

    

### [[2111.05451] Importance of Kernel Bandwidth in Quantum Machine Learning](http://arxiv.org/abs/2111.05451)


  Quantum kernel methods are considered a promising avenue for applying quantum
computers to machine learning problems. However, recent results overlook the
central role hyperparameters play in determining the performance of machine
learning methods. In this work we show how optimizing the bandwidth of a
quantum kernel can improve the performance of the kernel method from a random
guess to being competitive with the best classical methods. Without
hyperparameter optimization, kernel values decrease exponentially with qubit
count, which is the cause behind recent observations that the performance of
quantum kernel methods decreases with qubit count. We reproduce these negative
results and show, through extensive numerical experiments using multiple
quantum kernels and classical datasets, that if the kernel bandwidth is
optimized, the performance instead improves with growing qubit count. We draw a
connection between the bandwidth of classical and quantum kernels and show
analogous behavior in both cases.

    

### [[2111.05454] DP-REC: Private & Communication-Efficient Federated Learning](http://arxiv.org/abs/2111.05454)


  Privacy and communication efficiency are important challenges in federated
training of neural networks, and combining them is still an open problem. In
this work, we develop a method that unifies highly compressed communication and
differential privacy (DP). We introduce a compression technique based on
Relative Entropy Coding (REC) to the federated setting. With a minor
modification to REC, we obtain a provably differentially private learning
algorithm, DP-REC, and show how to compute its privacy guarantees. Our
experiments demonstrate that DP-REC drastically reduces communication costs
while providing privacy guarantees comparable to the state-of-the-art.

    

### [[2111.05458] Which priors matter? Benchmarking models for learning latent dynamics](http://arxiv.org/abs/2111.05458)


  Learning dynamics is at the heart of many important applications of machine
learning (ML), such as robotics and autonomous driving. In these settings, ML
algorithms typically need to reason about a physical system using high
dimensional observations, such as images, without access to the underlying
state. Recently, several methods have proposed to integrate priors from
classical mechanics into ML models to address the challenge of physical
reasoning from images. In this work, we take a sober look at the current
capabilities of these models. To this end, we introduce a suite consisting of
17 datasets with visual observations based on physical systems exhibiting a
wide range of dynamics. We conduct a thorough and detailed comparison of the
major classes of physically inspired methods alongside several strong
baselines. While models that incorporate physical priors can often learn latent
spaces with desirable properties, our results demonstrate that these methods
fail to significantly improve upon standard techniques. Nonetheless, we find
that the use of continuous and time-reversible dynamics benefits models of all
classes.

    

### [[2111.05469] Clustering of longitudinal data: A tutorial on a variety of approaches](http://arxiv.org/abs/2111.05469)


  During the past two decades, methods for identifying groups with different
trends in longitudinal data have become of increasing interest across many
areas of research. To support researchers, we summarize the guidance from the
literature regarding longitudinal clustering. Moreover, we present a selection
of methods for longitudinal clustering, including group-based trajectory
modeling (GBTM), growth mixture modeling (GMM), and longitudinal k-means (KML).
The methods are introduced at a basic level, and strengths, limitations, and
model extensions are listed. Following the recent developments in data
collection, attention is given to the applicability of these methods to
intensive longitudinal data (ILD). We demonstrate the application of the
methods on a synthetic dataset using packages available in R.

    

### [[2111.05478] SGD Through the Lens of Kolmogorov Complexity](http://arxiv.org/abs/2111.05478)


  We prove that stochastic gradient descent (SGD) finds a solution that
achieves $(1-\epsilon)$ classification accuracy on the entire dataset. We do so
under two main assumptions: (1. Local progress) There is consistent improvement
of the model accuracy over batches. (2. Models compute simple functions) The
function computed by the model is simple (has low Kolmogorov complexity).
Intuitively, the above means that \emph{local progress} of SGD implies
\emph{global progress}. Assumption 2 trivially holds for underparameterized
models, hence, our work gives the first convergence guarantee for general,
\emph{underparameterized models}. Furthermore, this is the first result which
is completely \emph{model agnostic} - we don't require the model to have any
specific architecture or activation function, it may not even be a neural
network. Our analysis makes use of the entropy compression method, which was
first introduced by Moser and Tardos in the context of the Lov√°sz local
lemma.

    

### [[2111.05479] Spatially and Seamlessly Hierarchical Reinforcement Learning for State Space and Policy space in Autonomous Driving](http://arxiv.org/abs/2111.05479)


  Despite advances in hierarchical reinforcement learning, its applications to
path planning in autonomous driving on highways are challenging. One reason is
that conventional hierarchical reinforcement learning approaches are not
amenable to autonomous driving due to its riskiness: the agent must move
avoiding multiple obstacles such as other agents that are highly unpredictable,
thus safe regions are small, scattered, and changeable over time. To overcome
this challenge, we propose a spatially hierarchical reinforcement learning
method for state space and policy space. The high-level policy selects not only
behavioral sub-policy but also regions to pay mind to in state space and for
outline in policy space. Subsequently, the low-level policy elaborates the
short-term goal position of the agent within the outline of the region selected
by the high-level command. The network structure and optimization suggested in
our method are as concise as those of single-level methods. Experiments on the
environment with various shapes of roads showed that our method finds the
nearly optimal policies from early episodes, outperforming a baseline
hierarchical reinforcement learning method, especially in narrow and complex
roads. The resulting trajectories on the roads were similar to those of human
strategies on the behavioral planning level.

    

### [[2111.05486] Multi-Agent Learning for Iterative Dominance Elimination: Formal Barriers and New Algorithms](http://arxiv.org/abs/2111.05486)


  Dominated actions are natural (and perhaps the simplest possible) multi-agent
generalizations of sub-optimal actions as in standard single-agent decision
making. Thus similar to standard bandit learning, a basic learning question in
multi-agent systems is whether agents can learn to efficiently eliminate all
dominated actions in an unknown game if they can only observe noisy bandit
feedback about the payoff of their played actions. Surprisingly, despite a
seemingly simple task, we show a quite negative result; that is, standard no
regret algorithms -- including the entire family of Dual Averaging algorithms
-- provably take exponentially many rounds to eliminate all dominated actions.
Moreover, algorithms with the stronger no swap regret also suffer similar
exponential inefficiency. To overcome these barriers, we develop a new
algorithm that adjusts Exp3 with Diminishing Historical rewards (termed
Exp3-DH); Exp3-DH gradually forgets history at carefully tailored rates. We
prove that when all agents run Exp3-DH (a.k.a., self-play in multi-agent
learning), all dominated actions can be iteratively eliminated within
polynomially many rounds. Our experimental results further demonstrate the
efficiency of Exp3-DH, and that state-of-the-art bandit algorithms, even those
developed specifically for learning in games, fail to eliminate all dominated
actions efficiently.

    

### [[2111.05496] ResNEsts and DenseNEsts: Block-based DNN Models with Improved Representation Guarantees](http://arxiv.org/abs/2111.05496)


  Models recently used in the literature proving residual networks (ResNets)
are better than linear predictors are actually different from standard ResNets
that have been widely used in computer vision. In addition to the assumptions
such as scalar-valued output or single residual block, these models have no
nonlinearities at the final residual representation that feeds into the final
affine layer. To codify such a difference in nonlinearities and reveal a linear
estimation property, we define ResNEsts, i.e., Residual Nonlinear Estimators,
by simply dropping nonlinearities at the last residual representation from
standard ResNets. We show that wide ResNEsts with bottleneck blocks can always
guarantee a very desirable training property that standard ResNets aim to
achieve, i.e., adding more blocks does not decrease performance given the same
set of basis elements. To prove that, we first recognize ResNEsts are basis
function models that are limited by a coupling problem in basis learning and
linear prediction. Then, to decouple prediction weights from basis learning, we
construct a special architecture termed augmented ResNEst (A-ResNEst) that
always guarantees no worse performance with the addition of a block. As a
result, such an A-ResNEst establishes empirical risk lower bounds for a ResNEst
using corresponding bases. Our results demonstrate ResNEsts indeed have a
problem of diminishing feature reuse; however, it can be avoided by
sufficiently expanding or widening the input space, leading to the
above-mentioned desirable property. Inspired by the DenseNets that have been
shown to outperform ResNets, we also propose a corresponding new model called
Densely connected Nonlinear Estimator (DenseNEst). We show that any DenseNEst
can be represented as a wide ResNEst with bottleneck blocks. Unlike ResNEsts,
DenseNEsts exhibit the desirable property without any special architectural
re-design.

    

### [[2111.05498] Attention Approximates Sparse Distributed Memory](http://arxiv.org/abs/2111.05498)


  While Attention has come to be an important mechanism in deep learning, there
remains limited intuition for why it works so well. Here, we show that
Transformer Attention can be closely related under certain data conditions to
Kanerva's Sparse Distributed Memory (SDM), a biologically plausible associative
memory model. We confirm that these conditions are satisfied in pre-trained
GPT2 Transformer models. We discuss the implications of the Attention-SDM map
and provide new computational and biological interpretations of Attention.

    

### [[2111.05501] Inclusive Speaker Verification with Adaptive thresholding](http://arxiv.org/abs/2111.05501)


  While using a speaker verification (SV) based system in a commercial
application, it is important that customers have an inclusive experience
irrespective of their gender, age, or ethnicity. In this paper, we analyze the
impact of gender and age on SV and find that for a desired common False
Acceptance Rate (FAR) across different gender and age groups, the False
Rejection Rate (FRR) is different for different gender and age groups. To
optimize FRR for all users for a desired FAR, we propose a context (e.g.
gender, age) adaptive thresholding framework for SV. The context can be
available as prior information for many practical applications. We also propose
a concatenated gender/age detection model to algorithmically derive the context
in absence of such prior information. We experimentally show that our
context-adaptive thresholding method is effective in building a more efficient
inclusive SV system. Specifically, we show that we can reduce FRR for specific
gender for a desired FAR on the voxceleb1 test set by using gender-specific
thresholds. Similar analysis on OGI kids' speech corpus shows that by using an
age-specific threshold, we can significantly reduce FRR for certain age groups
for desired FAR.

    

### [[2111.05505] DACFL: Dynamic Average Consensus Based Federated Learning in Decentralized Topology](http://arxiv.org/abs/2111.05505)


  Federated learning (FL) is a burgeoning distributed machine learning
framework where a central parameter server (PS) coordinates many local users to
train a globally consistent model. Conventional federated learning inevitably
relies on a centralized topology with a PS. As a result, it will paralyze once
the PS fails. To alleviate such a single point failure, especially on the PS,
some existing work has provided decentralized FL (DFL) implementations like
CDSGD and D-PSGD to facilitate FL in a decentralized topology. However, there
are still some problems with these methods, e.g., significant divergence
between users' final models in CDSGD and a network-wide model average necessity
in D-PSGD. In order to solve these deficiency, this paper devises a new DFL
implementation coined as DACFL, where each user trains its model using its own
training data and exchanges the intermediate models with its neighbors through
a symmetric and doubly stochastic matrix. The DACFL treats the progress of each
user's local training as a discrete-time process and employs a first order
dynamic average consensus (FODAC) method to track the \textit{average model} in
the absence of the PS. In this paper, we also provide a theoretical convergence
analysis of DACFL on the premise of i.i.d data to strengthen its rationality.
The experimental results on MNIST, Fashion-MNIST and CIFAR-10 validate the
feasibility of our solution in both time-invariant and time-varying network
topologies, and declare that DACFL outperforms D-PSGD and CDSGD in most cases.

    

### [[2111.05508] Training Generative Adversarial Networks with Adaptive Composite Gradient](http://arxiv.org/abs/2111.05508)


  The wide applications of Generative adversarial networks benefit from the
successful training methods, guaranteeing that an object function converges to
the local minima. Nevertheless, designing an efficient and competitive training
method is still a challenging task due to the cyclic behaviors of some
gradient-based ways and the expensive computational cost of these methods based
on the Hessian matrix. This paper proposed the adaptive Composite Gradients
(ACG) method, linearly convergent in bilinear games under suitable settings.
Theory and toy-function experiments suggest that our approach can alleviate the
cyclic behaviors and converge faster than recently proposed algorithms.
Significantly, the ACG method is not only used to find stable fixed points in
bilinear games as well as in general games. The ACG method is a novel
semi-gradient-free algorithm since it does not need to calculate the gradient
of each step, reducing the computational cost of gradient and Hessian by
utilizing the predictive information in future iterations. We conducted two
mixture of Gaussians experiments by integrating ACG to existing algorithms with
Linear GANs. Results show ACG is competitive with the previous algorithms.
Realistic experiments on four prevalent data sets (MNIST, Fashion-MNIST,
CIFAR-10, and CelebA) with DCGANs show that our ACG method outperforms several
baselines, which illustrates the superiority and efficacy of our method.

    

### [[2111.05528] Lightweight machine unlearning in neural network](http://arxiv.org/abs/2111.05528)


  In recent years, machine learning neural network has penetrated deeply into
people's life. As the price of convenience, people's private information also
has the risk of disclosure. The "right to be forgotten" was introduced in a
timely manner, stipulating that individuals have the right to withdraw their
consent from personal information processing activities based on their consent.
To solve this problem, machine unlearning is proposed, which allows the model
to erase all memory of private information. Previous studies, including
retraining and incremental learning to update models, often take up extra
storage space or are difficult to apply to neural networks. Our method only
needs to make a small perturbation of the weight of the target model and make
it iterate in the direction of the model trained with the remaining data subset
until the contribution of the unlearning data to the model is completely
eliminated. In this paper, experiments on five datasets prove the effectiveness
of our method for machine unlearning, and our method is 15 times faster than
retraining.

    

### [[2111.05529] Understanding the Generalization Benefit of Model Invariance from a Data Perspective](http://arxiv.org/abs/2111.05529)


  Machine learning models that are developed to be invariant under certain
types of data transformations have shown improved generalization in practice.
However, a principled understanding of why invariance benefits generalization
is limited. Given a dataset, there is often no principled way to select
"suitable" data transformations under which model invariance guarantees better
generalization. This paper studies the generalization benefit of model
invariance by introducing the sample cover induced by transformations, i.e., a
representative subset of a dataset that can approximately recover the whole
dataset using transformations. For any data transformations, we provide refined
generalization bounds for invariant models based on the sample cover. We also
characterize the "suitability" of a set of data transformations by the sample
covering number induced by transformations, i.e., the smallest size of its
induced sample covers. We show that we may tighten the generalization bounds
for "suitable" transformations that have a small sample covering number. In
addition, our proposed sample covering number can be empirically evaluated and
thus provides a guide for selecting transformations to develop model invariance
for better generalization. In experiments on multiple datasets, we evaluate
sample covering numbers for some commonly used transformations and show that
the smaller sample covering number for a set of transformations (e.g., the
3D-view transformation) indicates a smaller gap between the test and training
error for invariant models, which verifies our propositions.

    

### [[2111.05530] Linear Convergence of Stochastic Primal Dual Methods for Linear Programming Using Variance Reduction and Restarts](http://arxiv.org/abs/2111.05530)


  There is a recent interest on first-order methods for linear programming
(LP). In this paper, we propose a stochastic algorithm using variance reduction
and restarts for solving sharp primal-dual problems such as LP. We show that
the proposed stochastic method exhibits a linear convergence rate for sharp
instances with a high probability, which improves the complexity of the
existing deterministic and stochastic algorithms. In addition, we propose an
efficient coordinate-based stochastic oracle for unconstrained bilinear
problems, which has $\mathcal O(1)$ per iteration cost and improves the total
flop counts to reach a certain accuracy.

    

### [[2111.05546] Biomarker Gene Identification for Breast Cancer Classification](http://arxiv.org/abs/2111.05546)


  BACKGROUND: Breast cancer has emerged as one of the most prevalent cancers
among women leading to a high mortality rate. Due to the heterogeneous nature
of breast cancer, there is a need to identify differentially expressed genes
associated with breast cancer subtypes for its timely diagnosis and treatment.
OBJECTIVE: To identify a small gene set for each of the four breast cancer
subtypes that could act as its signature, the paper proposes a novel algorithm
for gene signature identification. METHODS: The present work uses interpretable
AI methods to investigate the predictions made by the deep neural network
employed for subtype classification to identify biomarkers using the TCGA
breast cancer RNA Sequence data. RESULTS: The proposed algorithm led to the
discovery of a set of 43 differentially expressed gene signatures. We achieved
a competitive average 10-fold accuracy of 0.91, using neural network
classifier. Further, gene set analysis revealed several relevant pathways, such
as GRB7 events in ERBB2 and p53 signaling pathway. Using the Pearson
correlation matrix, we noted that the subtype-specific genes are correlated
within each subtype. CONCLUSIONS: The proposed technique enables us to find a
concise and clinically relevant gene signature set.

    

### [[2111.05547] ICDAR 2021 Competition on Document VisualQuestion Answering](http://arxiv.org/abs/2111.05547)


  In this report we present results of the ICDAR 2021 edition of the Document
Visual Question Challenges. This edition complements the previous tasks on
Single Document VQA and Document Collection VQA with a newly introduced on
Infographics VQA. Infographics VQA is based on a new dataset of more than 5,000
infographics images and 30,000 question-answer pairs. The winner methods have
scored 0.6120 ANLS in Infographics VQA task, 0.7743 ANLSL in Document
Collection VQA task and 0.8705 ANLS in Single Document VQA. We present a
summary of the datasets used for each task, description of each of the
submitted methods and the results and analysis of their performance. A summary
of the progress made on Single Document VQA since the first edition of the
DocVQA 2020 challenge is also presented.

    

### [[2111.05558] Deducing of Optimal Machine Learning Algorithms for Heterogeneity](http://arxiv.org/abs/2111.05558)


  For defining the optimal machine learning algorithm, the decision was not
easy for which we shall choose. To help future researchers, we describe in this
paper the optimal among the best of the algorithms. We built a synthetic data
set and performed the supervised machine learning runs for five different
algorithms. For heterogeneity, we identified Random Forest, among others, to be
the best algorithm.

    

### [[2111.05576] Topic-aware latent models for representation learning on networks](http://arxiv.org/abs/2111.05576)


  Network representation learning (NRL) methods have received significant
attention over the last years thanks to their success in several graph analysis
problems, including node classification, link prediction, and clustering. Such
methods aim to map each vertex of the network into a low-dimensional space in a
way that the structural information of the network is preserved. Of particular
interest are methods based on random walks; such methods transform the network
into a collection of node sequences, aiming to learn node representations by
predicting the context of each node within the sequence. In this paper, we
introduce TNE, a generic framework to enhance the embeddings of nodes acquired
by means of random walk-based approaches with topic-based information. Similar
to the concept of topical word embeddings in Natural Language Processing, the
proposed model first assigns each node to a latent community with the favor of
various statistical graph models and community detection methods and then
learns the enhanced topic-aware representations. We evaluate our methodology in
two downstream tasks: node classification and link prediction. The experimental
results demonstrate that by incorporating node and community embeddings, we are
able to outperform widely-known baseline NRL models.

    

### [[2111.05589] Safe Real-Time Optimization using Multi-Fidelity Gaussian Processes](http://arxiv.org/abs/2111.05589)


  This paper proposes a new class of real-time optimization schemes to overcome
system-model mismatch of uncertain processes. This work's novelty lies in
integrating derivative-free optimization schemes and multi-fidelity Gaussian
processes within a Bayesian optimization framework. The proposed scheme uses
two Gaussian processes for the stochastic system, one emulates the (known)
process model, and another, the true system through measurements. In this way,
low fidelity samples can be obtained via a model, while high fidelity samples
are obtained through measurements of the system. This framework captures the
system's behavior in a non-parametric fashion while driving exploration through
acquisition functions. The benefit of using a Gaussian process to represent the
system is the ability to perform uncertainty quantification in real-time and
allow for chance constraints to be satisfied with high confidence. This results
in a practical approach that is illustrated in numerical case studies,
including a semi-batch photobioreactor optimization problem.

    

### [[2111.05639] Graph Transplant: Node Saliency-Guided Graph Mixup with Local Structure Preservation](http://arxiv.org/abs/2111.05639)


  Graph-structured datasets usually have irregular graph sizes and
connectivities, rendering the use of recent data augmentation techniques, such
as Mixup, difficult. To tackle this challenge, we present the first Mixup-like
graph augmentation method at the graph-level called Graph Transplant, which
mixes irregular graphs in data space. To be well defined on various scales of
the graph, our method identifies the sub-structure as a mix unit that can
preserve the local information. Since the mixup-based methods without special
consideration of the context are prone to generate noisy samples, our method
explicitly employs the node saliency information to select meaningful subgraphs
and adaptively determine the labels. We extensively validate our method with
diverse GNN architectures on multiple graph classification benchmark datasets
from a wide range of graph domains of different sizes. Experimental results
show the consistent superiority of our method over other basic data
augmentation baselines. We also demonstrate that Graph Transplant enhances the
performance in terms of robustness and model calibration.

    

### [[2111.05641] Parallel Physics-Informed Neural Networks with Bidirectional Balance](http://arxiv.org/abs/2111.05641)


  As an emerging technology in deep learning, physics-informed neural networks
(PINNs) have been widely used to solve various partial differential equations
(PDEs) in engineering. However, PDEs based on practical considerations contain
multiple physical quantities and complex initial boundary conditions, thus
PINNs often returns incorrect results. Here we take heat transfer problem in
multilayer fabrics as a typical example. It is coupled by multiple temperature
fields with strong correlation, and the values of variables are extremely
unbalanced among different dimensions. We clarify the potential difficulties of
solving such problems by classic PINNs, and propose a parallel physics-informed
neural networks with bidirectional balance. In detail, our parallel solving
framework synchronously fits coupled equations through several multilayer
perceptions. Moreover, we design two modules to balance forward process of data
and back-propagation process of loss gradient. This bidirectional balance not
only enables the whole network to converge stably, but also helps to fully
learn various physical conditions in PDEs. We provide a series of ablation
experiments to verify the effectiveness of the proposed methods. The results
show that our approach makes the PINNs unsolvable problem solvable, and
achieves excellent solving accuracy.

    

### [[2111.05643] Conditional Alignment and Uniformity for Contrastive Learning with Continuous Proxy Labels](http://arxiv.org/abs/2111.05643)


  Contrastive Learning has shown impressive results on natural and medical
images, without requiring annotated data. However, a particularity of medical
images is the availability of meta-data (such as age or sex) that can be
exploited for learning representations. Here, we show that the recently
proposed contrastive y-Aware InfoNCE loss, that integrates multi-dimensional
meta-data, asymptotically optimizes two properties: conditional alignment and
global uniformity. Similarly to [Wang, 2020], conditional alignment means that
similar samples should have similar features, but conditionally on the
meta-data. Instead, global uniformity means that the (normalized) features
should be uniformly distributed on the unit hyper-sphere, independently of the
meta-data. Here, we propose to define conditional uniformity, relying on the
meta-data, that repel only samples with dissimilar meta-data. We show that
direct optimization of both conditional alignment and uniformity improves the
representations, in terms of linear evaluation, on both CIFAR-100 and a brain
MRI dataset.

    

### [[2111.05645] Social Fraud Detection Review: Methods, Challenges and Analysis](http://arxiv.org/abs/2111.05645)


  Social reviews have dominated the web and become a plausible source of
product information. People and businesses use such information for
decision-making. Businesses also make use of social information to spread fake
information using a single user, groups of users, or a bot trained to generate
fraudulent content. Many studies proposed approaches based on user behaviors
and review text to address the challenges of fraud detection. To provide an
exhaustive literature review, social fraud detection is reviewed using a
framework that considers three key components: the review itself, the user who
carries out the review, and the item being reviewed. As features are extracted
for the component representation, a feature-wise review is provided based on
behavioral, text-based features and their combination. With this framework, a
comprehensive overview of approaches is presented including supervised,
semi-supervised, and unsupervised learning. The supervised approaches for fraud
detection are introduced and categorized into two sub-categories; classical,
and deep learning. The lack of labeled datasets is explained and potential
solutions are suggested. To help new researchers in the area develop a better
understanding, a topic analysis and an overview of future directions is
provided in each step of the proposed systematic framework.

    

### [[2111.05670] DeCOM: Decomposed Policy for Constrained Cooperative Multi-Agent Reinforcement Learning](http://arxiv.org/abs/2111.05670)


  In recent years, multi-agent reinforcement learning (MARL) has presented
impressive performance in various applications. However, physical limitations,
budget restrictions, and many other factors usually impose \textit{constraints}
on a multi-agent system (MAS), which cannot be handled by traditional MARL
frameworks. Specifically, this paper focuses on constrained MASes where agents
work \textit{cooperatively} to maximize the expected team-average return under
various constraints on expected team-average costs, and develops a
\textit{constrained cooperative MARL} framework, named DeCOM, for such MASes.
In particular, DeCOM decomposes the policy of each agent into two modules,
which empowers information sharing among agents to achieve better cooperation.
In addition, with such modularization, the training algorithm of DeCOM
separates the original constrained optimization into an unconstrained
optimization on reward and a constraints satisfaction problem on costs. DeCOM
then iteratively solves these problems in a computationally efficient manner,
which makes DeCOM highly scalable. We also provide theoretical guarantees on
the convergence of DeCOM's policy update algorithm. Finally, we validate the
effectiveness of DeCOM with various types of costs in both toy and large-scale
(with 500 agents) environments.

    

### [[2111.05672] Automatically detecting data drift in machine learning classifiers](http://arxiv.org/abs/2111.05672)


  Classifiers and other statistics-based machine learning (ML) techniques
generalize, or learn, based on various statistical properties of the training
data. The assumption underlying statistical ML resulting in theoretical or
empirical performance guarantees is that the distribution of the training data
is representative of the production data distribution. This assumption often
breaks; for instance, statistical distributions of the data may change. We term
changes that affect ML performance `data drift' or `drift'.
Many classification techniques compute a measure of confidence in their
results. This measure might not reflect the actual ML performance. A famous
example is the Panda picture that is correctly classified as such with a
confidence of about 60\%, but when noise is added it is incorrectly classified
as a Gibbon with a confidence of above 99\%. However, the work we report on
here suggests that a classifier's measure of confidence can be used for the
purpose of detecting data drift.
We propose an approach based solely on classifier suggested labels and its
confidence in them, for alerting on data distribution or feature space changes
that are likely to cause data drift. Our approach identities degradation in
model performance and does not require labeling of data in production which is
often lacking or delayed. Our experiments with three different data sets and
classifiers demonstrate the effectiveness of this approach in detecting data
drift. This is especially encouraging as the classification itself may or may
not be correct and no model input data is required. We further explore the
statistical approach of sequential change-point tests to automatically
determine the amount of data needed in order to identify drift while
controlling the false positive rate (Type-1 error).

    

### [[2111.05684] Learning to ignore: rethinking attention in CNNs](http://arxiv.org/abs/2111.05684)


  Recently, there has been an increasing interest in applying attention
mechanisms in Convolutional Neural Networks (CNNs) to solve computer vision
tasks. Most of these methods learn to explicitly identify and highlight
relevant parts of the scene and pass the attended image to further layers of
the network. In this paper, we argue that such an approach might not be
optimal. Arguably, explicitly learning which parts of the image are relevant is
typically harder than learning which parts of the image are less relevant and,
thus, should be ignored. In fact, in vision domain, there are many
easy-to-identify patterns of irrelevant features. For example, image regions
close to the borders are less likely to contain useful information for a
classification task. Based on this idea, we propose to reformulate the
attention mechanism in CNNs to learn to ignore instead of learning to attend.
Specifically, we propose to explicitly learn irrelevant information in the
scene and suppress it in the produced representation, keeping only important
attributes. This implicit attention scheme can be incorporated into any
existing attention mechanism. In this work, we validate this idea using two
recent attention methods Squeeze and Excitation (SE) block and Convolutional
Block Attention Module (CBAM). Experimental results on different datasets and
model architectures show that learning to ignore, i.e., implicit attention,
yields superior performance compared to the standard approaches.

    

### [[2111.05685] Efficient Neural Network Training via Forward and Backward Propagation Sparsification](http://arxiv.org/abs/2111.05685)


  Sparse training is a natural idea to accelerate the training speed of deep
neural networks and save the memory usage, especially since large modern neural
networks are significantly over-parameterized. However, most of the existing
methods cannot achieve this goal in practice because the chain rule based
gradient (w.r.t. structure parameters) estimators adopted by previous methods
require dense computation at least in the backward propagation step. This paper
solves this problem by proposing an efficient sparse training method with
completely sparse forward and backward passes. We first formulate the training
process as a continuous minimization problem under global sparsity constraint.
We then separate the optimization process into two steps, corresponding to
weight update and structure parameter update. For the former step, we use the
conventional chain rule, which can be sparse via exploiting the sparse
structure. For the latter step, instead of using the chain rule based gradient
estimators as in existing methods, we propose a variance reduced policy
gradient estimator, which only requires two forward passes without backward
propagation, thus achieving completely sparse training. We prove that the
variance of our gradient estimator is bounded. Extensive experimental results
on real-world datasets demonstrate that compared to previous methods, our
algorithm is much more effective in accelerating the training process, up to an
order of magnitude faster.

    

### [[2111.05694] LSP : Acceleration and Regularization of Graph Neural Networks via Locality Sensitive Pruning of Graphs](http://arxiv.org/abs/2111.05694)


  Graph Neural Networks (GNNs) have emerged as highly successful tools for
graph-related tasks. However, real-world problems involve very large graphs,
and the compute resources needed to fit GNNs to those problems grow rapidly.
Moreover, the noisy nature and size of real-world graphs cause GNNs to over-fit
if not regularized properly. Surprisingly, recent works show that large graphs
often involve many redundant components that can be removed without
compromising the performance too much. This includes node or edge removals
during inference through GNNs layers or as a pre-processing step that
sparsifies the input graph. This intriguing phenomenon enables the development
of state-of-the-art GNNs that are both efficient and accurate. In this paper,
we take a further step towards demystifying this phenomenon and propose a
systematic method called Locality-Sensitive Pruning (LSP) for graph pruning
based on Locality-Sensitive Hashing. We aim to sparsify a graph so that similar
local environments of the original graph result in similar environments in the
resulting sparsified graph, which is an essential feature for graph-related
tasks. To justify the application of pruning based on local graph properties,
we exemplify the advantage of applying pruning based on locality properties
over other pruning strategies in various scenarios. Extensive experiments on
synthetic and real-world datasets demonstrate the superiority of LSP, which
removes a significant amount of edges from large graphs without compromising
the performance, accompanied by a considerable acceleration.

    

### [[2111.05708] STNN-DDI: A Substructure-aware Tensor Neural Network to Predict Drug-Drug Interactions](http://arxiv.org/abs/2111.05708)


  Motivation: Computational prediction of multiple-type drug-drug interaction
(DDI) helps reduce unexpected side effects in poly-drug treatments. Although
existing computational approaches achieve inspiring results, they ignore that
the action of a drug is mainly caused by its chemical substructures. In
addition, their interpretability is still weak. Results: In this paper, by
supposing that the interactions between two given drugs are caused by their
local chemical structures (sub-structures) and their DDI types are determined
by the linkages between different substructure sets, we design a novel
Substructure-ware Tensor Neural Network model for DDI prediction (STNN-DDI).
The proposed model learns a 3-D tensor of (substructure, in-teraction type,
substructure) triplets, which characterizes a substructure-substructure
interaction (SSI) space. According to a list of predefined substructures with
specific chemical meanings, the mapping of drugs into this SSI space enables
STNN-DDI to perform the multiple-type DDI prediction in both transductive and
inductive scenarios in a unified form with an explicable manner. The
compar-ison with deep learning-based state-of-the-art baselines demonstrates
the superiority of STNN-DDI with the significant improvement of AUC, AUPR,
Accuracy, and Precision. More importantly, case studies illustrate its
interpretability by both revealing a crucial sub-structure pair across drugs
regarding a DDI type of interest and uncovering interaction type-specific
substructure pairs in a given DDI. In summary, STNN-DDI provides an effective
approach to predicting DDIs as well as explaining the interaction mechanisms
among drugs.

    

### [[2111.05711] Counterfactual Explanations for Models of Code](http://arxiv.org/abs/2111.05711)


  Machine learning (ML) models play an increasingly prevalent role in many
software engineering tasks. However, because most models are now powered by
opaque deep neural networks, it can be difficult for developers to understand
why the model came to a certain conclusion and how to act upon the model's
prediction. Motivated by this problem, this paper explores counterfactual
explanations for models of source code. Such counterfactual explanations
constitute minimal changes to the source code under which the model "changes
its mind". We integrate counterfactual explanation generation to models of
source code in a real-world setting. We describe considerations that impact
both the ability to find realistic and plausible counterfactual explanations,
as well as the usefulness of such explanation to the user of the model. In a
series of experiments we investigate the efficacy of our approach on three
different models, each based on a BERT-like architecture operating over source
code.

    

### [[2111.05721] Important Sentence Identification in Legal Cases Using Multi-Class Classification](http://arxiv.org/abs/2111.05721)


  The advancement of Natural Language Processing (NLP) is spreading through
various domains in forms of practical applications and academic interests.
Inherently, the legal domain contains a vast amount of data in text format.
Therefore it requires the application of NLP to cater to the analytically
demanding needs of the domain. Identifying important sentences, facts and
arguments in a legal case is such a tedious task for legal professionals. In
this research we explore the usage of sentence embeddings for multi-class
classification to identify important sentences in a legal case, in the
perspective of the main parties present in the case. In addition, a
task-specific loss function is defined in order to improve the accuracy
restricted by the straightforward use of categorical cross entropy loss.

    

### [[2111.05736] Multimodal Approach for Metadata Extraction from German Scientific Publications](http://arxiv.org/abs/2111.05736)


  Nowadays, metadata information is often given by the authors themselves upon
submission. However, a significant part of already existing research papers
have missing or incomplete metadata information. German scientific papers come
in a large variety of layouts which makes the extraction of metadata a
non-trivial task that requires a precise way to classify the metadata extracted
from the documents. In this paper, we propose a multimodal deep learning
approach for metadata extraction from scientific papers in the German language.
We consider multiple types of input data by combining natural language
processing and image vision processing. This model aims to increase the overall
accuracy of metadata extraction compared to other state-of-the-art approaches.
It enables the utilization of both spatial and contextual features in order to
achieve a more reliable extraction. Our model for this approach was trained on
a dataset consisting of around 8800 documents and is able to obtain an overall
F1-score of 0.923.

    

### [[2111.05754] Prune Once for All: Sparse Pre-Trained Language Models](http://arxiv.org/abs/2111.05754)


  Transformer-based language models are applied to a wide range of applications
in natural language processing. However, they are inefficient and difficult to
deploy. In recent years, many compression algorithms have been proposed to
increase the implementation efficiency of large Transformer-based models on
target hardware. In this work we present a new method for training sparse
pre-trained Transformer language models by integrating weight pruning and model
distillation. These sparse pre-trained models can be used to transfer learning
for a wide range of tasks while maintaining their sparsity pattern. We
demonstrate our method with three known architectures to create sparse
pre-trained BERT-Base, BERT-Large and DistilBERT. We show how the compressed
sparse pre-trained models we trained transfer their knowledge to five different
downstream natural language tasks with minimal accuracy loss. Moreover, we show
how to further compress the sparse models' weights to 8bit precision using
quantization-aware training. For example, with our sparse pre-trained
BERT-Large fine-tuned on SQuADv1.1 and quantized to 8bit we achieve a
compression ratio of $40$X for the encoder with less than $1\%$ accuracy loss.
To the best of our knowledge, our results show the best compression-to-accuracy
ratio for BERT-Base, BERT-Large, and DistilBERT.

    

### [[2111.05764] A framework for comprehensible multi-modal detection of cyber threats](http://arxiv.org/abs/2111.05764)


  Detection of malicious activities in corporate environments is a very complex
task and much effort has been invested into research of its automation.
However, vast majority of existing methods operate only in a narrow scope which
limits them to capture only fragments of the evidence of malware's presence.
Consequently, such approach is not aligned with the way how the cyber threats
are studied and described by domain experts. In this work, we discuss these
limitations and design a detection framework which combines observed events
from different sources of data. Thanks to this, it provides full insight into
the attack life cycle and enables detection of threats that require this
coupling of observations from different telemetries to identify the full scope
of the incident. We demonstrate applicability of the framework on a case study
of a real malware infection observed in a corporate network.

    

### [[2111.05790] Early Myocardial Infarction Detection over Multi-view Echocardiography](http://arxiv.org/abs/2111.05790)


  Myocardial infarction (MI) is the leading cause of mortality in the world
that occurs due to a blockage of the coronary arteries feeding the myocardium.
An early diagnosis of MI and its localization can mitigate the extent of
myocardial damage by facilitating early therapeutic interventions. Following
the blockage of a coronary artery, the regional wall motion abnormality (RWMA)
of the ischemic myocardial segments is the earliest change to set in.
Echocardiography is the fundamental tool to assess any RWMA. Assessing the
motion of the left ventricle (LV) wall only from a single echocardiography view
may lead to missing the diagnosis of MI as the RWMA may not be visible on that
specific view. Therefore, in this study, we propose to fuse apical 4-chamber
(A4C) and apical 2-chamber (A2C) views in which a total of 11 myocardial
segments can be analyzed for MI detection. The proposed method first estimates
the motion of the LV wall by Active Polynomials (APs), which extract and track
the endocardial boundary to compute myocardial segment displacements. The
features are extracted from the A4C and A2C view displacements, which are fused
and fed into the classifiers to detect MI. The main contributions of this study
are 1) creation of a new benchmark dataset by including both A4C and A2C views
in a total of 260 echocardiography recordings, which is publicly shared with
the research community, 2) improving the performance of the prior work of
threshold-based APs by a Machine Learning based approach, and 3) a pioneer MI
detection approach via multi-view echocardiography by fusing the information of
A4C and A2C views. Experimental results show that the proposed method achieves
90.91% sensitivity and 86.36% precision for MI detection over multi-view
echocardiography.

    

### [[2111.05791] Distribution-Invariant Differential Privacy](http://arxiv.org/abs/2111.05791)


  Differential privacy is becoming one gold standard for protecting the privacy
of publicly shared data. It has been widely used in social science, data
science, public health, information technology, and the U.S. decennial census.
Nevertheless, to guarantee differential privacy, existing methods may
unavoidably alter the conclusion of original data analysis, as privatization
often changes the sample distribution. This phenomenon is known as the
trade-off between privacy protection and statistical accuracy. In this work, we
break this trade-off by developing a distribution-invariant privatization (DIP)
method to reconcile both high statistical accuracy and strict differential
privacy. As a result, any downstream statistical or machine learning task
yields essentially the same conclusion as if one used the original data.
Numerically, under the same strictness of privacy protection, DIP achieves
superior statistical accuracy in two simulations and on three real-world
benchmarks.

    

### [[2111.05792] HARPO: Learning to Subvert Online Behavioral Advertising](http://arxiv.org/abs/2111.05792)


  Online behavioral advertising, and the associated tracking paraphernalia,
poses a real privacy threat. Unfortunately, existing privacy-enhancing tools
are not always effective against online advertising and tracking. We propose
Harpo, a principled learning-based approach to subvert online behavioral
advertising through obfuscation. Harpo uses reinforcement learning to
adaptively interleave real page visits with fake pages to distort a tracker's
view of a user's browsing profile. We evaluate Harpo against real-world user
profiling and ad targeting models used for online behavioral advertising. The
results show that Harpo improves privacy by triggering more than 40% incorrect
interest segments and 6x higher bid values. Harpo outperforms existing
obfuscation tools by as much as 16x for the same overhead. Harpo is also able
to achieve better stealthiness to adversarial detection than existing
obfuscation tools. Harpo meaningfully advances the state-of-the-art in
leveraging obfuscation to subvert online behavioral advertising

    

### [[2111.05803] Gradients are Not All You Need](http://arxiv.org/abs/2111.05803)


  Differentiable programming techniques are widely used in the community and
are responsible for the machine learning renaissance of the past several
decades. While these methods are powerful, they have limits. In this short
report, we discuss a common chaos based failure mode which appears in a variety
of differentiable circumstances, ranging from recurrent neural networks and
numerical physics simulation to training learned optimizers. We trace this
failure to the spectrum of the Jacobian of the system under study, and provide
criteria for when a practitioner might expect this failure to spoil their
differentiation based optimization algorithms.

    

### [[2111.05808] BagBERT: BERT-based bagging-stacking for multi-topic classification](http://arxiv.org/abs/2111.05808)


  This paper describes our submission on the COVID-19 literature annotation
task at Biocreative VII. We proposed an approach that exploits the knowledge of
the globally non-optimal weights, usually rejected, to build a rich
representation of each label. Our proposed approach consists of two stages: (1)
A bagging of various initializations of the training data that features weakly
trained weights, (2) A stacking of heterogeneous vocabulary models based on
BERT and RoBERTa Embeddings. The aggregation of these weak insights performs
better than a classical globally efficient model. The purpose is the
distillation of the richness of knowledge to a simpler and lighter model. Our
system obtains an Instance-based F1 of 92.96 and a Label-based micro-F1 of
91.35.

    

### [[2111.05814] SwAMP: Swapped Assignment of Multi-Modal Pairs for Cross-Modal Retrieval](http://arxiv.org/abs/2111.05814)


  We tackle the cross-modal retrieval problem, where the training is only
supervised by the relevant multi-modal pairs in the data. The contrastive
learning is the most popular approach for this task. However, its sampling
complexity for learning is quadratic in the number of training data points.
Moreover, it makes potentially wrong assumption that the instances in different
pairs are automatically irrelevant. To address these issues, we propose a novel
loss function that is based on self-labeling of the unknown classes.
Specifically, we aim to predict class labels of the data instances in each
modality, and assign those labels to the corresponding instances in the other
modality (i.e., swapping the pseudo labels). With these swapped labels, we
learn the data embedding for each modality using the supervised cross-entropy
loss, hence leading to linear sampling complexity. We also maintain the queues
for storing the embeddings of the latest batches, for which clustering
assignment and embedding learning are done at the same time in an online
fashion. This removes computational overhead of injecting intermittent epochs
of entire training data sweep for offline clustering. We tested our approach on
several real-world cross-modal retrieval problems, including text-based video
retrieval, sketch-based image retrieval, and image-text retrieval, and for all
these tasks our method achieves significant performance improvement over the
contrastive learning.

    

### [[2111.05818] Efficient Projection-Free Online Convex Optimization with Membership Oracle](http://arxiv.org/abs/2111.05818)


  In constrained convex optimization, existing methods based on the ellipsoid
or cutting plane method do not scale well with the dimension of the ambient
space. Alternative approaches such as Projected Gradient Descent only provide a
computational benefit for simple convex sets such as Euclidean balls, where
Euclidean projections can be performed efficiently. For other sets, the cost of
the projections can be too high. To circumvent these issues, alternative
methods based on the famous Frank-Wolfe algorithm have been studied and used.
Such methods use a Linear Optimization Oracle at each iteration instead of
Euclidean projections; the former can often be performed efficiently. Such
methods have also been extended to the online and stochastic optimization
settings. However, the Frank-Wolfe algorithm and its variants do not achieve
the optimal performance, in terms of regret or rate, for general convex sets.
What is more, the Linear Optimization Oracle they use can still be
computationally expensive in some cases. In this paper, we move away from
Frank-Wolfe style algorithms and present a new reduction that turns any
algorithm A defined on a Euclidean ball (where projections are cheap) to an
algorithm on a constrained set C contained within the ball, without sacrificing
the performance of the original algorithm A by much. Our reduction requires O(T
log T) calls to a Membership Oracle on C after T rounds, and no linear
optimization on C is needed. Using our reduction, we recover optimal regret
bounds [resp. rates], in terms of the number of iterations, in online [resp.
stochastic] convex optimization. Our guarantees are also useful in the offline
convex optimization setting when the dimension of the ambient space is large.

    

### [[2111.05820] Multi-Task Neural Processes](http://arxiv.org/abs/2111.05820)


  Neural processes have recently emerged as a class of powerful neural latent
variable models that combine the strengths of neural networks and stochastic
processes. As they can encode contextual data in the network's function space,
they offer a new way to model task relatedness in multi-task learning. To study
its potential, we develop multi-task neural processes, a new variant of neural
processes for multi-task learning. In particular, we propose to explore
transferable knowledge from related tasks in the function space to provide
inductive bias for improving each individual task. To do so, we derive the
function priors in a hierarchical Bayesian inference framework, which enables
each task to incorporate the shared knowledge provided by related tasks into
its context of the prediction function. Our multi-task neural processes
methodologically expand the scope of vanilla neural processes and provide a new
way of exploring task relatedness in function spaces for multi-task learning.
The proposed multi-task neural processes are capable of learning multiple tasks
with limited labeled data and in the presence of domain shift. We perform
extensive experimental evaluations on several benchmarks for the multi-task
regression and classification tasks. The results demonstrate the effectiveness
of multi-task neural processes in transferring useful knowledge among tasks for
multi-task learning and superior performance in multi-task classification and
brain image segmentation.

    

### [[2111.05826] Palette: Image-to-Image Diffusion Models](http://arxiv.org/abs/2111.05826)


  We introduce Palette, a simple and general framework for image-to-image
translation using conditional diffusion models. On four challenging
image-to-image translation tasks (colorization, inpainting, uncropping, and
JPEG decompression), Palette outperforms strong GAN and regression baselines,
and establishes a new state of the art. This is accomplished without
task-specific hyper-parameter tuning, architecture customization, or any
auxiliary loss, demonstrating a desirable degree of generality and flexibility.
We uncover the impact of using $L_2$ vs. $L_1$ loss in the denoising diffusion
objective on sample diversity, and demonstrate the importance of self-attention
through empirical architecture studies. Importantly, we advocate a unified
evaluation protocol based on ImageNet, and report several sample quality scores
including FID, Inception Score, Classification Accuracy of a pre-trained
ResNet-50, and Perceptual Distance against reference images for various
baselines. We expect this standardized evaluation protocol to play a critical
role in advancing image-to-image translation research. Finally, we show that a
single generalist Palette model trained on 3 tasks (colorization, inpainting,
JPEG decompression) performs as well or better than task-specific specialist
counterparts.

    

### [[2111.05834] Searching in the Forest for Local Bayesian Optimization](http://arxiv.org/abs/2111.05834)


  Because of its sample efficiency, Bayesian optimization (BO) has become a
popular approach dealing with expensive black-box optimization problems, such
as hyperparameter optimization (HPO). Recent empirical experiments showed that
the loss landscapes of HPO problems tend to be more benign than previously
assumed, i.e. in the best case uni-modal and convex, such that a BO framework
could be more efficient if it can focus on those promising local regions. In
this paper, we propose BOinG, a two-stage approach that is tailored toward
mid-sized configuration spaces, as one encounters in many HPO problems. In the
first stage, we build a scalable global surrogate model with a random forest to
describe the overall landscape structure. Further, we choose a promising
subregion via a bottom-up approach on the upper-level tree structure. In the
second stage, a local model in this subregion is utilized to suggest the point
to be evaluated next. Empirical experiments show that BOinG is able to exploit
the structure of typical HPO problems and performs particularly well on
mid-sized problems from synthetic functions and HPO.

    

### [[2111.05841] Physics-enhanced deep surrogates for PDEs](http://arxiv.org/abs/2111.05841)


  We present a "physics-enhanced deep-surrogate ("PEDS") approach towards
developing fast surrogate models for complex physical systems described by
partial differential equations (PDEs) and similar models: we show how to
combine a low-fidelity "coarse" solver with a neural network that generates
"coarsified'' inputs, trained end-to-end to globally match the output of an
expensive high-fidelity numerical solver. In this way, by incorporating limited
physical knowledge in the form of the low-fidelity model, we find that a PEDS
surrogate can be trained with at least $\sim 10\times$ less data than a
"black-box'' neural network for the same accuracy. Asymptotically, PEDS appears
to learn with a steeper power law than black-box surrogates, and benefits even
further when combined with active learning. We demonstrate feasibility and
benefit of the proposed approach by using an example problem in electromagnetic
scattering that appears in the design of optical metamaterials.

    

### [[2111.05850] Towards Green Automated Machine Learning: Status Quo and Future Directions](http://arxiv.org/abs/2111.05850)


  Automated machine learning (AutoML) strives for the automatic configuration
of machine learning algorithms and their composition into an overall (software)
solution - a machine learning pipeline - tailored to the learning task
(dataset) at hand. Over the last decade, AutoML has become a hot research topic
with hundreds of contributions. While AutoML offers many prospects, it is also
known to be quite resource-intensive, which is one of its major points of
criticism. The primary cause for a high resource consumption is that many
approaches rely on the (costly) evaluation of many ML pipelines while searching
for good candidates. This problem is amplified in the context of research on
AutoML methods, due to large scale experiments conducted with many datasets and
approaches, each of them being run with several repetitions to rule out random
effects. In the spirit of recent work on Green AI, this paper is written in an
attempt to raise the awareness of AutoML researchers for the problem and to
elaborate on possible remedies. To this end, we identify four categories of
actions the community may take towards more sustainable research on AutoML,
namely approach design, benchmarking, research incentives, and transparency.

    

### [[1809.03474] Universal Multi-Party Poisoning Attacks](http://arxiv.org/abs/1809.03474)


  In this work, we demonstrate universal multi-party poisoning attacks that
adapt and apply to any multi-party learning process with arbitrary interaction
pattern between the parties. More generally, we introduce and study
$(k,p)$-poisoning attacks in which an adversary controls $k\in[m]$ of the
parties, and for each corrupted party $P_i$, the adversary submits some
poisoned data $\mathcal{T}'_i$ on behalf of $P_i$ that is still
``$(1-p)$-close'' to the correct data $\mathcal{T}_i$ (e.g., $1-p$ fraction of
$\mathcal{T}'_i$ is still honestly generated). We prove that for any ``bad''
property $B$ of the final trained hypothesis $h$ (e.g., $h$ failing on a
particular test example or having ``large'' risk) that has an arbitrarily small
constant probability of happening without the attack, there always is a
$(k,p)$-poisoning attack that increases the probability of $B$ from $\mu$ to by
$\mu^{1-p \cdot k/m} = \mu + \Omega(p \cdot k/m)$. Our attack only uses clean
labels, and it is online.
More generally, we prove that for any bounded function $f(x_1,\dots,x_n) \in
[0,1]$ defined over an $n$-step random process $\mathbf{X} = (x_1,\dots,x_n)$,
an adversary who can override each of the $n$ blocks with even dependent
probability $p$ can increase the expected output by at least $\Omega(p \cdot
\mathrm{Var}[f(\mathbf{x})])$.

    

### [[1810.00481] Two new results about quantum exact learning](http://arxiv.org/abs/1810.00481)


  We present two new results about exact learning by quantum computers. First,
we show how to exactly learn a $k$-Fourier-sparse $n$-bit Boolean function from
$O(k^{1.5}(\log k)^2)$ uniform quantum examples for that function. This
improves over the bound of $\widetilde{\Theta}(kn)$ uniformly random
\emph{classical} examples (Haviv and Regev, CCC'15). Additionally, we provide a
possible direction to improve our $\widetilde{O}(k^{1.5})$ upper bound by
proving an improvement of Chang's lemma for $k$-Fourier-sparse Boolean
functions. Second, we show that if a concept class $\mathcal{C}$ can be exactly
learned using $Q$ quantum membership queries, then it can also be learned using
$O\left(\frac{Q^2}{\log Q}\log|\mathcal{C}|\right)$ \emph{classical} membership
queries. This improves the previous-best simulation result (Servedio and
Gortler, SICOMP'04) by a $\log Q$-factor.

    

### [[1910.09616] Volterra Neural Networks (VNNs)](http://arxiv.org/abs/1910.09616)


  The importance of inference in Machine Learning (ML) has led to an explosive
number of different proposals in ML, and particularly in Deep Learning. In an
attempt to reduce the complexity of Convolutional Neural Networks, we propose a
Volterra filter-inspired Network architecture. This architecture introduces
controlled non-linearities in the form of interactions between the delayed
input samples of data. We propose a cascaded implementation of Volterra
Filtering so as to significantly reduce the number of parameters required to
carry out the same classification task as that of a conventional Neural
Network. We demonstrate an efficient parallel implementation of this Volterra
Neural Network (VNN), along with its remarkable performance while retaining a
relatively simpler and potentially more tractable structure. Furthermore, we
show a rather sophisticated adaptation of this network to nonlinearly fuse the
RGB (spatial) information and the Optical Flow (temporal) information of a
video sequence for action recognition. The proposed approach is evaluated on
UCF-101 and HMDB-51 datasets for action recognition, and is shown to outperform
state of the art CNN approaches.

    

### [[2002.11318] Can we have it all? On the Trade-off between Spatial and Adversarial Robustness of Neural Networks](http://arxiv.org/abs/2002.11318)


  (Non-)robustness of neural networks to small, adversarial pixel-wise
perturbations, and as more recently shown, to even random spatial
transformations (e.g., translations, rotations) entreats both theoretical and
empirical understanding. Spatial robustness to random translations and
rotations is commonly attained via equivariant models (e.g., StdCNNs, GCNNs)
and training augmentation, whereas adversarial robustness is typically achieved
by adversarial training. In this paper, we prove a quantitative trade-off
between spatial and adversarial robustness in a simple statistical setting. We
complement this empirically by showing that: (a) as the spatial robustness of
equivariant models improves by training augmentation with progressively larger
transformations, their adversarial robustness worsens progressively, and (b) as
the state-of-the-art robust models are adversarially trained with progressively
larger pixel-wise perturbations, their spatial robustness drops progressively.
Towards achieving pareto-optimality in this trade-off, we propose a method
based on curriculum learning that trains gradually on more difficult
perturbations (both spatial and adversarial) to improve spatial and adversarial
robustness simultaneously.

    

### [[2003.03917] BitTensor: A Peer-to-Peer Intelligence Market](http://arxiv.org/abs/2003.03917)


  As with other commodities, markets could help us efficiently produce machine
intelligence. We propose a market where intelligence is priced by other
intelligence systems peer-to-peer across the internet. Peers rank each other by
training neural networks which learn the value of their neighbors. Scores
accumulate on a digital ledger where high ranking peers are monetarily rewarded
with additional weight in the network. However, this form of peer-ranking is
not resistant to collusion, which could disrupt the accuracy of the mechanism.
The solution is a connectivity-based regularization which exponentially rewards
trusted peers, making the system resistant to collusion of up to 50 percent of
the network weight. The result is a collectively run intelligence market which
continual produces newly trained models and pays contributors who create
information theoretic value.

    

### [[2006.09984] Interpolation and Learning with Scale Dependent Kernels](http://arxiv.org/abs/2006.09984)


  We study the learning properties of nonparametric ridge-less least squares.
In particular, we consider the common case of estimators defined by scale
dependent kernels, and focus on the role of the scale. These estimators
interpolate the data and the scale can be shown to control their stability
through the condition number. Our analysis shows that are different regimes
depending on the interplay between the sample size, its dimensions, and the
smoothness of the problem. Indeed, when the sample size is less than
exponential in the data dimension, then the scale can be chosen so that the
learning error decreases. As the sample size becomes larger, the overall error
stop decreasing but interestingly the scale can be chosen in such a way that
the variance due to noise remains bounded. Our analysis combines, probabilistic
results with a number of analytic techniques from interpolation theory.

    

### [[2006.13409] When Do Neural Networks Outperform Kernel Methods?](http://arxiv.org/abs/2006.13409)


  For a certain scaling of the initialization of stochastic gradient descent
(SGD), wide neural networks (NN) have been shown to be well approximated by
reproducing kernel Hilbert space (RKHS) methods. Recent empirical work showed
that, for some classification tasks, RKHS methods can replace NNs without a
large loss in performance. On the other hand, two-layers NNs are known to
encode richer smoothness classes than RKHS and we know of special examples for
which SGD-trained NN provably outperform RKHS. This is true even in the wide
network limit, for a different scaling of the initialization.
How can we reconcile the above claims? For which tasks do NNs outperform
RKHS? If covariates are nearly isotropic, RKHS methods suffer from the curse of
dimensionality, while NNs can overcome it by learning the best low-dimensional
representation. Here we show that this curse of dimensionality becomes milder
if the covariates display the same low-dimensional structure as the target
function, and we precisely characterize this tradeoff. Building on these
results, we present the spiked covariates model that can capture in a unified
framework both behaviors observed in earlier work.
We hypothesize that such a latent low-dimensional structure is present in
image classification. We test numerically this hypothesis by showing that
specific perturbations of the training distribution degrade the performances of
RKHS methods much more significantly than NNs.

    

### [[2007.08199] Learning from Noisy Labels with Deep Neural Networks: A Survey](http://arxiv.org/abs/2007.08199)


  Deep learning has achieved remarkable success in numerous domains with help
from large amounts of big data. However, the quality of data labels is a
concern because of the lack of high-quality labels in many real-world
scenarios. As noisy labels severely degrade the generalization performance of
deep neural networks, learning from noisy labels (robust training) is becoming
an important task in modern deep learning applications. In this survey, we
first describe the problem of learning with label noise from a supervised
learning perspective. Next, we provide a comprehensive review of 62
state-of-the-art robust training methods, all of which are categorized into
five groups according to their methodological difference, followed by a
systematic comparison of six properties used to evaluate their superiority.
Subsequently, we perform an in-depth analysis of noise rate estimation and
summarize the typically used evaluation methodology, including public noisy
datasets and evaluation metrics. Finally, we present several promising research
directions that can serve as a guideline for future studies. All the contents
will be available at this https URL.

    

### [[2008.05759] MICE: Mining Idioms with Contextual Embeddings](http://arxiv.org/abs/2008.05759)


  Idiomatic expressions can be problematic for natural language processing
applications as their meaning cannot be inferred from their constituting words.
A lack of successful methodological approaches and sufficiently large datasets
prevents the development of machine learning approaches for detecting idioms,
especially for expressions that do not occur in the training set. We present an
approach, called MICE, that uses contextual embeddings for that purpose. We
present a new dataset of multi-word expressions with literal and idiomatic
meanings and use it to train a classifier based on two state-of-the-art
contextual word embeddings: ELMo and BERT. We show that deep neural networks
using both embeddings perform much better than existing approaches, and are
capable of detecting idiomatic word use, even for expressions that were not
present in the training set. We demonstrate cross-lingual transfer of developed
models and analyze the size of the required dataset.

    

### [[2009.05283] Fair and accurate age prediction using distribution aware data curation and augmentation](http://arxiv.org/abs/2009.05283)


  Deep learning-based facial recognition systems have experienced increased
media attention due to exhibiting unfair behavior. Large enterprises, such as
IBM, shut down their facial recognition and age prediction systems as a
consequence. Age prediction is an especially difficult application with the
issue of fairness remaining an open research problem (e.g., predicting age for
different ethnicity equally accurate). One of the main causes of unfair
behavior in age prediction methods lies in the distribution and diversity of
the training data. In this work, we present two novel approaches for dataset
curation and data augmentation in order to increase fairness through balanced
feature curation and increase diversity through distribution aware
augmentation. To achieve this, we introduce out-of-distribution detection to
the facial recognition domain which is used to select the data most relevant to
the deep neural network's (DNN) task when balancing the data among age,
ethnicity, and gender. Our approach shows promising results. Our best-trained
DNN model outperformed all academic and industrial baselines in terms of
fairness by up to 4.92 times and also enhanced the DNN's ability to generalize
outperforming Amazon AWS and Microsoft Azure public cloud systems by 31.88% and
10.95%, respectively.

    

### [[2010.04196] A Fully Tensorized Recurrent Neural Network](http://arxiv.org/abs/2010.04196)


  Recurrent neural networks (RNNs) are powerful tools for sequential modeling,
but typically require significant overparameterization and regularization to
achieve optimal performance. This leads to difficulties in the deployment of
large RNNs in resource-limited settings, while also introducing complications
in hyperparameter selection and training. To address these issues, we introduce
a "fully tensorized" RNN architecture which jointly encodes the separate weight
matrices within each recurrent cell using a lightweight tensor-train (TT)
factorization. This approach represents a novel form of weight sharing which
reduces model size by several orders of magnitude, while still maintaining
similar or better performance compared to standard RNNs. Experiments on image
classification and speaker verification tasks demonstrate further benefits for
reducing inference times and stabilizing model training and hyperparameter
selection.

    

### [[2010.11187] G-Elo: Generalization of the Elo algorithm by modelling the discretized Margin of Victory](http://arxiv.org/abs/2010.11187)


  In this work we develop a new algorithm for rating of teams (or players) in
one-on-one games by exploiting the observed difference of the game-points (such
as goals), also known as a margin of victory (MOV). Our objective is to obtain
the Elo-style algorithm whose operation is simple to implement and to
understand intuitively. This is done in three steps: first, we define the
probabilistic model between the teams' skills and the discretized MOV variable:
this generalizes the model underpinning the Elo algorithm, where the MOV
variable is discretized into three categories (win/loss/draw). Second, with the
formal probabilistic model at hand, the optimization required by the maximum
likelihood rule is implemented via stochastic gradient; this yields simple
on-line equations for the rating updates which are identical in their general
form to those characteristic of the Elo algorithm: the main difference lies in
the way the scores and the expected scores are defined. Third, we propose a
simple method to estimate the coefficients of the model, and thus define the
operation of the algorithm; it is done in a closed form using the historical
data so the algorithm is tailored to the sport of interest and the coefficients
defining its operation are determined in entirely transparent manner. The
alternative, optimization-based strategy to find the coefficients is also
presented. We show numerical examples based on the results of the association
football of the English Premier League and the American football of the
National Football League.

    

### [[2011.02089] Extended Missing Data Imputation via GANs for Ranking Applications](http://arxiv.org/abs/2011.02089)


  We propose Conditional Imputation GAN, an extended missing data imputation
method based on Generative Adversarial Networks (GANs). The motivating use case
is learning-to-rank, the cornerstone of modern search, recommendation system,
and information retrieval applications. Empirical ranking datasets do not
always follow standard Gaussian distributions or Missing Completely At Random
(MCAR) mechanism, which are standard assumptions of classic missing data
imputation methods. Our methodology provides a simple solution that offers
compatible imputation guarantees while relaxing assumptions for missing
mechanisms and sidesteps approximating intractable distributions to improve
imputation quality. We prove that the optimal GAN imputation is achieved for
Extended Missing At Random (EMAR) and Extended Always Missing At Random (EAMAR)
mechanisms, beyond the naive MCAR. Our method demonstrates the highest
imputation quality on the open-source Microsoft Research Ranking (MSR) Dataset
and a synthetic ranking dataset compared to state-of-the-art benchmarks and
across various feature distributions. Using a proprietary Amazon Search ranking
dataset, we also demonstrate comparable ranking quality metrics for ranking
models trained on GAN-imputed data compared to ground-truth data.

    

### [[2011.04185] Robust Batch Policy Learning in Markov Decision Processes](http://arxiv.org/abs/2011.04185)


  We study the offline data-driven sequential decision making problem in the
framework of Markov decision process (MDP). In order to enhance the
generalizability and adaptivity of the learned policy, we propose to evaluate
each policy by a set of the average rewards with respect to distributions
centered at the policy induced stationary distribution. Given a pre-collected
dataset of multiple trajectories generated by some behavior policy, our goal is
to learn a robust policy in a pre-specified policy class that can maximize the
smallest value of this set. Leveraging the theory of semi-parametric
statistics, we develop a statistically efficient policy learning method for
estimating the de ned robust optimal policy. A rate-optimal regret bound up to
a logarithmic factor is established in terms of total decision points in the
dataset.

    

### [[2011.14126] Risk-Monotonicity in Statistical Learning](http://arxiv.org/abs/2011.14126)


  Acquisition of data is a difficult task in many applications of machine
learning, and it is only natural that one hopes and expects the population risk
to decrease (better performance) monotonically with increasing data points. It
turns out, somewhat surprisingly, that this is not the case even for the most
standard algorithms that minimize the empirical risk. Non-monotonic behavior of
the risk and instability in training have manifested and appeared in the
popular deep learning paradigm under the description of double descent. These
problems highlight the current lack of understanding of learning algorithms and
generalization. It is, therefore, crucial to pursue this concern and provide a
characterization of such behavior. In this paper, we derive the first
consistent and risk-monotonic algorithms for a general statistical learning
setting under weak assumptions, consequently resolving an open problem Viering
et al. 2019 on how to avoid non-monotonic behavior of risk curves. We further
show that risk monotonicity need not necessarily come at the price of worse
excess risk rates. To achieve this, we derive new empirical Bernstein-like
concentration inequalities of independent interest that hold for certain
non-i.i.d. processes such as Martingale Difference Sequences.

    

### [[2102.05295] An Efficient Pessimistic-Optimistic Algorithm for Stochastic Linear Bandits with General Constraints](http://arxiv.org/abs/2102.05295)


  This paper considers stochastic linear bandits with general nonlinear
constraints. The objective is to maximize the expected cumulative reward over
horizon $T$ subject to a set of constraints in each round $\tau\leq T$. We
propose a pessimistic-optimistic algorithm for this problem, which is efficient
in two aspects. First, the algorithm yields $\tilde{\cal
O}\left(\left(\frac{K^{0.75}}{\delta}+d\right)\sqrt{\tau}\right)$ (pseudo)
regret in round $\tau\leq T,$ where $K$ is the number of constraints, $d$ is
the dimension of the reward feature space, and $\delta$ is a Slater's constant;
and zero constraint violation in any round $\tau>\tau',$ where $\tau'$ is
independent of horizon $T.$ Second, the algorithm is computationally efficient.
Our algorithm is based on the primal-dual approach in optimization and includes
two components. The primal component is similar to unconstrained stochastic
linear bandits (our algorithm uses the linear upper confidence bound algorithm
(LinUCB)). The computational complexity of the dual component depends on the
number of constraints, but is independent of the sizes of the contextual space,
the action space, and the feature space. Thus, the overall computational
complexity of our algorithm is similar to that of the linear UCB for
unconstrained stochastic linear bandits.

    

### [[2102.06961] PerSim: Data-Efficient Offline Reinforcement Learning with Heterogeneous Agents via Personalized Simulators](http://arxiv.org/abs/2102.06961)


  We consider offline reinforcement learning (RL) with heterogeneous agents
under severe data scarcity, i.e., we only observe a single historical
trajectory for every agent under an unknown, potentially sub-optimal policy. We
find that the performance of state-of-the-art offline and model-based RL
methods degrade significantly given such limited data availability, even for
commonly perceived "solved" benchmark settings such as "MountainCar" and
"CartPole". To address this challenge, we propose PerSim, a model-based offline
RL approach which first learns a personalized simulator for each agent by
collectively using the historical trajectories across all agents, prior to
learning a policy. We do so by positing that the transition dynamics across
agents can be represented as a latent function of latent factors associated
with agents, states, and actions; subsequently, we theoretically establish that
this function is well-approximated by a "low-rank" decomposition of separable
agent, state, and action latent functions. This representation suggests a
simple, regularized neural network architecture to effectively learn the
transition dynamics per agent, even with scarce, offline data. We perform
extensive experiments across several benchmark environments and RL methods. The
consistent improvement of our approach, measured in terms of both state
dynamics prediction and eventual reward, confirms the efficacy of our framework
in leveraging limited historical data to simultaneously learn personalized
policies across agents.

    

### [[2102.12466] Information Directed Reward Learning for Reinforcement Learning](http://arxiv.org/abs/2102.12466)


  For many reinforcement learning (RL) applications, specifying a reward is
difficult. This paper considers an RL setting where the agent obtains
information about the reward only by querying an expert that can, for example,
evaluate individual states or provide binary preferences over trajectories.
From such expensive feedback, we aim to learn a model of the reward that allows
standard RL algorithms to achieve high expected returns with as few expert
queries as possible. To this end, we propose Information Directed Reward
Learning (IDRL), which uses a Bayesian model of the reward and selects queries
that maximize the information gain about the difference in return between
plausibly optimal policies. In contrast to prior active reward learning methods
designed for specific types of queries, IDRL naturally accommodates different
query types. Moreover, it achieves similar or better performance with
significantly fewer queries by shifting the focus from reducing the reward
approximation error to improving the policy induced by the reward model. We
support our findings with extensive evaluations in multiple environments and
with different query types.

    

### [[2103.10932] Multi-Time-Scale Input Approaches for Hourly-Scale Rainfall-Runoff Modeling based on Recurrent Neural Networks](http://arxiv.org/abs/2103.10932)


  This study proposes two straightforward yet effective approaches to reduce
the required computational time of the training process for time-series
modeling through a recurrent neural network (RNN) using multi-time-scale
time-series data as input. One approach provides coarse and fine temporal
resolutions of the input time-series to RNN in parallel. The other concatenates
the coarse and fine temporal resolutions of the input time-series data over
time before considering them as the input to RNN. In both approaches, first,
finer temporal resolution data are utilized to learn the fine temporal scale
behavior of the target data. Next, coarser temporal resolution data are
expected to capture long-duration dependencies between the input and target
variables. The proposed approaches were implemented for hourly rainfall-runoff
modeling at a snow-dominated watershed by employing a long and short-term
memory (LSTM) network, which is a newer type of RNN. Subsequently, the daily
and hourly meteorological data were utilized as the input, and hourly flow
discharge was considered as the target data. The results confirm that both of
the proposed approaches can reduce the computational time for the training of
RNN significantly (up to 32.4 times). Furthermore, one of the proposed
approaches improves the estimation accuracy.

    

### [[2103.12833] Bandit Learning for Dynamic Colonel Blotto Game with a Budget Constraint](http://arxiv.org/abs/2103.12833)


  We consider a dynamic Colonel Blotto game (CBG) in which one of the players
is the learner and has limited troops (budget) to allocate over a finite time
horizon. At each stage, the learner strategically determines the budget
distribution among the battlefields based on past observations. The other
player is the adversary, who chooses its budget allocation strategies randomly
from some fixed unknown distribution. The learner's objective is to minimize
its regret, which is the difference between the payoff of the best mixed
strategy and the realized payoff by following a learning algorithm. The dynamic
CBG is analyzed under the framework of combinatorial bandit and bandit with
knapsacks. We first convert the dynamic CBG with budget constraint to a path
planning problem on a graph. We then devise an efficient dynamic policy for the
learner that uses a combinatorial bandit algorithm Edge on the path planning
graph as a subroutine for another algorithm LagrangeBwK. It is shown that under
the proposed policy, the learner's regret is bounded with high probability by a
term sublinear in time horizon $T$ and polynomial with respect to other
parameters.

    

### [[2105.03962] Stochastic Multi-Armed Bandits with Control Variates](http://arxiv.org/abs/2105.03962)


  This paper studies a new variant of the stochastic multi-armed bandits
problem where auxiliary information about the arm rewards is available in the
form of control variates. In many applications like queuing and wireless
networks, the arm rewards are functions of some exogenous variables. The mean
values of these variables are known a priori from historical data and can be
used as control variates. Leveraging the theory of control variates, we obtain
mean estimates with smaller variance and tighter confidence bounds. We develop
an improved upper confidence bound based algorithm named UCB-CV and
characterize the regret bounds in terms of the correlation between rewards and
control variates when they follow a multivariate normal distribution. We also
extend UCB-CV to other distributions using resampling methods like Jackknifing
and Splitting. Experiments on synthetic problem instances validate performance
guarantees of the proposed algorithms.

    

### [[2105.10049] Learning Modular Robot Control Policies](http://arxiv.org/abs/2105.10049)


  Modular robots can be rearranged into a new design, perhaps each day, to
handle a wide variety of tasks by forming a customized robot for each new task.
However, reconfiguring just the mechanism is not sufficient: each design also
requires its own unique control policy. One could craft a policy from scratch
for each new design, but such an approach is not scalable, especially given the
large number of designs that can be generated from even a small set of modules.
Instead, we create a modular policy framework where the policy structure is
conditioned on the hardware arrangement, and use just one training process to
create a policy that controls a wide variety of designs. Our approach leverages
the fact that the kinematics of a modular robot can be represented as a design
graph, with nodes as modules and edges as connections between them. Given a
robot, its design graph is used to create a policy graph with the same
structure, where each node contains a deep neural network, and modules of the
same type share knowledge via shared parameters (e.g., all legs on a hexapod
share the same network parameters). We developed a model-based reinforcement
learning algorithm, interleaving model learning and trajectory optimization to
train the policy. We show the modular policy generalizes to a large number of
designs that were not seen during training without any additional learning.
Finally, we demonstrate the policy controlling a variety of designs to locomote
with both simulated and real robots.

    

### [[2105.13570] Detecting the hosts of bacteriophages using GCN-based semi-supervised learning](http://arxiv.org/abs/2105.13570)


  Background: Prokaryotic viruses, which infect bacteria and archaea, are the
most abundant and diverse biological entities in the biosphere. To understand
their regulatory roles in various ecosystems and to harness the potential of
bacteriophages for use in therapy, more knowledge of viral-host relationships
is required. High-throughput sequencing and its application to the microbiome
have offered new opportunities for computational approaches for predicting
which hosts particular viruses can infect. However, there are two main
challenges for computational host prediction. First, the empirically known
virus-host relationships are very limited. Second, although sequence similarity
between viruses and their prokaryote hosts have been used as a major feature
for host prediction, the alignment is either missing or ambiguous in many
cases. Thus, there is still a need to improve the accuracy of host prediction.
Results: In this work, we present a semi-supervised learning model, named
HostG, to conduct host prediction for novel viruses. We construct a knowledge
graph by utilizing both virus-virus protein similarity and virus-host DNA
sequence similarity. Then graph convolutional network (GCN) is adopted to
exploit viruses with or without known hosts in training to enhance the learning
ability. During the GCN training, we minimize the expected calibrated error
(ECE) to ensure the confidence of the predictions. We tested HostG on both
simulated and real sequencing data and compared its performance with other
state-of-the-art methods specifcally designed for virus host classification
(VHM-net, WIsH, PHP, HoPhage, RaFAH, vHULK, and VPF-Class). Conclusion: HostG
outperforms other popular methods, demonstrating the efficacy of using a
GCN-based semi-supervised learning approach. A particular advantage of HostG is
its ability to predict hosts from new taxa.

    

### [[2106.02988] Causal Bandits with Unknown Graph Structure](http://arxiv.org/abs/2106.02988)


  In causal bandit problems, the action set consists of interventions on
variables of a causal graph. Several researchers have recently studied such
bandit problems and pointed out their practical applications. However, all
existing works rely on a restrictive and impractical assumption that the
learner is given full knowledge of the causal graph structure upfront. In this
paper, we develop novel causal bandit algorithms without knowing the causal
graph. Our algorithms work well for causal trees, causal forests and a general
class of causal graphs. The regret guarantees of our algorithms greatly improve
upon those of standard multi-armed bandit (MAB) algorithms under mild
conditions. Lastly, we prove our mild conditions are necessary: without them
one cannot do better than standard MAB algorithms.

    

### [[2106.07963] Capabilities of Deep Learning Models on Learning Physical Relationships: Case of Rainfall-Runoff Modeling with LSTM](http://arxiv.org/abs/2106.07963)


  This study investigates the relationships which deep learning methods can
identify between the input and output data. As a case study, rainfall-runoff
modeling in a snow-dominated watershed by means of a long- and short-term
memory (LSTM) network is selected. Daily precipitation and mean air temperature
were used as model input to estimate daily flow discharge. After model training
and verification, two experimental simulations were conducted with hypothetical
inputs instead of observed meteorological data to clarify the response of the
trained model to the inputs. The first numerical experiment showed that even
without input precipitation, the trained model generated flow discharge,
particularly winter low flow and high flow during the snow-melting period. The
effects of warmer and colder conditions on the flow discharge were also
replicated by the trained model without precipitation. Additionally, the model
reflected only 17-39% of the total precipitation mass during the snow
accumulation period in the total annual flow discharge, revealing a strong lack
of water mass conservation. The results of this study indicated that a deep
learning method may not properly learn the explicit physical relationships
between input and target variables, although they are still capable of
maintaining strong goodness-of-fit results.

    

### [[2106.08377] Implicit Finite-Horizon Approximation and Efficient Optimal Algorithms for Stochastic Shortest Path](http://arxiv.org/abs/2106.08377)


  We introduce a generic template for developing regret minimization algorithms
in the Stochastic Shortest Path (SSP) model, which achieves minimax optimal
regret as long as certain properties are ensured. The key of our analysis is a
new technique called implicit finite-horizon approximation, which approximates
the SSP model by a finite-horizon counterpart only in the analysis without
explicit implementation. Using this template, we develop two new algorithms:
the first one is model-free (the first in the literature to our knowledge) and
minimax optimal under strictly positive costs; the second one is model-based
and minimax optimal even with zero-cost state-action pairs, matching the best
existing result from [Tarbouriech et al., 2021b]. Importantly, both algorithms
admit highly sparse updates, making them computationally more efficient than
all existing algorithms. Moreover, both can be made completely parameter-free.

    

### [[2106.08855] Beyond Tikhonov: Faster Learning with Self-Concordant Losses via Iterative Regularization](http://arxiv.org/abs/2106.08855)


  The theory of spectral filtering is a remarkable tool to understand the
statistical properties of learning with kernels. For least squares, it allows
to derive various regularization schemes that yield faster convergence rates of
the excess risk than with Tikhonov regularization. This is typically achieved
by leveraging classical assumptions called source and capacity conditions,
which characterize the difficulty of the learning task. In order to understand
estimators derived from other loss functions, Marteau-Ferey et al. have
extended the theory of Tikhonov regularization to generalized self concordant
loss functions (GSC), which contain, e.g., the logistic loss. In this paper, we
go a step further and show that fast and optimal rates can be achieved for GSC
by using the iterated Tikhonov regularization scheme, which is intrinsically
related to the proximal point method in optimization, and overcomes the
limitation of the classical Tikhonov regularization.

    

### [[2106.11723] Neural Distributed Image Compression using Common Information](http://arxiv.org/abs/2106.11723)


  We present a novel deep neural network (DNN) architecture for compressing an
image when a correlated image is available as side information only at the
decoder. This problem is known as distributed source coding (DSC) in
information theory. In particular, we consider a pair of stereo images, which
generally have high correlation with each other due to overlapping fields of
view, and assume that one image of the pair is to be compressed and
transmitted, while the other image is available only at the decoder. In the
proposed architecture, the encoder maps the input image to a latent space,
quantizes the latent representation, and compresses it using entropy coding.
The decoder is trained to extract the common information between the input
image and the correlated image, using only the latter. The received latent
representation and the locally generated common information are passed through
a decoder network to obtain an enhanced reconstruction of the input image. The
common information provides a succinct representation of the relevant
information at the receiver. We train and demonstrate the effectiveness of the
proposed approach on the KITTI and Cityscape datasets of stereo image pairs.
Our results show that the proposed architecture is capable of exploiting the
decoder-only side information, and outperforms previous work on stereo image
compression with decoder side information.

    

### [[2106.11959] Revisiting Deep Learning Models for Tabular Data](http://arxiv.org/abs/2106.11959)


  The existing literature on deep learning for tabular data proposes a wide
range of novel architectures and reports competitive results on various
datasets. However, the proposed models are usually not properly compared to
each other and existing works often use different benchmarks and experiment
protocols. As a result, it is unclear for both researchers and practitioners
what models perform best. Additionally, the field still lacks effective
baselines, that is, the easy-to-use models that provide competitive performance
across different problems.
In this work, we perform an overview of the main families of DL architectures
for tabular data and raise the bar of baselines in tabular DL by identifying
two simple and powerful deep architectures. The first one is a ResNet-like
architecture which turns out to be a strong baseline that is often missing in
prior works. The second model is our simple adaptation of the Transformer
architecture for tabular data, which outperforms other solutions on most tasks.
Both models are compared to many existing architectures on a diverse set of
tasks under the same training and tuning protocols. We also compare the best DL
models with Gradient Boosted Decision Trees and conclude that there is still no
universally superior solution.

    

### [[2108.04167] Training very large scale nonlinear SVMs using Alternating Direction Method of Multipliers coupled with the Hierarchically Semi-Separable kernel approximations](http://arxiv.org/abs/2108.04167)


  Typically, nonlinear Support Vector Machines (SVMs) produce significantly
higher classification quality when compared to linear ones but, at the same
time, their computational complexity is prohibitive for large-scale datasets:
this drawback is essentially related to the necessity to store and manipulate
large, dense and unstructured kernel matrices. Despite the fact that at the
core of training a SVM there is a \textit{simple} convex optimization problem,
the presence of kernel matrices is responsible for dramatic performance
reduction, making SVMs unworkably slow for large problems. Aiming to an
efficient solution of large-scale nonlinear SVM problems, we propose the use of
the \textit{Alternating Direction Method of Multipliers} coupled with
\textit{Hierarchically Semi-Separable} (HSS) kernel approximations. As shown in
this work, the detailed analysis of the interaction among their algorithmic
components unveils a particularly efficient framework and indeed, the presented
experimental results demonstrate a significant speed-up when compared to the
\textit{state-of-the-art} nonlinear SVM libraries (without significantly
affecting the classification accuracy).

    

### [[2110.05192] Convex-Concave Min-Max Stackelberg Games](http://arxiv.org/abs/2110.05192)


  Min-max optimization problems (i.e., min-max games) have been attracting a
great deal of attention because of their applicability to a wide range of
machine learning problems. Although significant progress has been made
recently, the literature to date has focused on games with independent strategy
sets; little is known about solving games with dependent strategy sets, which
can be characterized as min-max Stackelberg games. We introduce two first-order
methods that solve a large class of convex-concave min-max Stackelberg games,
and show that our methods converge in polynomial time. Min-max Stackelberg
games were first studied by Wald, under the posthumous name of Wald's maximin
model, a variant of which is the main paradigm used in robust optimization,
which means that our methods can likewise solve many convex robust optimization
problems. We observe that the computation of competitive equilibria in Fisher
markets also comprises a min-max Stackelberg game. Further, we demonstrate the
efficacy and efficiency of our algorithms in practice by computing competitive
equilibria in Fisher markets with varying utility structures. Our experiments
suggest potential ways to extend our theoretical results, by demonstrating how
different smoothness properties can affect the convergence rate of our
algorithms.

    

### [[2110.12634] Accelerated Almost-Sure Convergence Rates for Nonconvex Stochastic Gradient Descent using Stochastic Learning Rates](http://arxiv.org/abs/2110.12634)


  Large-scale optimization problems require algorithms both effective and
efficient. One such popular and proven algorithm is Stochastic Gradient Descent
which uses first-order gradient information to solve these problems. This paper
studies almost-sure convergence rates of the Stochastic Gradient Descent method
when instead of deterministic, its learning rate becomes stochastic. In
particular, its learning rate is equipped with a multiplicative stochasticity,
producing a stochastic learning rate scheme. Theoretical results show
accelerated almost-sure convergence rates of Stochastic Gradient Descent in a
nonconvex setting when using an appropriate stochastic learning rate, compared
to a deterministic-learning-rate scheme. The theoretical results are verified
empirically.

    

### [[2111.00790] Dynamic Pricing and Demand Learning on a Large Network of Products: A PAC-Bayesian Approach](http://arxiv.org/abs/2111.00790)


  We consider a seller offering a large network of $N$ products over a time
horizon of $T$ periods. The seller does not know the parameters of the
products' linear demand model, and can dynamically adjust product prices to
learn the demand model based on sales observations. The seller aims to minimize
its pseudo-regret, i.e., the expected revenue loss relative to a clairvoyant
who knows the underlying demand model. We consider a sparse set of demand
relationships between products to characterize various connectivity properties
of the product network. In particular, we study three different sparsity
frameworks: (1) $L_0$ sparsity, which constrains the number of connections in
the network, and (2) off-diagonal sparsity, which constrains the magnitude of
cross-product price sensitivities, and (3) a new notion of spectral sparsity,
which constrains the asymptotic decay of a similarity metric on network nodes.
We propose a dynamic pricing-and-learning policy that combines the
optimism-in-the-face-of-uncertainty and PAC-Bayesian approaches, and show that
this policy achieves asymptotically optimal performance in terms of $N$ and
$T$. We also show that in the case of spectral and off-diagonal sparsity, the
seller can have a pseudo-regret linear in $N$, even when the network is dense.

    

### [[2111.05651] Porting incompressible flow matrix assembly to FPGAs for accelerating HPC engineering simulations](http://arxiv.org/abs/2111.05651)


  Engineering is an important domain for supercomputing, with the Alya model
being a popular code for undertaking such simulations. With ever increasing
demand from users to model larger, more complex systems at reduced time to
solution it is important to explore the role that novel hardware technologies,
such as FPGAs, can play in accelerating these workloads on future exascale
systems.
In this paper we explore the porting of Alya's incompressible flow matrix
assembly kernel, which accounts for a large proportion of the model runtime,
onto FPGAs. After describing in detail successful strategies for optimisation
at the kernel level, we then explore sharing the workload between the FPGA and
host CPU, mapping most appropriate parts of the kernel between these
technologies, enabling us to more effectively exploit the FPGA. We then compare
the performance of our approach on a Xilinx Alveo U280 against a 24-core Xeon
Platinum CPU and Nvidia V100 GPU, with the FPGA significantly out-performing
the CPU and performing comparably against the GPU, whilst drawing substantially
less power. The result of this work is both an experience report describing
appropriate dataflow optimisations which we believe can be applied more widely
as a case-study across HPC codes, and a performance comparison for this
specific workload that demonstrates the potential for FPGAs in accelerating HPC
engineering simulations.

    

### [[2111.05654] Utilising urgent computing to tackle the spread of mosquito-borne diseases](http://arxiv.org/abs/2111.05654)


  It is estimated that around 80\% of the world's population live in areas
susceptible to at-least one major vector borne disease, and approximately 20%
of global communicable diseases are spread by mosquitoes. Furthermore, the
outbreaks of such diseases are becoming more common and widespread, with much
of this driven in recent years by socio-demographic and climatic factors. These
trends are causing significant worry to global health organisations, including
the CDC and WHO, and-so an important question is the role that technology can
play in addressing them.
In this work we describe the integration of an epidemiology model, which
simulates the spread of mosquito-borne diseases, with the VESTEC urgent
computing ecosystem. The intention of this work is to empower human health
professionals to exploit this model and more easily explore the progression of
mosquito-borne diseases. Traditionally in the domain of the few research
scientists, by leveraging state of the art visualisation and analytics
techniques, all supported by running the computational workloads on HPC
machines in a seamless fashion, we demonstrate the significant advantages that
such an integration can provide. Furthermore we demonstrate the benefits of
using an ecosystem such as VESTEC, which provides a framework for urgent
computing, in supporting the easy adoption of these technologies by the
epidemiologists and disaster response professionals more widely.

    

### [[2111.05777] Power-of-two Policies in Redundancy Systems: the Impact of Assignment Constraints](http://arxiv.org/abs/2111.05777)


  In classical power-of-two load balancing any server pair is sampled with
equal probability. This does not cover practical settings with assignment
constraints which force non-uniform server sampling. While intuition suggests
that non-uniform sampling adversely impacts performance, this was only
supported through simulations, and rigorous statements have remained elusive.
Building on product-form distributions for redundancy systems, we prove the
stochastic dominance of uniform sampling for a four-server system as well as
arbitrary-size systems in light traffic.

    

### [[2111.05364] Towards Tractable Mathematical Reasoning: Challenges, Strategies, and Opportunities for Solving Math Word Problems](http://arxiv.org/abs/2111.05364)


  Mathematical reasoning would be one of the next frontiers for artificial
intelligence to make significant progress. The ongoing surge to solve math word
problems (MWPs) and hence achieve better mathematical reasoning ability would
continue to be a key line of research in the coming time. We inspect non-neural
and neural methods to solve math word problems narrated in a natural language.
We also highlight the ability of these methods to be generalizable,
mathematically reasonable, interpretable, and explainable. Neural approaches
dominate the current state of the art, and we survey them highlighting three
strategies to MWP solving: (1) direct answer generation, (2) expression tree
generation for inferring answers, and (3) template retrieval for answer
computation. Moreover, we discuss technological approaches, review the
evolution of intuitive design choices to solve MWPs, and examine them for
mathematical reasoning ability. We finally identify several gaps that warrant
the need for external knowledge and knowledge-infused learning, among several
other opportunities in solving MWPs.

    

### [[2111.05391] Statistical Perspectives on Reliability of Artificial Intelligence Systems](http://arxiv.org/abs/2111.05391)


  Artificial intelligence (AI) systems have become increasingly popular in many
areas. Nevertheless, AI technologies are still in their developing stages, and
many issues need to be addressed. Among those, the reliability of AI systems
needs to be demonstrated so that the AI systems can be used with confidence by
the general public. In this paper, we provide statistical perspectives on the
reliability of AI systems. Different from other considerations, the reliability
of AI systems focuses on the time dimension. That is, the system can perform
its designed functionality for the intended period. We introduce a so-called
SMART statistical framework for AI reliability research, which includes five
components: Structure of the system, Metrics of reliability, Analysis of
failure causes, Reliability assessment, and Test planning. We review
traditional methods in reliability data analysis and software reliability, and
discuss how those existing methods can be transformed for reliability modeling
and assessment of AI systems. We also describe recent developments in modeling
and analysis of AI reliability and outline statistical research challenges in
this area, including out-of-distribution detection, the effect of the training
set, adversarial attacks, model accuracy, and uncertainty quantification, and
discuss how those topics can be related to AI reliability, with illustrative
examples. Finally, we discuss data collection and test planning for AI
reliability assessment and how to improve system designs for higher AI
reliability. The paper closes with some concluding remarks.

    

### [[2111.05409] Pipeline for 3D reconstruction of the human body from AR/VR headset mounted egocentric cameras](http://arxiv.org/abs/2111.05409)


  In this paper, we propose a novel pipeline for the 3D reconstruction of the
full body from egocentric viewpoints. 3-D reconstruction of the human body from
egocentric viewpoints is a challenging task as the view is skewed and the body
parts farther from the cameras are occluded. One such example is the view from
cameras installed below VR headsets. To achieve this task, we first make use of
conditional GANs to translate the egocentric views to full body third-person
views. This increases the comprehensibility of the image and caters to
occlusions. The generated third-person view is further sent through the 3D
reconstruction module that generates a 3D mesh of the body. We also train a
network that can take the third person full-body view of the subject and
generate the texture maps for applying on the mesh. The generated mesh has
fairly realistic body proportions and is fully rigged allowing for further
applications such as real-time animation and pose transfer in games. This
approach can be key to a new domain of mobile human telepresence.

    

### [[2111.05514] Discovering Latent Representations of Relations for Interacting Systems](http://arxiv.org/abs/2111.05514)


  Systems whose entities interact with each other are common. In many
interacting systems, it is difficult to observe the relations between entities
which is the key information for analyzing the system. In recent years, there
has been increasing interest in discovering the relationships between entities
using graph neural networks. However, existing approaches are difficult to
apply if the number of relations is unknown or if the relations are complex. We
propose the DiScovering Latent Relation (DSLR) model, which is flexibly
applicable even if the number of relations is unknown or many types of
relations exist. The flexibility of our DSLR model comes from the design
concept of our encoder that represents the relation between entities in a
latent space rather than a discrete variable and a decoder that can handle many
types of relations. We performed the experiments on synthetic and real-world
graph data with various relationships between entities, and compared the
qualitative and quantitative results with other approaches. The experiments
show that the proposed method is suitable for analyzing dynamic graphs with an
unknown number of complex relations.

    

### [[2111.05527] LUMINOUS: Indoor Scene Generation for Embodied AI Challenges](http://arxiv.org/abs/2111.05527)


  Learning-based methods for training embodied agents typically require a large
number of high-quality scenes that contain realistic layouts and support
meaningful interactions. However, current simulators for Embodied AI (EAI)
challenges only provide simulated indoor scenes with a limited number of
layouts. This paper presents Luminous, the first research framework that
employs state-of-the-art indoor scene synthesis algorithms to generate
large-scale simulated scenes for Embodied AI challenges. Further, we
automatically and quantitatively evaluate the quality of generated indoor
scenes via their ability to support complex household tasks. Luminous
incorporates a novel scene generation algorithm (Constrained Stochastic Scene
Generation (CSSG)), which achieves competitive performance with human-designed
scenes. Within Luminous, the EAI task executor, task instruction generation
module, and video rendering toolkit can collectively generate a massive
multimodal dataset of new scenes for the training and evaluation of Embodied AI
agents. Extensive experimental results demonstrate the effectiveness of the
data generated by Luminous, enabling the comprehensive assessment of embodied
agents on generalization and robustness.

    

### [[2111.05548] Deep Attention-guided Graph Clustering with Dual Self-supervision](http://arxiv.org/abs/2111.05548)


  Existing deep embedding clustering works only consider the deepest layer to
learn a feature embedding and thus fail to well utilize the available
discriminative information from cluster assignments, resulting performance
limitation. To this end, we propose a novel method, namely deep
attention-guided graph clustering with dual self-supervision (DAGC).
Specifically, DAGC first utilizes a heterogeneity-wise fusion module to
adaptively integrate the features of an auto-encoder and a graph convolutional
network in each layer and then uses a scale-wise fusion module to dynamically
concatenate the multi-scale features in different layers. Such modules are
capable of learning a discriminative feature embedding via an attention-based
mechanism. In addition, we design a distribution-wise fusion module that
leverages cluster assignments to acquire clustering results directly. To better
explore the discriminative information from the cluster assignments, we develop
a dual self-supervision solution consisting of a soft self-supervision strategy
with a triplet Kullback-Leibler divergence loss and a hard self-supervision
strategy with a pseudo supervision loss. Extensive experiments validate that
our method consistently outperforms state-of-the-art methods on six benchmark
datasets. Especially, our method improves the ARI by more than 18.14% over the
best baseline.

    

### [[2111.05578] Conversational Recommendation:Theoretical Model and Complexity Analysis](http://arxiv.org/abs/2111.05578)


  Recommender systems are software applications that help users find items of
interest in situations of information overload in a personalized way, using
knowledge about the needs and preferences of individual users. In
conversational recommendation approaches, these needs and preferences are
acquired by the system in an interactive, multi-turn dialog. A common approach
in the literature to drive such dialogs is to incrementally ask users about
their preferences regarding desired and undesired item features or regarding
individual items. A central research goal in this context is efficiency,
evaluated with respect to the number of required interactions until a
satisfying item is found. This is usually accomplished by making inferences
about the best next question to ask to the user. Today, research on dialog
efficiency is almost entirely empirical, aiming to demonstrate, for example,
that one strategy for selecting questions is better than another one in a given
application. With this work, we complement empirical research with a
theoretical, domain-independent model of conversational recommendation. This
model, which is designed to cover a range of application scenarios, allows us
to investigate the efficiency of conversational approaches in a formal way, in
particular with respect to the computational complexity of devising optimal
interaction strategies. Through such a theoretical analysis we show that
finding an efficient conversational strategy is NP-hard, and in PSPACE in
general, but for particular kinds of catalogs the upper bound lowers to
POLYLOGSPACE. From a practical point of view, this result implies that catalog
characteristics can strongly influence the efficiency of individual
conversational strategies and should therefore be considered when designing new
strategies. A preliminary empirical analysis on datasets derived from a
real-world one aligns with our findings.

    

### [[2111.05628] Machine Learning Models Disclosure from Trusted Research Environments (TRE), Challenges and Opportunities](http://arxiv.org/abs/2111.05628)


  Trusted Research environments (TRE)s are safe and secure environments in
which researchers can access sensitive data. With the growth and diversity of
medical data such as Electronic Health Records (EHR), Medical Imaging and
Genomic data, there is an increase in the use of Artificial Intelligence (AI)
in general and the subfield of Machine Learning (ML) in particular in the
healthcare domain. This generates the desire to disclose new types of outputs
from TREs, such as trained machine learning models. Although specific
guidelines and policies exists for statistical disclosure controls in TREs,
they do not satisfactorily cover these new types of output request. In this
paper, we define some of the challenges around the application and disclosure
of machine learning for healthcare within TREs. We describe various
vulnerabilities the introduction of AI brings to TREs. We also provide an
introduction to the different types and levels of risks associated with the
disclosure of trained ML models. We finally describe the new research
opportunities in developing and adapting policies and tools for safely
disclosing machine learning outputs from TREs.

    

### [[2111.05691] HASA-net: A non-intrusive hearing-aid speech assessment network](http://arxiv.org/abs/2111.05691)


  Without the need of a clean reference, non-intrusive speech assessment
methods have caught great attention for objective evaluations. Recently, deep
neural network (DNN) models have been applied to build non-intrusive speech
assessment approaches and confirmed to provide promising performance. However,
most DNN-based approaches are designed for normal-hearing listeners without
considering hearing-loss factors. In this study, we propose a DNN-based hearing
aid speech assessment network (HASA-Net), formed by a bidirectional long
short-term memory (BLSTM) model, to predict speech quality and intelligibility
scores simultaneously according to input speech signals and specified
hearing-loss patterns. To the best of our knowledge, HASA-Net is the first work
to incorporate quality and intelligibility assessments utilizing a unified
DNN-based non-intrusive model for hearing aids. Experimental results show that
the predicted speech quality and intelligibility scores of HASA-Net are highly
correlated to two well-known intrusive hearing-aid evaluation metrics, hearing
aid speech quality index (HASQI) and hearing aid speech perception index
(HASPI), respectively.

    

### [[2111.05794] PIMIP: An Open Source Platform for Pathology Information Management and Integration](http://arxiv.org/abs/2111.05794)


  Digital pathology plays a crucial role in the development of artificial
intelligence in the medical field. The digital pathology platform can make the
pathological resources digital and networked, and realize the permanent storage
of visual data and the synchronous browsing processing without the limitation
of time and space. It has been widely used in various fields of pathology.
However, there is still a lack of an open and universal digital pathology
platform to assist doctors in the management and analysis of digital
pathological sections, as well as the management and structured description of
relevant patient information. Most platforms cannot integrate image viewing,
annotation and analysis, and text information management. To solve the above
problems, we propose a comprehensive and extensible platform PIMIP. Our PIMIP
has developed the image annotation functions based on the visualization of
digital pathological sections. Our annotation functions support multi-user
collaborative annotation and multi-device annotation, and realize the
automation of some annotation tasks. In the annotation task, we invited a
professional pathologist for guidance. We introduce a machine learning module
for image analysis. The data we collected included public data from local
hospitals and clinical examples. Our platform is more clinical and suitable for
clinical use. In addition to image data, we also structured the management and
display of text information. So our platform is comprehensive. The platform
framework is built in a modular way to support users to add machine learning
modules independently, which makes our platform extensible.

    

### [[2111.05819] Look Before You Leap: Safe Model-Based Reinforcement Learning with Human Intervention](http://arxiv.org/abs/2111.05819)


  Safety has become one of the main challenges of applying deep reinforcement
learning to real world systems. Currently, the incorporation of external
knowledge such as human oversight is the only means to prevent the agent from
visiting the catastrophic state. In this paper, we propose MBHI, a novel
framework for safe model-based reinforcement learning, which ensures safety in
the state-level and can effectively avoid both "local" and "non-local"
catastrophes. An ensemble of supervised learners are trained in MBHI to imitate
human blocking decisions. Similar to human decision-making process, MBHI will
roll out an imagined trajectory in the dynamics model before executing actions
to the environment, and estimate its safety. When the imagination encounters a
catastrophe, MBHI will block the current action and use an efficient MPC method
to output a safety policy. We evaluate our method on several safety tasks, and
the results show that MBHI achieved better performance in terms of sample
efficiency and number of catastrophes compared to the baselines.

    

### [[2111.05825] A Two-Stage Approach towards Generalization in Knowledge Base Question Answering](http://arxiv.org/abs/2111.05825)


  Most existing approaches for Knowledge Base Question Answering (KBQA) focus
on a specific underlying knowledge base either because of inherent assumptions
in the approach, or because evaluating it on a different knowledge base
requires non-trivial changes. However, many popular knowledge bases share
similarities in their underlying schemas that can be leveraged to facilitate
generalization across knowledge bases. To achieve this generalization, we
introduce a KBQA framework based on a 2-stage architecture that explicitly
separates semantic parsing from the knowledge base interaction, facilitating
transfer learning across datasets and knowledge graphs. We show that
pretraining on datasets with a different underlying knowledge base can
nevertheless provide significant performance gains and reduce sample
complexity. Our approach achieves comparable or state-of-the-art performance
for LC-QuAD (DBpedia), WebQSP (Freebase), SimpleQuestions (Wikidata) and MetaQA
(Wikimovies-KG).

    

### [[2111.05827] Data-Driven and SE-assisted AI Model Signal-Awareness Enhancement and Introspection](http://arxiv.org/abs/2111.05827)


  AI modeling for source code understanding tasks has been making significant
progress, and is being adopted in production development pipelines. However,
reliability concerns, especially whether the models are actually learning
task-related aspects of source code, are being raised. While recent
model-probing approaches have observed a lack of signal awareness in many
AI-for-code models, i.e. models not capturing task-relevant signals, they do
not offer solutions to rectify this problem. In this paper, we explore
data-driven approaches to enhance models' signal-awareness: 1) we combine the
SE concept of code complexity with the AI technique of curriculum learning; 2)
we incorporate SE assistance into AI models by customizing Delta Debugging to
generate simplified signal-preserving programs, augmenting them to the training
dataset. With our techniques, we achieve up to 4.8x improvement in model signal
awareness. Using the notion of code complexity, we further present a novel
model learning introspection approach from the perspective of the dataset.

    

### [[2103.07356] Hippocampal formation-inspired probabilistic generative model](http://arxiv.org/abs/2103.07356)


  We tackle the challenging task of bridging the gap between the
neuroscientific knowledge of hippocampal formation (HPF) and the engineering
knowledge of robotics and artificial intelligence. Simultaneous localization
and mapping (SLAM) has already been realized in robotics as a basic function
for spatial cognition. In this study, we aim to investigate how the SLAM
functionality corresponds to the HPF. To this end, a hypothesis based on a
literature review is suggested and a direction for its verification is
presented, without performing any new simulations. We survey HPF models and
various computational ones, including brain-inspired SLAM, spatial concept
formation, and deep generative models. Furthermore, we discuss the relationship
between the findings of HPF in neuroscience and SLAM in robotics. Thereby, A
hippocampal formation-inspired probabilistic generative model (PGM) was
constructed using a methodology for constructing a brain reference
architecture. We propose an HPF-PGM as a computational model based on a
modification of the conventional SLAM model, which is designed to be highly
consistent with the anatomical structure and functions of the HPF. By
referencing the brain, we suggest the importance of the integration of
egocentric/allocentric information from the entorhinal cortex to the
hippocampus and the use of discrete-event queues.

    

### [[2103.14953] OLED: One-Class Learned Encoder-Decoder Network with Adversarial Context Masking for Novelty Detection](http://arxiv.org/abs/2103.14953)


  Novelty detection is the task of recognizing samples that do not belong to
the distribution of the target class. During training, the novelty class is
absent, preventing the use of traditional classification approaches. Deep
autoencoders have been widely used as a base of many unsupervised novelty
detection methods. In particular, context autoencoders have been successful in
the novelty detection task because of the more effective representations they
learn by reconstructing original images from randomly masked images. However, a
significant drawback of context autoencoders is that random masking fails to
consistently cover important structures of the input image, leading to
suboptimal representations - especially for the novelty detection task. In this
paper, to optimize input masking, we have designed a framework consisting of
two competing networks, a Mask Module and a Reconstructor. The Mask Module is a
convolutional autoencoder that learns to generate optimal masks that cover the
most important parts of images. Alternatively, the Reconstructor is a
convolutional encoder-decoder that aims to reconstruct unperturbed images from
masked images. The networks are trained in an adversarial manner in which the
Mask Module generates masks that are applied to images given to the
Reconstructor. In this way, the Mask Module seeks to maximize the
reconstruction error that the Reconstructor is minimizing. When applied to
novelty detection, the proposed approach learns semantically richer
representations compared to context autoencoders and enhances novelty detection
at test time through more optimal masking. Novelty detection experiments on the
MNIST and CIFAR-10 image datasets demonstrate the proposed approach's
superiority over cutting-edge methods. In a further experiment on the UCSD
video dataset for novelty detection, the proposed approach achieves
state-of-the-art results.

    

### [[2111.04462] Creating A Coefficient of Change in the Built Environment After a Natural Disaster](http://arxiv.org/abs/2111.04462)


  This study proposes a novel method to assess damages in the built environment
using a deep learning workflow to quantify it. Thanks to an automated crawler,
aerial images from before and after a natural disaster of 50 epicenters
worldwide were obtained from Google Earth, generating a 10,000 aerial image
database with a spatial resolution of 2 m per pixel. The study utilizes the
algorithm Seg-Net to perform semantic segmentation of the built environment
from the satellite images in both instances (prior and post-natural disasters).
For image segmentation, Seg-Net is one of the most popular and general CNN
architectures. The Seg-Net algorithm used reached an accuracy of 92% in the
segmentation. After the segmentation, we compared the disparity between both
cases represented as a percentage of change. Such coefficient of change
represents the damage numerically an urban environment had to quantify the
overall damage in the built environment. Such an index can give the government
an estimate of the number of affected households and perhaps the extent of
housing damage.

    

### [[2009.06009] Efficiency Near the Edge: Increasing the Energy Efficiency of FFTs on GPUs for Real-time Edge Computing](http://arxiv.org/abs/2009.06009)


  The Square Kilometre Array (SKA) is an international initiative for
developing the world's largest radio telescope with a total collecting area of
over a million square meters. The scale of the operation, combined with the
remote location of the telescope, requires the use of energy-efficient
computational algorithms. This, along with the extreme data rates that will be
produced by the SKA and the requirement for a real-time observing capability,
necessitates in-situ data processing in an edge style computing solution. More
generally, energy efficiency in the modern computing landscape is becoming of
paramount concern. Whether it be the power budget that can limit some of the
world's largest supercomputers, or the limited power available to the smallest
Internet-of-Things devices. In this paper, we study the impact of hardware
frequency scaling on the energy consumption and execution time of the Fast
Fourier Transform (FFT) on NVIDIA GPUs using the cuFFT library. The FFT is used
in many areas of science and it is one of the key algorithms used in radio
astronomy data processing pipelines. Through the use of frequency scaling, we
show that we can lower the power consumption of the NVIDIA V100 GPU when
computing the FFT by up to 60% compared to the boost clock frequency, with less
than a 10% increase in the execution time. Furthermore, using one common core
clock frequency for all tested FFT lengths, we show on average a 50% reduction
in power consumption compared to the boost core clock frequency with an
increase in the execution time still below 10%. We demonstrate how these
results can be used to lower the power consumption of existing data processing
pipelines. These savings, when considered over years of operation, can yield
significant financial savings, but can also lead to a significant reduction of
greenhouse gas emissions.

    

### [[2111.05617] Software Model-Checking as Cyclic-Proof Search](http://arxiv.org/abs/2111.05617)


  This paper shows that a variety of software model-checking algorithms can be
seen as proof-search strategies for a non-standard proof system, known as a
cyclic proof system. Our use of the cyclic proof system as a logical foundation
of software model checking enables us to compare different algorithms, to
reconstruct well-known algorithms from a few simple principles, and to obtain
soundness proofs of algorithms for free. Among others, we show the significance
of a heuristics based on a notion that we call maximal conservativity; this
explains the cores of important algorithms such as property-directed
reachability (PDR) and reveals a surprising connection to an efficient solver
of games over infinite graphs that was not regarded as a kind of PDR.

    

### [<title data-react-helmet="true">ÂÖ®ÁΩëÈ¶ñ‰∏™OpenPromptÂ∞ùÈ≤úÊä•ÂëäÔºöPromptÁ†îÁ©∂ËÄÖÂøÖÂ§áÂÆûÈ™åÂà©Âô® - Áü•‰πé</title>](https://zhuanlan.zhihu.com/p/432016893)

### [<title>Extrapolation of a known function - XGBoost</title>](https://discuss.xgboost.ai/t/extrapolation-of-a-known-function/2536/1)